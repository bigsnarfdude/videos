So as Karina told you, I'm going to be building up on her talk and I'm going to present two examples of how you can use combinatorial threshold linear networks to model two neural functions. In particular, I want to talk today about central pattern generators. So I think we're all sort of familiar with it, but central pattern generators are Central pattern generators are networks of neurons that can create rhythmic output with no need for sensory feedback. So back in the 20th century, scientists realized that a cat could still make stepping movements without any sensory feedback, right? So this is what the central pattern generator is. The output doesn't have to be rhythmic necessarily, but the key thing is that it doesn't need any sensory feedback. So examples of So examples of CPGs are of behaviors controlled by CPGs are like walking, breathing, chewing, dancing, etc. The interesting thing about a CPG is that a single CPG can serve many purposes. So the one CPG that helps you to walk also helps you to jog, run, and spring, right? So what we want to do is that we want to encode Code several different dynamic behaviors in the same circuit, right? There are many different models. This is like not a very recent research area. There are many, many different models for CPGs. A lot of them consist of units that oscillate, like half-central oscillators and abstract oscillators, but the one we're using today are units. Our units do not self-oscillate, right? So the oscillation arises as a consequence of their recurring connections, right? Which is the model that Karina just talked about. A very interesting example of Azifigi is quadrupleted locomotion. Humans have always been interested in quadruple locomotion. Back in the beginning of cinema, there was this bet where you have to, where they were interested in. To where they were interested in how the horse makes their stepping movements, right? So, in 1997, Kolubitsky proposed an eight-neural network that can prong, pace, bound, trot, jump, and walk, which is amazing because remember I told you that we want to encode several different gates into the same network, right? So, this network consisted of eight. Consisted of eight Fitzyuna Guo neurons. So they were oscillators, it's not like our model. And they actually proved in this paper that whatever network you have will have to have these symmetries shown here. And that's pretty interesting. However, this network requires changing five parameters, right? So you have to change the four coupling strengths and then you have to change the charge. Coupling strengths, and then you have to change the parameter C over here in the equations, right? So we want to have a model that doesn't require changing parameters to access different attractors, which is what Karina said before, right? All these, we can have networks with coexisting attractors that only different, only differ by initial conditions, right? And that's what we want to do with our model. Do with our model. This is today is very much a very exciting research area. This paper is from 2019, this is a nature paper from 2019, where they have designed a four nano-oscillator circuit that can encode three different gates, right? Wagtrot and bound, right? So Right, so the key thing when you're designing robots is first you want to have several gates coexistent in the same network, and you also want to reduce the number of control parameters so that the implementation of the circuit is more simple, right? So, okay, so that's our first goal. So, our first goal is to create a circuit that can have many different gates coexistent, all accessible by a change. All accessible by a change in initial conditions doesn't require change in parameters, and that reduces the number of control parameters as much as we can. Okay, another very dynamically different example of a CPG is the chaotic swimming of this little mollusk here called Cleon. So this little sea angel, it's called, lacks a visual system, right? Lacks a visual system, right? Since it lacks a visual system, how can it hunt? Well, whenever it senses a prey near its head, like it chemically senses the prey, then it has to chaotically swim to cover as much space as possible and then find the prey and feeds. Right? So the tail of the sea angel is controlled, receives input from the statuses, which is this thing shown here in the right. And the statuses. And the status has mechanoreceptors here. You can see my own cursor. Thank you. Here, right? And inside of it, there's like a stone-like structure, right, that moves freely under the effect of gravity, right? So here you can see in this paper in 1995, they recorded the directional switching of Plion. So there's like in this little. Like in this little, in this part is ventral, then right, dorsal, left, ventral, right. So there's like really not a pattern in the way in which Cleon is switching directions. So we want to have a model for that. In the same paper, oh no, it's a different paper. In 2002, the one here on the bottom, yes, they proposed a network of A network of six lot cavalteroneurons, and the six neurons represent the possible swim interactions of Cleon. And they successfully reproduce the chaotic behavior. But the model was very fine-tuned and required very, very precise initial conditions to give rise to this behavior. So, what I want to do again. want to do again a CPG where several gates coexist in the same network and I want okay and I want a chaotic swimming network that is very robust and can mimic and possibly explain the chaotic switching of directions of Klio. Okay, so I'm gonna go super fast through this because Super fast through this because Karina just talked about it. So, I'm going to be using a special case of threshold linear networks. So, threshold linear networks have been widely used in modeling neural functions. The XI represent the firing rates. We have a leak term that will make the neuron turn off in the absence of input, a connectivity matrix, external inputs, and our threshold non-linearity that makes the mathematics super simple, but still the dynamics super simple. Simple, but still the dynamics are super rich. Okay, a special case of virtual linear networks is combinatorial threshold linear networks. And here, the special thing about it is that the connectivity matrix is prescribed by a directed graph, right? So if J weakly inhibits I, we have this value here. If J strongly inhibits I, we have a little less than minus one, right? We make the input constant to make sure that the dynamics are internally generated and do not arise because of ever-changing input. And we have very few parameters, which is like a key point that I mentioned at the beginning, right? So here, your network, your whole dynamics are going to be determined by a choice of the graph and of three parameters, theta, epsilon, and delta. Peta, epsilon, and dirta. Okay, again, uh, this uh, Karina also mentioned: uh, at a fixed point, you're gonna have uh a bunch of hyper planes, so you're gonna have a bunch of hyperplane hyperplanes dividing your state space. So, your state space is going to be dividing in chambers. We're going to label all the fixed points and we're going to, in each chamber, the system is linear, right? So, there will be at most one fixed point per One fixed point per chamber, right? So I'm going to talk about the fixed point supports and the fixed points interchangeably. And we collect all these fixed point supports into this set that we call FP of G. Oh, and we can also recover easily recover the fixed point from its support. So there's no damage in talking about the support instead of the fixed point because it's really easy to find it up. Really easy to find it out. Karina also mentioned this already. So there's a way in which we can relate the graph architecture to the fixed points, right, which are giving us the dynamic information, right? And that is the graph rules, which they have devised and I'm going to be using a lot of. So a very nice theorem is a cyclic theorem. So this is the one. So, this is the one I'm going to be using for the talk because we want rhythmic and behavior. So, we want to have sort of like a cyclic kind of structure. So, in a cyclic union, you have a we have a partition of our nodes into components, tau1, tau2, tau3, and tau n. And these component, the nodes in this component have to send edges to. This component has to send edges to everyone in this component. The notes in this component have to send edges to everyone in this component, and so on and so forth until it wraps up. So, that is the definition of a cyclic union. Whatever happens since I tau one and tau two and tau three, it doesn't really matter for the definition of a cyclic union. What we know about a cyclic union is that my fixed points of the whole network are made up or are made up of building blocks from the individual. Blocks from the individual components, right? So, for example, here we have a cyclic union of the node one, then two, three, and four, right? Because one sends to everyone here, and then six and five, five, and six, right? Because two, three, and four sends to everyone there, and then it wraps up to one. So, that is a cyclic unit. The individual fixed points, so this thing over here are for. Over here are for this subgraph is one, or this subgraph is oh, this is a type point. It should be two, three, four. And this subgraph is five, six, right? So the fixed point of my whole graph is going to be one, two, three, four, and five, and six. And we can see it here that indeed it is. And two, three, four, and five and six turn out to be synchronized, right? To be synchronized, right? So, with this theorem, with this theorem, we have a lot of control over cyclic activities, right? So, this already Karina mentioned, these theoretical tools will allow us to engineer networks with prescribed tractors. So, the first one, what did we want? We wanted a CPG where several gates coexist in the same network. We don't want any parameter changes and keep the parameters. Keep the parameters to a minimum. So we start by modeling a single gate, which is the bound gate. In the bound gate, this is like an Australian squirrel, doing the bound gate. So in the bound gate, the front legs are synchronized, the back legs are synchronized, and they alternate, have a period of pause, right? So what did we do? The most reasonable thing was to put The most reasonable thing was to put front legs in a single component. So, this red square means this is a single component. And these red arrows here, the color arrows, means this goes from everyone in this red component to nine. And nine goes from like nine to everyone in this component, right? So it is a cyclic union, but I just we draw it in a way that's easier to see, right? Easier to see, right? So we group front legs in a single component, back legs in a single component, and then we hope, right, by our theorem that our theorem, like, which is true, give us what we want. And indeed, it does. So the minimal supports, which is what Karina referred to as four motifs, are a bunch of them. But the important thing is that we have the ones that we want. That we have the ones that we want, right? So we have the ones in which 10 is firing, and two and three are firing simultaneously. And then nine is firing, and then one and four are firing simultaneously, right? Which is exactly the boundary. When we simulate this, we get what we expected. We have front legs synchronized, back legs synchronized, and they are alternate. Synchronize and they are alternating between each other. So, in this pink rectangle, this is an extract of what the pink rectangle is, right? So, you have the back legs here and then the front legs here. And then you can sort of see, it's very clear to see the squirrel jumping there, right? So, that's pretty fun. But remember that our goal was to encode as many gates as possible, right? So, uh So we tried to model a few more and we did pace in the exact same way, right? I'm going to talk about these other notes later. So we did pace in the same way. In pace, the left legs are synchronized, the right legs are synchronized, and again, they have a period of. These colors again means 15 goes to everyone in these colors. To everyone in this color square and so on. Same way we did trot. In trot, diagonal legs are synchronized and the other diagonal legs are synchronized. It works well. It follows the exact same idea, so it shouldn't fail. Walk was a little harder, but again, we're using the cyclic union idea in which we go to left front, then we go to right back. Go to right back, then we go to right front, and then we go back to left back, and then start over, right? So that is walk. And finally, we have front, which is the simplest one, which is all legs at the same time. So it's just like jumping, right? And then at the end, we were able to fuse all of these gates into a single network, which is the reason why we needed these auxiliary nodes in the first place. And all the attractors. And all the attractors were still present in this super network that I like to call super network over here, right? So we do have quite a bit of nodes. We have 24 nodes, but we were able to encode five gates, right? So, and these gates are accessible depending on initial conditions. So, for example, here I initialize in a node 13 the activity. So, since node 13, So since node 13 corresponds to the bound gate, then it goes into the bound gate. Similarly here, we initialize in node 15. Since node 15 corresponds to pace, it goes into phase. And so the key is to initialize your activity in the node corresponding to the gate. Now, since this is the case, we should be theoretically, it should be theoretically possible to change gates by To change gates by a pulse to a neuron, right? So here the activity was initialized in 14, right? So it went into bound. And then you send a pulse to a theta pulse to neuron 15. So you momentarily increase the value of theta in your equations, and then it goes into pace, which is very similar. To pace, which is very similar to what I think we do when we change in paces. It's like a very, we receive an external input and we seamlessly change gates. If we send a post to 19, 19 corresponds to walk, it goes into walk, and so on. So everything's looking good. And since everything was looking good, we were wondering what happens if we have noise, right? If we have noise, right? So here we add a little bit of noise into the connectivity matrix, just the connectivity matrix, and the transitions are still there. It takes a little while to settle into the appropriate attractors, but they're still there, right? However, when we went up to 5% noise, we realized that there is a transition failure here, right? It should have gone into trot, which is the one here. One here, and well, it failed. This shows that, uh, well, a little noise is fine, right? So, the perfect synaptic connections that we had device in W are not really necessary to have this behavior, but still we have to like test a little more for like a stronger, a bigger amount of noise. Okay, so the next network. So the next network is the chaotic swimming network. So this is the network that we design. So it's an octahedral network where each neuron represents a direction in which you can swim, in which the sea angel can swim, right? You can swim up, down, left or right, or back in front, right? So you cover all the three-dimensional space, right? So you have all the possible directions in which you can swim are in code. Directions in which you can swim are encoded here. And we run our simulation and discover that, depending on the initial conditions, then the network will go into some attractors. So, for example, here, we initialize in one, two, five, and six are the strongest firing ones, but five and two, five and two are opposite directions, so they cannot fire. Opposite directions. So they cannot fire at the same time, right? You wouldn't want to have, it would be like contradictory, right? And then what the system does is that it goes into the highest of the two, right? So it goes into one, two, and six, which is one, two, and six, right? This three cycle on the top here. So since this is made up of, well, we can redraw this network in a different way. We can draw it as a cyclic union. It as a cyclic union. So here I drew the network as a cyclic union. So one and four are here and here because it's wrapping up. And what we have here is two nodes. They don't have connections between them because they're opposing directions. So they are supposed to weakly inhibit each other, right? So you don't want to have up and down at the same time. The cyclic union theorem is telling us that we're going to have the following We're going to have the following minimal supports, right? We're going to take, oh, yes. So we're going to take one note from here, one note from here, and one note from here, right? So in total, we're going to have eight of them, which exactly corresponds to the eight coherent directions in which you should be able to swim, right? So we're going to have eight attractors, as I showed you before here. We have, at least we know that we have one, two, zero. We know that we have one, two, six, right? But since this network is so symmetric, we know that we should have, because for the rest of the network, one and four are the same thing. They receive from the same neurons and they send to the same neurons, right? So by the symmetry of this network, it's not easy to see that we're going to have at least, we don't know that there's no others, but at least we have the eight attractors that we want to have with the respective basins of attractions. Again, Again, which transitions are possible is going to be dictated by the minimal supports. We can only transition between these minimal supports. So when we transition from one attractor to another, we have to transition between opposing neurons, right? So I have this hat notation, which is just telling me the opposing neurons. So for example, one hat. Opposing neurons. So, for example, one hat is four, two hat is five, three hat is six, six hat is three, right? So, uh, yeah, so we were trying to see the experiment with the transitions that we know are only coherent. And here, we started in that in downright back, and then oh okay, we started in downright back, and then we went into we sent a And then we went into, we sent a pulse to left, right? Uh, the degree here. So, if we send a pulse to left, right has to turn off, right? And left has to turn on, right? And it's exactly what happens, right? Back turns, back keeps just doing its thing because nothing happened to it. Down is also turned on, but then left replace right, which is makes a lot of sense, right? Then. Then we were doing more experiments, and then we found this. So we had up, right, and back, right? And then we sent a pulse to down, right? So what we want is a down turns on and replaces up. However, this transition failed. But why was that? So if we look closely here, the baby green, right, which is the pulse we sent to, managed. We send to manage to get slightly bigger than its opposing neuron, which is color-coded, and so is the dark green. And the transition was successful. However, here, we sent a dark blue pulse, and the dark blue was not able to get stronger than baby blue at the end of the pulse, right? So the transition failed. So, so there's like here we can see that there's like some sort of uh randomness in the direction switching of Cleon, right? And it could be it could be because of many things have to happen at the same time so that Cleon can transition, right? The pulse has to be long enough or strong enough, or it has to get, it has to arrive at the exact time so that the pulse. So that the pulse neuron can win to the, can like get bigger than its opposing neuron, so that the transition is successful. And we were actually able to prove that the basis of attraction, which we label as IJK, depending on which neurons are high-firing. So, for example, this would be 4-5-6, right? Are contained in these cone-like spaces in state space, and where yes, where the high firing neuron is the one corresponding to that attractor, right? So, this would imply that if a neuron gets like a little bit bigger than its supposing neuron, right, if this is Supposing neuron, right? If this inequality gets switched, I'm thrown into the basin of attraction of a different attraction, right? So I successfully transition. And this wasn't very hard to prove because if we have two opposing neurons, I and I hat, so thing one and four, they receive from the same neurons, the exact same neurons in the network, right? So it's going to be impossible to cross the hyperplane. The hyperplane that separates these sets over now, I was pointing with my finger, that separates these sets over here. So this is very easy to see from the equations if these are the equations from I and I hat. Since they receive from the same neurons, these two terms are exactly the same. And if at some point they go into this hyperplane, right? A hyperplane, right? At some T0, they are the same, then their derivatives are going to be the same and they're going to keep being the same forever, right? So I'm stuck in these sets over here, right? And then I would go into a different basin of attraction. Yes, so that is it for the swimming network. I think it's very fun, is very reasonable, and it's exactly what you would expect from a direction controlling network. Controlling network. And it's almost time, right? Yes. Oh my God. So what I wanted to show you today is that I show you several different models, right? But then I gave you examples of a single model controlling the same functions, right? So we hope that this illustrates how CTLS can provide a unifying framework for pattern generation. Again, this Again, this Karina also mentioned these theoretical results allow us to engineer very simple circuits performing the desired neural computations. Doesn't have to be neural computations. Whatever you can think of, I think you can use these theoretical tools, right? Well, whatever you can think of. Or maybe. We have very few control parameters, which is something I emphasized during the whole talk. And the dynamics seem to be robust. Seem to be robust for from what our experiments have shown.