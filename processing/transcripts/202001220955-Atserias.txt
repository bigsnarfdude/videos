The result should be given by Moritz, but we couldn't make it uh on the worksheet, so so I'll give it myself. All right, so we want to talk about this result about automating resolution. And let's start with a little bit of a history of the solability problem. So it all starts with the Davis Platon procedure back in the 60s, and then Robinson took the procedure and interpreted it as a proof system, and that's the definition of a solution the way we know it today. Way we know it today. Robinson was doing it for first on the logic, but the core of the proof system is really a propositional proof system. So, by the way, these are journal of the ACM papers. I had thought that this would have come from logic, but they really come from computer science. The first one is communications of the ACM, the nineteen sixty one. Is it? Yeah. Okay. Okay, so a few years later Cook uh proved uh that sat design to be complete. Proved that SAT design be complete. And in his paper, he was actually, as we all know, interested in the complexity of theorem-proven procedures. That's the title of the paper. And one thing he was asking is what are the hard examples for the Davis-Padma procedure? And he didn't know at the moment. And 15 or 16 years later, Hagen solved this question by showing that the Pigeon-Hole principle is exponentially hard for resolution and therefore also for the Davis-Padman procedure, which is a special case. Okay, so we fast forward. Okay, so we fast forward several years, and then in 2001 something a little bit weird happened. On the one hand, there were people building SAT solvers, and this was the very first SAD solver of each generation that started solving very big problems coming from industry and benchmarks that were actually huge with thousands of variables. And it was giving the first evidence that the proof search problem, in some sense, was easy. Sense was easy because it was producing resolution proofs of formulas that were quite big, unsatisfiable ones. But at the same time, this was the on the practical side, on the theoretical side, there was this big result by Alekovich and Rasvorov, where they gave the first evidence that the proof search problem should be hard. So there is this uh tension here and the tension got worse when by twenty sixteen, I mean a lot of things happened here in the side solving. Of things happened here in the SAT-solving area, and this one I like particularly because it's giving a proof, a resolution proof, which is absolutely huge: 200 terabytes of resolution proof for a problem in combinatorics, the Boolean-Pitagon triple problem. And so, this solves a big problem in Ramsey theory, and it was solved by running a SAT solver. By the way, you may think, well, this is because the proof is just the proof is big, so it's not that it's searching for It's not that it's searching for proofs and finding a good one. That's not the case because if you think of this, this problem has 7,800 variables. So the brute force resolution proof of that, or the decision tree of that, would be something like 7,800. And 2 to the 7,800 is a lot bigger than this. So it really is searching for a proof in a tiny, in a huge search space. Okay, so what else can we say then? Okay, so what else can we say then? There is this thing, and automating resolution, on one hand, seems to be easy, but it also seems to be hard. What else can we say? So, here I can skip, I think I can skip this. What are clauses? What are CNFs? But I want to say that n is always going to be the number of variables. Now the resolution rule, you have two clauses, you resolve and you get rid of one literal. Left premise, right premise, and resolve it. These are my words. And now we look at resolution proofs. And now we look at resolution proofs. Uh uh we want to see that it's as a graph. So the way I represent the proof is as a sequence of uh clauses and every clause follows from resolution by a left premise or a right premise and the right premise or follows from it's a hypothesis basically. And the last one you want it to be because you want the refutation you want it to be the empty clause. And the measure that we're going to look at is the size or sorry the length which is the number of clauses that you have in your Which is the number of clauses that you have in your proof. And let's define that refs f, so the resolution complexity of the formula is the infimum of all the length refutations. Now this could not exist because, for example, if f is satisfiable, then nothing, this set is empty, there are no refutations, and the infimum is going to be infinity. So that's how we would take it. It is always the case that the resolution complexity of a formula is never more than exponential in the number of variables. Variables. Okay, we all know this. And now the proof search problem is just asking for this. You're given an answer, this variable formula, and that's given as a promise. It's a promise that it's an answer this variable. Give me a resolution refutation of f. Now, of course, by Hawking's theorem, the complexity of this has to be exponentially the size of f, but that's just by a trivial reason. And the question you can ask now is what happens if you promise not only that it's unsatisfiable, but you also promise that it has a small resolution of it. Can you find it? Refutation. Can you find it? So, the problem of automating resolution would be the following. So, one way to put it is: could we find short proofs under the promise that they exist? Another way to formulate this question is actually equivalent, is by saying not as a promise problem, but as a problem where you're given the right to run in time that depends on the actual output, the actual optimal resolution. So, could the problem So, could the problem be solved in reasonable time as a function of the number of variables, number of processes, and the size of the smallest resolution reputation? So, if this were the case, we would say that resolution is automatable, and we can say that it's reasonable, can be polynomial time, quasi-polynomial time, anything less than the obvious 2 to the n. 2 to the, I mean, yeah, anything better than just the obvious. And this is definition due to Bonnet, Pitassi, and Raspberry. All right. Alright, so the theorem that I want to discuss today is this, that in fact resolution is not automatic at all, unless p equals. I mean it would be automatable, it would imply p equals n p. All right, so in fact we can even say more. You can see that it cannot be automated in sub-exponential time unless the exponential time hypothesis fails. And in fact we say more. We can say that we cannot even approximate the smallest proofs and proof. Approximate the smallest proof length. So the way we prove the non-automatizability is by constructing a polynomial time map. So take a CNF and we produce another CNF in such a way that we create a huge gap in the resolution complexity. So if F is satisfiable, then the resolution complexity of the formula we build is almost linear in itself. While if it's unsatisfiable, it's exponential, some exponential. Weakly exponential, we can call this. Or weakly exponential, we can call this. So from now on I don't want to m carry these numbers around, so I'm gonna say that this is small and this is big. And the point is that this is a huge gap between them. So this is what we're going to construct. And as a corollary, it already follows not only that it's not automatable, well why is it that? Why does it follow that it's not automatable unless p equals n p? Well because if you could find resolution proofs in time polynomial in the smallest proof, then you would run it in Then you would run it in time polynomial in this, and if by that time we didn't find a resolution refutation, then we know that we are in the second case. Okay? So this is the case. Probably G is always in satisfiable. No, it doesn't have to be. So in this case, if it's satisfiable, then this is infinity, which is okay. In this case, it's always going to be unsatisfiable. But when F is satisfiable, G is unsatisfiable for sure, and it has a small performance. For sure, and it has a small reputation. Okay, so it also says that then the resolution complexity of a formula as an input cannot be approximated at all, right? I mean, this is the corollary we have. The minimum resolution proof length is not approximable in sub-exponential error, within sub-exponential error, in polynomial time on S P. And again, that's because if we could approximate it, then we would be able to tell this apart and therefore satisfiable from unsatisfiable. Great, so let me tell you a little bit the history of this to appreciate a little what it takes to show. We already discussed the electron-rescharts product, but I'm gonna state the theorem more precisely. So the history of this goes back several years, and there are some positive and some negative results. So it's not going to be a complete survey, but let's start with With positive results. So, and for that, we need to introduce other kinds of proof systems. So, we start with negative. I don't know. We'll see. So, let's look at. So, this is the cut rule. The cut rule is the resolution rule, right? You have C or X, D or not X, but instead of X, now we take an arbitrary formula. And this is one way to phrase it. And this is one way to phrase the cat rule, so this is always sound. And now instead of as a set, instead of literals, we allow arbitrary formulas, arbitrary circuits, or formulas from some complexity class. And this is the main rule. And then once you allow arbitrary formulas, you need to allow other rules that are just for bookkeeping the connectives, right? But this is the real one, the one where the inference happens. And now we have this family of this hierarchy of proof systems where Your proof systems where the solution would be when you allow your formulas that you allow are only closes. And then you can, for example, allow instead of closes, you allow K DNFs, where K bigger than 1, 1 DNFs being closes themselves. You could allow, this we call res K or K DNF Frege. Now if you allow formulas of bounded depth, that's A C0 Frege, T C 0 Frege would be threshold formulas and so on. Frege are formulas and extended Fregi R formulas and extended Frege R circuits. Now these are stronger and stronger as you go up, and of course you can also do weaker and weaker, but not by restricting closes because what can be easier than closes? It's not simpler. And keep it to closes, but then you restrict the proof graph. And regular resolution would be read one's proofs. And three direct resolution would be three direct proofs. Alright, so we have this hierarchy which we are quite familiar with. And the first positive result was this. This pretty nice result of DMM-Pitasi, where they showed that trigger resolution, in fact, it's a little bit automatable, it's automatable in quasi-polynomial time. That's time n number of variables raised to log of the optimal proof size in three-light resolution. So the way this is done is by a little bit like Mark was discussing, that you can see three-like proofs when reversing them upside down, you see them as decision-based. See them as decision trees. And once you have decision trees, you can do divide and conquer methods, where the sort of things where you want to say a decision tree must have one size, one of the two sides must be half the size of the full thing. And that's the sort of thing that we use to produce this algorithm. I mean, for us, it's important because it says that when we want to show this part of the proof, of the theorem, then we know that we cannot hope to have We cannot hope to have actually a resolution proof that it's tree-like. Because then we would applying this algorithm would falsify the image. Okay, so that's good information. And then the next question is, okay, can you go beyond tree? Like resolution is at the bottom of that hierarchy. Can you do resolution? Can you do, what can you do? So for resolution, in fact, there is a non-trivial automatizing algorithm, and that's the Pencilson-Victorson, which Paul mentioned that it also follows from Paul mentioned that it also follows from their results. And so it's probably the same algorithm, right? It's a different algorithm because we had to do it recursively. We hoped we didn't see what they saw, that there was this simpler algorithm, which is nicer. This is a nicer algorithm, but the bounds are the same. Okay. Okay, so by the size width trade-off, you can get this crime time. And notice here that if s is polynomial, so you're in the Here, that if s is polynomial, so you're in the case where you promise that you have a polynomial size proof, then this is log of a polynomial, so that's log n, and therefore you get this running time, which is non-trivial. So, there's a non-trivial automating algorithm. That's pretty good information. It puts some limits on the efficiency of a reduction. Again, unless ETH fails. So, that's in some sense that makes the problem hard because there are some positive results on triple. Now, on the negative side, what can we say? Is there any proof system for which we can show that it's not automatable at all? And the very earliest result on this is due to Kraijk and Putlak, where they showed, they were talking about feasible interpolation, but this is a corollary of their result. But they showed that extended free gate is not automatable in polynomial time unless the crypto system RSA is broken by polynomial size circuits. So that's a So that's a believable assumption that RSA is not broken. But it's a crypt assumption. They did it for extended fregate improvement. Extended freight. It was S12, so it was not a technical catchable. So they need for extended freaking. The assumption is that crypto is far from optimal than that. Crypto. It's far from optimal, and I'm not mentioning what I mean by that. It was later done for Frege and for TC0 by TC0 Frege, by Bonnet, Pitasi, and Raz, I believe, and others for AC0 Frege. But still, they are basically evolutions of the assumptions are still crypto and they are far from optimal. Finally, we have what I mentioned already. What I mentioned already, the electro-bit transport result, which finally touches on resolution. It took several years to find this, and it gives the first evidence that resolution is not automatable. The assumption here is that WP is not tractable. So if it were automatable, then WP would be tractable. Now, one thing to notice is that from this theorem, it says nothing about whether it's automatable in quasi-polynomial type, for example. So if you look at the blue, you So, if you look at the clue, in fact, if you analyze the construction, this was done only very recently, because this theorem is quite hard to understand. So, they finally understood the construction well enough to see that the best lower bound they give, this construction they give, is something like n to the log log n. So so there's uh and finally the other thing to notice is that this lower bound, this this negative result applies to even to three-legged resolutions, but in in Sachsa's paper itself. In Sasha's paper itself. And so, what this means is that, well, there's a reason why we didn't get this beyond n to the log n, because we know that it's automatic in quasi-polynomial time, three-like resolution. So, this is what was known before our result. And now, about the optimality of the assumption I want to mention. So, what do I mean by optimality? So, our result says that it has So our result says that resolution is not automatable unless P equals NP. Now if notice that if P equals NP, then resolution of course is automatable, because you can search for the proof bit by bit. So that means that our result is N if and only if. Resolution is automatable, if and only if P equals N P. While this is certainly not optimal, you cannot reverse this. And these assumptions also you cannot reverse. These are not optimal. If RSA is blocked, I don't know what that is going to be. Is blocked and I don't know what that is going to say about the extended frame. All right, so that's it. So let's discuss now the new construction. Is it? It's correct. So this is what we want to do. We want to take a formula F, produce another formula G with this gap. So the beginning of this, in fact, goes back a long time ago. Time ago when I was studying with Bonnet, we were studying weak automatizability. And there, what is relevant for weak automatizability is the canonical pair for the proof system that Pavel discussed yesterday. So the reflection principle is a formulation of the canonical, the disjointness of the canonical pair. So remember the canonical pair were a set of formulas that are satisfiable on one side, and on the other side, the set of formulas. And on the other side, the set of formulas that have a short refutation. So if you formulate this as a CNF, which you can because it's an NP equation, whether X is satisfiable, and you formulate this as an NP equation as well, this is going to be contradictory because it cannot be that the formula is satisfiable and refutable at the same time. So remember X encodes C and F, Y encodes satisfying assignment, Z encodes resolution refutation of each of them. Now the deflection principle The reflection principles, this is a classical topic in metamathematics. Steve Koop was the first to look in his original paper on PV. He was looking at the reflection principle for extended resolution, in fact. And Rasvorov was studying the canonical pair for several proof systems, in particular resolution. And Putlak, I think, was the first to formulate this as a CNF and study its proof complexity. Now, for resolution itself, Pavel showed a couple of things, and we noticed with Bonnet, oh, maybe I should say before I say what do we notice, let me say a little bit more about this because I need to discuss details. So, remember, this is how we see refutations, and Z is supposed to encode such a thing. So, how is this encoded? Well, X is a CNF, so the way we say it is that the ith clause contains variable XQ with Contains variables xq with sine p, p0, 1, positive or negative. One variable for every clause, every variable, and every sign. The truth assignment is for the variables, one for every variable, xq, which says whether it's true or not in the truth assignment. So that's for the sad part. For the ref part, we want to encode this structure of application. And so here is one way to say, you say, I have ijk. We say we have i, j, k, q, that means that clause di is inferred from clauses dj and dk by resolving on variable xq. And these are supposed to be indices that are smaller. This is going to be imposed actually by the closes of the form. And finally, we have a clause that says, we need to say what are the literature of each of the clauses. We say clause di contains variable xq with sine b, which is the same kind of thing that we have for x. So with this, So, with this, we have all the information we need to describe refutation. Let me mention just now, for example, how would we say if I closed that this is empty? Well, you just go here and you say that for s, i equals s, that's the last one, this is always false. So not that. Yes. Sorry, I how do you encode that uh uh close D is an axiom? Just uh a couple of minutes? Ah yeah, I forgot it here. So I have to uh About it here. So I have to need another index, another format that says it's a hypothesis, and where does it come from? I'm missing some variables already. Thanks. Alright, so this is how you encode these things, and we put clauses that express this. Now, what we notice is that, building on Pavel's observations, we notice that in fact the reflection principle for resolution Refraction principle for resolution, well, on one hand, we showed that it doesn't have polynomial size resolution refutations itself. But if you go a little bit higher up in 2DNF pre-g or REST2, then you can actually refute it to polynomial size. And this is absolutely essential for the construction. So I'm going to describe a little bit the proof because once you see this proof, you realize that this is the right formula. So how does this go? So we want to refute. So w how does this go? So we want to refute this. What's the human proof? Well the human proof is that you have a satisfying assignment and the rules are sound, so you should be able to find a literal that is satisfied in every of the lines of Z one by one by induction until the last one also has a literal, but we just said that this thing here, we said that the last one is empty, so how is it going to have a literal? So that's the human proof. Let's do that for Volaska. So Let's do that formula. So we start with the clauses. And now we've built here two DNF formulas that say: so this first one says that the first clause in this list is satisfied by the assignment y. So how is that saying? How is this 2DNF saying that? Well, you say there exists a variable that is set to 2 and appears in the first clause with sign 1. Or, Or there exists a variable that is set to false and appears in the first clause at sine 0. So this is saying that the first clause is satisfied by y. Now you can say this for all the things, and the only thing you need to show is that once you have proved these two DNFs, you have produced them in race 2, you have proved them up to i, you can produce the 1 for i plus 1. And that is based on soundness. And that is based on soundness of the rule and just syntactic manipulation. So in fact, you can do it, and you can do it one by one until the last one. So here's the last one. This is saying that the S clause is satisfied. It contains a literal. But we just said that these are forced to zero by the process of ref. So in fact, this is the empty clause, equivalent to the empty clause. So we got the contradiction that we were looking for. We were looking for. That's all. That's what the thing is. But there is more, something important here, which is the structure of an addition. So these are not only arbitrary 2DNFs. They are 2DNFs where every term has one variable coming from Y and one variable coming from Z. They are completely separated and they are always like that. The ones you produce in the middle, they're always like that as well. So they have this nice structure and this has a consequence, which is And this has a consequence, which is that we already have one half of the construction that we are looking for. So remember, what we're looking for is to get a formula G out of F that has this one condition when F is satisfied. Yes? So let's check that this is actually this ref part for gets that. This ref part with X increased by F has the property we want. So why is that? Let's assume F is satisfiable and then say why is the satisfying assignment. And then say y is the surface line assignment. Now you take the reflection principle, you plug f instead of x, you plug y instead of y. So this becomes now true because it's a design sign. Now this is the formula G that we want. And now what happens with the four with the two DNFs that we had in our clients? Well, this is now an assignment. It's it's black, it's not blue. Yeah? It's an assignment, so this is really a close at this moment. So, this is really a clause at this moment. So, we turned a 2DNF refutation, S2 refutation, into a resolution refutation by plugging the satisfying assignment. Trivial. Well, I mean, extremely easy. And this goes back to 2002. And the reason we thought that this was the correct thing is the following. So, what is happening here? The satisfying assignment, we know finding satisfying assignments is hard. We know finding satisfying assignments is hard. That's a sad problem. And we want to show that finding resolution refutations is hard. The satisfying assignment is giving you the refutation. It is producing it. So you can think of this as a pattern, and by plugging inside the satisfying assignment, you're narrowing completely and you're pinning down where it is the resolution refutation. So, satisfying assignments give you small refutations. And finding remote refutations means finding satisfying assignments. I mean, we haven't proved that yet, but that was intuition. So, we had the right, we were on the right track. So, what is left to do then? By the way, we didn't realize of this in 2002. We realized of this two years ago with more int. So, what we said, oh, we only need to do this. Okay, we just need to show it. So, two years ago, we had that two years ago. So, two years ago we had that, two years and a half, and we tried for a long time. So, here is what we were trying. I want to discuss how we got into it rather than the technical details of the proof. So, towards the lower bound, so we want to show that this is hard when f is unsatisfied. It's hard to refute. So, there's one case that it's going to be super hard to refute. That it's going to be super hard to refute, which is when there actually is a satisfying assignment for this, right? And you cannot refute this because it's satisfying. But in general, there could be small refutations, and you want to refute this fact. You want to show that it's hard to refute this fact. Now, one thing we know is that this is always going to be satisfiable. If you look at the same formula where the proof length is 2 to the n, then this is not hard to refit. It's impossible to refit because it's satisfiable. It's impossible to refit because it's satisfied. That's exactly what I said. So, what you want to show is that these two things, in some sense, they are indistinguishable. There is a way where you could do this by saying, could we, for example, pretend that this assignment is given and you kind of hide the length of the meditation to a projection somewhere. Because now you already see how is there going to be a rejection, I mean, from a huge set to a small set. Huge set for a small set. Well, the thing is that you want to only pretend that such a projection exists. And this is something that was done by Sasha when he was showing that resolution cannot prove that NB has polynomial size circuits. And it's also an observation. What did I say? I mean, resolution cannot prove that SAT has polynomial size circuits. Yes, that's it, right? And Karinček was also doing this for Ramsey principles. So the question So, the question we want to ask is whether we can transport this bijection. And the technique that was behind this thing is to actually show that if the very weak piece of principle from these many pigeons to these many holes was hard for a certain system, then you could actually do this transportation. You can transport and you can preserve specific local structures. So, this is what we tried for a long time. Presumably, to bijes. Presumably, to bijective, very many pieces of hole principle, these many pictures, these many holes, presumably this is hard for 2D and F3. If that were the case, we were convinced we would be able to get what we wanted. So we tried for a long time to do this. Alas, this is not known. It's known for resolution. This is half for resolution. This is, of course, class and transport of result. But for the bijective in transport. But for 2DNF, we did a map. But for 2DNF we didn't manage. We tried for two years. We were almost ready to write a conditional paper where that would be the paper and set it to you guys so that somebody can help and prove it. Luckily we didn't need to do it, we did something else. So we said, okay, let's, if we want a lower bound, so let's at least prove a width lower bound. I mean, we should be able to do that. So this is the argument for the width lower bound. We want to show that. Lower bound. We want to show that this requires large width to reflect in resolution. And what we're going to say is we're going to show that these two things are actually indistinguishable in some sets. So we're going to say this is the truth assignment, the variables of this formula. And we're going to play a game, usual game where Buluber asks variables and we assign values to these variables. How do we assign values to these variables? Values to these variables, we're going to do it by partial maps into this huge exponential resolution refutation, which we know exists. We know it exists because that is unsatisfied. So we play the game here, and we say, okay, that variable saying whether a literal belongs to a clause or not, is it true or false? What we're going to do is we're going to say, here, is it, what do you say, here, true or false? It says true or false. That's what we answer. Now, of course, we need to make sure that this can be done without the proven matter. That this can be done without the prover noticing, and that you can do because we have in this local width on the only windows because we're doing a game for width. The windows are small, and you can do this carefully if you choose the parameters correctly. Okay, so we could do it. We got this. If f is unsatisfiable, then we have a listing of this required with linear inf. That's the case for MQ. And it's even more, not only width, we could show that index width. So, what is the index width? Well, every clause, every variable has an index, natural index, which is the position into the proof. So, we could show that it must have index width at least n. Again, this is not enough to apply the size width trade-off. And the reason is that this n compared to the number of variables is Number of variables is not limited. Okay, so we are not done, so what could we do? So we knew that one thing we could do is apply lifting, and Mark told us nicely about lifting. But lifting, okay, lifting for sure is going to give, from the width lower bound, it's going to give you the size lower bound, but it's going to have the bad effect of breaking the other part of the piece. So that's another work. And what works, in fact, is Work and what works, in fact, is this method of relativization, which is also available since more than 19 years ago. And this was introduced by Grejek and analyzed later by Dancieff and Ries. So what is relativization? This comes from logic. So the way you see, you think of this as actually a propositional translation of a first-order form. And this is essential for understanding what the classification does. Does. So the new formula has an R in front, and what it says, basically what it's introducing is some new variables that say that a clause is active or not active. So in the non-active clauses, we're going to not impose anything. So the other literals, the other variables that we already had, they now have a protection that says, clauses are going to protect this in a sense as saying, if I is active, then it is inferred. Then it is inferred from two previous clues. If it is active, then it contains this variable and so on. If it is not active, what do we know? Nothing. And so dj and dk don't have to be active. In fact, they have to be if that's enforced. That's going to be in the so let me give you a few representative clauses of this relativization. This is the natural thing you want to do. I mean this this from logic is quite clear because This, from logic, is quite clear because you have a structure and you want to say that the formula holds on a subset of the structure. You have a universe, the formula holds on a subset of the universe. So that you can do by axiomatizing the translation that it gives exactly this. So these are an example. So let's look at the first one. The first one says that activity propagates upwards. So think of this as an implication. If I is active, and I is supposed to activate. And I is supposed to be derived from J and K, then J must be active as well. So we impose this closure on activity. But that's something you do in first-order logic. You say a subset has to be closed under the functions in order to be a sub-model. Okay, the other thing is that the proof shape is required on active clauses, but only on active clauses. So treat this again as an implication: if i is active and i is derived from j and k by result. By resolution on Q, then the variable Q should appear negatively in the left premise. That's one of the conditions of structure. But notice, so proof shape is required. Proof shape means that the retinals come from the precision they have to, but only on active clauses. And finally, we need to say at least something is active, and it would be nice if the anti-clause is active. All right, so this is relativization, this is the new formula. This is the new formula. And now everything boils down to a random restriction. So, and this will completely close the proof. So, we apply a random restriction in the following natural way, and that's Tachen and Ries. This is how they invented this. So, we want to show, assuming F is unsatisfiable, we assume we have a small length refutation resolution. We apply a random restriction that is defined this way, and what's going to happen is that the restriction is going to map the relativized version of the. The restriction is going to map the relativized version into the non-relativized version. For this, we have an index with lower bound. And the restriction guarantees this. Guarantees that with exponentially high probability, with very high probability, the integraph is smaller than n. So what's this restriction? Well, for one thing, we need to say that the last thing has to be active. But for the rest of closing, But for the rest of closures, I differing from S, we're going to set them active or inactive with probability one-half. And the last one is going to impose the closure condition that we need. So if I is active, but not both J and K are active, then we set this to zero. Saying just make sure that you're not using something that is inactive. For the rest of things, whenever something turned out to be unactive by this, To be unactive by this, we just set it completely around. So, what's happening then is that the clauses, because they come protected from the activity, things that are active become closes from the original ref. If it's active, it means that this is false. If it's false, it's as if it was not there. And this is one of the, sorry, especially this one, is one of the clauses of ref. Of ref. These ones we set them to two by the assignment. This part here sets them to two. And whenever something is unactive, what happens if it's unactive? If it's unactive and it's a zero, so this is a one, so the whole thing goes away. So unactive clauses disappear. Active clauses preserve the same structure. And then what happens with the way the index width disappears? Well, because if you have a clause of large index width, Of large industries, then if it has large industries, it's quite likely that one of the lithologs is going to be unactive and therefore set completely at random. This is the same argument that, very similar argument that Mark was showing in the case of the exodification. And everything goes smoothly and you have the lower bound after that. Alright, so now we need to say, but does this break the upper bound? Careful, right? Does this break the upper bound? Careful, right? So let's go back to the upper bound. Does it break? Well, it doesn't break because it has a meaning. So here is what it is. So now we have, we want to refute this. Now we're assuming that f is satisfiable. It should be f. Oh, no, it's just a reflection principle, just a full thing, generically. You don't assume anything. You just want to refute this with an R. The only thing you need to say is that you want to derive the things, but only for active closure. To derive the things, but only for active process. So if it's unactive, then the process are going to allow you to derive the next one. And if it is active, then the soundness is going to allow you to derive the next one. So it perfectly works. It's still two DNFs, and it still has the shape that we want. So we get the contradiction that we want, and we get the structure that we want. So when F is satisfied, So when F is satisfiable, the relativized reflection principle with n cube has a small refutation. When F is unsatisfiable, it doesn't have a small refutation. It would even be infinite. All right, so out of time, I have lots of questions. If you want. So to conclude, So we have this picture, right? We have this tension here, and it looked like it was easy and at the same time hard. But the hardest evidence was not super strong. Now we have the strongest possible evidence that is hard. But is it, I mean, maybe after all what's happening is that P equals MP, right? So we should really realize what's happening with this side. Realize what I mean what's happening with these such overs. They are absolutely amazing. And again, I mean, this this idea didn't notice this thing I said. I think it's quite important. His proof, after all, is tiny compared to the 2 to the 7,000, right? So it's searching the absolutely huge space of resolution refutations. It's searching it and finding it. And it's finding it in a very big one. So it's really, really amazing. Another thing to notice is that this is a combinatorial problem. It's not an industrial problem. Problem. It's not an industrial problem, a problem coming from industry. It's a rampy problem. These are supposed to be the hardest problems. So, in case you don't know what this is, it's the following question. So, you have a set of natural numbers. So, what's a Pythagorean triple? Yes? Pythagorian triple is three numbers, ABC, so that A squared plus B squared equals C square. Pythagorean triple. And now let's say that Pythagorean, so now you have a Pitagorian, so you now you have a set of numbers, you color them 0, 1, black and white. You say that the triple is monochromatic if all three are white or all three are black. So the question is, what is the largest number so that if you largest or smallest? What is the largest number so that every coloring of the initial segment contains always a contains always a Pythagoreo uh a monochromatic Pythagorean type. So the largest number happens to be seven thousand eight hundred something. All right, so I'll finish here. Thank you very much. So there's a quantitative thing. So the so how do the quantitative bounds for your gap compare to the Gap compared to the benefits on the upper bound. Yeah, so I really was bugging more. What is the main result? Here. So the Benson-Wickerson one looks very similar to this, the square root in the exponent, but it's on terms of the number of variables times log, the size of a refutation. So the size of the refutation The size of the refutation, if it were linear, well, I mean, here instead of the number of variables, we have the size of the formula, and that's bad, because the size of the formula is much bigger. Ah, I see. So it's not giving the optimality of benches of the question. But it's some specific exponential, it's not like square root, it's like into the 16th or something like that, or I don't know what, you know, it's something like that. Yeah, yeah, you you can get a concrete constant in this. And but it's not their constant. It's not one half. You can get a concrete constant one over sixteen or something. I really, I mean, I was very confused, but I wanted to show that Benson-Winterson is optimal, but this doesn't give it. And I think I asked five times the Moritz, can we do this? And he kept saying, No, this doesn't work, because it's it's the same as the four workers. Forward. Any other questions? We didn't look at it. I don't know the answer. So the upper one is this thing that we discussed. And I don't know. I mean, this proof is, it works out the details. Work out the details is quite messy. In fact, Arnold was giving a proof in boundo-diagnetic in DAXTool a couple of years ago, which may be easier to analyze to understand if it's regular or not, but I don't see. I don't know. Maybe the question is simply whether uh a regular reservoir should cover how to escape this day. So what a bit of a motion or not. Old-fashioned German style. Old fashioned German style automotive. Yeah, I have no question about it. I think I missed the memo. When did we switch from automatization? So I think it's very clear that the authors who used automatizable instead of automatable would have in retrospect preferred to have used automatable. The term has survived, I used it in line, but but I use it in mind, but I've talked to automatable would have been a better term to begin with. It's an accident. I mean, there's enough people here that we can decide to switch for good. It should be a major outcome of this meeting. Okay. What's your experience? Is it still old on the oil and so plus it's like a new bearing? So plus it's quite when you bearing your result on this question. Whether resolution is a modern. So it's still open, yeah? It's still open. So it's like a very little problem. I'm not quite understanding. What is it the role of the REST two REST two in the proof? And why did you is it just because of uh a co coincidence that that of the REST uh Well no, press 2 happens to be the smallest depth one proof in the sense of Avon who was mentioning this thing. So he he mentioned this thing that the the interpolation pair for depth one is equivalent to the weak automatized We have to practice a lot of this. It's equivalent to the one-four resolution. And in fact, it's not only depth one, it's actually this one, S2, equivalent to REST2. REST2 appears and that it's the it's the smallest fragment above resolution, the smallest fragment of the text one above resolution. I didn't state it. I was intrigued by the fact that uh you cannot prove uh the reflection principle, but once you have a satisfying assignment, you can prove uh principle. Exactly. That's exactly what's happening. Okay, this doesn't work for uh three-light resolution for obvious reasons, but this is the way to rescale this kind of proof. Scale this kind of proof to work on the regime of three-like resolution and quasi-to get non-quasi-polynomial time, better than quasi polynomial time. I don't know. Just for scaling something and I mean this is very much non-tree-like, but uh there are tricks you can do. I mean you can add variables to indices and so on. I've heard some people One, I've heard some people do. So are there uh are there exponential three like low bounds for this? For the factions? For this four for this? Yeah. Right now, in fact, Michal is going to talk tomorrow. So for three life, it's always been known that it's hard. But Michal is going to show that this is exponentially hard for a solution. No when, but no, when exponential. It's a satisfiable formula. Oh, so the question is: in the case of F satisfiable, where we know we have a small resolution refutation, can we show that it must be hard three? Other cases where. Well, I guess it depends on the formula F. So on the formulas I have other examples of formulas, where I don't so how I don't know how to characterize the case when it's harder. I mean, I'm pretty sure some F's are going to be harder for the real solution, but Be hard for the it must follow from this, in fact. It must follow from this at least conditionally because otherwise you could just sort of use the three automatized forms. You could use the three Causic algorithm to distinguish the existence of formulas for other complex formulas. If these were three like so if there was a start here. If there was a star here, then you would run the automating algorithm like this, and you would distinguish. So, and I'm sure unconditionally should also be doable. I mean, this is a clear follow-up that if you can prove an index degree lower bound, we can probably recast everything. Yeah, so in fact, for polynomial calculus Thomas was working on this for Thomas was working on this for some time and we didn't succeed. But I heard that uh that there is something. I mean the next talk I think is going to talk about this. And for degree lower one for the sum of squares, that's totally open. So index degree lower one for sum of squares. That's wide open and I would love to prove it. So then we would show that the sum of squares is not automatable in SP COSI. This isn't in the middle. It's easier in the middle. What's the significance of n cubed? I mean, that's the smallest you can take the size. Yeah, n cube. Michael has looked at the right numbers. For what? For the length. Ah, the upper bound for the reputation of. No, what length do we need here? The polylength Z. In our proof, n cube works, but would it work for smaller, like n square? Like n square? The parameter, the length parameter. It's a small polynomial in the size of the formula. It depends on how technically you want to present your proof, but basically it has to be at least something small polymer, but it can be any size. You'd want it. But could you make it smaller than n? I mean, for the optimality there. N cubed. Smaller than n cubed. Well, there are parameters. N is the number of variables, then you have number of clauses, which also enter. Of closes, which also enters into this. So, in these two parameters, well, that rule or my proof give you some polynomial, but how to optimize that didn't try. Maybe one should remark that uh uh one part of this proof is actually proving that uh it's hard to prove marupa, which is not exactly what you did because of verticalization. Okay, so uh I think we sh we can uh finish the discussion and be it to the break and uh we'll uh return again at uh 11:10. 