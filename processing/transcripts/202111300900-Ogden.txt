Thank you very much, and thank you very much to the organisers for organising this meeting, being planned, and it seems like a really interesting meeting. I'm very happy to be taking part in it, even if I know that it's a bit of a shame that it has to be remotely. So, I wanted to talk a little bit about some recent kind of current work that I've been doing to do with mixture models. With mixture models. And this is an area that's quite new to me. I've kind of become interested in mixture models just as a kind of flexible model that can be used to model a distribution in a flexible way. And I'm really interested in them in general as kind of using them as a kind of component in a larger model. So, for example, I'm very interested, and a place that I've been interested in there is. interested in there is replacing that assumption that we might have a normal random effects distribution with a more flexible distribution, such as a mixture model. So that was where my interest in these models came from. It's been quite recent area of interest, as I say. And along the way, I've kind of come across a few problems, which is what I want to talk about today. So in particular, these mixture models often have some kind of tuning parameters. Of tuning parameters, or from a Bayesian context, we could think of them as hyperparameters which we need to select. And it'd be useful to be able to do that in some objective way, such as through plus validation. And that's what I want to talk about now. So, first of all, let's just set up some notation, what are we talking about? So, for a finite mixture model, I'm sure most people are aware of what I'm talking about here because there's so many kind of experts in the room with this, but I just want to. Kind of experts in the room with this, but I just wanted to make sure we all got the notation set. So we're going to suppose that we've got k components, and essentially to generate data from this model, we'd say that with probability pi j, we're going to sample from a component density gj. So the overall, the k component finite mixture model would have density, which looks like this. And often all the components would have the same parametric form. So there are only different. So they're only differing in the values of these component parameters, theta j. So, some examples are just a normal location mixture where we've got a common variance parameter for each of the components, but a different mean, a normal location scale mixture where both the mean and the variance are allowed to vary, or we could have other things like Poisson mixtures too. So, we've just got some component parameters which are varying. And in this talk, I'm going to focus on the normal location scale mixture, but Scale mixture, but in other settings, we could be thinking about different types of models, too, and that's not particularly important for what I'll talk about today. Okay, so in order to do Bayesian inference, we first of all need to specify some priors on the parameters. And typically, those priors might be a Dirichlet prior for the component probabilities. So I'll just say a Dirichlet alpha, some common prior to each of the component parameters. Some common prior to each of the component parameters. For example, we might just use a conjugate prior if we've got a normal location scale model. That might just look something like this. So we've got a normal inverse gamma prior there. And here we've got quite a lot of parameters that we have to set. So we could set alpha or we could put yet another prior on that. So we could put a hyper prior on it. We might need to choose k, which we'll talk about in a minute. In a minute, and we also should choose all of these hyperparameters. And it's often not that obvious what good values for these hyperparameters and other parameters could be, because they're kind of quite well hidden inside the model. So for that reason, I thought it would be useful to have some kind of objective way for choosing these hyperparameters. So I should say this is kind of where. So I should say this is kind of where I'd like to get to doing full Bayesian inference. Where I'm at at the moment is doing something a little bit simpler. So I'm going to talk about simpler, essentially sort of maximum likelihood type inference first. And then at the end of the talk, I'm going to go back and talk about full Bayesian inference using the priors I just discussed. And that will explain why I'm first. That will explain why I'm first of all just looking at this sort of slightly simpler maximum likelihood type approach, these two approaches, which I'll talk about in a minute. So, the first approach I looked at was really just this maximum likelihood approach. We can't use just maximum likelihood directly for this type of models. Essentially, if we try to do that, then we just end up fitting a model which has some component which just has. Some component which just has one of the components means equal to one of the data points, and one of the component variance is very, very small. And if we do that, we essentially get a likelihood which can approach infinity at that point. So if we do that, we get some very bad properties. So, for example, the maximum likelihood estimate coming out of this model is not a consistent. To this model, is not a consistent estimator, even if the model is completely correctly specified, because we just tend to focus too much too strongly around the data. So, we can have that really bad property if we just do maximum likely. There are ways to avoid it, but one way is to include a penalty term on each of the component standard deviations. So, we want to avoid these component standard deviations being too small to get something like this. Hi, Helen. And so, can you stop your video, please? Can you stop your video, please? We lost your audio sometimes. Thank you. Oh, okay. Sorry about that. I'll stop the video to see if that improves the. Is it any better now? Yes, I'll carry on and hope that that's improving things a little bit. Okay, yeah, interrupt me again if it doesn't improve. But okay, so I'll hope that. So I'll hope that you kind of were following where I was, at least from the slides. So what we're trying to do is to avoid having fits which are having a very small component, is a penalty on the standard deviation. So one penalty I've put here was suggested in this paper. Was suggested in this paper. And this paper was essentially focusing on consistency of finite mixture models. So, consistency of maximum penalized likelihood estimates in finite mixture models. And it's equivalent to having an inverse gamma prior on each of the component standard and component variances with the two parameters being both one over n. So in total, what we're doing by... In total, what we're doing by doing this maximum penalised likelihood approach is really equivalent to what we had on the previous slide. So I'll go back to it having these conjugate priors for the whole thing, but instead using certain fixed values for the hyperparameters. And we're just saying that there's not any penalty on the alpha term, there's not any penalty on the component probabilities, which is equivalent to setting alpha equal to. Equivalent to setting alpha equals k at my previous one. So, this maximum penalized likelihood approach here is really just a special case of the maximum aposteriar inference with the Bayesian case I said on the previous slide. Okay, so how do I do the maximum penalized likelihood model fitting? A very common approach that's used in these models is to use the EM algorithm to estimate the parameters, but it can actually also be done by direct optimization of the Be done by direct optimization of the penalized log likelihood. And I found after some experimentation with it that it seemed that the actually the direct optimization of the penalized log likelihood was a little bit faster. I'll talk about how it was, you know, important things to make it quite fast in a moment. So I used this direct optimization approach. It was necessary to reparameterize, to use the multinomial logic of the component probabilities. The component probabilities. That's just to avoid needing to put constraints into the optimization. So we know that each of the component parameters probabilities has to be between zero and one, and they all have to add up to be one. And we can avoid having to put those constraints into the optimization just by reparameterizing the mod. And it's important for efficient optimization to be able to find the parameter estimates efficiently to be able to make use of the first two derivatives of the Make use of the first two derivatives of the penalized log likelihood. So, those are things that we can actually calculate analytically, and it makes the optimization much more efficient to use those derivatives. So, the penalized log likelihood surface is not a particularly nice object. It has lots and lots of local maxima, particularly if there's a large number of components. And because of that, we don't just want to start the optimization algorithm one place, want to use a large number of random. Place wants to use a large number of random initial values for the optimization, and essentially, because of that, the process becomes computationally expensive and less stable as we increase the number of components. So we don't want to choose the number of components larger than we have to. So as an example of how this model fitting process worked, I just simulated not at all actually a finite mixture. Actually, a finite mixture. Just because my emphasis was really on trying to use these models as flexible models for a distribution, which may or may not actually come from a mixture distribution. So I define a mixture model for each of one up to eight components. So k from one up to eight. And then I And then I used a maximum approach to fit the model. And this is the inference that I got out. So it's kind of maybe what you would expect. You start off with one cluster and it just fits an oral distribution. You get two clusters. Clusters, you can get this and so on. Once it got above four clusters in this case, the inference just stabilized to essentially only really use four components in each of these cases for up to eight. Okay, and so in this case, it seems to make sense that there's not really much need to go above four clusters. We might as well stop when we get to that point. So just to see what happens, I decided to take that data and normalize it. Data and normalize it. So, I'm going to go to the next one. Okay, it seems that my internet connection is quite unstable. I don't know how well you're able to follow me, whether I just dropped off or okay. We can hear you again. You can hear me again. Sorry, it's so bad. I don't know what's why it's. Sorry, it's sober. I don't know what's uh why it's being so yeah. Do let me know if you can't hear me too well. Um so can you start again your slides? Oh, is it not showing? Uh well sorry, let me there we go. Can you see that again? Perfect, yes. Okay, great. Um, so yeah, I was just saying that I fit to a new So we need to set normalize the data and refit the model. And this time I get a slightly different behaviour. So this time, every time I increase the number of clusters, I see that I get a new rib text. A new real estate component. So, why is this happening? Used in the two cases is the same. I've just got this fixed penalty, but my data has changed a bit. So, now in my normalized example, I've spread everything out a little bit more. You probably can't see the scale too well on the bottom here. But because I've normalized this to have sample variance one, it means that the scale here is approximately Hi Helen, we lost you again. Maybe your internet connection is not the best for the moment. Can you hear us? Okay, you're back. Um okay. I'll carry on for now, but let's carry a little bit We heard you very very low all in the zoom meeting. Can you hear the same? The same I will stick to the now. That's okay. Okay. Um so so what I was just saying is that we get quite different inference. Get quite different inference than you take it. And effectively, that's because we're putting a very small penalty. We're putting a tall penalty in a small class, so we should be very low. There's still a problem with the sound. Is that what can you hear me okay? Or is it yes, yes? Yes, okay, it's a bit better. Sorry. So let me carry on. So yeah, so essentially, although we have two data sets which are essentially the same, just up to a scale, we've got quite different inference. And we could get the scale invariance back if we replace the component standard deviation just by the standard deviation divided. The standard deviation divided by the sample standard deviation in the penalty, but it just highlights that the penalty we're using was a little bit arbitrary. So, for example, we could just scale up the penalty we're using by some constant term and then it would be kind of equally valid in terms of the asymptotics that we were using before. Okay, make some kind of choice of the parameters in the model, the tuning parameters. The parameters in the model, the tuning parameters, things like this, this here, and we would like some nice way to do that. So, I'm going to focus now on choosing the number of components. So, in this case here, it's not quite obvious just from looking at the model fits, particularly if we didn't have the true density superimposed on here. It would be helpful for us to be able to have some way to determine which of these. To determine which of these fits that we should prefer. And then there's a secondary thing which I'm not going to talk about too much, but similar methods could then be used if we wanted to try and choose values for some tuning parameters, such as something like this A parameter that I've added in here. So we want to choose the number of components to get a good approximation to the unknown distribution. And here, what I'm not directly interested in is trying to kind of recover the true number of components that we have because we're Number of components that we have because we're just viewing this finite mixture model as a way of getting an approximation to a true density, not as a sort of true model where we really believe that there's some fixed number of clusters in them. So we've got a special case here of a more general problem. We've got a set of models which are indexed by k, in this case the number of clusters. Model k has fitted distribution f at k, and the true distribution. And the true distribution is F. And ideally, we want to choose the number of clusters to make the estimate as close as possible to the truth. So, of course, this is not going to be an easy thing to do because in reality, we don't know what the true density is. So, similar principles would apply for selecting other tuning parameters. And I'm just going to focus on choosing the number of clusters here to demonstrate some of the ideas. So, what would we do? If we had a large test set, we We had a large test set, we could choose k to minimize some test error. So, if we've got n points, n test points in the test data set, we could just add up minus log of the fitted densities evaluated at each of the test points. And I just average this over the number of test points so that we can see what happens as the number of test points tends to infinity. This quantity is just going to tend towards this integral here where we're integrating over the true density. And this form essentially. And this form essentially means that minimizing this limiting test error is equivalent to minimizing the callback libel divergence of the estimated density from the true density. So if we really knew the true density, then we could minimize this quantity, but of course, this is an unknown quantity. And I'll just say that where multiple values of k give very similar tests. Values of k give very similar test error. We should choose the smallest such k just from numerical computation in our uniform example. We see that it gets down to four and then it stays constant because the bits really stay constant. So there we prefer k is equal to four. Of course, we can't compute this test error in a practical setting because we don't know what the, in a real situation, we don't know what the true underlying distribution was. And in our normalized example, where we got more and more complex fits as k increased, we get down to some K increased, we get down to some minimum, and then we start to increase the test error again because here we're kind of really overfitting. Okay, so we don't know this test error, in reality, we don't know the true density, so we can't find the test error. So, in a regular case, we could use AIC to do this. And I started off when I was looking at this problem seeing is there some version of AIC that we could use for finite mixtures. Unfortunately, the asymptotic argument which justifies the use of AIC really doesn't apply in this setting. AIC really doesn't apply in this setting, and there's not a simple term that we can include which would adjust AIC in the correct way. So instead, an obvious thing that we could do is to look at a leave one out cross-validation estimate. So for each of our models K, we could fit the model leaving out the point YI. So Y minus I is the whole data with the ith point removed. We could get an estimate from that, and then we could find From that, and then we could find the same quantity that we used for the test error. So we calculate minus log of the fitted densities from the data with the ith data point removed, evaluated at the ith data point. And this hopefully should give us a sort of approximation to the test error that we were interested in. Okay, so what's the problem with this? Well, the obvious problem is computation. Well the obvious problem is computation expense. So if we want to calculate this thing we have to once at a time take each of the end data points out and refit the model therefore n times. And given that we had to restart the model with different random restarts each time, so we had to potentially use a large number of initial values in the optimization. This could be really quite computationally expensive and sort of infeasibly so. And one other thing to say, mention is perhaps we're most interested in this, in being able to do this for sort of moderately large values of n. If we had a really extremely large n, then we could genuinely take out a large test set and use that to approximately large values of n than for really extremely large values of n. But just doing this in a sort of naive way, even if we've got a moderately large value of n, will very quickly become infeasibly. Will very quickly become infeasibly expensive. So, how could we reduce that computational burden? Well, the main idea is that the fit with the data with just the data point removed should be pretty similar to the fit with all the data Y. And so a first idea is just to use the parameter estimate from the full data as the starting point for the optimization when fitting with the data with just the IS data point removed. Why is that good? Well, typically only... Why is that good? Well, typically, only a very small number of iterations of optimization are required to get convergence. So, this is one point where it's important to use the analytical derivatives that we have because it means that the optimization can very quickly hop to at least the local maximitosis to the point where we start. And the main idea used for approximating cross-valid deputation criteria in many contexts is often similar to this one, that if you just remove one data point, If you just remove one data point, that shouldn't change the fit too much. So I should explain what this plot is. So, first of all, I'll say that this works very well in quite a few cases, very well in all of the cases with the first edge, with the The with the north clusters, and the first place I found problems with it were for five clusters. So, this is what I'm going to talk about here. And along the x-axis here, I have which of the point the point which is left out, the point which is removed. And on the y-axis here, I have the contribution to the criteria. And I've got two cases that I'm showing in red and in black. So, in black, I'm showing the approximation that I just described. That I just described, just starting off using the full fit as a starting point for the optimization. And the red is using a large number of points again for the starting point for the optimization. So the red is a sort of much more computationally expensive approach, but should get to a more accurate approximation. And what we see is that for really the vast majority of points that we leave out, the approximation I described. Out, the approximation I described does very well. But in a small number of cases, that goes wrong in those small number of cases. So let's look at what happens at the point that's left out, 0.63 is left out. So this is this point here where we get quite, there's three points around here where we get quite a big change happening. So just to remind us, this is the fitted density with the full data, in this case, with five clusters. And what And what happens if we leave the point 0.63 out as initial values? Well, we get a fit which is really very similar, as we kind of expect, and we get this maximum p-line slot likelihood minus 259.99. What happens, though, if we use a large set of initial values and rerun the optimization? Well, we get something which looks quite a different fit with a penalized log likelihood of minus 2.5. Likelihood of minus 2.59.77. So, in other words, very, very slightly more than the previous case. So, what's happened here? Well, we know that the penalized log likelihood surface has a large number of, potentially large number of local maxima. For the full data, we find if we look at the sort of top two of those local maxima that we identify, one of them has this penalized log likely and one has this one. So, this distinct local maximum, which is essentially corresponding to this second bit that we're going to be. Corresponding to this second bit that we see here has a very slightly smaller penalized log likelihood under the original data. But after we leave out this 1.0.63, the ordering of these two swaps. So this local maximum here, this second one, becomes the new global maximum. So this kind of means that there's really some instability in the maximum ping lives of livelihood estimation. We can change the data. Livelihood estimation. We can change the data a little bit and we get quite a different inference coming out as a result, which I'll talk about a little bit more in a moment. So if we want to try and improve our leave one out approximation, so we can still do this if we want to get these, the contribution that we'd get from adding up all of these red dots, we can do that. And it doesn't actually cost a huge amount more than the original approach. Than the original approach. So, when we do the original model fitting, we do store, we do already do optimization from lots of local modes. So we find lots of local modes already, it's just that we weren't storing them. So if we choose to keep all of the local modes which have a sufficiently large penalized likelihood and which are sufficiently different from one another, we can then use that full set of local modes as starting points. Of local modes as starting points for the fit when we're leaving out one of the data points. So, if we use that full set of starting points, we then still typically only need a very small number of iterations of optimization to get convergence starting at each of those local modes. And typically, things still don't cost a huge amount. It's still feasible to really compute the leave-one out criteria. And having done that, if we try a similar thing again and If we try a similar thing again and try quite hard to really approximate the leave-one out criterion by really refitting, starting with a huge number of new random initializations, or if we take this approach, we get very much the same results each time. So in that sense, it seems like we've got quite a good approximation to that leave-on-out criteria. Okay, so what does it look like? Well, the dashed line here is just the test error I showed before, and here's the leave-on-out criteria. Before, and here's the leave-one-out criteria computed in that example. So, we can see in that case it replicates the shape really very well. Things work a little bit less well in the normalized uniform example. So, I've chosen this case because it does seem to be quite challenging, particularly when we get a large number of clusters. Things do become a bit unstable. So, this behaviour up here doesn't seem to be too satisfactory. Not necessarily that we've got a poor approximation to the leave one out criteria. Approximation to the leave one out criteria, but rather that this is not a particularly useful approximation than to the test error. So, a better approach to take when we've got this would probably be to look at the sort of first minimum that we get over here, rather than paying attention to what happens over when we've got a large number of components where things become unstable. So, this is still very much work in progress, and I want to do more work to check whether that's in a much larger scale. In a much larger scale simulation, it would give good results. So, what we've seen just now is really that maximum penalized likelihood fitting is quite unstable. So, a small change in the data can give a big change in the fitted response density, which is not a very desirable property. And I think that's what really leads to this sort of slightly problematic behavior when we get a large number of clusters. So, I think that the leave one-out cross-validation would be a much better. Cross-validation would be a much better surrogate for the overall test error if the fixing process we used was more stable. The first idea that comes up when trying to do that is to try and use a weighted combination of inference from local modes rather than just using the single global maximum. There are some potential problems in doing that though, particularly that we'd need to use, need to make sure that the local modes that we were looking at really were. Local modes that we were looking at really were distinct from one another, and that we weren't counting the same local mode twice. Because when we start off our optimization of lots of different initial values, many of them might converge to the same local mode. We need to make sure we remove those repeated local modes. So that's something that I'm looking into, whether it's possible to stabilize this maximum penalized likelihood difference. Okay, so I just wanted to go back to where I was at the start, talking about a fully Bayesian approach, which is really where I wanted to. Is really where I wanted to start off wanting to look at, and I ran into some difficulties which I want to talk about now to explain why I went back and looked at the maximum penalized likelihood approach or maximum a posteriori approach instead. So just to go back to say what our model, what sorry I've put slightly, this sigma j squared should be at the top here, but anyway, we've got a conjugate prior distribution for the component parameters. Distribution for the component parameters. And we can choose these, the values of the hyperparameters, just to match the previous penalized likelihood approach for now, or we could consider them being some other values too. But for now, just to make things as similar as possible to what we have before, let's fix those. And we could consider inference using various possible values of alpha. Okay, so what's Okay, so what's typically done rather than doing inference on k directly, it's quite common just to take the potential number of components to be quite large. So have some sort of upper bound on the number of components and then use the hyperparameter alpha essentially to control how many of the mixture components are actually typically occupied. An alternative is to take the limit as the number of components tends to infinity. So to take that sort of upper bound on the number of components that could be used to infinity, and that gives the Dirichlet process mixture one. And that gives a Dirichlet process mixture model. So I tried using this Dirichlet process mixture. This was some time ago when I was first looking at this. And there are many different MCMC algorithms which have been suggested. So I tried just implementing one of them, which seemed reasonable. I don't want to go into the details of the algorithms that they used at all. I just wanted to note the algorithm which I used here. And then I checked what I did by using an R package. Using an R package which has just been written, which enables us to fit these models. And the results that I got using the two approaches were quite similar. So I just wanted to really check that I hadn't done my code wasn't just completely wrong, something like that. This is why I did a check with an existing R package. And this is what I found when I fit the DP mixture for the uniform example setting alpha is equal to one. Setting alpha is equal to one. And I was very surprised when I saw this because it seemed like it was, well, first of all, not very, very close to the true density, and secondly, quite confident of the values and excluding the truth. So I was really unsure at this point what was various parameters. So I tried changing alpha, for example, to be 10 and really got back exactly the same inference. So, what happened? So, what happened? I'm really not sure. And since there's many, many experts on these kind of models here in the audience, I'd be interested for feedback on this. So, changing alpha does have an impact here. It has an impact on the number of components used in the posterior samples. So, the average number of occupied components goes from 1.4 for our phrase 1 up to... 4 for alpha is 1 up to 4.4 for alpha 10. But we can see that the response densities still seem to remain very similar in the two cases. And if we look at the average proportion of observations which are assigned to the largest component, then it's so we could also give alpha its own hyperprior. That also gives similar inference. And I tried also changing, obviously I had some very specific choice of the hyperparameters here. And I tried changing those and I still got. And I tried changing those, and I still got the very similar inference out, which is why I didn't choose to kind of continue on too much with this approach. My initial plan was to try and use semi-like leave-one-out criteria to try and decide what good values of some of these hyperparameters should be. But since the inference I was getting out was just essentially seemingly getting stuck at this inference, which is very close to just fitting a normal distribution for the data, I decided not to. I decided not to kind of persevere with that example, which is why I went back to the maximum penalised livelihood approach. So I'm not really sure what's going on in this case, and I'd be very interested still to try and do some work with my original plan. But it'd be interesting to know what people think could be going on here and whether I could be using some much better scheme for doing inference, a much better MCMC scheme, which perhaps. Much better MCMC scheme, which perhaps wouldn't come up with these problems. So, for my normalized uniform example, I did the same thing. I got different inference out, which was perhaps slightly better. It looks much more like a two-component model in this case, but still suffers a lot of the same problems as before. Okay, so let me just wrap up. So, first of all, it's often possible to get an accurate approximation to the leave-one out cross-validation criteria in fixed models. Validation criteria in fixed models if we're just doing maximum penalized likelihood fitting. So, this was a surprise to me really that it was actually feasible to find these things in a reasonable amount of time. I kind of assumed at first that this would just be an impossible problem because we'd have to refit the model so many times, but it turns out to not be as difficult as it seemed at first. So, this can be used to choose k, the number of mixture components, which is the focus that I had earlier, but it could also be used to choose the values for tuning parameters, which is in some way. Tuning parameters, which is in some ways a more interesting problem. More work is still needed to check how well that actually works. There's some numerical instabilities which remain, particularly for fits with large numbers of distinct components. And there's definitely potential to improve the stability of penalized likelihood inference by using the local modes of the penalized log likelihood. And I think that by improving the stability of the fits themselves, The fits themselves, we would really make the leave one-out cross-validation criteria a more useful surrogate for the test error. And I would really also like to consider full Bayesian inference, but so far I've struggled to get sensible inference and I'd welcome your feedback and ideas about the best way to go about this in a fully Bayesian way. So that's all I have to say. I hope that you managed to hear at least some of the talk, and sorry for my rubbish internet connection. Sorry for my rubbish internet connection, but I'll happily answer any questions. Hi, thanks for the talk. It's a bit d intriguing, your results. I was wondering what's the histogram of the data? Because you gave the true distribution, but you didn't give the histogram of the data. Maybe there is a data. Yeah, I should have included a slide of it because I guess it's a bit difficult to show it. I mean, so I should say that I tried this with, obviously, it's a bit weird just looking at a single data set, right? a single a single data set right because you you can't rely too much on what happens in just one case um i have tried this with a a number of um a number of samples and i still get quite a similar thing often often happening so i guess i think if i remember correctly that the histogram of the two data the the data had kind of like maybe two modes that looked sort of obvious in it um so yeah okay and so that so then i have two comments uh one is um there are some One is there are some papers in the frequencies literature using penalized likelihood along like Berger and Massa methods by Mogis and Michel and Katie Mogis. It was at least 10 years ago. So they use penalized maximum likelihood methods in a sort of model selection procedure to estimate the density. So you might want to compare with what they're doing. Their penalization is much stronger than the sort of penalization you're using because there is a login type of penalization on top of it. A login type of planarization on top of it. And then the second is for density estimation, as far as I understand the story, at least I would not use a location scale mixture like that. I will use the way you put, because then the problem is that you're looking for small scales and too many small scales, because essentially you're looking for each component with a small scale, and so there is a problem with the prior mass of this kind of. So, there is a problem with the prior mass of these kinds of things. And I'm not so sure it's such a good prior, but there is a hybrid version of it where you put the prior on the scale for the Dirichlet process mixture. So the base measure itself for the scale is a Dirichlet process. And so you can borrow strengths then in this case. And it works better, I think. Okay, so maybe that would, maybe if I, maybe, yeah, maybe that's why you would change the model, not the, for the, to make the Bayesian inference work kind of. A basic difference for that kind of thing, yeah. I'm not sure it will make such a difference because your result is super surprising, but I don't know, it's just a comment. Thanks. Okay, hi, can you hear me? Just a quick question. Sorry. Changing the kernel of your mixture, instead of the normal, you can try something like exponential power distribution family, maybe a family of distributions. If I remember well, that family is flexible enough and covers behavior similar to the uniform all the way to habitat distributions going through the normal. So I don't know. So, I don't know. Maybe that will over-parameterize the model, but might be useful to consider. Yeah, that sounds sensible. I don't know too much about them, but I would definitely have a look. Thank you. Hi, can I please have a look at the hyperparameter choice that you have taken for the Bayesian update? Sorry, let me share my screen again. So, I should say, I chose these only really because then it matched the These only really because then it matched the penalised likelihood approach. I tried it also with less kind of, I mean, these are very strong, right? These are very kind of unusual choices, probably. I tried it with, for example, the defaults that I found in the R package because they seemed like maybe more usual suggestions. And I got very similar results from doing that. So I'm a bit confused if N0 is zero. bit confused if n0 is zero doesn't doesn't it make a problem here then it's like then there's a then it's like no no kind of a sort of improper thing on on here uh but i tried it i mean i tried it also with n0 is one and it did the same basically okay thanks for the colour one yeah it so i you is your So is your is your zero zero creating an improper prior? So and noise zero, well is yeah, it's making this sorry, this this is this is wrong, right? This should be on but anyway, yes, um yes, it then makes this just an improper prior here, right? So, yeah, I'm not sure that this is really a sensible, uh, sensible choice of the prior. It was really just so that I could say that you know, this was the kind of Say that this was kind of in some way the similar model to the previous one with the maximum penalised likelihood. Yeah, I'm really not sure whether that's a sensible thing, and I thought it could cause problems, but I got exactly the same with other choices too, where it was definitely proper. So, okay, we have a question from David. So, yeah, David, can you hear me? Yes. Hear me? Yes. Okay. No, sorry. It's just a silly one. So within the fully Bayesian framework, I may have missed this. Are you looking at the posterior predictive? Like the natural analog to the leave-one-out cross-validation in the frequentis setting, in the Bayesian setting, would be to work out what's the posterior predictive after leaving each observation out. out um and then use that yeah so i mean at the moment i haven't done any yeah at the moment i haven't done any like leave one out right because because i essentially because i i sort of failed with just doing my model fitting too badly and thought okay i can't really there's not much i can do with with this at the moment because i just get such such bad behaviour and yeah i think definitely if um if i've got some reasonable behaviour here um then that would be a sensible thing to to look at um To look at? Well, it just feels like the solution you're getting is driven by the criterion you're using. It feels if the goal is prediction essentially in your case, then some posterior predictive criterion would be more suitable, maybe. I don't know. Just, yeah, it feels it might just be an artifact of the criterion you're using to decide. You're using to decide the number of components. But in this case, I'm not really using any. I mean, I'm fixing the hyperparameters at something, but then I'm not really using any particular, you know, I'm not using any criteria. I'm then just looking at the inference. So it's, I mean, I'm sure that some posterior predictive checks should pick up on the fact that this is a bad model fit, right? So, and then you could look at. And then you could look at what to do. But yeah, I have a feeling that I mean, when I first saw this, to be honest, I had a feeling that my code was just totally wrong, right? So, which is why I tried doing it with an existing R package to check what's going on. Am I doing something very stupid? And at least when I found that that R package was getting the same results as me, it seems like probably I am still doing something stupid, but it's probably an easy, stupid thing to do if it's also being replicated in somebody else's code as well. In this, in somebody else's code, as well as mine, so um, yeah, yeah, I hear you, I hear you. Yeah, never mind. It might be just that this situation what you have is that the support is so different between the normal and the uniform that somehow this is like a really hard. Yeah, I mean, I think I started off with a sort of difficult problem for this. By accident, I probably started off with a problem which was quite difficult for the model. And obviously, changing where the data. Changing where the data is is obviously changing the inference quite a lot, though, in both cases, the inference is quite bad. Okay. So, but yeah, it would just be possibly if other people try to fit this data to using their favourite method, then possibly everything just works. And that would be great. So, yeah, it would just be really interesting to understand when this problem occurs. Cool, thank you. Cool, thank you. In front of everyone, thank you. Hi, morning. Can you put again your slide with your Bayesian model? Let me. Yeah, not this one. I don't know whether you have a type of I don't know whether you have a typo there in your notation. Yeah, this is a typo, if that's what you mean. Yeah, yeah, it's same as queries variants, right? Yeah, yeah, that should be. Yeah. Should be on the top. It should be on the top. Yeah, that's absolutely a typo. Yeah. Okay. I corrected it on the earlier slide and then missed it on this one. Okay, so do we have other questions? Okay, so if not, uh, we will thank Ellen again.