Mighty as we switch from the previous talk to this one. By way of background, I think I understand a fair bit, but I'm a relative novice when it comes to all of the modeling and the sophistication that I've seen. I started life after graduate school as an economist in the federal government, spent 35 years as a public servant, a decade in central policy agencies, finance, treasury, privy council office, and then 25 years. StatCan was an absorbing StatCan was an absorbing state because I was inside the cookie jar, as it were, with unexpurgated access to a lot of the greatest data. At the same time, during those 35 years, I happened to hang out with folks in the population health program of the Canadian Institute for Advanced Research. So I've been following along with that. Within STATCAN, you know, we started major new health surveys. I also was responsible. I also was responsible for a research group where we developed fancy microsimulation models, and I was keen to do longitudinal linking of health data. So let me turn this a little bit. So my view is, I'm a data geek, I guess. So I see the potential of really having much better data in order to support modeling and pandemic preparedness. Preparedness and data are relatively useless without their being analyzed. So, analytical capacity goes along with it. Unlike economic cycles, pandemics are rare. That's the good news. Bad news is that interest in supporting the kinds of data collection and analysis or analytical capacity tends to wane. We've seen that with the public health agency, you know, decades of worth. So, a crucial thing in my mind as I'm thinking about data is how to make sure. As I'm thinking about data, is how to make sure these initiatives or improvements in data are sustainable. So, one of the implications is not only should be as much as possible designed to be useful during the quiescent periods, you know, if there's a pandemic every once every 10 years for a year or two, and then there's a decade in between. During that quiescent period, it would be useful if there was something building on existing platforms and. Platforms, and I've been saying for quite a long time that Canada is suffering from a privacy chill when it comes to accessing data. But for the last five or even ten years, you know, it used to be was the privacy chill. Now it's a privacy chill for public good uses of data, plus enormously expanding privacy invasion on the part of the large social media companies. So talk about because I opened my mouth a couple of days ago. A couple of days ago, and you know, said, you know, we should think big and all that. You know, I've adjusted my slides quite a bit over the last couple of days. And so here's a few examples. So the Public Health Agency, fortunately, and in wisdom, convened an expert advisory group, which I spent a little bit over a year on, published a year and a half ago, May of 2022. And these are some of the key points that there were observed: irreparable fragmentation. Fragmentation. These needs are strongly linked for healthcare and public health, and data should be linkable, so music to my ears. Subsequently, a year later, you know, this went through all kinds of machinations inside the bureaucracy, but all of the deputy ministers and minister of health have gotten together and agreed that there really is a problem here and something should be done. There is an organization that's been around for 20 years now called. Around for 20 years now, called Health InfoWay Inc. It's been the recipient of billions of dollars of federal money. Its board of directors is provincial deputies, and ministers and deputies have endorsed the idea of interoperability. I was pleased as punch when I saw that they actually used the word blockage when we referred to the effect of the privacy chill on health data in that report. So, people need to be able to access and analyze large data sets. And analyze large data sets, and not just for patient care, that's the number one driver, but they explicitly mention for analytics and research and population health. So I don't know, maybe I'm too much of an optimist, but I see this as some significant progress. I used to work in finance, so I do pay attention once in a while to federal budgets. I was extremely disappointed back when I was there. I would say to some friend of mine, Did you see what was in the budget? And he'd say, What budget? So I'm not sure how many of you pay much attention to. I'm not sure how many of you pay much attention to these things, but $505 million over five years for data. And so something's moving here. There's always a fight after the budget is announced, you know, that sort of very broad-brushed things. And then inside the bureaucracy, everybody's fighting over exactly which chunk of money they will get and all that. So we don't know what's going to emerge. I don't. And then most recently, you know, there's been a lot of, I guess, concern expressed. I guess concern expressed about the way the granting councils work. I was chatting with a number of you and observing that funding for the kinds of infrastructure that one needs to do modeling on a sustained basis, there just isn't a window at NCERC or SHERC or CIHR or CFI. Well, this came out just a little while ago, I think within the last couple of months, and they basically say SHERC NCERC, CHI, IR, and CFI should be blown up. IR and CFI should be blown up or at least majorly changed. So that's another, you know, I commend you to find this report and have a look. I did write a brief to them saying that things are screwed up. So, in short, there's a window of opportunity here. Now, let me get back into what I was originally going to talk about. So, you know, there's this important There's this important interplay between data and models. I was able at Stackhand to have my fingers in both. So, when you have data, you use it. All of you are doing this and telling us about it, each other about it, to create a model. But then you play around with the model for a while and you say, wait a minute, I can really use some more information and therefore we should improve our data. So it's part of the cycle that we're involved in here. I'm an amateur. I'm an amateur. Did it come? No. The trouble is, as we look here, there's all these data silos, and you guys, I think, know far better than I do about the problem. So we have some of this, but it's not integrated in any useful way. So I looked around for a diagram of an evolving spiral, so data to model, to more data, to better model. And then let me throw out the idea of a digital twin as a new kind. Of a digital twin as a new kind of data that might be useful. So, hopefully, that will lead to better models. And as an amateur astronomer, and sometimes sole interested in cosmology and all that, if any of you know about that stuff, the iteration that's going on over the last 20 or 30 years between bigger telescopes, different wavelengths, satellite telescopes, the iteration back and forth. Somehow the astronomers are able to persuade the funding agencies to put mega bucks. Agencies to put megabucks into new observational instruments, which in turn lead to whole new sets of models, whole new sets of inferences, and then another round of multi-million dollar, billion-dollar telescopes. Further context, you know, this has come up and I think I've mentioned it. You know, my experience in both the Department of Finance, the downtown economist-dominated ministries, and the health area, is there's a dramatic difference in the corporate culture. In the corporate culture, you know, where economists tend to be quantitative, do regen, and health people tend to be more lawyers or whirling about, worrying, you know, and deputy ministers worrying about hospital deficits, you know, nurse negotiations. So the receptor capacity for quantitative analysis and evidence and foreign policy, I think, is significantly different. I would never say worse in the health sector than in the finance and economics. Finance and economic sector. Another thing that's emerging at Statistics Canada is what's called virtual data labs, because I think the more detailed the data are, inevitably there's going to be a bigger risk of privacy disclosure. And so there's a kind of a continuum of if you want access to the really wonderful data, as I was inside the cookie jar, you have to go through more hoops. And Stack Hand, sooner or later, maybe by March of next year, will create not just the research data centers, which you're going to Not just the research data centers, which have been around for a while, but you'll be able to, I actually can do it, sit on my computer at home and play with census microdata and other things directly. The emergence of cell phone data, I think, is really important. It's not without its controversy. I thought the Conservatives did a terrible job saying that the TALIS data that were provided to FAC were disclosing the details of 30 million individuals. I watched some of the parliamentary tests. I watched some of the parliamentary testimony. There was just garbage being uttered by the opposition party. Another important thing is there's a really wonderful company called Environics Analytics that takes the census data and a whole bunch of other stack-hand surveys and produces very fine-grained small area data. And they were bought by Bell Canada a year ago, so they're starting to move into using the cell phone mobility data. I'm not sure under what kind of privacy protocols, but Of privacy protocols, but this is evolving, so that's an important other part of the context. And given my previous experience with Pop Health, you know, the social determinants of health are there. I've not seen nearly as much as I would have liked in terms of occupation or socioeconomic status in the models that have been produced here. It's been much more at the biochemical level, but that's an important part of the context and the reality of the way the pandemic was managed or not managed. Not managed. So let's talk about digital twins. Why would we want one of these things? To provide detailed support for how a new infectious disease is likely to spread within the population, to provide the basis for simulating that evolution. And I'm a great fan of agent-based modeling because it's much more open in terms of the range of assumptions that you can use. I treat it as being upward compatible from ODE kind of modeling. ODE kind of modeling and to inform interventions. So, the stuff I used to do in finance and pension policy and all that was all about what-if questions, which you can only do if you can rigorously construct a counterfactual scenario. How many people have heard of Episims? Okay, this is 20 years old, came out of Los Alamos, which is the place where the Adam Bomb was developed. A little bit of trying to beat some swords into plowshares. So, who knows? Shares, so who knows? You know, it's funded to millions of dollars, I think. But it's an infectious disease model where they had 40 million, or no, 200 million agents in this thing. They didn't have a budget constraint on accessing computing. But again, this was 20 years ago. There's a nice book by Epstein and Axtel where they talk about growing artificial society, social science from the bottom up. This is a wonderful example of the bottom-up kind of approach. The bottom-up kind of approach. And so they have sort of where people are located geographically, where their transportation networks are. They have time use data showing how people spent their day, whether they were going to school, going to work, going shopping. And one of the things that you can do once you've generated a population is saying what is the contact pattern in that population. And this is an emergent graph of the degree distribution of the Graph of the degree distribution of those contacts from EPISIMs. And it's not scale-free, it's not power law, but it's certainly skewed and it's definitely not random. So I've been very interested in trying to think through, you know, how would we, what's the impact on the modeling if you abandon the assumption of random mixing? But my various dinner, lunch, coffee break conversations over the last couple days have persuaded. Over the last couple days have persuaded me that it might not change too much, but still, I think what's coming out to my mind is it's the heterogeneity in the small area aspect that's really the critical thing for improving the model. And here's the attack rate in Los Angeles that comes out of it. Fast forward a decade or so, and one of my formative experiences in this area was meeting Babak Purbal, passed away unfortunately, where he had built this model for Vancouver. Where he had built this model for Vancouver, and he used the contact network. It's a simpler thing, but it's in the same spirit as Epi Sims. So here's pictures of classrooms and schools and shopping centers and all that. The reason I got involved is he had completed this model for Vancouver, and he got, I don't know how he got a hold of me, but he said, Michael, can you help us? Me, but he said, Michael, can you help us do this for Canada? So I, you know, without too much trouble, being inside the cookie jar, generated a bunch of tables from the population census that were non-confidential. He was keen to split out healthcare workers because they're more likely to be exposed to infection pressure, as somebody put it in one of their talks the other day. But, you know, provide a foundation for doing this. Unfortunately, I don't quite know why. I don't quite know why the project died. But, anyways, and here we see real skewed distributions of household size, so large households, more likely transmission in there. School size distribution, another factor. So what are the implications of FESIMS and the Vancouver model? Well, my big takeaway is pervasive heterogeneities, particularly when you are at the small area level. Area level so that I'll be keen to hear your response. You know, it's like, how much would it open up, if at all? I assume would your modeling thinking and what you would be attempting to do if you had access to this highly detailed data with much more realistic population heterogeneity. Now, this isn't inexpensive to create data like that, but you know, in my previous But, you know, in my previous experience, there's a huge demand across Canada for all sorts of reasons for small area data. In fact, Moon Veronics Analytics makes their money by providing small area data to banks and to large retailers and things like that. So the idea is for sustainability, it's important to try and construct something that, while designed to be very useful for epidemic preparedness, is at the same time going to be useful for a whole lot of. Same time, going to be useful for a whole lot of other purposes. The little phrase, collect once, use many. So, think of a data feeder system or supply that can be widely useful, and this is critical for the longer-term sustainability. So how would one actually go about constructing this beast or device or data set? Well, you start out with the idea that what we're after is highly multivariate individual. Highly multivariate, individual-level microdata, so that from a distance it looks like you're looking at the actual raw data from the population census. It's a way of synthesizing in a way that is not confidential, census-like data. So 40 million records, preserving the correlation patterns within individuals, fine-grained geographic detail, hierarchical structure, individuals within households, within communities. Within households, within communities. And the first instance, when I said, you know, as I've been sitting here over the last few days, there's a whole lot of stuff that's clearly left out of a digital twin. So the key one is the infection incidence, vaccinations, and medics. But, anyways, let's leave that aside for a moment. I'll return to that at the end. How can you construct a digital twin database? A digital twin database. Well, Epicim started at the bottom and built it up. Babak Poorbalal and colleagues was a much smaller effort, not funded by the Department of Defense or anything like that, and had only 200 households. So my sketch here is something that's of intermediate scale. I think it's a little faster than walking, but it's certainly not, you know, walk before you run, maybe trot before you run here. We run here. So, the idea is to construct a synthetic micro database for the geodemography first of the entire Canadian population and then impute key-added variables of general interest in order to make sure this is of interest not just to epidemic disease modelers but to a much broader audience. Little parentheses, I have to worry that Stack Ham is a retailer for a lot of stuff, but it's a wholesaler to Environics Analytics. So, one of the things we used to worry about is we don't want to cause them to bankrupt. We don't want to cause them to bank. We don't want to invade their turf. So, how can we construct this thing in such a way that they would still be happy in business? And then, added value stuff that's particular for this. For confidentiality, the approach is quite simple in concept, is if we start with the building blocks all already being in the public domain, then anything we do by mushing around, that's one of my fancy statistical terms, right? These public domain data, the result will be, by construction, public, and we want to make sure it's sustainable. There's an increasing amount of talk I've been exposed to about artificial intelligence and synthetic data, but as I spent some time looking through this literature a couple of weeks ago, I pulled this quote out. There's some potential there, but so far it's not ready for prime time. Ready for prime time. And just to indicate how seriously this is being taken, if you go to the Office of the Privacy Commissioner of Canada and look, there they are talking about improving their skill, having a watching brief, digging into synthetic data. So I would worry that one of the things that will emerge from the Privacy Commissioner, or all of them across the country, because they meet regularly, is a push to say, well, if you guys want this kind of data, go get somebody with an AI algorithm. Somebody with an AI algorithm and they'll synthesize it for you. And I fear that if you accept that, you're going to get crappy data, pardon my language. So the basic geodemography. Well, Stack Hannah Ray publishes very detailed tables, 30 different this by this by this at the census tract level. So that would be one thing. And there's a couple of public use microdata files that are published. So with simulated annealing, you can say, okay, I want to create households and then. To create households and then within households, individuals such that they match the census tract cost tabs. So that's one thing. So I'm waving my hands, which is perfectly allowable in a math lecture, I guess. And then next step is go to the public use microdata files and do synthetic matching, where you have already constructed a synthetic microdata file that has a set of X variables, and they're the same X variables. The same X variables on the microdata files, and then you just say, Well, let me search for individuals or households on these microdata files and pull over vectors of things so you preserve a fair amount of the correlation structure, not all of the fair amount. And then if you really want to get fancy, you can do multiple mutation to preserve some sense of the variability or noise in what you're doing and pull over key variables. Variables that are most important. Not only for infectious disease modeling, but also to maintain the support of all of the people across the country for a whole bunch of other different reasons. Would love to have a synthetic, but as realistic as possible, microdatabase for the Canadian population that mirrors the census. Another problem is the 2021 census is now available, but in general, the census data are going to be from two to seven years old. Is old, but StackAn has this beautiful model inside called DemoSim, which I've been pushing them for years to make into a public use thing. It runs off the actual census. If we had one of these digital twin micro databases, it wouldn't be too hard for Stack Hand to create a second version of DemoSim that was in the public domain so that one could create, say to Stack Hand, can you just give us the updated ones, which they already do for the social policy simulation? Do for the social policy simulation database, something else. And you can also grab data from more frequent surveys like the Labor Force Survey, which is monthly. The Canadian Community Health Survey is in the field monthly, even though it only produces annual stuff. The Retail Trade Survey is monthly. If you want to get some idea of what's happening there. But in addition, as we saw in the EpicIMS and the other, time use patterns are critical. Use patterns are critical. Stackhan does do a time use survey once every five years. It seems to me that understanding how much time people spend commuting or at home or at their work or riding the bus is another critical factor. We can imagine pulling those data over. There's disability screening question on, you know, everybody who fills out the long form, so 25% of the census population has there's disability data, there's commuting data. Data, there's commuting data, lots of stuff. If you want to get fancier, click on us. And for real-time updating, the big enchilada here is the cell phone use by detailed geography and partially aggregated so that it's not going to cause the kind of apoplexy that conservatives were having with the TELUS example a year or so ago. A year or so ago. So, my penultimate slide here, Carolyn said: you know, we had this conversation about thinking big, and I hadn't realized until last night that there was this report of a document about charting a future and creating a permanently funded institute for emerging infectious diseases. But here's my last slide, which Slide, which is what would be the implications? For better or worse, and for my sins, you know, I have had the experience inside the federal government that said, you know, like we want $20 million a year in perpetuity for the Canadian Community Health Service. Good news there was there had been previous cross-country consultations, and there was almost virtually unanimous demand for saying the province-level data is just not good enough. We need health region-level data. Need health region-level data to get going. And I have no doubt that if one did further consultations, you'd get a similar strong message of support. So getting the dollars is one thing, but we're not talking a few hundred thousand dollars. We're talking tens of millions of dollars here. But in the context of $500 million for data in the budget and $200 billion in federal fiscal transfers, you know, $10 or $50 million is. $10 or $50 million is small. And also, in the context of the billions or tens of billions of dollars of economic harm that arguably was done by poorly targeting, especially the non-pharmaceutical interventions of the last three years. We need better privacy legislation for a whole host of reasons, and I'm sure many of you can tell me in excruciating detail more about that. The info The info way interoperability. One of the things just to emphasize is the kinds of vision that's here, as we said in the expert advisory group, this has been around for 30 or 40 years. So it's useful to ask, why hasn't it happened if it's so obvious that this is a good thing? And I'm sorry to say that there are really important vested interests that are below the radar for almost everybody. But if you look at the various But if you look at the various vendors of software for hospital information systems, they have a profound incentive to lock the hospital in. So the last thing in the world they want is interoperability. And I was around and involved in the formation of Canada Health InfoAway. And one of the fundamental objectives in 2002 and 2003, when I was on the FPT committees that got this, you know, involved in getting it started, was to say there should be monopsony purchasing power. One of the criteria should be that. Power, one of the criteria should be that the software is interoperable. Well, here we are more than 20 years later, and it still is not. Well, you know, TELUS and those other companies, Cerner, whatever, they are big, powerful organizations. They don't talk through the newspaper, they quote, lobby the minister. So, support for InfoWay, you know, they've said the right words this time, and in pretty blunt fashion, but there's a bunch of But there's a bunch of deputy ministers on their board, and the deputy ministers are going to hear other voices. Given the fiasco with the Conservative opposition are lying basically about what TALIS had done vis-à-vis on the cell phone data, it's important to be really clear about establishing privacy-respecting protocols for the cell phone data access. That's crucial to watch what's happening to the extent there's an NPI that's trying to. Extent there's an MPI that's trying to encourage social distancing, physical distancing. The Googles and the Microsofts of the world are busy doing RCTs every day on their website where they will experiment with where to put a little blurb for this ad or that ad in order to figure out what the optimal place for that. So, the idea of real-time sampling of things ad hoc is well embedded in the private sector. I've been dreaming about this for decades. I've been dreaming about this for decades: that if we actually had proper health information systems and computer systems for electronic health records, if you wanted to collect data on people who have been presented at the hospital or the ER with ILI, and you wanted to ask a few more questions, it's entirely feasible from a computer science point of view today, it was feasible 20 years ago or 15 years ago, to have a questionnaire pop up. That can easily be part of our thinking. Thinking, nationwide vaccination registry. I'm blinking on the name of a colleague of mine at the University of Ottawa, but he's got the software all developed, and somehow FAC or Health Canada isn't bothering to pay him. They want to pay Deloitte's 10 times as much money for something that doesn't work. Population prevalence surveys. I'm running out of time, so we can talk about that later if you want. And another thing is there's too much of, you know, we want a database here. You know, we want a database here, or we want a database here. The national accounts are perhaps the star example where you have a conceptual framework, and StackCan has on the order of 100 different data feeder systems for the national accounts. So you have the idea of systems of statistics. I think wise for folks here to think about, you know, we have all these different things, you know, whether it's a time use survey or something on the census, but if we're interested, we need to make sure. We're interested, we need to make sure that the concepts and definitions in these different data systems, because they're never going to be all at once. You're not going to put a timely survey on the census, but you want to be able to link them in some way or smush them using my sophisticated statistical methods. So thank you very much.