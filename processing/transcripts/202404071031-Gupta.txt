So this work didn't stop, so it will seem like we're going back in time by getting it an older result, but there's still value. I think so the perspective that I'm taking in this talk is a little different than Ali was taking in his talk. So it's like a one-side summary of what I'm planning to go through. So the first sort of important takeaway from this slide would be that there are many problems that people look in online resource allocation, which on their servers look very different. What I'm trying to do in this talk is to sort of present one unified framework through which we can sort of One unified framework through which we can sort of view all of these different problems, like the organ transparent problem that early mentioned, network management, pinpacking. And just for this talk, I'll try to shoehorn in assortment optimization also. So I'll cook up a sortant optimization problem that also fits within this framework. The second takeaway is that by giving sort of all of these different problems through this one framework of multi-wave matching, we can also just sort of use one PD algorithm for all of. Use one PD algorithm for all of these problems with a good degree guarantee, right? So either constant or logic making the time horizon. Under what I'll say or claim is a mild condition that the basis of the deterministic LP is or transferred, we'll sort of get into that why I think that's a reasonable assumption. And in terms of methodological or technical novelty, because I'm looking at a very simple greedy algorithm, we can also analyze this. We can also analyze this by going beyond stationary settings. So, I'll present briefly if I have time, a non-stationary version of the problem. And they sort of to analyze this repair guarantee, we had to develop a new technique which combines Java Hood Rift, which people in Qing Theory are familiar with, with amortized analysis, which comes from the TCSP. So, sort of marrying these two different communities and the techniques that people like. So, hopefully, I'll sort of get through a few of these things at pretty strong. So, the genesis of this work was this paper that was written by Suleiman Keremov, Itai Ashlagi, Itai Kurvich, and the motivating problem that they were trying to look at was how to operate organ transplant exchanges efficiently. So, let me briefly walk you through what this picture says. So, they were trying to look at these kidney exchanges where someone is looking for a kidney. They have a relative who would like to give their kidney to them, but they are not the partners. To them, but they are not compatible. So, I have these different types of users in my system. Each pair is a receiver and a donor, and the donor is incompatible with the receiver, but they may be able to give the kidney to another pair. So, the donor here is compatible with a receiver there, and I have some poison arrival rates at which I get these pairs in my system. And this is a high-dimensional MDP, right? Because I have different sites at any point. Because I have different sites, at any point of time, I have a state in my system, which is how many people are waiting in each of these sites. And my decision for the MDP is to figure out which match to execute. So a match in this situation would be a cycle. So for example, either I can match the donor here to recipient here, recipient to donor here, or I could sort of execute a longer match. So this is the hydrogen MDP, very intractable. The main result from their paper was that there is a simple policy which Uh there is a simple policy which uh uh keeps queues bounded and the simple policy was that at every sort of con uh regular frequency you just find the max bit matching. So, assume there is some utility that we have from each max so you just do that. Uh the tricky bit here is that intently it turns out that the performance is very sensitive to choosing the right frequency. So that sort of was one thing which is okay, it's sort of nice that you have a good policy but tuning it seemed hard. And then they have a second result which says that And then they have a second result which says that if all the feasible matches just involve pairs, so just these kind of matches and not anything bigger than that, then there's a simpler policy which is a greedy policy which does the following. You basically just figure out some basic activities that the optimal solution will use. And whenever you have availability on both those vertices, you just execute that match. So that's the greedy policy that they use. And based on something I had done earlier, I knew that there would be a sort of a greedish policy which would work more generally. A greedy policy which would work more generally than this, and that's how the paper started. But then it sort of grew beyond just proving that greedy works for sort of the general multi-way matching. That's sort of how it started. And I sort of, while thinking, okay, how I can sort of think about boundary regret policies for that last result, there was also this sort of feeling that people have been coming up with these boundary regret results for many other different problems. So is there something you find that ties on? So, if there's something unifying that ties all of them, and that's how this unified framework came about, right? So, for example, one policy that I mentioned that falls within this unifying framework is just networking management. All of us sort of know here, but just to sort of quickly say what it's about. So, you have these offline resources which are planes, and you have some inventory of seats on them. And if a passenger who, let's say, wants to go from Chicago to Pittsburgh arrive to your system, you can offer them multiple bundles. So, either I can give them a direct ride from Chicago to Pittsburgh, or I can Chicago to Pittsburgh, or I can connect them through Detroit. Each of them has a different revenue. I don't know the future request I'll see. I want to operate this system so I get my maximum revenue. So looks very different than the last problem I showed you, but we'll see that the same algorithm would work for all of these problems. A simple to order system is something similar where you, for example, if you are running Dell, then you have these different parts which are sitting in your inventory. Someone comes and says, can you build a system for me? Can you build a system for me? And there may be different ways in which you can fulfill this order, right? So, for example, for this type 1 order, I can either combine parts of type 1 and type 2 and fulfill this order, or I may just sort of use part 1 and maybe get part from somewhere else, and then it's sort of more expensive for me. So I don't get as much profit. Two questions. One is, okay, how do I replenish the inventory? That's not something that we will look at. But the other question is, how do I decide which orders to accept and how to fulfill them? We accept and how we fulfill that. And just to sort of show that there's also some assortment of mention problem that falls within this framework, imagine this setting where I have a few products and the inventory is getting replenished online. So there's a dynamic availability, a type I customer shows up, and then I have to figure out from amongst the items for which I have inventory which assortment to show them. So that's a decision for me to make. That's a decision for me to make, and then based on the choice probabilities, they decide which item they want to buy. And again, my goal is to figure out how to dynamically figure out which assortment to show this customer to maximize performance. Lambda's means exogens. Lambda's exogens. Right, so the endogenent replacement quotient is something I'll say is that it's a future open work, but for now, just exogence. I mean, the framework also works if you just have off inventory for a work. Inventory for a work. But that I think many people have looked at. So another problem that fits within the framework is just a queuing control problem where I have these servers and customers who could be served from different servers, they get different value from each of them and have to figure out how to run this queuing system efficiently. Okay. Let me skip this. So here's the outline for the rest of the talk. So I'll first tell you what is that unified framework under which all of these Unified framework under which all of these problems fall. And then I'll mostly spend time on giving you the special case of the algorithm and some intuition for when I have just one sort of class of resources and this will become clearer after the next slide on NQ resources and I have ID arrivals and then I'll just tell you what the result is for the general case. Cool. So let's sort of quickly go through what's the unified framework. Go through what's the unified framework. So, here I just have three examples of the applications that I showed you. So, the unified framework will be the following. So, on one side, I will have resources, and on the other side, I will have matching configurations, which will be multi-sets of these resources. And the thing that sort of unifies all of these problems is that we will allow resources which come from three different families. So, I could have either resources which are offline for which I know some inventory at the beginning of my problem. Beginning of my problem. I could also have resources which arrive online and I cannot store them. So I can't queue, but there's some rate at which they're arriving in my system. And I'll have resources which are arriving online, but I can queue them and then use them later. And then I have some matching configurations, each of which combines some multiplication of these resources and gives me some revenue. For example, if I had to For example, if I had to uh consider the A-Ride network uh revenue management problem, I can think of the inventory of uh seats as an offline resource and this person who's arriving to book a flight as an online non-queuable resource. I either match this person right now or he goes away. For the queuing control problem, the service requests are queuable resources because I queue them and the service opportunities are something which I can't queue. So I lose them if I don't use them. So, I'll lose them if I don't use them. And for the assembly order problem and the kidney exchange problem, everything is queuable. I can sort of store orders in my queue and then serve them later. I don't want to keep them in my queue for long, but everything is queuable. That's sort of the unifying framework. Questions so far? Make sense? So, Tomly, here's the model. So, we look at a finite horizon problem over t-time steps. We'll have n resource. We'll have n resources which are either offline, online queuable, or online, non-queuable. And then I'll have these D matching configurations. For each matching configuration M, this matrix MIM says on average how many resources of type I get used by this matching configuration. And I'll also have some remote RM for that. And we assume that this matrix M also includes singletons. So for example, if I want to discard resources. So, for example, if I want to discard resources, then I can always use the singleton configuration and I don't get any reward for it. So, we'll assume that this includes singletons also. For now, I will assume that there is a stationary demand distribution or arrival distribution for these resources. What that means is the following. So, if I look at an offline resource, having a demand distribution lambda basically says that my offline inventory is this rate. inventory is this rate lambda i times the time horizon. And for online, I just assume that at each time t, this item i arises probably lambda i. If I said earlier that the non-cavable resources are lost if I don't use them, the metric that we wouldn't want to optimize is what we call any time regret, which basically says that if I look at any time t, the reward that I have gotten so far minus what the optimal would have gotten by time t. What the optimal would have gotten by time t uniformly over all time t. I want that to be small. So, for example, if I just have cubable resources, I can wait until the end and master because then I would add some part of time t equal to terms. Just a few clarification: so the consumption is stochastic, so I can only do a match if it's feasible with probability one. Yes, so if I don't have resources, then I can't match. Is that the question? Oh, okay. The resources consumed is stochastic, or it says consumed. Or it says consumed average of root type I. So think of like assortment optimization. So I must have the item, but on average I will fractionally use it. So exactly. So if I don't have the inventory, I can't even execute the match. Okay. But it will get used up fractionally. Okay. And then my second question is, time is discrete and multiple things can arrive at the same time. We will assume not. That only, yeah, exactly. So only one of them will arrive with property allowed. I don't think things break down if even. Uh I don't think things break down if even if multiple items can arrive, but okay, we'll assume these something less than once, right? Yeah. Lack of like penalized QN somehow? Or because of the anytime regret, it doesn't matter too much. So I don't have holding costs without regret. You can have holding costs. Okay. The anytime regret embeds the fact that you cannot have too big cues. Exactly, because I could have used those resources to get some more work. Exactly. To get some more work. Exactly. So that's implicitly how we control Q. Exactly. What do we mean by average number of type Io disources? Do we not know exactly what D consumes out of type I, what M consumes out of type? So like the assortment optimization problem, right? So I offer an assortment, but if I offer that assignment, then the probability that the average number of times item I gets used is sort of the probability that I pick this resource. That's why I mean on average. That's why I mean on average. Makes sense, okay? Alright. So, as I said, so what I'll do is I'll first give you what the algorithm and some results are for the special case where I just have online QB resources. So, this is the assembly to order and the Kidney exchange problem and IID arrivals just for intuition. That basically has all the ideas that we need, and then I'll briefly tell you what the results are. Okay, so first some definition. Okay, so first some definitions. So, Ali already introduced what he called the deterministic linear program, which I am calling the static planning problem. What that basically says is that since I know the rate at which I am going to see resource of type I, I can just solve this LP, which tells me the rate at which I should be executing matches of configuration to maximize my remote. So that's a static pattern problem. All of you have seen that already twice today. That already twice today. And this is the robustness of basis condition that I was telling you about. This is called the general position gap with a parameter of epsilon. What this says is the following. That if n plus is some optimal basis for this static tuning problem with demand rate lambda i, we say that this satisfies this general position gap assumption requirement of epsilon. If I perturb my lambda within some epsilon ball around. Within some epsilon ball around this lambda, and I can still find an optimal solution with the same basis. My basis doesn't have to be unique, but for some basis for a perturbation of my lambda, I can still find the optimal solution with that basis. So that's what this says. And on the next slide, I'll try to convince you that this is sort of not such an extreme assumption to make. So far, so good? So here's an equivalent way of writing. Is an equivalent way of writing this general position gap assumption in terms of the dual. So this condition is true if the dual solution to this problem is unique and if I perturb my lambda to some lambda hat in an extended ball, then my dual still remains the same. So that's sort of an equivalent definition. And this is sort of easier to show that it should be true more most often because this is the Most often, because this is a dual program when I'm trying to find the dual alpha to maximize this reward, which depends on, of course, minimize this loss, which depends on the demanded lambda i. But the feasible set for alpha i doesn't depend on my lambdas, it just depends on the reward and the matching expectations. So, this is some polyto where my duals can build, and I'm trying to find the point which is minimizing in the direction of kappa. So, most often this should be a vertex. So, most often this should be a working series. So, that's why I'm saying that this is a mild condition to assume that you have this condition. And under this condition, the result that we have is that the greedy algorithm that I will present in the next slide has a rate rate which grows cubically in the number of resource types and as 1 over epsilon, which is the cap. What if this condition didn't hold? Could you just build up some parameters in the L D and then it would work? Or that's fine? And then it would work, or that's fast. So, if that doesn't work, then you will still get root d regret. So, that is not that's easier to show, in fact, that you'll have root d regret. But to get constant, you need just one over epsilon elements. So, for some slide, I'll tell you what the greedy algorithm is. It's super easy. So, what you do is the following. So, let's imagine that we have this problem that we are trying to solve. We have three resources and three Have three resources and four configurations. As the first step, you solve for the optimal basis. Let's call it M. And let's imagine that my optimal basis just had these three matches of configuration. So we'll just forget the configuration which was not even in my optimal basis. And now what the grid algorithm does is it essentially, as I am seeing these resources arrive online, I will commit them to the configuration in which they will get matched. They will get matched. So, as soon as they arrive. So, what I'll do is I'll maintain for every magic configuration in my basis resources which I have committed to them. And as you see, there's like one queue which is empty for each of them because if this queue was not empty, then I would have executed one match. So, one queue should always be either small or empty. And now, how do I commit resources to these configurations? I define These configurations, I define this potential function for each configuration, which roughly says the following. So, okay, let's look at configuration M. Q sub I1M is how many resources of type I1 I have committed to configuration M. M I11M is how many resources of type I one do I need to execute a match. So this is in some sense the potential number of matches I could execute uh if I was only constrained by resource I one. I was only constrained by resource I1. So, this is the potential number of matches which I could execute if I was just constrained by I. So, I sort of look at all of these adjacent differences and square them and add them by taking any permutation. How you pick this permutation doesn't really, for example, if I was trying to look at the potential function for this configuration, I have, let's imagine all the m's were 1. So, then I have 2 minus 3 squared plus 3 minus 0 squared plus 3. Plus three minus zero squared plus zero minus two squared. That that's what I think. That's right. Here is the empty solution. No, that's the resource usage matrix. The ratio, what is the intuition? So Q is how many resources I have of type I1 committed to that configuration. And for executing one match of M, I needed these many resources. So this is how many potential matches I could execute. By being constrained by resource of like IO1. So think of all the capital ends as one. If you don't use more than one resource, it's just a human resource. Is the algorithm greedy, or is the algorithm kind of greedy with respect to some potential form? Like, how does it invent? So it's greedy in terms of how it commits resources. So you commit the item so that my Commit the idol so that my sum of square potential function decreases the most. And when I say decreases the most, this is like once I submit the resource and then I execute a match effect. So for example, let's imagine that I have this purple resource arriving and I could send it to each of these three matching configurations. If I decide to send it to this configuration, before this resource arrived, my cyclic potential function was 2 minus function was 2 minus 0 squared plus 0 minus 2 squared so that's 8. If I send it then I will execute one match and then this q decreases to 1 and my new potential value would be 2. So sending it here reduces my sum of squares potential by 6. If I send it here then actually decreases 1. So I'll commit it to this configuration and then a match will happen. So that's the algorithm. So the only way you use the The only way you use the NP is through the bases. Exactly. But what about the following intuition? That after solving the NP, I might realize that some bases are used very infrequently. How would that come about within the algorithm itself? I don't know. The keyboards won't be increasing very quickly because I wouldn't be, right? So I only come into resource when the other resources have a point out for that configuration. So here's the thing. It's essentially, if I look at just M plus, the resource usage matrix restricted to M plus, that's a Usage matrix restrict to m plus that's a full length matrix. So if a resource arrives, I have to commit it to one of those configurations. And as long as I'm doing that, I'm basically using, sort of assigning resources as the NPO wants me to do, assign resources to configurations. So, yeah. Just checking intuition. So, this step of minimizing the sum of potentials and looking at the potential, is it that And looking at the potential, is it that, for example, that if all q's are the same, you kind of have the potential being zero, and that's a nice situation because one is equal to two, two is equal to three, and so on, and you can probably match one to three, two to four, something like that. Exactly. Plus, one of the q's will always be small because I would have executed a match. So one q is small and I'm minimizing the sum of squares differences which will make sure all the q's remain small. So that's why you don't care about the order, you just uh exactly the sum of squares and sum on, exactly. So this is just like a simulation to as a proof by simulation that this works because I'm not proving how. So this is an example from the paper by Soleiman, Ita, and Etai. And this was an example of a multi-day matching where they Where they were saying that the 3D argon, their version of 3D argon doesn't work. So, in some sense, what's happening here is that I have four configurations. One of them gives me a very high reward. But this configuration uses a resource which arrives very infrequently. And to execute this match, I also need these three other resources, which are also being used by these other configurations. And in this configuration, I get the other resource at a higher rate. So, in some sense, if I So, in some sense, if I just have a naive RD algorithm, it will be very hard for me to have this resource available as well as this available, as well as this available, at the time where my rare resource arrives. So, that's why their greedy algorithm doesn't work. But we are doing GD in a different way. We are committing resources to this configuration so that we can sort of reserving some spots for them. So, this blue curve tells you over time what's the regret of our algorithm compared to the of an optimal anecdote. Will be often optimal, and you can sort of see that it's bound. So that's sort of true. So let me sort of in one minute tell you some analysis ideas that go into it. It's like super easy. So the first thing that you need to prove bound regret is to know that your regret at any time t is a function of the queues that you have accumulated, as Ani was saying, times the dual value of those resources. So that's what I've Value of those resources. So, if you can control Q lens, then you can control the regular of your algorithm. And now you sort of do the standard thing. So, you define a Lyapunov function which will allow you to control the Q. So, for us, the Lyapunov function is just the sum of squares potential function. Why square root? Because this was Q squared, phi. So, I'm taking square root, so I get q's. And the other thing that allows me to do is that I can say that at each time t, my phi increases. My feet increases by some constant. So it's a random walk with bounded steps, and now I just have to control it to drift. That's where this GPG condition comes in. That if my potential function is large, then it will decrease. So that is helpful already. So after this, we have to do something more. So there's a classic result of Bruce Hayek, which under these two conditions, bounded step size and negative drift. Size and negative drift bounds the moment genetic function of the random walk. The problem with that is that moment generative function depends on the tail. And if you just bound the MGF and then try to get a bound on the mean, the dependence on epsilon is 1 over epsilon square. Because the tail of V actually does decay as 1 over epsilon square. So it's not really tight enough. So what we had to do was instead to upper bound this process in increasingly. This process in increasing convex order by just an INE excursion and the walk. So that's sort of a much nicer process. And the other advantage of doing this is that for the special case I'm just showing you, we just need to bound the mean of fee. But once you start adding these non-QB resources and offering resources, you have to bound the soup of this random walk. Now, if you just try to analyze this process, this is very ambitious. Try to analyze this process. This is very complete for bounding soup. But soup is an increase in convex function. And increase bounding the soup of like these I and D increments is super easy. And that's what gives you the log of to decrease because the soup of these over the excursions is growing like logic. For this sort of in process, you can just export it like that. Exactly. Yeah. Exactly. Alright. Uh how much time? Three minutes? Two minutes? Zero? Minus five. So the yeah. So I don't have to say much more. I mean so the only thing I was going to say was like what's the extension of results for non-stationarity. Let me just tell you like what is a non-stationary extension that we can look at. So in the non-stationary version, we allow the distributions to change over time as a pre-visible process. So what that means is that if I first look at the actual item I first look at the actual item arrivals until time t minus 1, and based on that, I decide the distribution at time t. So, we allow that much non-strationality. The only thing we need is that the general position gap condition holds on average. So, for any window of size t, if I look at the distribution, average distribution over that window, it's roughly close to my sort of a target distribution, right? A target distribution. And under that, we can prove that if you have no configurations which have both offline cubil and online non-QB, you get bounded. It grows in this smoothness value. But if you have both online Quiller and online non-Qible, then you have this logarithmic dependence more likely. So for the assortment problem that I showed you and the queen control problems, you have logarithmic degree, otherwise, you get more quick. And that's basically it. So, as Vineet was asking earlier, there's one extension which is: can we make replenishment decisions endogenous? So, that's sort of one thing. And the other is can we sort of look at progo non-stationary demand, not like things sort of almost non-stationary. Have a minute for a question for the community. Can you provide any intuition on like Any intuition on like general position gap and performance of 3D matching? So, okay, so I understand like your analysis sort of relies on the general position, but is it true that you know, in general, if the general position gap is like large, that 3D matching algorithms will perform bad? Or can you find any counterintuitive instances where like the gap is small, but then like or vice versa, where the gap is large, but really it's actually good. So, I think one result is this paper of One result is this paper of Ita Albert Wen said, where even if you don't have general position gap, you can still get bound to credit. So that's one