Oh nice. Okay. You can do it if it's easy. You can also use Pinkum on Slammy so you can have a code if you want to. I'm going to use the if you have to. Yeah, you do it. Yes, I did it. Oh, you're moving? Oh, man. You don't want to. You don't want to put your shoulder in the top. As far away as possible. I throw chalk. That's true, that's true. It's harder for you to aim if he's in the fourth row. I won't cut you. Although, second the cut. I have the toilet paper t-shirts. The thing is, chalk is a much more effective technology than a whiteboard marker if you apply the chalk. You than a whiteboard marker. If you apply the chocolate board, you get a line. Okay, all right. So, welcome everyone back to our second tutorial talk of the day, which will be given by Joel Trump. He's the Steele Family Professor of Applied and Computational Mathematics in the Computing and Mathematical Sciences Department in Caltech. And he is an expert in numerical linear algebra and random matrix theory. So, looking towards your talk, thanks. Oh, that was charitable. So, I'm going to be talking today about randomized matrix computations. I apologize that there's no abstract available, but this is an experiment, so we'll see how it goes. Here's my email address and website. There are a bunch of sets of lecture notes or survey papers available there, including some surveys of various aspects of randomized linear algebra. Linear algebra, matrix concentration, matrix concentration of linear algebra, and a course, a set of course notes on randomized linear algebra. So you'll find all that stuff on my website. So the question I want to talk about today is, how is probability used to design algorithms for matrix computations? And I will warn you that the answer is not just sketching. There seems to be a popular Popular opinion, but I think that one of the things, this wouldn't be a very interesting subject if all we did was like randomly dimensionally reduce things. I think that there are also a lot of other ways in which we use probability. My goal is to try and communicate some of the conceptual ways that probability plays a role in the design of algorithms. So you may disagree with my taxonomy, or there may be things that I've left out because this is a very short talk. Short talk. If so, please send me an email. If you have more examples, I would love to hear them. And please don't argue with me during the talk, otherwise, we won't get through very many. However, it looks like I think we have a lot of time built in the program for argument, and I would love to argue with you. Okay. So I want to emphasize. Yes, I expect our eyes like that. Glaring is okay. Scowls, no. All right. So we're going to be interested in algorithmic randomness in the sense that computer scientists think about it. So what is algorithmic randomness? These are computer programs that make random choices during their execution. And part of the question is: what is the purpose of these random choices? What role do they serve? And I want to contrast this with statistical randomness, which we are not going to be talking about. So statistical randomness is probabilistic models for things like data or errors or round off. Or a round off or other stuff. I'm not going to assume that the problem instances we're interested in are random. That would be a statistical modeling question. I'm going to focus on computer programs that use randomness in their execution. Now, one of the interesting things about randomized algorithms is that their outputs are random variables, and as a consequence, Since the output of a random algorithm is, well, random, you can use statistics to design better algorithms. And there are a lot of ways you can do this. So, for example, you can This. So, for example, you can take an existing method and apply variance reduction to make it better. You could apply the exchangeability principle to make the algorithm better. You could apply the jackknife to estimate errors a posterioriori. You could apply the bootstrap to try and understand the sampling distribution of the output. I'm not going to talk about that either, but this is a beautiful subject that's been emerging in the last few years. Okay, so what I Okay, so what I'd like to do is focus on, as I said, concepts, ways in which the randomness plays a role in the success of the algorithms. And so I'm going to divide this into a collection of different schemas. And I think that they are conceptually different, even though some of them may appear similar to each other. So the first one I'd like to start with. So, the first one I'd like to start with is the idea of random initialization. So, this is one of probably the oldest way in which randomness has been used in linear algebra computations. Yeah, I think that's fair. And the idea is that we're going to start in usually iterative algorithm. Usually, iterative algorithm at a random point in the hope that this random initialization will somehow be good on average, will allow us to avoid failure modes for the algorithm or otherwise improve its performance. So, here's an example: the randomized power method. So, method. So let me briefly say that numerical analysts really, really hate the power method. Really, really hate it. So if numerical analysis will be invective. So what I am trying to do is illustrate ideas to you. These aren't necessarily the algorithms that I would propose to deploy in practice. So, okay. So there are more problems. A positive semi-definite real m by n matrix. My terminology is a little different from Yusuf's. So this is real symmetric and has positive eigenvalues. And our goal is to compute the maximum algebraic eigenvalue. And there is a very And there's a very simple scheme for doing this, which is called the power method. The power method asks us to choose an initial vector, x naught, in Rn. And then at each iteration, we apply the matrix to the previous iterate, and we normalize. Iterate and we normalize. And then when we get tired of doing this, we report an estimate for the maximum eigenvalue, which I'm going to label CK, which is the quadratic form in our current vector xk, which we think about as an estimate to the maximum eigenvector. And so this is the Rayleigh quotient because this is a normalized vector. Because it's a normalized vector. So we think that this is maybe a reasonable estimate for the maximum vector. Okay, so we will evaluate the relative error in this estimate. So the error in CK is lambda max minus CK divided by lambda max. So because this is a Rayleigh quotient, this is always smaller than lambda max. This is always a positive number. It's always between 0 and 1. Always between 0 and 1. Okay, we'd like to try and make this small. Okay, so what does the classical analysis of the power method say? So the classical analysis is really an asymptotic one. So we assume that lambda max exceeds, lambda 2 exceeds lambda 3. So these are the decreasingly ordered eigenvalues. And we also assume that x0 is not. X0 is not orthogonal to the maximal eigenvector, which is unique in this setting. And in this circumstance, by some relatively simple manipulations, we can see that the ratio of successive errors converges to lambda 2 divided by lambda max squared. Squared as the number of iterations k goes to infinity. So this is an asymptotic bound, and it tells us that the errors decrease at this rate, at least if we've done this algorithm long enough. So it is informative. It tells us that It tells us that, at least under some mild assumptions, the behavior of this method in the long term is good. The error will go to zero, provided that there is some spectral gap. The spectral gap controls the rate of convergence. Now, a natural idea you might propose is random initialization. So what do we do? x0 is, for example, normal with mean 0 and covariance identity. So each entry of this vector is an independent Gaussian random variable with mean 0 and variance 1. And the classical justification of this idea would have been that x naught is not orthogonal to the maximum eigenvector. Maximum eigenvector with probability one. It's kind of weak sauce. So under this assumption, this theorem is active, provided this assumption on the eigenvalues. But something more interesting actually happens here, which was recognized by Dixon in 1984 and really elaborated by Kukinsky and Voznya. Elaborated by Krucinsky and Vrozniakovsky in a remarkable paper from 1992, where they observed that you can actually compute the expectation of the error quite precisely. Remember, the error is a random variable because the initialization is random. And so the error also is random. And at the kth iteration, we can compute a bound for the expectation, which looks like root pi over 2. Looks like root pi over 2 times the dimension of the matrix times the ratio of the second eigenvalue to the first to the kth power. And that this is valid for all choices of k. So now we have a non-asymptotic result. So that's cool. Whereas this somehow is fundamentally asymptotic. It's sort of hard to untangle what's happening until you drive the error terms to zero. Whereas here, you actually Turns to zero, whereas here you actually have a very precise understanding about how the errors behave, in part because the averaging works out neatly. There's some other things you see here. So it turns out that this is non-trivial when the number of iterations, k, is greater than the, what I'm going to write as the log base gamma of n, where gamma is some kind of spectral gap. So the bigger the spectral gap, the faster this thing converges. But there's a Converges. But there's a burn-in period. So here's a spectrograph, here's a convergence rate, but there's a burn-in period. We don't see the behavior start until after we've run the algorithm for a little while. This is a real phenomenon, which is uncovered by this analysis, which is also pretty cool. Now, sometimes when people see expectations, they're like, oh, but what about the failure probability? One of the other interesting things is you can indeed compute the failure probability, and it's very small. It's very small. So the worst thing that happens if your normal vector wasn't quite right is that the algorithm will take a couple extra iterations to get what you want it to, which is also cool. And all of these things are insights that come out of the probabilistic analysis that just aren't visible in this deterministic setting. There's actually something else that's even more interesting, which also comes back to the work of Kachinski and Bosniakovsky and Dixon, which is that Which is that, oh, in this case, you need lambda 1 greater than lambda 2. But without any assumptions, you have the following bound. So even if there is no spectral gap at all, the power method will produce a convergent sequence of error estimates. The convergence now is polynomial in the number of iterations. Now is polynomial in the number of iterations. So it's going down quite slowly. It's vacuous up until the log of the dimension. But at that point, you actually start seeing some non-trivial errors. This is a real phenomenon. And what this tells you is something that is totally non-classical. The power method works without a spectral gap. And all of these are consequences of the fact that we drew this vector at random. And it wouldn't be true if you just said, well, I'm going to take something that's orthogonal to the first. Orthogonal of the first eigenvector without any further assumptions. So roughly the reason this stuff all works is because not only is the normal vector not orthogonal to the first eigenvector, it is kind of oblique with all of the other eigenvectors. And so nothing bad happens. And that's what these averaging arguments uncover. Okay, so this is all cool. And so you can see that initialization is actually already. Already helpful. And there are other algorithms that have a similar flavor that are based on random initialization. So these include randomized Krilov methods. George, can I ask a question? You may. So the analysis I typically teach when I do the randomized power method is by Luca Trevison, actually, and I assume you've seen that. So he doesn't even. You're saying that. So he doesn't even need, as a matter of fact, to use the gap because all he's interested in is an epsilon relative error approximation of the top eigenvalue, right? In which case, who cares? You convert to lambda 2, you still get it, right? Yeah, so there's something that's not. And you just pipe. I just swept something on the rug here because this analysis tells you nothing about the quality of the maximum eigenvector, whether it has an absolute real one. It only tells you about the eigenvalue. But eigenvalue is perfectly conditional. So that's why you can do this. So that's why you can do those. Yeah. Okay. But the I mean vector, the vector would converge to some linear combination of the two largest. Typically that's what you see, exactly. Yeah, if they just go like that. So there are more things in a similar vein. There's a randomized Krylov and Block Krylov methods for computing maximum eigenvectors. There's randomized S V D's and randomized subspaces. And randomized subspace iteration. There are a lot of citations for this, but this is the main subject of the paper I wrote with Gunner. There's also randomized block Krilov, which is also the series of the subject of a long series of papers. There's a very nice analysis by Cameron and Chris Musco from 2015. Yeah. So in the beginning, you said that this helps you in the design of the eye. This helps you in the design of the argument. So, what does it help you in designing the angle? Well, so in this case, what this is telling you is that this randomness is not, it's intrinsic actually to the success of this method. Step one is pick a vector. And this is telling you that you should pick it randomly. The other steps in the algorithm are the same as the classical steps. But that's true for all the other things I'm writing down here. These algorithms aren't new algorithms, but I think we understand them better because we've taken into account the randomness in the Because we've taken into account the randomness in the initialization. And that's what makes them work. And there's one other kind of random thing worth mentioning here, which is that if you start gradient descent at a random point, it will converge to a global minimum under some mild assumptions. So this is kind of an old idea that's re-emerged in machine learning literature recently. So lots of benefits just from random initialization, but in some ways. Just from random initialization, but in some ways, this is the most trivial of the schema, which is why I'm starting here. Okay. Anything else you want to talk about before we move on? I realize I'm talking super fast. So in your lambda 2 over lambda max to the k, you lose the 2k that you would get at the moment. Really interesting. These things are not in conflict with each other. They're both true. So what can happen is that there are some probably. Probability of doing a bit worse, which eventually washes out when you run the algorithm for a long time. This is real also. Like, if you actually compute the expected error, you will see that this is the right behavior. And Krzinski and Vozniakovsky verified that. Yeah. Question: With the gradient descent approach, is there also a way to avoid a gap dependence in the gradient descent? In the gradient descent, or I don't know. Gradient descent is not a linear algebra algorithm. Yeah. I don't know. Some of the other folks here probably know a lot more about this than I do, so I would rather defer to ask them later. We ready? All right. Oh, the the classical analysis also gives you the same rate for conversion for the eigenvector. No, you need usually to usually the way this works is going through perturbation theories. So, like, once you have some estimate for the eigenvalue and there's something about the gap, you can nail down angles. The eigenvector is a kind of a messier story, which is part of the reason I'm not talking about it. Okay. All right. All right, number two: Monte Carlo. So, Monte Carlo is also one of the oldest probabilistic methods of computing of numerical linear algebrais. Hate Monte Carlo methods because they tend to have pretty high error and they're not super reliable. Nevertheless, I'm going to talk about them and say a lot of people. Talk about them and say a few words about how you can fix some of these issues because they actually have some beautiful applications, as many of you know. So let's start with the scalar case. So in the scalar case, the idea is that we're going to approximate a hard to compute scalar quantity by the empirical average. Empirical average of accessible random quantities. So we're going to approximate a hard to compute scalar by averaging together some random estimates. That's what Monte Carlo is. And the example we will look at is trace estimation. I know many of you Many of you are experts on trace estimation. And I am only going to show the simplest version of this result. So we're going to let A again be emission positive semi-definite matrix. And suppose that we only have access to this matrix via matrix vector product. So matVex. So you can multiply X in and So, you can multiply x in and get ax out. This is quite common in applications in numerical PDE, for instance, or in computational statistics, where A is the inverse of a sparse linear system. So, for example, a discretization of a PDE, or perhaps a regularized line smoother. Okay. So, the goal is to do what? It's to Do what? It's to estimate the trace of A with as few mapcs as possible. Now, if you're willing to do n mapx, you just read off the diagonal and you're done. But one hopes that you can do a lot better than that. And that's where Monte Carlo comes in. So, this idea is, I guess Tyler probably has some other theories about. Has some other theories about where this comes from, but the place to center the modern literature is in a paper of Gerard in 1989, where he was interested in the spline smoothing example. So he wanted to find the right regularization parameter for a smoothing spline. Okay, so how does this work? We once again let X be a We once again let x be a normal vector with mean zero and covariance identity. And what is the consequence of this? The consequence is that the inner product between x and ax is equal to the trace of A. So we can obtain this quantity by multiplying x into a, that's our primitive, and then we just compute the inner product. So this is an unbiased estimator of the trace. Of the trace. It's appalling, so we're going to improve it by averaging. So we have a first instance of variance reduction. So how does this work? So we're going to call trace hat k 1 over k sum from i equals 1 to k of xi transpose axi where the Where the xi are iid copies of x. So we're going to take a lot of these estimators and average them together and hope that we've reduced the variance. And in fact, that is true. The expectation of this trace estimator is equal to the trace. And the variance of the trace estimator is equal to twice the Frobenius norm of A under these assumptions. Form of A under these assumptions squared. Okay. So there's a very simple analysis divided by K. Thank you. Okay. Great. That's an inequality, right? Sorry, what? The forgiveness is less than equal to the forgiveness number. No, it's equal. Because I assumed it's a normal. Because I assumed it's in more money. Yeah. I mean, a lot of these choices in this talk are to make things easy to present. Okay, so the conclusion is that with constant probability, not so great, the trace estimator minus the trace is less than or equal to epsilon when k is bigger than. K is bigger than epsilon minus 2 times some measure that I'm going to call the stable dimension of A, which somehow reflects a continuous estimate of the rank. This is like the Frobenius norm divided by the trace squared or something. Sorry. Divided by stable engine. Okay. So it is, you need fewer samples to estimate the trace of a matrix that has a spectrum that's mirrored out. It's spectrum that's mirrored out. It's harder to estimate the trace of a matrix that has a spiky spectrum. And this is cool because this number does not always depend on the dimension. So you may have a dramatic improvement over the n samples, the n map x you would need if you did this naively. On the other hand, can you make epsilon small? No. No. The curse of Monte Carlo has arisen. So epsilon minus 2 is huge. So if you want to take epsilon to be 0.1, you need a factor of 100. If you want to take it to be 0.01, you need a factor of 10,000. Okay, so this is not an effective strategy. And so what do you need to do to solve the problem? We need variance reduction. We need variance reduction, real variance reduction to make this work via a control variant, which is a low-rank approximation. There are tons of papers on this too. I will mention the paper by Meyer, Muskin, Muskin, and Woodruff, which sort of crystallizes this idea, although I would like to say that there are a number of devices. That there are a number of the mesodons. So just a side point we can argue about over coffee is trace estimation or gradient descent more of a linear algebraic problem? Let's box that. Second, the one over epsilon squared was something we got in the least squares case originally, and then I think it was Ilsa, someone pointed out if you're more careful about exactly the linear algebraic structure, it's a multiplicative perturbation, so you get one over epsilon, which. Perturbations, you get one over epsilon, which in Miller's not going to get to any machine precision. That's a neat FY there. Yeah, I mean, here, you wouldn't get it here, I don't think. Yeah, here, I mean, this is really a pure, like, classic statistical Monte Carlo algorithm. And unfortunately, this is the right analysis. So this, yeah, although we've seen in the last few years that you can do dramatically better by using a low-rank approximation as a control variant, I would argue that that is actually within the range of using statistics to make better. Of using statistics to make better algorithms, but the basic idea is still Monte Carlo. So, empirical average of accessible things to compute something that's expensive. Okay. Good. Do you have a sense if you do that to the more linear algebraic problem of these squares? It already gives you the one over epsilon. Do you have a sense of how much that could be improved? Take advantage of the multiplicative perturbation of the squares and the. For a linear algebraic problem, you don't get the one over epsilon squared, you get a one over epsilon. Squared, you have one over epsilon. We never thought about it back in the day. There are lower bounds, right? So if you don't assume anything, the one over epsilon is optimal for those problems. Three squares it is. Yeah. So yeah, so I'm not sure they apply to this exact case. We have to think about that actually. So this is a nice presentation. We need to think about whether that gives advantage for this credit. Yeah, I also want to say that I'm really presenting the most elementary versions of these ideas because I want to illustrate. Versions of these ideas because I want to illustrate some conceptual points rather than you know talking about the most advanced developments of these ideas. All right, so where else do we use this? We use this for norm estimation. So this appears in a ton of places. It goes back at least to Mark Tigert's work on matrix approximation. You can use it for stochastic Lanchose quadrature, which is Quadrature, which is Golubin neuron. There's a book from 2010, and Ayusa has pursued this more recently. There are other applications like some kinds of spectral density estimation, which, for example, you'll see in a paper of Commonwealth Valiant. So, anyway, there are tons of applications of things that look like Monte Carlo. Low. And really, in the end, you're just averaging stuff together to estimate something. And you can sometimes improve those estimates by variance reduction or some other mechanism. Okay, good. Now, there's an interesting extension of this in the linear algebra world, which is Matrix Monte Carlo, which tends to have a different flavor. Of a different flavor. So, in this case, you're often trying to approximate a complicated matrix by an empirical average of simpler matrices. Okay. So Okay, so I will do an example that is not useful so far as I can tell in and of itself, but actually plays a role in some more sophisticated algorithms, and that is sparsification. So, what's going on here? We've got a matrix B, which is now rectangular and has And it has entries B I J, and you should think about this as being dense. And your goal is to approximate it by a sparser matrix. It's not actually obvious that you can do this, or in what sense you can do this, but there is an elegant Monte Carlo approach to this, which I'll describe at sort of a high level. So this idea goes back to October McSherry from 2001, I think, on database-friendly projections. Not really projections, though. So the idea is that we're going to construct a random matrix. Differently about it. Database friendly unconjecture. Database-friendly round conjecture. This is sparse. This was for low-rank accommodation. I should make sure I get this reference right. So we're going to make a one-sparse random matrix by the following mechanism. We are going to put one entry in the ij position, which is proportional to the entry of B, scaled by entry of B scaled by some probability. And I'm going to pick this entry with probability pij, where this is some schedule of probabilities that I have to determine. So this has one non-zero entry in a random position, and the size is somehow related to the size of the entry in the original matrix. You can check using elementary means that this is an unboxing. That this is an unbiased estimator of B. Is it a good estimator? No, it's garbage because it has one non-zero entry. So, how do we fix it? We average. So, we'll call BK hat. 1 over K times the sum from L equals 1 to K of X L, where X L is L is an IID copy of X. Now this has K non-zero entries, and so you can see as you increase K, maybe it's going to get better. So again, the idea here is we're going to approximate something complicated by an empirical average of things that are easy to get your hands on. And because each of these has one non-zero entry, their sum only has k non-zero entries. Simple. Simple. Now, how do we pick the sampling probabilities? I'm not going to be explicit, but if you choose them well for good probabilities, which you will find in a paper of Petros and Kundu, and using the And using the matrix Bernstein inequality, you can show that the expected spectral norm difference between the approximation and the original matrix is less than or equal to epsilon when epsilon is how big? It should be proportional, sorry, when k is proportional to epsilon minus 2. So here's your curse of Monte Carlo. Occurs in Monte Carlo. There's some number called the stable rank of B, which reflects the number of linearly independent columns, and you pay for the log of the dimension. This is a small number. So when the columns of B tend to be very parallel with each other, this number is small and you can sparsify the matrix. When the columns of B are orthogonal to each other and the rank is high, you can't sparsify. High, you can't sparsify. You can't sparsify an orthogonal matrix. So this bound captures both of these extremes pretty accurately. So what we see here is that we have this mechanism for approximating something complicated by something simple using an empirical average. So now it's a matrix and not just a number. Sushan? You can specify it down to n polynomial axes, right? Even if the stable rank is n? Sometimes. Sometimes, not for generic matrix, I don't think. How do you sparsify an orthogonal? No, the the stable rank here is and just look at the bound that you're writing. It says the stable rank is bounded by n. So if n is n of the microphone. Yeah, sorry. Maybe there's another factor missing here. Is there a north bound on E assuming? Yeah, what am I missing here? I must have failed to copy something in my notes. What? Yeah, n plus n. Yeah, there's no new vision. There's another factor of m plus n. factor of m plus m no it's m plus n times thank you or what it's like yeah m plus n log n plus m exactly okay thank you so you cannot do that that would be fantastic we'd change everything thank you for your help yeah that was a mistake um probably one of many also maybe i missed it are we assuming he is operating on mine um yeah so it should be relevant yeah we'll ignore that thank you yep i said one of many mistakes I said one of many mistakes. All right, good. So, how is this scheme used in other settings? Well, Laplacian solvers. So a rather different kind of sparsification scheme is inside the paper of Rasmus and Suchant, but really is just Monte Carlo sparsification with. With much harder to compute probabilities. This is inside approximate matrix multiplication, which is back in this original paper of Pedros and Michael from 2004. This arises also in RLS sampling. I think that's right. I think that's right. And another application of this kind of thing is random features. So I'm just trying to give you some idea about how these ideas ramify through the literature. It's not just like these one examples I'm showing you, but they actually have had an influence on a range of other algorithms. Okay, let me pause and take a breath and ask if there are anything you wanted to talk about with Monte Carlo before we move on. We move off. Okay, so random initialization, Monte Carlo. Next stochastic approximation So I like to think about this as progress on average. And this term comes out of the optimization literature. So the idea is exemplified in randomized catch marks. So Joe, before you get into this, we back to any of the general questions. I kind of understand the taxonomy, right? Because I get the so so the in a sense is a measure concentration. In a sense it's a measure concentration and it touches various places. The thing that struck me most from what you said of the first piece is that you're getting the strong analysis. You start with a random initialization, but it's nothing really simple. You just get a vector. But you really get the analysis precisely, not because it's orthogonal to the first one, approximately, but because it's orthogonal to all the vectors. You need both of those pieces. It plays well with everything. There's nothing, and this gets to the question I was asking before about that one over something that Elsa had. Where it doesn't seem, where does the It doesn't seem where does this second piece, I think you call the matrix of Monte Carlo, touch the linear algebra in an analogously strong way? The reason I was asking the question about that multiplicative piece is firm. I mean, here's one example. I mean, it applies in approximate matrix multiplication. So to the extent that this is a useful primitive, this is certainly a linear algebra computation. But the question is, can you do that deterministically or not, right? Can you do it deterministically or not? They do it deterministically or not? And the answer is to a certain extent. So, a lot of these things go through, you know, norm something, calm or something, leverage scores, reach leverage scores. And in some cases, those are equivalent to deterministic processes where you just sum up the largest leverage or reach leverage scores, assuming something about the matrix. So, you are removing those assumptions based on the. So, these are philosophical questions. So, we should argue about this later because I don't think that. Because I don't think that I'm I don't think I'm going to be able to yeah I think this is a longer conversation so save this okay all right good so stochastic approximation is basically stochastic gradient in this context because there aren't very many optimization problems that arise in core linear algebra. So we have a tall matrix with rows AI in RN and we have the right-hand side, B. And the goal is to solve minimize over X in Rn one half A. 1 half Ax minus B L T norm squared. And one approach to this problem is to do gradient descent, approximating the gradient randomly. And there's a very specific instantiation of this idea, which is called randomized catch marks. We saw it briefly in Yusuf's talk, where Talk where we solve a random equation and this leads to the iteration xk plus one equals xk plus di minus a i x k. A i x k divided by norm A i squared times Ai, where we choose I, index i with probability and I squared over the V S normal squared. So the ideas at each step, we pick a random index. Each step, we pick a random index, we apply this formula. This is essentially the gradient of the objective up to a step size. So we're taking a step in the direction of the negative gradient, then not the negative gradient, a random approximation for the gradient obtained by sampling. There are a lot of alternative views of this procedure. And there's a nice analysis of this from Stromer and Vershinen, 2008. 8, which tells us that the rate of convergence toward the optimal solution is less than or equal to 1 plus some kind of condition number to the kth power times the initial error. So, progress on average. 90th k? Plus k? Maybe even minus k. Better than 1 plus k. Better than 1 plus k. I think he was right, actually. I don't know. I like the other formula is definitely true. Okay, so there's some notion of condition number here that controls the rate of convergence. On average, we make progress toward the solution at each step. We assume A is consistent. Yeah, let's assume it's consistent. Otherwise, this drag wouldn't happen right there. Yes, there are different things that happen if it's. Yes, there are different things that happen if it's not consistent. Sorry, I'm rushing. No, no, no, no problem. It changes the progress on average. I have a question. How does this compare with the standard cap command? Where you go from not randomly but from random to cycle? Well if you um if you permute them randomly and then cycle, you will do better than this. This is turns out to be quite hard to analyze. Quite hard to analyze. Sorry, is that always true or sometimes true? You will always do, I think, at least as well as this. But yeah. I think, I mean, empirically, it's definitely better, like, dramatically. Okay. Yeah. Yeah. I'm not up on all of the details. I know there are settings you do strictly better. I forgot if it was always, but yeah, I'm happy to discuss this later. Okay. Okay. Good. So, progress on average is more or less stochastic gradient. Or you can also do projective stochastic gradient if you're interested. Constrained least squares problems. Okay. Just to clarify, this is because of something with or without replacement, because if you assume that all the rows have the same norm, Assume that all the rows have the same norm, then this is uniform sample space. That's right. So the perturbation argument, you perturb, then you move through it. You can never sample the same row twice, whereas here it might. So is this not the difference? Yeah, but the thing is, like, after you've hit two different rows, like, you know, you've changed X, and so you can still make progress by going back. If you happen to sample the same guy twice in a row, there's like a minor hit from doing that. Okay, I think there's one more that I'd like to do in some detail and then some other. That I'd like to do in some detail, and then some other things I'm just going to write on the board. So I don't have a good name for this, but there's more of a concept of exploration where we make random choices to find structure that That is maybe not completely visible. So, the example is an algorithm called randomly pivoted Chilbaski. The problem is to take Is to take a positive semi-definite matrix and the goal is to find a low-rank approximation of A with few entry evaluations. And the procedure is a very important thing to do. And the procedure is to use a partial Cholesky decomposition with randomly chosen pivots at each step. So this idea appears in some form in a paper of Shepand and Mampala from 2006. It was mentioned very briefly in a paper of Kim and David. And David in 2017, and we've been studying this the last couple of years. So, let me show you how this goes, and I'll give you some brief idea about why it works. The idea is that when you have a positive semi-definite matrix, the diagonal is positive, and it signals something about the size of the off-diagonal entries by Hadamard's in it. Entries by Hadamard's inequality. So Aij squared is less than Ai times Aj. Alright, so the diagonal tells you something. I mean, Petros and Mike have a paper on diagonal sampling that's around the same time. It's based on this insight. And the idea is that we're going to sequentially approximate the matrix by starting with the zero approximation. At each step, we draw At each step, we draw an index with probability proportional to the current diagonal entries, which form a probability distribution if you normalize. So we sample the big entries with higher probability than the small entries. And then we update the approximation residual. So A hat gets A. gets A plus the ith column of A transpose divided by Aii. So this is take the guy we sampled, form the outer product, normalize it, add it to our current approximation, and the matrix is reduced by the same amount. And at each step, And at each step, we're exposing structure in the matrix. The diagonal changes. It's easy to update the diagonal using this formula. Then we cycle through sampling at random. Classically, people used to pick the biggest entry, or they would sample uniformly. Neither one of those work as well. They're failure modes. Whereas somehow, by choosing a sampling distribution at each step, it is represented by the diagonal. Represented by the diagonal, we're exploring the directions that have a lot of energy. And on some level, the reason this works is because the one-step residuals are equal to A minus A squared over trace A. So if you apply this procedure once, on average, the new matrix is reduced from the previous matrix by an amount that's strictly positive. An amount that's strictly positive. And using this formula, you can develop an analysis that shows that this is an effective strategy. Alright, good. So, we can use randomness to explore and try and find structure that's not immediately visible to us. All right. Now, we've been through a lot of things. The other things I want to talk about, I'm not going to go into so much detail, but I want to indicate that there are. So much detail, but I want to indicate that I think there are more ideas. So regularization. So in this case, we've used randomness to transform a problem. Without failure modes, or where the failure modes are extremely unlikely. So one example of this is to put the matrix in general position. So there's a paper by Parker in 1995 who says that if you take a matrix A and you randomly write a Randomly rotate it on both sides by multiplying with, for example, par uniform orthogonal matrices, that Gaussian elimination works with probability one, right which just means you'll never encounter a zero limit. And the reason for this is because you put the matrix in general position. And that means that all the submatrices also have non-zero determinants. Have a non-zero determinant, and as a consequence, you can avoid the failure modes. There are other examples of this. More examples include things like the randomized URV approximation that was studied by Dimitri Holtz. In what sense is this regularization as opposed to preconditioning? Preconditioning means something else. Preconditioning specifically means that you are in. Preconditioning specifically means that you are improving the conditioning of a problem, like the usually the spectrum of the problem. I think about this as a different sphere. Preconditioning is, of course, that's such that Gaussian, I mean, such that Gaussian elimination. I guess the question is, what is the conditioning of Gaussian elimination? You could argue that this is reducing the condition number. That's how Parker refers to this. But I think that what's really happening here is we're putting the matrix in general position. Putting the matrix in general position. So, this is really the idea here. But the reason this works is because the matrix is now in general position in some sense. Okay. There is another similar idea, which is called smoothed analysis. So, what is smoothed analysis? The idea now is that you perturb the problem to regularize it. So, there is a paper by Spielman Tang and a forgotten student's name, unfortunately. Sinkar. Sinkar. Thank you. Sack R. Thank you. Where if you take a matrix A and you map it to a very slightly perturbed copy by adding a Gaussian regularization to it. Then the pivots that arise in Gaussian elimination are at most polynomial in the dimension. In the dimension and the inverse of the regularization parameter with high probability. So now, by poking the problem, you avoid the failure modes of the algorithm. So this is, I guess, what you would call a realization of smoothed analysis, where instead of saying, well, typical real instances are going to be like this, you could actually implement this by poking the matrix. This, of course, is a crime. Matrix. This, of course, is a crime in numerical computation, but perhaps it serves some purpose. And there are more examples of this. And for example, the pseudo-spectral shattering work that Nikhil has been pursuing. So this was thanks. Okay, so these are some of the ways in which people use randomness. I guess you can argue about whether they're actually different from each other, but I think that conceptually these ideas are distinct, even though we often kind of mix them together in our heads. There are more things. There's the probabilistic method. So you can use randomness to construct an object that's maybe not so easy to get your hands on. So, for example, a spectral sparsifier can be drawn at random. Of course, there's a deterministic algorithm. Of course, there's a deterministic algorithm as well that can do that. There is sampling. You draw a typical object from a distribution in the hope that it's favorable. And last, there is dimension reduction, also known as sketching, where you take a large problem, you Large problem, you project it randomly onto a smaller dimensional space, preserving the geometry, and use the reduced problem to learn something about the structure of the larger problem. And this has tons of applications, building randomized preconditioners for least squares as in Rockland and Tygert's work, blocked householder QRs and Gunner's work. UG and I use this first sketch subspace projection methods, building on use of so. Methods building on Yusuf's work. And I'm not going to talk about this, even though it's an important scheme, because Chris Musco has an entire tutorial on this, which he will share with you on Wednesday. And I cannot resist adding one more comment. Sketching does not work in isolation. It has to be used as part of a computational pipeline. If you just sketch something, typically the solution is so bad as to be worthless. And so it's the same. And so it's the same with a lot of the things I've talked to you about. The things that I showed you aren't necessarily real computational algorithms you should deploy, but at the same time, you can start to see how probability expands the kinds of algorithms that we can use for linear algebra and hopefully allows us to solve problems that we maybe couldn't solve before. All right, thank you. So it seems like most of the things that you talked about can only hope to get sort of bullish precisions. Where are the examples? There are examples, though, of random numerical linear algorithms where we can get machine precision. Which ones are those? Actually, a lot of these methods, when used as part of a pipeline, can get you to full precision. Individually, the ideas maybe only help you with one step of an iterative. Maybe only help you with one step of an iterative procedure. But you know, with randomized initialization, you can run the power method or the Creolon method as long as you like, and you'll get excellent results. Same for the randomized SVD algorithms and derivatives. Things like randomized catch marks, although there's some question about the practicality of that scheme by itself. Nevertheless, it converges exponentially fast. If you run it long enough, you'll get something pretty good. And even things like sparsification, used inside the You know, used inside the King and such Davis solver, you know, it is used to build a preconditioner for a Laplacian system, which you can then use with conjugate gradient to solve the problem to 10 or 12 digits, if you like. So the way I think about all of this is that these are ingredients and complete numerical methods where they help you typically implement one step more effectively, more reliably, and then. And then after you proceed, you can drive the error down as much as you want. There are some problems where low precision is enough, and in some of those cases, you don't need to do iteration. But it's a little bit harder to square that with the sort of historical perspective of the numerical linear algebra community. So for the POT, the Scholiski, partial POD randomized, what uh so what did you say about the fact uh if we take the largest entry on the diagonal, so a deterministic algorithm, you say it's worse than the randomize? And what is the metric? What is the metric of success? So you can actually produce worst-case examples where it does very badly. Our empirical experience has been Experience has been that there are situations where sometimes 3D works very well, but it's brittle. So, this is what I guess is called complete pivoting. So, what will happen is that it gets distracted by shiny things. So, if there's an entry that's sticking out, it'll always pick it. And that might be an outlier in the data or something that's somehow distracting. So, it works pretty badly when you apply it to matrices that are noisy. So, and just to repeat, the matrix is how a hat. Just to repeat, the metric is how a hat the distance of this concerns trace A minus A hat and compares it with the trace of A minus its best low rank approximation. And then there's some error. So after K steps, where K is maybe comparable with R, you hope to do almost as well as a best rank R approximation. Okay. Okay. So while I agree with you that Parker's work is best classified as regularization, you could look at it from the perspective of using this to reduce the growth factor, and then it would be more of a preconditioning. I would argue that the reason it works is because of this principle. And he does call it preconditioning, absolutely. So, wait, does this reduce the growth tactic? Well, Tom has some evidence that that's true, as does Parker, but this has been elusive. It does kind of reduce. The growth factor. Proof of this is elusive. This is not the same result, of course, because you're changing the matrix, but this is guaranteed to reduce the growth factor. By the way, I would love more examples of ways in which you believe that randomization has played a role in linear algebra computations, because I am sure that I'm missing a lot of things or that y'all would classify some things differently, so you should send me an email or talk to me later. Just for that last one, compressed sensing recovery, is that exploration or is that a compressed sensing? I mean, so I'm going to say the I mean, so I was going to say that compressed sensing is 98% useless, but it's also not linear algebra. Yeah, that is my next question too. It's dimension reduction. Okay. So what you're doing is you're taking a large problem and you're compressing it to a lower dimensional instance that nevertheless still captures the geometry. So what's happening is the geometry of the set of structured vectors is still visible. Vectors is still visible to you from a low-dimensional projection. I mean, there's something that's. I mean, there's also another geometric picture which you can think about, like tangent spaces to non-smooth convex sets. Okay, but you put in the dimension reduction category. I would probably put it in the dimension reduction. But I also don't think about it as linear algebra, really. Okay. Happy to discuss more later. Thank you. Test more later. Thank you. I'm curious about something. In the case of the estimating the trace, we had this averaging which worked wonderfully. I have not seen that idea extend to other things, like some linear systems with averaging results with a few initial guesses. For example, a given gradient of CD. And see why. There are cases. Well, there are some. I mean, I've seen some papers where. Some I mean, I've seen some papers where they use bagging methods for so for example for sparse recovery. Like there are methods that take a collection of different solutions and try and combine them to find the right sparsity pattern. I have not seen people use this, as you say, for linear systems. There are some randomized SVD algorithms that do this. Not linear systems again, but that try and take different realizations of of the randomized SVD. Of the randomized SUD, somehow average them together in some better. There's some analysis. I mean, I don't know how convincing it is, but whether you wouldn't just be better off, like, you know, solving a bigger, you know, taking a larger number of vectors or running the iteration longer in the first place rather than trying to bag them. I think it's quite common in distributed settings where you might have, you want to do. Setting where you might have. You want to do an SVD of data that's on different machines, and people will do techniques where they get an approximation for each machine than after. Maybe one motivation why you do it over. Maxim had done that with other people like 15 years ago or maybe. For some of the methods you presented, I find it useful to categorize them according to a matrix access model. For example, if you sample the columns, it's great. If you sample the columns, it's great if you have the discretization of an integral operator, because you don't want to have a whole matrix. But then you want to sample a column to the column ones, and that's typically not what you have access to. You mess up the pieces a little bit. I think this is one of the most important computational questions in the design of these algorithms. What can you get in your hands? And so a lot of the time, you know, in these sort of classical applied math things, you have access to matrix vector products because you're thinking about those as like a discretized, you know. Think about it as like a discretized differential operator or something like that. And in that case, it's very natural to use algorithms based on MapMex. Here, this is designed for things like kernel matrices, where you really have to pay every time you look at an entry. And I think that was the motivation actually behind a lot of the work that Michael and Petrus did on applications and machine learning or AgCore or trying to understand how you can handle a kernel matrix, which is expensive to access. And you can't multiply with it easily. So let's save what the rest is going to later. Before you leave, before you leave, very quickly, so some announcements. So we have lunch now, but please be back here at 1 o'clock because we will have the group photo in the foyer up here, just where we had the coffee breaks. After that, we will have the open mic problem session, and Erin will make The open mic problem session, and Erin will make some announcements too regarding. So, I just found out that I'm moderating the open mic session. I put a sign-up sheet in the Google Doc, so I thought it'd be helpful to have like an order rather than raising hands and me randomly pointing at people. So, please, if you want to present an open problem, go to the Google Talk and put your name somewhere on that list. So, we have a little bit of an order. The open problems pose were both for discussion over the week, and my understanding is also for these breakout sessions that will happen over the next few days. Sessions that will happen over the next few days, we'll be using what unfolds during this open mic session to be signed up. So please sign up. 