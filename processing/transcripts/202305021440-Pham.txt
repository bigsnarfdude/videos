For inviting me here. It is really worth traveling so far to meet colleagues. This is a beautiful place. Okay, so today I would like to present some joint work with Pierre Oila Valla, who is now working at Cube, and Moimelandouch, who is a PhD student that we co-supervise. So, as you see, there will be again Schrodinger Bridge, as in the talk. Bridge, as in the talk of Camille and Biatrice, but here it is in the context of machine learning, in fact, more precisely about generative modeling for time series. So what is generative modeling for time series? So suppose that you have access or you have access or you can observe some data samples of some time series distribution, which is typically unknown. Unknown. Okay, so it's a such situation arising in many real-world applications, typically in medicine, in climate change, but of course in finance, you may think typically about asset price or over DT surface. And so the goal of Generative is to generate synthetic samples in order to learn this unknown metadata solution. Solution. And of course, it's very useful. And then the goal is to generate a synthetic sample of this data distribution. And so it's very useful in particular in the financial industry because typically you don't have a lot of data. So if you are able to generate many scenarios, so it will be very useful for stress tests. And in the context of wind. And in the context of reinforcement learning, you can hope to improve the learning of the optimal strategy. So, anyway, so this falls, because this topic falls into the general topic of generative AI, which is now a classical task in machine learning, with several competing methods. So, historically, Okay, so historically with a likelihood-based method where you try to learn directly the property distribution of your model, by typically maximizing through maximum likelihood. Then you have the class of implicit relative models where you will try to learn implicitly your your your distribution through uh through Lord Google. And so the most prominent example is of course the GAN Is of course the GAN method. And over the last two years, a new generation of methods that have been developed by researchers from Google Brain in Stanford called score-based division model, where you, in fact, you generate a data sample from a prior distribution by adding progressively noise through a division process with with a drist, okay, which is given by some score. Which is given by some score. And so, in fact, such a new method has shown remarkable performance by outperforming GANs in terms of visual quality. So, they are used notably in image processing with spectacular success, and of course, also controversy with all this deep fake. So, here I just show you an image, a photography, that maybe you have already seen. Okay, that maybe you have already seen, which has recently won the contest of art photography, but which was in fact generated by AI, by Midjoint. So you see very look like we are pictures. Okay, and so now, okay, our goal is now to how to transpose this in the context of time series. And so if you want to generate time series, it's a really General time, it's a it's a really challenging problem because it's okay, the it's want to capture the potentially complex dynamics of the variable across time. Okay, and so it's obvious that it is not enough to learn only the time marginal, and it is even not enough to learn the joy distribution if you don't take into account the sequential structure of your time series. Okay, and so this topic has attracted recently a lot of attention in the machine learning community. So, here can probably not exhaustive but state-of-the-art generative methods for time series. So, mostly around a GAN type method where essentially you have to design architecture for neuronic group in order to capture the temporal dynamics of your. The temporal dynamics of your time series. Okay, and so in particular, I quote a very nice paper by Beatrice. So this is the cold zone of matrimonial password. You are in the AR. Okay, it's where you use the theory of adapted variation distance to generate time series. There are also recent work using the theory of signatures of the reliance, so in particular by Adeli. Particular by Adeline Fermagnon. And so, in this work, we would like to propose a GRT model which will be based on Schrodinger bridge. So, it is in the spirit of score-based division model, but adapted for time series. Okay, so the rest of the talk is following. So, I will formulate the problem and give the solution, and then give some just Okay, give some uh okay just some numerical experiments, so both on simulated and on uh real data sets. Okay, so so we have already seen what is a Schrodinger, I mean the classical Schrodinger bridge problem. So here is the formulation of a Schrodinger bridge for a time series. Okay, so it so you are given a distribution of the time series, okay, so that uh okay, which corresponds to the Which corresponds to the time theory of some process that you observe at end dates. Okay, so it's a property measure on Rd times capital N, so it's a joint distribution. Okay, so we do not, so zero by convention is T0, and you observe at T1 until Tn, which is the final time. Okay, and so the Schrodinger Gerbish-Limzy prime is the prime where you want to find a diffusion. Diffusion, where you can control the drift so that you minimize this energy function which comes from the relative entropy by Gauss of theorem. And under the constraint, okay, that this diffusion interpolates your time series. So, in other words, you perfectly match your data distribution at the time where you observe your process. So, this is a problem that we want to solve. That we want to solve. Okay, and so here notice that we start from, in contrast to the classical bridge problem, here we start from some determined C values, so for example, zero. Okay, and then we want to interpolate at the T1 Tn. Okay, so in order for this problem to be well posed, we need of course to make some assumptions. Okay, and so here are the assumptions. So we assume that the data distribution admits a density respect to the lower density. The density respect to the Lebanon measure, okay, and denoting by mu w, okay, the distribution of the burn motion along the grid time, which is explicit. Okay, we assume that the real terotropy of mu respect to mu double du is finite. Okay, so such assumption is a natural assumption of what is imposed in the classical Schrödinger. Is a classical Schrodinger garbage problem, and it is typically satisfied if your time series comes from a Gaussian noise or if it has heavy tear deception, but with a second one. Okay, and so under this condition, okay, so here's the main theoretical result, okay, which says that the solution, okay, so in other words, optimal control diff, okay, takes this form, okay, so it is, in fact, in a path-dependent form, okay, so which means that at any Okay, so which means that at any time alpha task t is a decay function of t, of the current time t, but you have to take into account all the past observation of your time series up to time 80. And this function a star has this following form which looks like a score function. Okay, so it will take the gradient of the look guide or the expectation. Of the expectation of the density ratio under the Billian measures, the property under which X is a blend measure. And so the application is that now, if you use this drift, then you have precisely a regular model for time series because now if you are able to simulate Now, if you are able to simulate this process by construction, you will interpolate perfectly your data time series. But of course, now the key point is how to simulate such processes because A star involves your distribution μ, which is by nature unknown. Okay, and so here is the next. Okay, so. Okay, so the about the Schrodinger drift function. By working a little bit on this function, in fact using Bayes' formula essentially, you can rewrite your drill function in the following form. So what is important in this form is that it is written under expectation under mu. In fact, conditional expectation under mu. Okay, and so y of some function f, which is here. Of some function f, which is here. And so the main advantage of this expression is that now, okay, you can directly estimate the delta function by relying on the sample of the data distribution. We estimate by the empirical mean under mu that under when you observe directly is a sample of your data. Okay, so note that in general, Okay, so note that in general, okay, so your drift is pass dependent, because you don't know a priori if new comes from a Markov chain, but if you know a priori that your data description is a Markov chain, then in this case, your S tar will depend on the path only through the last value. Okay, uh so and so now that you you you can now estimate You can now estimate this condition expression by the classical P and L regression method. So this means that now, assume that you have some data samples of pure data distribution, then you are given some kernel with some bandwidth, and then you simply compute directly. So it is explicitly a computer. Explicitly computable, okay? But here you have the product of the scanner because you have to take into account all the conditional expectations with respect to all the class value of. Okay, so in practice, what we choose is the quartic camera because it's very easy to culture. Okay, so now let me go to the numerical statement. So we have performed several numerical statements. Perform several Newmaker experiments, but I will show you two examples. So, the first one is a fractional Boeing motion. So, we take a fractal Boiler motion with a Hearst index 0.2. So, in other words, this rough path. So, we assume that we have a data sample of size 1000. We observe at 60. We observe at 60 with capital N is equal to 60. Okay, and so the runtime for generating 1000 generated paths is 100 seconds. And so on the left, okay, you see four sample paths of the real fractional volume motion. And on the l on the right, you see four sample paths of the Schrodinger bit. Okay, of the Schrodinger bridge. This is an estimation of A star. Okay, so it's it looks okay, not bad, but it's just a visual plot. Okay, but now if you want to go more about the matrix, okay, so here on the top, okay, we compute the the quadratic Brandon distribution of both the the real functional Brownian motion and of the Schrodinger bridge. and of the Schrodinger bridge. Okay, so the distribution, the histogram of the freshman motion, volume motion is on is on rows, okay, and the one from the Schrodinger bridge is on green. Okay, and on the bottom, okay, you can see the covariance matrix of the fractional body motion and of the GLT model. Okay, so they look like rather similar. Okay, and another interesting aspect is about the estimation of the house index. Okay, so there is a well-known estimator of the host index, which is given here. And so if we use the sample path of the of the Schöninger bridge, okay, and we plot into the estimator, okay, what we found is uh is zero point two one zero six. Which is not far from the real one, which is 0.2. Okay, so this is for the toy example. And now to finish, I would like to give some application on real data set. So it's a console data from the company Apple. So from January 2010 until January Until January 2020, okay, so before the pandemic. Okay, so it means that we have about 2,500 daily data. We use a sliding window of 60 day. And so here you see a four-generated path of the realm company, Apple, and here. Apple and here uh four real parts of the of the Schrodinger pitch. Okay, uh so here are some metrics. Okay, you okay, so empirical cobalt of Apple of the Schooling out which here's a quality version. So it's not perfect because of course you don't have IID data because of the fact that you just have a one realization. So we in order to create that data sample, okay, we just use a sliding windows. Okay, we just use sliding windows, which may explain why the quantity rationale does not fit perfectly. But from a practical point of view, what is maybe interesting for practitioners is how it can be used, for example, for deep hedging. So if you consider a collopion, for example, on Apple, at the money collapsion, you want to find, to look for a price and a hedging strategy, which means And the hedging strategy which minimizes your replication error. So, in other words, you minimize your keynote, so portfolio value minus the correction that you have to deliver. So, in other words, replication error. And so, you can keep hedging in that. You will parametrize your hedging strategy by a new network. So, here, in fact, we use a long short term memory because at priority we don't know if the model is Markovian. So We don't know if the model is Markovian, so we just take a neural network which appears will depend on the whole historical path of Apple. Then we train this neural network from synthetic data produced by the Schrodinger bridge and we compare the result with the real data set to see if we can improve the performance. Okay, so in other words, so this is a the back test procedure. Backtest procedure. So you have your data set. You divide it into a training set and a tester set. From the training set, you generate synthetic samples. And then you have to train in order to obtain your data hedging, denoted by data SB, SB for showing your bridge. Okay, from the training you also generate directly the dip uh the data deepaging from the data set. Hedging from the data set, okay, and then you compare these two hedging strategies on the test set by looking at the corresponding PL. Okay, and so here are the results. Okay, so here is the PL for the Schrödinger bridge, so it is on orange, and the PL from the data. And so you see that, okay, the See that the PL from the Schoenger Bridge has a smaller variance and yields less extreme value, which means that it has a smaller replication error in the sense that it improves the performance. And also you see on this table that the price that you will charge from the sugar garbage is higher than the price that you will obtain from the data set, which means that you are more conservative. And the standard deviation And the standard division, so in other words, the replication error from the sharing average is much smaller than the one from the data set. So, in other words, you really improve the performance using this synthetic sample. Okay, so I'm just on time, so I can give my conclusion. Okay, so in this work, we have proposed a new generative model for time series, which is Model for a time series, which is based on a Schöninger-bridge approach. And so the solution is described by a forward SD over a finite horizon, which matches perfectly the data distribution. The path-dependent drift, you have a pass-dependent bridge which captures the temporal dynamics of your time-serial distribution. And the drift is estimated by a kernel regression. By kernel regression, possibly by vectorization, which is very practical and low-cost computational. And so, compared to GAAN-type method, in fact, the simulation of this synthetic sample from Sherry Garbage is much faster because you don't need to train your network. You just need to estimate your drill, but this can be done immediately by a kernel estimator, and then you see. By a kernel estimator, and then you simulate using Eurastine. So we perform some similar numerical experiments. Okay. Yes, and just okay, so there are room for improvement, so there are some limitation and further development to be done. Okay, so the solution that we obtain is obtained under the finiteness of the relative entropy. Which may be related typically for heavy tail distribution, which you don't have some other moments, so it means that maybe you should find another criterion than the relative entropy. And if you go in very high dimension, okay, so typically for pixel in image, there is some numerical instability due to the fact that the function which appears in the drift is having exponential form. And yes, I think it's close. And so, if you are interested, there is a paper which is now on SSI and archive. Okay, and we stop there. Thank you.