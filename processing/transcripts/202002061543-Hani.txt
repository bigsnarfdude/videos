 Okay. Should I stop? It's one minute. Okay, so we have Zamir talking about on the regress derivation of the wave kinetic equations for NLS. Thanks. Thanks so much. Thank you for the introduction and thanks for the organizers to invite me to such a nice workshop. So the topic of today's talk falls under the umbrella of something that goes back Of something that goes by the name of Hilbert's sixth problem, which corresponds to an axiomatic justification of the laws of physics, mainly the laws of statistical mechanics, from the laws of dynamics. So what we would like to do is to go from dynamical laws and justify rigorously the laws of statistical mechanics. So the laws of dynamics are usually they govern they govern the microscopic system under question and they are another characteristic feature of them is that they are reversible in time. On the other hand we want to go to something that governs macroscopic quantities like temperature like the Like temperature, like the velocity of fluid parcel, like entropy, etc. And usually those laws are, of course, irreversible in time. Like the second law of thermodynamics. There's a particular time direction that is preferred. Okay, so the main example that Hilbert was interested in, I guess, in his problem, is the case of particle systems. So this is also, let's say, this is the classical example. And this passage is achieved through something, through Boltzmann's kinetic theory. And this identifies a sort of an intermediate between the microscopic scale and the macroscopic scale, sometimes it's referred to as a mesoscopic scale. Okay, so what does this theory say? What does this theory say? It says that, roughly speaking, suppose you have n particles of radius r, then the effective density f of t x v, which you can think of it as a probability of finding a power. Think of it as a probability of finding a particle at position x moving at velocity v, obeys the celebrated Boltzmann equation. Which looks like V T F plus V dot grad X F is equal to a equal A collision curve. This is called the collision curve. Okay? And once you have this equation, you can derive sort of the macroscopic equations for macroscopic quantities, like the velocity of fluid, there's an entropy that increases for this equation, etc. Now the rigorous, so that this, the answer to this question now boils down to the rigorous justification of this equation out of laws of Newton, Newtonian laws that govern the microphones. Newtonian laws that govern the microscopic system. And this was done by Lanford in 1975. So this is a rather monumental piece of work that had to be revisited and details to be filled and understood. And here I should mention names like Chin Shayani, Pulveranti at Ilner, Gallagher, Saint-Vermont at Texier, who managed to even extend this to more. To even extend this to more general systems. And also, there's work of Pulveranti, Simolina, and Safiro and Simona. Okay, so what does Landford's theorem say, roughly speaking? It says that the Boltzmann's equation appears in the limit where n goes to infinity, the radius of the particles. The radius of the particles goes to zero such that n times r to the d minus one is equal to order of one. This is called the Boltzmann-Grad scaling scaling. And this was discovered by Grad in 1958, even before Landford's theory. Landford's here. Okay, so what we'd like to do today is we'd like to see what happens if we try to carry this sort of framework. I should mention that this framework has been highly instructive and people have been doing it for other particle systems other than those colliding particles. And it has been a very valuable approach. So, what we'll be trying to do today is we'd like to consider what happens to this approach when our dynamical system is given by a non-linear wave system, and then we'll see, I'll tell you. And then we'll see, I'll tell you what is the theory of statistical physics that comes up out there and whether we can, we'll see whether we can justify it or not. Okay, so this leads us to first, I need to describe for you what is the theory of statistical mechanics of nonlinear wave, and this goes by the name of wave tolberence theory. So in the early 20th century, soon after physicists realized that waves are fundamental objects, or as just as fundamental objects as particles, they tried to carry out Boltzmann's theory for waves just like it was done for particles. So this was first done by Hughes in 1928 for a phonal. 1928 for phonals, and then by Hasselmann for waterways in 1962, I believe. And then this became an industry after the work of Zakharov and his school later in the 60s, 1960s. And later on. Okay, so in order to explain a little bit what this theorem says, let us fix a number. Nonlinear wave system. So I'll start with the NLS equation R d T V plus Laplace V is equal to V squared V. I'll comment on the sum of the nonlinearity soon. Basically, it doesn't make any much difference for the theory. And V at the time t equals to 0 x is v initial. So t is the t. So, T of course in R represents time, and the spatial variable, we take it in a torus T D sub L, which is a torus of size L. So this could be something like 0 gamma 1 plus 0 gamma Z D, where if gamma 1, so the results that I would mention today cover both rational and irrational cases, if gamma 1 and up to gamma d are If gamma 1 and up to gamma d are rational independent, this is irrational torus. And in some cases, we require that gamma 1 up to gamma d are generically irrational numbers. Okay, but we'll get to that. Alright, so of course the aim here is to understand the long-term behavior. And one can try to do that deterministically. So deterministically, it turns out that this system is too complex to understand deterministically. Even starting from arbitrary small neighborhoods of the origin, you can exhibit very different and vastly different, qualitatively different long-time behaviors. You can see, for example, KM quasi-behavior. Example, KM, quasi-periodic solutions in time, periodic solutions in time. You can see soiltone-like behavior, growth of soil norms. So it's complex. And I should point out that the same goes, it is equally complex if you study this equation instead of dealing with the torus of size L, you study it on R D. So on R D, it is actually a remarkable fact that we can A remarkable fact that we can, in many cases, particularly for this equation, we can describe the asymptotic behavior, but we really know very little about the long-term behavior, the intermediate behavior between zero and before it, for example, scatters. Okay, so, and everything that I will talk about today can be done, not rigorously, but can be done at a formal level, starting from RD as your initial tool. But I want that's for maybe another meeting to discuss. Maybe another meeting to discuss. Alright, so the solution is to talk about what happens for maybe generic data. And this, of course, leads us to the realm of statistical mechanics of this system. Okay, so how to describe the statistical mechanics of such a non-linear system? So there's already one So there's already one important parameter that is playing a role here. Another important parameter is the characteristic size of solutions. So for this, I will adopt the answers v equals lambda u and think of you, for example, if you normalize u in l2 to be equal to 1, then lambda would be correspond to the L2 naught of v. Then if you do this answer, the equation Then, if you do this answer, the equation satisfied by u is essentially i plus u is equal to lambda squared u. I will also, lambda squared u squared u, I will also for simplicity so that I'm only working on the torus x. x dg sub L, which is L times 0, 1 to the D, our rescale space. And this comes at the fact at the expense of adding rescaling the Laplacian. So this Reescale de Laplacian, delta sub beta, is just sum j of j equals 1 to d beta sub j d squared by dxj squared. Okay, and then again, u of t equals 0 is u each. Okay, so Okay, so then the uh whether you work on irrational torus corresponds to whether you take those better chips be one or uh rational irrational rationally independent. Okay, so what does the kinetic theory say? Or if you like uh the uh kinetic uh conjecture, did I miss anything? Oh well. Oh well, let's say kinetic description. So in order to put this kinetic description in a nice way, let us introduce one more thing. Let's introduce alpha, which is lambda squared L to the minus t. So this is the characteristic size of the non-reaction. Okay, why is that so? Why is that so? As you see over here, for example, you can normalize u to have L2 size 1, and u will be a random field in L2, and the random field in L2 is essentially uniformly distributed, and as such, its L infinity norm will be bounded by L to the minus D over 2 with high probability. Okay, so let's and if this is so, of course, this needs. Of course, this needs to be, this might be true at t equals to 0, but it needs to be propagated by any proof. So let's say u0 L2 order of 1 random, then it's L infinity norm heuristically. So this is a heuristic argument why I'm calling this the characteristic size of the nonlinearity. Okay? Okay, because for now it's just a new definition. Then if you measure lambda squared u squared u, you put two copies in L infinity, one copy in L2, which is normalized, then this has a size order of lambda squared L to the minus T, which is what I call alpha, with high probability. Okay? So that's a justification of why I'm calling this a characteristic side, size of non-reactive. Now the conjecture is the following. Is the following. It says that the effective dynamics of the expected value of the Fourier transform of the solution at frequency k squared. So remember, since you're working on a torus of size L, k belongs to what I will call C D sub L, which is So we call Zd sub L, which is L inverse Zd. So it's a mesh of spacing 1 over L. Then the effective dynamics of this is given by the wave kinetic equation, which I will refer to from now on as WKB. And this equation looks as follows. Write it over there so that Write it over there so that we keep it on the board. Looks like this. dtn is equal to k of n where k of phi is given by the integral of x1 minus xc2 plus xc3 is equal to xc. So this is k of phi at xc. Alright, so and then you have phi, phi1, phi2. phi, phi1, phi2, phi3, 1 over phi1, minus 1 over phi2, plus 1 over phi 3, minus 1 over phi. And then I have a delta function over c1 squared beta minus c2 squared beta plus c3 squared beta minus c squared beta, where I always But now, where I owe you some definitions, so I denoted phi j, this is phi at xj. This is the shorthand for phi at cj. I'm integrating c1, c2, and c3. So this is dig c1, dig c2, dig c3. And what else? And this c squared sub beta is just the sum of our beta j c j squared, j from 1. J from 1. Okay, it's the symbol of this Laplace. Okay, so what do we mean by the effective dynamics are given by this equation? It means the following. So more precisely, for t similar to the what is called the kinetic time scale, which is one over alpha squared. Again, alpha, given the derivation of alpha, it's add to the 2d over. Of alpha x l to the 20 over lambda to the 4, you have that, or you expect that you have of t k squared is approximated by n, the solution of this equation. Okay? At time t over t k okay? And this holds for, should hold for L very large and alpha very small. Okay? That's the conjecture. Of course, for this conjecture to hold, you need it to hold at t equals to zero. So at t equals to zero, we need So at t equals to zero we need what we call well prepared data and by this I mean that u at time or u hat at zero k is given by so that this is satisfied at zero so it's given by square square root so this is u initial right I denoted u at zero so u initial is the initial data of the kinetic Initial data of the kinetic equation, this one, at k multiplied. So this fixes the modulus, but now I need to randomize the phase. So this phase I would denote by nk of omega, where nk of omega is a random variable which has basically what you necessarily need is that it has expectation is equal to one. For us, eta k of omega will be will be either the standard Gaussian, mean zero variance one, or it could be a random sign of the form e to the third i theta k omega, where theta k omega is something that is uniformly distributed in zero to pi. Distributed in 0 to pi. Okay? Alright. So this is the analog of Wolfman's equation for a nonlinear wave. It is the kinetic equation. There is also a version for this equation that is inhomogeneous. It has a transport term. But again, that version would come if you start with your domain being Rd. But we're not discussing that today. And it is something. Uh and it is it is something that is used uh heavily in uh in applications, like in uh particularly in uh uh ocean forecasting and and many other uh so in particular the rigorous derivation of uh this equation seems to be like a scientifically interesting problem because it is uh it helps understand the limitations and the regimes of this wave turbulence. Alright, so finally I want to make a remark on the sign of the nonlinearity. Sign of the nonlinearity. So, given all the ansats that I told you, so if you look at the potential energy over the kinetic energy, the characteristic size of the potential energy over the kinetic energy in the error equation, this is order of alpha, which is much less than one. And it is for this reason that it really doesn't matter in our theory whether you put a plus or minus sign. Theory, whether you put a plus or minus sign on the in front of the radio energy. The kinetic energy itself has size alpha times L to the D. And this kinetic energy will diverge in the limit as L goes to infinity. Okay? So what we are really talking about are solutions that will be taking the limit because this is something that is supposed to hold in the limit of large L. In the limit of large L, we're talking about solutions that have vectors. Talking about solutions that have very large, that have asymptotically diverging total energy, which is something that is typical when you take thermodynamic limits and when you do statistical mechanics. Okay. Okay. So, what is known about the rigorous justification? So, this equation has been written for by physicists and for different For by physicists and for different nonlinear wave systems. I just wrote you what the equation looks like for NLS, but it has different shape and different formulations if you start from another microscopic system. Okay? So in terms of the rigorous regulations, there have been some partial attempts and partial results at that by Cooksin and Faub and others that I won't be able to name completely. But the first attempt at the full problem. Attempt at the full problem was done in a work by Akmasi, German, myself, and Chattan. Okay, and what we did in that work what we did there, because we What we did there, we justified basically, let's call this star, justified star for t smaller than some t star, but this t star was much smaller than the kinetic time scale. Okay, basically something like altar minus 1 over 3 in some regimes than the kinetic time scale. And this is for And this is for V greater than or equal to 3 on the irrational. In addition to starting the like or laying down a scheme in order to justify this equation, the importance of this work for us today is that it really solved the number theoretic issues that are rather deep that will come up here in the irrational case. In the irrational case. In the rational case, there are some number theoretic issues, but they turn out to be considerably simpler. So we will rely very much on this work in terms of resolving the number theoretic problems that come up. Alright, so what we do is, so today I will be talking about a joint work with you, Dang. So of course you directly notice that the challenge now is to challenge The challenge is to extend this result or to justify this approximation star for times that are as close as possible to the kinetic task. Okay? Okay, and this is the subject of today's talk. So, what you are able, roughly speaking, what you are able to do with Vidang is we are able able to do with Udain is we are able to get optimal results up to epsilon loss for this question and we are able to consider the rational and irrational torus uniformly. Okay, so let I will make everything here precise. So let d greater than or equal to 0, to 2. So of course the d equals 1 case is a very particular case which we exclude from our analysis. And Taurus is rational. Either torus is rational or irrational. Okay? So we take the initial data for the kinetic equation for simplicity. Let's just take it to be a Schwarz function. Of course, you don't need that. A finite amount of smoothness and decay is enough. And u initial, we take it to be well prepared as we did over. As we did over there. So let epsilon greater than zero be arbitrarily small. So it will denote an arbitrary smart constant. And what else? And again, remind me that alpha remind me that alpha is equal to lambda squared l to the minus t. Okay, then there are two parts for this result. Now remember when we talk about the derivation, the rigorous derivation of the Boltzmann equation, this was only done in the so-called Boltzmann-Grad limit, where the number of particle n goes to infinity and the radius goes to zero in such a way such that n times r to the d minus one remain order of one. Now it turns out, appears that there's something similar. Out appears that there's something similar happening here, but interestingly enough, there are two favorable scaling lows. So, two favorable scaling lows. So, if alpha goes to zero, like al to the minus epsilon, or al to the minus one, or L to the minus 1 minus epsilon, then basically we can justify star up to L to the minus epsilon for any epsilon times the kinetic time scale. Okay? Then star, then let's just say then the expected value Johan Tk squared is equal to N T over T kinetic. n of t over t kinetic, k plus an acceptable error of that looks like l to the minus theta t over capital over t kinetic. Okay, and this holds for so l to the zero plus less than or equal to t less than l to the minus epsilon times the kinetic time scale. And the kinetic time scale was one over alpha squared. Okay, so this is the first part of our talk. The second part deals with what happens for the other scaling laws. So for other scaling laws, we basically justify, we have a similar result. For T less than T star, where T star, the best way to tell you what T star is is by drawing the graph. So this is the log plot of, this is log L of alpha inverse, and this is And this is log L of T star. Okay? And then, so this is the kinetic target scale corresponds to t equals alpha to the minus 2. Okay, so this is what you want to approach. Okay, and then this would be, for example, one parenthesis two. And this is two it looks something like this. So this is the regime of our theory. So T star less than is bounded by this line, if you like. And those correspond to the two points where we touch the kinetic time scale over here. These are the two points. Time scale over here. These are the two points that are mentioned in part one. Okay, so you really, the aim is, as we said, this is to get as close as possible to the kinetic time scale. I will explain hopefully that this corresponds to the probabilistically critical problem. And this is the sub-critical regime from a probabilistic point of view. And then we're able to touch the critical regime at two points. Well, basically, up to epsilon. Almost touch it at two points. It at two points. One thing that I should also qualify is that this corresponds to the range of the any torus. And if you want to access this regime, you want to, this is for only generically irrational torus. Of course there are countering Of course, there are counterexamples if you go above. So this corresponds to t equals l squared, and another limit here is t equals l to the d. Okay, so you cannot go farther than t equals l squared on a rational torus, for example, and there's a counterexample. This is a sharp threshold. And you can also not go for t bigger than l to the d on a generically irrational torus, and this is also a sharp threshold. Okay, and all the essence that I'm mentioning over here are sharp up to absommer losses. Up to absomo loses. I should mention short around the time where this was announced, there was a work of Charles Collot and Pierre Germain where they consider, which corresponds, they consider the rational torus, and the result corresponds to this region over here. Okay, so this is collogue. What should I say? Okay, so another more, one more thing is that what happens in this region, we have counter-explored. Region, we have counterexamples for the convergence about what happens in this region. So, in some sense, this and let me explain what I mean by that. So, in complementary regions, we have what I would call absolute divergence of the expansion. Of the expansion. You will see that the proof is based on an expansion of the solution of analysis in terms of trees. And basically, the blue region over here corresponds to a region where, in a way, this series converges absolutely. This region, we provide a counterexample that shows that the series diverges absolutely in this region. So if one wants to extend this result, part A. This result part A to other scaling laws that are not govern not those two things, then completely new ideas need to be involved. In particular, something related to conditional conversions as opposed to absolute conversions. Okay, so let me maybe, once I go through the proof, you will understand it will become more clear what I mean by those diversions. Any questions for now? Any questions for now? Okay, so here's So the proof is basically three main components. The first component is a long-time existence for the solution. But it's not just one thing. But it's not just long time existence. So long time exists a solution as a, if you like, a power series expansion. So we write the solution as u0 plus u1 plus u n. So these two will correspond to the nth the Hamel iterates. And I will give you more information how they look in a minute. Plus the remainder are n plus 1. So really this is this part is the most challenging part in this work. Challenging part in this work because what you want is to provide plus sharp estimates. We obtain sharp estimates on the u n's and on the r i plus one, as we would use that. The second component is expanding the expected value of u hat of k squared. So this gives you an expression for u hat of k. gives you an expression for your height of k, you expand it and you derive out of it leading order terms. And this is something that I will not discuss today. It's something that very similar to what we did with Buckmaster, Germain, and Chateau. And the third part is a number theoretical part, theoretic component, which the idea is to prove that the leading Because the idea is to prove that the leading terms converge to k of phi, which appears over here, as L goes to infinity. Okay? So in the remaining time, what I will do is that I will give you an idea of the proof of this part and a little bit, very little about the third part. So for this expansion, well, we know that if you write the integral equation, so when n is 0, this expansion is just the integral equation that we're all used to, or the Haman's formulation. u initial minus i integral from 0 to t e to the i u minus s plus u square u d s. The s. And then when n is 0, this will be u0, and this would be r1. But what we can keep on repeating, and if we do that, we get e to the i t to the Laplacian u initial minus i, the integral from 0 to t, e to the i t minus s to Laplacian u initial squared, u initial u s, plus r2. And in this case, this would be u1. This would be u1, this would be u0. So in Fourier space, here I would have u hat at tk. This is something that would be represented by a dot, by a vertex, which has k identified to it. This would be represented by the following three, where you have k here, you have k1, k2, the 3 over here where k is equal to k1 minus k2. equal to k1 minus k2 plus k3. And in general, u n, or the Fourier transform of u n will be represented by terminal trees of scale n, where the scale is the number of parent modes, of parent vertices. So let me draw you an example. An example and create this now. I'll have something like this, something like this, something like this, and maybe let's just stick with those two. Okay? So this would be, okay? So this is a Okay, so this is a root node, this is a parent node, this is a parent node, so here i is equal to 3. And to each of those vertices, there's associated a frequency k. So this would be k1 up to k2n plus 1, where n is the scale of the chain. Okay? So in particular, in Fourier space, if I look at ak of t, so this is something that's intimately related to the Fourier transform. If you relate it to the Fourier transform, think of it as the analyzed solution. It is the sum over trees of scale less than or equal to n. If I denote the term associated with this j sub t, j sub t plus r n plus 1. Now the issue here is two points, two problems. The first problem is to obtain estimates To obtain estimates on J sub T. Okay, we obtain those estimates in XSP and prove their sharpness. And the second important thing is to obtain estimates on R. And both of those problems turn out to be quite interesting. Okay, so let me tell you a little bit. I won't of course go into the technical Course, go into the technical details of XSP, etc., but I will tell you what is the probabilistic and combinatorial component of that needs to be added in order to obtain those as that. Okay? All right, so for A so the sum in A, So the sum in A, or each of those three terms, looks something like this in caricature. You are summing, this is a sum over K1 up to K2m plus 1, okay, in some set S. Okay, so this is a caricature of the problem. And then you have Gaussians GK1 plus minus up to GK2n plus 1 plus or plus or minus. Or plus or minus denotes whether you are taking complex conjugates or not. Now, using large deviation estimates, those estimates tell you that this sum sigma can be bounded by the size of s to the one half. In other words, it tells you that you have a sort of a center limit type scaling as opposed to counting the number of elements in s. And this is a big gain. This is a big gain that you really need in order to get the time scale. Now, there is a caveat to this. Now there is a caveat to this, that you get this estimate provided that all the k1s, up to 2, k2, and plus 1s are distinct. And if they're not distinct, what we have to do is we have to introduce a pairing structure onto this ternary teeth. So I'll erase this, allow me. I'll erase this, allow me, because I want to draw this. And we denote this sparing structure by adding an arrow that could be over here. This is plus, minus, plus, minus, plus, yes. Okay, for example, there could be an arrow from here to here. There could be an arrow from here to here as well. Okay? And then we would call those two vertices as pert vertices, and those two vertices as pert vertices. Now, if you have Now, if you have such a pairing in a tree like this, then the estimate of sigma has to be corrected as follows. It is less than the sum over the unpaired vertices, unpaired kj, of the sum over paired kj one squared. This is something that makes sense, because if you have pairings, then you have killed all the Then you have killed all the cancellation that is possible that could possibly come out of the Gaussians. Okay? And this we estimate it as simply as the sum over all kj one times the supremum over the sum over unpaired kj of the sum over third kj. This is the product of two things. So in other words, the XSB estimate on UN can be reduced into a counting estimates on such ternary trees and how to assign frequencies to the vertices of these ternary trees. And in order to do that, this can be again rephrased by a combinatorial problem as follows. So what you do is that you look at the unpaired vertices. Look at the unpaired vertices. So, this vertex is unpaired, this vertex is unpaired, and you allow a possible coloring to those unpaired vertices. Maybe some of them are colored, and you also allow the root node to be colored. And the estimate on those, on the JT, reduces ultimately into an estimate on how much you pay in order to start from this coloring and color all the vertices of the chips. So, if you have this vertex, So if you have this vertex, you are allowed to color its siblings and its parents, but you pay for that. What you pay for is the number of solutions of k equals k1 minus k2 plus k3, k squared equals k1 minus k2 plus k3 squared is equal to 0. You pay for it by counting the number of lattice points, solving some algebraic equation. But so this is what it reduces to, and it yields the sharp estimates on G. Okay? Unfortunately, I cannot go more in detail to it in such a way. Now, let me talk a little bit about the estimate on the remainder. So, the equation satisfied by the remainder looks like satisfies an equation I dtu plus equation I dtu plus La plus U is equal to a linear part, so this is R, sorry. Linear part plus a quadratic part plus a cubic part. So those two parts are turned out to be harmless because you can bound them through the bootstrap, and this is really the main danger, bounding the contribution of this linear part. Now this linear part Now this linear part is essentially a, in Fourier space, it's essentially a random matrix multiplied by some pi a k, or a k, or let's call it rk. rk corresponds to the few points. So this is a large random matrix, where the entries of this random matrix are linear combinations of products of Gaussians. And the way we bound this operator on XSP is Is we use it basically a high-order version of the TT star method. So basically what we use is that if you want to bound the, for example, the Hilbert-Schmidt norm of a large matrix, you do this by transpose m raised to the power d. You compute the size of this and you divide by 1 over 2d. And this allows you to take advantage of This allows you to take advantage of all the cancellations that come from the Gaussian entries of this matrix. So this is not a usual random matrix because the entries are not actually Gaussian, they are actually linear combination of Gaussians and they could be trees themselves. So the advantage of this approach is that, okay, so this high-order TT star allows you to take advantage of all the possible cancellations, again up to absolute loss. Another advantage is that it allows you Another advantage is that it allows you to accept losses like L squared or L3, because everything is raised to the power 1 over 2d, and d will be taken very, very large at the end. Now, of course, the disadvantage is that you have replaced a big matrix by something that is much, much bigger matrix. But it turns out that the entries of this matrix can be put into the language of those trees and can be bounded in using the same combinatorial lemma, and it would lead good. Lemma and it would lead good estimates for this random matrix. In particular, it allows you to deal with this linear part, which is the most dangerous part. Okay? So this is the first concerns the first component of the proof, which was the long-time existence of the expansion. Okay? And really this part is what determines your time scales and how large how long you can go at the time scales. So in the remaining few minutes, how much time do I have? Um How much time do I have? Five minutes. Okay, excellent. So, in the remaining five minutes, let me tell you a little bit about why there is an important number theoretic component in this problem. So, turns out that in caricature again The leading order terms for this expansion, remember the second step was the computation of the expected value of u hat of k squared, which you now have in somewhat explicit expression, what u hat of k is, what I erased over here. And so you use that in order to compute the expected value of u hat of k squared. And in caricature, this looks like the following. This is a sum of The following. This is a sum over k in some z to the n, some integers, w of k over l, g of mu omega of k, where omega is a quadratic form, omega of xy is the sum from j equals 1 to d, for example, dj xj, yj. So for example, So for example, n would be 2d, for example, n would be 2d. And what you would like to do, you'd like to say that this thing over here, as L goes to infinity, this is something that could be approximated by L to the N or L to the 2d times the integral of W of Z g of mu omega of Z dz. This is just Riemann summation. You want to justify Riemann summation for Justify Riemann summation for this sum. But the first remark that I want to do is that this is not just a trivial Riemann summation, and this depends whether mu is large or not. If mu is less than l to the minus 2, this is a trivial thing. This is exactly Riemann's summation. But once mu steps a little bit over L to the minus 2, then you enter into the realm of Then you enter into the realm of analytic number theory. And let me tell you why. So if you take w to be the characteristic function of the unit whole of size 1, say, and g to be the characteristic function of the integral minus 1, 1, then this thing over here is nothing but the number of lattice points, k and z to the n, such that k is less than. such that k is less than l or less less than l and omega is less than a mu inverse. Now the scale of omega, since omega is a quadratic form, is L squared. So if you are restricting it to a mu that is less than, so you are restricting it to mu inverse, which is strictly less than L squared, then this is something non-trivial. This is something non-trivial. Actually, in the irrational case, mu is equal to one. This is called the quantitative Oppenheim conjecture, which is not a conjecture. It was of Pennheim. This was something that was proved by Marbulis Icekel, Margulis and Schaef. I skimmed who is Ansha, and Moses, sorry, in the early 90s, if I'm not mistaken. Okay, so this is using homogenous dynamics, so this is already non-trivial. And mu equals 1 corresponds to reaching time scales t equals l squared for our problem. So if we want to reach higher or bigger time scales in the irrational setting, we need to do something that is much finer than the To do something that is much finer than the quantitative Oppenheim conjecture. In the rational case, this is related to the wearing problem, which is counting basically such lattice points. And for this, the standard tool for such thing is the Hardy-Luther-Circle method. So the results that relate to this problem are the following. So in the rational case, this is the problem. In the rational case, this is what we treated in our most recent paper of New Testament. We justify this approximation up to mu less than L to the minus delta for any delta greater than zero. And it's actually false when delta is equal to zero. So this is, in a way, sharp. Sharp. It doesn't hold at that endpoint. So, in the irrational case, this is we use as a black box the result that we had with PacMaster, Germa, and Cheta. And in that case, you can do much better. You can justify this approximation up to mu less than Um L to the minus uh sorry mu what I want to say uh yes, mu plus plus all right and the the way to do that is we relied so uh turns out that the circle method does not give you very good results in this case, but luckily for us there was a paper of Burkin from 2016 which deals with a similar problem. 16, which deals with a similar problem, and we're able to adapt the techniques there into this setting. But both those results are essentially sharp up to epsilon losses. So in some sense, this resolves all sorts of number-theoretic problems that will arise in such questions for analysts. Okay? I'll stop here. Questions? So what all do you know about the actual kinetic equation dynamics? The solutions? Yeah, it has been studied by several authors. The most important dynamics is the formation of a condensate from this equation. That in finite time you can start with Schwarz data and you can form a condensate at zero and this is At zero, and this is used in order to dynamically justify the formation of a condensate for analysis. And also, but mostly physicists use the stationary solutions of this equation, because the stationary solutions equation, they are the Kolmogorov spectra in this setting. Okay, and there's a lot of scientific emphasis on that. But uh none of this relates uh can reach such types of solutions. Reach such types of solutions. Okay, thanks so much.  Yes, there is not enough free. Okay. But then some interventions. Like there's something different from the microphone. Things that are troubling to start for us. Yeah, yeah, yeah. What's your business? You're out of the light and working like I was feeling pretty tired. I was gonna sketch my family. I was going to skype my family and then in a way also the fact also even locally after that. So what's that roughly what you expect? Go earlyish, 530 or 60. Anyway, so I because it's good to tell you what? Well, there is a version of has it been discouraged. It's just that we assume here there's a homogeneous assumption of there are some decent places to eat that we could eat. Okay. Wave wave uh stuffing recording was discussing.