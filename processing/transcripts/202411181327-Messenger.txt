has a long history in the Igwin ran the cosmology group for a minute, did all about science. Was it CBC group? It was actually just generally CBC chart. Okay, all right, whatever. Yes. Anyways, you're going to talk about the new pipeline that this presentation was put together, so it's great. Thank you very much. Right. Yes, so thank you to the organizers for letting me speak. Specifically, on behalf of this guy who did not really want On behalf of this guy who did not really want me to put his picture up. This is my PhD student, Narin, and this is all of his work. I'm sure he's listening right now. And hopefully, if there are any technical questions, he won't be able to answer them, as opposed to what you want, but I will be able to answer them. So, this is matching match filtering with machine learning. And Mervyn pointed out to me the other day, isn't that the title of one of your old papers? And that's true. This paper, which was 2018, which is when we started. Which was 2018, which is when we started trying to do machine learning for finding CBC signals in noise. It was 2018, it was the early days, it was the wild west of gravitational wave machine learning. Such low-hanging fruit, it was very easy to get a PRL back then. But we have come a long way from there. This is Hunter, Michael, and Fergus that were working on that. Naurin's work is the same idea to do the same search, but it's using completely far more advanced machines. The far more advanced machine learning technology than we had back then. So, what I'm going to talk about specifically is this idea of trying to do binary classification. Basically, is there a signal or is there not, a CBC signal in a piece of gravitational wave time series. But the focus that Narin has put on his work is to do with biases in machine learning that is well known in the machine learning world. And in the machine learning world, and we had no idea about it until we narrow and delved into this, to find out why exactly we're not really at optimality at the moment. And in doing so, we have developed yet another machine learning pipeline, which is called SAGE. It doesn't stand for anything, it's SAGE, which does these searches. And I'm going to talk about results of a study on a standardized piece of data, a data set from the mock data. Piece of data, data set from the mock data challenge, from the, I never remember what it's called, machine learning gravitational waves science data challenge, one from Schaefer. I'm going to compare with my CBC and Ares, GW, look at some of the results and then I'll summarize. So just to clarify, it's a very basic situation. We have a model which is somewhere over here, but that Is somewhere over here, but the bit I'm going to talk about mostly is about data generation and how we train our models. So we're generating binary black hole gravitational wave signals from a distribution of parameters, the gravitational wave parameters. And we're also associating that with real detector noise. And we are either adding some real detector noise, but we're always adding real detector noise, or we're not adding a signal. And so we have a signal class and a noise class, and then we have a classifier that we're going to train to detect. Classifier that we're going to train to tell us whether there's a signal there or not. Basic stuff. However, the motivation for this, because if you thought there were already too many such pipelines out there trying to do this, is that from the Schaefer et al. paper that was reporting the mock data challenge results from a handful, five or six or so people that tried to do this, or teams of people who tried to do this, coupled with some follow-ups. Do this. Coupled with some follow-up papers from Colin Yari with this Michael Ares GW coming out of Greece and Ethan Marks with A-Frame and others coming out of MIT and Minnesota, I think. Or is it Minneapolis? In all of these cases, biases were reported in the output results. In terms of Results in terms of not necessarily detecting, in this case, a sort of reduced bias, reduced number of detections in the tails of chert mass distributions, reported bias in, again, lower chert mass and high mass ratio signals, and similarly for the original Mockday's challenge paper. So, this was sort of what we latched onto, what Narin latched onto. And so, what I'm going to talk about in the next two slides is a sort of overview of some of the possible what you Some of the possible known biases that exist in machine learning when you're training models and try and associate them with some aspects of the gravitational wave problem. So, first of all, these are all three of these training biases associated with how you generate your training data. First one is called bias due to limited sample representation. An imbalance in the sample distribution for a certain gravitational wave parameter might introduce biases. Biases. So this is just about: have you got enough, are you distributing your signal waveforms in the right places in the parameter space? Have you got them in... Now I've got to tune my words carefully because you don't want them to be uniformly distributed. That's the point. They have to be correctly distributed for the machine to learn optimally. There's a very similar one, biased due to limited feature representation, which is sort of an abstraction. Representation, which is sort of an abstraction of the gravitational wave parameters. It's what the machine is really seeing in the data, what features in the data it's seeing that it's latching onto to help it learn about the question you're asking, whether there's a signal better or not. And there's a bias due to the same thing, limited feature representation in the data. So if you've not generated your data with the appropriate number of samples in that feature space, then it will have trouble. There's a thing called bias due to insufficient information. Again, and this is. Again, and this is due to sort of experimental error in the sense of your pre-processing of your data not including all the relevant information that the machine requires to make the decision. So you could downsample your data too much and then lose high-frequency information which you needed to do the analysis. Things like that. And on the right-hand side is just a diagram to just show naively thinking that you're Naively thinking that if you uniformly distribute things over the M1, M2 space, you are obviously not uniformly distributing it over a different representation of the space, which might be represented by the time to coalescence. And so the time to coalescence is constant times of coalescence along constant light of shirt mass here. So it's just your choice of what seems to be a sensible uniform uninformed prior is not necessarily the case, depending on what parameterization you think you have. Bias due to lack of variation. So, this is just: do you have enough data in general? Is the machine able to interpolate between your training data points? Because it's going to see data that doesn't lie on your training points. It's going to be your general ground point somewhere. So, have you got enough there? Have you got enough variation in both your signal and your noise characteristics in your training? Bias just to the fact that some areas of parameter space are just more difficult to model. The signals that you're trying to find are. The signals that he's trying to find are more complex or lower amplitude, got more structure. And so there's a bias due to that. Similar thing, spectral bias, machines tend to latch on to low frequency features in your data, low frequency variations before they latch onto high frequency variations. And this is just a plot showing that, as a function of training iteration, the frequency content of a particular test piece of data from this paper shows fit on a measure of zero to one. On a measure of zero to one, where zero is bad and one is good learning. It learns quite quickly the low frequency features, and it takes a long time to get even anywhere near good at the high frequency features. And you can see how that might relate to low mass and high mass binary black holes, because low mass have got low frequency, and no, low mass have got high frequency components compared to high mass having lower frequency components. And I will not go into these. And I will not go into these, but there is just countless more biases documented in the literature. And they all kind of overlap with each other as well. So it's not once you've fixed one, but once you've fixed one, you've probably affected another one as well. So this comes on to Sage, and Narin won't be pleased. I've got his picture up again, but I really want to stress this is all his work. Taking all those biases into account and what we knew about binary black hole and machine learning applications for finding signals, Narin has broken it down into. Narin has broken it down into six big sort of steps that he's put together to optimize his network. One is increased variance provided for signal. So he's able to generate data on the fly. So it's none of this generating vast amounts, gigabytes, hundreds of gigabytes of data, and then that's your training set. It's generating the data as it trains, and it's generating it quicker than it trains, so it's not slowing it down. And so we're not limited by the amount of data. If we just kept training, it's seeing more and more. If we just kept training, it's seeing more and more data. Number two, reduce the need for larger receptive fields. So, this is a technical aspect of machine learning and how it can relate disparate parts of your piece of data you're giving to it. And again, through physics-informed sort of what we understand about a chirping signal, we've got a relatively simple compression scheme, a multi-rate sampling compression scheme that makes the data a lot easier to handle, and then we don't need very large flat maps. Then we don't need very massive receptive fields in our network. Increased variance provided for the noise classifier augmentation. So, making use of the fact that we have noise data from gravitational wave measurements or gravitational wave observations already, and the fact that we can augment that in a very similar-ish vein to time slides, where we can associate different pieces of noise from different times with different detectors, and therefore have many, many, many noise realizations to try and trick the network. To try and trick the network into learning the true noise distribution. Complex enough waveform approximant and sufficient sample length. So, this is to do with, what's this to do with? I'll have to have Narrow and tell you that one. Yeah, that's one that's gone from my mind. I'll move on to number five. Fine-tuned training strategies to handle biases due to limited samples. So there are the network intrinsic. The network in training that they seem sometimes need hand-holding. And so, if I get to it, there's a slide that shows how halfway through training you have to kind of change how it's training to trick it into learning something first and then learning something later. Because of the fact that noise is non-Gaussian, it turns out not to work. And the final one, proper inductive bias, used fancy words for design your network well for the problem at hand. For the problem at hand. So, this is a well-designed two multi-stage network to pre-process the data and to do the actual inference, the classification of the end. The study was all, we tried to, we really desperately want to compare apples to apples. And so we wanted to stick in line with this mock data challenge paper where there was a nice data set and multiple people. Data set and multiple people had already had a look at it. And so it was sort of it's a nice, nice thing, a standardized test. And it's one month, the test data is one month of O3A noise from two-sex network H1L1. It's using IMR phenomen XPHM, mass prize 50 to 70, spins isotropic distributed, 0 to 0.99, and all the other gravitational wave parameters from what you would necessarily expect. Parameters from what you would necessarily expect from the standard distributions. The data that the challenge participants were given is the same, almost the same as we viewed. The testing data is 30 days, one month at the very beginning of O3A. Then there's five days of validation data, and then there is 46 days left for training your network on the noise distributions of O3A. And because of one of the things, one of those biases that NARI discovered. That Narrow discovered, having access to more noise, and not just more, but more noise with more variation in the non-Gaussianities was crucial to getting rid of some of the biases and improving the sensitivity. So we used about 140 days of O3B also for training. And this is where I wanted to get to with just about enough time, maybe, the results, the meat of it. So, Sage versus. So, Sage versus PyCBC, sort of the standard non-machine learning benchmark algorithm. What we're seeing on this plot over here is a corner plot type thing, but each of these little blocks are little hexagons, they're little histograms counting the difference between the number of signals detected by Sage compared to PiCBC. So, when it's red, more things are detected by Sage in that pixel, and when it's blue, more things are detected. And when it's blue, more things are detected by PiCBC. This is all a false alarm rate of one per month, which is the lowest we could go with the one month of testing data they provided us. And what is nice, and I'll be provocative, annoy anybody who's a PiCBC fan, it's more red than blue. We detect more signals. We detect, what's it, 450? What is it, 443 more signals in total? We detect 3,344 signals commonly with PiCBC. PiCBC does find 513 signals that we do not find, but we find 946 that they find. And it's quite nice. You may say, all this talk about bias, and look, you're still biased, but this is significantly better than it used to be. There is still residual bias against lower mass, lower chirp mass. Mass, lower church mass events. They are harder to find. But the majority of the ones that PiCBC are finding that we're not are at this low end. Yeah. Next plot is a comparison with Aries GW, one of the other two machine learning algorithms that's out there doing this kind of thing. And in this case, is there any Aries GW piece of here? Recorded by the. Recorded, by the way. You are recorded? No. We're very friendly with them. We are emailing them at the moment. This is again now nearly almost red, and we're detecting 48% more signals than RHW in this particular case, at this false alarm rate of one per month. And the bias is a little bit more exaggerated here as well. Sure. Which is a known known thing. Which is a known thing in the RHGW analysis. One of the figures of merit that was used in the original Mockbase Challenge paper is this thing called sensitive distance, and I'm not a fan of sensitive distance, and I'll explain why. It's worked out basically, well, it's using this expression here, where you essentially count the fraction of events that you've detected at a particular false alarm rate. Particular false alarm rate, and then you weight each of those detections by the chirp mass of that detection to the five-halves. So, what you end up with is this overall fraction of a volume that you would have been sensed that included all the signals. And if you don't quite find all the signals, it's a fraction of that volume. And then it's converted into an actual distance by taking the cube root and some factors. I've got four or pi in there. And it's supposed to be representative of a distance that you can see. Of a distance that you can see. PiCBC is the benchmark here, the dashed line, and both Air EGW and Sage beat this line. But I've also highlighted this is biased due to disproportionate evaluation. This is one of the biases I flashed by before. Just because of the particular metric here, it is by design disproportionate towards high chirp masses. So, if you're really good at finding high chirp masses at the expense of low chirp masses, even though you will find significantly less signals than another pipeline, Signals in another pipeline, this metric will reward you and you will end up giving a high sensitive distance. But you will not necessarily be detecting as many signals. Hence, we're very close to say areas of areas of unique sage are very, very close here, but there is a bigger difference in the number of signals detecting. And we're doing for time. The chair though. Yes. Mine says three minutes. Mine's a three minutes, so I will crack on. So, what was this one? This is about. So, this is some injection studies about how the network was trained. So, there's this D4 ICBC, so that's the result from data set 4 of the MockPage challenge, the most realistic piece of data. And then Sage was run in two different modes here: broad and limited, and broad in this case is meaning a broader distribution of PSD. A broader distribution of PSDs, noise PSDs, it was trained on. And you can see how, and limited as a smaller range of PSDs. And you can see in this particular case, that had a major effect in both the sensitivity and the bias as well here. So having it see lots more versions of PSDs, not necessarily ones that are in the testing data, just seeing a broader distribution and getting to learn that distribution helped it a lot. And then over John. And then over, jumping over to the right-hand side, this is data set three for PyCBC. This is a purely Gaussian noise data set, not real data. And what we used here was it's called the D3 metric, which is where we distributed our mass training samples according to the metric that you would use in Python CBC and template placement. So far more densely populated towards lower masses or lower church masses. And in this case, in the Gaussian noise case, we're very, very close to Python. Case, we're very, very close to ICBC overall with very little, not really noticeable bias here in that case. And we thought we're onto something. We'll just run it on the D4 data, and you get the green curve. It just didn't like the metric placement of our training data in mass for the data that the only difference was it was slightly non-Gaussian data. It had glitches and like it. So Narin came up with a strategy to trick it, as I was mentioning before, and train it. As I was mentioning before, and train it just on our basic uniform distribution on M1, M2 initially, and then halfway through training, switch the distribution of the training data over to the metric. So have it learn how to deal with the non-Gaussianities, and then trick it into learning about where to optimally place things to not be biased. And that pushes us up into the red distribution in comparison to four. So that was solving, going a long way to solving the bias that we're seeing in church. Then I'm not going to show you all the ablation studies. There's five ablation studies where you take this is the same results I showed before on the corner plot comparing Pi CBC and SAGE. And this is a relationship to be one where we just use less training data. We just said don't do training on the fly, just use two million pieces of training data and don't do as much of this augmentation with extra noise. And it made it worse. And it made it worse, which was nice to see. We needed that bit. So, this is sort of an example of we do need to have lots more, really pushing it to have lots of signal realizations and lots of different noise realizations. And then, this is evaluation study two and three. That's my timer. So, I'll finish this slide and then go to the summary. So, this is where we took two networks instead of the one with the specific. Instead of the one that was specifically bespoke design for this problem, we took simpler 1D ResNet networks, but one that had a considerably higher number of trainable parameters, 36 million, compared to our narrow, we ought to give you the number, but it's a lot less than 36 million. And we also did one where it was a simpler network, it had a similar amount of trainable parameters. And you can see that in both cases, we are not doing as well as we used to do, which is the red curve in this. Which is the red, the red curve in this block. So that was again taking away all the complexity we put in our network and real and proving to ourselves it was worth putting that complexity in the network. Oh, and that is, that's my summary slide. So just to summarise, Sage exists. Methods paper will be circulated next week if you're in the collaboration, or the week after on Archive, if you're not. Hopefully we would like to perform this at a real search on O three and see if our claims are correct, that we do. See if our claims are correct, that we do find 11% more signals. I should say that that is all of our statements are within the scope of this mock data challenge, which was limited to a false alarm rate of one per month. We do not know whether this will all go downhill if we start pushing to lower and lower false alarm rates. I'm optimistic. But that's one of the things to check. And just careful design of the training set, the network architecture, and the strategy for training. And the strategy for training has alleviated a lot of the bias that's been seen in the past. But there are obviously still improvements to be made with regards to bias. It's still a little bit there. The longer, lower mass signals are harder to detect. And also always trying to push up the sensitivity. So I'll stop there. Thanks very much. Who chair? Oh, our chair is up. Michael? Yeah, well, Michael, where is that? Oh, okay, I can't share. There's also a question online. So, and I don't know who raised big hands first. Okay, okay, great. So, really excited to see this. As heard the future search you have to run on Geo Data. I'm wondering what the results look like, second event. Like, do you report false alarm rates? False alarm rates? Is that the key measure for detection? And how do you is that an assumed or any output? Yes, so that we are at the moment we're limited to the strategy, which we only realise a bit later in the game. We analyze the one month of background data that that challenge gives you, and that allows you to set false alarm rate at one per month. We have outlined in the paper draft three strategies of which will allow us to go further than that, including stealing from the standard techniques of doing. From the standard techniques of doing a single detector analyses and doing time slides on that. Okay, but the roster is not necessarily like the output of the search is not a false alarm rate, it's not a detection statistic. No, it's a detection statistic, and then you just go from there. Yeah, okay. Awesome. Let's go with cost of online. So then we can like cost of young answers your question. Please, can you hear me? Yes, yeah, we can hear you. Yeah, I have one question. So, when you were training for model, you were just trading on binary platform signals, and you were also trading on any kind of niches or something. The reason I'm asking you is that I see that the call makes most of the comparison that you get with PyCdc and your pipeline or your algorithm actually is comparable and like order of anything makes sense. So, I was asking that particular respect. As being effective in this job. So again, Narin can maybe jump in, but the data does contain glitches. And Narin dug into all the gravity spy glitches that were present in that data set so that he could make plots like this to look at the significance of our detection statistics as a function of glitch SNR as reported by Gravity Spy. And in this particular case, we're comparing against high CBC SAGE results. Sage results compared to PiCBC for those particular glitches. And it seems we are quite robust, or as robust at least as PiCBC for glitches. It's as good at rejecting them. I don't know if that was quite the question, as in there are glitches in this data, is the basic answer. Got it. Thank you. And can people hear me? Sorry? Yep. Yeah, and those are not the only secrets that were present. There were about 30, 40,000. Present, there were about 30, 40,000 more triggers, but they were present for false land rates that were below that. So we have false numbers, like 100,000 more which I can plot. So this is only from thousands of error. Okay, so there is the photographer outside. I know that you have all a lot of questions, but he is around the whole week. So if we can go and get the photo, then we can. Macro is headside outside. It's outside. Well, no, I mean, you're going to have to go back. I'll show up, get free lunch, get a photo, go back.