So it's a pleasure. We'll have two lectures this afternoon, and it's a pleasure to start with our first speaker, Andrea Namot from University of Massachusetts. And she's going to talk at Amherst, and she's going to talk about propagation of randomness, Gibbs measures, and random tensors theory for NLS. Great. Let me start by thanking How and Jacob, Alex and Sasha for the invitation. Sasha for the invitation. And as you see, there is no fluid equations in what I'm going to talk about, but I guess that there is the ideas and there is long time dynamics. And the ideas in here are some ideas that can be used also in some fluid equations. So let me start by actually setting up a little bit the notation that everybody knows in this audience. Everybody knows in this audience. So, we are going to be talking about the periodic NLS. P here is going to be odd, and the sign is only going to matter whether when we talk about long-time dynamics or Gibbs measures, and in which case I'm always going to be referring to the defocusing case. And this is the Hamiltonian, and we have mass conservation. And just let's recall that the scaling, this equation has a scaling, which is the dimension over two minus two over p minus one. 2 minus 2 over p minus 1. So the basic local in-time theorem will positness for this equation is that if you take this number to be bigger or equal than zero, then you have local in-time solutions provided that the data is in HS with S strictly bigger than this critical number. So you may wonder why, I mean, because if you take S critical to be zero, then the equation is at the L2 critical level. And so what happens, for example? And so, what happens, for example, in the cubic NLS on the two-dimensional torrent? I mean, when the critical index is the same as L2, and actually, surprisingly, this is an open problem to this date. In other words, there is not nobody knows if you have a local well-posedness for the cubic NLS on the two-dimensional tori with data NL2. And if S is less than equal than the critical index, this critical deterministic, then there is no that there is. Then there is known that there is ill poseness. So, what we are interested here is, of course, what happens generically, like instead of individual solutions, we are interested in the family of solutions that are distributing according to some canonical law, like the Gaussian law, or it could be also the uniform distribution on the unit circle, something that is rotationally symmetric. And so, we are going to be considering the And so we are going to be considering the NLS with data that looks exactly like the one I have here. Can you see my hand in the cursor here? Okay, so the data is going to be essentially like Fourier coefficients 1 over k to the alpha. Alpha is always going to be taken like s plus d over 2. And s actually is the regularity. So s is going to be linked to the regularity of this function. I'll tell you in a minute. And so, and these are the random variables. And these are the random variables, so we are going to inject randomness into the problem through the data. In one could inject randomness through the noise, but here we inject randomness through the data. And these, as I said, are sort of like complex Gaussians, center, you know, sort of like normalized. But you could take also what is called the random phase, which is used in the wave turbulence, which is the uniform distribution and the unit circle. And so formally, the Gaussian measure is given by the exponential of the H alpha norm squared times the Lebesgue measure. And what I wrote here is totally nonsense because this doesn't make sense. It only makes sense as the weak limits of finite dimensional approximations. And what is known is that if you want this Gaussian measure to be countably additive, there is a trace theorem that tells you that then this measure should be supported in the Soviet space, which is. In the sobole space, which is just below hs. So, in other words, the support of this measure belongs to any hs minus epsilon, but with probability zero belongs to hs. So, what that means is that almost surely, in omega, this function f belongs to hs, where s, if you solve from here, is simply alpha, which is the power here in the Fourier coefficient, minus d over 2. Okay, so now that we have this. Okay, so now that we have this set up, so there has been, sorry, so by switching to this unmosure point of view, the point is that one can cross the scaling by the deterministic scaling barrier and get almost sure local responses for values which are below this critical index. So historically, people only know of a weak solution in this supercritical case, and there is no uniqueness. I mean, there is no uniqueness results that are known, very little results are known. Are known, very little results are known. But in 1996, Bourguen in Grand Work, in sort of like a seminal paper, showed that if you consider canonical random data in some deterministically supercritical space as less than the critical index, then almost sure in the probability sense, you can get strong solution. You can get uniqueness. And so, what's the key point? The key point is that if you start with random data, then that leads to better linear and non-linear estimates thanks to sort of large. estimates thanks to sort of large deviation estimates and other types of, I'm going to refer to this again, random matrix estimates. And so there has been a lot of work after Bourguin and it has been all incremental, you know, epsilon improvedness, in improvedness in various equations, but there have been some fundamental questions that remain that we are interested in. In other words, we want to understand, I mean, what is really going on here? I mean, it's like, what is the optimal value? I mean, it's like, what is the optimal value of regularity for the almost sure local well post-ness to hold? I mean, what this is telling you is that the critical scaling is not the right measure, the right lens. And so we are trying to find out which is the right lens to understand the local weld poseness in an optimal sense. And so the other question, which is actually more interesting, is if I give you a random initial You random initial data, then what I would like to know is what happens, how this data gets transported under the NLS flow. So, if it's Gaussian initially, then how is this Gaussianity propagate? And more important, I want to understand the solution at a more granular level. I would like to understand the description of the solution beyond the linear evolution. Okay, and I'll say a bit more about this later. So, such questions are very important. So, such questions are very important in many areas. One is that I'm going to focus on today, which is the invariance of the Gibbs measure, which is sort of something nice that is at the intersection of PDE and statistical physics. And there is some papers that I put here in the footnotes that are very beautiful to read in the context of limits. Oops, sorry, what happened here? I messed up. Of limits of rank canonical thermal. Of limits of rank canonical thermal states in manybody quantum mechanics. And it's also linked to the weak wave turbulence question, which is about the density statistic of interactive waves. And I'll say a little bit about this, but tomorrow Uden is going to tell you much more about this. And so in the context of invariant measures, let's talk about invariant measures. The point of invariant measures is that the statistical example of the support of these gifts measures OCT2NLS, they always have this alpha, which is the part. Always have this alpha, which is the power of the k, to be one. So the typical element in that support looks of this fashion. And if you are in dimension one, this is a function in h half minus. But if you're in dimensions two and above, this is a distribution which gets worse with the dimension. So if you are in 2D, you are just below L2. And if you are in 3D, you are below H minus a half. So you're in H minus 12 minus. And so from the Hamiltonian. And so, from the Hamiltonian structure of NLS, one could formally write the formula for what the Gibbs measure should be, which is essentially the exponential I'm defocusing to the minus Hamiltonian times the Lebesgue measure. And again, this is quotation marks because as written, it doesn't make any sense. And you can actually think of this as being sort of like the exponential of the potential energy times the Gaussian measure with alpha equals to one. With alpha equals to one. So, in other words, you could think of this as being a weighted Gaussian measure. But now, the construction of this measure has been a topic of a lot of research in sort of, for example, statistical mechanics and quantum field theory. And in some cases, this measure can be rigorously constructed as a weighted Gaussian measure. And moreover, some properties under various dynamics. Properties under various dynamics have been understood. So, for example, the construction, I'm only going to focus on the focusing for this talk. In dimensions one and two, it could be done for any p, which is all. And in those cases, you get that the measure is indeed absolutely continuous with respect to the Gaussian measure. But in dimensions three, the construction can only be done for d equals to three. And in that case, the measure that you obtain. And in that case, the measure that you obtain is not absolutely continuous with respect to the Gaussian measure. And this was kind of part of the folklore for a long time, but there has been recent work of Barashkov and Guvinelli who have shed some light and some better understanding of what's going on. And if you're in dimension bigger or equal than four, then the measure cannot be constructed for any p. And this follows from early work of Frohlich, an Einstein, and recent work. Of Frohlich and Einsmann, and recent work, groundbreaking work of Eisman and Dominique. Okay, so how about the invariance of the measure? So, if you remember what I said, it can be written as the exponential of the Hamiltonian, the Hamiltonian is conserved times the Lebesgue measure. So, formally, the conservation of Hamiltonians and Liouville's theorem should tell you that this measure is invariant. But I mean, this is just informally. This is just informally, this is heuristics. So, how can you justify that formally? So, how about the invariance of the NLS under the NLS flow with actually a very strong ingredient, which we want this invariance in the strong sense, in the sense in which we also want to know the existence of global strong solutions on the statistical ensemble of a measure? So, what's the difficulty? The difficulty is that this measure in dimensions bigger equal than two is supported in very Equal than two is supported in very rough, in very rough spaces of distributions, as I told you before. So this invariance actually was justified in dimension one for any p-bigger equal than threefold by Borgheni in a 1994 paper. And in dimensions two, it was only justified for the cubic equation in this seminal paper of Borgheni in 1996. But nobody could do or nobody knew how to do. Nobody could do, or nobody knew how to do, and this was a problem of great interest to Bourguin, and it was open for many, many, many years. What happens in dimensions 2 when P is bigger or equal, say, than 5? And so this was open, as I said, since 1986. And in 2019, we actually solved this open problem together with Yu Deng and Hei Ken Yue, who was a former PhD student of mine, in which we actually proved invaluable. In which we actually proved the invariance of the Guys measure and the almost sure existence of strong global strong solutions to the NLS for any odd power nonlinearity bigger equal than five. And so given what I just told you, the only problem that was left open or that is left open until today, given that there is no measure beyond this, is what happens in dimensional three for the cubic equation. This is open and it's very hard. But one question I want to pose to you. But one question I want to pose to you is: why so hard? I mean, if p is 5 and dimension is 2, this equation has a deterministic scaling of h one half. And if you're cubic in dimension 3, you also scale like h one half. So what happens? Why are these two problems so different? We could solve this, but this problem is still open. So in order to understand that, Understand that, let me give you some answers to all these questions. And the answers go through this notion that we define and we brought to where, which is the notion of a critical index in the probabilistic scaling. In some sense, what we are saying is that the right lens to understand this problem is not the deterministic critical scaling, but what we call the probabilistic scaling, which is simply minus one over p minus one. And I will explain to you in a minute where this comes from. This comes from. So, heuristically, the scaling is based on the idea that from the central limit theorem, you have a square root gain or square root cancellation of sums of independent random variables. And so what is it that, how do we answer these fundamental questions that I've posed before? We can actually really answer them in the sense that we obtain locally in time strong solutions for random data in the full. Full subcritical range SB or equal than this probability scaling in any dimension. And what is more beautiful, we give a precise description of how the solution looks like in terms of multilinear Gaussians for short times. I want to emphasize that in the context of singular stochastic heat equations, the parabolic scaling that was introduced by Martin Here was Introduced by Martin Hare was minus two over p minus one. In that case, you have a lot of smoothing. And the solution, and there was actually they could prove some almost some sort of like local in time well poseness close to that number. But the solution to the full probabilistic subcritical range in that case, in the heat case, was only done very. Case was only done very, very recently. Okay, so this is actually the parallel to that result. And furthermore, as I said before, we can actually prove the invariance of the Gibs measure and we prove the existence of global strong solution, that means unique, the uniqueness is the whole point, in the statistical ensemble of the measure for any odd power. And so, what is the key, what is the reason why we can solve this? It's because the Gibbs measure in two dimensions is Gibbs measure in two dimensions is supported right below L2. But the probabilistic scaling is actually minus one over p minus one, which for any p bigger equals n three is actually below, is below the support of the measure. So that means that this problem is what we call sub-probabilistically sub-critical for NEP. And this is the key why we can solve this problem in dimensions two. In dimensions two. And we also have a result in which we improve Burgund's result for the three-dimensional heart hatchery and ALESIC Geese measure. And I will tell you something at the end about this if I have time. Sorry, some phone is ringing. Okay. If I have time in a minute, later on, at the end of the talk. So let me actually try to explain the probabilistic scaling. Probabilistic scaling. So, where is the probabilistic scaling coming from? Hold on a second. Okay, sorry. So, where is the probabilistic scaling coming from? So, it's just some heuristics, which are actually, you can do the same heuristics to tell me what is the deterministic scaling. You take beta, which is a single frequency, n to the minus alpha, and you just take simply the sum over k of order of size n. Of size n of the random variables that exponential. So if you expect, if you want the NLS to be almost sure local will pause, then the second iteration, which is this object, should be bounded in HS for any fixed time, right? So if you take the Fourier transform of this, what is it that you get? You get n to the minus p alpha times the sum of one over the Japanese bracket of the resonant factor times the Gaussians. Okay, the Gaussians this. The Gaussians. Okay, the Gaussians this f are have these Gaussian variables, and this is complex, so you have bar non-bar from the complex. And so, I'm going to, for simplicity, I'm going to take omega to be zero, so this term becomes one. So what is it that you have? You have simply this expression. And so, if you assume that k1 is not equal to k2 and k2 is not equals to a t, so that you keep the independence between these random variables, okay. Between these random variables, okay? And this is usually guaranteed by some green normalization. Then, this leads to the fact that if you take the absolute value of this, then and you apply some large deviation estimate, then what you get is n to the minus p alpha times the sum of one, which is the counting of this set, which by dimension counting gives you n to the minus p alpha n to the p d minus d minus two divided by two. So if you were deterministic, you would not have this one half k. You would not have this one half k here, you wouldn't have a one-half, you would have a one, you wouldn't have these two. And the point is that then this is bounded in H says if and only if minus p plus p d minus d minus two divided by two plus s plus d over two is negative, which means that s should be bigger than minus one over p minus one. And this is exactly where the probabilistic scaling comes from. In the deterministic case, you don't have these two, and this gives you exactly d over two minus two over t minus one. Minus 2 over p minus 1. So let's actually try to understand a little bit here what is it that I'm saying. So if the dimension is 2 and p equals to 3, the probability scaling is always minus 1 over p minus 1. The sort of what I call the scaling that is associated to the Gibbs measure is always 1 minus d over 2, and the measure is always supported slightly below this regularity. And so when these two and p3 will And so when P2 and P3, Bourguen was exactly in this scenario, in which the scaling, the deterministic scaling is L2, the measure is supported right below, epsilon below L2, but the probabilistic scaling is very far. So this is probabilistic subtrolithical. In the case in which the dimension is 2 and p equals to 5, then the support of the measure is in the same place. You are still probabilistic subcritical because then this number. Because then the Edis number is one quarter, and then the scaling deterministic is one half. So, in order to be able to solve this problem, the whole point was that you cannot, you have no gain of regularity from the Schrodinger semi-group. And so, in order for him to close his estimate, he had to somehow produce some smoothing or some gain of regularity out of nothing. So, he had to go from something that was just below L2. Something that it was just below L2 to something that it was just above L2. So he had to produce some epsilon gain of regularity out of nowhere. In the quintic case, you see, you have, in order to close your estimates, you need to somehow be able to reach something which is above one half. And so you need to produce now not epsilon regularity, but one half plus epsilon regularity out of nowhere. Okay. And if p is larger and you're still in dimensions two, then you get. And you are still in dimensions two, then it gets worse because you have to go from something below two into something which is even bigger than one half. Okay, all the way, this can be as close as you want to one. Now, in all these cases, though, this number, which is the support of the Gibbs measure, is always above the probabilistic scaling. Okay, so these problems, these three problems, they are all probabilistically subcritic. While if you go into dimensions. Well, if you go into dimensions 3 and p equals to 2, these two numbers flipped in the sense that now the probabilistic scaling becomes minus one half, the Gaussian scaling becomes minus one half because this is three, so this is also my, these two numbers coincide, and the support of the measure is here, and so that means that this problem becomes what we call probabilistically critical, and although this problem And although this problem, d equals to three, p equals to three, and d equals to two, p equals to five, have the same deterministic scaling, which is one half, one is probabilistically subcritical, but the other one is probabilistic critical. And this is what explains why these two problems are so different. Okay, so let me state formally the theorem. So the theorem, the first one, actually, we didn't prove them in this order. So the first one is exactly the local well-posedness that says that. Exactly the local well-posedness that says that if p is equal to three odd, and you are in the full subcritical regime for any s bigger than minus one over p minus one, and alpha is the one that I told you before, then the NLS under some suitable renormalization, which I tell you in a minute, with the alpha random data that I showed you before, is almost surely local will pose strongly in a and Uh, in a and the uniqueness is in some suitable sense, and actually, what is more beautiful, the part that I like the most about this, is that this solution has an explicit expansion in terms of multilinear Gaussians with adaptive random tensor coefficients. In other words, we can give a very precise description of how the solution looks like. And so, in a graph, what was known deterministically until this result is the green region. The green is the green region, and what we are proving probabilistically is all the pink region. So, we can actually prove all the way up to here. And this is optimal. Okay. So, let me give you some remarks. The renormalizations that we use are the weak ordinary renormalizations. And the reason is because you are below L2, so an infinite L2 mass. L2, so an infinite L2 mass implies that the potential energy is almost surely infinite, and that the non-linearity doesn't make sense almost surely as a distribution. So you need to remove this infinity. And this is usually done, this normalization is done under the weak ordering. In terms of the uniqueness, the way that we understand uniqueness is in the sense that our solution is the unique limit of all possible choices of canonical approximations of regularization. Canonical approximations of regularization. So it doesn't matter whether you actually do finite-dimensional sharp cutoff or if you want to do any other finite dimensional approximation, they all lead to the same result. Okay, and this is what I said before that it barely misses the 3D cubic because it's critical, not subcritical. Now, let me talk about a result that is a consequence of the first result, which is a long-term result that says that if you have well-prepared smooth data, Have well-prepared smooth data, such as the one that arises from the derivation of the wave kinetic equation and wave turbulence, then we have a long-time existence that predicts the time of the first energy cascade, which is consistent with the wave turbulence theory. So in other words, if you take S and alpha as before, n-dyadic, and phi is a Schwarz function, and you take data now that looks like this, this is essentially the data that you take up to some rescaling in the wave kinetic. To some rescaling in the wave kinetic conjecture. Then, what we can prove is that with high probability, there is no energy cascade between the Fourier modes, okay, up to a time scale which is n to the p minus one times s minus s minus the probabilistic scaling minus epsilon. So here you have s minus the probabilistic scaling, right? This number is much larger than s minus the probabilistic scaling. Larger than S minus the deterministic critical scale. Okay? And here we get this result because we are in the square torus. If you actually were in the sort of like a generic irrational tori, then you would expect here to get a number two. And justifying, so justifying the wave kinetic equation up to the sort of kinetic time scale would be removing this, we're doing. Removing this, redoing that and removing this epsilon, which is what Yudeng is going to tell us tomorrow, which was solved by Sakhar Hani and Yudeng. And another thing that I want to say here is that in this problem, we don't need to weak order the NLS. And the reason is because the worst contributions that can be rescued by the weak ordering come from high-low interactions where the high frequencies form a pairing that produces. Pairing that produces a master. But when you are actually in this case of random homogeneous data, there is no distinction between high and low frequencies. So this issue is not a problem for us. And the third result, which actually was the first one that was proved, it was proved before the first result, is that actually we actually get this open problem. This was an open. This was an open problem for a very, very long time. We get that the renormalized NLS is almost surely global well-paused on the support of the Gibbs measure. The global nonlinear flow maps a full measure set into itself, forms a one-parameter semi-group, and keeps the Gibbs measure invariant under the flow. Okay, so this is again the picture Bourguin had done in two dimensions, only this little green, and we are doing all of this up to NEP odd. Up to NEP odd. So, fully solving the two-dimensional problem. So, once we have proved this result, the only outstanding problem is the three-dimensional problem and cubic. So, let me talk a little bit quickly about Bourguin's method. How did he prove it? So, Bourguin considered the cubic NLS, and this is actually the weak reordering. Actually, the weak reordering written in a very informal and improper way. It's not really this, but let's pretend that it is. And his random initial data was simply given by this. Remember that in the support of the measure, alpha is one. Okay? And so the initial data, actually, as I said before, is just below L2, and the probabilistic scaling is minus a half. So this is a sub-critical problem relative to the probabilistic scaling. But it's super critical. Scaling, but it's super critical relative to the deterministic scaling. So, what was Bourguin's idea? Bourguin's main idea was to make a linear, non-linear decomposition where the linear part is rough and random and the nonlinear part is smoother. So in some sense, he said, I'm not going to construct solutions u in some ball by fixed point argument because that is never going to work, but I'm going to look for solutions that have the form linear evolution of the random data plus a remainder which he A remainder which he shows to be in some regularity which is above L2, some positive regularity. And so the idea is to solve for the difference equation. You solve for V. And what you want to do is you want to prove these nonlinear estimates in a space of regularity, which is above L2. And how does he do this? Well, he uses, of course, Marching, I mean, here you have to understand all the possible interactions. Understand all the possible interactions between random, random, random deterministic, and so forth. And so he uses multilinear large deviation estimates, he uses sort of elementary analytic number theory or integer lattice counting estimates. And he uses a very, very, very crucial property that is crucial to the Schrodinger and is not used on the wave, which is he uses the TT star arguments or what I will call random matrix estimates, which are actually the correct. Which are actually the correct way to exploit randomness in the absence of a gain of regularity. This is something that happens in Schrodinger, but doesn't happen in wave, in which you have a gain of one derivative, and doesn't happen in heat, in which you have a gain of two derivatives. But in Schrodinger, that you don't, you need to do this argument. So, what is happening for the quintic in 2D? Okay, what's the problem? 2D. Okay, what's the problem? So let's take the quintic in 2D and recall then that the deterministic Kelly is one-half. And suppose that we want to do Borgen's argument. We want to do a linear, non-linear decomposition and write U as the linear plus something that we want to be smoother. Okay, so, but if we want this V to be smoother, then this is going to be below L2, but smoother for us means that we want this V now to be not below, not. To be not below, not above L2, but above H. That means that we need to gain regularity, not epsilon, like we're gained, but we need to gain regularity one-half plus epsilon. Okay. And so the problem is that if you do this, the best you can do is to put V in H half minus epsilon, which is still deterministically supercritical. We cannot put it in H half plus. There is no way of doing that. And so you cannot close the estimate. And so you cannot close the estimates following this argument. Okay, so the poor regularity, then you analyze, you look what's going on, and you realize that the poor regularity on V only comes from the high, low frequency interactions, where the high the high in the linear evolution of random and low in the V. And so what you would like to do is you would like to identify from V, okay, a term, some you want to peel off from V, the offending. peel off from V the offending term. So you want to peel off from V a term X, which is actually para controlled by the linear evolution of random. And what that means is that you would like to find the term X, which is the Duhamel operator of a para product between the high frequencies of the linear evolution of random and the low frequencies of U P minus 1 up to some smooth remainder. And you want to show that this X is And you want to show that this x essentially behaves like the linear evolution of random, and that if you subtract this y from the v, then the term that remains is really above half. So you can close. Okay, so in other words, you don't do linear, non-linear, but you peel one more term and then you could close. So that's the idea. And this was inspired by work in regularity structures and in the paracontrol calculus on the stock. Para-control calculus on the stochastic heat equation side. But what's the problem? The problem is that in order to justify what I wrote just here, you need to have some control on the low frequencies of u p minus 1. But the low frequencies of u to the p minus 1 actually contain part of the low contain the low frequencies of x or of x to the p minus 1, which is only also in half minus. And so there is no half minus. And so there is no way of controlling this x to the p minus one, assuming only this. Okay, there is no way of doing any of that. And as I said, in the wave or the heat, you are done because this is really smoother and you can actually put it in a smoother space and close your estimate. You can really just do this. But in the Schrodinger's, you cannot do this. Okay, it doesn't work. This will amount to doing a paracontrol calculus. Para-control carpus, but in the Schrödinger doesn't work. And so, what is it? How do we tackle this? Well, we need to zoom in and unveil and invoke the structure of x. That means we need to sort of understand what is the randomness of x, not just the regularity of x. And so how can this be done? Because what we need to do is, in order to justify the behavior of x, we need to justify the behavior of x. And so what we need to do is an induction of frequency and justify the behavior. And justify the behavior of the lower frequencies parts of X before we can justify the high frequencies part. We need to prove bounds on the lower frequencies because before we can prove bounds on the high frequencies. Okay, so to simplify, let me just write the little Paley projection of X as the sum over all L, which are much frequencies much smaller than L, of the Duham L, forget the para product. This is the high frequencies of the linear evolution of random, and these are all the low frequencies. And these are all the low frequencies of U. And so now, what you would like to say is you have to say, well, if I want to do an induction, what I would like to do, I could presumably do a recursion reduction in which I write this object as an expression of the low frequencies of this. I start to undo. I go from N to L, and then I go from L to R, where R is lower, and you keep iterating and so on until you. And you keep iterating and so on until you reach one. But if you want to actually do this recursive reduction, then it's a nightmare because you get an iteration process, which is a very complicated tree expansion that involves all scales from n to one, which you have to tree simultaneously and it's very hard. And so the combinatorics of doing this are overwhelming. And you could try to do it for p equals to five, but if you want p equals to five but if you want p equals to seven or any other p that is larger to that any power non-internet larger than that it's impossible so the the the answer to this conundrum is that you have to change the way in which you view the problem okay so what is the goal the goal is to capture the implicit randomness of the structure of the of x in some norm or quantity that propagates in other words that allow us for an induction from the low frequencies Induction from the low frequencies to the high frequencies. So, if we can prove estimates for the low, then we can prove estimates for the high. And so clearly, this norm, okay, h a half minus, the norm of this, is not going to do the job since it's a supercritical norm. And so, to find the right quantity, we need to actually change the way in which we view this term. So, instead of viewing it in this form, what we do is This form, what we do is we take the summon here and we view it as an operator that maps y. So, call this y maps y into this object. So, we don't view it as an estimate of matic expansion, but we view it as an operator which I'm going to call P and L. L is the low frequency, L is the high frequency. So, this P and L depends on P L of U, which has an important. P L of U, which has an implicit random structure that looks like a paraproduct structure. And you can think of this as a projection onto frequencies n, followed by a weighted average over smaller scales L, say when you apply to the Gaussian free field. And so this is the reason why we call this operator a random averaging operator or a matrix operator. And this operator And this operator is the key to propagating the right probabilistic bounds. So now the bottom line of this is that this operator PNL, whose coefficients will depend on the frequencies of the linear evolution random, contains all the random misinformation of the low frequency component and can be carried by two operator norms, which are very easy, which are both L2 base. This is the operator norm. two base this is the operator norm as when you build this as an operator from l2 into l2 or from a some xsb into some xsb and this is simply the hiller-schmid norm so this tells you just that this operator remains in h half minus which you cannot use directly but this bound is the key bound that allows you to propagate um to propagate the estimates okay and so once you prove this then you can implement uh You can implement this inductive argument and you are done. Essentially, that's a little bit the idea. And what I want to point out is that if you had general functions in, say, L infinity, H half minus, so in other words, if you were dealing with general functions in X one half minus in some X S V as the low inputs, we will never be able to prove this estimate. So the structure of X is crucial. The fact that we are actually writing That we are actually writing the structure of X in this way is actually crucial. You cannot do this unless you understand the structure of X, the random structure of X. This is not done in the wave and is not done in the heat, but you have to do it in the Schrodinger. And so these bounce really capture the randomness structure of the low part. Part. So let me actually revisit this a little bit quickly in some graphs so that I can tell you which is the ansat that allows you, in other words, what is the ansat for the solution instead of being linear, non-linear like Burgundian, what is it that we do? So let me call again I to be the Duhamell operator. The blue ball is the linear evolution of randoms at frequency n, and the light ball is some low frequency, which I'm going to call n to the delta for Which I'm going to call n to the delta for some delta less than one. And so this configuration means that I have random, this is Lujuamello of cubic. I mean, I'm going to take cubic because quintic is too hard to draw. So you have the elevation of random low frequencies, and now you iterate, where in the first position, you iterate this configuration, and you can do this forever. So if you sum the linear evolution of this plus each of these terms, Plus each of these terms, which I'm going to call psi, or I'm going to call it this blue cube. This is an infinite series of trees, is equivalent to actually solving this paralinearized equation where here this n to the delta is the L that I had before. So in other words, solving this is essentially solving this equation where here, instead of having the linear evolution of random, you have the psi itself in the first entry. And you can do that explicitly. And you can do that explicitly. So you know that psi and delta is simply this operator that I'm going to call Pn of the U-linear, is the inversion of this. And so if you take Fourier transforms, what you get is that the Fourier mode of this is simply a matrix, right? A random matrix applied to a Gaussian. Okay? So this HKK1 is a matrix and is what we call Is a matrix and is what we call a one-one random tensor. And the key point here is that this random one-one tensor is independent of this Gaussian random variable. So the full answer in the 2D gives is essentially the following. Then instead of doing linear, non-linear, we write u as the sum over all n of this passive remainder, which is the same as the linear evolution of random plus the sum over n of all these. Plus the sum over n of all these operators plus the remainder. Okay, so on the Fourier side, what that says is that the Fourier coefficient of, you see, Bourguin only did u linear plus remainder. But in our case, this remainder is too rough. You cannot close the estimate. So what we did is we peeled from the remainder the bad term, the offending term, which is this one. And now this one is smoother than H. So we can close the estimates. So, we can close the estimates. Okay, actually, you can put it as close as H1 as we want. And so, expanding the solution, so in Fourier space, what you take is that the Fourier transform now looks exactly like a Gaussian, a matrix applied to a Gaussian plus a remainder. And actually, what is very beautiful here is that this random tensor, actually, which contains all the information on the low frequency components of the solution U. Components of the solution u is almost diagonal. So, this term here is actually what we call a color Gaussian. It's like a twist of a Gaussian. So how does your solution look like for short time? It looks like a linear evolution of Gaussian, something which is a matrix which is an almost diagonal times a Gaussian. And the next term, as I will show later, it will be like an almost diagonal matrix of a Gaussian. Diagonal matrix of a three-dimensional matrix, also almost diagonal on some hyperplane, times the product of two Gaussians and so on and so forth. So we have a very beautiful and precise description of how the solution looks like, at least for short times. This is global, but I will tell you more. So in order to treat the first theorem, okay, we cannot do this. So this is just linear evolution. So, this is just linear evolution, this is like to first order. So, in order to treat the first theorem, in which we want to go all the way to the subcritical, not just this problem, we need to actually deal with what is called the theory of random tensors. And the theory of random tensors that we develop is actually built on top of this theory of random averaging operators. So, the random averaging operators is much simpler and is usually enough. Simpler and is usually enough if you are not too close to the probabilistic scaling. Okay, and it has a very simple and very easy form and an answer that it can be is very portable, it's very useful. It's less notationally technical, also heavier. But in order to prove the theorem one, in which we want to reach the full subclitical regime, we cannot just expand to first order. We actually need to consider higher. Actually, we need to consider higher expansions, which are going to lead to multilinear expressions as well as associated random, not just one, one tensors, but Q1 tensors where Q could be arbitrarily larger. And these tensors, these are random tensors, are going to all depend as before on the low frequency components of the solution. Okay, so in order to find the answer, what we are going to do as before, we are going to actually keep iterating. Actually, keep iterating now every single entry, expanding the non-linearity using the equation itself, and we will stop as soon as we hit some low frequency prescribed, which I'm going to say is n to the delta. And so assume that you have expanded to some arbitrary order d, d can be as high as you want, then the answer that you have for the problem of the random tensor is. Of the random tensor is that the Fourier coefficients of the solutions are going to look like some expansions of arbitrary order d of these random tensors times multilinear Gaussians times a remainder, which can be as smooth as you want. The longer you expand, the smoother you become. This remainder is also small in L2. And something that is absolutely key is that these random tensors that contain the information on the low frequency components. The information on the low frequency components of view are actually random, are independent of these Gaussian variables. Okay, and this is actually encoded into this, in which if you take the maximum of all the frequencies here, and you call that maximum n, then all these nj's are going to be bigger than this n to the delta, while this is going to be a Borel function of the random variables g k we k. Variables GK with k is strictly less than n to the delta. And that's what guarantees independence. Okay, this is crucial. And so, as I said before, this should be viewed, this random tensor theory should be viewed as a dispersive analog of the theory of regularity structures for singular parabolic PVE. And the difference is that, sort of, in the regularity structure theory, the solution is expanded as a Taylor series with local. Taylor series with local sort of series which is local in space because the heat operator localizes you in space involving multilinear Gaussians. In this case, for dispersive PDE, we also have expansions involving multilinear Gaussians, except that these expansions take place on the Fourier side. But in both cases, the key is an analysis of the high-low interactions, which are the really problematic interactions. Interactions. And so let me hurry up. Okay, so these quantities here, HKK, these random tensors become the main object of study. And the convergence of this, actually, which is what you want to prove, is completely determined by the bounds that you can prove in this. So everything that we do from now on is. So everything that we do from now on is to actually get suitable bounds so that we can actually sum the series, so to speak. I'm actually not writing this exactly in the best kosher way, but this is the idea. And so as I said before, these higher order tensors come from the higher order iteration trees. And so here I wrote some examples. For example, this is a 2-1 tensor in which you have two linear evolutions and something low. Revolutions and something low. And so you see when you take the Fourier transform, which is which is it, what is the 2,1 tensor? It's simply this object, right? You have the linear evolution, linear evolution, which give you these two Gaussians, and this is low frequency. And so you put it here. And this is exactly the 2-1 tensor that carries the information on the low frequencies of the solution. Okay? So this 2-1 tensor is a map that is something. Tensor is a map that is something that maps k1, k2 into k. And here I have another example of a 21 tensor, which is more complicated, but I'm going to skip it. And so what the random tensors framework allows us is to handle this exploding complexity that I mentioned before that arises from the higher order iteration trees. And in order to prove the estimates on the random tensors, we need to develop an algebraic theory which focuses on the structure of H. The structure of H in the expansion, and how these tensors are built from smaller tensors using operations such that tensor products, contractions, what we call semi-products, and two crucial algebraic operations, which are called merging and trimming. And we also develop an analytic theory, which entails choosing suitable norms for these random tensors, which we help behave well under. We help behave well under actually these algebraic operations, okay? And so, um, Alex, can I take five more minutes? Yeah, we actually started a couple of minutes or later. Yeah. Okay, great, excellent. So, let me give you an idea of what this merging operation is. So, here you have something very easy, which you have a one, this is a one-one tensor, so because you only have linear evolution here. You only have linear evolution here and too low. This is an example of a 2-1 tensor, and here is an example of a 3-1 tensor. And suppose that there are no pairings, meaning that the Gaussian of this, the corresponding A, the number A here corresponding to this Gaussian is different than this B and so on. Then if you take the Duhamel of this cubic term between the one, one tensor, two, one tensor, three, one tensor, what you produce is a six-one tensor, which I wrote here. Okay? So this is a Okay, so this is how you build bigger tensors from smaller tensors. And this object in green is what is called the base tensor, which is constant in the random variable in omega, and it comes simply from the multilinear form of the nonlinearity. Now, suppose that you had a pairing. Suppose that the Gaussian here, the A, remember that this is you are complex, so this is a cubic thing. You have u, u bar, u. Have u u bar u. So suppose that here the Gaussian, the ga is equal to the g b bar. Okay, then here between these two, you lose independence. And so in that case, what you have to do is you have to contract. And what you get is not a 6-1 tensors, but what you get here, you get the merge, what is called is a 4-1 tensor because you need to contract against. Uh, you need to contract against the pairing, okay? So, um, what is streaming? So, what I told you before is that in order to have the independence between the tensors and the random variables and the products of the random variables, you need that the maximum of all the frequencies that appear in the random variables, which I call n, should all be larger than the low frequencies that are on the tensor. But when you do these merging operations, it may very Merging operations, it may very well happen that you end up with something where these two terms have frequencies n1 and n2, which are bigger than n to the delta, so you're fine. But this one, as a result of the merging, drops lower than n to the delta. And if this happens, you lose this independence between the tensor and the Gaussians. And so, what you do is you have to trim it. So, what you do is you trim it, which means that you contract. Means that you contract against these three Gaussians and you replace this by just a low frequency. So, what you do is instead of considering this, you consider this tensor, which I call H prime K A B C, which is contracting against these three Gaussians. So now this new tensor is going to depend on the Gaussi D, E, and F, which have frequencies N to the delta, and are going to be indeed. And they are going to be indeed independent of the Gaussi A, B, and C. Okay, because A, B, and C are above N delta, and these are going to be below N delta. And this streaming operation is what allows you always to restore independence. For example, that's one way in which it's used. Now, the analytic theory, as I said, is very simple. And this is the beauty of everything because you don't need any crazy norms. As I told you in the random. Norms. As I told you in the random averaging operator, you can just deal with operate L2 based operator norms. And so the only operator norms that we need are these operator norms of the tensors in which if you have, for example, a tensor K, so K X Y Z, you can do this as a mapping from Kz into YZ. And so it's simply given by this L2 norm, or you could sum over all four estimates, in which case you get a Hilbert-Slig norm. Get the Hilbert Spit norm. Okay, so it's again as before, you just get the operator norm or the Hilbert Spit norm. And so for the merge tensors, we have two main estimates. One is the estimate for the merge tensors that said the following, that if you take, if you do sort of a tensor product of three tensors, then if you want to take, say, the norm. The norm from BZ to CW of this object, then that's less than or equal than H1 from AB to C, EF to A, Z. But here, what I chose here is totally random because the order in which I choose this, I can choose any order I want. So this here does not depend on the order, but the estimate here does depend on the order. So what that means is that we actually gain. We actually gain not just one estimate, but we get sort of like a family of estimates or a set of inequalities for which we are free to choose the best one possible. And so in this proof, there is actually a selection algorithm that at each stage, you need to actually be very clever which one is the best one to choose. So there is something there. And then there is something more beautiful, which is actually very, very Uh, well, it's actually very, very powerful, which is the trimming estimate. So, this comes, for example, suppose that you have a tensor H prime k x z, which is just the sum over yw of the tensor h k x y z w, g y g w. And suppose that this tensor is independent with these two random variables. Then what you prove is that with high probability, for example, For example, the norm of this operator from L to say Kx into Z, so Kx is fixed, Z is fixed, is less than or equal up to a loss of N to the epsilon, which is the maximum of all the frequencies, of the maximum of all possible choices that you can take here. You can take this choice, you can take this choice. These are all possible choices because KX and Z are fixed, but you can move any way you want Y and W. Y and W. You can put it here, you can put W here and why here. You can flip it, or you can put it like this. And so, this estimate is extremely powerful. This is also what we call a random matrix estimate. And the proof of this goes back to Bourguin result. And it's actually a TT star, a higher order, is a TT star argument in steroid. It's actually proved by doing a higher order TT star argument using the multiple. Using the multilinear estimate. And even in the simplest case, it's very hard to find a non-trivial proof. And so, what we think is that this is a very elegant inequality that might be useful in the study of random matrices with general reduction entries anyway. And so, once you have this to conclude, then you can prove the following bounds for these random tensors, which I wrote in blue. So, then you see these growth estimates, beta is. These growth estimates beta is less than alpha, so these ends get killed by this decay. Okay, and then this decay here is what allows you to sum here the tensors. And once you have this, you are done. And the proof of this is based on induction, some counting lemmas, and the delicate selection algorithm to choose the right order in this merging estimate to exploit the flexibility. To exploit the flexibility of this, and this trimming estimate or multilinear merging estimates. And once you have that, you are done. And moreover, what we prove is a Fourier weighted estimate that localizes the tensors as multilinear, Fourier multipliers in the support of the tensors. And this is what I was telling you before. So your solution, your fully coefficients are going to look like linear evolution of Gaussian, a matrix that is almost diagonal to apply to. That is almost diagonal applied to a Gaussian, then a cubic matrix, if you wish, which is sort of like localized on a hyperplane applied into the product of two Gaussians, and so on and so forth. So, for short times, you have a very precise description, and you can go as far as you want. Okay, you can get this as long as you want. And so, my time is up, but I just want to say one thing because I mentioned the Hartree equation. So, we were interested in actually proving the invariance of the measure. In actually proving the invariance of the measure for the Hart equation in three dimensions, which is simply this equation where V behaves like a Bessel potential. So, here the Fourier transform of V is like one over Japanese bracket of K to the beta. Bourguen had proved that if beta is bigger than two, then you have an invariant Giles measure and you have almost surely global strong solutions on the support of the measure. The critical regularity here is one minus beta over two. Is one minus beta over two. Okay. And so what happens here, what we prove here is Bourguin has proved that if you're above two, you can do it. And what we prove actually using simply the random averaging operators is that you could actually drop this beta below one and still get an invariant measure here and the almost sure global existence of strong solutions use only using the random averaging operators idea. idea and if you actually want to use the random tensors you can actually probably go all the way to beta equals to zero or have a student that is working on this and so um what i want to note here is that you see the the three-dimensional the three-dimensional nlls is critical right because the the sg is minus and half minus and the probability and half but the but the hard tree is sub-critical okay it's subcritical here but it has a catch But it has a catch. And the catch is for the first time in this equation, we see the probabilistically critical monster showing its head. And the reason is because now we cannot do any more the answer as in the 2D gives, which is linear evolution of random, random averaging operator, and remainder. There is one more term here, okay? Here, okay, one more term here, which is kind of like a modified random averaging operator that you need to peel off from W and has and arises because of the following.