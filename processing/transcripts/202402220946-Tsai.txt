You know, what kind of influence a training data can have to learning. It's partly because, you know, I like solving PDEs and/ODEs. And so during the pandemics, we were thinking about, okay, let's try to do something that we're not used to. And since we're all interested in machine learning, so why not looking at data and try to see whether we can say something interesting about it? But I have to say that we're still kind of like. Say that we're still kind of like you know, beginner in this deep learning business. So hopefully, you'll find the result kind of interesting, has some insights. So, this is a joint work with a former postdoc of mine, Jun Tsai He, who is currently at CAOS. I think he's on the job market this year, and a current student, Louis Liu, who was trained by the excellent Canadian education system. And I usually go by Richard. And I usually go by Richard. I'm at UT Austin. Research is funded by the National Science Foundation and Planning Research Office. Okay, so we started out by talking about data, and our starting point is whether this so-called manifold hypothesis makes sense or not. So the manifold hypothesis kind of postulates that a lot of those kind of interesting data sets, data sets that we're interested in, they are somehow embedded. They are somehow embedded into some most naturally or conveniently embedded into high-dimensional Euclidean space. But if you look at the distribution of such kind of data sets, they tend to be concentrating on a lower-dimensional subset in this high-dimensional Euclidean space. So then if you hypothesize, then you say that, okay, maybe we can think about this distribution kind of roughly concentrating on some kind of lower-dimensional sub-manifold in a high-dimensional inclined space. Of incoming space. So, this is a beginning example, right? So, if you think about this hand-written digit data set that Elena used, so it's each one of the data points is actually an image containing 28 by 28 pixels. And each image is formally considered as a point in R784. I used to tell my students: well, if you look at this, I mean, you know, you have this handwritten pigeon. Why do you embed it into R784, not R800? 784, not R3000 or R30. So there's some kind of arbitrariness in this. So the starting point of our investigation is like: so, what is the consequence of this arbitrariness in embedding to your final learning outcome? So then, one of the simplest tools that we can use is try to use principal component analysis to try to understand. Analysis to try to understand the distribution of data sets from the perspective of how they distribute relative to linear subspaces in this embedding clean space. So here, so if you look at these four pictures over here, they are projections of some kind of credit card usage data sets after some cleaning up, before they can, otherwise they would not be useful. And then you project them onto, say, the first two principal components. The first two principal components, these are first two, the second and third, third and fourth, and fourth and fifth principal components. So that we can visualize them as two-dimensional distributions, projections of these data points. These data points are grouped by k-mins, so you know, so, you know, and and each groups are colored by accordingly. Um that's shown in other states over here. So what do you see over here is that they have a lot of structures, these kind of data points. Lot of structures, these kind of data points. And furthermore, if I just single out this one, this particular group, you see that even after being projected from some higher dimensional space onto R2, right, spanned by the first two principal directions, it shows a varied variance in their distribution. Okay, and same, you can say for other groups. So, in this talk, what we meant, a data set, A data set to be multi-scale, it means that the variation, so let's say from the perspective of principal component analysis of such data set, then the variances discovered by the principal component analysis varied a lot in different principal directions. So, in this talk, that's what we meant by multi-scale data set. But you know, if you can take, you can also take the step further and say that suppose you have. Step further and say that suppose you have a data distribution, probability distribution that kind of concentrates on a lower dimensional sub-manifold, and then on top of that, maybe you have a little noise in the normal directions of the sub-manifolds, then we also think about, I mean, this would be ultimate goal of thinking about analysis in such a line of research of ours. Okay, so going back to NNIST, so if you Going back to NNIST, so if you do this principal component analysis, so the figure on the left shows you in the ordered fashion the variances discovered by, basically the singular values discovered by your principle homo analysis. And what you can see is that they do drop tremendously fast. And in fact, if you're familiar with some work in manifold learning, it is believed that the smallest number they discovered is that you can kind of try to fit some. Can kind of try to fit some kind of smooth manifold of dimension eight to the minus data set, even though this data set is artificially embedded into part 784. So there's a lot of free space away. And if you look at, again, if you look at projections under the first two principal directions, then you see groupings, and you still see this kind of like vary. This kind of varied distributions. Now, so before I tell you what I really wanted to talk about today, let me summarize a few things that we have discovered in this regard. And these are kind of published in these two papers. So if data are concentrating on, or distributed on sub-manifolds, so for example, you know, we see that if you try to learn something in a system that has some kind of invariance. System that has some kind of invariance to this, and your data set might really just live on the trajectories, lives on some lower-dimensional manifolds. Then, if you use the common neural network structure, then there is a set of parameters in your network that's not changable because your data doesn't allow you to kind of measure the sensitivity of the network's output relative to these setups. Relative to these data, these set of weights. And so accordingly, if you randomly initialize those parameters, then this randomness will persist, will get into the network's output. If you evaluate the network at an input, which is away from your original distribution, a little bit away from the sub-manifold where your chain data concentrates. Okay, so and then we also show in this work. And then we also show in this work that suppose your manifold has curvature in there, then in a sense, the curvature will have non-trivial effect to your results. Non-negligible results influence to your results. And then we can kind of derive explicit estimates. Kind of derive explicit estimates, you can try to understand it as some kind of a posteriori estimates, not a trained neural network's output stability. Because starting from this thing, there are a set of parameters that's not trainable. So you train your network which obtains pretty good test accuracy on the on the same distribution. But then there's this set of untainable parameters still there. This set of untainable parameters is still there, right? So now, if you evaluate the input a little bit away from your training data set distribution, then this is like evaluating the stability of the training network in the aposteri setup. So these could all be quantified. And our theory goes that if your manifold actually just roughly concentrates on the sub-manifolds, then with a small variance in the normal directions, then you can control this. Then you can control the stability of the network's output in the sense that you can increase the number of points, data points. And you can understand the need for increasing this number of data points as the need to resolve this very concentrated profile in the probability density in the normal directions of the sub-manifolds. These could be quantified for deep radar network. Okay, so well, now comes to the Okay, so well, now comes to the driving questions for the remaining of my talk. So, how does this kind of multi-scale data affect training? Affect whether one can design a more efficient training algorithm or not, or does one need a more, is there any room to improve training algorithms when phasing this kind of multi-scale data set? Okay, so we're kind of motivated by these two papers. So we're kind of motivated by these two papers. One is by Molletao, who was here a few days ago, where they assume that, so this is the data pair, x is the coordinates, y is the label, and then they construct the label which has a multi-scale expansion structures like that, together with some noise. And together with this one, then because of the multi-scaledness in the label, so the loss function will be multi-scaled, and then they kind of explore the effect. For the effect of full gradient descent, the step size of full gradient descent relative to which kind of minimum you discovered. So it's a very nice and beautiful theory like that. But their starting point is the data has this form and then you have periodic activation. So you can use more sophisticated tools developed for Tools developed for being the dynamical system theory. So then in MIL, so they consider two-layer neural network radio activation and multi-scale data. So this is closer to our assumption, meaning that they just explicitly assume that the coordinate part of your paired data X is going to be written, can be arranged in the order that, so the first group of coordinates will be. Of coordinates will be larger than the second groups, et cetera. And then they study multi-scale landscape effective how this will affect the stability of gradients. So let's, you know, thinking about how do multi-scale data affect training. Let's give a concrete example, which we studied in one of our papers. So let's say we consider Let's say we consider a linear neural network. So the activation function is just the identity. And this kind of question was kind of explored by a very interesting paper by Auroland and his collaborators in 2018. So what they find out is that suppose the weights, the initial weights over here are initialized in a particular way. Then they can derive a simple A simple ODE that tells you how the gradient descent, if you perform gradient descents for each one of the weight sets. But ultimately, the gradient descents of these weight sets can be summarized by a simple dynamical system where the lowercase old phase w is the result, the multiplication of all the internal weights. All the internal weights, all the weights involved in each one of the layers, by this simple very algorithm for the ease. You may question, why do you consider linear networks, right? So it turns out that you can, so there are people who try to use such kind of thing, such kind of setup, linear network, to discover low-rank matrix vectorization, which can be used in some applications. I believe that Molle has also a paper using. has also a paper using this one to do factorization. So if you look at, so let's take a look at in 2D. So this lowercase w has two components. In 2D, let's take a look at the phase portrait of this dynamical system. Okay. And we kind of so in this case we kind of assume that the data will concentrate on the x-axis with a small variance in the y-direction. In the y direction. Let's just understand what's going on. So, what's going on is that when the variance in the y direction is non-zero, then there is a unique fixed point over here. You note if you squint a little bit harder, this is plotted by this white star over here. This is x-axis, this is y-axis, these are the flows. So, what you see is that there is some kind of very slow, you know, slow manifold, you know, kind of connecting to the You know, kind of connecting to the equilibrium points over here. So, if you do a full-graded descent, what's going to happen from randomly selected conditions over here, you're going to flow quickly to here, and you're going to stay here for a very long time, very, very long time until you get to, you know, get to a reasonably close neighborhood of this unique equilibrium. And if you're unfortunate to get here, you'll be attracted to the origin. You'll be attracted to the origin and you will never escape. So the method will converge to the wrong thing, and your mouth will not work. So, this face portrait can be explained also from, or the consequences of this can be explained on this picture over here. If you just monitor the training loss, you see this red line quickly drop. This kind of denotes that you get closer to this slow manifold. Slow manifold. And after that, your training loss doesn't tell you too much. So monitoring only the training loss in this particular situation when your data is multi-scale is not sufficient to tell you when you try to make inference. And if you look at the error respectively to the coordinates of this unique equilibrium, then you will see that, okay, so the y-coordinate actually will take a very long time for you to be attracted down to the right one. To be attracted down to the right one. Okay, so that's the consequence. So these are just simple toy problems in two dimensions that kind of already shows you that there are particular challenges that arise if your data set has this kind of widely varied variances. Okay, so this is, we start out with a setup, right? So if you believe in this kind of PC analysis to a data set, then up to some kind of Up to some kind of translation and F on transform view rotation, then we assume that our data is in this form, already centered. X is again the coordinates part, and then G is the label part. We don't make any assumptions on G. G could be very smooth. We only care about the distribution of the coordinates of your data points and their variances having widely varied values. Values. So then we can arrange in Rd, we can arrange the coordinates into groups, like this is the second group of coordinates, etc., and then assume that we have m set of such kind of coordinate groups like that. And so this is the starting point. So what we can show very quickly, I mean, this is a very simple kind of calculation that one can do, is that if you're That one can do is that if the coordinate part of your data does exhibit such kind of structure, then if you look at for linear regression or logistic regression, the loss function with respect to the weight or to the trainable parameters, well, you know, the gradients can be grouped accordingly into groups of order one quantities, et cetera, something like that. All right. Then we can see. Then we can say a little bit more. So if you have really deep neural network where L is greater than 2, it's the same thing. But this time, it's the easy computation. The simple thing is that if you just look at the gradient of the loss function with respect to the weights that's at the first layer, that's kind of connecting to the input vectors, it still shows this kind of groups of magnitudes that has this kind of ordered structure. That has this kind of order structure like that. Okay? Now, what this shows you is that, well, if you just monitor the critical point of L, then when these are very small, you would probably likely ignore their influence when you're training. Okay, so to prove that we're not insane about this thing, right? Because this epsilon 1, epsilon 2, whatever, this is. 1, epsilon, 2, whatever, this is what we assume. So let's try to connect it to some more, a little bit more realistic data set. Let's take a look at CFAR data set. And it's too simple. Let's take a look at a CFAR data set. So this is the first layer gradient and the second layer gradient of the loss function. What you see over here, the red one, is just we put the distribution of the singular value of data onto there for reference, for comparison. For reference, for comparison, right? You will see two sets of weight, two sets of plots. So the one set is when we, so the first one, W1 is related to the gradient with respect to the first layer weights. And then here, W2 is the gradient of loss with respect to the second layer. Plotted, organized according to the singular directions. According to the singular directions. Okay? Ordered in this way. And the first set here, the blue ones, the blue color is when those weights are, when you're training and you reach a 30% test accuracy, and then you plot out, you study how the size, the distribution of the components, the gradients. They have this kind of behavior. And then the second set is: okay, let's train further to 50% and see whether you still exhibit such kind of. Whether you still exhibit such kind of multi-scale distribution. And it does. It's pretty consistent. And if you look at the second gradient, second gradients of the loss function with respect to the weight in the second one, it is larger than this one, but it still has this very quick drop, and then some kind of relatively shallower plateau, and then a severe drop later on. So here we plot to 3,000, right? But if you think about this, the embedding dimension. You know, the embedding dimension is much, much higher. Okay? All right. Okay. So then we can do a little bit more if we assume, if we make a stronger assumption on the multi-scaleness of the data set. So suppose now, instead of epsilon one, epsilon two, so we have powers of epsilon, a single small parameter epsilon, we have powers of it. Then we can prove with, for me, excruciating computations. Excruciating computations by hand, what you can show is that, well, the loss with respect to any weight in your deep neural network have a multi-scale expansion in epsilon, in the powers of epsilon. And furthermore, the coefficients, so this bracket here with a subscript n denotes some kind of empirical mean, right? So they're all driven by the data. So the coefficients here has this kind of structure. Coefficients here have this kind of structure where this comes from the fitting, and then this lambda comes from very complicated, for me, very complicated manipulation. I got dizzy a little bit by looking at that one. Anyway, it has this nice structure where the coefficients at level k will depend only on the previous coordinates, but not on the higher orders coordinates. So that kind of also, you know, for me, it's kind of Know for me, it's kind of interesting to see this one. But then you say that, okay, when it comes to training, right, the selection of a stable step size really requires some knowledge about the Hessians, the eigenvalues of the Hessian. So what can we say about the eigenvalue of the Hessian if you have a multi-scale loss, right? Our loss is just the loss formally, our loss function is just L2, right? All these multiscaleness is really just driven by how your data is distributed. Now, Now, the first layer is very simple. The Hessian with respect to the weights in the first layer is very simple. And that's where we stop, actually, for now. Because after some manipulation, you will see that this Hessian over here depends on this kind of covariance matrix over here, where cap A X is a covariance matrix from your centered data. Then you can show. you know then then you can show that well the Hessian you know of the first layer do have a I didn't I didn't cite the theorem but but of just because of this you can you can see that the eigenvalues of Hessians with respect to the first layer weight will have most will inherit this kind of multi-scale properties from your data directly. Okay? So this is a very strong evidence that there is room to improve your gradient. To improve your gradient-based training algorithms, if you understand a little bit more, because there will be gaps that you can explore or exploit in order to improve the efficiency. Now, again, let's look at CIFAR. I just want to try to bring, say, our consideration, just a sanity check to see whether the assumption is just too ideal. Whether the assumption is just too ideal, and it is very far from reality, even though training for CIFAR is, I think, very far from the forefront of deep learning research. Anyway, so again, training the network onto a 30% test accuracy or 50% test accuracy. And these are ordered. So these are the eigenvalues of the first layer Hessian. So this is the data, these are the So, this is the data, these are the distribution of eigenvalues. And this is the second layer. It still has that, but if you notice, that it says several orders of magnitude larger than this one. Now, what can we do to improve? Well, we try to use our understanding about the separations of the groups of eigenvalues in the hessence. In the half sense. So we idealize. So we go back, revert back to the study of strongly convex quadratic case, right? So where we can say a little something more concrete. So we consider this kind of loss function, where A is the symmetric positive definite matrix where the eigenvalues as this kind of structure can be grouped. So within each group, the gap, the ratio, will be denoted, will be quantized. Will be denoted, will be quantified by the local condition, what we call local condition number, k. And then there is a larger gap between each groups, and this will be quantified by r, or 1 over r, or these r, of course, can depend on between each adjacent groups. Like that. So these are the basic quantities that will influence the determination of the learning rates that we're going to use in our training gradient. Training gradient descent algorithm. Okay, so then it's probably not hard to imagine that we're going to propose this, right? So we're going to propose a composition of four Euler style gradient descent involving a group of small steps that's responsible for. That's responsible for kind of stabilizing the largest singular values. And then you go to the next one, you go to the next one, you go to the next one, and you can consider this as one gigantic step, and then you repeat. So this is like a particular judiciously designed cyclic training schedule. So the point is that how many steps do you need to do for the red one, and how many steps do you need to do for the next one, etc.? Do for the next one, etc. And how do you choose them so that this cyclic training will be stable? So, this is a simple exercise once you realize this, you know, that many of us could do. And that's what we did. I think I'm just going to skip over the description of the algorithms, right? You just evaluate the gradients and you predetermine the so-called learning rate, and then you just Rate and then you just repeat. Okay, so you know, error analysis or conversion analysis would be related to determining that the magnitude of this operator Qj has to, you know, when they multiply together. That's the point is that when you multiply each one of these operators together to the suitable, each two suitable powers, right? The powers is related to how many steps you want to do for each one of the learning rate. One of the learning rates. So then this guy has to, the norm of this guy has to be less than one. So yeah, so choose appropriate numbers of iterations for each learning rate so that the norm of S has to be less than one for convergence. But what's interesting in this composition is that, well, I mean, if you, if you know, when you're using much larger When you're using much larger, so I operate much better with pictures than this formulas. So when you're doing this, right, this step size is not stable for this group of eigenvalues. So in the composition of this Q1, Q2, whatever, some of them are going to be larger than 1. So therefore, you need to rely, the intuition is that you need to rely that you have a sufficient number of small steps to stabilize. To stabilize when they are not being stabled later on. So the aspect could be a little bit painful, but we kind of did it. So essentially, you have this kind of inequality that helps you determine how the number of steps for each learning rate should be. I'm not going to bore you with the details, but they determine, they're dependent on these conditioned local. You know, these condition local condition number for each eigengroups, and then the little gap that's between each adjacent eigengroups, like that. You know, so then. How do you estimate this local condition? How do you find it? Well, we haven't done it, right? But you can do, so you can try to estimate it somehow. But this is so far, just suppose you know. You know, in situations like suppose you know. In situations like, suppose you know the gap, right? Then you can do this thing. So, yeah, so what I forgot to say is that all these estimates on the Hessians and the gradients of the loss function, that's fine. And then, you know, there's a huge gap. You know, you have to kind of bear with me, right? There's a huge gap when we talk about this multi-rigg gradient descent algorithm for a Know for it to be fully useful at the moment. So, this is, yeah, I'll skip that. So, convergence, well, so this is just to say, so if you say that all the condition numbers in each group, you know, let's say ideally they're the same or they are bounded above by some constant, and then the gap also bounded above by some constant, then you can. You know, you can show through previous theorem that the norm of S is going to be less than or equal to 1 minus this guy, where eta is just a constant that you use to scale your subsequent learning rate. It's like a baseline thing. Then you say, okay, suppose you want to reach a training error of epsilon, then how many steps? Of epsilon, then how many steps do you need? Then you can easily put it in, and then you see that you need a big O of this number of steps with a log epsilon, which is typical. But then you have to accumulate how many steps that you have. And you have n groups, right? So you have n number of n i's. So then the ultimate question is, in terms of efficiency, right, of this algorithm, would be. Efficiency of this algorithm would be, you know, how do these ni add up? So we need to get an upper bound on this. I'm going to bore you on this thing because I got bored also looking at them. So the point is, you know, go straight to the bottom, right? So the total number of steps for each gigantic leap involves the total number of gradient descent steps, right? Gradient descent steps, right? For each cycle, it scales like this. Condition number, local condition number to the power of n minus 1, and then this gap, log of r to the power of n minus 1. So that's how it scales. So this basically tells you the kind of the complexity of this Mr. GD, as my students like to call it. So we can put it up for comparison, right? Just formally, how efficient is Mr. GD compared to classical GD or some kind of momentum-based method? And if you have full knowledge about the eigenspace and custom design and multi-crit methods. So complexity, so this is a gradient descent, momentum, you have a square root. And what we do is we, instead of these, one over Over r, one over square root of r, we have log of r, absolutely log of r. And if you can custom design, then you can even bypass this log term. So these are two numerical studies, toy problems in 100 dimensions, trying to use Mr. GD, compare Mr. G using Mr. GD and using full gradient descent in training. And using full gradient descent in training in just trying to solve this linear regression problem on the Gaussian target. The number of points are 10,000 data points. This is a two-scale problem where data points are, you know, kind of, well, so because of this thing, so what it means that you have kind of variance gap in just forming two groups, and this is. Two groups. And this is three-scale. These are the gap, quite small actually. Like that. So K is the number of gradient descent steps. This is the log base 10 error residual. So this is the performance of the full gradient descent. This is Mr. Gd. You see a little bit of oscillations because of this small steps, large steps. Because of this small step, large step kind of mixture. This is also like that. This is synthetic data or whatever? Yeah, this is just synthetic data just to study like that. I will show you something later on. So we can extend this theory from quadratic problem to strongly convex problem, where, say, the Hessian is, of course, have to be. The Hessianists of course have to be bounded above from zero. And so we relax the, so we assume a more restricted kind of setup where there is an orthogonal matrix that can be partitioned into n groups like that. And there's some kind of the so-called cross-spectrum is bounded. Okay, like that. So, in this particular setup, right, so yeah, so in this particular setup, we can prove a theorem like that, where, so theta star is the unique minimum, theta k is what is being computed after k steps, and this is the usual one from strictly convex from quadratic case, and then you. Quadratic case, and then you have some trailing terms like this, where C and E, they again depend on modus condition number and R that I don't show here. But we can prove this. So I think for me at this stage, the most interesting plot would be this, these two. So again, these are kind of synthetic data, right? So problem with two scales and epsilon I think this epsilon should be what is epsilon? I guess ah, because you you have different uh uh setup there. Um so you see these oscillations, but then you see that you can always come down, you can always stabilize it, and then they decrease well again much faster than a full-grade descent. But then this will be why I think this is interesting is because in reality, if you want to apply this to Reality, if you want to apply this to deep learning, you have to think about non-convexity in the landscape. And then what's more important, or what's equally important, is whether your gradient descent algorithm will discover a flatter local minima instead of being trapped in some kind of very narrow local minimum region of attraction, right? So I don't know. I think this is very tempting. So if you buy in the theory. So, if you buy in the theory that Mole Tao developed for their idealized setup, then for me, this is very tempting. We haven't done it. And I was telling Mole Tao, and he was like, oh, it would be interesting to say something about it. Well, so again, let's try to do one step further, right? Instead of synthetic data, let's just try to train, see whether we can use, apply Mr. GD. We can use apply Mr. GD to, you know, this is an experimental map now, right? To train a three-layer MLP to classify N list. So I think, according to my student, these learning rates are pretty stable. He analyzed the eigenvalues in the training process as we have shown earlier of the talk. So the lighter dots, I don't know why he just discussed. I don't know why he chose his colour scheme. The lighter dots are the multi-ray gradient, and the darker ones are the gradient, full gradient. So the blue is training accuracy, and then the lighter ones are, or the reds are the test accuracy and loss value. So this is radiant descent. This is the multi-rate gradient descent. So this is a minimization of the loss, and this is a test. Organizational loss, and this is a test accuracy like that. And this just shows you the fit of, you need a lot more small steps in order to stabilize this kind of composite four-order calculations. Okay. Let's say, so this is a typo. So let's say that MNIST is too simple. Let's try C far. So this is actually C far. And that's trying. Actually, C far. And that's try to study if we push the GD, you know, the classical gradient descent algorithm to have a larger learning rate. And that's try to see, compare, you know, for now, experimental math. So I apologize for this. The best ones are full-graded design using very small alerting rate, but they oscillate. I guess I would draw your attention to the library. Draw your attention to the light blue, which is the multi-ray gradient descent. And then you have these colors, these are the GDs with larger and larger loading rate. And ultimately, they become unstable. You can unuse them. But you see this kind of in the lost landscape, this kind of multi-rate thing will exhibit this kind of crazy junk, but then the theory tells you that they will come down, and in fact, they do come down, and then they further decrease your Further decrease your training loss, my cat. Okay, so essentially, that's all I have to tell you, and all I can tell you at the moment. So the point is that even when your point-loss function is just a simple L2 function, and your label function, or the function you try to learn, is a very smooth function. The distribution of your Of your training data set can be very important. When they're multi-scale in this sense, then we try to show you that you can say that the corresponding loss function or the corresponding loss landscape do exhibit, do inherit this kind of multi-scale properties from your data. So I like to say that when one design a deep learning model, it's always good to think about the data, look at the data, and see how they connect and what's the And see how they connect and what's the influence to the resulting learning algorithms. Then we discussed this multi-rate gradient descent. It's an explicit gradient descent algorithm that involves some kind of judiciously chosen, determined, cyclic learning schedule. And we have a theory on how many points, how many steps you need to do for each rate so that you have a stable. Rate so that you have a stable convergent algorithm. We also present its performance compared to a few other usual suspects for gradient descent boring, momentum methods, and multi-grid methods. How practical it is? I don't know. We'll find out. This is very tempting, but maybe next time when we meet, I can tell you a little bit. When we meet, I can tell you a little bit more about this. So that's all I have to say. Thanks for watching. This is super interesting to me. I'm the next speaker, and I'm actually going to talk about trying to use networks with internal linear layers to estimate dimensions of anifolds. So, one question that we've always struggled with is: how do you choose the number? Is how do you choose the number of linear layers? Because I mean, formally, it's equivalent to one matrix multiplied. But it's the issue of the. You're talking about network architecture? Yeah, so you're way back at the beginning. Right. So you've chosen a number of linear layers at some point. Oh, well, first of all, I'll. Well, they're all linear layers, I guess, in your. They're all linear layers, I guess, in your case. No, no, so there are not. So, only this particular example is just purely linear layers. All the subsequents are as deep as you want. But still, I guess I don't quite understand the structure you're proposing, man. This particular linear neural network, I don't know how they choose it. But in practice, I think there are some applications that require. There are some applications that require, you know, that can benefit from, you know, let's say, of course, if I augment the output to be non-scalar, then this would be a matrix, right? This would be a smaller matrix. And the objective is to rewrite this perhaps dense, small matrix that you want to discover by a few sparser, more structured. Yeah, so that's my question. Yeah, what's a good way to do that? I don't know. Fine. I don't know. I don't need it. Yeah. I was hoping you didn't need something. Very interesting thought, Ricky, Frank. I'm not sure if I understood it perfectly, but I mean, can you think of this multi-regulated descent as really you're replacing Euler's method with some any stage for computer effect? I don't, I don't, I haven't thought about that to make that a jump because literally the C. Jump because literally the CFAR computation was done last night. And we posted the paper maybe one or two weeks ago. But the idea is that we want to just stick with forwarder. And we're going to do the composite way of... But sequence is a four-door with different steps. So this is, I mean, you can view it as a runger kind of method. And then you could use, I mean, I think the conditions you're imposing can also be interpreted in terms of, you know, I. Interpreted in terms of absolute stability theory from the OD model. And there's a lot of work that's been done in this direction. Of course, you might be able to pull off the shelves an existing OD method that has all the properties you want and maybe is even more optimized. So that I don't know because the existing method, so for example, it goes back to Lebedev, right? And then it goes back and then further. So this kind of Lebedev methods, precisely they want to do this. They precisely want to do this and to maximize regions of absolute stability or regions of stability on the negative real axis, right? For as large as possible. And then you see this kind of pattern that they did. And the focus at that time, I believe, was to solve, let's say, parabolic equations where you know the spectrum. So the regional stability would be something like this. Yeah. Right? And then this was later generalized by, I think. This was later generalized by, I think, at least one of the very beautiful papers written by Asiar Ghul involving runa colour method, very beautiful theory that kind of enlarged this region of stability. But these are determined, this region is determined by the algorithm. So Mr. Gd is determined, is driven by the features of your data. So, okay, so could we say that what you're trying to do is given some interval on a negative real axis, On the negative real axis, maximize the data. You can try to say that, yeah. Okay, I have a spirit that just solved that problem. Oh, okay. Great. Strangely enough. Okay, then we should see. Yeah, we should solve that. Yes. Yes. So you so