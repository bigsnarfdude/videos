Our next speaker is Tatiana Sherbina from the University of Wisconsin-Madison and she's going to tell us about finite rank complex perturbations information. Thank you. First of all, I also would like to thank the organizer, especially for choosing such a wonderful place for the conference. It's a big pleasure for me to be here, and especially on occasion of Tsima Girls Day. So I'm going to talk about random matrices, which generally is not directly related to Not directly related to your field, but at least some universality random matrices related to these universalities, and at least I have some universalities in my talk, so I hope that you will not too far from the general topic. So the model that I'm going to talk about has the following form. So generally it's a deformation of a matrix H, which is taken from some classical. From some classical randometric ensemble with a certain symmetry type, so Termitian or real symmetry, and gamma is a positive deformation of a constant franc M. So for the most classical random matrix ensembles that you program to short about, such as Gaussian random ensembles or Wigner matrices or Bidrand samples, they are unitary invariant. So you can proceed with the unitary transformation and then gamma can be reduced to the And then gamma can be reduced to the form when you have just diagonal elements. And that is how gamma will be considered during the talk. And moreover, I'm going to restrict your attention on the case when H is actually Hermitian matrix and M is equal to 1. So we have a finite random deformation of order 1. That is the main topic that I'm going to consider. But most results that I'm going to talk about will be. Going to talk about will be also available for general fixed M. So, if we speak about generally the final rank deformation of matrices, if it's not a complex one, then this type of model are well known and well-known studies in random matrix theory. So, in this situation, so if you have H to be such some classical random matrix, then if you take gamma to be a function of the If you take gamma to be a finite rank, it somehow slightly pushes the spectrum to the right. And the interesting thing there, in general, so there are also the interesting results here. The most interesting thing here is that depending on gamma, so on the value of your deformation, it can be that it appears some outlier of the eigenvalue, and that is what we heard in a previous talk. That is called the BBP transition. A transition. And that comes from the pioneering broker by Beneraus species that actually considered not that kind of deformation. They considered a multiplicative deformation of the sample covariance matrices. But after them, there was a couple other works that did that for many other classical endometrios. That's not a full list, just a few of them. But that's really pretty well enough. So if a pause. So, if you return back to the anti-termitian deformation of I gamma, then the situation becomes much much different because now the metric that you have is not Germitian anymore, so the eigenvalues are not real anymore. So in general, they are complex. So, however, in contrast to the classical non-termition matrices, such as, for example, Geneva and Cyber, so the matrices with IAD Gaussian entries without any kind of D-Gaussian entries without any kind of symmetry. In this case, if m is fixed and m is large, then these matrices are weakly non-transition. What do I mean by that? It is straightforward to check that all the eigenvalues of the matrix HF that you consider, they look like that. So you have lambda J of H, which is the eigenvalues of the ensemble without perturbation plus, small perturbation with a positive image. With a positive imaginary part. And moreover, if you consider, for example, H to be the standard Gaussian unitary ensemble, then we know that eigenvectors are uniformly distributed along the sphere. So it is natural to expect that this that we consider here are of order this one. So psi j here is a j's eigenvector of the ensemble, and E11 is a matrix with 1 at element 11. 1 at element 1, 1 and 0 everywhere else. So it means that, so that guy, we know that psi j 1, 1 is of order 1 over square root of m, so product we get 1 over m. So you see that it is natural to expect that the imaginary part of all this j are of order 1 over m in general. And actually, that is indeed the case. So one can prove that for Gaussian human, for example, For Gaussian human-turning sample, or more generally, for ligner matrices, so the matrices with the Germitian symmetry, but distribution is not necessary, so elements are independent, but distribution is not necessary in Gaussian, one can consider the eigenvalues of this deformation, and generally all eigenvalues will lie so the spectrum of the Gaussian unit rand sample as well as the Wigner matrices are minus to two. Pigner matrices are minus to two, so without deformation. So if you put the deformation, then eigenvalues will be just under the interval minus to two in the complex plane, and their imaginary part will be, for almost all of them, they will be one over n, of order one over n, and it can be that there is again a one outwire that depends, the presence of this outwire depends on what is the value of gamma. What is the value of gamma? So, if the deformation that you had, so once again, I'm talking about rank one deformation, that is when we have only one outliers there. So, if gamma is smaller than one, then we don't have any kind of outliers, but if gamma is bigger than one, then there is one eigenvalue whose image and repair actually of a constant value, so it's much larger than general form of one over m case. And that work again was obtained. Again, it was obtained. That's already for some ensembles, it's well enough studied for fixed gamma. So, and even the transition around gamma is been in that recent paper of Douglas Erdos and Fyodor Harjankopopovsky. They even study what will be the transition of gamma. So, you take gamma falls to one depending on n. So, the question. Depending on n, so the question is: how actually close should be gamma to one to see that outwire? That is one of the questions that generally is very interesting here. So, apart from the mathematical curiosity, actually that type of deformation has a physical motivation. So, in physical literature, the eigenvalues of such kind of matrices with anti-transmission deformation are associated with a zero. Are associated with the zeros of the scattering matrices, and so the convex conjugates are actually the poles of the same scattering matrices. And so certainly if you want the complex conjugate of the eigenvalues, it will be the same as you just change the sign in gamma. And in physics these poles are called resonances and their distribution generally is very important questions. Is very important questions. And that starts from the opinion work on Gerba, Schwartzworth, MÃ¼ller, and Zimbauer. And then it was a lot of physical work that studied that topic. So it's really important topic in scattering theory. So in this context, from the physical point of view, one of the main questions that you can ask about that, so actually a resonances is the imaginary part of eigenvalues that you have. So one of the main questions you can ask is what is the distribution. You can ask is what is the distribution of this imaginary part? So you take n eigenvalues, you consider the imaginary part, and we are interested, what is the distribution of this imaginary part. And let me emphasize one more type, that actually the most eigenvalues imaginary part are of order one over n, so you need to rescale them before you start the same distribution. So as I most of the results about solar. are some results about so if H is GUE there was a plenty of physical results that was obtained in the early 90s and then a series of them on different level of recursion like that and I also want to so why GUE is better so GUE is better because that is a determinant-to-land sample and generally even with so even with the complex perturbation one can actually write Actually, write a joint distribution of the eigenvalues in the determinant of form. So, somehow, this in a very deterministic structure is also certainly present in the beta end sample. And somehow, even with the complex information, you also can do that, at least for rank one perturbation. That was done, so the joint distribution of that eigenvalues was obtained by Kojan with different coarters. With different co-opters for different kinds of B2N samples. So, circular, usual B2N sample, circular samples, and many others like that. So, as I told you, that matrices that you consider now are non-Chermitian. That means that, in generally, the standard method from the random matrix theory are not works in the full strength here, say, like that. So, what is what that is? So what is what that is why, generally, to study such kind of ensembles, you need to combine methods from the Hermitian or symmetric matrices with the real eigenvalues with the method of studying non-Hermitian eigenvalues. So that method, actually the only method that random metrics theory has currently, that's a method that was proposed by Gerbohn. It based on this formula that if you have some function and you If you have some function and you integrate with it with log, then and take log action, you get back the function. So if you introduce matrix x, which is now h minus z, h minus z star, then for the if you're interested in a linear eigenvalue statistics, that's simply a function. So zi is the eigenvalues of your random matrix that you have. You compute the sum of h of z i z i bar, then you certainly can write. Then you certainly can write it using this formula this way: that you multiply it by bars twice, and you get this formula. So you can see that if you're interested in the behavior of this guy, you actually need to study this guy, the logarithm of the tutorial. Why it is much better? Because X is a Humission matrix, and so all the methods of random matrix theory are in your possession. So you can study that one. So, in particular, if you take the expectations. If you take the expectation of this linear eigenvalue statistic, you can write it in this way. And so, if you take, so suppose Zj is eigenvalue, so you define the density of x and y to be just average, that is certain average density because we have expectation. So, I probably have to emphasize that expectation that I'm taking is expectation with respect to h that I have. So, deformation is fixed deterministic, so expectation is just. So, expectation is just on the random matrix you have. So, if you have this density, then it actually can be obtained from the expectation of the flow of the determinant by taking again the Laplacian. That is the idea, how you can access this density. Generally, and once again, as I told you, you need to rescale this density first before study because most eigenvalues are for the imaginary part of eigenvalues are for the one over n. So, what we do, we just rescale. So what we do, we just rescue the imaginary parts by n times the density. So Ï here and everywhere else is the limits and density of the eigenvalues that we have. So after that you define this density and so this certain joint density. So x is a real part here. So y is a image on the real part and x is a real part here. So what we are interested in, we are interested in the limit of this density. In the limit of this density, as the size of the matrix goes to the thinnest. That's more or less what's going on. So, starting again from Fiodor of Sommer's Rock, so that is what they propose. So, studying this guy is actually also not so simple because the journal can be zero, so you have some problems to work with. So, sometimes it's So sometimes it's more easier to find instead of this exponent of war that we study the ratio of the two determinant. And then expectation of war can be of density derivative, sort of remain of a certain kind. So if you have this one, then the density you're looking for. So this E now is the energy on a real line that you have, and Y is the corresponding rescaled image of R. Can be obtained from. Can be obtained from Zn by certain derivatives of different kinds. So why that one is helpful? So it's probably not helpful for any kind of technique, but there is a technique that can work with the ratio of the determinant. That is what is called a supersymmetry technique. The idea of this technique is the following. You write the determinant in bottom as a usual Daushan integral. As a usual Gaussian integral, and the determinant in the stroke is kind of Gaussian integral, but that is Gaussian integral is in what is called Grassmann variables, so anti-computing variables. Again, I don't have much time, so I'm not going to talk much about that technique. I just show you, so the idea is that you represent both determinants as an integral of certain Daussian vector. One is just the usual complex vector, the second one is the Russian. The second one is the Graspan vector with anti-commuting variables. After you can perform different kinds of algebraic tricks and integrate out all the anti-commuting variables, and what you left with is just the usual integral representation in the complex variables. So, for example, for Gaussian unit random sample, if you consider this deformation, what you will get will be the following thing. So, you have that is, you will see. That is, you will see you have four variables. Two of them are just real variables from minus infinity to infinity, and two of them are only human circle. So it doesn't matter much what is the function fin. Here are some functions. So as you can see, that is a classical saddle point analysis problem. The only problem that generally is not so easy because you so settle point analysis here is rather involved, you need some complicated. And both you need some complicated conclusions and stuff like that. But, and also, because of n to the 4, you actually need to expand quite a lot here. So it's not an easy task, but that is just a technical deal, how you can obtain that. And that is actually water to some extent of vibrosity, as Fyodor van Somers did. So and what that is the result that they they actually obtained. That I actually obtained. So, if you consider the limit of that, guys, so you compute the z and then you compute the all necessary derivative, what you will get is a certain density limits and densities that looks like that. So, it's a very simple formula. And this tau, the parameters that you have here, is of the following form. So, it's this gamma plus gamma inverse times something that depends on the density here. So, and for So, and for cases they have, this wasn't a usual wigner semicircle, so that is this density of this case. So if you have this joint density, so density in y, that certainly depends on e, you certainly can compute, for example, the fraction of eigenvalues that lies above certain level. Yes, so what you need to do, you just need to integrate it. So you take the integral from y to infinity. So you take the integral from y to infinity of this density and you integrate it with respect to dE where that is semicircle density, it's distribution by the semicircle. So what you will get will be the following expression that you can see there is no density anymore here, so that kind of object that depends only on gamma and this Iy is a modified Bessel function. So you get expression and generally there was a physical conjecture. There was a physical conjecture that this density should be universal in certain sense. And the sense is that you actually, if you take any kind of random ensemble of random matrices that has a local behavior SgV, so that is called universe statistics, then, and take deformation of that guy, then what you will get will be the same thing. So that is the result for Gaussian-Jimmy ensemble. But there is another very rather involved ensemble that is very much interested in physics. That is what is called the random band matrices. So what is that? In this case, we also consider Fermi symmetry and independent entries, but now the entries are not the whole. Entries are not in the whole matrix, as for the Wigner case, but in the band that near the main diagonal. So, I want to emphasize that this ensemble is not unitary invariant. So, generally, when you consider the deformation, you cannot assume that it is gamma 1, gamma 2, and so on on the diagonal. But we still use that one. And that is what is called non-min field random matrices. That means that they have a special structure, spatial structure. Spatial structures. So they are in general much more complicated than the regular ensembles, such as sample covariance, GV, beta ensembles, and even regular graphs and stuff like that. So if we speak just about that ensemble in general, so first without deformation, then what we know is that if only the double, width of the band goes to infinity, then the dense. Goes to infinity, then the density will be the same as for Gaussian unit and cell. So it's still in your semi-soccer. But if you look on a local statistic of eigenvalues, so you consider the correlation function of eigenvalues, then this correlation function will be different. And the change is conjectured to be: so if W bigger than square root of n, then this matrix in a local region should behave, should have wigner-based statistics. So it's the same as Same as GV matrix. But if W goes to infinity, but slower than the square root of m, then you have localization and Poisson statistics. So generally, so in general, the conjecture is still open, but there is a model where it's actually proved already. So one can consider instead of this matrix of that form, one can consider what is called the block random value matrices. Block random value matrices. In this case, you consider the block matrix of this site. So, this A are G V matrix and this B are just IID W times W matrix. So, they have IID Gaussian entries. And the covariance in each block is just of this form. It is 1 minus 2 beta on the diagonal, and it is beta for B1, B2, and so on. B1, B2, and so on. That's more or less like go. So, certainly, if you have n blocks in a row, then the size of the matrix will be double times n. So, if the conjectured here, the transition should be on square root of n, where n is the size of the matrix, then this here, the transition should be near W of order. So, W is the width of the band or the size of the block, n is the number of blocks in a row that you have. That you have. And the transition should be here. And for Gaussian case, for this model, the conjecture is actually proved. So it's our result for case, random matrix case where W bigger than square root of n. And also recent result of Post-Then is that actually if W is smaller than square root of n, then we have localization and Poisson statistics. So for the general distribution, it is still not optimal. It is still not obtained. And generally, so somehow what we did here was only for the second correlation function. If you can do it for more for any correlation function, then it is possible to apply what is called the four-moment theorem to match that with the general distribution. So, in principle, it is possible from that result, it is possible with improving some bounds to get that result for the That result for the general distribution with the Fog Auschm moment, but even that we cannot do at that point. So now, what we are interested in, okay, so let's now have this bank matrix, let's take the deformation of it. Then we can apply the same mechanism that we did for the GUE case and do what we get. So similarly to the GUE case, one can write an integral representation and do all this stuff. And do all this stuff. So, the problem with this representation is that it will be much more complicated than the Gaussian case. So, that is why, before considering the model in full, physicists consider what is called the sigma model approximation. So, the representation that you get if you do that will be very much like a statistical mechanics model with a complicated spin, and this spin are, so you may Spin are so you may think about Iseng module, Iseng module, or Heisenberg model, or whatever model of statistical mechanics, you're pretty sure. But in this case, the spins instead of plus minus one, like an Isink Z will be 8 by A matrices with 4 by 4 matrices with both commuting and anti-computing variables. That's too much. So there are certain techniques that allow you to extract the important symmetry from that. The important symmetry from that matrices. And that is what physicists call going to the sigma model approximation. I will try to explain a little bit that. So mathematically, that means that, so let me remind you that we have covariance here in this optiogonal block to be beta over W. So the idea is that instead of beta over W, you put here beta over W square. And then instead of putting both and W to infinity, Both and W to infinity, you first put W to infinity and then B and beta and N fixed, and then beta and then both to infinity. That somehow simplified the wife into it because and that is what we actually did for the initial band matrices. Here, the same technique curve, but certainly you need some modification, and actually it's not so simple modification because of this non-Hermitian component. Of these non-terminal components that you have. So, what we finally can prove is that if you consider this ratio of the determinant, then in the limit as w goes to infinity, it comes to what is called the sigma model approximation. And for this sigma model approximation, so the simplified model of the real model that we need, to prove, we can prove that actually in the regime of the localization, so when B. The localization, so when beta is bigger than m, we indeed can obtain the same density as Ferdinand-Somers obtained, which that is the main result. So, actually, on a physical level, the quantum part is also known. So, if you consider the localized regime, the density of the imaginary parts will be completely different. So, that also done in the sigma model, and even the sigma model, it was done in non-inverse ways. So, it's really kind of just Really, kind of just, but anyway, it's kind of interesting to see what people's answers there. So, let me show you how general that kind of models looks like. So, what is a sigma model in general? So, this QJ are super matrices. So, all these super trace and super determinant is certain analog of usual trace and usual determinant for super matrices. So, super matrices is a matrix which, as I told you, contains both real and Contain both real and complex variables and anti-combutant variables. And so that is indeed looks like a statistical New Keynes model. You have beta over 4 here. So you have a nearest neighbor interaction here. You have some external field. And that's how that works. So once again, going to sigma models. So previously, Q has a more complicated structure. The idea of a sigma model is that you somehow diagonalize Q, integrate out. Analyze Q, integrate out the eigenvalues first, and then consider only that symmetry type of Qs. So, for example, if you have a Hermitian matrix, then the sigma model approximation means that you de-analyze it and consider only the unitary symmetry on the sphere. So, if you fix eigenvalue and consider only the unitary symmetry. Same thing is going on here. And in this case, you just again get some statistical mechanics model that can be analyzed using the transform. Analyze using the transfer metrics approach, that is why we actually need this nearest-nabbo interaction, and that is why we need this block structure of that type. That is the cause of that. So, once again, if you consider this one, you can write that as a product of as an action of certain integral operator with a kernel that looks like that. And then you actually need a spectral analysis. You actually need a spectral analysis of this die, which is quite complicated once again because the Grassmans or anti-computing variables that you have inside. So generally, if you have so each Grassman, so this K, you cannot analyze Grassmann variables and commutes. So you need to reproduce it and action on a certain polynomial space. So for each kernel like that, the more Grassmann you have, the Grassman, you have, the so if you have just no Grassman, then K will be just operator on some L2 space. If you have two Grassmann, for example, that K will be four by four matrix, which each element of them is some operator on L2 space. In the full module, you have actually eight Grassmann, so that means that your K will be 256 times 256 matrix, all of elements like that. So that is. Element like that. So that is, and a sigma model approximation is just 6 by 6. That is why it's much better to study that first. Okay, so that is more the idea. And the last thing that I want to say is that returning to the conjecture about the universality of this measure of image layer. But so bad matrices are very interested for the physical point of view, but not very interested in the sense of checking the connection. Of checking the conjecture because they actually have the same spectral measure, so it's still linear semicircle. So, if you want to check the conjecture, the most reasonable thing to study is the sample covariance matrices, because they have a different measure. So, if you take sample covariance matrices, which is simply you take rectangular matrix with IAD element and consider x star x, then you again do the same thing, and you assume that the size of this matrix m times n, it goes to. M times n, it's close to sort of a constant. Then it is well known that the limits in density is a marginal robuster law. That's written here. Then actually you also can do the same stuff and you can compute the density and this appears that it really looks almost like that. And the only difference that actually you have this gamma over E here. So previously we have gamma plus one over gamma and it was it depends only on density. But in March-Kopostu case you But in the Marshian Composter case, you have this key here. That was actually a big surprise because physicists told that it should depend on the density. So I talked with the young theologian, and he told me that it's completely impossible to have that. But in two days, when he sees, he gave me a new conjecture. So now let me formulate this one. So the new conjecture by Jan is the following. That actually, if you take any kind of human. If you take any kind of Hermitian matrices, then the density that we will get here will be indeed the same formula that we have here, like that. But instead of just gamma plus gamma inverse, you also have a parameter r of e here that actually the limit of the diagonal element of the green function. So, roughly speaking, it is known for most models. It is known from most models of random matrices that the limit of this average of the resolvent element of the group is actually the Stipsy's transform of the limiting measure. So that is why we can check actually the conjecture for these cases. So for GUE case, the Stephius transform is the following. So its absolute value is just one, and that is why we don't have R both here and in gamma, and we have this thing. This thing. But if you take the Martian copper rule, then the CGS transform is also something about no, but it is different. And its absolute value is 1 over the square root of E. And that is why actually we get this gamma over E plus 1 over gamma. So you see that indeed for matching Capacity 2, it depends not only on the density, it depends on the whole absolute value of the... So density is just the imaginary part of CTS transform. Imaginary part of Stuttgart's transform, but it actually depends on the full absolute vector of the Stuttius transform. But in a certain sense, you may consider it still you have the same formula, so it's still universal of certain kind, but not that kind, the physicist or previous kids like that. Okay, so thank you for your attention. So, this density, does it change? Like, when you say density, do you mean that it changes as you move around the spectrum or you average over time? Yeah, so as I told you, that one, so what is that one when I told you for Gaussian? Yeah, so that one, yes, it is changed with the density E. Yeah, so you fix E and you consider the density under, say like that. But knowing this one, you can compute the general density. You can compute the general density by just integration, and that density that you have here. So, you just compute the number of eigenvalues that exceed the level y. Yes, you just take this value, integrate it over E, and that is what you get. So, this density is the general density of image and parts. So, it's indeed just the density of you take all eigenvalues, take image of mirror parts, and get the density. It's not depends on energy that you have here. The second order is no way how it's universal, right? It's only the first order, which can. It's only the first one which has any chances to win. Yeah, so that one, no, if it would be like it was considered previously just density here, then when you change it, so when you scale your level by density, it would be reversal. But with this energy there, it's and in your result you don't get any averaging like over the horizontal point, so there's just a really stop line. 