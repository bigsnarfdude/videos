My talk will be far away of mainstream of this conference, whatever it is. Okay, so this will be the joint work with Piot Naier, who is here, and my former PhD student, Marta Seletska. Okay, so so I will be talking about moments of random vectors. Of random vectors, and I will consider two questions. So, first, okay, we have a random vector in Rn and we have some norm. So, one thing when we can think about the moments is the so-called strong moments, so that's the usual L P norm of the norm of X. So, you know, a norm is supreme of all functionals. And the other thing related to the The other thing related to the strong moment is so-called weak moment. It is when we make this supremum outside of the parentheses. So we take supremum of LP norms of functionals applied to x. So obviously this thing is smaller than this thing. I think this is trivial. And okay, so I have a remark that instead So I have a remark that instead of looking at norms, one may look at the supremum of linear functionals. So in other words, supremum of linear forms indexed by the certainty. And I will formulate rather the results in this language. Okay, so the question is, okay, obviously the weak moment is is smaller than the strong moment, but the question is what what if you would like to to to go for the opposite inequality? Go for the opposite inequality without an assumption of the vector x. So we fix dimension, we fix p bigger or equal than 2, and we are looking what is the smallest possible constant such that for n x we have this comparison. Okay, so maybe, okay, I'm sorry, this is counterintuitive. Okay, so in another language, So, in another language, we may fix this to be 1. So, if this is smaller than 1, it means that this inside script m of x, it is that this is smaller or equal than 1. Okay, it should be smaller or equal. I don't know why I put it. So, it means that this norm, this supremum over t, is smaller than supremum over this set, and this is so-called script ZP norm over. called script Z p norm of a vector x. And our goal is to look what is the upper bound of the L p norm of this Z p norm of X. So this is equivalent, these two things. Okay, so this norm and these unit balls in this norm appeared earlier and were studies intensively for people working. For people working in convex geometry. So they were introduced, I think, at least in 1997 by Lutfack and Jank under slightly different normalization. And let's look at how do they look like. So for isotropic vector and p equal to 2, this is by definition almost the Euclidean ball. If we start our Gaussian, then ps moments grow like square root of 1. Grow like square root of p, so it means basically that this is comparable to Euclidean norm with radius square root of p. Then, if we have, for example, products exponential measure, then this is a combination of L2 and L1 ball. If we have Rademacher or uniform distribution on the cube, either discrete or irregular, then this is When this is like Gaussian, so Euclidean bulbi, but restricted to the unit cube. And in general, okay, if we have a symmetric log concave distribution, then basically this ZP ball is nothing but the level set of rate function that appears in large deviations. So we go. So, we go for the logarithm of Laplace transform and we look at the Legend French transform of this and look at the Laplace sense. So, everything of this is pretty easy, but just to show you what is this thing. Okay. So, let's first look at the most trivial case. So, so what would be the most trivial case? One would think that independent, but apparently the less trivial uh and The less trivial and easier to compute is the case when vectors are rotationally invariant. So, if any rotationally invariant may be represented in a form uniform distribution on the sphere, time independent distribution on the radius. And if we compute the moments of linear forms of this, then for uniform distribution. When for uniform distribution, this is obviously proportional by rotational invariant to the PDM norm, and the constant of proportionality is L P norm of the first coordinate of uniform distribution, which behaves with growth basically like square root of p up to the moment n. And L2 norm is 31 over square root of n, so this is like this. So if we compute just L P norm of X, then we should. Then we should multiply this u by something independent, so this something independent would factor out. So this will be proportional to Euclidean, so obviously the dual will be proportional to Euclidean, but the constant of proportionality would reverse. And we have the formula. Okay, so this Zp is these two constants times the Euclidean epinom of Euclidean sink. Euclidean thing, but okay, this thing where if you compute some, if you compute, okay, what is what is the Euclidean norm of x? This is just r. So this thing will cancel out with this, we will get reverse of L P norm of U1, which is reverse of this number. So square root of n by p basically, up to the level P. Okay, so, and for rotational invariant vectors, you see that everywhere there were equations. Yes, everything was. Yes, everything was just formulas, so the only thing when we estimate was at the end. Okay, so the question, natural question is, is the general case more complicated or just the rotationally invariant vectors are the things that saturate the maximum. And it turns out that exactly the maximum up to a constant The maximum up to a constant, and we don't know if this constant is one, but up to a universal constant, rotational invariant vectors are the worst one. So, if we would like to saturate the largest possible constant, we should just look for the class of rotational invariant vectors. So, and this constant we were able to compute is 2 square root of E. Okay, so natural question is: okay, so suppose that our Question is: Okay, so suppose that our distribution is not rotational invariant, maybe this is just equivalent to this. And it turns out that no? Okay, so if you look at the very simple example, namely you take plus minus EI, plus minus versals with probability, uniform probability, then you may compute this thing, and it turns out that the constant will be. Constant will be a piece root of n. So much, much smaller than square root of n by p. So, so, so, so, so, so you see that one cannot hope for a lower part here, of the same order. Okay, I will not torture you with the proof. Okay, maybe let's say two minutes about the proof without writing anything. So, for p equal to two, everything is trivial because we are Trivial because we are in L2 theory and all norms are basically Euclidean norms. So everything should be maybe computed and it will be equality here, like square root of L. Then our idea was to make some equivalent formulation of this, but basic idea was to change the vector x and X and increase the dimension but decrease P. So one may basically increase the dimension to n choose P and make this to norm L2 and then use estimates for L. Okay, so this is very very rough idea without going into details. Okay. Okay so so so what what was So what was my motivation to look at this problem was so-called the question about concentration in the spirit of Talagrand two-level concentration. So first the classical results about Gaussian concentration, one may state that if we enlarge a set by Euclidean ball with radius square root of p, then the measure of complement will decrease by p to the negative p. At least if the set is is. At least if the set is at least one-half. So Talagrand proved that the same is true for product exponential measure, but obviously this is not side to measure. So one cannot hope for just Euclidean concentration, but one should add only a small thing, namely some multiple of L1 ball. Ball. So this is usually much smaller than this. Yes, but in random direction. Okay, so if you add, you take this, so this is so-called two-level concentration of taragram. So if you add this combination of these two balls, the same phenomena holds. So if you remember when I was talking about these ZP balls, this was ZP ball for gamma n, for Gaussian, this was Zp ball. For Gaussian, this was that people for exponential. So, natural question is, okay, so this is how to have the form that we enlarge the set by the multiple of these Z people, and we hope that the measure of complement will decrease drastically. And the question was, is this more general phenomena? So, for what which other measure this this is true? this this is true. Okay, and actually one may easily show that this is the set we would like to we cannot enlarge by a smaller set, namely if we just have the same phenomena that the measure of the complement decreases exponentially and we apply this only to the half spaces. So so very very simple set then uh this this and we this this and we enlarge by a context set this this must contain this this Zp. So Zp is the smallest possible so this is why why with Voitastric we introduce the name of optimal concentration because somehow this is the smallest set we can add. And the question is okay which other measures satisfy this optimal concentration. So we were able to prove that all products That all product, log concave measures, satisfy it. And obviously, one needs to have some regularity of a measure. So, the natural class to look for is the class of log-concave measures. And application of our result with Puyoped enable us to say that this conjecture holds, but with a dimension-dependent constant. with a dimension-dependent constant which is n to the power 5 over 12. So we are quite proud that we beat square root of n, which is maybe not totally trivial, but rather easy. But still we are very far from the K less when we can get first root of the best possible of Lienpm power. And okay, here the constant must Here the constant must be bigger than in colours, because it's uh this Z people always contain uh a multiple of Euclidean balls also so uh or is contained in a multiple of Euclidean balls. So so so so this optimal concentration implies colours. So one cannot if we go anything smaller than force root of n we will make the world record. But but obviously it it is I think impossible by the null the non code. By the noun code. Okay. So Lehman Pala did not give you the optimal concentration? Sorry? Didn't Lehman Paula? No, no, no. I didn't consider this question because. In the second paper, no? No, no, no, no. So if I had some concentration of Richards functions, but with respect to Euclidean distance. And here, you know, even for exponential measure, it's not Euclidean distance. You should take into consideration both Euclidean distance and L infinity. Euclidean distance and an infinity norm of a gradient. So this is something more complicated. Okay, so then I would like to mention another application to P-summing operators. So I know in the room there are specialists in this subject. So the definition, I think, okay, if you know, you know, if you don't know, probably after three talks, you will. you will it will be har hard to to find. Basically, okay, if this is similar that what you are putting out outside the supremum, so suppose that t is just identity. But this is as before. Here you have supremum over functionals, here you have supremum outside. Yes, and here is the general operator. So this is so-called P-summing operator. This is important important important notion in the theory of operators and it was studied in 70s and 80s and in particular if one may ask about what is this P-summing constant of identity when it is called the P-summing constant of a Banach space. So this makes sense only, oh sorry, only in finite data Also, only in finite dimensional spaces, because it's it's pretty easy to see that if this constant is finite, then the space must be finite dimensional. It's not hard to see that p2 of f is just square root of dimensions. If we look at the other p-summing constant of Banach spaces, they were computed only in few cases. So, one of the cases was done by Joram Gordon. So, he computed P-summe. So he computed P-summing constant of Hilbert space. And you know, if you remember a few slides before, I also had this uniform distribution and the first coordinates. So this is exactly the same thing which appears in this, the best constant for rotational invariant vectors. So this square root of n plus p by p. Okay, so now, okay, since this is exactly the same problem, yes, if t is Exactly the same problem. Yes, if T is identity, here we have suprema out inside, here outside. And also, you know, instead of taking some, you may integrate with respect to uniform distribution. So when you apply our result to the uniform distribution on a discrete set, and you get the following consequence, that for any Banach space, if you fix the dimension up to some uniform constant, the piece of constant, the P summing the P summing uh constant is saturated by by uh Hilbert space. So so the the worst P-summing constant is for the Hilbert space. And we don't know if this constant is one or not. From our proof is something like, I don't know, maybe four square root of V or something like this. Okay, and okay, standard things, we may any opera, any finite rank operator is a compact operator. Finite rank operator is a composition of identity with some inclusions, so we have also a bond for finite rank operators. But this is just an unusual thing. So how am I doing? I have 10 minutes. Okay. So this was about the the work with Piot and now let's let's go to another thing. Uh okay, so let's go back for a while uh Let's go back for a while to Gaussian concentration and to Gaussian vectors. So, again, this is another formulation of Gaussian concentration, that any Lipschitz function concentrates and the talis is like Gaussian with the variance square root of square of Lipschitz constant. So, if you do integration by part of this, it's easy to see that L P norm of this is like bounded by L P norm of Gauva. like bounded by epinom of Gaussian that is bounded by square root of p times l or if you use the Triangle inequality you get that for any for any Lipschitz function its L p norm is bounded by the mean L1 norm plus the square root of P times the Lipschitz constant and let's apply it to to to to to the norm so so if you have a supremum of if you have a norm it's Uh uh if you have a norm, its Ipshis constant is just supremum of Euclidean uh norms of a dual ball. So then if you if you notice that also inner combination of Gaussians is again Gaussian, then this L P norm is proportional to the Euclidean norm of t and the constant of proportionality square root of p. Then if you apply this thing, then you will get so-called comparison. So-called comparison of weak and strong moments. So here you have LP strong moment, here you have LP weak moment, and you know that there is a uniform constant in front of it. And the only price you have to pay for it is that you have L1, L1 strong moment. So the expected value. So up to expected value for Gaussians, weak and strong moments are comparable with uniform constant. Uniform constant. So again, the question is: is this phenomenon only Gaussian phenomena or it holds for larger class of distributions? And so this is my favorite question since, I don't know, 10 years, and I didn't succeed much, but I will comment a bit on it. So so this is another formulation. So for the Gaussian, you had here C one equal to one. Here C1 equals 1. But you may ask, okay, take some other vectors when we have the same thing, that strong moment is compared to weak moment via minor or median or something. Okay, so this thing in another language for Radamacher sequence was studied by Dirwalt and Mortomeri-Smith. And Mortgo Marie-Smith in 1993, and they proved that really for Radha-Machel you have this. So they proved it with two constants, actually for Radha-Machel, one may make C1 equal to one, but this requires a bit more subtle tool, but not more. Okay, so then I noticed that using this Talagrand two-level concentration, you may replace Radamacher or Gaussian here by MA, by MA symmetric independent. By any symmetric independent random variables with low concave tails. Then, okay, then Sheleski and Tomek Koch proved that actually for such random variables one can get C1 equal to 1. And everything here is based on concentration. Okay, so then we generalize it with Tonek to to slightly slightly To slightly more general class of distributions, when we have that LP moments of coordinates grows in sublinear way. Then we have similar comparison and the constants only depend on things alpha. And the thing starts to be interesting because that's, as was noticed in the same paper I see a slide before. A slide before, this constant must be strictly positive if this alpha is large. So Shaleski, Sheleski, and co-constructed an example when they showed that one cannot have this constant one for some regular distribution. Okay, so with BATA, we were able to somehow almost characterize, as you will see on the next slides, the vectors by. Slides, the vectors but with independent coordinates when we have this comparison of weak and strong moments. And it turns out what really matters is that if you, you know, the L P norms of such coordinates of these vectors cannot grow too fast. They should if you multiply, you compare L to P with L P, there should be uniform alpha here. So So, one of the examples is Weibull distribution. And if you look at the Weibull distribution with r smaller than 1, with param then, the tail of Weibo is again like e to the negative t to the r, so it's not exponential, it's hard tail. So, it doesn't have exponential moments. And Matel Goslam proved that if you have dimensional free concentration inequality, then you must have C1. have C1 or exponential behavior. So this is not exactly concentration type results. I mean we use some concentration inside the proof but these random vectors with such coordinates doesn't satisfy any dimension free concentration. So this is something something in slightly different direction. But but okay, and one may ask okay if this condition is optimal or not. Condition is optimal or not? And it turns out that it is. So, in the following sense, that if we have this comparison of weak and strong moments, and we take IID random variables, so we take IID copies of some random variables, and this constant does not depend on how many copies do we have. So, dimensional dependent, then we must have. Dependent, then we must have this growth of LP norms I showed on the slide before. So somehow this condition, at least in the IID case, is optimal, is characterization. In the general, we don't know. But the, okay, so everything I talked so far in the second part of the talk was concerning independent situation, yes? But interesting But interesting thing starts to be when the coordinates are dependent. So one may ask what's going on there, and there is very nice and very deep, I guess, results of Grigoris, who showed that for low-concave vectors and Euclidean norms, we have this comparison of weak and strong norms. So with dimension-independent constants. And one important, so there are two important open questions here. One I think is closely related to KLS and some like teenshell estimates and etc. is if one can have here the first constant equal to one. And I think it's very very hard problem, so so probably not so easy, but but the other question is okay. Other question is, okay, what so other question, what other distributions satisfy this? And one more question, what other norms satisfy this? This thing for log concave vectors. So we were able to slightly answer, partially answer the third question. So which other norms one may apply to log concave vectors to have such comparisons? such such comparisons. And okay, so so I have no time to make you that this is this is pretty delicate thing. So if you have like a Gaussian times independent uniform distribution, then you don't have Paulist type result. Then this constant must be like false results. But this is this is I'm running out of time. So this is my last slide. So we were able to show So so we were able to show with Marta that if we instead of Euclidean norms look uh for norms which come from from LR spaces. I'm not using L P because P is reserved for this P. So L R norms or norms that come from isometrical or isomorphic embeddings in L, then we R L, then we have these things, but the constants depend on linearly on this R. So if there will be no dependence on this R, then in particular we could go to L infinity and we know that any Banach space embeds isometrically L infinity, at least if this is finite dimensional, then we will solve the conjecture. So the conjecture is So, the conjecture is the following: that there should be not dependent on R here, and this should hold for arbitrary norm, otherwise. So, if you have any norm on Rn and any load-concave random vectors, we should be able to compare weak and strong moments with uniform constant up to this expected value of X. Okay, so. Okay, so I think any questions to suggestions if this not the case let's think rapidly.