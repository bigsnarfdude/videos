Okay. Thanks for the organizers for the invitation and thank you for staying up to the last talk. I would also like to thank the previous two speakers to give a background about knockoff. So I don't need to go over that and they give an excellent introduction about knockoff. So my title of the talk is Deep Link, Deep Learning Inference Using Knock. Inference using knockoffs with applications to genomics. This is a joint work with my colleague Ying Yingfan, Jin Shi Li, and Dr. Gong. It's also mostly done by my students, Zu Fan Zhu, and who already went to Meta. So anyway, so I give this talk. This paper was published later last year in PIS. Here in PIS. So the main problem we talk about here is variable selection with the false discovery control. And so the two previous speakers already talked a lot about this one. So I just give a very, very brief introduction. And then I will go directly to what we have done. So in the previous two talks, we mostly talk about SNPs or About SNPs or mutations, and then they try to find those genetic mutations that it's truly the cause of the phenotype of interest. We are considering a more general framework, like gene expression or the microbiome abundance, etc. And so in general, we have a feature vector that's Xi for each individual, and that's the P. Individual, and that's the p-dimensional vector. And then we have a response, that's the y. And then here, we have n samples, and then the number parameters can be larger than n. So we want to try to find out which of those features are truly associated with the phenotype of interest. And we want to control those FR and at the same time to achieve high power. To achieve high power. So I will forget about this, and you can see those references are exactly the same, like what they talk about and those knockoff, design knockoff, and the get the knockoff statistics, etc. And how do you get the threshold, etc.? So I'm completely skip this part, and then I will go directly what we did in this work. So in this study, So in this study, we consider these features x to be a vector model. And so that means that xi is a function of these factors, these low-dimensional vectors. And so we related the factors to these features by an unknown function G. And this G can be linear or non-linear or vector. Or non-linear or vector, or so on. And so now we have this, and then this, we have an arrow vector and epsilon i. And then we link those phenotypes using another link function. This yi just equal to the h x i and these are the features. And then there are some error terms. Again, the link function can be linear or no linear. Both P and H are unknown. And H are unknown. And how do we find those associations between those F, the phenotype of interest, or the response variable with those features? And that's what we are interested in. Because we don't know what the function G and what the functions H are, and so we try to use deep learning in both of these steps. So the first step is about the step is about the latent factor model with these features. And to do this, and then we use the autoencoder to try to estimate those features and then design knockoff variables based on those autoencoder. And for this one, for the second one, that is related to the features to the phenotype, and then we use the deep neural network. The deep neural network, and that's a DN, and then we try to evaluate the significance of those features. And then we try to use knockoff, and then we try to see which features are truly associated with the phenotype. So that's the basic idea of this work. So, before we introduce this work, actually, it's my collaborator, Dr. Fang and Li. They already studied. And they already studied the similar kind of problem before. And so in their studies, what they considered is that for the first step, the latent factor model, they considered linear factor model. And so in that sense, they use QCA to find those factors. And then they use these factors, and then they get the error term, and so on, and then they do the studies. And then they do the studies, do the knockoff. And so in the second part, the link function, and then if we know that it's the linear link function, and then they can use Lasso and for no linear link, and then they used random forest to do this. And then after they're doing this, and then they get to the importance of each feature. So that's called iPad. And that's mostly restricted to linear factor. Restricted to linear factor models, and for this second step, this can be linear or no linear. Okay, that's their studies. And what we try to think about is, yeah, because for linear factor model, naturally we use the linear factor model to get that, but it's when we transfer that to the deep neural network, deep learning framework, and that's what. Deep learning framework, and that's automatically get us to the following auto-encoder. Okay, so the basic I already, many people already talked about auto-encoder in the previous talks. And so here you can see we have this for each X, we have this X1, XT, X2, and so on, XP. And then we built auto-encoder. And this is a low-dimensional representation. Uh, representation of those X, and then we do the decoder, and then we get this C1 C2 CP, and then we try to minimize this C and the X. So that's the basic idea of the auto encoder. Okay, so now we do this auto encoder, and then we definitely the question is about how do we design. It's about how do we decide the dimension of this bottleneck layer, and then we will talk about that later. So, anyway, somebody through this auto encoder, we can get those C for this X, and then we can get C, that's the output from this autoencoder. And now with this, and then we can make the difference between X and C, then we can get the error term. Error term. And with this autoencoder, if we choose this autoencoder correctly, we will get the error term will have a much better distribution that it's relatively easy to construct those knock-off variables. So that's the basic idea. So now we have this one and then we get these error terms and then we get the knock-off variable. Get the knockoff variables for this E, for this error term, and instead of the original X. Okay. And so with this one, and then for the second step, for the knockoff statistics, and then we need to see how we can get this knockoff statistics. And the previous two speakers already mentioned, and one way to do that in the general linear model, you can get the coefficient, and then you take the absolute evaluation. And then you take the absolute value of the square, then you can get this. But it's the next question we ask is: okay, so in those deep neural network, how do we evaluate the significance of this thing? And so that fortunately, Dr. Bienven and then they, together with my former students, they developed this one called deep pink. And for those For those deep neural network, and so the basic idea is for those deep neural networks, and then you have the original X, and then you have this knock-off variables, corresponding knock-off variables, and then the most important part actually is the pairing of this X and its knock of copy. So, actually, what they did before actually. Actually, what they did before, actually, first, if you try to just completely use this X and it knock off as input, and then you build those deep neural network, actually, that would not work well. And so the key here actually is if you put this X1 and X1 total, that's the knockoff copy, and then you use a linear model. And then you use a linear model between them, and then you put this into one term, that's the F1. And similarly, for X2 and X2 star, and then you put them together, and then you get those terms. And then you just run the standard deep neural network. And then from here, then you have this coefficient, this F and this is the F star, F tilde. And then F tilde and then up for this neural network and you can for each f and then you can get those the width for this w and actually you can get the importance score for this one and now with this we measure the importance of this xj by zj that's just fj wj and then the knockoff copy is this and the sun then with that and then you can uh just get this just to get this WGA just equal to the GCG score minus ZJ tata score. And so you can definitely use the absolute root value too. Okay, so that's the basic idea for our work and how do we use this knockoff and then try to find the features that I put in. Okay, so basis and then we try to see if those type of method will really give you Method will really give you a correct type of error, the correct of false discovery rate, and also has decent power and certain situations. And so in that case, we simulated the fact model in terms of both linear as well as non-linear model. And don't need to ask me why you choose this function. Just think this is a Uh, just think this is just an example, okay. And so we have the linear model, and then we have the non-linear form model, and that's the factor model. And then for the linking functions, and then we also considered linear link functions as well, non-linear functions. And for this, when we study power, we need to indicate which one is the true signal, which one are the no features, and so on. Had no features and so on. And so then you have this. We just choose a set of true features and then we give it A and all negative if it's equivalent when we have the massive design feature on particular. So for the linear model, it's clear to see that this A is a measured strength of those features. And so for non-linear model, it's trickier, and I will talk about that later. Okay, with this one, so now this you definitely see, okay, so in your deep neural network model, and which activation function we should use, and which pay note function you should use. And so I will talk about that later. Here is what I can tell you is we find you is we find that the aloe that's the aloe magnification factor generally give us quite a decent power as well as already give us the correct pipe by error okay so now we try to see okay so we try to compare the work our new work with the the iPad The iPad. So remember, the iPad was designed for the linear effect model. And so we just see, okay, so we first assume the linear effect model and then try to see which one gives you the best result. And so for this top A and B, and then you can see the red one indicates the iPad. And so for this solid one, Pad and so for this solid one give you the power and for this dashed one that give you the FDR and so under the FDR you could point to and you can see here for both iPad and this the deep link give us the I mean it's at least the FDR it's controlled okay and in terms of power if you have the linear link function that's for the link function that's for the second step and then you can see if both are linear and then the linear the ipad model give us higher power than the deep link i mean that's understandable because the ipad was designed for both linear cases okay and so now if you have no linear link function and then that's essentially just the c and the d the difference between them is the p is this high dimension Is this high dimensional and this is high dimensional on the left? We have the low dimensional. And so you can see here it in this kind of scenario, if the link function between the features and the phenotype are no linear functions, and then you can see here is the power for the deep link is much, much higher than the iPad. For the iPad, if For the iPad, if the naval features are large, and then you can see the power is actually just like the FDR. So that gives you really low power. So that says that if it's the factor model, it's linear factor model. And then if the link vector is also linear, then those we use the iPad is good. But if it's non-linear, then we say that we should use. We say that we should use deep ink. Okay, so now for the non-linear factor model, and then we didn't show, but here is actually the iPad that would not work and the type of the FDR will not be controlled. And the same for the non-linear model, and you can see here it's the same thing, it will only show the result for the deep link, and then you can see. The deepening, and then you can see here is again the FDR is controlled, and then the power is higher, high in both the linear and non-linear case. So that indicates that those are the deep link can work and almost every conditions that we simulated. So, I mean, that's getting to the question about The question about how the R determine the how do you determine R that's the number of neurons in the autoencoder model and how does the misspecification of the number of neurons will affect the results? That's definitely the next question, right? And so we did some analysis and I mean we in this simple simulation we We assume that this factor, the number of factors, actually it's only three. And then we try to see, okay, so if you use different number of neurons and then in the auto-encoder model, what is the first discovery? And you can imagine that when you have the number of neurons in the autoencoder. Neurons in the autoencoder is smaller than the true number of neurons, the true number of factors. And then the FDI is not controlled. It's understandable because the auto-encoder will not be able to completely capture the dependency between the variables between the features. So, therefore, we expect that, but it's when you have the number of neurons are. Neurons are slightly higher than the true level neurons, you are still okay. You still keep, I mean, the FDR will be controlled and the power actually is quite stable. And although some cases is slightly lower. So that's the indication that, yeah, if you slightly overestimate the number of neurons in the autoencoder and it's complete. Okay, but it's don't don't. Don't it don't they lower the number of neurons? Okay, so but then you say, okay, so we actually understand how many neurons do I use? And so what should we see is we try to use the cross-validation to decide how many neurons we should use. And then that's generally give us some decent results, or you can strategate over. They overestimate the network neurons is okay, and we just mentioned so put it a little bit higher, that's pretty good. Okay, so that's about the network neurons. And then the other question that you will ask is, okay, so how many of what kind of activation functions should we use in the both of the autoencoder as well as the At the deep neural network DNA in the second part. Okay, so in that sense, actually, we tried a variety of different activation functions that include ALU, ROLU, and TAN, so on. So the first part gives you the activation function for the auto-encoder, and the second part gives you the activation function for the DNA. And then I. And then I again here is in small number of cases if you use RALU and then sometimes this does not give you the correct FDR. So that means FDR is not that well control. But it's when if you use ALU, then in the simulations that we get, it just gets quite all those FDR. All this FDI is controlled and the power actually is quite good. And you can see the power actually, it does not really, in most situations, all those activation functions give us quite a decent result. You cannot really see the difference except for this situation that the analog give us a much better result. Okay, so that's the simulations, and so the next question here is. So the next question is, what is the time complexity? And so here is just at least for the simulations and we get the time is okay. And so it's quite simple and storage and so on. And those kind of situations. But then the question becomes if the P it's very high and they will your method work or not. So and that's getting to the end. That's getting to everything in stimulation very good, but now we try to say, okay, so for the real data, are you okay? I mean, unfortunately, there are some kind of trick we have to play around. Okay. So for the real data applications, actually we applied to three different data sets. One is two single cell data sets, one is the marine. One into the marine, and then the second is the human single cell data. And then the other one is the microbiome data set on cholera graphic cancer. So because I think I have almost 20 minutes. Okay, so let me just give you one example that essentially give you the basic idea of what we do. What we do, and also give you the challenges when we apply to the real data. So, and our group have been working on metagenomics for quite some time. And then we try to see, okay, try to identify those microbial species that is associated with certain phenotype. In this particular case, we are working with clinical cancer. To clarify the cancer. And we choose one large data set. Actually, it's the 99 cases and 93 controls. And then we map the short read into the NCBI database and then we get the metrics that's the standard. And then we do some normalization for those abundance. So again, the objective is to try to identify the cholera cancer associated microbial species. Okay, so now Okay, so now naturally we try to use those framework directly and then we try to see what we get are readable or not. And because we really don't know the truth, so it's hard to see what you find are true or not. But at least you try to see if you identify those features and then if you retrain your DNA model and train your DNA model and this will your at least the prediction accuracy okay or not and if we directly use that i mean we did the the cross validation and then we directly apply those deep link to the training set and select features and we fix the things and we repeat this 500 times and we calculate each time we calculate it to the misclassification rate and the misclassific And the misclassification rate for this particular example gave us 0.5. I mean, you can see here it's not working, right? It's not working. So quarter mile, turn left on the Monterey Pass Road. So in that sense, it's not working. And then what should we do? So what we did is actually we have another independent data set. Independent data set. And then we try to screen the features. And then try to screen your features. And then we try to say, okay, so after you screen in the features, and then you use DNA in the screen features, and this is not related to the knock at all. And so actually we use the metrics association to screen in the features. And then we try to say, okay, so in the training data, and then what is the And then, what is the misclassification rate? And so, yeah, actually, it's the number of features you screen. The first is too few features, then the prediction is not good. And then if you use the smaller number of misclassification rate decreases and then increases. And so that's where we stop and so here in this. And so here in this particular example, we did it 30. That's we pre-screen the certain features. And then with these certificates, and then we use our deep link model to find those features that are associated with the CSD cholera cancer. And fortunately, some of them actually are quite meaningful. And I mean, I know most of you. I mean, I know most of you probably don't care about this part, but anyway, so that's after we look at this with some clinicians and we did get some meaningful result. And that's the basic thing. What I try to say is that we also apply the same thing with single-cell data. It's kind of same phenomenon. And so here is again is what we do is try to pre-screening. Pre-screening the features, and then we try to see where we should stop. And then, for this particular example of single cells, we pre-screened 200 features, and then we use our deep link to identify these features. So, that's again, I mean, getting to the biology support, and then that's just something that we check whether those are meaningful or not. Something went wrong. Something went wrong. Try again in a few seconds. So, anyway, so that's what we have. And so, the deep link just the feature selection. And then we also, from simulations, it does quite well. But for real data, there are some problems that we need to do some kind of feature screening. So, how do we do this without feature screening? That's another thing that we're still working on. thing that we're still working on trying to see whether it's possible or not and then we also currently working on extending this framework to the time series data instead instead of factor models and try to see whether we can extend this deepening to the time series data. That's closely related to my collaborations with the biologist. Okay, so that's the final part is the software. That's the final part is the software, and then my collaborators and the great support. Thanks for your attention.