Let me introduce the speaker. So it's my pleasure to introduce George Bampalias. I know him very well because he was in Leeds a long time ago. Now he's a professor at the Chinese Academy of Sciences and he's also a thousand young talents scholar at the Chinese Academy. Scholar at the Chinese Academy of Sciences. And okay, so the title of his talk seems to have changed a bit. Is that right, George? Has it changed? On my, what I printed out, it sounds a little bit different, but anyway, so it's compactness and measure in second-order arithmetic. Please. Thank you. Right. So this. You right so this is joint work with one wave from Juan Joe, and uh basically uh we're looking at some principles between WKL and WWKL. So WKL is Wicker and its lemma. Every tree here will be a binary tree and it says that every infinite tree has a Every infinite tree has a path. On the other hand, WKL, Wickwick Kern's lemma, is only refers to positive trees. Positive tree means positive measure. So every tree of positive measure has a path. And we're looking at principles in between those two and separate them. And separate them by building omega models. So the building materials of a model for WKL is basically PA degrees, so complete extensions of PA. Why? Because those reals can find a path through every non-empty Python class. So by iterating, adding Iterating adding PA reals and real devicing, you can build a model of WKL. If you think about, because I'm going to mention randomness in this talk, I just want to make it clear that it's a natural thing when you to consider when you thinking of models for WWKL. Why? Because what the PA reels, the role of PA reels that PA reels play in That PAU reels play in WKL, the same role play Martin Love random reels in WWKL because of a certain ergodicity fact for random reels by Cutera, which says that if you have a random real then and a Python class of positive measures, then some tail of that real must exist in that class, which means that the real Which means that the real has to compute a path through that class. So, what PA reals do for Pfizer on class is non-empty ones, random reals do exactly the same when you have positive measure. Okay, so you think of WWKL immediately, you think of random reals, WKL, PA reals. Okay, and what's interesting is to interrupt the. Sorry, Josh, to interrupt. There was the request whether you maybe could make it full screen. Right, okay. Yeah, I was trying. Yeah, okay. I would do that. I wanted to see some faces so that I know that people are listening. Well, it's up to you. It's just somebody thought it might be better. Yeah, or should I? How about this? Yeah, looks good. This, yeah, looks better. Oh, okay. Well, if you I could change it if you miss this, okay. Let's do it right. So what's interesting is what happens in between. So the idea was instead of looking for a path, a single path through the class, let's look for many paths. That's harder, right? For example, For example, you could ask: given a positive tree, find an infinite countable family of paths in that class. That is probably harder. Or you could ask for avoiding isolated paths. So you want a perfect set inside, but maybe a bit harder. Or you can even ask for a perfect perfect. A perfect set of positive measure. Okay, so these are the three principles we're looking at. This has been written up, there is a preprint, and that's the archive link. Okay, so there are some quite directly related references here. So the first one, this is the paper about... The first one, this is the paper by Chong, Whaley, Wang Wei, and Yang Yue on the computability of perfect subsets of sets of positive measures. So that's where this program started from. And it had quite a following. So there was a subsequent paper by Greenberg, Miller, and Nies, where they somehow connected to the previous. Connected to the previous work by a different rule looking at coverings of open coverings of sets. And a subsequent paper by Yu Liu, which answered some questions from the previous paper about coverings. And a recent paper by Herschel, Jokus and Shu, which they prepared at the same time. Prepared at the same time with us, more or less, the writing up, so we had communication, which is a huge paper, it's more than 60 pages, so it has many things, but part of it relates, is about what we're talking about here. And I will digress a little bit for finitary consequences of these results. And these have a lot to do with a paper by Levin about. 11 about finitary versions of ghettos incompleteness and completions of PA, which he quantified using algorithmic information. And that's our paper. Okay. I just put this table again and again so that we know what's what. The strength goes down as you go down this table. Okay. Okay. So there are two goals in this work. First of all, to establish certain properties of pathways random trees. These are trees that have all of their paths being random. You could say with a fixed randomness bound on the randomness deficiency or not, this doesn't really matter in our context. So we can just say random. We can just say random paths. They're not random trees in the sense that the branching is random. So it's different than what most people in randomness call random tree. This is the classical analog is instead of random branching processes is Poisson random point processes. If you effectivize that, you get this. If you effectivize that, you get this notion of randomness. And mainly to apply these ideas for separating these principles, building models or mega models. So our results are these. The implications are not very hard. The first one was noticed in the paper by CP Chong, and the others are provable. Are provable quite in a straightforward way in RCL. For the separations, we have three. One is missing and it's another question, which I'm going to mention in the end. So we build omega models for separating the last one and then P plus strictly implied. Strictly implies big and the first one. So there is a missing one. Okay. Right. I will avoid talking about randomness, but I may mention deficiency, which means how far the real The real or the string is from being random. So random reals have finite or even negative deficiency. So this is a prefix tree commonworth complexity. And path-wise random trees is basically what I said. All of the paths are random. And if you think about building models for the above separations, at least two of them, then pathways one and three. Then, pathwise random trees are a natural thing to think about. Why? Because when you think of WWKL, you think of random reels as building materials for the model. So when you think about P minus, for example, or P, you would think something like, for example, P minus, you might think something, but it's something. It's something pathways random tree, but with infinite width. So you want many, accountably many, perhaps paths. When you think of P, you think of a perfect pathways random tree, and P plus, you think of a positive perfect pathways random tree. Okay, let's start with the first one. So we want to separate WW. So we want to separate WWKL from that principle P minus. And how do we do that? Well, we need a model of WWKL, which means we have to use random reals. But we want the model not to be a model of P minus. So we don't want these things to compute trees or arrays of random. Of random paths which have infinite paths. Okay, perhaps I should mention some connection between randomness and PA. So the first result was by Jokos and Soar that PA degrees have measure zero. Degrees have measure zero. Okay, so a sufficiently random real cannot compute a PA degree. Then there was a refinement by Frank Stefan, which said that if a random real computes a PA degree, then it has to be complete, which means that to build a model of, to separate WWKL and WKL, basically you can do it with as long as you use incompatible. With as long as you use incomplete random reals, Marginoff random reals, incomplete, not necessarily too random as what the original construction was. Okay, so now what we do is, because we are way below WKL, we say that actually you can replace TA with the property that you compute an infinite array of. Compute an infinite array of random reals. So, in other words, if you have an incomplete random, then it cannot compute an infinite array of random reals with bounded deficiency. Now, this might sound a bit wrong because if you know, for example, van Bargson's theory says that if you have a random reel, you can That if you have a random real, you can start peeling it off into columns, and you still get random reals. You do get an infinite array of reals. So this theorem here implies that this array you get by this method, basically you increase the randomness deficiency. So as you keep on peeling off past the randomness. Pass the randomness deficiency becomes higher and higher, so there is no bound on the deficiency. Okay, so in this recent paper by Herschel, Jokus and Su, they looked at kind of the same thing, but they did it in a different way. They obtained a similar statement for perfect trees, so that's weaker. And they needed two randomness instead of random and incomplete. Random and incomplete. So, the way they did it is they used the theorem about a strong mean map over random degrees. But this is more flexible. You only require incompleteness. And it does have a corollary. So, let me just read the statement. It says if Z is a real and it's random, and computer even enumerates a pathwise random tree. A pathwise random tree that has increasingly many paths, then it has to be complete. So, as long as you keep it incomplete, you have the building materials for separating P minus from WWKR. In the usual way, you just iterate and relativize and you get the model. Okay, there is one last corollary about this part of the work that is kind of interesting. The work that is kind of interesting. It does imply that if you have a computable space of trees, so you sort of set up the space where you're looking for reals or models. Then you cannot put any computable measure in that space such that the pathways, random members of that space. Random members of that space has positive measurements. So it has to be null. What does it mean? This means it means that if you want to produce such trees, pathways random trees with infinite many paths, all of which are random, you cannot do it by randomness with respect to a computable measure. That's important because Because if you try to say, do there are other separations in a similar way, which kind of makes sense. What I just described is basically saying that if you have one random real, it's difficult to produce a tree with many random reals of bound deficiency. So that kind of separates it. If you want to. Of separate, so if you want to, you could try to emulate that argument above and say, Well, if you have, for example, one pathways random tree with countably infinitely many paths, it's hard to effectively produce one that is perfect, pathways random tree. So, you could try to follow the problem is that now you would have to work with non-computable measures rather than with non-computable measures, and that is. Non-computable measures. And that is kind of hard. What hasn't been done for one? I mean, in computability, people do study randomness with respect to non-computable measures, but they are not using them for practical purposes, for building models. They are kind of hard to handle, at least apparently. Maybe there is a way. I think there is a way, but we haven't found it yet. So you don't hear measure. Measure theoretic results in computability with respect to non-computable measures. And there is a reason for that. They are kind of not very easy to handle. Okay, so that's the reason why the rest of the models are produced by forcing constructions. I would prefer if we got them by random reals, but that would mean that we would have to have a good Have to have a good grip on constructions with respect to non-computable measures. That's one of the open questions in the end. Because the okay, forcing constructions are more flexible, but when there are about such statements, which are kind of measure theoretic, then they become kind of if you heard bushy trees, they become something between forcing argument and a A forcing argument and a randomness argument. So it kind of something in between. Okay, before I talk about the other two separations, I don't know how much interest there is, but there are some finiter consequences that I cared about. And since this is not directly related to the topic of this meeting, The topic of this meeting, I will not, that would be brief. Okay. So, if you think about my previous statement, what we proved is that for infinite reals, you cannot, unless you're complete, that's the premise, unless you're complete, you cannot take an infinite random real and produce an array of without random real without losing deficiency, in this case, without. In this case, without getting unbounded deficiency. So this has a finite analog from the proofs. It basically says that if you have a string, no matter how long it is, it's hard to effectively produce from that string an array of, I don't know, K-man strings. Many strings that are random, and you don't lose more than a constant of a randomness deficiency. And what does this have to do with Levin? Well, Levin, remember what I said at the beginning about PA and incompleteness. We said that if you are right. said that if if you are random and incomplete then you don't uh compute a complete extension of the a so let me produce the finite array version use more or less using the same argument so he said that if you can uh produce a completion of a finite segment of arithmetic so the k-bit sentences so think of a universal partial partial recursive binary function uh but yeah so you Yeah, so you're looking at total extension. K-bid, think of strings, right? So if you can produce that using by a random algorithm, means by a random string, then actually you more or less compute the k-bit-halving problem, which means that the mutual information, that's an algorithmic information, is almost, almost. Is almost as much as the information of the k-bit housing problem. Now, the precise expressions depend on what version of the housing problem you're talking about, what sort of machines you're talking about, but in each case, you get a very, very precise optimal expression that says so using what I just said, you can do exactly the same thing and get exactly the same balance as let. Exactly the same bounds as leving god. But instead of pa, you use basically this other property where you can split a random string into two to the k columns. And then you get the segment instead of having basically the k-bit segment of PA, you have that k. That k is the same. Okay, which means that what is shown here, if you try to do this randomly, Try to do this randomly, that for so from a random, it's basically almost as hard as computing the k-bale problem in terms of finite information. It's a precise statement. Okay, and finally, just following Levin's sort of presentation, this has a consequence about the production of arrays of random strings from a random string. Random strings from a randomized machine. So Levin, for example, said that the probability of randomly guessing a complete extension of the PA, the K-bit sentences of arithmetic, is more or less the same as guessing the halving problem, the K-bit, halving problem randomly. And there are similar statements here for. are similar statements here for our notions which I'm not going to read in detail. However, there's an interesting thing that if you vary the length, make it a random variable, then actually there is a randomized algorithm that does this, which corresponds to the fact that if you have access to a pathwise random tree that has a certain width, then you actually have access to random strings of bounded deficiency of the same Of bounded efficiency of the same length, right? Okay, I move on to the next one. So, P doesn't imply P plus, so I remind you that P means that there exists a perfect sub-tree, the positive tree is to find a positive. The best is to find a policy. So, well, this is a convoluted thing. I guess the first statement is the unrelated version, but as we know, in order to do the version the model, you need to redo device it and apply it again and again. But the main statement is that there exists a perfect pathways random tree such that. tree such that no positive tree that is computable from it is pathwise random. Actually the results are a bit stronger than we need. So what comes out from the proof is not even enumerate such a tree. So again, I say it again. We build this using a forcing argument and the conditions involve sets of sets of positive measure. For some reason, you need to do that. You need to keep closed sets of sets of positive measure in the usual topology of the space of closed sets. Uh, the space of closed sets in the kind of space, and the it's kind of we were trying to do this originally, as I said, as a measure argument, and we knew that the measure had to be non-computable. And eventually, that was a quicker route because forcing arguments are more flexible. However, some notions we dealt with in the first argument were In the first argument, were very useful here. For example, heating sets from the theory of Poisson point processes. These kind of work natural to use to achieve, to make the argument work. Okay. The last one is P plus doesn't imply WKL. Now, how do we not? Now, as we now, this is pretty close to PA. Well, basically, you want to build a model of P plus. P plus means that there exists a perfect positive substring inside every positive tree in model. And you want this not to imply WKL. So basically, you want to avoid the P areas. The PA reals while keep adding these subtrees that are required for P. And as we're moving closer to WKL and far away from WWKL, randomness plays less and less role. However, we still have to deal with positive measure. So this started from looking at a theorem of Pate, who A theorem of Pate, who showed that every positive tree contains a perfect sub-tree, which does not compute complete extension of PA. In other words, he separated P from WKL. Well, we are attempting something a little bit more refined. So we took that argument and we extended it, but the extension is kind of The extension is kind of more involved, so it's not a trivial extension. And we prove that there exists a positive perfect path as random tree, which does not compute any complete extension of PA and the relative device version that we need for building the model. That was independently obtained in that paper about open covers by Greenberg, Miller. By Greenberg, Miller, and Nees. So, this is again a forcing argument of Lebette density in it. Okay, now this is the picture I showed at the beginning. There is this underlying thing that basically if you Basically, if you start increasing the dimension of a tree, so you start with a real, if you want from a real to get something more bushy, like maybe an array of infinite many paths, and remain abounded randomness deficiency, that's kind of hard. And the more you increase, for example, you make it perfect, that's an additional requirement, or you make it positive and perfect, that's even more bushy. That's even more bushy. So, the intuitive idea, although we didn't realize it to the fullest, we really were only in the first argument. The intuitive idea is that as you increase the dimensions, you can't do this transformation without losing, without increasing the deficiency, losing randomness inside the paths. And that could be the basis for separating For separating the models. For the last two models, we had to use more traditional methods along with the ideas, these versions of this idea inside the forcing argument. But ideally, at least I would like to do these separations completely measured theoretically. So have a theorem that says for that particular measure, this kind of tree, for example, is a random element. For example, it is a random element, but this one and the Turing upper closure is not, which means that if this is sufficiently random with respect to that measure, it does not compute anything like that. And the same for this. But we've already shown such a measure has to be non-computable and we're working on it. It remains to build such measures. remains to build such measures. And then in a sense, the models would be more natural, I feel. And of course, there is a remaining reduction, sorry, non-implication, which is build a model for P minus not P, which means that thing would basically contain things like that, say countable. Say countable trees or arrays of random reals, which cannot compute perfect pathways, random trees. Some reason that had an additional difficulty in the forcing argument, but it's possible with the forcing argument. However, it would be easier if we managed that second approach. That second approach to make it work would be kind of more natural. Okay, thank you.