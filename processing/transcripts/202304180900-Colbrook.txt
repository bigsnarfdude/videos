Okay, so then let's start with the talk. Et Holbuk from Cambridge is talking, and it's a pleasure. So, and yeah, it's about spectral computations in infinite dimensions. Please? Great, thank you. Thank you very much for the invite. Pleasure to be here. So, I'm going to talk about spectral computations in infinite dimensions. Really, the emphasis on Really, the emphasis on infinite rather than finite. So, I've had the pleasure of collaborating with a whole bunch of people on this program. A special mention to Andrew Morning back there goes away, Andrew, and also Alex Townsend, who's a co-author of the paper that I'm going to talk about in the second half of this talk. So, the talk is going to be split into two parts. In the first part, it's going to be a bit more pure. We're going to look at the foundations of computation, what you can and cannot do in infinite dimensions, and then we're going to. Dimensions and then we're going to look at non-linear eigenvalue problems and get a bit more sort of dirty into computations. And I'm going to talk about an algorithm that can provably converge without spectral pollution and spectral invisibility. Okay, so let's start off with the linear case. So having the eigenvalue of infinite matrix, for example, acting on L2N, we're all comfortable with eigenvalues of a finite matrix, and then we only go to And then when we go to infinite dimensions, we have to put the spectrum, so that's lambda, such as that A minus lambda, is non-invertible. And you can ask yourself the question: is there an algorithm that given this matrix representation approximates this set? For example, when it's bounded in the household method? So I've got a quote here from Arviston, who is an operator theorist, saying that when you're given an operator in practice, it's never diagonalized, right? You're not given the set or the step. You're not given all the spectral properties, and it's very difficult to locate points in the spectrum. So you have to settle for numerical approximations. Last time I gave this talk, Mark Emery didn't like this settling to numerical approximations. It's pessimistic. But there's a quote here that says, there was a dearth of literature on the space environment, so far as we've been able to tell, no proven general techniques. Emphasis here on general. So, of course. So, of course, there are certain classes of operators where there are algorithms that we know will converge to the spectrum. But what we want is really to understand what's the minimum requirement on this operator that allows me to construct such an algorithm. And if I can prove such an algorithm doesn't exist, I want to know why, right? I want to understand the reason. Okay, a motivating problem that I'm going to come back to are shredding our operators. Schr√∂dinger operators. So, Schringer and his work in the 50s and 60s examined the foundations of quantum mechanics and asked the question whether, given a self-adjoint shredding or operator on the middle line, can you use finite matrix approximations to the spectrum? And then a partial answer was given in 1994 by Digenet, Marin, Arajan, and Faradhan, who gave a convergence algorithm for a class of potential speed that generated compact. Generated compact result. They in fact proved that Spring's original approximation converged. But you can ask a more general question. For what classes of differential operators on the real line does there exist an algorithm that converges to the spectrum? And if such an algorithm exists, can I get error control on the outputs? That's a stronger condition, which is really important if you want to use it in applications or for. Use it in applications or, for example, as part of a computer assistance proof. Okay, so what can go wrong when we approximate spectra? So we saw a bit of this in Marco's talk yesterday on spectral pollution and spectral visibility. So if we take an infinite matrix, then it's standard to truncate this in the upper left corner. So Pn will always denote the projection onto the span of the first n basis elements or an open space. Elements or an open space. Oh, I should say I'm always working in head separable open spaces in this talk. So you look at this finite section and then you compute eigenvalues. If you've got a PDE in an unbounded domain, like a Springer's problem, then typically you truncate and then you'll discretize. You've got sort of two sets. So some key issues, spectral pollution. Even as you increase the truncation size, you can have eigenvalues that accumulate at points not in the spectrum. In the spectrum. Okay, so there's a really nice paper that I think is going to make an appearance in Sabina's talk later on where they characterize exactly where spectral pollution occurs or can occur for the linear eigenvalue problem or the linear spectral problem. Spectral invisibility, so that's where you can miss parts of the spectrum. And that's really tricky to understand when that happens. If you have a bounded self-adjoint operator, you don't get spectral invisibility. Get spectral invisibility. You can have an unbounded self-joint operator. Given any sequence of projections, you can get another sequence arbitrarily close, that's a sequence of projections, where the spectrum of this guy will now shift off to affinity. Okay, so this is a really tricky problem. Dealing with essential spectra, continuous spectra, so two different notions here, but dealing with them is difficult when you truncate because you've now got a discrete set of eigenvalues. Discrete set of variant values, accessibility, normality also play a role, and verification. So, can you get an output with some form of error control? Okay, so just to warm you up, since this is the first talk of the morning, I want to show you that not all spectral problems are created equal. So we're going to look at some examples, which might be surprising. Which might be surprising. So let's start off with what looks trivial. Let's consider the class of bounded diagonal operators. And let's assume that our algorithm can query the entries of this matrix. Then it's very simple to write down an algorithm that converges. Just take the first n elements of the diagonal. This will converge to the spectrum in the Hausdorff metric. So I've written down the Hausdorff metric here for reference. Hausdorff metric here for reference. But if you think about this a bit harder, you'll also see that anything in the output of this algorithm is in the spectrum. Basically, you've got this one-sided form of error control. In other words, you can control exactly one half of this household metric. And you can ask the question, can I control the other half? And then you can show that that's impossible. There's no algorithm, could be a different algorithm, that converges to the spectrum. Algorithm that converges to the spectrum from above. Hey, and that's because you've got information added for it. What about compact circle joint operators? It's a bit harder. So there, the finite sections will converge, so that's a classical result. But can you verify the output? So can you do something similar for diagonal operators? Does there exist an algorithm that converges to the spectrum where the output is contained? Where the output is contained in the spectrum up to some puncture in a small ball. So here it slightly weakens the one-sided error control weakened before. And so anyone has a guess whether you can do this? The circle-droid compact operators? Norm brave enough? Okay. The answer is no, you can't. No, I haven't done this over the whole class of self-worth compact operators. So, compact operators. Operators. So compact operators, structural compact operators, are harder than diagonal operators. No real surprise there. But then things become a bit more sort of interesting. So let's look at Jacobi operators. So computing the spectrum of these guys is very non-trivial because you have things like spirio sign-by spectral position. So let's make it even harder. Let's look at sparse normal operators. Normal operators. So by sparse, I mean it has finitely many non-zero entries in each column. We can actually replace sparse by approximately sparse, doesn't really matter. Surely this is now going to be hard, right? This looks harder than itself, which is very compact, surely. But there does exist an algorithm that converges to the spectrum where the output is guaranteed to be inside the spectrum after some controllable error. Yes, you can. Error. Yes, you can get one-sided error control for this class of operators, and you couldn't for self-worth compact operators. So, this is telling you that the assumptions you make on your operators really do matter when you try and determine what kind of algorithm you get at the computing spectral properties. Okay, final example, and this algorithm is very interesting. This goes back to a paper of Anders Hansen in 2011. And what's interesting about this algorithm is that it relies on three parameters that take successively to infinity. It's what I call a three-limit algorithm. So I'm going to consider general bounded operators to get to the Zargiston's original problem. I'm going to look at something called the injection modulus, which is generalizing at the smallest singular value of the matrix. So this is just the ephemera over A V. Is just the affium over A V, where V is in the domain array with norm one. And what you do is you first look at these two injection moduli. So PN1 on the left here means I look at the first n1 rows of the matrix. Pn2 on the right, first N2 columns. I look at this shifted with the identity Z, I look at this rectangular matrix here, computed the smallest single value, and I did the same thing. Single value and do the same for the attract. This attract is very important, by the way, for non-normal operators. If your operator is normal, follow the work of this thing only. Okay, so what happens when we take n1 to infinity? We're increasing the range of these operators, which means the singular values increase. You can prove that this function as a function of z both is uniformly on compact subsets. On compact subsets, it's the complex plane up to this function here, where we've got rid of the first projection. So, now what we're going to do is we're going to deal with this second projection by taking n2 to the beta. Exactly the same argument shows that this will converge down to this function here. Now we've got no projections. Now, this is really, really nice because this is just the reciprocal of the resolvent norm in this. Of the resolvent norm in the skies where I define this specifically to be zero on the spectrum. So, in other words, I've got a two-limit approach to accessing the resolved norm of general operators. And these limits, by the way, don't commute, right? You can't just take n2 to infinity then, n1. It's really all to do with this monotonicity here in the limits. Okay, so we've got a way of accessing the resultant norm. To seeing the resultant norm, and then the final limit in Hanson's algorithm, it's pretty hard to see, but this should be sort of a shaded region here. You look at the pseudo-spectrum. So the pseudo-spectrum for a given epsilon is the set of Z's such that this reciprocal of the resolvent norm is less than or equal to epsilon. And as you take epsilon to zero, this will shrink down to the spectrum. So you can write down. So, you can write down an algorithm. Okay, so you look at this algorithm here with epsilon 1 over n 3, and you can prove that this will converge to the spectrum in three successive limits. Okay, so this should be troubling, right? Because when you do computations, you can never get past the first limit, right? You don't have infinite time. Even more troubling. Even more troubling, you can actually prove that three limits is sharp. There's no two-limit algorithm that will converge to the spectrum of general bounded operators. And this is all to do with the intrinsic complexity of the spectrum for this class of operators. And so the way you prove this is you can take certain combinatorial problems from descriptive set theory. These have well-known complexity, which you can characterize in terms of limits. And you can embed. And you can embed them into your spectral problems. You have a tool that allows you to take problems with known complexity, improve lower bounds for the difficulty of spectral problems. And you can do this for all sorts of different things, not just the spectrum, things like spectral measures and stuff like that as well. So if you're interested in that sort of technique, then you can read this paper down here. Okay, so three legs is sharp, this is bad, and this will. Is bad. And this also explains Alverson's laminate, right? There was no general algorithm because an algorithm simply doesn't exist. So the question then becomes, what assumptions are needed to get past this barrier? And this is really the motivation for the hierarchy that we're going to talk about now. Right. The solvability complexity index. The solvability complexity index IR. So, this was introduced by Hansen in the paper that I mentioned. So, suppose you have a class of operators, or could be another object, and you want to compute a map from this class to some metric space. Then I'm going to let delta zero be the problems that can be solved in finite time exactly. So that's very rare for problems in infinite dimensions. Pretty much never happens. Problems that can be solved in one limit with full error control. So, for example, if this is the house. So, for example, if this is the Hausdorff metric, this is a stronger form of error control than we saw for bounded diagonal operators. Delta 2, problems that can be solved in one limit. Note that 2 and the 1 here is a bit confusing, but there are historical reasons for that. Delta 3, problems in two successive limits, and so on. Okay, so you might think this is a bit crazy, but actually, the earliest example that I know of this hierarchy in action is a problem of. Is a problem of Steve Smale, who asked whether you can construct a rational maps that when you iterate will converge to zeros of polynomials. So the problem there is that if you take Newton's method, it won't always converge for cubics. And Macmillan shows that for certain degrees, you can't bounce off this problem, but you can this successive limits in exactly the same at the same manner. So this successive limits thing is actually quite. Successive limits thing is actually quite all over the place in computational mathematics. Final comment for those of you who are into sort of computability things like that. You could do this in any model of computation, and in infinite dimensions, the model actually doesn't matter. So whether I consider a Turing machine, a PSS machine, so it's a machine that can deal with exact arithmetic, integral arithmetic, whatever. All of the classifications in infinite dimensions are the main. Dimensions that we may say. Okay, so we've got this full error control, but the case of diagonal operators had a half error control, so I can actually enrich the hierarchy to include those notions. So suppose I want to compute the spectrum, and I have an algorithm that converges so that the maximum of the distance of any point in the output to the spectrum is bound by. To the spectrum, it's bound by 2 to the mice. You can replace this, of course, with any null sequence that you know. So, pictorially, it looks like this. You've got sets which converge up to the spectrum, and they never make an error up to this tolerance. So, you can trust everything in the output of this set. Pi one is the is the dual notion, so that's where you cover the spectrum up to some arbitrary small parameter. Small parameter. So that's down here. So these two classes are really the most important in infinite-dimensional spectral computations because they allow verification. I can trust everything in the output here. Over here, I know that I've nearly gone on with the spectrum. So I can answer different questions and you can actually use them, for example, on computer assistants. Proofs of people have done it. So let me So, let me just give you a sample of this hierarchy. So, from left to right, we've got increasing difficulty as measured in those classes. In green here, we've got classes where you can get error control. So, delta 1 is full error control. Sigma 1, pi 1 is sort of half error control. Sigma 1 is convergence from below. Pi 1 convergence from above. This blue narrow strip here is where you've got one limit. Of one limit, potentially without any form of variant control. Here you have two limits, three limits, and it actually goes off to infinity. So, compact operators live here, normal operators live here, sparse operators live here, general bounded operators live here. I'm not going to go through this. One really cool thing about this is that once you About this, is that once you start to classify these problems, you get a feel for how they interact. So, there's actually a countless of the SEI algorithms. So, for example, the intersection of pi 2 and sigma 2 is delta 2. So, now you can see if I have a sparse and normal operator, I can get one of it algorithm. So, that includes Jacobian. And in fact, you can do a bit better. You actually get SQL. You actually get signal. So, approximately sparse normal log traces there for you. So, this allows you to sort of mix and match, figure out where your problem sits. Another example I thought I'd mention, because this is certainly that Mark and I got excited about, is that there are pi one algorithms for apiroidic running robots in one dimension. So, if you combine these two guys, you get delta one. There's a whole zoo of spectral problems that various people have classified in this hierarchy. For example, the spectral type of surface joints and unitary operators, spectral measures, vague measure, fractal dimensions, speech spectra, central spectra, multiplicities, radi, it goes on and on, spectral gaps, resonances. I just thought I'd give you a few references here. So I'm going to put these slides up on my website afterwards, so don't worry about sort of. Up on my website afterwards, so don't worry about sort of if you miss them. But I just wanted to say sort of that there's a really good group of Cardiff who are competing resonances in this hierarchy. There's also a lovely paper of Frank Rosler and Patricia Treder, where they look at Klein-Gordon spectrum, which is a non-linear spectral problem, which is relevant to this workshop. I think they even managed to prove a delta one that offers certain classes of potentials. Okay, so what about going back to Schwinger's original problem, right? This motivational problem. I'm going to look at the class of self-adjoint differential operators on R D. I'm going to assume I have access to this operator in the strong form, and that smooth, completely supported functions form a core of this operator. I'm also going to assume that. I'm also going to assume the coefficients are polynomially bounded. You can generalize this to things like Coulomba tensions, but that's another story. And the coefficients are of locally bounded total variations, they don't oscillate too much on the subsets. And I'm going to assume some very weak conditions that my algorithm can simply point sample the coefficients at rational points to arbitrary precision. To arbitrary precision. So I give it an epsilon and a rational, and it gives me back this coefficient of value from rational to epsilon. And I'm also going to assume that I know polynomials that bound the coefficients. And then you can start to be quite precise about the information, the additional information you need to get different classifications. So for example, if you know a bound on the total variation, A bound on the total variation norm on each compact hypercube, you can actually show that the spectrum, the problem of computing the spectrum, it's inside sigma 1. So it's verifiable. So there's an algorithm that converges, and I know that everything in the output is at distance minus 2 to the minus n, or whatever x I want to choose, away from the spectrum. So it's verifiable. If I slightly weaken the assumptions and only assume Weaken the assumptions and only assume I have an asymptotic bound on the variation law, then it's not verifiable. So you can still do it in one limit, but you can't do it with convergence from below or convergence from above. Basically, you can start to look at things like shredding operators and conditions on the potential that give you different classifications in this hierarchy. So So if we go back to that crazy picture, for example, that's the class that I just discussed in the theorem. Self-adjoint bounded V with locally bounded tenth variation. Unbounded sectoral V, so it's no surprise that this is the same classification as compact operators. These will actually help compact as well. Bounded potential, not necessarily self-a joint, only bounded sales variation. Total variation. If you drop locally bound total variation, you then go to in free limits. A bit of a teaser for, I know there are a few people in the audience who like non-emission quantum codes and those sort of things. You can almost put things like the imaginary cubic oscillator here as well. Okay, so you've got this picture, and it's safe to say that most of the sort of numerical analysis results in the literature will prove when they In the literature, they will prove when they prove convergence delta 2. Okay, so they'll show that an algorithm converges to the spectrum, possibly with an asymptotic rate, but typically you won't know those constants. But actually, for a lot of problems, that's sub-oximal. Most nice problems in the wrong. So the challenge is if you've got an algorithm that converges, you've actually upgraded the convergence to screen single problem. Okay, so before we Okay, so before moving to the second part of this talk, I just wanted to say why study this hybrid. And it's really all to do with this interaction. So I've talked a bit about foundations of computation. We're now going to shift gears and look at some remote remotes and stuff. But to study these two things properly in infinite dimensions, you really need both, right? So to design algorithms that converge, Algorithms that converge, unstable, and things like that, you really need to know your functional analysis and the foundations of the computation in this area. And when you're sort of proving lower bounds, so impossibility results, or designing algorithms that achieve the best that you could hope for, it's really important that you have an idea of how the numerics work. So if you have an SCI, so some of its complexity index is the number of limits. Complexity index is the number of limits greater than one. This tells you the assumptions you need to lower the index. So, an example of that is Hanson's three-level algorithm, and studying that algorithm is what led to that algorithm there. So, it might seem crazy that why would you care about a three-level algorithm up here? Well, it actually tells you how to get there. Well, it actually tells you how to get them there. Sharp classifications lead to new techniques. Sigma 1 gives you a lookup table for things like computer-assisted proofs. If you prove a negative result, you stop wasting time searching for an algorithm that doesn't exist. And much of the sort of computational literature doesn't actually prove sharp results. Hence, you want algorithms that are optimal to achieve the best. Yeah, so Matt, can I ask just a pedestrian question that you've answered, I think, in a sense, but just as a translation. So, functional analysts may say I have an approximation method, an infinite dimensional operator, prove a theorem about convergence of the spectrum, and in terms of a finite dimensional problem, draw a little box at the end of the proof, and I'm done. A numerical analyst will take that approximation method and turn that into numerical eigenvalues. That is a limit in itself, right? Can you just maybe talk through that? Limit in itself, right? Can you just maybe talk through that step in terms of your hierarchy? In terms of, for example, computing this one. Where does the actual, you know, got a handle on a matrix? Right. There's numerical eigenvalues I must compute using QR algorithm and all that kind of thing. It'll be stable-headed. Right. So for example, let's look at this one here. So the algorithm is based on computing small sigma values. Now, of course, you can't compute these exactly. Compute these exactly, but you can up to a known tolerance, right? So, what you can do is you can hide that tolerance in the first limit. So, numerical analysis will often involve a limit or two, and when you're doing the sort of the proof, you have to combine those limits with the functional analysis limits to make sure that you don't commit any crimes with things, not commuting with things. So, we'll see an example in the second half where I should show that it's really essential to It's really essential to computely resolve adaptively, which is exactly the similar idea here, where you sort of increase your location size. But again, just sort of tie this into kind of the orthodoxy of numerical linear algebra. I mean, here you're advocating computing smallest singular values of these particular sections with N1 and N2 on rectangular matrices. I don't see the QR algorithm here or, you know, or NOD or something like that, right? Right. So, for example, if you wanted to compute For example, if you wanted to compute this to epsilon to epsilon, you could. And you could check whether if you take away epsilon times an identity when there's positive definite semi-definite. Now you don't have to use an iterative methods. So that'll be out of the composition. Right, I guess what I'm pushing at is, like, I don't see eigenval A here. Oh, okay, yeah, yeah, but uh, you know, right, right. So I just want to comment that you to comment on, like, does eigenvalue Like, does eigenvector have no place in your oh, it definitely does, and we'll see it for discrete spectra in a second. But if you want to deal with essential spectra, really, the only technique that I'm aware of, Mr. Mung, future techniques, but is using this single band because it avoids the spectral collision, which will always happen when you're a substance mode. You can actually use I care if you multiply this by its adjoint here and then take a script. I guess what I'm trying to highlight is that you're kind of advocating in here. Highlight is that you're kind of advocating in here a different approach to the kind of conventional way. Exactly, yeah, yeah. Rectangler of square. I thought that's pretty important. Okay, so let's shift gears and go into non-linear spectral properties or any piece. The E here stands for eigenvalue. I'm just going to use this sort of shortcut as well. Okay, so I'm going to consider a holomorphic. Okay, so I'm going to consider a holomorphic pencil on some Elbert space. Okay, so holomorphic means that for all u in its domain, lambda to t lambda acting on u is holomorphic, in u represents. The spectrum is all lambda such that t of lambda is not bounded available. The discrete spectrum is all points in the spectrum such that t of lambda is not thread hole. Oh sorry, is thread hole. The essential spectrum is the rest of the spectrum. And the pseudo-spectrum. And the pseudo-spectrum, in this case, is the closure. You have to take closure here because the operator's not valid potentially of points lambda such that the resolvent norm is greater than whatever epsilon. So these four sets would play a role in what follows. So just a sort of very quick fire overview of what's currently known in terms of foundation for computation for these problems. Computing pseudo-spectra is typically sigma-wire. Is typically sigma 1 for lots of classic differential operators, speed operators. Hence, the spectrum is at worst pi 2. So why does that hold? Well, consider a spectrum, then you shrink down to the spectrum. So that second limit is above S is pi. Computing discrete spectra, if you're in a region that you know doesn't contain any essential spectrum, even if you don't know how far away from the essential spectrum you are, Is delta 2. So we'll do it in one limit with no error control. So getting error control for general classes of these pets is. So you may ask, well, why it's called discrete spectrum is a little bit misleading. Why? This is the same definition as the medicine is right? Sorry? Sorry? I mean, it's not what it is. I'll show you a second a theorem that shows that the isolated eigenvalues of finite multiplicity will hold up for this set as well. But this is clear, yeah. But we could have more, I think. Okay, sure. Yeah, that'd be good actually. Okay. So there's a theorem called Caltrus theorem which goes into different names of different Which goes into the different names of different communities. But what it does is it allows you to decompose the result. So, if you're in a region where there's no essential spectrum, and if the spectrum is not the whole of the region, and then for any Z not in the spectrum, the inverse, this resolvance, can be written down in terms of two quasi-matrices of generalized eigenvectors. So, each column of these guys will live in. Guys, will live in the Hilbert space. So you can think of an infinite number of rows. There are finitely many columns, m columns corresponding to the sum of algebraic multiplicities. J is block diagonal and consists of Jordan blocks. So you have this guy here. And then you have a bounded polymorphic remainder. Okay, so this is very useful for computational purposes because we can use contour integration to get rid of Contour integration to get rid of this holomorphic part. So we can convert our nonlinear spectral problem to a linear eigenvalue problem that's actually finite dimensional of what is ahead. So I'm going to assume that this region only contains a finite number of eigenvalues. Okay, so there's an algorithm called Bynes method, which goes back to this paper here. And what I'm going to do is show you how it generalizes to infinity. It generalizes to infinite dimensions. So let's pick a contour in this region omega. And what we're going to do is compute two moments. First of all, the zeroth moment, where we have the integral of this resolvent acting on a matrix of, in this case, random Gaussian processes or vectors drawn from a random Gaussian process. So for the examples I'm going to look at, So for the examples I'm going to look at, T is typically a differential operator on some domain, so I can make sense of these Gaussian processes. So just think of these as the infinite dimensional analog of Gaussian matrices. Okay, and A1 is where I stick an extra as well. Now you can approximate these using quadrature. I'm not going to go into the quadrature rules that you use, but typically you get exponential conversions or whatever quadrature points. So what you do is you take your approximation of You take your approximation of A naught, do the truncated S V D with M singular values, and then you form this linear pencil here. Okay, so remember U has an infinite number of, if you're thinking about it in terms of basis functions, has an infinite number of rows, but M columns. So this is an M by M pencil, and then you can recover the eigenvectors. Then you can recover the eigenvectors approximately by this fault here. Okay, so you might worry, am I cheating? Because I know M? Well, it's relatively straightforward to compute them using this trace fault. And there are arguments that can prove that we do that. But that's another story. You can actually ask Andrew. Andrew's an expert on computing these trace things here. Okay, so you've got this finite linear pen. Finite linear pencil that is somehow related to your non-linear eigenvalue problem, and you can prove this result. Okay, so remember, Calvish decomposes the resolvent. You have this inverse of the Jordan block with these two quasi-matrices here. You've got this bounded holomorphic part. I'm going to let N, a couple M be the supremum of this holomorphic part. I'm going to suppose that I can approximate each AJ to Approximate each aj to quantitative error epsilon j. And I'm also going to introduce this condition number here. So this is the condition number of VW star. So m is the nth single value. So I know this is a rank m. And what the theorem says is that if you sufficiently oversample b, then with overwhelming probability, you can bound your quadrature. You can bound your quadrature error. So, this is the difference in the injection moduli between your two pencils, the exact pencil, no quadrature error, computer pencil, in terms of a bunch of stuff involving the epsilon one and the epsilon zero. So that's fine, we can control that. But more interestingly, you can relate the pseudo-spectrum of the pencil, this finite pencil, to the infinite dimensional pencil. Um so if epsilon, the pseudospectral parameter, is small enough, you get the sandwich in it. Okay, so the pseudo-spectra of T is contained with a different scaling of epsilon and the pseudo-spectra of F, which is contained in T. Each time you do this, you lose a factor of the condition number, which is unavoidable. There's no frequency. I should also say this isn't a statement. I should also say this isn't a statement on computing pseudospectra. I'm not going to use this to compute pseudospectra. There's another algorithm that does that that I can discuss if people are interested. Rather, what it shows is that as you decrease your quadrature error, you're going to converge without spectral pollution, without spectral visibility. Moreover, because you've got the sandwiching, doing so in a stable manner. Doing so in a stable manner. Stable relative to the stability of the infinite dimensional problem, okay? Which is intrinsic. So, no matter what algorithm you do, you can't get past this value if you want to approximate the scratch problem. Okay, so a very quick sketch of the proof. Calvis again here. I'm going to introduce two pseudo-inversants here. First, we're going to take the resolvent act on the right with the pseudo-inversant. The right with the pseudo-inverse in F. A little bit of playing around, you can convince yourself that this is equal to this rank M matrix here plus this holomorphic part which you can bound. So for example if the injection modulus of F is less than epsilon, it's relatively easy to then show that the norm of the resolvent of T is greater than 1 over epsilon times the product of these MT. Product of these mthingler values. Note that I've got this extra script V here, minus m. To get the other way, what you do is you apply this pseudo-inverse on the left, with your pencil F, okay, to this part of the result. Okay, so the result minus the holomorphic part. This just comes out to be this product here, which means that if the result is the result of the result, Which means that if the result of normal t is greater than epsilon, the injection modulus that is less than epsilon times this product of norms here, operator norms is of rank m over 1 minus m epsilon. So the final part of the proof, which is relevant for people who work in numerical analysis, is that you want to be able to get rid of these script Vs. You want to relate to something intrinsic. Something intrinsic on the operator. To do that, you use some tools for sketching, randomizing your algebra, to bound everything explicitly in terms of this condition. Okay, so in the last few minutes... Can I? Yeah. That's a question that told you that the balance you end up with, I think there was a finite dimensional case, you probably have a failure probability, right? Yes. About all the probability of the correctly. Right, exactly. So the probability depend on how. So does probability depend on how much you have a sample? You can explicitly. Right overwhelming problem. Yeah. So it's typical for these sketching. Yeah. So maybe I should be more precise here and say probably right so I just wanted to look at a couple of examples quickly to finish of this in action which I think Of this connection, which I think demonstrates some interesting phenomena. One of these is the problem that I wrote on the board yesterday. We came to the consensus that spectral pollution has something to do with the basis functions not interacting well with the geosynchronous normal math, but it's quite hard to understand exactly why. But okay, so I'm going to look at a collection of non-linear eigenvalue problems from this paper here. The first one I'm going The first one I'm going to look at is this Helmholtz equation in the unit box, where I have Deuteronomic boundary conditions and impedance boundary condition here. The spectral parameter corresponds to resonant frequencies. And following this paper here, I'm going to use the same discretization using linear formula elements. 25 out of 52 of this problem set actually come from infinite dimensional problems. Okay, so infinite dimensional non- Infinite dimensional non-linear eigenvalue problems are really important. Right, so what happens? Here, I've got the spectrum of different discretizations using FEM and then solving those finite-dimensional discretized problems to a very small error. Okay, so you can think of this as if I discretize then size. If I discretize then solve, this is what I get for different discretization parameters. This is a region in the upper half plane, and what you'll actually see as you increase n is a region of spectral pollution. So there are no eigenvalues in this region, and you can prove that using the trace form. So if you go down a bit in the upper half plane to note the different scale, there's actually a bunch of eigenvalues that do exist. So these black dots here correspond to the output of Here correspond to the output of infbine, the algorithm I just showed you. And you can bound the pseudospector or residuals of these guys as well to be confident that what you're doing is correct. You also have, of course, the stability bound. And what you see is that a bunch of them are accurately resolved for this truncation size. And as you increase truncation size, the discretization size, they'll converge towards these guys here. Converged with these guys here. So you have two regions in the complex plane, relatively close to each other, one where you get spectral pollution and one where things are very nice. Yeah. I'm really convinced that it's spectrally pollution because the blue, red, and yellow rises up a bit. It's almost as if maybe it's marching off to infinity. Maybe logarithmically. Could be. I mean, so I tried this. Could be. I mean, so I tried this for super large N and it was still clustering in this picture here. It could be very slow, right? And that's another problem, right? You don't know whether spectral pollution is actually occurring or whether it's just very, very slow. I mean, from a practitioner, maybe it doesn't matter which one happens, right? Yes, you can actually look at the 1D example of this problem, and there you won't get spectral pollution. It march off to infinity, but it will. March off to infinity, but it will do so very slowly. So that's the point, yeah. That's what I should say. I think that spectrum is shifted. You can actually prove in the SEI hierarchy. Knowing for sure that spectral pollution occurs requires one extra limit to computing the spectrum. So let's see what we're in there as well. Right. This is an example where I can prove spectral commission using tokens theory. Topis theory. This is the butterfly example from this collection. What it is, is a rational function of a bilateral shift operator, L2Z, rational in S and also the spectral parameter. If you discretize using a finite section with 500 basis elements or a matrix of 500 by 500 and compute pseudo-spectra, you get these. Two day spectra, you get these beautiful plots here. And the eigenvalues will cluster along these four curves, two of which are infinite. So proper picture of it is down. But you can actually prove this using a non-linear version of Topic's theory. If you instead compute the pseudo-spectrum using these injection modified techniques, you get this very nice picture here. You get this very nice picture here, which shows you, which is guaranteed to be inside the true pseudo-spectrum, and shows that here you're getting instability, these three regions, spectral pollution here, and spectral invisibility. There's all sorts of different parameters you can feed into butterfly in this toolbox, and you get very similar effects. Effect. Okay, final example, which I think is probably the most interesting, is a planar waveguide problem. So this comes from a spectral problem on the whole real line. So eta here is a refractive index. Phi corresponds to some physical thing I can't remember. But the important thing is you've got the infinite domain. Okay, and the fact of index is in this case. Index is in this case piecewise constant, but I can shove it whenever I want the algorithm. So it shoots off to infinity, it's constant, and then fits it that way as well. And what you do is you truncate the domain for this region here, and you do a good bit of playing around, and you end up with a non-linear agency problem that is equivalent to computing so-called guided and leaky modes. You can think of that as a discrete spectra in resonances. Resonances. So it involves this rational of lambda, rational function here, and then these two boundary conditions at x equals 0 and x equals 2. So the important thing for this talk is that this problem here will have a central spectrum, the original linear spectral problem. But when I distribute, sorry, when I truncate to this infinite-dimensional non-linear eigenvalue problem on a bounded region, Value problem on a bounded region. There's only one point in the essential spectrum, and the rest is discrete. Okay, so this is what happens. So in blue, I've shown the mapped. So remember, mu and lambda are related. Essential spectrum of the original linear problem on the whole line. In red are the eigenvalues when you discretize. In black are In black are resonances computed using this contour method. Here's a zoom in section. So, what you'll actually see is within this circle, you'll get spectral portion. And then some of these guys are accurate. As you increase the resolution, this will eventually snap back, but it'll always snap back to this line here, eventually if you get far enough up. So, let's zoom in again. So, this is, remember the. So, this is remember the essential spectrum of the non-truncated problem. And what you see is that the eigenvalues corresponding to pollution that'll accumulate actually sort of collapse onto this essential spectrum. So, I call this ghost essential spectrum, but I don't really understand this phenomenon. So, if anyone in the audience knows precisely why this occurs, please let me know, because it's been spotted in a number of papers. Because it's been spotted in a number of papers on this wave cut problems. Here are the resonances that you can compute with these quantum methods. And you can do so very easily. So here are some of these eigenvalues. Here's the convergence in terms of the number of quasi-check points for an elliptical contour containing several eigenvalues with a left-wing rule. Okay, so final plot is showing everything in the mu plane. It's showing everything in the mu plane again, where the essential spectrum is much simpler, it's just this interval. So now you can see these resonances, okay, spurious modes, and then you zoom in again, you see this collapsed resonance introspection. Okay, so that's just three examples of this algorithm actually. So I wanted to wrap up with comments on developments in open problems. So key foundations development. So key foundations developments. So using this hierarchy, we can now classify the difficulty of computational problems. This allows us to prove that algorithms are optimal in any given computational model and means we can mix and match and find assumptions and methods for our computational goals. It also leads to universal algorithms for different classes of operators. Some of my collaborators are now using this. Laboratories are now using this framework for computational PDEs as well, computer-assisted proofs, foundations of AI and optimization. So, typically, you can take those problems I talked about and embed them in other areas as well. Key algorithmic developments. We now have a new suite of infinite-dimensional algorithms, and really the philosophy is to solve as much as we can at the infinite-dimensional level, then discretely. Dimensional level, then discretized at the last possible moment. This typically, for spectral problems, resolves around two different things. Algorithms built on computing injection moduli, which you do so by approximating this way, you have one projection on the right, or A star A here, taking a square root. As this includes spectral error controls, the sigma one, pseudo spectra, more exotic things like fractal dimension from region two. And then the second tool that we've developed is adaptively, and this is really important, this goes back to Mark's question, computing the resolvents, either in the linear case or the non-linear case. So to really achieve, solve, and discretize and avoid things like spectral pollution, if you want to compute the resolvent, for example, at different points z, that will require different truncation sizes. So I'm not just Truncation sizes. So I'm not just considering truncating, then studying how the spectra of those guys possibly converge to the infinite dimension guy. What I'm doing is adaptively looking at different truncation signs. It then allows you to use contour methods for discrete spectra, convolution methods for spectral measures. So there's a paper here with Andrew Morning and also Alex Townsend where we do this. And functions, the functional calculus. And functions, the functional calculus as well. You have an arrow control quite human. Okay, so fitting the smoke problems. Structure preserving infinite dimensional methods for non-linear eigenvalue problems. So structure preserving, or structure preservation is a very well-studied topic in finite dimensions, whatsoever. I talked about computing discrete spectra. What about essential spectra? If I was a betting man, I would bet that the SCI is greater than 1, even if the pencil is emission. It's probably very, very difficult. Note that in the linear case, this would be equal to 1. Foundations of data-driven spectral problems. This is related to model reduction. Wants to study dynamical systems using transition operators. There's a lot of interest in collecting data from your system, then inferring. Data from your system, then inferring spectral properties of these operators. What's the foundations of that, right? Can you and can't do it? Characterizing spectral pollutions, this is what I wrote up yesterday for eigenvalue-dependent boundary conditions, like the acoustic wave 2D, or even figuring out whether there is spectral pollution, who knows. Polynomial pencils would be a good place to start. You can use higher order moments in fine, so figuring out the stability conversion. So, figuring out the stability convergence for that. And then Python algorithms for more general aparotic systems as well. So, yeah, I think that's all I had to sort of say. Thanks for your attention.