Okay. Sorry, Zoom seems to think it wants to install a plug-in for sharing the iPad. This has never happened before. Okay, maybe now it's okay. Well, we can't see any at least I can't see anything. Oh, yeah, now it's getting better. Okay, great. Okay, is this okay? I see somebody asking in the chat if the talk can be recorded. The talk is being recorded at the moment. The talk is being recorded at the moment. Okay. So great. Thanks a lot. And thanks to all the organizers for the invitation. So I'm excited to tell you about this joint work with David Gemarnik and Akos Jaganoth. So let me jump in and tell you what this will be about. So I want to talk about random optimization problems. Problems. And to kind of illustrate this by way of example, you can think: imagine, for instance, I give you just a random graph, an Erd≈ës-Reni graph, and you have to find the largest clique that you can, right? Find as large a clique as possible, a clique being a fully connected subgraph like the one shown in red. Other examples, I give you a random KSAT formula and try to find a satisfying assignment or maybe an assignment that satisfies as many clauses as possible. As many clauses as possible. Or I give you a random degree p polynomial, and you need to find a point on the sphere that maximizes the value. And so on. So there are many problems of this flavor. And crucially, I'm thinking here about problems that do not have a planted solution. So I'm not talking so much about statistical inference problems, but rather where you're given just pure noisy data and you need to try to find a good solution. So, there's a number of questions we would want to ask ourselves about these types of problems. You know, what typically is the optimal value? But also, what objective value can we reach algorithmically, meaning using a polynomial time algorithm? And we'll see that there are some cases where it seems to be computationally hard to reach a particular objective value, and you would like to understand why. And we'd like to understand why. And we'd really like to do this kind of in a unified way that applies to many problems. Okay, so the concrete example that I'm going to talk about is the independent set problem. So imagine that we are given a sparse random graph. So this is just n vertices, and each potential edge occurs with this probability d over n, where you think of d as a large constant. Think of D as a large constant. And the optimization problem we want to solve is just find a large independent set, right? Where the meaning of independent is just that S is a set of vertices that has no edges among it. So like, you know, if you have S being a set of vertices, there is no edge in between, right? This is not allowed. No edge. Allowed no edge that has both endpoint and s, but an edge like this would be okay. Okay, so I give you a random graph, find the largest independent set possible. And so what's known about this problem already? So we know that with high probability, the true optimum value will be something. But interestingly, the best known polynomial time algorithm, which is sort of a greedy algorithm. Which is sort of a greedy algorithm, can only find something half as big. So we can only find sort of an independent set of half optimal size, as far as we know. And a number of years ago, Karp famously asked whether there is a better algorithm. And more recently, evidence has started to emerge suggesting that perhaps the answer is no, that really there is something special happening at this threshold, some structural properties. This threshold, some structural properties of the independent sets, where the independent sets larger than this sort of cluster together in a way that would seem to make it hard for certain types of algorithms to explore this space. And perhaps a bit more concretely, it's been shown that the class of so-called local algorithms can achieve the value alg, but can not surpass this. Not surpass this. And it won't be so important, but roughly speaking, local algorithms are those where each node just looks kind of at its local neighborhood, which locally will look like a tree. You get to go look out to, say, some constant radius r. You look at sort of what does the tree look like there, and also some random variables attached to each node. And then each node, say the root. And then each node, say the root here, each node looks at its own neighborhood and needs to make a decision about whether I should be in the independence there or not, just using this local information. Okay, and so this class of algorithms really fundamentally breaks down at this point. I won't spend too much time on this example, but another example is the so-called P-Spin model. Maybe don't use Maybe don't even worry so much about the formal definition, but you can just think of it as maximizing a random degree p homogeneous polynomial over the unit sphere. In this case, it is sort of also known that the true optimum value will be some constant, but the best known algorithm achieves some other constant. And these two constants are different, at least for p greater than three. And again, we have. And again, we have some kind of evidence that maybe this is fundamental, that the class of approximate message passing algorithms can achieve this conjectured value and no better. Okay, so what's missing? Sort of at a philosophical level, what do we want here? We want to be able to think: how do we give the best type of evidence that there are no better algorithms than a certain threshold? Threshold. And so, prior work, you know, a lot of interesting prior work ruling out certain classes of algorithms, for example, local or AMP. And so, we should think, you know, is this good evidence that the problem is actually hard? Right? Do we actually expect these types of algorithms to be optimal, to be the best thing we know how to do? And maybe one thing that's concerning is that the AMP algorithm, you know, which don't worry about the precise definition of it, but is actually not optimal. But is actually not optimal for a very similar problem, tensor PCA, which is sort of the B-SPIN model with a planted signal. And so maybe that's concerning. We'd also like sort of a unified framework for lower bounds that applies to many different problems, ideally. And so this notion of local algorithms is quite specific to sparse graphs. And so this also kind of does not give us a general purpose theory. And so the solution that we sort of And so, the solution that we sort of propose here is to prove lower bounds against this larger class of algorithms, which will be low-degree polynomials. And these actually contain both local algorithms and AMP and a variety of other things. And so, hopefully, by proving these types of lower bounds, we will attain sort of a stronger form of evidence for why these things should really be hard past a certain threshold. Okay, so let me tell you what exactly do I mean by a low-degree polynomial algorithm. So when I say a low-degree algorithm, I'm talking about a multivariate polynomial that basically takes in your entire input. And so it's, you know, in the case of a graph, imagine that you just have n choose two different input variables that are zero, one valued, one for each edge. Valued, one for each edge. And so I'm talking about a multivariate polynomial that takes in all of this, all those input variables, and maybe it outputs just some vector, say. Imagine it takes in a graph, it outputs a vector, and the vector is supposed to be sort of an indicator vector for a large independent set, right? If we're trying to solve the independent set problem, and when I say low degree, let's imagine. Let's imagine logarithmic degree, which I'll justify in a second. Right, so okay, why is this an important class of algorithms? Well, I claim that this captures a number of interesting and useful things. So if we imagine the input is a matrix, for example, it could be the adjacency matrix of a graph. There are various things you can implement using polynomials, right? One thing you could do is power iteration if you want the top ID. Is power iteration if you want the top eigenvector of a matrix? You can just kind of start with, say, the all ones and multiply by your matrix a bunch of times. It turns out the number of times you need to do this will typically be about log n, and so this is where the log n comes from. It turns out you can also implement things like approximate message pathing, local algorithms and sparse graphs, and so on, but I won't get into the details of that. And crucially, And crucially, you don't have to do power iteration on the original matrix you're given, but you could instead pre-transform it in some way. And this would let you capture more sophisticated spectral methods, such as the non-backtracking Hawk matrix used in community detection or the tensor unfolding method for tensor PCA, if you've heard of any of these things. Okay, so I claim that this is sort of a large class of algorithms that captures. Class of algorithms that captures a lot of the sort of state-of-the-art methods for these types of problems. I should say a bit about where the motivation for this comes from. So, low-degree algorithms actually are already well studied for planted problems, where you do have sort of a planted signal that you're trying to find. That comes from this excellent line of work on the sum of squares method. And we also wrote a survey on this. Survey on this. So it has now sort of been shown that for a whole bunch of different planted problems, such as the ones listed here, that low-degree polynomials really do seem to explain why the problem is hard in certain regimes of parameters and easy in others. More specifically, these low-degree polynomials are able to capture the best-known algorithms, but they can go no further, right? They provably fail in the They provably fail in the hard regime. And so the motivation for this work is whether we can extend this to problems with no planted signal, right, and hopefully show that Lowde-Gree polynomials provide kind of a unified explanation across both planted and non-planted problems for what makes these types of average case questions hard. Okay. So, let me say kind of what the results are. So, this is just remembering the setup, right? We're given a random graph. Goal is to find a large independent set. And there's this factor of two difference between the true optimum value and the one we're able to achieve with all known polynomial time algorithms. And sort of one of our main results is that no low degree polynomial can surpass this barrier. Can surpass this barrier. To be a little bit more formal, this is just saying that no polynomial of low degree can sort of succeed in some sense. The notion of success is a little bit technical and I won't worry about it too much. But basically, you have to output something that's approximately an indicator vector of some set, and then that set has to be almost an indicator. And then that set has to be almost an independent set. You're kind of allowed to make a small number of errors. But what's sort of crucial about this definition is that it's defined in a way so that below this threshold, low-degree polynomials do succeed in this sense. So we really are, you know, we're not cheating. We're really proving that polynomials succeed up to this point, and then something suddenly changes and they start to fail. Okay. Okay. So I want to give you a rough idea of how the proof of this type of thing would go, since conceptually it's actually not so complicated. So how do you prove that all low-degree polynomials fail to solve this kind of task? So if you're familiar with the previous work on this stuff for problems with a planted signal, the techniques are actually totally different. So for a planted signal, Totally different. So, for a planted signal, the techniques are largely based on linear algebra, possibly with this trick using Jensen's inequality. But those types of things totally don't seem to work here when you have a non-planted problem. So instead, we will use a fairly different approach, which has sort of two steps. The first step is to somehow show that low-degree polynomials are stable in a certain sense, that when you perturb their inputs, the output doesn't change too much. Inputs, the output doesn't change too much. I'll show you the formal statement of this soon. And then we're going to also appeal to some variant of what's called the overlap gap property. This is inspired by these works listed here. And this is some kind of a structural property of the large independent sets that basically rules out any stable algorithm. Okay, so let me try to say a bit more about each of these ingredients. About each of these ingredients. So, what do I mean by low-degree polynomials are stable exactly? Okay, so let's imagine that y is IID Bernoulli P. So, Y you should think of as like the random graph, the random input to your problem. So here, P would be sort of the edge probability, and the dimension of Y would just be like the number of edges in the graph, let's say. And I want to think about what's called the interpolation. To think about what's called the interpolation path. And what this basically just means is that at each step, I'm going to start with just a random graph or a random IID Bernoulli vector. And then I'm going to resample one coordinate or one edge at each step. So here, the first one is just random, and then you start by resample. You start by resample coordinate one, and then at the next step, you would resample y two, and then the next step you would resample y three, and so on. And by the time you get to the end, you've resampled all of the edges. M here is just a dimension of y. Okay, so you imagine we start with a random graph, we fix an arbitrary order on the edges, it turns out to not matter, and you resample them all one by one to get. Resample them all one by one to get this sequence of correlated random graphs. Is that okay? And now what's the actual statement? So if I take any low degree polynomial, right, this is a polynomial that sort of takes in y, outputs some vector. Well, I'm going to say that the ith step of this path is going to be called C bad. Is going to be called C bad if basically the output of f changes a lot at that step, right? If the difference between this vector that f outputs at step i versus the vector that f outputs at the next step is sort of big compared to the typical value of f's output, right? Kind of parametrized by the c. And so here's the actual theorem on stability. Stability. You know, here's an informal statement. It says with non-trivial probability, F's output is smooth. Smooth meaning there are no big jumps as you go along. Right, so with some probability, it's not high probability, but it's sort of non-trivial. It's only P to the D instead of, say, P to the N. With non-trivial probability over the choice of these random sequence of graphs, there will be no bad. There will be no bad edge on the whole path. So, with non-trivial probability, the entire path is kind of smooth. Is this okay? So that is the precise sense in which we'll need to have this stability. Now, let me talk about the second ingredient. So, that's this overlap gap property. So, this comes originally from the work of Gemarnik and Sudan. And so, what does this say? So, now it's going to be crucial to actually think of Y as a graph instead of just, you know, IID Bernoulli's. So, for a random graph drawn from our model, this is saying there do not exist two independent sets with certain properties. So, let me draw a picture of what this is. This is just saying that This is just saying that you have these two independent sets. Individually, they are both large. Okay, this factor of one over root two. So remember, this thing is the real algorithmic threshold that we're hoping for. Here, I'm going to actually, assuming that they're a bit larger, right, I'm going to first show you how to prove a weaker result that doesn't get quite to this threshold. To this threshold. Okay, so I'm assuming these two independent sets are quite large, and crucially, their overlap is exactly a specific value phi. And remember, this is saying that with high probability over a random graph, there will not exist any two independent sets that intersect in this way. Right, so what this is saying heuristically is that. This is saying heuristically is that if you look at all the large independent sets, it's okay to have two large independent sets that overlap a lot, right? But they're kind of almost the same. You know, it's okay to have two large independent sets that overlap just a little bit. But somewhere in between, there's sort of this forbidden overlap, right? That if you look at overlaps that are exactly phi or very close to it, that you never will see two independent sets that overlap in exactly this way. This way. Okay. And the proof of this is actually quite elegant and simple. It's just a first-moment method, meaning you compute the expected number of these sort of bad patterns that you don't want to see, and you show that the expected number of these is as little low of one, implying that with high probability there will be none in your whole graph. Okay. So, let me now, in the last few minutes, tell you how to put these two things together. It turns out what we actually need is a slight strengthening of this OGP. We actually need something you could call ensemble OGP, which means that if I draw this correlated sequence of graphs like before, where I resample the edges one at a time, then it's still true that you never find two large independent sets that intersect. Large independent sets that intersect in exactly this way, even if you're allowed to take one of them from, say, this graph and one of them from another graph. Right, so even if I'm allowed to take S just has to be an independent set in one of these, T just has to be an independent set in one of these other ones, potentially different, you can still never find sort of this bad overlap. Okay. This again can just be proven by the first moment method. And it turns out the bottleneck is actually kind of when i and j are the same, which is the case from before. So putting these two ingredients together, how do we actually prove failure of low degree polynomials? Okay, so let me try to sketch the proof. So I'm going to prove that here that low degree. Going to prove that here that low-degree polynomials can't reach this threshold, right? So, this is not quite the optimal one, right? Because this factor shouldn't be there, but this will at least give you an idea. Okay, so let's assume on the contrary that we do have a low-degree polynomial that outputs all that almost always outputs these large independent sets. Then we're just going to take our f and apply it to each. And apply it to each of these random graphs, right? So these are the sequence of correlated random graphs, like I talked about before. And we're going to sort of get an independent set at each point, right? So each one of these graphs will map to some independent set. Maybe I should draw it more like this. And so there's a few things we know. The first thing, which I haven't talked about yet, is sort of separation, that the first and last graph. The first and last graph, these are just two independent random graphs, and it turns out you can show that any independent set in this one has to be far from any independent set in this one, right? So this first one is sort of far from the last one. And like we talked about before, there's also stability, at least with non-trivial probability, this path will sort of be smooth, right? The sets can only change by a little bit at each point. But of course, if at each step, your independent set only can change. Your independent set only can change a little bit, and you eventually need to get to something far away. Well, at some point in the middle, you're going to find something that has exactly that forbidden overlap phi, right, which would contradict the overlap gap property, right? Because we know that there did not exist any two independent sets of that overlap. So, this tells you how to sort of take use of If there were a polynomial, you could use that to sort of construct an instance that contradicts the OGP. Okay, so just to wrap up with a few comments, it turns out you can improve this to the optimal threshold by using something a bit more sophisticated. Instead of an OGP that just involves two sets with a certain overlap, it's a more complicated forbidden structure that involves. Complicated forbidden structure that involves sort of a whole bunch of different sets that have certain constraints on how they overlap. That type of idea is roughly inspired by this work. We have similar results for the p-spin model, although we don't get the sharp threshold. And this leverages some deep results in spin glass theory. And we can also rule out the Langevin dynamics algorithm in addition to low-degree polynomials. Re-polynomials. Some follow-up work recently has taken these types of techniques and applied it to also the random K-SAP problem, kind of establishing a sharp threshold there as well, where low-degree polynomials work only up to the kind of conjectured threshold there. And sort of more generally, I think this line of work is sort of interesting in that. Line of work is sort of interesting in that it connects different types of heuristics from hardness, right? So, this is a formal connection saying that OGP implies low-degree hardness. All right, so thank you for your attention. That's all I've got. Yeah, okay. Thanks, Alex. Thanks. Any questions for Alex about low-degree polynomials or? Polynomials or independent sets or solutions.