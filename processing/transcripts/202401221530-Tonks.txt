Insonance algebras. So thank you guys for giving me, I think, the easiest topic. But there's so much that I will leave a lot out. So I'll also change the title slightly. So let's start with the easiest possible. Possible setting incidence card of a CP of a process that's the easiest possible classical setting. You start with a locally finite Well, a locally finite poset in a second. So you've got a vector space CP for the basis. This vector space is all the pairs C P Q P C Q or it's easier to think of the basic elements as Think of the basis elements as intervals. So your basis of your vector space is all the intervals in your first set. And then you define the co-multiplication. You co-multiply an interval by decomposing in all possible ways. In all possible ways into some intervals and the co-unit, well the co-unit of an interval by vector space over q, for example. Of a q, for example. So the covenant is either 0 if p is less than q, or 1 fp equals q. So that's the very, very classical definition of instance co-algebra. Okay, let me make sure I understand something. Let's say my coset is just a square. Right? So let's say capital P is just a square, so like the power set of two elements. Then does the coproduct then have four terms? Four terms? The coproduct of 0, 0 to 1, 1? Right, so empty to 0, 1. Yeah. So the coproduct has four terms in that? Yeah? Okay, thank you. Yeah, exactly. I was thinking you were asking about my handwriting. Uh so uh I should say this sum should to make sense be finite, so I should here assume some local finiteness otherwise it won't work. Don't need to say anything else. I should say also that this co-multiplication is co-asensitive, and co-asensitivity is just transitivity. Transitivity. Okay, instance algebra, which is my title, is just the linear dual. So Vincent algebra is the linear dual. Okay, now let me say this in a different way. Let me say this in a different way. So, Cp is a co-algebra, Q is an algebra, and so I can take the product here as being the convolution product. So, you get convolution of maps from co-algebra to algebra. So, if I'm given two elements, what do I call them, A and B? So, I'm given A and B two elements, two linear maps. Yeah. Then the convolution product is co-multiply apply A and B and do the canonical multiplication. Okay, so we have the convolution product and what we can also see What we can also see is that, well, clearly, A and B are determined by the values of the intervals, because the intervals are the basis for your vector space. So A determined by the values and intervals. So determined by this matrix, which is an upper triangular matrix. Up a triangular matrix. Upper triangular in the sense that it's zero if P that's just a convention. Set it to zero if P is not less than or equal to Q. And you know that upper triangle matrices are invertible if they're diagonal entries invertible. So this matrix is invertible. That's a ball with respect to the convolution product if all the diagonals A T P are non-zero. And again, very classically, the what's called the Merby subversion principle. So, classical example: the zeta function of the poset is a particular element in instance algebra. So, this is okay, maybe I shouldn't label it with capital P. Since I know what to say, PQ is. Q is 1 for all P less than or equal to Q. So obviously this is invertible because the diagonal elements are 1 as well. So we have what's called the Mobis inversion principle. Let me say it this way. So B is The easy function convolution A, if and only if A is, let's write, zeta inverse equal to mu, the Mobius function for the boset. So the Mobius function for the boset is just a way of inverting this. So that says if beta is zeta beta is zeta can volve with a then a is Mobius function can volve with b so classically this just says that if BPR is the cumulative sum of AQR okay that's the same as convolving with zeta then we can invert that that's P that's equal to Q that's the classical Moe's inversion principle That's the classical Mobius inversion principle. A P R is sum over Q Mobius function PQ EQ. So using this upper triangular matrix notation, you can see that the convolution product is just matrix multiplication, and that's just one example. I won't do a general theory of Theory of such classical things. Okay, first generalization. First generalization is the instance co-algebra of a category. So we've been talking about this earlier. So if X is a category or its nerve, so a one siegel set Set. Then the convolution algebra or colour algebra, we have the basis. As before, the basis here was all p less than equals q, exactly the same here. The basis is all arrows. So in other words, the basis is x1. So I just say b Say b is x1. That's the basis of the vector space. And the co-multiplication is the sum copying this formula for categories rather than for posets. I just say all the ways of decomposing F into a product HG and G H. H. And just for later, let's rewrite this simplicially. So we've got all of the two simplices such that d1 sigma is equal to f and we've got d2 times d0. Okay, that's the formula which I'm sure most of you have seen before. Okay, so what about the co-unit? Okay, this is completely trivial. So the Kevin is, well, as before, it's one if f is the identity on some object and zero if f is not an identity. Okay? But now I'd really like to write this in this language. Okay? So it's the sum of what? It's the sum of one. It's a sum of one. Okay? What's it a sum over? It's a sum over all p in x0 such that s0 p is equal to f. Okay? So this is a sum over all sigmas with d1 sigma is equal to f. This is a sum over all p's such that s0p is equal to f. Okay? Um. So, apart from the notation, all very classical so far. I could talk about the dual, so the category algebra or the Or the instance instance algebra once again we just take the convolution algebra into Q and now I want to go to the second generation The second generation, but now I need a big detour into explaining the language. So the first step was to upgrade from post-sets to categories. And again, I should really say categories of some sort of finiteness condition, modus categories, whatever the definition should be. Second generation is what we call Second generation is what we call talk about topic linear algebra. Or, you know, I once gave a talk called linear algebra made difficult. Okay, so what do we do? We upgrade, well, we replace Q by, we had this discussion earlier. Somebody was saying, well, it's either sets or groupoids or infinity groupoys. Groupoids, or infinity groupoids, or spaces, or can some digital sets? Well, I just say spaces. Okay, so we place Q by spaces. So instead of saying we're talking about a Q vector space, no, our coefficients are not spaces or infinity group boys, we're replacing vectors. Okay, what is a vector? A vector is a collection of Of, okay, let's say we have a fixed base, a basis for our vector space. A vector in a vector space with a given basis is just a collection of scalars indexed by a vector is a B-indexed family of scalars. Okay, so V is the basis of my vector space, and you agree that a vector is just a collection of scalar, well, almost all zero, because you have a finite linear combination of basis vectors. So you have an almost always zero B-index found in its galaxy. Okay, so let's upgrade that. If our scalars are replaced by spaces, let's write S for the category of spaces or the category of infinity group loads or the category of sets. Then here I'm going to have, well, I could either say let's have all the comma categories S over some S over some object B. Then, if you look at what this is, well, it's some map from X to B. So, if you think what this is, this is a collection of all the fibers. So, you can think of it as a B index collection of fibers plus some gluing data. Gluing data. Or you could think of it as the function category. If B is an infinity group voice into the category of infinity group voice. And in some sense, I want to say that these are both the same thing. All I want to say that, well, if you're talking about finite dimensional vector spaces, the dual is just still the category of finite dimensional vector spaces. But I'd like to say this is really some sort of duality. Some sort of duality between the slice categories, the common categories, and the functions categories, the pre-sheaved categories. I don't know how many words we can be in. B here is a set, the basis set. B here is any object of S. So B could be a set, B could be an infinity group void. Okay, so for the examples, we know that the basis B earlier we knew what we meant when two vectors were equal. Here, are 2x is equal. There's a heating diagram to B where the map between total spaces is how much of the equivalence? But it's Well, it's considered all of them. Okay, so yes, I don't I don't don't want to talk about equality, that's much too much too strict. Okay, so let's consider this. So what's next? After talking about the ground field and the vectors and the vector spaces, so vector spaces are now all things of this form. All things of this form. What about the category vector spaces? What are the maps? So the category vector spaces on this side. Here, I'm going to have all the slice categories, all the common categories of this form. Here, I've got linear maps. Here, I'm going to have some sort of linear functors. So, what I mean here by linear functor. Well, let's go back here. A linear map, if you're given specified bases, a linear map is a matrix. So linear map, B to W, if you're given your bases, then you've got a linear map. A linear map. I want this to call it Mij. So a linear map is going to be some sort of matrix. On this side, I want the same thing. But as a vector was an element in a slice category, well, a matrix will also be an element in a slice category. So now I'm going to say that a linear map is going to be represented by an element. An element. It's not quite right. I mean, for intuition, it's not very good. Better would be to say that I want to think of a linear functor as something of this form, say some sort of span. And then this span represents a linear function. This fun represents a linear functor, actually represents a functor where it represents an honest functor, or an honest infinity functor from this given by what you do. If you're given an object in here, you're given something from an object from S to B, then you can pull back along. Then you can pull back along M. Make this pullback, and then you can compose that for this. So post-compose. So a linear functor is just a particular example of a functor. A functor from this slice category, this slice category obtained from Category obtained from a span by doing a pullback followed by post-composition. Okay? And if you want to make it look like a matrix, you still can. You can take the fibers over, I mean the fiber over an object of here and an object of here simultaneously. So you can still think of it as a collection of infinity groupoids, Mij, indexed by objects of B and B prime. So you can still think of it as a matrix. So you can still think of it as a matrix. Ah, matrix multiplication. We know how that works. So over this side, I've got Mij, that's right. M N I K. Okay, we've seen this before. On this side, how do I actually? On this side, how do I formally compose spans? Well, again, I think we'll know this. If I want to compose the span from b to b prime and the span from b prime to b double prime, then the composition of this linear map with this composition. map with this composition of the linear map represented by the span represented by this span is represented by the pullback. So take the pullback and that represents the composite of the linear functors represented by the original spans. So pullback of spans represents It represents the composition of the values. Okay. So I suppose I could say, the regular definition says that lim is the monoidal infinity 2 category whose object Two category whose objects are all slices whose morphisms are the linear maps whose ah I said monodal. So I also want to say that the tensor is just going to be modelled by the Cutesy product. So it's a monoidal category, and that gets me into the situation where I can now talk about what a co-algebra is. So I have a monodole category, and I can talk about algebra and co-algebra in this monodal category. So, again, I hope you all know what I'm going to say. So, a cardium in this particular category is, well, it's an object together with a co-multiplication. Comultiplication is going to be a map in this category, so we're going to have something represented by a Something represented by a span. So it'll have to be a span from well, the co-multiplication should be something of this form. So it's got to be a span. I called it N for some reason. Okay, and then we also want a co-unit. So that's the ground field. S is the ground field. So we need a span representing the co-multiplication and a span representing the co-unit. And then we have to write down. And then we have to write down the axioms of co-associativity, etc. Let's just do co-associativity. So, if I take the co-multiplication, so the co-multiplication goes from i to it times i, and I co-multiply again, so I do This is I times the co-multiplication, identity times co-multiply. Okay, so that's delta and identity tensor delta. And in the other direction, I do code multiply and And then n times i. Okay. So here I have, I haven't given these things names, I well, you know, you can guess what labels I have to put on these arrows if I've named them. But I also want to compose these two spans, and so the composite is the same as the composite of the composition of these two spans. Composition of spans is given by Pullback. Composition of spalance is given by pullback. So, if I want to compose delta with identity tensor delta, then I need to take this pullback. Whereas this one, I need to take this pullback. And in both cases, to be co-associative, the objects they get here. The objects they get here, together with the composites, this should give the same unbiased co-multiplication into three parts. Okay, so these should be the same with both pullbacks. So this data defines a co-associative multiplication if these two pullbacks give the same middle span. I need this. Yeah. I need this middle span to be the same unbiased decomposition of I into three factors. Okay. Okay, is that enough kind of general nonsense? Yeah, okay. So now I can go back to incidence card. Instance instance guardians. So if X is a syndicial object. So what I'm going to do is look at these two formulas here. This formula here says all objects from X2 whose D1 is F and then return D2Z0. And this one says all objects whose S0 is F. Whose S0 is F and return just one. So for X is a simplistical object, I can define the incidence card by the spans. So the colour multiplication, I take X1, the arrows, I then do a I then do a pullback of the arrow to see which triangles have D1 equal to the arrow I'm given. And then I return, for each arrow of that kind, I return the first arrow and the last arrow. Okay, so D2D0. Okay. X1. Too close. And so that's the co-multiplication. And the co-unit, take each object, see which arrows are hit by the degeneracy of this object, and return one video. That you can see this one gives me. That you can see this one gives me a co-multiplication and this one gives me a coherent so what's next something messy implement this diagram Implement this diagram to say whether it's a co-units, uh co-associative or not for this data. Okay, so we can do it. Obviously, this will not be co-assistive for all simplist objects. For which x, For which X is this current there's no tries this for getting the answer. I think we all know the answer. Okay. Do I have a diagram I can copy without making mistakes? Probably not. Okay, um Um so what's our span? Our span is x2, x1, x1, x1, x2, x1, x1, x1 x2. Okay, that's the diagram. Now I need to do the pullbacks. And I need these two to be equal. Now I don't think I'm going to try and claim an if and only if. I'm just trying to say that it's co-assensitive if x is a decomposition space. Well, if x is two siegel. Okay? In this direction, I think we can check it. I think we can check it. So let me just put in the projection to so if I project away from the identity project onto the second factor. So if I project this onto the second factor, this is a pullback. I mean that is this is d1 times identity. This is d1. This is D1. Okay. Same thing here. If I project here, so this is identity times D1. So this is D1. Okay. So then by our favourite pasting lemma, PRISM lemma, this This square here is a pullback if and only if the outer rectangle is a pullback, which by the decomposition space axioms we know is true if we put x3 here. Fill in the labels of the arrows using your focus simplish identities, which I've not done. Same thing here. x3, x2, x2, x1, x3, x3. So we will get the unbiased decomposition into three factors precisely, well, certainly if X is two-sigma. Questions about that? At this point, you could have also just reversed all the spans and just spoke about the social duty. But maybe later there will be some finiteness conditions which make this direction preferable. I think it's fair to say this is where we started many years ago. But if you linearize further and then you do some kind of transfer, like pull-push, then it may matter sort of in which direction you do this. You're just seeing somewhat different type of finance conditions. That may kind of make one of the directions. Maybe I should talk about that. Um yeah, let's talk about some finite x conditions. The oh no, I want to talk about homomovolums. Okay, so classically I think you can imagine that any all of the any post-epetomorphism will induce a calgebra homomorphism between the instance calgebras. Okay, so if I have a poset homomorphism, so if that's order-preserving, then the coalgebers will have a naturally induced coalgeuborphism. But that's not going to be true in this case. So the next question is. So, the next question is. And I think it's why we can shoot for projects. No? Okay. Good. I was wondering. Okay, special case of what I'm going to say next. How is order preserving different from being a post-immorphism? Poseidomorphism is the same as order preserving. As order preserving? Yeah, but they're just saying that not every post-A. No, what I'm saying is this won't necessarily preserve the co-multiplication. I mean, there will be a function of a linear map here induced by this. But the question is whether this will be a co-multiplicative linear map. That might not be convex. The extra R is between. Exactly, exactly. Let's put convex here. And anything else? Anything else? Okay, so let's do the grown-up version. So, definition A map from Y to X of some visual objects. Now I don't know if there are alternative names for this going around, but for us this is called Kulf and if for all active Active now you'll know active means active means I should put a bar here is that right the naturality square the pullback so I have a naturality square one Square Y N Y M F N and I want that to be a pullback for all if the naturality square for all active maps is a pullback. I don't mean I don't really need you to know all of these. Let me just give you the two basic examples. So, for example, if we look at the active map zero, so y zero y one head of CS0. Okay. Well, this would be a pullback. And that's where the C in Kulf comes from. So we're going to say that F is well, if this diagram is a pullback, we'd say that F is conservative and And the other one Y2 Y1 D1 X2 X1 Okay, so Kulf means conservative because this is the pullback and Alf means by the Kault means, well, I interpret this diagram as saying unique lifting of factorizations. So Kault means conservative with unique lifting of factorizations. And then the result, the motivation for the definition. We'll do anything else first. No, that's okay. A cool map of decomposition spaces. Of decomposition spaces of two signal objects induces a cardiovomorphism. Alright, so if you already know that Y and X are two SQL, the two example squares you showed us are sufficient to demonstrate goals. Yeah, for Segal, I think so. Do you want to see the proof? I mean, this is easy. Effectively all I want to Effectively all I want to say is that if we have, well, what does it mean to be co-multiplicative? It means that if we start with the co-multiplication in y, so you co-multiply in y and then we do f one, f one. F1 or we can we could by F1 and then commult by X will say that this will induce a S over y one. S7Y1. Right, so what do I need to do to prove this diagonal bit? Well, what we could do is to split this up into is to split this up into this one is pullback along D1 and then this is post-compose with D2D0 six is better at this okay so why should this diagram commute well Why should this diagram commute? Well, Bec Chevrolet says that if this is a pullback, which is exactly what I've asked for here, if this is a pullback, then this square commutes. So post-compose with F1 followed by pullback along D1 is the same as pullback along D1 and post-compose with F2. So if this is a pullback, If this is a pullback, then this induced map is co-multiplicative. Okay. So, ah, example. So, someone earlier asked. Someone earlier asked why decouage was useful. So I thought I should add examples of decouage. So if we have X and we take Okay, so the natural comparison, the unused top face map. I mean, here we throw away the top face. Is that right? Okay. The unused top face map is cool. So let's just check that for you. Check that for you. That's X. There's D1. Check X. So this will now be to a mean digital top or the bottom. That's still D1. That's the one. That's a pullback. So, what we saw this morning is that if x is a decomposition space, the large is one single object, and also the comparison is that. Of the comparison is a call from that between the one signal and the original two signals. So, what's that example tell us? It tells us that whenever we have a decomposition space, there are two things we can do. We can either take the instance co algebra of x itself, okay, so if x is a decomposition space or a two single object, Or two single objects, then we can form instance cartebra of X. Or we can form the instance coalgebra of Y, where Y is the decader X. And then there is a quadruple map between this instance calgere and this instance calgere. Induced by By the comparison, this calls map morphism of instance cardios. So maybe you're not convinced it's useful yet. Let me give a couple of examples. There's many examples in the literature where, classically, you're interested in. Classically, you're interested in some instance corruptory you can't construct, because you don't know about two single objects yet. So you construct it for the one single object, which you know how to construct an instance corroder of a category, and then you do some quotient map ad hoc to get the instance algebra you care about. So many classical examples. Of instance cardibas CX were constructed by as quotients Okay, where y is opposite or a category. So before decomposition space, before two single machinery was available, this object didn't exist apart from some ad hoc quotient of something based on instance quantum as a process or Instance cartoons of process or categories. So, let me give you a couple of examples you probably do know. So, suppose we have a S-dot construction of a Waltham category. That's what we all like. So, the decades of this, so if this is X from the This is X and the decalage of that is the category of covibrations in Y. So it's a category, or the nerve of this category, a W. Okay, that's a category, that's a decomposition space, that's a category, that's one sigma. One zero. Okay. So does it matter that you switch from the top technology to the bottom technology? Oh. Probably it does matter. And I can't remember which one it is. Actually, I have a related question. It seems to me from that square down at the bottom right sample that lower deck is not not the bot is not cold, is that It must be. Let me just. So, what you're saying is, I do lower deck and then do D bottom. Check. So, same thing. X2, X1, D1, X3, D2, X1. Yeah, this works. What's alpha? Hmm? Okay, I can ask. I can ask. I mean, this is D1 in deck. Oh, D2. Maybe that's what. If I do the lower deck, then the indices shift on the face patch. Yeah? I don't mind answering easy questions. Okay, so what I want to say here is that I can claim that the incidence coalgebra of the category of co-vibrations in your Maltausen category, you can obtain from that the instant algebra of the S-dot construction. Of the STOC construction, which is a Siegel, which is two-seagle or decomposition, and this is the whole co-algebra of W. Okay, so that's one example. The other example, suppose I have the another example which is not a category, but it is two-single. But it is two single. If I take all forests and layers of miscible cuts, you don't know this example. We can talk about it later. And then the decoulage of that, of that simplification object, is for cuts. The decolage of that is The decorage of that is the category of, or the nerve of the category of forests and inclusions of forests preserving the rich. Take all the inclusions of forests that send roots to roots. That's the category. And that category is the decoulage of the simplificational object government by forests and the vertex. So, in the same way as here, you can obtain the start with the algebra instant algebra of This is the calj of the category of these inclusions of forests and obtain the co-algebra of trees and cuts. Okay, that's just that. That's just based on the question we had earlier. I have four minutes left. What should I skip? I suppose I should actually go to the title and actually talk about this as algebra. And I should talk about this as algebra. So the dual incidence algebra, well, we know what it is. It's the convolution algebra of, I want to look at, on in Lin of of this Karajiba, this is Kahjiba of a decomposition space. Of our decomposition space. Okay, so if I have two objects in here, well, this means I've got something represented by a span from x1 to 1. I've got two things in here, A and B. If I want to do the convolution product, I've got to, well, I've got to, you know what we do. First of all, we do the co-multiplication, then we compose of the product, so the convolution is exactly is exactly x1 x2 x1 plus 1 a and b might have different okay that's what I meant next one next one thank you one okay so the complex Okay, so the convolution is, I can call it P. So that's how you, in instance algebra, you can think of the basis as all spans, or you can think of it as maximum h x1. And the convolution of two of them, the product of two of them, is this pullback. In this instance, as usual, we know the algebra contains some special elements. So we know there's a convolution unit. The units with respect to convolution is just going to be given by S0 and the we know there's a zeta We know there's a zeta functor. In the same sense, in the composition algebra classically, there's a zeta function. Well, the zeta functor is just going to be the identity here. We can talk about well, we can try and say. Well, we can try and say in what sense we can invert these matrices. Well, we can't, so let me just end with one result. So Mobis inversion doesn't work. This inversion fails, but only for the obvious reason that For the obvious reason that all we can do in infinity group points or spaces is take the sum of spaces. We can't. We don't have negative spaces. We don't have anti-spaces. We don't have virtual spaces. So Webis inversion works, except there are no minus signs. So you can, so if x is a decomposition space with the property that S0 is. Is fully favourable, then you can very quickly consider in his algebra these spaces of non-degenerate cells, non-degenerate syntheses. Cells, non-determined sympathies. And then you can prove that the zeta function involved is this element is just the sum of these two. That's a little pullback argument. I won't do it now, there's no time. But as a consequence, But as a consequence, you're going to see that if you do zeta convolved with the sum of all the even ones, that's the same as the sum of all of them, which is the same as the unit plus the sum of all the odd ones. I should bring the CETA outside. Okay, so what am I trying to say here? I'm trying to say that if you do zeta convolved with even, you get a unit convolution unit plus zeta convolved with odd ones. Plus zeta can volve with the odd ones, just because one goes to the next ones. And you can think of this as saying that zeta convolved with if there were minus signs, then you would have a convolution inverse to zeta. So you would have a Mobius function, except you need a minus sign. Minus that. But you can prove this. This is an equality of infinity, or an equivalence of infinity group weight. And I should stop. Two minutes ago. And delta was called epsilon before. By fully faithful example of spaces, do you mean it's an inclusion of what you write on the right? Concom plus Freemason. CY to CX, you've got something next to it, but what did you write? I've got Con Concremer. The Concremer College Common. Cadiba of trees and cuts. Okay, so we have a category card here, and we have a classical conclimber trees cardio here, related by this d top. Was it the bottom? The case that I started with the stable Ethereum category, I have X is the S dot. And X is the SDOT construction, you mean the space of, say, AM representations. That's a case where it's a decomposition space, and that map is one faithful. What does the Mobius inversion formula look like? What is MLR? So we would have all of the Practically, if it's just the vector space, let's say they have a finite field, then some physical formula with minus one to the n and some binomials and q to the if you if you're just looking at the even the posted example, this is just the change. Just the chains of odd lengths in the process and the chains of even length in the process. Probably an obvious question, but is it possible to take computations that you make of this capital limb category and decategorify into statements? Yes, that was something I didn't have time to do. So, can I have two minutes? Can you Good question. Is it possible to take construction studies capital lin categorical, sort of spatial in nature, and decategorify them to get theorems about classical sense all those processes and whatnot? I don't think you get new theorems, but yeah. So there's a nice notion of cardinality. So given any infinity, well given any groupoid, or given any set, you can get a number by taking cardinality. For a groupoise, you would the number would be well for a group it would be one over the goddamnity of the automorphisms. For an infinity groupoid, I think the best way to say it is alternating product of the alternating product of the yeah, so you know, it's some of the components, the alternating product of the minus one to the n of the cardinality of the Cardinality of the fundamental group. So you need finiteness condition where finiteness now will mean either the sum converges, product converges, or maybe just say all the homotopy groups are finite, for example. So once you've got the notion of cardinality, you can also say that, well, if you have anything in Lin of the form S over X1, you can. X1, you can associate to it a vector space q pi 0 x1. So think of this as the basis and think of this set as the basis, silicon variance. If you have any cards within here, if you take cardinality, you can get cards over here. There's something a bit strange about the cardinality because if you're given a span, if you're given a span, A span, if you're given a span, so this is a linear map here. I want to know what the linear map, because what the linear map is, is here. So I'd like to say, well, here I've got these mij. I'd like to say, just put the cardinality of mij. But it's not quite right. So the matrix you get here is not quite the matrix of the cardinalities of the fibers here. The reason is that. The reason is that you've got this on one side. Oh, right, so you've got maybe got this on one side and you've got this on the other side. They're really vigilant. And so the cardinality, you've got some factor of the size of the component. You've got to multiply by. So you've got something like cardinality of J. J. So this is the matrix you have, okay? And then you can check that this does become a completely functorial. Every theorem you prove here, you can take its cardinality, but it had to do some strange. Well, we thought about taking square roots and using square root of i and square root of j to make it symmetric, but no. I think there's some some slight. I think there's some some slight non-symmetry between these two. Because the duality isn't quite an equality. Okay, but if you understand that this matrix has a matrix of rational numbers given by something a bit strange, then yeah, everything you do here, a monoid, a co-algebra or an algebra on this side, becomes a co-algebra or an algebra on this side. And thank you for the question because that was just the thing I didn't have time to do. So you proven the CULL condition for co-algebras? Is that the most general? Ah, is it an if and only if? I'd like to say yes, but given we had the surprise that on objects, associativity of A sensitivity and clientality. Maybe for morphism something else is something weak is enough, but I don't think so. I think Gulf is necessarily and sufficient for the level of correspondence. It should probably be clear. Maybe after you linearize it, it could happen, doesn't it? Can I use this as an excuse to say one more thing I didn't have time to say? To say one more thing I didn't have time to say. I said cool to implies homomorphism of co-algebras. I didn't say what would give you a homomorphism of algebras. Something different, which I think we've also seen.