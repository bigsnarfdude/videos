So, we're delighted to have Tami tell us about efficient algorithms for random graph matching. Okay, yeah, thanks, Miki. Okay, yeah, so hello, everyone. Welcome to the talk. So, I'm going to talk about this random graph matching at auto threshold by counting chandeliers. This is actually based on joint work with Chen Mao from Georgia Tech. So, he will actually speak tomorrow morning. So, he will actually speak tomorrow morning, and then Yi Hong Wu from Yale, and then my student Sophie Yi at Duke. So, personally, I like this work a lot. As you will see that the results is kind of a clean, and then but the idea is actually very interesting, like these chandeliers. I think in the Asian language, this is called a stinging trees. This idea was actually proposed by my student Sophie. Yeah, I'm going to try to explain to you the intuition as much as I can. As much as I can, but probably Sophie is the best person to explain this. In case you have any questions, you can always contact her to get more intuition. Okay, so okay, so let me just briefly talk again about this motivating example in network deonization. Just try to make sure we're on the same page, like so we know this graph matching problem. So, this is actually a very nice kind of example. Like, imagine you have a group of people who are gonna fit. Group of people who are going to form a friendship network, say, on one digital platform, let's say Flickr, which is a photo sharing website. And then you're going to see the node identities and also the edge connections. But now imagine the same set of persons may actually form a different friendship network, say on Twitter, right? But now let's say due to the privacy constraint, like when the Twitter release the data, they do this so-called network sanitization. So basically, it's like removed identities of the nodes, and also then they actually perturb. Nodes and also they may actually perturb like some of the edges. Okay. But now the issue is that in this situation, based on the network structure, we can possibly actually match the nodes in the two graphs. And if we can do this successfully, then we can use identities that we observe on the flick network to actually recover the removed identities on the Twitter. Okay, so in fact, so the previous researchers in this social network analysis they actually indeed are looking Now, analysis, they actually indeed look at these real data sets, like real social networks on these two digital platforms. They're able to actually identify a large portion of users between these two networks. So, this is actually indeed quite influential in the sense that it's kind of in contrast to conventional wisdom that the network is private after you removed identities. Here, they're saying that even this is not that private. So, when the companies release the network data, they have to think twice about how to best. Have to think twice about how to best protect the user's privacy. I don't work on this, but you guys will say this many times. But what strikes me as a little strange is that here, all this research, right, on algorithmic side, essentially, right, especially, right, is trying to help the bad guys. Usually, the way you say it, right, and the way it works, right, if you say like in crypto, blah, blah, blah, so let's help avoid this, right? Let's help all your work to like, you know, Snatch. So, like, you know, it's a natural question: who's like funding this? I mean, not in the sense of downloading, but just mentally. Yeah, it's kind of work on this from JHU. It has a fund from NSA to you on a social network. Yeah, yeah, I want to say that variant application, right? But Yeah, it is kind of an upper analog. No, no, I know, of course, yeah. Yeah, it's kind of like the animalized model and block, right? Because otherwise, it's not identifiable. Yeah, it's like biological kind of works. Yeah, I'm going to mention that. Sorry, it's in the middle. Yeah, but I guess another way to think about this is this is like the two sides of the coin, right? Here we're thinking about attacking, but I think the reason why we want to understand this is hopefully we can get a better. Is that hopefully we can get a better like anonymization scheme? So that's the only thing you're saying that for whatever it can do, yeah. I think, um, I mean, uh, not really, right? Like when the trader releases network data, they can decide like how much fraction of the edges they want to kind of perturb. So that can be possibly like, right? So there's, but then there's always a trade-off, like the utility versus privacy, right, as you can imagine. Manager. Okay, yeah, I guess there's also motivating examples in biology, but I'm not going to the details in case you are really interested about this application. I can talk more. But instead, to this particular crowd, I just want to say another aspect of graph matching that I feel like this is a reason why I got into this particular problem initially. You can view like this many planted problems, at least conceptually, as a form of graph matching problem. Of form of graph matching problem. For example, let's look at our favorite plant-click problem. In this case, we observe one graph, right? But then you can always view this hidden click as a template. And then you can view the problem of finding the click as a way of matching this particular template, like this hidden small click, to the observed graph. Okay. I mean, they're not exactly the same, but this is conceptually at least shows that graph matching can be viewed. Sorry, the point click problem can be viewed as a graph matching. Click problem can be viewed as a graph matching problem. You can also do the same kind of analogy in this stochastic worker model. In this case, the template is like two disjoint clicks, and then you can try to match this template to the graph that you observe. Another favorite example is actually the so-called planted Hamiltonian cycle problem, or you can view this as a planted version of Chauvelin-Selsman problem. Here, the template is this big like cycle, and then Cycle, and then you want to actually match this cycle to the graph that you observe. If you can do that, then you kind of recover this hidden Hamiltonian cycle. Okay, so you can see that in many of these planted problems, you can view as sort of graph matching problem, but you can see that in this problem, these templates, they have very special structure, like click or low rank or like cycle. So, this naturally kind of inspires us to look at beyond these specific templates. Specific templates. One example is that let's just look at other shares in the graph, let's look at two of them, let's try to match them. So that's another kind of aspect, at least from a theoretical perspective, like why we look at this graph matching problem. But think about this, this graph matching problem. In the real world, the networks are in large scale and complex. So we naturally face this kind of computational challenge. If you want to look at all the possible mappings between the two networks, this actually grows as n factorial. Growth as n factorial, where n is the number of nodes in the graph. Also, as we can see in many of these applications, the networks that are given to you may not be exactly kind of isomorphic. They could be edges deleted or added in one graph or the other graph. So that's why ideally we want to develop some efficient algorithms that can tolerate as much noise as possible. Okay. Okay. So at least in this general field of graph matching, a very canonical. Field of graph matching, a very canonical model is actually known as this correlated auto-Shangyi graph model. And this actually was proposed back in 2011, where they actually studied this network privacy problem that I mentioned earlier. So let me actually walk you through this model. I think the previous talks motivate the example, sorry, this model from the so-called sub-sampling model. You have a parent graph and you sub-sample twice to get a two-children graph. Another way to look at this problem is really look at this two-children graph. Is really look at this two children graph directly and then look at their edge correlation. So let me explain to you. So basically, imagine you observe two graphs. They are other Shang Ye-Randian graphs with n nodes and the edge probability is Q. And then here we're going to further assume that these two graphs are correlated, in particular edgewise correlated under some hidden vertex correspondence that are denoted by the dashed line here. So for example, you can imagine this is the first network is this is the the first network is this friendship network formed by this uh yeah uh these characters uh say on on flaker and then this is the second graph is actually this facebook network on the twitter and then this uh hidden node correspondence is actually the true kind of uh mapping between these different node identities okay and now you can imagine if you look at uh say Mario with Luigi whether they're friends on the first graph Friends on the first graph should be correlated with their friends on the second graph. So, a natural way to model this correlation is just to think about these are the two bonus random variables, but they are correlated through this parameter rho, where rho is an edge correlation. Okay. And then basically you do the same thing for every pair of the other nodes, and then you get this correlated order sharing graph model. Make sense? Okay. Okay, there's another parameter, very important. As you can see, n times q is actually the average degree for either one of these two graphs. So when nq is large, the graph gets denser. And also, I forgot to mention when rho actually approaches one, then the graph gets more close to each other. So when rho equals one, then these two graphs are exactly isomorphic to each other. Okay, make sense? And now the problem that we want to study is that now you observe these two graphs. Now you observe these two graphs, but you do not observe this hidden vertex correspondence, and we want to recover it as the network size grows. Okay, is the problem clear? Okay. So let's think about how to solve this problem. Like at least like very natural first perspective, like first approach that we try is just write down the likelihood function, right? And see how you're going to maximize the likelihood function. Well, if you, for this particular problem, if you write down the likelihood function, If you write down the likelihood function, I mean, you have a bunch of Bernounce, right? If you do a little bit of derivation, you will see that this likelihood function is actually a monotone to the total number of shared edges in the two graph. So this actually leads to the so-called quadratic sounding problem. Basically, the hidden vertex correspondence can be viewed as some permutation. And now what you want to maximize is this quantity Aij times B pi hat i pi hat j that basically just calculates whether. That basically just calculates whether you'll have two, like two edges appear in both A and B. And then you take the summation. So that gives you the total number of edges shared in this two graph under this particular mapping pi-hat. Okay, and then you want to maximize this. And this is known as quadratic assignment problem, as many of you already know. I just want to briefly talk about this quadrant assignment problem. Actually, it has many applications. This actually was initially proposed by actually Kupmans and Beckman. Actually, Kupmans and Beckman for actually studying this facility location problem in the logistic supply situation. And you can also view this Koja assignment problem in compass, this famous traveling salesman problem as a special case. So for this particular problem, actually, in our earlier work, we're able to show that this maximum likewise meta actually enjoys a very strong statistical guarantee in the sense that it actually. Guarantee in the sense that it actually achieves the information through equal limit with a sharp constant. Okay. But then the natural question now is that this quadrature assignment problem, at least in the worst case, is empty hard to solve or actually even to approximate. Okay, so somehow this suggests that you cannot hope to directly apply some worst case like approximation results, and then hopefully to get some good estimate in this. Hopefully, to get some good estimate in this problem. So, then this leads to a very natural question: can we develop some more scalable algorithm that can recover this latent permutation, but still at the same time enjoy the strong statistical guarantee? Okay, so that's mainly the kind of the holy grail of this graphic matching problem. Okay, so let me just talk a little bit about the stable art in terms of efficient outwards. Stable art in terms of efficient algorithms. In particular, I'm going to focus on the polynomial time algorithms. Okay, well, let me actually kind of summarize this stable art using this table. So basically, for the rows, I'm going to divide it into according to different correlation. So this is the first so-called extremely high correlation. That just means that the correlation row has to go to one as the network size n goes to infinity. And then the second row is this just high correlation. So row is a constant, but it should be very close to one. Be very close to one, and then last row is some ideal situation that we want to tolerate like a small constant correlation. Okay, and then for the columns, I just divided into different regimes according to the sparsity. So the first one just means that average degree is kind of like sub-polynomial. And then the second column just means the average degree is indeed a polynomial to some small power. Okay. Okay. So now if you look at this, I think Lola also mentioned in her talk yesterday. So when row Uh, yesterday, so when rho equals one, then the two graphs are just isomorphic to each other. Then, this reduced to this very classical random graph isomorphism problem. And actually, many famous people, even Paul Aldos, actually studied this problem initially. It actually goes back to the 1980s. Basically, they show that in this perfect correlation case, you can design an efficient algorithm, actually can recover this latent permutation up to the connectivity threshold. Okay, so in some sense, there's no so-called scale. In some sense, there's no so-called statistical computational gap in this perfect correlation case. Okay. But now, actually, more recently, now people get interested into this problem when actually rho is less than one. So we are in the noisy case. Okay. And there has been actually kind of progressive advancements in this field by trying to kind of push down the correlation of rho, starting from rho equal to like one minus. equal to like one minus one over poly n and then we actually have an earlier result kind of improve this for rho equals one minus one over poly log n and then i think mao and then his co-authors chang and his co-authors actually can even prove improve this further to one minus one over poly log log n okay so you can imagine how this game kind of is played right so very recently like in the last two years there's some breakthrough results basically they're showing that now Basically, they're showing that now the row, the correlation, can be a constant that is very close to one, like one minus some constant, C and C, some unspecified constant. Okay. But then one limitation of this recent result is, as you can see from this table, they actually only work in the sparse graphs. The reason is that their algorithms actually crucially exploit the local neighborhood like a tree with high probability. probability. So the results actually do not extend to the dense graphs. I should also mention that there's another actually very influential published work in this literature by Barack et al. They actually look at this problem. So their algorithm is not polynomial time, but it's very close. It's like n to the power of log n. They can actually get to the correlation row is a little over one for both sparse and dense graphs. But the issue is that their result But the issue is that their result is not polynomial time. Okay. Yeah, like is it actually no? I mean, you mean this one? Oh, no, I understand this one is no. Oh, you mean this thing? Yeah, like for this algorithm, are they action like global or they just like. Oh, you mean this one? Yeah, yeah. Yeah yeah I I think probably like n square because it's a very sparse graph so I think the time yeah is yeah basically I think they just look at a particular node look at a local neighborhood based on that they came up some smart construction of the pattern of the neighborhood and then based on that they can align neighborhood by neighborhood sorry so you mentioned neighborhood by neighborhood yeah yeah exactly yeah Exactly. Yeah. Yeah. So it's kind of like very efficient, at least for this last. Yeah, yeah. In fact, for, yeah. So for the last one. Okay. Okay. But now this naturally leads to a question like what happens in the other kind of space of this table, right? So what happens in the dense case? Can we still develop some polynomial time algorithms and hopefully have a constant correlation rule? And basically our results kind of. And basically, our results kind of fill in the gap in this table. We basically show that now you can match the graphs as long as the correlation rho squared is bigger than this very explicit constant alpha. That is roughly 0.338. And this alpha is actually known as Otter's tree counting constant, because this is actually, if you look at the number of unlabeled trees with capital N edges, this turns out to be exponential in N. And then out to be exponential in n and then this growth rate is just given by one over alpha okay so your result subsumes like all yeah yeah yeah so i guess that's the reason why i kind of showed this uh in the yellow color it kind of subsumes all the previous result yeah any questions okay okay let me actually more formally uh like tell you the results so basically uh we're able So basically, we're able to show that when rho square is bigger than this Altus tree constant alpha, then we can develop some polynomial time algorithm. Actually, it works with high probability. And I can actually achieve the following. So when the average degree, remember NQ is average degree, is above some big constant C, then we can achieve the so-called partial recovery, which means that we can correctly match a positive fraction of the vertices with high probability. Probably. And then, when the average degree continues to grow, like say diverges as n goes to infinity, then we can actually upgrade this to so-called almost exact recovery in the sense that we can correctly match like one minus little one fraction of the vertices. And then when the algebra degree is even higher, say in the logarithmic degree regime, then we can get the exact recovery, meaning that we can correctly match all the vertices with high probability. Okay. So let me mention like two things. The first thing is that our algorithm actually does not have like a mismatching error with high probability. So what do I mean here is that the algorithm will not mistakenly match, say, Mario with Luigi. Okay, so there's no such mistake match. In other words, our algorithm may only return a partial match. So it may actually not match. So it may actually not match like Mario with himself, but it can never like mistakenly match to another person. Okay, so that can be nice in practice if you think about that, because then you don't need to worry about the mismatching error. You mean with high probability there will be expenses? Yeah, yeah, with high probability. But still with high probability. Yeah, yeah, yeah. Still, still with high probability. Yeah, it's not a purely mistake. Yeah. And also, I want to kind of briefly. Also, I want to kind of briefly comment this condition for the exact recovery, this actual condition. And this one is actually, it turns out just to be the connectivity threshold of the intersection graph. So this is a slightly different expression than the one that you see because we look at this like a Q and a row parameters. So if you look at this so-called intersection graph, that just means that the edges in the intersection graph, if the endpoints both appear in the two graphs, like they are connected in the two graphs. Like they are connected in the two graphs. Then, under the two node correspondence, if you did a little bit of calculation, you will see that it's again other shiny, and then the edge probability is given by this particular like expression. And then this condition is just saying that the intersection graph has to be connected. And as we actually talked about this before, this is actually information theoretically necessary for exact recovery because then you would have isolated vertices in the intersection graph. Okay, make sense? So, in some sense, this actually. Make sense? So, in some sense, this actual condition is information threatening necessary. Let me also briefly mention that actually, after the posting of our paper to archive, like a few days later, actually, the Ghanai Masulia and then Semergen, so they actually also propose, like show a different algorithm, actually achieve the partial recovery in the sparse regime when the rho is bigger than this square root of alternative constant. Okay, but they do not cover the almost. But they do not cover the almost exact recovery and then the exact recovery regime. So, because their algorithm is still kind of locally tree-like based. So, they still look at local neighborhood, assume that it's locally tree-like, and then they kind of study this like a message passing algorithm based on this local tree structure. And then that's how they get this same threshold. And they got exactly the same threshold. Yeah, exactly the same threshold. Yeah, yeah, I will actually tell. Yeah, yeah, I will actually tell you later that, yeah, so we kind of believe this is actually the computational threshold, at least in the sparse regime. Okay, let me actually show you some diagram to kind of help you to appreciate this result. So as you probably can see in the traditional like a stochastic block model, so here let me actually parameterize this in a slightly different way. So I'm going to focus on this logarithmic average degree regime. So NQ just equal to So, nq just equal to some constant lambda times log n, and then the x s is the lambda, and the y-axis is still the correlation rule. Okay, and now the first thing that you can say is that I can look at the boundary of the information theoretical limits, and this is actually given by this solid curve here, and then it's actually the formula is actually exactly given by this. Okay, so in particular, in the gray regime, then there's no way you can get exact recovery regardless of the computation. Recovery regardless of the computational cost above this in this possible regime, and the maximum likelihood estimator actually achieved the exact recovery with high probability. And then our results like basically show that as long as rho is bigger than square root of alpha, then you can actually show that achieve the exact recovery in polynomial time. Okay, and then you can see that another thing is that if you focus on this particular part. Focus on this particular part. So, this polynomial time feasible regime actually touches this impossibility regime. So, that just means that in this part, our algorithm is actually information theoretically optimal. Okay. But you can see that as the density grows, then there's still kind of a big gap, right? And this part, actually, we kind of conjectured that it's actually hard, at least like a computationally hard. At least, like, like a computationally hard to actually achieve the exact recovery. Okay, at least in this, like a logarithmic regime. Okay, can you comment on the last line? You mean you mean this one? Oh, sorry. Yeah, maybe I. You mean this one? Yeah, this is the one that I'm talking about. This? Yeah, so I haven't talked about this. Okay, so yeah, so let me mention this. So I just want to give like two pieces of evidence. They are partial. Like two pieces of evidence, they are partial, they are not rigorous. One thing is that this recent work basically since they look at the local neighborhood and then they actually show that if you look at the local neighborhood, it's a tree with high probability. And then if the two nodes are the true match, then these two local neighborhoods are actually correlated Gaussian-Watton tree. Okay, so basically, but then if the two nodes are not the true match, so Not the two match. So then the local neighborhood is kind of approximately like independent Galton-Watson tree. So they show that if you just look at a hypothetizing problem based on the local neighborhood, whether they are correlated, Galton-Wasen tree, or whether they're independent, Gautam-Wasen tree, they show that when rho is less than square root of Vi for this hypothetical problem, actually, so you cannot solve it. Okay, so this naturally leads to that, the belief that all the local algorithms. The belief that all the local algorithms based on the local neighborhood actually will fail, at least in the sparse 3G. Okay, does that make sense? So that's one piece of evidence. Slowly growing. Yeah, yeah. So it's not exactly constant. Yeah. Well, that's suggested, right? Yeah, it's not proof. Yeah. Yeah, that's our intuition. Yeah, there's just some actual work to formalize this. But I think the pieces are all there. It's just you need to more formally write down arguments in this particular context. I mean, they actually do not make such a claim in the paper. Paper, they just yeah, that's something I'm talking about. Like this basically, this reduce after you're reducing this problem to this locally tree like hypostatizing problem, then they've uh rigorously showed that this hypostating problem, so you cannot solve when rho is less than square to alpha. Yeah. Another piece of evidence, I mean, it's a little bit weak here, is that you can also look at this low degree framework, as Alex mentioned in like yesterday. So basically, Yesterday. So basically, here we show that if you look at all the low-degree polynomial estimator, it actually fell, but we can only show that when rho is a little one. So ideally, we want to show these fails when rho is actually less than square root of alpha, but we're not able to like do that at this moment. So yeah, this is this is for the constant degree. This is a, I mean, this is okay. This is a, I mean, this is okay, like it doesn't depend on whether it's sparse or like dense. Yeah, this paper is all about sparse. So actually, there's a kind of discrepancy because here we look at logarithmic kind of degree regime. Yeah, they are from the sparse. So that's why I'm saying it's kind of not rigorous. I'm just curious. Anyways, um, yeah, not the kind of exactly same kind of thing here for this graph matching problem, yeah. Okay, yeah. Okay, any other questions? That the broad degree polynomial argument that you just become a pro before like and point two to before. Yeah, yeah, we believe that uh for example in this polylog and regime, like a login regime, we believe that you can push row to square. We believe that you can push real to square root of alpha. I mean, that's our belief. But currently, there's some difficulty in our current argument. But we kind of think that there's a way to get around this. Yeah, using some so-called conditioning argument here. Yeah. Yeah. Yeah, yeah. So let me mention that Jen Ding and his co-authors actually are working in this direction. They may actually. In this direction, they may actually post this result sometime soon. Okay, any other questions? Okay, let's see. Hold on. Okay. Okay. Yeah, then the rest of the talk, I just want to give you like how we get this result. In particular, I want to explain to you where this autist threshold comes up. And also, in particular, I'm going to come back to the talk, like why are we counting these channel layers? Okay. Okay, so let me actually first talk about. Okay, so let me actually first talk about some meta-algorithm, at least there's some very natural approach to solve this problem. So remember, we want to match the nodes in one graph to the other graph. So one way to do that is that for every graph, we can try to come up some vertex signature that helps us to distinguish this graph from the other graph. Okay, you can also view these signatures like kind of a feature, and then you're trying to kind of engineering the feature, trying to capture the information about this particular node. Information about this particular node. So, for example, here we have five nodes. Let's just imagine we come up some signature for every one of the vertexes. So, let's just call that S1, S2, up to S5. And imagine you can do the same thing for the second graph B, and then you get a different vertex signatures given by T1 up to T5. Okay, let's first imagine we already get these vertex signatures. Then, the natural step is that by looking at these tables, we can come up with some similarity score. We can come up with some similarity score for every pair of vertex i in the graph A and then J in the second graph. We can try to compute some similarity score, let's say phij, based on their signatures. For example, you can take some inner product between their signatures if these are the vectors, or you can do some other more sophisticated kind of procedure. And then ideally, what we want is that for the true pair, like J is equal to this pi i. equal to this pi i then we want the ideally we want the signatures to be close and then the similarity score is high okay on the other hand if they correspond to the fake pair then we want the signal signature to be far apart and then the similarity score is low okay and then if we can do this then the very natural way is that you can try to threshold the signature right to kind of output the true pair versus the fake pair or you can do this more sophisticated procedure by running a linear A procedure by running a linear assignment. Like you can look at this by J and then look at a permutation that maximizes the similarity score. Okay, so basically, in some sense, the high-level procedure is that you start with a quadratic assignment problem, you do some clever signature embedding, and then you reduce this to a linear assignment problem. That's roughly the high-level idea. Then the natural question is then how are you going to come up this vertex signature, right? Okay, so let's think about this together. If you want to come up. About this together. If you want to come up with some signature for vertex, then at least the very natural idea is that you can look at the degree, right, or the popularity of this node in the graphs. Because you could imagine a superstar in the Flickr, maybe it's also a superstar in the Twitter network. And then you can basom the degrees and then match them. But you can see that this has some issues, right? Because often you will have networks where the nodes have similar degrees, right? And also we have some. Degrees, right? And also, we have some edge perturbation. So, this signature based on degree are not that robust, okay? But then, naturally, this leads to that you can, I mean, why you stop at the first hop, right? You go to the second hop, and then you look at the so-called degrees of neighbors, and this is also called degree profile, okay? And then this actually, in our earlier work, we showed that this actually works when this correlation row converges into one at this particular rate, one over polylog n. And then you can try to kind of take a step further, right? Look at not just stop at the second hub, look at a third hub, as Julia mentioned, or you can actually look at the multiple hubs. And then this is actually the idea of this previous work. And then they actually show that in the sparse case, then this local neighborhood is a tree. And then based on this tree structure, they come up with more kind of sophisticated signatures. Okay, and then that's the reason why they can. Okay, and then that's the reason why they can tolerate like rho is a constant that is very close to one. But then the main kind of puncture line here is that these above vertex signatures are either kind of sensitive to noise or actually only work in the sparse regime. So somehow we need some new idea that to come up with a new signature that can work in both dense and sparse regimes, and hopefully can tolerate a high level of noise. Of noise. So, actually, that leads to kind of our idea. So, our idea is actually based on the so-called subgraph count. Okay, so let me actually explain to you. So, let me first talk about generic like subgraph count. So, if you give me a graph, A, and then another, and then particular template, say like a wedge here, and then I can try to count how many copies of this edge appear in the graph A. So, for example, in this case, you can do a very simple Example, in this case, you can do a very simple sanity check. You can see that there are in total like six copies of this wedge. Okay. And you can see that this kind of sub-graph count captures some graph information. And this is actually very popular, not only in theory, but also in practice. This is actually related to this concept of so-called motif counting, where motifs are just some very special subgraphs that appear like atypically in the graph. And then you can try to count these motifs actually to capture some special structure of the graph. Structure of the graph. Okay, now the question is that here we're not just interesting about the graph information, structure information, we're interested about like distinguishing like a particular node with others, right? So somehow we need to understand how to capture the vertex information of a particular node. But that's not that hard because the only thing that you can do, I mean, one thing that you can do is that you can look at the rooted subgraph count. So basically in this case, let's focus on particular node i. Let's focus on a particular node i, and then I'm going to consider a particular rooted subgraph. And now, instead of counting the copies everywhere, I'm just going to count the copies of H rooted at I in this graph. Okay, and I can denote this by Wi comma H A. So this I is just the graph that I'm going to count, and H is the template, the rooted graph, and A is just the mother graph. Okay, so for example, in this case, you can check that there are three copies of H. There are three copies of H that are rooted at I appearing in graph A. And then this number is three. Okay, so you can see that this captures some vertex information because you can imagine if you see a lot of rooted subgraph count in graph A, since they are correlated, you might be able to see a lot of such subgraph count in the second graph B. Okay, and now the idea is that we want to actually construct a rich family of a different rooted subgraph. Of a different rooted subgraph, and then try to kind of think that each rooted subgraph captures some information about this node i, and we want to actually combine this information together and then to get this vertex signature. And hopefully, by combining all these different rootie sub-grop count, then this vertex signature is kind of robust to the noise. Okay, make sense? Yeah, so now you can see how this leads where this leads to, right? Because now I can think about there's a family. Think about there's a family, calligraphic edge of rootie subgraphs, let's say fixed like capital N edges. So, this is a particular family where n is equal to four, you can list all these different rootie subgraphs that you want to count. And then for each vertex i in the a, the signature now is a vector si. And then each coordinate is just a particular rooted subgraph count. Okay, so every coordinate, the index is the rooted The rooted subgraph H in this family, and then for each coordinate, I just count the number of rooted copies. Okay, since there's one thing that actually relates to my earlier idea is that eventually we want to combine these different rooted subgraph count, right? So ideally, we want these different coordinates are uncorrelated. But now the issue is that if you look at the rooties of graph count, this different coordinates. Subgraph count, these different coordinates are highly correlated. The reason is that if you look at two different rooted subgraphs, they may overlap. And if they overlap, that means that a certain part of the graph already appears, and this will influence like the other copy of the graph to also appear in there, right? Okay, so somehow we want to decorrelate like the different indices in this vertex signature. And how to do that, there's a very natural idea. To do that, there's a very natural idea: is that I just replace this graph A by the centered adjacent matrix A bar. Okay, so what do I mean is that I take A and I just subtract the mean. In this case, it's just a Q. And then I count these so-called weighted copies. Okay, so basically what I'm doing is that I'm going to consider all the possible subgraphs that are isomorphic to H. And then here, instead of saying whether they appear in the graph or not, now I'm going to take the product. Not. Now I'm going to take the product of the edge weights. Okay. Yes. I was going to wait till the end, but maybe this is really good time to ask questions. It seems like everything you've said so far, and especially the kind of general heuristic about building a linear assignment problem out of a quadratic one would apply to matrices also, right? And now, I mean, you're really writing this as kind of a matrix function. It doesn't have to be a graph to kind of make sense of these statistics. So, I mean, is there going to be? So, I mean, is there going to be kind of some step that really breaks down or general or some like big class of matrix quadratic? Okay, so if I understand your question, you're saying that instead of matching the graphs, you can think about matching the matrices. Just correlated matrices. Yeah, for example, the Gaussian matrices. Yeah, correlated Gaussian matrices. Yeah, I think all the stuff that I'm talking about actually will also hold in the Gaussian correlated Gaussian matrices. Because you're right, all I'm doing is actually try to combine. Try to combine. So, this sub-glock count is just the polynomial of these matrix entries. So, we do not show the result in our paper for the correlated Gaussian case, but we believe that the result should also hold in that case. I guess with even the same threshold. Yeah, I mean, that's my guess at least. Make sense? Yeah, okay. And then that's the so-called weighted sub-graph count. There's a special name in the literature, it's called a Stein sub-graph count. In the literature, it is called a sine subgraph count. The reason is that instead of one, zero, now we subtracted the q, it becomes a sign. And this actually was first studied actually in the Bubeck et al. And also Raxi. So they actually use this to testing the random geometry graph versus like Audition graphs. Okay. Okay, so the nice thing is that after this very simple kind of like kind of a surgery, now these different coordinates just become uncorrelated. Different coordinates just become uncorrelated across different subgraph H. Okay, so now the ninth thing is that if I look at the vertex signature, every coordinate is uncorrelated. So the ninth thing is that it kind of captures some kind of independent information about this vertex. Okay, now I can do the same thing for the other graph. I can construct a vertex signature TJ for the graph B, right? Okay, now once I have these vertex signatures, then I'm going to construct a similarity score. In this case, A similarity score. In this case, I'm just going to take this weighted inner product. Okay, so it's like the inner product. The only thing is that we weight each, I guess, summoned by the automorphism of the edge. Okay, so the reason why we multiply the automorphism of the edge is because if you look at the completed graph, the number of copies of this edge is actually inverse proportional to the automorphism. So if you look at a star, the number of copies is actually smaller. The number of copies is actually smaller than just the pass. So, this automorphism of edge is kind of like try to compensate that effect. Okay. Okay. And now once you have this, then if you can indeed show this similarity score have this nice kind of separation, then what you need to do is that you can just threshold this similarity score at a certain threshold top. Okay. So you don't actually need to do this linear assignment. Yeah. Yeah. Yeah, like for this context, so you don't even need to do that. Like this very simple thresholding already works. Okay, any questions? Well, cost script age time. Yeah, I think that's the main focus next. Yeah. Oh, and then one comment I should say is that the similarity score, as you can see, is actually the polynomials of this matrix entry, Aij Bij. And then the degree is just the, depends on the size of this subgraph that you're going to count. The size of this subgraph that you're going to count. And this actually goes back to Alex's earlier work on this low-degree polynomial. So you can also view this subgraph count as coming up some particular low-degree polynomial estimator. Okay, now let's think about which family to count. Well, let's first think about the simple case. Well, we have decimality score. Ideally, at least we want on expectation, the phij is separated. The phi j is separated. Right. Now, if you think about that, if you take the expectation, it turns out that we need this edge to satisfy this property called uniquely rooted. Okay, so what does it mean? That just means that if you look at a particular rooted graph H, if I change the root, then this rooted subgraph will change. So it will not be the same as before. Okay, so for example, this one, if it's rooted at the center, then if I change the root to Then, if I change the root to the other vertex, then this will be a different rooted subgraph. So, that's indeed uniquely rooted. Well, this triangle is certainly not uniquely rooted. Okay, so now the nice thing is that if we assume the H is uniquely rooted, then we see that this subgoph count in A and the subgraph count in B are actually correlated if and only if that i and j correspond to the true pair. That I and J correspond to the true pair. Okay. So, why is this the case? Let me actually pause a little bit to give a little bit of intuition. So, let's say I and J are the fake pair. Okay. And now in this case, these are different. Hold on a second. Let me see. Yes. So if I and J correspond to different, like Correspond to different, like it's not, sorry, so it's a fake pair. Then, if we look at the rooted tree, sorry, so rooted H and I and then rooted H and J, since they have different roots, so the graph are different. So that means when you can see the overlap, then there must be an edge that are not appear in both of the graph. And when you take the expectation, then this after centering, this will contribute to being zero. Does it make sense? So that's why it's uncorrelated if it's for the It's uncorrelated if it's for the fake pair. For the true pair, since i and the j are the true pair, then they will completely overlap. So then that's why when you're computing the mean of this phij, it will be not zero. So that's why you do this calculation, you will see that the mean of phij will be equal to zero for the fake pair and will be positive for the true pair. So what does this lead to? This basically leads to this nice kind of mean separation of Nice kind of mean separation of this phij. But then, as we all know, that that's not the end of the story, right? We also want to capture, control the fluctuation of the similarity score to be relatively small compared to this mean separation. So, in some sense, we ideally want the similarity score of Ij to be kind of highly concentrated on the mean. And if this is the case, then we can simply threshold at a particular, say, the midpoint. Okay, so now the question is that how can we control the fluctuation? So that turns out to be actually the most challenging part. And this actually leads to the question is that which family of H should we choose? Okay, so let's think about this. Let's look at the similarity score. Let's actually do a sort experiment. So let's actually first ignore the potential correlation across. Ignore the potential correlation across different summons. So I'm going to ignore the cross-correlations between like this R to H or H and then a different summant like R to I. Okay. I mean, this is kind of an idea thinking. If this is the case, then you basically get some like a summation of uncorrelated random variables, right? Then that's not that hard to compute the variance divided by the mean square, like the kind of capture the separation. And this, if you do a little bit of calculation, you will see that it's actually given by one. You will see that it's actually given by one divided by the coloniality of this family and times rho to the power of 2n. Okay, where rho is the correlation and then capital N is the size of the graph edge that we're going to count. Okay, and then ideally you want this ratio to go to zero, right? So now if you think about that, if you want this to go to zero, you have to satisfy a few things. First, this family has to be rich in the sense that the coronality has to grow exponentially in n. Has to grow exponentially in n because we want to actually capture, tolerate a constant row here, right? So, for example, that immediately concludes that you cannot just count the cycle because if you for a fixed number of edges, there's only one like a cycle to count. So this doesn't satisfy this requirement. And also to drive this to go to zero, we also need the capital N goes like growth in N. So in particular, we need the subgraph H to be relatively large. Relatively large. And then the last point is that remember, we want to get an efficient algorithm, so we want this edge to be simple to count. So if you look at these three different requirements, and if you think a little bit, then this actually naturally leads to the subfamily of trees. Because if you focus on the trees, the nice thing is that by this alter's result, we know that the coronavirus, the number of unlabeled trees. The number of unlabeled trees actually grows exponentially in n. The rate is given by one over alpha. And now, if you plug in this in, you can immediately see that when rho square is bigger than alpha, then this ratio will dry down to zero. So that's where this alternant naturally appears in this calculation. And also a nice thing is that this tree has this very nice recursive structure. It's actually very simple to count using this strategy called a color coding. So basically, just a way of writing the Basically, just a way of writing the counting as a dynamic programming using a recursive tree structure. So, in some sense, the tree is actually very simple to count. But the main issue here is that we cannot ignore the cross-correlation. That's just a kind of two idea to assume. So, then the next thing that we need to deal with is how can we kind of control the cross-correlations across different edges in the family. In the family. Oh, yes. There are some trees that are not uniquely rooted, but that one is not that a major constraint. Yeah. Yeah, so we just need to make sure that the family grows as one of alpha to the n. I mean, I actually put a little one here because I can tolerate a little bit of the kind of slack here. Kind of a slack here. So if you impose the unique rooted constraint, this probably won't actually reduce the coronality a lot. In fact, you're going to see that we actually impose even more kind of a special structure. Okay, so that actually leads to this channelia. So basically, remember, we want to suppress this undesirable cross-correlation across different edge to control the variance divided by mean squared. And also, remember, we still want the family to be rich, right? Because we still want to retain this. Right, because we still want to retain this condition rho square bigger than i phi, so then that's where this channel comes up. So basically, we're going to focus on this very special structured family of trees. So it starts with a root, it has a bunch of branches, and then why we call this chanelier is because this actually resembles this chanelier. So, in fact, okay, when we're working on this project, I was trying to get On this project, I was trying to get a new channel for my house, and then this is actually the channel here in my house. So that's the reason I kind of coined this term channel. You can see that it's pretty similar, right? And then because we are actually electrical engineer, at least we are trained. So then that's why we call this path. It's like a wire. It has like a capital M edges. And then this part we call the bulb. Okay. It's just a. Okay, it's just a rooted rooted subtree with k edges, and then here there's a special constraint that we require. We actually also require this rooted subtree to be like not that symmetric. Okay, so we actually constrain the automorphism to be exponential in k. I mean, if you look at that, you will have very symmetric tree, like up to k factorial, right? The reason why we constrain this number of automorphism is because if you look at this expression, Because if you look at this expression, we actually put the weight automotive h. When you compute the variance, you will naturally have this term popping up. And this actually somehow makes the variance to be larger. So that's why we want to constrain this automobile H to be kind of exponential in K. Okay, that makes sense? Okay, and then remember in the total, we actually have L branches. So then you can easily compute the total number of edges in this. Here, the total number of edges in this chandelier is just given by m plus k and times the number of branches l. Okay, now remember we want this family to be still rich, right? So let's look at this family, okay? So it turns out that as long as we're picking k, like the number of edges in the subtree to be much bigger than the number of edges in the past, we can actually show that this family is kind of still rich, like as rich as the original tree. As rich as the original tree in the sense that it still grows exponential, and the rate is given by one over alpha. Okay, why we can ensure this? Because of two things. The first thing is that if you look at this particular structure, where we lose kind of the entropy, okay, in one case is that we constrain this to be a pass. Well, for the genetic tree, this can be more complicated shape, right? But then the good thing is that we're choosing K to be much bigger than N. That we're choosing k to be much bigger than m. So most of the edges are in the blue, like in the bow. There are only a very small fraction of the edges on the path. So that's why you can imagine this won't actually lose too much entropy when you compute the cardinality of the tree. Another very cool result that we found is that, remember, we constrain the automorphism for each bow, but it turns out that there's a very nice result showing that the majority of the trees actually have only automorphism like up to exponential. Morphism like up to exponential in the size of a tree. So even though it can go to k factorial, but if you look at, say, half of them, they're actually only exponential to the k. Okay, so that's why here, even we put this automorphism constraint on each valve, again, this doesn't lose too much entropy. So that's why we can still retain this family to be rich. Sorry, alpha is just this autus concept. L is the branches. L is the branches. Oh, yeah, I'm actually going to talk about that. Yeah, so I haven't told you exactly the choice of K and M and L, but I'm going to mention that. So basically now restricting to this very special family, then we can actually indeed show that for the fake pair, then we have this idea of kind of like a situation. The variance divided by the mean square is given by one divided by the coronality of this family and times rho to the power of 2n. And times rho to the power of 2n. Okay, and then remember we needed this, this family grows as 1 over alpha to the power of n. So there's some missing minus n here. But then you plug in, when rho squared is bigger than alpha, then this will go to zero. And if you're choosing the total number of edges to be some small constant times log of little n, then this can be a little of one over n squared. And this is actually the reason, this is where we're going to use a union bound to bound all the fit pairs. So then you can see that for the fake pair, we can ensure that it's actually highly concentrated on the mean. For the true pair, it turns out that this ratio has a slightly different expression. It's given by L squared divided by NQ, where L is the number of branches. And now to make sure this is a little, we need to make sure L squared is a little over NQ. So back to the question, basically, this depends on the sparsity. So if the graph gets sparser, then you need to have a few branches. And in fact, you Branches, and in fact, you should have a long wire. Okay, okay. So, then in this case, you can show that for the true pair, it's again concentrated. Then, you can pick the midpoint as a threshold, and then you can achieve almost exact recovery with no mismatching error, because for the fake pair, we control the ratio, it's actually little one over n squared, so we can take a union bound. Okay, and then one thing that we can say is that. One thing that we can say is that after you get this almost exact recovery, it's actually not that hard to boost it to exact recovery because you already match almost all the nodes, right? You can just use them as a seed and then actually using this CD graph matching and then to get to the exact recovering as long as the graph is dense enough, like the intersection graph is connected. Here, when you're in a denser gene, right? I mean, you can think of the hand neighborhoods as digital. Of the neighborhood as you just joined their um yes, definitely for constant, I mean, for yeah, for so like right when you compute this, there's some extra complication, right? And yeah, yeah, yeah, might actually kind of overlap. Sure, yeah, remember, we actually do this so-called sine sub-graph count. So, we at the first step, we already turned this graph A to be a centered GC matrix. So, it's like already completely graph. So, it's like already a complete graph where the weights are either one minus q or minus q. So, then it's already complete in the sense. So, even for the sparse graph, we turn this into a complete weighted graph. And then back to the question, definitely, then you will have lots of overlaps. Like if you look at like a particular node and then look at an N-hop neighborhood, it's definitely overlap a lot with the others. But still, yeah, but still, yeah, we can show that. So, yeah, we can show that this is actually the variant, yeah, like to get this result. But let me actually maybe just mention a little bit, like why we can control the variance. Okay, so if you do the variance calculation, you can take the similarity score, take a square, so you can expand out this as a double summation, right? And then this will involve the covariance between this subgraph, this product for H and the product for I. H and the product for i, right? Okay, and now the thing is that this subgraph count for a bar, like for one of them, this can be further written as a summation over all the copies and that are isomorphic to H, right? So basically, now this will basically involves like four chandeliers, like two of them coming from H and then two of them coming from I. So we have this kind of so-called union graph. So-called a union graph consisting of four chandeliers. Okay. And now for each term, this covariance is not that hard to bound it. You can just write down exactly the correlation and then the overlap. You can bound this in terms of the Q, the edge density, to this little E. So that's just counting the number of edges in the union graph. Okay. But now you can think about, you can fix on a particular kind of template for the union graph. Of a template for the union graph, and then you can basically need to bound this bunch of summations. So, that just you just count like how many configurations of these channel layers that can induce a particular like a union graph. Okay. And then for that, that's the kind of the main challenging part. And it turns out that to do that, we need to realize that for this particular channel layer, it turns out that when the union graph contains a cycle, So that just means that if you fix the number of edges, then the number of vertices drive down because you contain a cycle. Then, this turns out that you can show that it contributes less to the variance. So, the main bottleneck case is that when the union graph is still a tree. And this is actually the dominant term in the variance. Yeah, let me maybe just say a little bit looking at this toy example, kind of give a little bit. At this toy example, kind of give a little bit more intuition. So, here I just look at the two chandeliers instead of four chandeliers, and now I have H and I. I can try to consider its overlap pattern. In the first case, you can see that in this case, you have this branch, like first, sorry, like this branch, like first branch out, and then these two dangling bobs, they actually overlap. In this case, you will contain a cycle. So, it turns out that this will reduce the number of vertices and reduce. Will reduce the number of vertices and reduce the counting like a factor. And then in this case, you can just do a very kind of coarse counting based on the number of edges and the number of vertices, and this actually suffices. The main challenging part is like this case, it'll branch out, and then these two bulbs are not overlapping, and then it's still a tree. In this case, then you need to do a very careful kind of counting scheme, but the good thing is that you know this. But the good thing is that you know this very special structure. So you can basically sit down and then try to draw out all this and do a very tight kind of bounding. Okay, so that's roughly the intuition. But once again, we only look at the two channel layer. Like in this particular problem, so we need to consider four. So the situation is more complicated than this. But this is like the high-level intuition behind this construction. Okay. Yeah, let me just give. Okay, yeah, let me just give a quick summary. So, basically, we want the chandelier to be large, also to be rich, and also to be robust in the sense that you can see that when the graph gets sparser, we want the branches to be fewer, but then we want the like the path, the length of the path to be longer. So, I think that intuitive makes sense because, in a sparse case, you only have a few neighbors. You want to reach to a larger neighborhood. So, that's at least intuitive explaining why you need a long wire. Intuitive explaining why you need a long wire and a fewer branches to reach more neighbors to kind of extract information. Okay, I guess that's all I want to say. This is actually the result that we have. We basically developed this polynomial algorithm that works when rho is above square root of alpha in both sparse and dense regimes. But there are actually a few very interesting open questions. For example, it will be interesting to prove the computation hardness when rho is less than squared of alpha. Squared of alpha, at least for the sparse case, when the average degree is polylog n. And we believe this should be true. In the dense case, we do not believe this rho less than square to alpha is actually the computational threshold. So we kind of believe that you can actually achieve polynomial time recovery even when rho is a little over one. But then we don't know actually a particular algorithm to do that. And then last but not least, we look at these other Shang Yi graphs just as a starting point. Definitely they're a much richer family you can consider. Much richer family you can consider. Okay, yeah, I think that's all. Yeah, thanks. Is it feasible that YAJ statistics is actually okay? You count you count all trees, but just you're not now either. Okay, that's a good point. I think strictly speaking, it is possible. We do not follow. It is possible. We do not formally rule out if you count all trees, this will fail. But I guess we have a strong belief that this is probably not true. Just would be nice that's the only way first correlation. Yeah, that's Yeah yeah, that's yeah, I I guess we're kind of getting stuck in this for quite a long time. So we're trying very hard to bound for the generic trees. But then in the end, the Soviet came up with this, I think, this nice and maybe easier route. You can just restrict to this particular family, and this greatly simplifies these possible overlapping patterns, and that's how we can bound the variance. Can download the variance. By the way, the final matching is by just maximizing the five. Yeah, or you can just threshold. Or you can threshold. Yeah, or yeah, because you already have this very nice separation. The mean is already separated, well separated, and it's very concentrated. So you can just threshold that. Yeah. Yes. So return to the thing I was thinking about before. I mean, this seems like almost, it makes sense even without the kind of statistical setting. The kind of statistical setting. Like, if I just ask for, I don't know, for an approximation algorithm for the optimization problem that is the quadratic assignment problem, right? Everything mostly makes sense. I mean, you don't have this, like these kind of concentration results or something like that. So, I mean, do you think there could be some like big class of quadratic assignment instances, maybe with some like smoothing or something like that, where this stuff would still be? Still be a good idea, or like some you know, version of this name with it here. It's a very special quadratic assignment, yeah. I guess I'm right, right. So, yeah, okay, yeah, maybe I can answer one part, at least I feel like we crucially explore it, is that we do the centering and that kind of assume that the graph density is at Q. But for deterministic graph, yeah, you need to do something else, I guess. You need to do something else, I guess. I mean, that's one point that would be the same. Is that what you're saying? Like, density of A and density of Q should be the same? Not exactly. Yeah, yeah, that you can send it differently. But I guess what I want to say is that if you do not consider a random graph, but just a two-family of graphs, sorry, two graphs, and then you want to solve a quadratic assignment problem. So if you just literally give me the two graphs, okay, I guess I can try to compute the empirical edge density in the. Compute the empirical edge density in the true graph and then just subtract that. But then this still, yeah, breaks this a lot of this kind of variance, like this nice, like uncorrelated stuff like already breaks down. But isn't the beaver problem that you assume that there's planted permutation? So this is like a much bigger assumption, right? For quadratic, like quadratic cost, right? I mean, originally this facility and distance. Sure. If you're interested purely in terms of the average test of QAP, there is a recent very nice result of Jen Ling and others that showed if A and B at only 10 G and Q and evaluate QAB in polyomial time up to one plus zero of one quantitatively was a very difficult result. But that is not doing anything similar. No, there's just no volume non-planted bottles. Yeah, so that's a nice thing. Sorry, so what yeah, what did you take to independently and yes and then what the state you can there is a point algorithm to solve QAP up to one plus little one yeah so basically they're trying to develop a point algorithm to approximate the objective function of the QAP when A and B are purely independent. And then you will think about so you will originally think these problems should be hard, right? Because this QAP is hard, but they actually surprisingly they found a polynomial town where they can really Found a polynomial tale where then they can really like approximate this objective function up to one plus epsilon. But this probably just means that the objective function at this very low is very high, right? Like it's so hard to align these two guys. No, no, there's any so yeah, I think it's it's deeper than that, I would say. Yeah, um what I mean it's like why would you why would you expect like what's like why do you try to align to random so just some because you don't like London. Just some because you don't like plant it. Oh, by the way, I so I should say that that problem can be also viewed as finding the largest common subgraph in this A and B. Right. So, I mean, it's not that big. I think the objective function, I mean, in certain regimes like on order of n. So, in other words, you can possibly like the largest common subgraph is like in terms of the number of edges is on order of n. So, it's kind of sparse. Yeah, but I think. Yeah, but I think it's not, yeah, it's kind of not that easy to do that, even, yeah, to us. Yeah, yes, the depth of your trees is that like less than or the same as in the Massulia result, but sparse case and they're sparking, they have these neighborhoods. Yeah, yeah, yeah. Are you sure his root can be on that radius or? Yeah, yeah. I think in our case, the Yeah, yeah. I think in our case the radius is like a roughly log n because the size of the tree is like log n. I believe they also look at log. It's up to log n. It's log n. I mean, yeah. Yeah. Yeah, maybe some small constant times log n. So then it's still locally tree-like. Yeah, in the sparse region. Yeah. Does L equal to one work? L equal to one work. L equal to one work. In the guarantee, we need L to be a large constant. But L should be smaller than the square root of NQ. So L is some big constant. Yeah. So it's not one. Yeah. Yeah. One probably does not work. Yeah, then you have only single branch. Yeah, that's probably doesn't work. Yeah. Degree polynomial like some sort of choice of the value of H or optimal low degree polynomial. I guess it depends on what you mean by optimal low degree polynomial, like in what sense. But generally speaking, like if you just look at the low degree polynomial, then it sounds, then it will reduce to count all the polynomials. Then you will reduce to count all the possible subgraphs of certain size. So you don't have the restriction of the particular structure, let's say even trees. So you need to consider all the possible subgraphs. And then that's a reason why in the, I said earlier, we can only show this low-degree polynomial fails when row is little or one, because there you have a much family of subgraphs to consider. And then that will potentially drive up the signal toys ratio. But then from positive results, But then, from possible result direction, if you want to show this works, then you need to consider care about this correlation. And then, for generic subgraphs, it just becomes very hard to bound the correlation. So, that's why right now there's a kind of a gap between the positive result and the negative result in this low-degree polynomial framework. Yeah. So, I think one, yeah, yeah. Time you've got to calculate all those theta ijs. Yeah. What is the polynomial? You mean the power? Yeah, that's actually bad, so I don't show that. It's actually depends on how close rho to the square root of alpha. So let's say rho is the square root of alpha plus epsilon, like epsilon away from this auto threshold. Then the running power, sorry, running time is like n to the power of one over epsilon. Okay, so you know this. Okay, so in some sense, if you're getting closer and closer to the threshold, the running time, I mean, the exponent in this polynomial actually diverges. Yeah, so this is actually, in terms of practical considerations, is actually the kind of the bottleneck for us to implement this algorithm in practice. So, what if the row is 0.9, say? 0.9, then that's fine. Then, I think if you optimize that, I guess you will get n to the power of eight or something. The path eight or something, but we're not very careful to optimize that exponent. Yeah, there's some power, yeah. But my perspective on this is from these algorithms are theoretically for the other parts, you know, more practical. I think I think we are, you know, too far, so we can try. Yeah, Masulia one, I guess I mean they do implement that. Actually, there's some physicists, I think Lenka and then her group actually indeed implement this kind of this local neighborhood like message passing algorithm. I think that actually runs relatively fast. That actually runs relatively fast. But again, the running time is still n to the power of a big constant. Yeah, the reason is that in this message parsing algorithm, if you want to compute like updated message, you want to consider all the possible mapping between the neighbor of i and the neighbor of j. So that one grows as factorial. So let's say the node, the degree is log n over log log n. Degrees log n over log log n, you will encounter with log n over log log n factorial. So it's still a very big constant. So I think that's why they only can implement this into a relatively small like graphs. But they have some heuristic though to kind of get around a little bit. But in terms of complexity, it's still n to the power of big constant, even in their setting. Yeah. I guess Johan mentioned that we implement the algorithm, but I guess that's for the testing problem. I guess that's for the testing problem. Like instead of recovery, you can consider testing dependent versus correlated. There, the nice thing is that you don't need to consider log n size. You can only consider, so you only need to consider log n over log log n. There, the theoretical capacity is n to the power 2. Yeah, so at least the theoretically it's pretty efficient. And then we indeed implement that up to, I don't know, 10,000 nodes. I think that's doable. I don't know. I was just gonna ask if the testing problem has the same threshold. Yeah, testing problems still have a row, it's like bigger than square root of alpha. But yeah, but then there, the big difference is that there you can just count all the trees, not rooted and then not chandelier. Here, you need to consider rooted and channelia because it's a special family. Yeah, so I mean, at a kind of high level, the strategy of kind of figuring out these features, calculating the Figuring out these features, calculating this matrix, and doing linear assignments or whatever seems very practical. And, like, maybe it's just that, you know, these features that are convenient to analyze theoretically are not the ones that you would actually use if you were trying to use that strategy in reality. You know, maybe there's like a lot of kind of practical optimization you can do on specifically what features you need. Yeah, I think so. But I mean, there are signatures constructed spectrally. Constructed spectrally. Yeah. Everything runs in PCA time. So you can view those other algorithms as doing this, but with like some very simple spectral effort or something. Yeah, yeah, yeah. Yeah, you can do that. And also, I want to say that you can also do this so-called graph neural network. Like instead of come up engineer by yourself, you can try to train. Right, right, right. Yeah. So do you know what people do in practice? I mean, is this the kind of problem we should fit? I mean, is this the kind of problem we should face? I think it's a mix of several things. Like, in practice, in lots of these applications, you do have this actual vertex attribute information. So, that goes back to Lola's talk. So, that will help a lot in certain applications. And then also, yeah, they kind of use those are very, very rare and overall. Yeah, that's doable. Yeah, it's and I mean, that you can then you can develop some first-order method to try to like approximately solve the quadratic programming. So that's still doable. Yeah. Okay. Yeah. Thanks.