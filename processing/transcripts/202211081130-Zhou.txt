Going to talk about our recent work on the likelihood of based on randomization method. So, this I want to acknowledge upfront that this is work done by my previous postdoc, Zhong Sangyuan. He is the one who really did all the method development and analysis. So, mandarin randomization, as we all know, that is a powerful statistical tool to perform causal inference in genome-wide. Perform causal inference in genome-wide association studies. It tries to investigate the causal effect of an exposure complex trait on an outcome complex trait. And it is a special form of instrumental variable analysis. That's why they use it in many other domains. And in this particular case, we treat SNPs as instruments to perform the causal inference. So, this is the simple setup. So, most of Mandarin randomization has been focused on a two-second. Have been focused on a two-sample MR setting, and where we have two different genome-wide association studies. So, we have exposure GWAS where we measure exposure complex trait, and we have outcome GWAS that we measure outcome trait. And our goal here is to understand whether there's a causal relationship between the exposure trait and the outcome trait. One simple example could be, for example, the exposure is smoking and outcome is a BMI. So, whether smoking BMI, so whether smoking has a causal effect on BMI. So, because those are measured on two separate GWAS samples, in order to link this exposure with outcome, we usually need to rely on a common set of SNPs. And we treat those common set of SNPs as the instruments. And we select among them the valid instruments to perform the Mandan randomization analysis. So, with the SNP instrumental variables, we assume that they have effects on the exposure. They have effects on the exposure. This usually refers to as the instrument effects or beta. And with that, we can infer eventually the causal effect of exposure on the outcome, which is denoted as alpha over here. So that's a basic setup of mandatory randomization. And there have been a lot of mandatory randomization methods recently developed because there's a wider, because the GWAS summary statistics have become widely available. And almost all those And almost all those Mandan randomization methods require a selection step, that they need to select a set of SNPs to serve as valid instruments. And the standard approach is to select independent SNPs. And the most common approach to select independent SNPs is through the so-called LD clumping. So the LD clumping is a relatively ad hoc procedure to select independent SNPs based on their marginal GWAS association signal, and it can be stopped. Signal and it can be sub-optimal because it may only identify the taking snaps rather than the causal ones. And in fact, in the relevant field of GWAS-5 mapping, studies have been demonstrated over and over again that developing using this probabilistic statistical model to identify causal SNP is much more effectively than LD clumping. And secondly, those existing randomization methods focus on using independent SNPs. Using independent SNPs, and they use independent SNPs because the common approach of IVW is an inverse-weighted, inverse-variance-weighted randomization approach. They use typical the formula requires independent SNPs to summarize their causal effect evidence across those SNPs. However, using independent SNPs may not be ideal either, because in any local region, it's possible that multiple SNPs. Multiple SNPs that are in LD, they may all influence the phenotypic variance. So, using independent SNPs alone may only capture a small fraction of the phenotypic variance in the exposure variable, so may not be effective. So, ideally, we want to develop a probabilistic modeling approach to select those SNP instruments as SNP as instrumental variables. And ideally, we want to model corrected SNP. We want to model correlated SNPs. And modern correlated SNPs have been demonstrated to be beneficial in many related fields, such as transgruntomide association studies and the TWAS field, which can be viewed as a form of mandate randomization. So therefore, we're interested in incorporating correlated SNPs and perform a probabilistic SNP selection to identify the useful instruments here. And doing that is And doing that is complicated by some further facts that the SNPs may not only influence outcomes through the exposure, but it may exhibit horizontal parotropic effects. And there are two types of horizontal pleotropic effects. One type is directly affecting outcome without going through the exposure. And this is so-called uncorrelated horizontal parotropy. An alternative approach is that SNPs can affect certain Can affect certain unmeasured confounding factors. And those confounding factors will have effects on both the exposure and outcome. And this path will create horizontal protropic effects on both the exposure and outcome, and these effects will be correlated with each other. So therefore, the SNPs may affect the, may exhibit uncorrelated horizontal prairie tropic directed to the outcome or exhibit correlated horizontal prairie tropic. Exhibit correlated horizontal prototropy through certain confounding factors that influence both exposure outcome. And it's very important to control for and model both types of horizontal prototropy. And unfortunately, most of the existing MRI methods have been mostly ignored horizontal prototropy. Some try to control for uncorrelated horizontal prototropy. And only two methods try to model both. And one is MR mix, the other is cause. Both relies on a mixture of normal distribution. Relies on a mixture of normal distributions to control for both uncorrelated and correlated horizontal approach of p-facts. But because of the model complexity, both methods has to rely on a non-likelihood best approach to compute p-values. I know, as we will show later, and those non-likelihood-based approach, as one would expect, may not work very well in many situations. So, therefore, motivated by those limitations of previous MR approach, we decide to Approach, we decide to develop a new method called MRAID or Mandan Randomization with Automated Instrument Determination. So, our goal here is twofold. We want to perform automated instrument selection or determination to identify a set of SNPs among a potential large set of correlated candidate SNPs. And two, we want to model both uncorrelated and correlated horizontal prototropy. And so, here's our modeling frame. And so here's our modeling framework. So the modeling framework will focus on the two-sample MR. So we have an exposure GWAS on top in equation one, and outcome GWAS, and describing both equations two and equation three. So in the exposure GWAS, our exposure trait is X, and we have a SNP genotypes measured. It's coded in M by P matrix of Z sub X, and the beta represents the instrumental effects. And here, instead of And here, instead of supplying independent SNPs to the ZX, we'll supply correlated SNPs. So basically, we will simply use a regular GWAS threshold cutoff and supply the thousands of SNPs that are potentially incorrect with each other into our matrix of the X. And the beta is the instrumental effects, and we try to perform selection on the beta. So that's our exposure GWAS. On the other hand, we also have our outcome GWAS. Have our outercom GWAS. In the outer come GWAS, we assume that if we have the exposure variable measured, well, we don't have the exposure measured, so we denote X tilde as a missing exposure variable. And we assume that a missing exposure variable is linked to the genotypes in the outcome GWAS, which is the Z sub Y, through the same equation as equation 1. And then we link the unmeasured exposure X tilde to the measured outcome through the equation 3. Outcome through the equation 3, where alpha is a causal effect. And besides modeling the causal effect, we have two additional terms: Zy times A to Z L and Z Y times A1. Both of these two terms are trying to model the horizontal palotropic effects exhibited by the SNPs. In particular, the Adazo is trying to model the uncorrelated one, and Ada 1 is trying to model the correlated one. So, this is a general modeling framework. So, based on this modeling framework, when you So based on this modeling framework, we need to specify additional modeling assumptions on three sets of variables, specifically on the instrument effects beta in order to enable selection and also eta zero and eta one. Those are the to modeling the horizontal plot effects. So for beta, in order to enable automatic instrument and determination or selection, we specify a sparsity inducing prime on the instrumental effects on the experiment. On the instrumental effects on the exposure. Specifically for the J-SNP, we assume it follows the SPAC and SREPRI. So with probability pi beta, it's non-zero. So it's selected into the model, and its effect size follows this normal distribution with mean zero and sigma beta squared. On the other hand, with one minus pi beta, it's exactly zero. So therefore, the SNP is not selected as the instrument in the model. And then for the eta zero and eta. And then for the eta0 and eta1, those horizontal protropic effects, we'll try to model them depending on whether we selected instruments into the model or not. Specifically, for the selected SNP instruments, they have non-zero effects on the exposure, and we assume that they may exhibit two types of horizontal preotropic effects. So it's either correlated or uncorrelated. So specifically, with the probability of pi C, it has a C, it has a chance of with the probability of pi C, it will escape the correlated effect size. And when it's because it's correlated, the effect size is simply low times the beta j, where the low represents the correlation coefficient between the horizontal prototropic effects and the SNP effects on the exposure. On the other hand, it may also exhibit uncorrelated horizontal prototropic effects. So with Pi 1, it will have a non-zero. Will follow, it will have a non-zero uncorrelated effect size, follows variance of follows the normal distribution with mean zero and variance sigma eta square. So you can see that for every selected SNP instruments, it may exhibit zero horizontal project effects, it may exhibit either correlated or uncorrelated, or it may exhibit both correlated and uncorrelated. Correlated and uncorrelated horizontal peltropic effects. So it's relatively flexible modeling assumption here. On the other hand, for the SNPs that are not selected to serve as instruments, so those are the beta GA does not equal to zero, we still model their horizontal platform effects. But in this case, we can only, they only, at the most, they will only exhibit uncorrelated horizontal plus effects. So therefore, we assume that with probability pi zero, it will exhibit uncorrelated. Pi zero, it will exhibit uncorrelated horizontal plural effects. And with one minus pi zero, it will have the exact zero effects. And the reason that we also model this horizontal plateau effects for those unselected SNPs is because by controlling for that, we can actually remove more variance, more account for more variance in equation three in the outcome model. So therefore, you can have more power, hopefully, to identify the causal effects. So those are the benefits. So, those are the basically the key model assumptions that enable us to perform instrumental selection and horizontal pelotropy modeling. And so, as a schematic, you can see that we incorporate a bunch of P different SNPs as our candidate SNPs. Those many SNPs will be in ILD with each other, and we will separate them into two sets: beta does not equal to zero and beta equal to zero. So, the SNP set one are the select instruments. Set one are the select instruments. SNP set two are unselected, are the SNPs not selected to serve as instruments. So for this SNP instruments, we'll assume them to display either correlated horizontal praotropy or correlated horizontal preotropy, and all of them will have effects on the exposure and towards outcome. While for the SNP set two, they will only exhibit uncorrelated horizontal preotropy to explain additional variance in. Be to explain additional variance in the outcome model. And so, this is our main model assumption. To fit the model, we develop an approximate inference algorithm under the maximum likelihood framework using that can be using either individual level GWAS data or summary statistics from GWAS. The algorithm is based on observation that the likelihood function of alpha is a ratio between the posterior and the prime. So, therefore, because So, therefore, because posterior is asymptotically normally distributed, so we can draw posterior samples in order to summarize our posterior distribution. And if we further assume that the pri is normally distributed, then we can basically subtract these two normal distributions to get our normal distribution for our likelihood function. So, this way we can get our alpha hat and the standard error of alpha hat. And with that, we can perform a water test to test. We can perform a water test to test our now hypothesis that alpha equals zero. So that's an approximate inference algorithm we use. So to examine whether it works or not, we perform the simulation. So we randomly select the 60,000 individuals from UK Biobank. We split them into two sets, equal size two sets, exposure GWAS and the outcome GWAS, each with 30,000 individuals. And we focused on all the SNPs from Um, this all the SNPs from chromosome are variable over there with about 650,000 SNPs. And then we simulate exposure and outcome under various null scenarios where alpha is zero and various alternative scenarios where alpha does not equal to zero, both under the model assumption and also with models that deviate from model assumption. And we consider many different settings, including absence of horizontal plow choppy, presence of either. Horizontal plot tropy, presence of either uncorrelated or correlated horizontal plot tropy, or presence of both. And we compare a list of MR methods, including various different versions of IVW, like the learning effects version, the weight and mode version, weight in the media version, robust version, four versions of IVW. And we also compare with REPS, the MR adjusted profile score like and the MR mix and cost. So among those methods, only MR mix and cost. Those methods, only MRMIX and COS models both uncorrelated horizontal approach will be, but as I mentioned earlier, both use a non-likelihood-based approach. So we first examine the type 1 error control under the null. So on the left figure, we show the QQ plot for the homogeneous causal effects, whereas the beta effect size basically was similar for the SNP instruments, the effect size was similar from a normal. The effect size was similar from a normal distribution. And we also examined heterogeneous causal effect site, the instrumental effect setting where we simulated from a mixture of two normal distributions so that some SNPs have larger instrumental effects and some SNPs have small instrumental effects. And in those cases, you can see that our method is a purple color. It adheres to the diagonal line pretty well. And so is also a couple of different methods, including the weighted. Of different methods, including the weighted median method and also IVWR. But on the other hand, you can see that the coarse method, which is the blue line, gives you, it's way below the diagonal line. And this means that it's very conservative and which is actually consistent with the simulation results provided in the course paper. And for the MI mix, you can see that under the homogeneous instrumental effects, it works pretty well. Instrumental effects, it works pretty well, but on the heterogeneous instrumental effects, it shows a high inflation. And then we also examine the presence of uncorrelated horizontal prototropy or presence of both uncorrelated and correlated, uncorrelated and correlated prototropy. And you can see the purple one sits on the diagonal pretty well, while the other methods either have some inflation or some deflation test statistics under the null. Under the noun. So then we move on to examine the power under various alternative scenarios. Again, we checked those four different scenarios. You can see the purple line works pretty well in these two scenarios where there's absence of horizontal prototropy. And its main advantage becomes more obvious when you have uncorrelated or correlated horizontal and prootropy effects, which intuitively makes sense because our method tries to model on both types of horizontal prolotropic effects. Type of horizontal cloud traffic effects. And we also examine the defect size estimates from our method MR8. And you can see that whether it's under the now in figure A, panel A, or B, C, and D, which are under alternative, the effects are size estimates all look pretty reasonable by sitting along the tubes. So then we examine the performed real data, examine a bunch of real data sets. So the first Real data sets. So, the first we look at one real data application. So, in that real data application, because usually in real data application, we don't know the ground truth. So, it's very hard to compare different methods. So, but we designed this real data application so that we actually know the ground truth. So, specifically, we analyze the trait on itself for the causal analysis. So, in this case, we first obtained 330 individuals of European ancestry. 30 individuals of European ancestry from the UK World Bank. We randomly split into two equal side sets with 170,000 individuals in the exposure GIWAS and the same number of individuals in the outcome GIWAS. And then we focused on eight cardiovascular disease-related traits and then examined the causal effect of each trait in the exposure GWAS on the same trait in the outer cum GWAS. So in this particular So, in this particular case, we would expect the true causal effect size is exactly one. So, we can examine different methods to see if they can estimate, the estimate is close to one and whether the confidence interval will cover one. So, these are the results. So, we look at eight different traits, including the SBP, BMI, DBP, PR, which is a pulse rate, I think, and TC total cholesterol, LDL. Tc total chaserl DL low density lipo glyceride and Tg is triglyceride and HDL is high density. So among all this and you can see that over the spectrum our method and the blue color one it has a provided estimate that's close to the horizontal line which is the true effect size of one and also the confidence interval it provides also covers the true line while all the um While all the other methods, they all failed to cover some traits. So, for example, for the second one, the IBWR, you can see that does not cover the first two traits and also the last four traits. And for MR Mix, for example, it does not cover BMI and LDL. And also for the for For the other methods, you basically see a very similar pattern. So, oh, sorry, I explained this side wrong. So, the MR red is a purple one. So, it's in the third one. It's actually not the first one. I was misreading that. So, the purple one, you can see that it covers in every case, and it has a relatively small confidence interval. Well, all the other methods, I think they the remaining. Think the remaining six or seven methods does not cover the truth in certain traits, while the cause is the only other one that also always cover the causal effect of one, although it gives you a very large confidence interval, which is consistent with simulations where we can see that the results from causes always are overly conservative. So then we move on to our second real data application. Data application, we perform a causal effect screening. So, specifically for most previous MR analysis, most people would usually try to pick up one exposure trait, one outcome trait, and perform a comprehensive analysis, try to see whether the effect, the exposure trait has a causal effect on outcome trait. But because our method provides you calibrated p-values, this allows you to perform relatively large-scale screening to identify. Larger scale screening to identify potential causal risk factors underlying the outcome trait. So, to do that, we focus on 31 exposure traits. So, those are the 31 lifestyle risk factors, including eight physical activity traits, 12 alcohol intake traits, 10 dietary traits, and eight smoking related traits. So, we are going to examine one exposure trait at a time. And then we also examine. And then we also examine the 11 outcome traits. So, those are the 11 cardiovascular disease-related traits, including four pulse wave traits, two blood pressure traits, four umbrella lipid traits, and BMR. And then for the 31 exposure and the 11 outcome, in total, we have 341 trait pairs. So we examine one pair at a time to study each of the risk factor effect on each of the disease risk traits. Of the disease risk trait. On the other hand, we also did a reverse causality analysis on the three or four trait pairs for the outcome on exposure effects. And the number, the reverse analysis is slightly lower because for some of these cardiovascular traits, we did not identify enough instrumental variable for the other methods. So, therefore, we can, you know, to ensure fair comparison for the other methods, we decide to restrict to 304 instead of 34. Instead of 34U1. So here shows the results, a QQ plot on the real data showing on the left-hand side. So very consistent with simulations, we can see that our purple method, it sits at the beginning, it sits on the diagonal line. It gives you very well-behaved p-values. And then later on, it goes up, indicating that for the majority of trades, it does not detect the causal signal, which is expected. You are hoping that for majority trade pay analyze. Majority trade pay analyze, you probably wouldn't expect a causal effect. But you do see some signals at the small end of p-values. While for the majority of other methods, you can see that their p-value seem to be deviates from the diagonal quite early on at the beginning, indicating that they are identifying a lot of causal effects among those pairwise analysis. And so, to confirm that, it's indeed It's indeed providing the calibrated one, we decide to permute the outcome variable. So, on the right-hand side, it shows the QQ product for the permuted data. So, because you permuted outcome, you wouldn't expect them to see any signal. So, indeed, you can see that our method sits on the diagonal again pretty well, but many other methods still gives you inflated p-values. And so, in total, through screening the causal effects, we detected eight causal trait pairs based on the Banfloni correct. Based on a panflony corrected p-value threshold. And all of them have strong biological support. So I'll just mention one example. So the MR8 identified a negative causal effect of smoking on BMI, which makes sense because smoking can influence BMI in two different ways. Specifically, this nicotine intake during smoking can, on one hand, reduce the net energy storage in edible tissues, and so therefore it may lead to a weight loss. It may lead to a weight loss. And on the other hand, nicotine intake will also activate the acetylcholine receptors in the hypothalamus. And so therefore, it will lead to a suppressed appetite and food intake. So therefore, it will also negatively impact the BMI. So therefore, this biological makes sense. And also, as another example, I think under reverse association, we did not detect any significant. Any significant effects. However, many methods detect the causal effect of BMI on the time driving. So they think that having a higher BMI is likely to influence it's going to, I think it's going to increase the time on driving, which does not seem to make sense. To make sense. So, those are the main results we have. So, as a summary, we have presented this new method called MR-AID, which is a MR method that can automatically select instruments while controlling for both uncorrelated and correlated horizontal potropy. And because of that, as a result, MR-8 can control for type 1 arrow wheel and displays high power. MR-8 is also effective in various simulations and In various simulations and real data applications, including the trade-on self-analysis and large-scale MR screening. And the MR method is available on our lab website and the menu scribes and recently published in science advances. So that's all I have. I'm happy to answer any questions. Thank you, Jean. Does anyone in the room have any questions? Um does anyone in the room have any questions? Um any questions online okay that's a question from Devon. Um what's the computation time, especially compared to weighted median? Hey, yeah, and that's a good point. I think um computation time is not bad. So for analysis. Bad. So, for analysis, typical analysis costs about a few minutes. But the computation time depends on how many SNPs, how many candidate instruments you have. So, if you have a couple of thousand instruments, that's a typical trade analysis. I think the computation time was a few minutes. And if you have more than like 2,000 or 3,000 SNPs, it's lower. It can be up to 10 minutes. So, it's not a bad. But certainly comparing with some weighted media, weighted media. Comparing with them, weight in the media, weight in the media is much faster, or with the medium may only take less than a minute too long. But I mean, 10 minutes versus one minute is not a big deal. Any more questions for Zhang? If not, then we break for an hour and a half. Oh, yeah. So yep, so uh, we've had