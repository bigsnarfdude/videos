Yeah, so my name is John D. Driesy and I'm at Western University. And I'm going to talk from a more applied perspective. So actually my background is that I'm a neuroscientist and I'm very interested in human brain organization. And so a good part of my lab is actually involved in designing and conducting experiments to study the human brain. And the data that we use. That we use for our statistical developments is mostly functional magnetic resonance imaging data. So, this is a technique that you probably know. It's kind of available in every hospital in Canada, most hospitals in Canada. It's a big magnet where you can put people in, and within about five minutes, you can take a very detailed image of their brain. And that's used in standard clinical care to look at strokes or tumors or whatever. But you can also read. Whatever. But you can also reprogram this machine to take a very rough image of the human brain within a second about. And you can do this repeatedly. So you get actually a movie of these images. Now, if you look at a single pixel in this image or what's called an image voxel, so a volume pixel in a way, you can actually see that over time, the brightness of that pixel is slightly changing. Okay. And that's related, it's been shown. And that's related, has been shown, it's related to the local brain activity in this area. So, this is a technique that has been around for about 25 years, and it's really revolutionized human neuroscience. It's a really fantastic technique. Basically, what you get, you can measure simultaneously about 200,000 or half a million brain locations in a single behaving human while they're doing tasks in the FMI, and you can observe what the brain is doing in this time. The brain is doing in this time period. It's also very noisy data, we have to say. So there's a lot of statistical development that is necessary to analyze this data properly. And so this is what my lab is mostly doing. So what is functional imaging research trying to do? Well, of course, we're trying to use this data to build general models of brain function. So that's one idea. So questions like, you know, what brain region does what? You know, what are the functions? What are the functions of different parts of the brain? And how do these different parts of the brain communicate with each other? And ultimately, how does this whole thing work? But they're also very, from the clinical perspective, they're very practical problems of trying to, in a single individual, trying to estimate their particular brain organization. And this is where issues about machine learning and prediction come about. So we would like to know what aspects of brain organization predict good function or dysfunction. Organization predicts good function or dysfunction. We would like to identify functional brain regions within a single person, either to study them further or, very prosac, to actually know whether surgeons should drill a hole in which parts of the brain they can take out or whether they should implant stimulators. So there are a lot of practical problems here that brain imaging is promising to solve. Now, when we think about models of brain organization, and this is also the model, type of model. This is also the model type of models that I'm going to use to present the algorithms that I want to show and the architectures that I want to show. One of the typical ways of thinking about the brain is to think about the brain as being partitioned into a discrete set of functional regions. And this idea has a long history in neuroscience, starting from about 150 years ago, that people realized that the brain is homogenous. It's really a distinct set of regions where we can actually. Set of regions that where we can actually identify boundaries either using anatomy, but nowadays also function and connectivity. Okay, so we can think about this: maybe the brain being composed of maybe 600 different regions, and they all have their own function, and they all communicate with each other. So it's basically postulation. It's basically postulation, oops, postulation model. Sorry, I went backwards here. Okay, so this is really important in neuroscience, and you can see that in the number of papers that have proposed brain parserations. So there are 50 or maybe hundreds of different brain parsellations that you can look at, and we have reviewed them in this paper. They're all somewhat related. They all kind of identify similar boundaries, but they're all distinct from each other. And the reason is that they depend on different data sets. Data sets. So there is one type of data set where we have a lot of data from a lot of subjects, either thousands or even tens of thousands of data sets in the case of the UK Biobank, where we have studied many, many individuals in what's called a resting state scan, where you just put people in the scanner and you just let the scanner run for 10 minutes while the subject isn't doing anything. So that data we have a lot from. There are also some other data sets where you not only Data sets where you not only observe the subject during resting, but also during while they're doing specific tasks. And these data sets are typically much smaller, so many fewer individuals. But there are so-called deep phenotyping data sets where you have from a few subjects, you have a lot of data. And you can also build brain parcelations of this. And then there are many other data sets now available that are kind of in between. So there are less data per subject, but maybe more subjects. Per subject, but maybe more subjects. Okay. And so, what I want to address in this talk are efforts from our lab and from other people to try to learn models of brain organization fusing these data sets across these different data sets. Now, this is especially important because each brain is actually different. Okay, so my lab is mostly interested, or especially interested in this little brain structure here in the bottom of the brain. That's the human cerebral. The bottom of the brain that's the human cerebellum, and if you stretch it out into a flat map, it kind of looks like this. And you know, two or three years ago, we have published this group map or group parcelation map of the human cerebellum. But when you look at individual subjects, so individual subjects within that group, you see that the brain organization is actually quite different across different subjects. Okay, so these are deep phenotype subjects, if you will. What you can see is that they all kind of have a similar, they all have this functional region. They all have this functional region, like in the group, but the size and the arrangement of these regions is actually reliably different between these different subjects. And the question is, of course, well, you know, does this functioning matter? What are predictors for good functions? What are the differences between them? So, what we really want is we would like to build a probabilistic model of human brain organization, not only get Human brain organization not only get a mean map that we can just present, but also one that gives us a probability distribution over these possible brain organizations. Now, that kind of capture the true variability in the population. So really what we want in the long run, this is kind of the end game, we would like a model that gives us a probability of a certain brain organization given some parameters of the population. Now, what should we be able to do with this? Now, one thing that we should be able to do is. One thing that we should be able to do is we should be able to take very restricted data from an individual, maybe something that we're going to get in a clinical setting, and actually use the knowledge that we've learned from many other individuals on this task battery to predict now together the left out data, how that subject would respond or their brain would respond if we studied it in this task. So it's kind of a missing data problem where you basically vast part of this potential observation. Part of this potential observable data is not observed, and you really want to make inferences about this. Another ultimate, really important application of a probabilistic model would be that you could look at some data from a single subject and you could get a estimate of how likely that is in the population or how unusual it is. And that might give you warning signs for clinical diagnostics purposes that you say, well, this is a very unusual brain organization and it might be. Unusual brain organization and it might be predictive of developing schizophrenia or some other disease. So, now learning a probabilistic model of brain organization, the mean is very easy to learn. If you really want to learn something about the variability and the interrelation, the complex probability structure, you need a lot of data. And there are many barriers to this. So after 25 years of neuroimaging, we don't have that model yet. And the barriers are a number of things I want to talk about. One is that it's a technical barrier. One is that it's a technical barriers, like how can you get this data? How can you manage that much data? And how can you learn from much data? There's a problem about models. What type of models can we actually train on this data, which models are appropriate? And then there's a problem of once we have our dream model that we rightly estimate, how can we actually do this feasibly on this amount of data? And I'm going to come to all these three things in turn. Things in turn. Now, on the technical side, luckily there's 25 years of research on this, and many of these technical problems now are actually solved. So, one problem that I think the brain imaging community have solved very well is that to address the natural human anatomical variability. So, it is the case that when you look at each single brain, each single brain is different. They all have different, you know, some are small, some are large, some are tall, some are white. Tall, some are white. You can actually, and this is an example from our work: if you have different cerebellums, so this is the brain structure in the back of the brain, as I said, you can actually, some people have this like happy cerebellum, somebody have a droopy cerebellum, you can actually look at diffeomorphic flow fields that you can estimate on this to bring them into one common space. Okay, and this is these algorithms work very well now, they're highly automated. They're highly automated and they're pretty standard in the field to apply. For the neocortex, so the big part of the brain, which is really a folded sheet, there are other techniques that are kind of state of the art where you reconstruct the surface of the brain and then you inflate that surface to bring basically the sphere up to something smooth where you can see it. And then you align different subjects within this manifold of the structural manifold of the brain surface. Of the brain surface. So those methods now have matured and are well accepted in the literature. So the next problem is how do you manage that much data? How do you get this much data into one framework so you can analyze different data sets together? And when I started with neuroimaging about 20 years ago, this would have been an absolute nightmare because the community didn't have common file formats or common Have common file formats or common structure. Every lab had its own kind of standard and own kind of software. And over the last seven to five years, the community really has done a lot of work in building file formats, imaging structure, and open databases that hold these imaging data sets. That this now becomes really feasible. And for the purpose of the project where I want to show you the first steps, we've now put on top of this. We've now put on top of this a software structure where we can read out these data sets that are hosted on different servers. We can read them out in different anatomical spaces. So we can actually ask the question at that brain location that corresponds in this subject, what is the activity that we have during this task, or what is the data that we have in this task. So we can actually now query these databases and toll data out in different And pull data out in different data sets without having to reprocess these data. And this is really crucial to make this feasible in the long run. Okay, so a lot of the technical problems are starting to be solved thanks to a lot of community work in the area. And so now what I want to mainly talk about is how can we use this? How can we leverage it to build actually probabilistic models of brain organization? And as I said, a parcelation model. Said a parcelation model, so building or dividing the brain up into discrete regions is one of the ways, and that's the one that I want to focus on. So, to find some terms here, what we're going to use is we're going to think about the individual brain organization of a specific subject S at some brain location i as a multinomial, a multinomial random variable that we can code in this like one hot encoded vector ui, where one place is one. You know, one place is one, and that indicates that this region would be part of functional region number four. Okay, and this assignment there are some commonalities across subjects, but it might vary across subjects which brain location is assigned to which brain region. Okay, so and then ultimately we want models that somehow approximate some probability distribution over the collective. Uh, the collection of vectors for that single subject across all brain locations, and we can just you know basically concatenate that into a big matrix. And I'm going to call this matrix U here, okay? And it's going to depend on some parameters that are the parameters of that spatial arrangement model, theta A. Now, we have the challenge that we want to now relate individual data sets to this arrangement model and analyze multiple data sets or different data sets. They're kind of like Apple. Data sets. They're kind of like apples and oranges in a way in terms of some data sets have very little data per subject, but maybe a lot of subjects. Some data are deep phenotyping data sets with a lot of data per subject, but very few subjects. We'd like to exploit them and analyze within this framework. And so the approach that we are taking is that we building what we're calling emission models. So from a generative point of view, this is kind of the central model, and this is the emission model that emits data. So kind of from a Markov Markov. So, kind of from a Markov model, a hidden Markov model type of view. So, these data-specific emission models specify the probability that there's a certain data set. So, this Y stands for one single data cluster from one subject where we observe the subject across a number of different brain locations, across a number of different tasks. It's a whole matrix of data, and we want to specify a probability distribution of. Specify a probability distribution of this y given the brain organization of that subject. And these emission models are all depending on different parameters. So these parameters, for example, specify for this brain region, for these tasks, what is the expected response of that brain region in these tasks study here. And for this one, we would specify the expected response of this brain region across these other different tasks. So these are actually in. Tasks. So these are actually independent models in a way, given you. Now, how do we do inference on this? Well, given some data, we can now do a message passing algorithm where we send up to the arrangement model these likelihoods, these data likelihoods. And then the job of the spatial arrangement model for every subject is now to approximate this posterior probability p of u given y and theta a. And in many cases, A and in many cases, it's not tractable. So we need to come up with a proposal distribution q of u that approximates it. Now, doing that, we can calculate the expectation. So I'm using these angled brackets here to signify expectation under the distribution Q over the individual brain organization. And that is an individual. And that is an individual postulation that not only has our best guess about in this subject which brain location belongs to which functional regions, but also quantifies our uncertainty of that. And that's very important. Now, how do we so this is how this can work? And we have that working for a restricted number of data sets now. But how can we actually learn this model? That's the key thing. And so this architecture is just a general architecture where we can plug in. The general architecture where we can plug in different types of arrangements models and different types of emission models and kind of play with them. And one of the big advantages of this is that the big problem of learning the overall parameters of the models breaks down. So to learn all the parameters of the models, we really want to maximize the log evidence of the data given all the parameters of the models, integrated out over all these possible brain organizations across subjects. Brain organizations across subjects. And that's, of course, intractable. So, what we can do is we're approaching here a variational type of approach where we're trying to maximize not this log evidence, but the evidence lower bound, which is a lower bound on this, on this. So that's basically the expected log likelihood of the data and the hidden variables under this proposal distribution queue. Okay, and as you know, you can show that that's a lower bound. You know, you can show that that's a lower bound plus some constant on this evidence. And so, the nice thing about this architecture that I showed you is that this evidence lower bound breaks down now into multiple parts. So, you can actually partition this into the expected conditional probability, log probability of the data given the U. So for each expectation models, and only summing over the subjects from this data set. And you can, so you get a separate term for each of the Get a separate term for each of the emission models, and you can get one term for the arrangement model that doesn't depend on the data anymore, but only on the use. So this is very nice because now you have a message passing algorithm. You can pass these messages back and forth between these different models, and you can then learn in the maximization step, you can learn or maximize, try to maximize these things separately. And that's from a software engineering point of view that makes it very feasible because you. View that makes it very feasible because you can actually distribute this. You don't even need to host all these data sets on a single server. You could potentially, we haven't done this, but ultimately, you could potentially pass these messages between different computers and do the updates separately if the data becomes really big. All right, good. So to the details. So as I said, these are really just the first steps of this. So we started out with a very, very basic arrangement. Arrangement model that is kind of the state of the art, unfortunately, in the literature, which is an independent arrangement model, where you simply, like in a mixture model, you just assume each brain location, the observed data, is a function, a directed function from some vector u that gives you the assignment of this brain location to this cluster. So, in this one hot encoded vector, and that causes this. Causes this observed data here. So, this is the observed data that you have, and all the brain locations are independent of each other. That's, of course, a wrong assumption, but it's a kind of a nice starting point to start with. Let me show you how far we can push it already with this model. Under this case, it's just a simple unnormalized log likelihood is, of course, just a similar function of these latent variables and the parameters. So, the parameters would be the Parameters. So, the parameters would be the probability, the prior probability that this brain location belongs to a certain region. So, this is kind of what's called in the literature a probabilistic atlas of the human brain. And the expectation of this arrangement model, we can, of course, just compute in one step. It's just a soft max of the data likelihood plus these bias parameters that now give us the prior probabilities. So, that's a very simple array. So that's a very simple arrangement model, and I'm going to show some more complicated arrangement models later on. For the emission models, it's a question of what we're going to use. And for imaging data, the question is what do you use for functional imaging data? And so what I'm plotted here is an example of some data that we recorded where every single dot here is a voxel in the human brain. And what I plotted here on the axis is the response of this voxel to when you watch a nature. To when you watch a nature movie, when you watch a point-line walker walking across the screen, or when you're doing a difficult working memory task. Okay. And what you can see here, well, I should have shown this in a 3D animation to see this better, but what you can see here is this big data cloud, but there are these kind of stars or sausages coming out out of this distribution. And so it's best actually described by a directional distribution. We can say, well, there are different voxels that have a different tuning or a different response. That have a different tuning or a different response, but the size of the response is actually up to a lot of different factors like measurement noise. Okay, so that's very natural. So the size of the response of a voxel isn't very meaningful. It's a relative profile, the relative distribution across different tasks that actually matters. So a natural choice here is a directional distribution. And we're basically using a von Mises, a high-dimensional von Mises-Fisher distribution. This Fischer distribution. So ultimately, together with our arrangement model that gives us an assignment, we basically have just a K-mixture of the von Mises Fischer distribution. Okay, so we're on very well-known statistical territory here, and we can cluster that based on this. So we're really in a K-clustering von Mises-Fischer distribution model in this very simple basic case. All right, so using this, can we already join different data sets? Would this work very nicely? And so here, these are simulations that These are simulations that I did together with my graduate student, Dat She, and it's just submitted to NIPS. So, this is very recent stuff. So, where we generated a simple kind of brain organization model on this very simplified 30 by 30 grid, which you can easily simulate and visualize, where we say, like, okay, well, in the population, there are these five different regions that we want to do. And the typical layout is something like this. Layout is something like this. From that, we can generate different subjects. They all, each subject has a slightly different layout. So, here the light blue region is smaller, here it's larger. But ultimately, they all kind of respect this underlying topology. And so, from that, we can generate some data, some observed data. And so, what we've done here is to simulate the fact that what we are dealing with is some data sets are very good, for example, at this data set might be very good at distinguishing. Data set might be very good at distinguishing the light blue from this orange region. But as real data set, it might not be very good at distinguishing these two regions, region one and four. So these might be visual region, and this task set might not include many visual tasks. Whereas some other data set might not be good at differentiating between the light blue and the orange region because it doesn't have any working memory tasks in them. And so we can generate these data where the These data where the expected response of these regions is the same in this data set or in this experiment, and another one where the response of these two regions is the same in this data set. So we have a number of subjects, a few subjects here, and we have more subjects in this experiment that we can study. And so now we can do the typical thing that is done in the literature, namely just estimate a single model from each of these data sets. From each of these data sets. And then we can try to take an individual data set from its individual subject and try to reconstruct what their true functional organization was: this latent variable u that we simulated. And of course, what we find is that when we make these two response vectors the same, then the model, of course, has no way of distinguishing where that functional boundary between these two regions is. Okay, so it's not going to find the right solution. For data set two, again, For data set two, again, there's very little information about the differentiation between these two regions, so it's very uncertain about this one. Now, what we can do is we can build a model like I described to you, where I have one arrangement model and two emission models. And so, by learning this collaboratively and doing this message passing, so the emission models pass a message up to the arrangement model, the arrangement model links the two emission models by sending the messages back. So, in this EM, So, in this EM, and we can learn this with expectation maximization, this particular problem, you get two emission models that are aligned. And so, you have a with the arrangement model that gives you a whole model, a fusion model, that you now can use to reconstruct these subjects. And obviously, in this toy example, it works very nicely that you can basically now take the same data from the subject and reconstruct the subject very well. Now, I should note that in the data for that subject, That in the data for that subject, there's no evidence where that boundary between region one and four is. So, this is just basically purely learned from the fusion model based on the other subjects from the other that have been studied under the other data set, where there is a differentiation between region one and four. So, this uses population knowledge to fill in this gap in this particular subject. Okay, so that works very well, very nicely in this context. And you can see. This is in this context. And you can see when we evaluate in simulation, we can, of course, compare just do the absolute error between the reconstructed brain organization and the real brain organization and then average this across subjects. Data set one leads to very poor reconstruction because it's small and very noisy. Data set two to better, but when we fuse the two wear better than either of them. Right. So that's one thing that the model can do in this framework. Another thing that the model can Another thing that the model in this very simple framework can already do is deal with missing data within a subject. Missing data in the subject in brain imaging is a really, on functional brain imaging, is a really big problem. Now, the fact is when we take an anatomical image, which I have here in gray underneath, we usually get complete data. Normally, the whole brain is covered. But when we study functional data, this is this image here. These are the voxels that are measured functionally. Are the voxels that are measured functionally? I overlaid this with red. We see that some parts of the brain aren't actually covered here. And this is very typical for imaging and has multiple reasons. One reason is what's called signal dropout or susceptibility artifacts. So this has to do with the physics of the MRI machine and the way that the ear canals and your nasal cavities are formed, because the air leads actually then to susceptibility artifacts in the magnetic field, which in turn leads to susceptibility. Field, which in turn leads to signal dropout. So you have these areas in some subjects where you don't get any signal. Now, in other subjects with other shaped ear canals, this might be different, and you might actually get data there. So it's not completely random at missing at random. So there are areas in the brain where this is more problematic than other areas. But we do have partial observations across subjects. And one of the challenges in building good group models is, of course, how to deal with this. This. You know, should you just throw away subjects where there's too much data missing, or can you actually utilize the good information that you have from the subjects? Another really big problem, and this is especially the case when you're interested in the cerebellum as me, is that some people just try to save money in the data acquisition and don't acquire that type of data. So they only scan the brain down to here and then cut the rest out because they just thought they weren't interested in that part of the brain. And so, you know, if we threw away all the data sets where this is. All the data sets where this isn't fully covered, we would end up with very little data. So, by having an algorithm that can deal with missing data effectively, we can utilize much more data. Okay, so can we do this? So, here's another simulation example, just to show you that there shows it also works on real data, I have to say. But here is another simulated brain organization. This time it's a little bit less smooth. You see, the subjects are all different. Here we can now punch holes in. Can now punch holes into our data organization. So, here these black spheres are the holes that are missing data. So, this you can think about these as susceptibility artifacts that we didn't observe in these subjects. First of all, the overall model learns very nicely. So, this is both the arrangement and the emission model together. You see that the parameters are actually very close to the real parameters. There's very little difference here, no matter if we have one, 10, or 20%. If we have one 10 or 20 percent of the data missing, it converges very nicely. And we can reconstruct the data from each subject very well with this. So here you can see that when this data is missing here, we can nonetheless kind of make pretty educated guesses about which functional region this are. Now, and again, here the reconstruction error is actually not too crazy, even if 20% of the data is missing. If 20% of the data is missing. Now, I should say that this has, of course, its limits, right? I mean, the model isn't magic. So, for example, here, you can see that in this subject, this blue area is actually very small, meaning that maybe a surgeon could go in and take this area out if the blue is really a kind of what's in neurosurgical terms, an eloquent area that doesn't really have a big function. So, you know, the model, when this data is missing, would fill this in by the normal observation from. This is in by the normal observation from the group, the group prior, if you will, and would fill this in with the blue, would overestimate this small region substantially, just because there's no data there. So this motivates us to not only build models that where each brain location is assumed to be independent, but we know that the brain organization is actually smooth. And there are the dependent structure between different brain locations. So if we could do this, we would obviously be able to know. This we would obviously be able to now fill in this data, this missing data, more informed from the data that we have on the fridge. Okay, and so this is the essence of really what we're trying to do, really novel here, is to learn a representation of a brain organization where we can model the dependence between different brain locations. Okay, so one One approach that has been used in the literature a number of times now is the Markov random field, where you basically the emission model considers all the emissions or the data points across the brain as conditionally independent. This is a graphical model, I want to say, is conditionally independent given the central arrangement. So, this brain organization. And then a central arrangement model is an undirected graphical model where you just have. Model where you just have edges here that link these neighboring nodes with each other. And if these are multinomial variables, you basically end up with a POTS model up here, okay? Where you have an energy model where the unnormalized log likelihood is basically the sum of the bias terms at each brain location and some weight, and then whether these two are assigned to the same or not, same class or not. same class or not. And this gives you an energy function and then you have a probability distribution on that. Now the problem with this is that Markov random field models are computationally very difficult to deal with. And the reason is that when you want to do calculating expectation, if you want to sample, when you do want to Gibbs sample, you need to do the Gibbs sampling node-wise. So the Gibbs sampling proceeds was holding all the nodes constant and then resampling. Constant and then resampling this given the neighboring nodes, the connected nodes. And then you go to the next node and you resample this node given the current value of those nodes. And given that we are dealing with not with a nice chain like in this toy example, but really in the cortical surface, we're dealing with about 60,000 vertices arranged on this grid. This is getting very, very slow. So we were looking for a better, faster architecture. Faster architecture, and we're just in the first stages of trying this. And so, the idea was here to introduce a latent layer of variables, also multinomial variables. So, here denoted H, they're also K-long variables, and to fully connect them to this layer that gives us an estimate of this outer layer that gives us some estimate about the brain organization. And then model all the spatial dependencies between these u's by Use by these latent variables h. Okay, so they're fully connected again using a POTS-like energy model distribution. Okay, so the advantages of this are, of course, that this type of model is very, very similar to a deep Boltzmann machine with a couple of differences. First of all, these nodes are not Bernoulli variables, they're multinomial, and in the output stage, we have these direct. And in the output stage, we have these directed nodes, but that's not a problem because they're just one-to-one connected. Okay, so we can use very simple, similar tricks to what's used in the De Boltzmann machine. We can use mean field approximation to calculate an expectation given the data. We can do layerwise GIP sampling. So we can sample this one, this layer, all in one bunch, given that all these conditionally on the data and conditionally on the hidden variables, all these are mutually independent from each other. So we can. Other. So we can just do layer-wise skip sampling, which is much, much faster than node-wise GIP sampling. And ultimately, we can learn this with variational stochastic maximum likelihood. So here's the first example of how this works. So here is again one of these toy simulations on a 30 by 30 grid. Again, this is the subject with the assignment to the different regions. Here I'm showing you the probability that this voxel belongs to region. That this voxel belongs to region three. So this is just an indicator variable: one being red and blue being zero. When we estimate it from the data only, this is very noisy data, so we get a very noisy reconstruction. You can see the model is very uncertain about the assignment. When we use our independent spatial arrangement model on this data set, we can already get much better. So we're exploiting the knowledge that this middle region is usually region three across the population, and that gives us a much better. Across the population, and that gives us a much better estimate already. And when we learn and then apply our deep, what we call a deep Markov random field model, the last model that I showed you, we can improve this substantially better. You can see here that the reconstruction error really goes down. And this gives us hope now to apply this to real data to really make a progress in learning the codependencies. In this case, the model only had to learn basically that these maps were relatively smooth. Snaps were relatively smooth, okay. So that there was a high likelihood of neighboring brain locations being assigned to the similar parcels. In the long run, what we would like to do is learn longer-range dependencies. So between the two hemispheres, between the cortex and the cerebellum. So there's a lot of complex structure that we can probably discover with these methods. Okay, so to end with, I would like to show you one example where we applied. one example where we applied this framework to real data and and i think it's just this simple uh it's just single data set and it's just the simple arrangement model or independent arrangement model so it's a it's it's computationally not really uh complicated um but it has already very encouraging results i think uh so what we did here we took a data set that that we acquired uh from which we derived our cerebellar group map uh that we published um and this data set is is what would be called Set is what would be called a deep phenotyping data set. So, with a lot of data from a few subjects, so 24 subjects, but we have 160 minutes of scanning on task set A from those subjects, and another 160 minutes of scanning on task set B, with 29 and 32 tasks. So that's a substantial amount of data per subject. And so, the nice thing about having so much data per subject is that we can now split this into a training and test set. Because with real data, we have, of course, have not. With real data, we have, of course, have not the luxury to evaluate models by comparing them to the ground truth. We don't know what the ground truth is. So, we need to figure out a good way of evaluating these different models. And so what we can do is we can use the training set and the model to estimate the individual parsellation, the expected value of these latent variables for that subject. Okay, so this is an individual parsillation map probabilistically. That probabilistically, if you will. And then we can predict the test data that we withhold from the model fitting for that subject. And of course, to do this, this individual parcelation tells us in the subject which brain location belongs to which region. And then from the other subjects that were studied on this task set, we know this particular region that in that subject is located there, how would it respond to those tasks in this one? Response to those tasks in this one. So you get the response vector basically, these v's from the other subject. And combining those two, you can start predicting this individual data. So how well can we do this? So what I'm plotting here is what we call an expected weighted cosine error. So I can go through in the details if you're curious about this. It's basically a cosine error that is weighted by the amount of signal-to-noise you have in this voxel. And then we take the expected value. And then we take the expected value given the uncertainty of the model at this brain location. Now, if we use only 10 minutes, so now what we wanted to do, we wanted to look at how much data is actually needed to reconstruct an individual, to estimate these latent variables reliable, to reconstruct a brain organization map. And so, if we use only 10 minutes of data, it's actually quite bad, as you can see here. If we use the same subject and we use Same subject, and we use uh okay, it's actually quite bad. Sorry, um, and the reconstruction error is quite high, it's actually uh much higher than actually taking the group map. So here we just take the group oscillation, the probabilistic group oscillation, and get the same error. And you see, actually, you can predict individual subjects better just by the group mean rather than in its individual map. Um, and this is, of course, very sobering. You wasted all these 10 minutes of scanning that you did, okay? And this is one of the reasons why. Reasons why this individualized approach is not common in functional neuroimaging. So, people call this scan here functional localizer, where they just put a subject in and before running the main study, they're trying to get a map of that single subject. Okay, and that there is a good amount of literature actually using this approach. However, it hasn't really caught on completely. And the reason is that you need about 20 to 30 minutes of individual data to break even. Individual data to break even, to be just as good as a group map that you can just download free from the internet, from somebody else's publication. Yeah. So you need about 60 minutes or a full hour to actually get substantially better than the screw map. Okay, so that's a big disadvantage. When we use 160 minutes of data, we are actually substantially better. Data, we are actually substantially better than the group map. So that's great. That also shows that with this individualized approach, we can actually predict the functional organization much better. So there's true intersubject variability. Now, when we use our approach to merge the data likelihood from a single subject, 10 minutes of a single subject, and the group map, we get a pretty good reconstruction that now combines both the strength. That now combines both the strengths of this 10 minutes of data plus the group prior. And you can see that we now reconstruct the data much better or predict the data, the missing data, much better than either of these approaches alone. And so this is a big advantage. It also improves the estimate even if you have 160 minutes of data. So even if you have a lot of individual subject data, applying this model regularizes the individual. The individual estimate towards what you might call a group, a learned group prior that we can do. Now, to put it in a practical perspective, our model using 10 minutes of data gives as good of a predicted value as when you're only relying on the data of the single subject for about 100 minutes of data. Okay, so you can either scan for one and a half hours or you can use our model and scan for 10 minutes. And that, you know, just in real terms, already saves. Just in real terms, already saves $550 per individual subject. So if you have a big study to run, that's a huge advantage, obviously. Obviously, there's much more work to do. So what we don't know is how this model estimate would actually be to estimate the to determine what differences between subjects are functionally relevant. We haven't really applied spatially informed models and so on. So there's a huge Uh, and so on. So, there's a huge amount of work to do. So, these are really the first baby steps that we've taken in this approach. So, I want to acknowledge the person who is the student who has done most of the work on this, Daji, is a computer science student, and this is part of his dissertation. But I want to stress that this is really a team effort of my lab involving a number of postdocs and graduate students listed here, and also collaborators, Berkeley at McGill, at Dalhousie. Berkeley at McGill, at Dalhousie, and at Harvard University. And I thank you for your attention.