Good morning, everyone, and thanks for the invitation to join this BERS workshop. Thank you also for the support from BERS over the years and the continued effort of two people here, Mayank and Kumar, with the various organizers over the years. And that's a great, great initiative. Very happy to be again part of this new session, bringing people together. Session bringing people together around multi-phase routes and upscaling and industrial applications. So, my objective today is far more modest than was described by Kumar earlier in his own talk. I'm going to discuss something very, very specific that are models for hydrodynamic false and torque fluctuation. I'll say a few words about what's the objective here, and it's sort of a review talk about what we've done over the years on that particular. Use on that particular topic. So let's see. Okay, so when we talk about multi-phase rows, this is this amazing movie that was recorded by the people at NETL. And then I think more than a thousand words, it tells you a lot what multi-scale particulated flows mean. And you can see that when you zoom in and zoom in and zoom in again, there's a relationship between There's a relationship between the large-scale dynamics and the small-scale dynamics. Okay. And so, with that particular picture in mind, so later the video is going to unzoom again. So, really, you see down to the level of each individual particle, you can see their motion. And then when you zoom back again, this will control the large-scale dynamics in this particular agent flow. So, that's a flow in a riser in a chemical engineering equipment. So, having this in mind. Equipment. So, having this in mind, I think this was very well introduced and described by Kumar in his own talk. We're trying to develop different models, okay, at different scales. So, generally in the community, there's now a consensus about describing the hydrodynamic scales with three different scales. Okay, so micro scale where we use particle resolve direct numerical simulation, a mesoscale where A mesoscale where we still track individual particles' motion, but we solve the fluid conservation equation in an average fashion. And then eventually you have the macro scale or large scale models that are called a layer-a-layer, where we introduce additional closures, especially for the dispersed phase that we see as a pseudo-continuous fluid. So, in this landscape of models, what I would like to Of models, what I would like to describe today and discuss is again far more modest. I want to talk about a little bit the upscaling from the micro scale to the mesoscale, and in particular, discuss one of the closures that we need, which is the interphase momentum transfer. And that brings us to the hydrodynamic force and torque exerted on each individual particle. And that's basically the objective of the talk. Okay, so what's the usual Okay, so what what's the usual uh workflow? It's it's pretty standard. Um I'm just uh stating the obvious here. I mean you do data collection, then you do data analysis, then you try to design a model that represents better the data you collected, and eventually you want to test the model. Okay, so you have these different four different stages, and in particular today I'll discuss data collection at the end of the talk, but most of the talk is about model design, okay. Model model design. Okay, so there are two types of model I would like to discuss today that are trying to capture the hydrodynamic false and torque fluctuations. One class of model is stochastic models and the other class of model is deterministic, but it's microstructure informed. And I'll say later what I mean by microstructure informed, although right now it has become a pretty widespread term in the literature and in the community. And in the community. Okay, what's the problem? So, again, I think this is well understood by most people in the audience, but anyway, if you look at the drug in a particulated flow system, if you take a small control volume and in that control volume, you compute the average volume fraction and the average velocity, you can find a very strong correlation between the mean drag and the mean volume fraction. And the mean volume fraction, and the mean, let's say, Reynolds number. However, if you look at each individual particle, the fluctuations are pretty large. Okay, so this is an example on the right-hand side, Reynolds number 40 at the level of the particle using the particle diameter and volume fraction 0.1. And the average drag is of the order of five, regardless of the units here, but the fluctuations are also of the order of five. Okay, the fluctuations delta F is between F is between roughly minus 2.5 to plus 2.5. So we have fluctuation of the order of the mean drag. So this is a well-known problem. You can design very advanced and very complicated equations to describe the average drag. I've given a few examples at the bottom here. To be very honest, I think the first one is Bestra, the second one is, I can't remember, maybe Tennetty. I'm not sure. Shankar is listening, so I shouldn't say something wrong here. Anyway. And say something wrong here. Anyway, so you have these complicated closures for the average drug as a function, average volume fraction, average Reynolds number. There's literally no correlation between the local drug and the local volume fraction. So this is pretty easy to understand. Again, these are four pictures, literally first year undergrad course of food mechanics. But anyway, well, when you take the average volume fraction, you're basically cost-graining the microphone. You're basically cost-graining the microstructure. So, this is an example of four particles. I'm interested in the hydrodynamic force, so only the drag, I mean, no matter. On the green particle, and if I use the usual approach where I have the same, roughly the same average velocity in that volume fraction, I have, of course, the same volume fraction because I have the same number of particles, but you can right away tell that the actual hydrodynamic force exerted on the green particle will be completely different in these four different microstructures. In these four different microstructures. Okay. So that's the problem statement. How can I recover the fluctuation or the variation of the force exerted on the green particle, although I have the same volume fraction, same Reynolds number? Well, the answer is sort of pretty intuitive. I'm trying to reincorporate the microstructure. So I need to take into account the fact that my three red particles around the green particle are. Around the green particle are located at a specific position. It's going to completely change the flow disturbance and eventually change the hydrodynamic force. So how do I design model where, again, I'm going to make an assumption that is extremely intuitive. I'm going to say, okay, the actual hydrodynamic force is the average hydrodynamic force that I can estimate with my usual correlations plus a fluctuation. A fluctuation, okay, or a deviation, or a variation. I'm not sure what's the right term, but anyway. And so, these new models are trying to come up with a close form or a form of estimation of what is the deviation or the variation. Okay, there's just a short note at the bottom of the slide here. As soon as you make everything dimensionless, you still have to decide what's the average velocity you're using to make things. Velocity you're using to make things dimensionless. And you can take the average velocity in your control volume, or you can actually take the average velocity just around the particle. So that means filtering. So we have to agree on what kind of definition we're using for that. But that's maybe some minor point anyway. Okay, another amusing comment is we would like to design models for the fluctuations, but do we actually agree on the average force? You know, so the traditional. Course, you know, so the traditional models, and we generally agree at low Reynolds number, but apparently we don't agree at high Reynolds number. Okay, so if you look at these different plots, I'm showing here the usual and well-known correlation from Pestra, Tenetti, Tong, and Bogner. And they seem to agree very well up to roughly Reynolds 50, and then they disagree quite a bit. And so I think that's an opportunity. And so I think that's an open question that we may want to address as well, the community, and try to come up with at least a correlation that agrees on the average force. And now I'm trying to go one step further and discuss the fluctuations. Okay, so three types of model quickly and trying to not to run out of time, a stochastic model that we looked at maybe five, six years ago, only for drag. It's not microstructure informed. It's not microstructure-informed, I'll explain why later. And two microstructure-informed models: one is probability-driven, and the other one is machine learning-driven. And so, in the two last cases, the difference, I mean, the difference or what's new is that the fluctuation delta F is a function of Reynolds and phi plus the position of a certain number of neighbors around the target particle. Okay, so we take M closest neighbors, we can discuss what M should be. We can discuss what M should be, but from experience right now in general with 20 neighbors, the model performs very well. Adding more neighbors doesn't change anything to the performance. Okay, let's go through the models quickly. So model one, coarse-grain stochastic model. This was originally proposed by Sunda Rezan in 2005 in the framework of Euler-Eulaire modeling. There's a very nice paper recently by Aaron Latenzi with Shankar and Jesse. With Shankar and Jesse, also that revisit maybe in a more theoretical way and in a cleaner way, I would say. I mean, stochastic models. So, what we did here was a more practical point of view. So, again, the drag fluctuates. There's an average value predicted by correlations. Can I recover the fluctuations by a stochastic model? So, I'm not using right now the positions of the closest neighbors. Okay, so how do we design the models? Again, this is only for. Design the models again. This is only for the drag. Oh, can I move that guy somewhere that it doesn't annoy me? Okay, that's better. So it's the same principle for the drag, an average value plus a fluctuation. The fluctuation is constructed as a first order autoregressive signal. So in the jargon of stochastic processes, this is called a Norstein-Ullenbeck process. Using a white noise, so the average is zero. A white noise, so the average is zero, and so we have two parameters: is A for sort of the memory of the signal and B for the magnitude of the random fluctuations. And the good news is that we were able to determine these two parameters from particle result direct numerical simulation. So they're not fully artificial, right? There's a way, there's a workflow to determine these two parameters. And one of the key quantities. One of the key quantities that helped us to determine these parameters, in particular how much memory the signal retains, is by looking at the autocorrelation of the hydrodynamic forces and trying to match that with an exponentially decreasing function. Okay, so these were the general ingredients of the stochastic model. Then we tested it in Euler-Lagrange simulations, and indeed we got some, I mean, noticeable improvement. It was not perfect, it was noticeable improvement. Not perfect, was noticeable improvement of the prediction on particle fluctuations, particle velocity fluctuations. All right. Okay, so the second model is a probability-driven model, and the starting point is what does the probability of finding a neighbor look like when the false and talk is conditioned to be lower or larger than the average value? So these probability maps tell probability. Tell probability maps tell us where to find a neighbor. Okay, this is for the first neighbor, for instance, around the target particle. So if I don't condition anything, it's geometrical. So I can find a neighbor at a given distance that depends on the diameter, and it's equiprobable. Now, if I say where should be the neighbor if the force is larger than the average, in other words, the fluctuation is positive, and then I'm getting this probability mark, which is again pretty standard fruit mechanics. Fruit mechanics. If I want the force to be lower, the highest probability is to have, of course, a neighbor upstream. It's going to reduce the hydrodynamic force, at least the drag on the target particle. Okay, so this is really the idea. Can I combine these probability maps with some sort of reconstruction of coefficients and coefficients, sorry, and come up with a complete model. So, what are the assembly? Of course, sorry, I can do that with the lift as well, and I can do that. That with the lift as well, and I can do that with the torque, and I'm getting similar probability maps. Okay, so this is now the model assumptions. Some of these assumptions are pretty strong. The first one is extremely strong, is that we assume that the model is constructed as a superposition of pairwise aerodynamic interactions. In other words, I'm going to take the target particle, I'm going to take one neighbor, look at the disturbance separately from all the other neighbors, then I'm going to take the second neighbor, et cetera, et cetera. Neighbor, etc., etc. The good news is that eventually we figured out that the separate probability maps for each neighbor can actually be combined together in a single probability maps. So the model is here. So what does, how's the model constructed? Again, superposition of pairwise ayodynamic interaction. So we're basically doing summation. So the fluctuation is the sum for all the neighbors of the probability of being above or below the average force. Below the average force multiplied by unknown coefficients. And these unknown coefficients, because everything is linear with these coefficients, are just determined by standard linear regression. And again, with 20 neighbors, we're doing a good job. So I want to point out here that the microstructure info model were first pioneered by Barasander at the University of Florida. And I'm sure a lot of you have read about the PIEP model, which is slightly different, but I mean. Which is slightly different, but there are a lot of similarities between these two models. So, how does the model perform if you have now a testing data set and you're trying to compare the predictions of the model to the actual results from particle resolve simulation, where we are able on average, as you can see in this table, so we're showing here the coefficient of determination, we're able to predict somewhere between the lowest value being fixed. Somewhere between the lowest value being 50% of the fluctuations up to 85%. And there's no surprise that when the volume fraction is low, when the rareness number is low, the model performs better because the linear assumption and the summation assumption is more valid. And when you increase the volume fraction, you increase the Reynolds number, the problem becomes more non-linear. And of course, the assumption breaks. So the third model is what we call So, the third model is what we call a physics-informed neural network model. The model assumptions are again superposition of pairwise aerodynamic interaction. We also, so we first try a fully connected neural network, it just doesn't work, we don't have enough data, so we have this specific design, which is called physics-informed neural network. And what we do is that we have one fully connected neural network per neighbor. Okay, so that leads to unified functional representation. So, that's very simple. Functional representation. So that's very similar somehow to what we were doing for the probability-driven model. Again, we need about 20 neighbors, and we added some symmetry assumptions to the model that we did not figure out when we were doing the probability driver. So there are a lot of similarities, except that now a neural network is doing the job of trying to find the coefficients. So if you look at the symmetry assumptions, you can figure out that the disturbance of this particle J on I, Particle J on I, you can only have a disturbance in the streamwise direction and then in this orthogonal direction. Because of symmetry on both sides, this has to cancel out. You can do the same for the top. And eventually, you can write the fluctuations delta F and delta T only in these specific directions. I mean, using these bases EX, EL, and ET. Okay. So that's what the, if you're familiar with neural networks, this is what the neural networks look like. What the neural networks look like. So, this is for each neighbor separately, and then eventually recombine and reproject to incorporate the symmetry assumptions. Okay, so what's the performance of the physics-informed neural network model again in terms of R2 coefficient of determination? Well, if you remember the previous table, you can see it's very similar. It's slightly better, okay, but it's only marginally better, and we have the same trend that the model. And we have the same trend that the model predicts. The model's predictions are, I mean, they're less and less accurate when the nonlinearity in the problem increases. But in some cases, we're still able to predict up to 80% of the fluctuations. Okay, so I'm not running out of time. I guess that's good news. So, some comments to sort of finish on the NPP, so the probability-driven model and the physics-informed model, where they're Form model where their prediction capabilities are in par. There's a design reason because everything is based on the superposition of pairwise hydrodynamic interactions. A solution would be to incorporate ternary, so with three particles hydrodynamic interactions, and I think that BALA is looking at that right now or has looked at that already. There's also a computing reason is that the scarcity of data, because the data we need for our neural network models, they all come from very expensive particle results. From very expensive particle resolve DNS. So there's a scarcity of data, and so presumably one solution is, of course, collect more data, and we need faster solvers and larger supercomputers, which is a pretty naive answer, I would say. Okay, so let's look at these two gaps in the literature. So data collection. So again, we don't really agree on the average force. I wonder whether we agree on the PDF of the force. The PDF of the force that would be an interesting exercise. So, why don't we agree on the average force on the PDF? Presumably, some of the results published in literature are not fully spatially converged. I'm just speculating. I'm not saying that's true. Maybe we haven't done enough cross-comparisons between the different methods, or maybe just don't have enough data to construct the correlation. So, these are just pure speculations on my side. So, what we've done to answer that, we have recently To answer that, we have recently worked on the development of accurate pure DNS on October grid so that enables you to do local mesh refinement. And we have developed everything in this open source software, Basilisk, that I highly recommend for whoever is interested in developing models on October grids. And we have different models right now, but I mean, our legacy fictitious domain method, we recently implemented the Lattice-Boltzmann method and the conservative. And a conservative calcium method, everything on our tree grids. So, we're hoping to get more accuracy and presumably try to answer this question of discrepancy of the average force in the literature. The other gap is scarcity of data to train the microstructure in form models. And what we've done recently is we followed the inspiration from Peter Minev and Jean-Luc Guillermond, who worked on direction splitting. Who worked on direction splitting solvers? So, what is specific to applying direction splitting solvers to Navier Stokes is that not only do you do that on the viscous term for the velocity, but you actually also do that on the pressure Poisson problem. So, there's a little price to pay is that your velocity field is not fully divergence-free anymore. So, okay, we can discuss whether this assumption, but you can a posteriori verify what's the magnitude of the term divergence of U. The term divergence of view on the grid, right? And to see whether this assumption is of being completely or not only slightly compressible, how valid this is. Okay, so the advantage here is that you're not solving multi-dimensional problems anymore. You're only solving a series of 1D problems. It's extremely fast and it scales extremely well. So this is just a Just a preliminary result, or a little bit more. So, we have first compared that we're getting the same PDF of the false as what we were getting with our legacy fictitious domain method. And you can see that the PDF matched very well. That's the case for 2500 spherical particles. I need to confess, I don't remember what's the Reynolds number and the volume fraction on that example. I should have pointed it out, but anyway, and that's the scaling of. Anyway, and that's the scaling of the solver up to 5,000 cores. This is where the fictitious domain method is, and this is where the direction splitting model is. So you can expect up to 80, 80% scalability, up to 5,000 cores. So having that in mind, and because there's a UBC supercomputer here that we can get, I mean, some sort of privilege access once in a while, we computed a case on 7,000. Computed a case on 7,000 cores with 7 billion cells on 150,000 spheres for Reynolds number 20 and 15% volume fraction. And now we're going to use that data set, something we did like two weeks ago. So we're going to use that data set to train our microstructure-in-form model and test the assumption whether the scarcity of data is really a problem. In other words, if I use 3,000 spheres to train my model, or I use 150,000 spheres to train 150,000 spheres to train my model. Do I really see a noticeable, substantial improvement? If not, it means it's not a problem of scarcity of data. It's really a problem of how we design the models. So what we're doing right now is we're trying to come up with a closed form of the probability-driven model through very simple multi-dimensional interpolations. So, what we want is a closed form of the model that works for any. Of the model that works for any Reynolds number and any volume fraction. And if you, I didn't do it here for the sake of conciseness, but if you plot, for instance, the probability maps for different Reynolds numbers or different volume fraction, you can see they continuously change. Okay, so there's a logic in interpolating between two sets of data that you actually computed for a given Reynolds number and given volume fraction. And given volume fraction, and see how the interpolated probability-driven model performs. The other thing you can do, except a pure interpolation, is you can try to reconstruct the probability maps with spherical harmonics. So I give you an example here of spherical harmonics. This is well known in the literature, the axisymmetric solution to the Elmholtz equation with Legend polynomial, etc. And you can try to reconstruct this probability map. So on the left-hand side, This probability map. So, on the left-hand side, this is what is coming from the standard construction, okay, with the data set and we're doing a kernel density estimation to reconstruct. And on the right-hand side, the spherical harmonics. So you can see first visually they look very similar. And if you compute the coefficient of determination to compare them really quantitatively, it's above 0.95. So, spherical harmonics also give you a gateway to have a fully explicit. Gateway to have a fully explicit form of the model that people can easily use later in the Le Lagrange simulation. So that's what we're trying to finalize right now. So the perspectives, I think that this is an exciting time. A lot of work's been done over the last five years on microstructuring formed, starting with what Bar has done at the University of Florida. More and more, we're combining physical and machine learning features into Machine learning features into a single model. And to me, right now, the next step, the most promising next step, is clearly including the ternary interactions and see whether it truly improves the model. More data, I just discussed it. I'm not going to say it again. I think these models have not been really tested enough in mesoscale or large Lagrange simulation in the literature. It's pretty straightforward. Of course, it's a self-criticism here. It's a self-criticism here. I haven't done it either, so I don't know why, really. And eventually, the two largely open questions, I think, that we need to answer over probably the next 10 years at least, is how do we extend these models to freely moving particles? So if you have freely moving particles, you're basically adding six degrees of freedom per neighbor. So the three components of the translational velocity and three components are the angular velocity. So we're not able really to train. So, we're not able really to train machine learning models with only three degrees of freedom per neighbor, the three positions. So, I don't know how we're going to do that with nine degrees of freedom per neighbor. And I'm not even talking about non-spherical particles here. And then the transition from meso to macro is the big step, at least to me, because we're just losing the microstructure that is extremely precious. The reason why these models work well in Olière Lagrange is because we're still tracking the position of individual particles. Of individual particles. So we can actually use these models. That's pretty much it. I want to thank my students. Most of what I presented is their work and their ideas, and then the funding bodies as well in Canada and CERC, Canadian Foundation for Innovation and Compute Canada. Thank you very much for your attention. I'm happy to take questions. Zoom audience, please type it in the chat box or unmute yourself. And any questions from the room? Yeah, very interesting. I have two questions. One is probably very simple. This delta F, I'm wondering how does that depend on system size? Because presumably the bigger your control volume, The bigger your control volume, more particles you have. Is there like a one over square root of n that's already included in that definition of delta f? I was just wondering. And then the second question was for the pin, what equation did you add into the cost function? I understand the pins, you have to put part of the physics as part of the cost function when you train the neural network. Yeah. Okay. So for the first question, What we're measuring is what we expect to be the exact force acting on the particle. And the system is overly simplified, right? It's a statistically homogeneously distributed array of static spheres. So now, if you consider a system with, let's say, 10 particle diameters, the box in each direction, or 100 particle diameters for the same conditions. Parameters for the same conditions, you're going to get the same false fluctuations. So, in that case, it's independent of the system size. And for the second question was about the cost function in the neural network model. So we are not really adding the symmetry assumptions in the cost function. We are using that as an additional step eventually to project everything. So, the cost function. Everything. So, the cost function is only the deviation between the prediction of the model and the actual value of the three components of the force, the two components of the force that we're interested in from the symmetry assumption and the single component of the talk from the symmetry assumption. So I fully understand what you're talking about. You can add additional physical principles to the cost function and forcing the model to satisfy them, but we haven't done that yet. One more question. Oh, okay. Yeah. Very interesting talk. And I really like this neighbor idea to use the neighbor information to understand the drag and torque. Near the end of your talk, you made a comment about the moving particles. And especially, I guess you. And especially, I guess eventually we're also interested in maybe non-spherical particles. And then we're not talking about nine vectors per neighbor. We're talking about what? Plus orientation and torque. I don't even know how to count that anymore. So the question here is, at that point, would that make more sense just to simply just use a stochastic way, assuming we don't have to rely on the neighboring information, just to randomly assign some kind of forces to each part. Randomly assign some kind of forces to each particle. Would that make more sense, or think there's a way to really utilize the information of different neighbors? Okay, so fully legitimate comment and fully legitimate questions. First, I want to thank you for making my academic life misery because already with nine additional degrees of freedom, that you want to add orientation plus shape plus aspect ratio plus plus plus. So I don't know, maybe I'll be depressed tonight. Sorry, I'm kidding. Okay. Tonight, sorry, I'm kidding. Uh, okay, uh, yeah, I think that no, uh, so first of all, I can't answer that because uh, I can't predict anything, however, I think there'll be a way that, oh, there'll be a point that we may have to give up on on designing this type of models exactly for the reason you pointed out. However, what we have tried recently in my group is to see whether, for instance, only for non-spherical particles. sense only for non-spherical particles. Just talk about that. Whether the orientation only matters for maybe a subset of closest neighbors and then when you go farther from the target particle, maybe the disturbance is sort of generic. So we might gain, we might reduce a certain number of degrees of freedom. I'm not sure. This is pure speculation. So I have a student right now working on taking a non-spherical particle, taking another non-spherical Particle, taking another non-spherical particle, looking at the flow disturbance, and now replacing this non-spherical particle by a sphere, and see how much the flow disturbance changes. And maybe this can be used as a simplification for microstructuring for model with non-spherical particles, for instance. The other widely accepted, and I think this assumption has to be tested, is that even if particles are freely moving, the location of neighbors matters more on the disturbance than their On the disturbance, than their actual relative velocity. I'm not sure that's true. That's why I'm right now suggesting that we first test this microstructure info model only with positions of the neighbors into a Layer Lagrange simulation and see what kind of improvement we're getting. If we're getting already 90 or 95% improvement, I mean close to what the dynamics should be, maybe we just all agree that yes, relative velocities matter. Relative velocities matter, but the price to pay is too high and the improvement is only marginal. Again, these are pure speculations. I think the next few years are going to tell us in what direction to go. But to finally, to conclude and to answer your question, yes, I think ultimately if we can sort of bring everything together into a stochastic model, that's much better. That's by far, much better. I just don't not sure I know how to do it at that point. All right. Thank you. All right, thank you. I think just going back to that, I think this is a, first of all, my intention is to make everybody studying the drag and torque life miserable because making it complicated eventually will, to some degree, it's not just making our life miserable, but also making the job secure because there's an infinite amount of problems ahead of us towards the industry application. Because presumably, all the particles you see in applications are not spherical. But another thing is I really want to. I'm interested in your opinion because when you have a fixed bad problem, you sort of assign fixed Reynolds number to all particles. Is that correct? So the Reynolds number, we don't look at the Reynolds number on the neighbors, right? We look at the Reynolds number on the particles. And this is what enters the model right now. Okay. Right now, okay, because if you assign a Reynolds number, okay, well, that's not a bad idea, by the way. Uh, if you assign a Reynolds number to the neighbors, that can be a simplified way of taking into account the relative velocity. So, instead of having three or even six additional degrees of freedom, you bring everything together into one scalar, which is one number. That could be the next step, presumably, just looking at velocity magnitude and not looking at the three-dimensional. The three-dimensional velocity of the neighbors. Yeah, that sounds great. Thank you. Yeah, you're welcome. So our next speaker is not on Zoom. Is Olivia on the Zoom? No. I don't.