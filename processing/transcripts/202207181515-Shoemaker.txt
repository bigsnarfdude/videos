I'm really sorry that I'm not there. I would have loved to have seen all of you again that I haven't seen for so long and in person, actually, in person, real people. And also, I understand it's a beautiful place and I've never been there before. So I'd like to be. So today I'm going to talk about some work I've done with one of my graduating PhD students on many, what's called many objective optimization problems. Problems. And so his name is Wang Wen Yu, and this is the department that he's in. So we're trying to address a difficult problem within optimization, which is many objective, which means we're talking about you've heard of multi-objective problems, but what most people talk about are when there's just two objectives, conflicting objectives. But we're talking about many objective problems. About many objective problems. So there can be maybe four, five, even more objectives that you're trying to deal with. It makes it a much more difficult problem. In addition, the problem is multimodal. So we're not assuming that you have any sort of unimodal functions. Everything can have multiple local minima. We also assume that the objective function is expensive to compute, so that you cannot afford. Compute so that you cannot afford to do thousands of evaluations. You're trying to be very efficient in terms of each evaluation is giving you a lot of information about how to move ahead in terms of the search. And we don't have any, fourth, we don't have any derivative information. We don't know how many local minima there are, nor is there an analytical expression for the objective function. So in the In the multi-objective world, they write things this way. This is sort of their notation now. You're trying to minimize a function, but you're trying to actually minimize many functions. And so there's some competition between which one you're minimizing for. So you're trying to look at that trade-off between minimizing objective two versus objective one versus objective K, for example. And we have For example. And we have our decision vector is d-dimensional, and script D denotes the decision space. And it's a bounded space. And this is the vector of real-valued objectives. So the objective function, there's K different objectives here. So it's a k objective multi-objective problem. So in the context of computationally expensive multi-objective Expensive multi-optimization. Each subjective function is assumed to be black box. So we don't have convexity or derivatives and it's computationally expensive. So hence it's hard to make very many evaluations. So you're probably used to the idea in single objective, multi-objective problems with just two objectives. Just two objectives that you might have, you have both a decision space and an objective space. So here's your decision variables, for example, that you might have many, many points in this area. And then when you map them into the objective space, say you have two objectives, you would get something that looks like this, covers a space in here. But what you're looking for is what's called the Pareto front, assuming you're trying to minimize both objective one and objective two, then you're trying to find this line. Then you're trying to find this line here, which is called the Pareto optimal. And that means all the points on that line are non-dominated. In other words, it's not necessarily, there's no other point that is better than this point in all the objectives. And so that's the best you can do with multi-objective is to reduce all alternatives down into this Pareto line. So that's the goal. And we'll use, because it's an We use because it's an expensive function, we're using a surrogate. So that's sort of like derivative-free, but it's but in the global world, we tend to talk about surrogates more because we're really not even trying to mimic the derivatives. We're trying to mimic the original function. And so the surrogate model is also called the response surface model, and we use a ray. And we use a radial basis function approach. This is a deterministic model, and we use a cubic kernel. This is the expression for the radial basis function. And so the idea is that you take the points that you have and then you fit them in this model to get the best parameters on that radial basis function to fit the function. function to fit the function. So here is, for example, on the yellow line here is the ACLI function. So that's the objective function in this example, the simple example. And this shows if you just used eight points on that ACLI function, what the surrogate would look like. It would be this red line. If you use 100 points, the surrogate would fit as it's shown here. It's not a perfect fit, but it's still getting. It's not a perfect fit, but it's still getting the wiggles and so forth. So, the more points you have, the better fit that you have. But even this curve here with very few points, note it's moving you in the right direction. It's moving you down here. So if you start searching in here, you're eventually going to find a good point. Now, we use reference vectors in this paper. This is a new thing that we've done within this many objective problem. Any objective problem. So we proposed several years ago Gormore's, which is a two-objective problem, that used an RBF surrogate and other features to enhance the multi-objective algorithm so it was more efficient. However, GOMORS and other multi-objective algorithms developed later did struggle if the number of objectives to be optimized became large because each objective had its own surrogate. So it's been proposed to use reference vectors. So it's been proposed to use reference vectors to decompose the objective space and to avoid the issues caused by maintaining many surrogates. So in the old case, for each objective, you had a surrogate. If you have eight objectives, then that means you're dealing with eight surrogates. There's a lot of computational time in doing that and some numerical issues. So we're trying to do it a different way using these reference vectors. Now, so what these are is you pick in this in this simple, we're showing it here and just going to plot like two dimensions. So in a simple two-dimensional case, two-objective case, the reference vectors would look like this. And they're equally spaced, as you can see. And what they're going to do is their purpose is to subdivide the points. is to subdivide the points in the space into similar units. Then you, the point assignment, a point X with a transformed objective function, you transform the objective functions by subtracting off their minimum value because you want all the functions to be zero for some value of x. And so after you make that transformation, then you can plot like here are here are Like here are some of the values of F at different points in space. And you're going to take a point that is a point, for example, you're going to try to find the reference vector that is closest to that point. So you're going to associate that point here with this reference vector because it's closest to this, whereas this other one is closer to this point. Is closer to this point over here. So the approach is you're going to take all of the objective values that you have and you're going to assign them to one of these reference vectors. Then you compute the angle of the, so minimizing this function here is angle function regarding the reference vectors to look Regarding the reference vector, is to locate the point whose transformed objective value is near to R, R being the particular reference vector here, and also converges at the origin. So in the example shown here, the objective function is first decomposed by assigning each point to its corresponding reference vector. So that's what that's done. And then among the points assigned to the reference vector, the one. The one that has the minimum value of this index here is assigned to the reference vector. So this, for example, we have several points here, but this vector, this point here, which is noted in orange, becomes the one point that's assigned to that reference vector. And here there's another. And here there's another competition. This is this one wins over these other ones that don't have that are not orange. So you're taking, you have a lot of points in space, you're only using some of them, and you're assigning them to these reference vectors in order to deal with the computational issues. So we did not invent this idea of reference vectors. Two recent papers on this topic. First paper proposed the concept of reference vectors. Proposed the concept of reference vectors. The second paper developed a Krugian-assisted Gaussian process surrogate that uses the same reference vector idea. And so those are previous works that use this reference vector idea. Now we're using the reference vector idea, but our algorithm is better. Our algorithm outperforms this second one here that uses a Krieging surface. And also, this one doesn't use a surrogate. And also, this one doesn't use a surrogate at all. So, you would think that in terms of expensive functions, it's not going to work very well. So, neither of these papers above use the RBF surrogate that we use, nor do they have the same algorithm. So, here's an outline of the overall algorithm. And so, I'll highlight here in certain places what we're trying to do. One is we're selecting the center reference vectors from all the reference vectors. Vectors from all the reference vectors. And so you take all these reference vectors and you identify an active set where each reference vector is assigned to at least one point. And then they're clustered. And then from that, you then are clustering the vectors in this way. Next, you have to compute. Next, you have to compute this angle function. And so we defined that earlier. You're looking at the angle between the point and the reference vector. And so, what we want then, this angle function becomes very important in terms we use that. Very important. In terms we use then to describe the angle function spread over all the x's and reference vectors. We build a surrogate. So this is different from everything else we've done before. We've just been building a response surface on the approximate function value. But now we're building it on this angle function instead. So the radio. So the radio the radio kernel function is R cubes. We use a this is a radio basis function with a cubic kernel and it that kind of kernel has to have a tail because it's not positive definite and they have a polynomial tail for this radio basis function. And the coefficients then in this term can be uniformly determined by or uniquely Determined by, or uniquely determined by solving the matrix equation. So then the next step is we select a center point, Xc from the points reassigned to a particular reference vector. And we're going to generate a group of Q candidates by adding Gaussian perturbations to this center point. So, among the best set of points reassigned to the center reference is determined by picking the one that has the minimum of this angle function. Sorry, my watch just went off and reminded me it was 7:30. So, then the next thing is that each candidate point is generated by Each candidate point is generated by adding a random Gaussian perturbation to the center point. So we take these center values and we generate a lot of random points about them. And then now we send the candidate with the smallest value of this S function away from the evaluated points for evaluation. Points for evaluation. So that's the next point that gets evaluated in this surrogate that we're trying to create. And we have a convergence analysis based on grids. I won't go into, but this is a statement of the theorem. There is an almost sure convergence of. There is an almost sure convergence of recast. The main proof steps are given a representation of Frado optimal objective function vector that the teeth point selected for evaluation and this angle function be non-dominated archive maintained by RECAS after T evaluations. Then there exists a delta such that this will be true and from that the And from that, the result follows. So, in terms of numerical results, we now look at numerical results on several different test problems. So, DTLG6 examines the ability of an algorithm to maintain both convergence and diversity since this Pareto optimal front is a degenerated curve that only covers a small subspace. That only covers a small subspace, even in the high-dimensional objective function space. And so, if you look at now, these curves, each of these graphs here is for a different algorithm. So, this is the well-known NSGA3 here, which deals NSGA3 is specifically designed for many objective problems. This is another. This is another K Riva, another algorithm, and these are all other algorithms. And this is our algorithm here. So look at our algorithm. Here is the best fronts that are found by our algorithm, and the worst fronts are here. So, in other words, So, in other words, our algorithm found a bunch of fronts, and if you plot them all, the best ones would be here, and then the worst ones would be over here. So, and the best solution is 0, 0. So, the best front is actually the point 0, 0. But nobody was able to calculate that. Okay, compare that to this one. This is M O E A. That to this one, this is M-O-E-A-D. And you can see that, oh, this line here is showing you what the true front is. And so you can see here, this one got almost nothing in terms of front. This one, E-M-A-S-O, does have a front. And let's see, now this is the worst front, is the gray. The worst front is the gray, and the best fronts they got were the black points. But it's still a long ways from the true front, which is here, which we did get with our algorithm. Same thing with K Riva. Actually, it's fronts all over the place, but it's still a long ways from the true front. And NSGA3 was the worst of all. Was the worst of all. It was way over here. And in fact, what's noticeable is the worst fronts by our algorithm, which is the gray lines here, were better than the best fronts in NSCH3, which are the black boxes. So, in other words, to summarize, In other words, to summarize, the black boxes are the best that that algorithm could do, and the front is down here. So, having lots of black boxes close to this front means you're more successful. And of each of the different algorithms that is looked at here, the best results obviously come from recast. Now, here's a test suite at DTLZ with domain. LZ with dimensions 10. So, I mean, dimension also has an issue here. The problem, the difficulty of computation goes up greatly with dimension. So, there are different test functions here. So, these are the different test functions along this axis. And these are the across here are the. These are the across here are the different algorithms and the best solution is gray. So you see our algorithm did the best on the DTZ1 and on DTZ2, this other one did better EMESO DT. Um, DTZ3, we did the best over here. Also, on DTZ4, we did the best, although some on some of the problems, some of the one of the other, this other method worked better, and so forth. So, you can see by looking the gray over here shows how many cases that of all of them in which. Of all of them, in which our algorithm was the best algorithm. Then also use the data profile. I think this is a data, yeah, data profile invented by Stephen Wilde, our moderator. And this shows, you're probably familiar with these, but the data profiles are a good way to compare different algorithms when you applied multiple algorithms to multiple problems and you're trying to look at the overall results. Trying to look at the overall results. And the high is best. And this is our algorithm. And it did the best of all the other algorithms. And the worst is NSGA3. And now we go to a dimension of 20 and the same. And the same here it gets more pronounced that our algorithm is doing the best in almost all the cases. And the data profile. So here among 20 trials, the average IDD value obtained by each algorithm after 600 function evaluations. On each problem, the best result is highlighted in gray. So then this refers to the data profile here, which plots the proportion of problems solved. Plots the proportion of problems solved by an algorithm against the number of function evaluations. High number is best, and also the data profile focuses on computationally expensive functions, which is also what our algorithm is looking at. And if you look here, then again, our algorithm has the highest curve here over all the other algorithms. So, in conclusion, So, in conclusion, the new algorithm Regis improves on previous algorithms by combining the use of reference vectors in combination with a new algorithm that employs a radio basis surrogate. One of the benefits of the new approach is that there's no need for the surrogate for a surrogate for each of the many objective problems. In other words, the previous approach when you had multi-objective problems was to be Multi-objective problems was to be building a surrogate for each of the objectives. But if you have many objectives, that gets to be very tedious. Building all these different surrogates takes time and can cause other complications. When this paper is published, the code will be made available in Python. And also, we have a lot of other codes that we're posting and for all sorts of different kinds of Of different kinds of global optimization problems, including noisy functions and germanistic functions and low dimensions, high dimensions, so forth. So if you're interested in learning about our codes, just drop me an email and I'll send you the information. So thank you very much. Thank you very much. And now you can hear us clap. The perfect timing. Thanks. So I'm going to ask that if you have a question, raise your hand and I'll get you to the mic. Questions for Christine? Yves. Yes. I was wondering if you try the comparison and you push it beyond I mean, usually you have here you have three hundred or beyond six hundred function evaluations. And red function evaluations. Does the NHCA free curve, I mean, keep cramping up and just catching up with the others? Are you talking about this? Which can you tell me which plot you're talking about? Well, that one, for example. This one that's on the screen. Okay. Yeah. Yeah. So did you try by going beyond 300 function evaluations? Oh, what would happen if we went beyond 300 function evaluations? Beyond 300 function evaluations? Yes. Yeah, we didn't plot it. The assumption is these are expensive, so that you'd be very happy to get a good solution with 300 evaluations. So that's why we didn't go beyond 300. But you are, well, I can't say it for sure, but our experience in other cases where we've been. In other cases where we've been comparing against NSGA2, is that more we still do better even with lots of evaluations? But our whole focus of using a surrogate is to try to not have to do so many real function evaluations. Because we work, like, for example, I work with a lot of environmental applications. And we have our objective function requires solving partial. function requires solving partial differential equations and sometimes those partial differential equations take four hours for one for one function evaluation is four hours so that's why stopping at 300 is still a lot of a lot of computation i mean we didn't test that particular model in this in this problem but that was why we were looking at fairly low numbers of function evaluations great jeff thank you Great, Jeff. Thank you, Christine. You can respond to that. I had a question, a different question, or a related question. Is it true that if your budget was only 100 function evaluations, you would recommend the EMASO method? It was on this chart and the other charts as well. It appears as if maybe everything was truncated from zero to 100, but on previous slides as well, the EMASO, the green. The green does very good with very, very few evaluations. So, if your budget was extremely expensive, would you recommend that method? It was the same on other previous slides where you went up to like 600 or something. Well, glad for pointing that out. I hadn't even noticed that in my rush to get this talk done. Well, that is pretty impressive, isn't it? And um so yeah, and that's like that's something that should be commented on that it does well. I don't know quite how I know that we of course were assuming we were going to go for 300. So all of our steps, initial steps, were assuming that we could get to 300. If we were only doing 100 steps, our own algorithm would be defined. Our own algorithm would be different because we, in our algorithm, we take into account what's the total number of evaluations we're going to do. And the rate at which we probabilistically change the decision variable to look at different values, that depends upon how many evaluations you have left. And initially, we're sort of doing a lot of exploration. And then, as time goes, so if you're in here in this And here, in this stage, we're doing a lot of exploration with our algorithm. And by the time we get out here, we're being much more cautious. We don't change things very much. It's kind of like, I don't know if you're familiar with probably with simulated annealing, but that's the way simulated annealing is. Initially, it looks all over the place, and then eventually it gets so that it's really very cautious about making new changes. So, to do a fair comparison on 100, we should run both algorithms, assuming that. Run both algorithms, assuming that the final goal is 100. But thanks for pointing that out. That's a good observation. And good that we should explain it. Great. Other questions? Hi, Christine. This is Warren. Jeff actually asked my question, but I did just want to say thanks for taking the time to give this talk from the other side of the world in a very awkward time slot. I do see there's one more question. I do see there's one more question, though, so I'll pass them on. But this is a wonderful time spot because it's actually almost a human hour for me. Whereas most of you guys were talking at two in the morning for me, so it would have been more difficult. So thanks a lot. And I'm really sorry I couldn't be there. I really wanted to be there. But go ahead with your question. Yes. So thank you for your talk. That was really interesting. The question that I asked is: how do you choose the number? Uh, how do you choose the number of reference vectors at the beginning? So, do you have some kind of rules or not? I'm sorry, I'm having a little trouble understanding. How do I determine the number of what? Of reference vectors. Variables? Is that what you're saying? No, reference vectors at the beginning. Do you have some kind of rules or not? How do you initialize the reference vector set? The cardinality of that set. Ah. I guess I don't know. As I confess, this is my student's PhD thesis, and the reference vectors are something that I always find very puzzling. I think they're sort of randomly generated, and then you sort of go through these tests of points and clustering things in that way. So I guess I really don't know the answer to your question. Really, I don't know the answer to your question. Um, and just about the comments on or any suggestions for doing that? He says no, but he has a follow-up question, I think. Yes, just and just okay, even if you in if even if you initialize them randomly, how just do you choose the number of reference vectors? Yeah, so Go ahead. I'm not quite, Stefan. Can you repeat what he? How do I choose the number of what? Yeah, it's a question about the reference vector set. Yeah, just the numbers. So she doesn't have an answer right now. But if I understand correctly, the reference vectors are some kind of a stratification in the output space, right? And so presumably you'd have to be careful with scaling in your different objective functions there as well. Functions there as well. But I suspect that if I'm reading this plot right and piggybacking on the other two questions, it looks like there's about 110 evaluations is when something happens. And 110 is D plus 1 times 10. And so if I had to make a guess, there would be something in the 10-ish or more reference vectors initially. The reference vectors initially, because you want to get some multiple points to map to each reference vector. So that's my back of the envelope backward guess for the answer. Other questions? One more. John, Dennis. Yes, hi, Christine. Have a good time back in Ithaca. The oh, first, let me point out one. Oh, first, let me point out one silly little picky point. You misspelled expensive on the title slide. Well, if that's the worst mistake I made, I'm not too worried. It is, you have to remember, I was the middle of the night for me, right? Okay, the other point, though, that I would like to hear you discuss. Would like to hear you discuss is when you showed the picture of the surrogate, one with just a few points and one with a lot of points. Could we look at that slide? Okay, I'm not sure. It was near the beginning. Yes, and had really little to do with your talk. It was just a matter of you trying to set the stage for us, I think. Okay, the cubic RVF that gets really wiggly. Say something if uh okay, so we did this one, no, this one right there right there. Oh, this one. Oh, okay, yeah. Yeah, so what's the question? Well, this is just a matter of terminology, I think. But if I look at those two and I say which is the best surrogate for optimization, I strongly prefer the one on the left because. Because the minimizer is where it should be. There aren't extraneous minimizers. And if I look on the right, to me, that's a much better approximation, but I wouldn't call it a good surrogate for optimization. So I've been thinking about this during your talk, and I realized one of your major uses of what you call surrogates was for approximation. Approximation to the value of this selection function that you had for reference directions. So I just point out that to me, the one on the left is the better optimization surrogate. The one on the right is a much better approximation. That's picky, but that's all I had to say. Well, that's a very interesting comment. So, a couple of answers to that. One of them is: yes, you're right. If the surrogate is so wiggly, that's some difficulty. And some of the things we've done in some of our work, especially with noisy functions, is to relax the curve, so to reduce the amount of wiggleness in it. Not necessarily down to the point of being as smooth as the function on the left, but at least reducing some. But at least reducing some. Now you lose some accuracy by doing that, but it may get you into the right region. But it's going to do other things that aren't more. But I think for surrogate optimization, you really do need to deal with, you can't just deal with the things on the left because it's going to take you doing that. You could just use a convex optimizer. Use a convex optimizer and get yourself someplace that wouldn't be the right solution, but might be kind of close. In this particular case, you happen to get to the right place, but that's just because this is this particular function. So I think that we do want our radio basis functions to have curvature in them where it's necessary. But at some points, we do relax the curve to smooth it out to make it easier for the optimization to get. Make it easier for the optimization to get into the right region. Does that answer your question? No. But it's. But you realize this picture is made up, right? I mean, our real problems don't look like this. That's not the point, though. The point is what you call it. And to me, if I'm trying to do optimization, the one on the left is much the better surrogate. The left is much the better surrogate. The one on the right, however, is much the better approximation. And I'm just trying to distinguish between those two pieces of terminology. But as I said, it has really nothing to do with the value of your talk or the work that you all have done. And the thing I'll point out is the one on the left is using four times d plus one points, if I calculate correctly, which is closer to the Correctly, which is closer to the budget that's being looked at than the one on the right. So, in fact, I think she is using the one on the left more than the one on the right in the numerical results. Maybe. Any other questions? All right, if not, let's thank Christine.