Afternoon session. We have two talks, and I'm very pleased to announce David Perez-Garcia from Madrid, who's going to give our first lecture, and that will be on matrix product operator algebras. So thanks a lot, Simone, and thanks to all the local organizers, though local now is spread in the world, for this very nice meeting. For this very nice meeting, um, I stood that in the end, it was a pity that we couldn't make it in Banff, uh, but it's true that I mean, I just came from KP and it seems next week is the March meeting, so it was kind of a bit a bit difficult. But I mean, it's nice that we can at least make it online. And so far, I'm enjoying a lot, all the talks. So let me explain you a bit of what we have been done in the last years. So, okay, I don't know if the topic is, I mean, I was having a look at. I mean, I was having a look at more or less the idea of the workshop, and which would be a nice topic. And I think one of the things that we're encouraged to discuss was connection with tensor networks. So that's why I decided to talk about this. And also because it's maybe something I didn't talk much before. So I hope that for most of you or of you, it's somehow new, or at least, I mean, the newest part of it. So I would like to start making a small disclaimer to make the talk a bit more. To make the talk a bit more friendly, theorems are not stated with all the hypotheses. Okay, I mean, so I mean, just to have an idea of what they are about, you can ask me or just check the references for the riborous statements. But I think this is a bit more easy to follow. Okay, so let's go. So that's the outlook of what I plan to do. I will introduce first what matrix product operators are, for those who are maybe not familiar to that. And okay, they look pretty. And okay, they look pretty simple objects. And then, my first, I mean, part of the talk will be to explain some, I mean, by now, all results of us, showing that they are not innocent at all, and indeed that they encode a lot of complexity in a pretty formal way. I will explain. And then I will move to the new research we did, which is this related to algebra or matrix plot operators. And I mean, our motivation. And I mean, our motivation there was twofold. I will focus mainly on the first application, which is understanding the classification of phases for mixed states, phases of matter in one dimension. And the second application that if I have time, I will only sketch a bit will be to go to two dimensions, I mean, with the same technology. So that's the plan. Of course, please interrupt me anytime if questions are. Me anytime questions. Okay, so what are matrix plot operators? Okay, so all we need to define them is just a linear map from C small d tensor C capital D to the same space. And we will see this diagrammatically as a box where A is the label of the linear map. And each one of these legs corresponds to one of the Hilbert spaces of this. One of the Hilbert spaces of these complex finite-dimensional Hilbert spaces. D and D are the dimensions, and the arrows indicate input or output. So, for instance, a linear map has an input, not the input space, at the output space here. So this A will take as input this arrow and this arrow. So the leftmost and the top correspond to the inputs and the bottom and the right to the output. Okay. Okay, so what I will do is just to concatenate. Okay, so what I will do is just to concatenate these objects. And of course, when I put two arrows next to each other, this is simply the tensor product. Okay, as I think it's kind of obvious from here. So essentially what I do is simply I take tensor products of these objects, but then of course this horizontal line, of course, one output gets the input of the next. Gets the input of the next linear map. Okay? And I do this L times, and this would be called a matrix polluted operator. That's it. Okay. Then, since usually, I mean, this Hilbert space associated to capital D is usually called virtual dimension because it's something which is not physical, depends on the context, anyhow. And this is small D. Anyhow, and this small d is the physical part. So, we would like to remove this remaining capital D's here, and we can do it several ways. One can, what is called fixed boundary conditions. So we start with a particular vector as an input, and we compose here with some linear operator on the corresponding Hilbert space. And therefore, this object is a map, a linear map, sorry, from the tensor product of L copies. Tensor product of L copies of C to the V to itself. We can also do predictive boundary condition, which is simply taking the trace in the corresponding space. And okay, I will mostly focus on the predicted boundary condition case. And therefore, I will write in formulas what it is. As I said, this is just a map from the L tensor, I mean, L copies, tensor copies of C to the D, which is, of course, isomorphic. Of C to the D, which is of course isomorphic to C to the D to the L to itself, and this is just a map that has this as an input, gets this as an output with these coefficients, which are simply product of matrices. Because of course, once I fix the input and output, I have just one matrix, and this is just matrix multiplication because it's linear combination of linear operators. Okay, so that's the formula, but I will not use the formula in it. I mean, at least not in the presentation. The presentation. Okay, any questions so far about the object itself? Before going to so that's kind of our main actor for today. So let's okay, it looks like a pretty simple thing, just a linear map concatenated with itself. Okay, so they look innocent, as I said, but they are not. And then I will just comment on a couple of old results I find cool to show that these objects are really complex. And the first is this result I have with David Elkos. Is this result I have with David Elkos, who's now in Delft, but moving to Japan soon, okay, like four years ago. And okay, first of all, these objects can model many things. And in particular, I mean, we'll consider classical channels with memory. I mean, meaning in that case, this A will be just a stochastic map from the, I mean, pair of, I mean, okay, let's say first what it is. Let's say first what it is. So we have like two types of the arrows, I mean two different things. So the small D, so the vertical arrow, input-output, is the channel, I mean it's the noise operator, like the communication channel, optical fiber, whatever I'm using. And I mean, we do not have IID copy, so the channel keeps some memory after its use. And then this memory is, we assume, is kept on some finite image scenario. Is kept on some finite dimensional Hilbert space if it's quantum or just, I mean, D possibilities if it's a number of bits, if it's classical. And this is the other arrow. And then, let's, I mean, if we were in the quantum case, we could assume these are quantum channels, but let's stick to the classical case. And this is simply a stochastic map, okay? From both inputs to both outputs. Okay, so. Okay, so then this matrix product operator, when I initialize the memory in some initial state, is just exactly LUCs of my channel. It's a classical quantum, but it's a classical with a finite memory. The memory in this case is an alphabet of the elements. Okay, good. So, So, in the context of transmission of information, particularly in the context of channels, the important thing one cares about is the capacity, is essentially the optimal number of bits one can send reliably per use of the channel in the scenario. Ramis, please tell me. Yeah, sorry, a very quick question. And I think you said this. It's just the afternoon. I might have missed it. By memory, you mean capture? By memory, you mean cap capital D? Yes, so it's the virtual index you're calling memory. Exactly. We're calling the same calling the memory. Exactly. Exactly. That's it. We're already interrupting. So you're viewing A in this case as the channel. Sorry? In what you're doing now, you're viewing A as the channel? Yes. Yes, on kind of memory and input and output in the new memory state. Exactly. But for that, it has to have the properties of a stochastic map, right? Exactly, exactly, exactly. I'm assuming that. Yes, I'm taking A and a stochastic map. Exactly, exactly. Exactly. So it's a particular case in this case. So sort of only particular A is the new which model? Exactly, exactly. Exactly. So, one is interested in exactly understanding how which is the optimal rate at which you can send information reliably, a rate meaning number of bits per use of the channel in the limit in which the error of the transmission goes to zero. So, it's the usual way channel already defined the capacity, but in this case with memory channels. So, I mean, as I mean, it happened with the usual memoryless case done by Shannon. There are also coding theorems in this case. There are also coding theorems in this case. I mean, the more general one due to Harlem Perdue. But I mean, in the most general case, the capacity does not correspond to the optimized mutual information rate, as is the case for the memoryless channels. But there are particularly nice family of channels with memory for which this is the case. And these channels are called information stable. So again, we will restrict to the most favorable family of channels with memory. Of challenges with memory, so information stable ones. Okay, so even in those cases in which A is stochastic and the corresponding channel is information stable, these matrix operators are pretty complex. And the idea is exactly the following result, that the capacity of the channel cannot be computed or approximated at all. I mean, in the worst case, of course. And this would be proof is that there does not exist any. That there does not exist any computable function that, for any information stable channel with sizes which are not that big, let's say four bits of inputs, one bits of outputs, and six bits of memory, gives an approximation of its capacity with third or less than one fifth. So essentially, and this is computable, I mean it's about the computability, it's not about efficiency. So it's, I mean, at the level of that there is no algorithm at all, no matter how efficient, to compute the capacity. To compute the capacity even to a constant precision. Okay, that's the result. But I would like to emphasize that: the best possible capacity if a channel has one bit of output is one. So the capacity is between zero and one. And in a number which is between zero and one, there is no algorithm whatsoever, no matter how efficient, or how inefficient, that can estimate the value. Estimate the value of the capacity even to precision one-fifth. Okay, so that's that's a pretty strong statement. So, okay, so these objects which seem very kind of innocent are not in the sense exactly that they encode, I mean, the most complex computability problems. Esven, please. Yes, just a quick question. You change a little bit the setting, right? Now you have, because before the input and output had the same dimension. Output had the same dimension, right? So four bits to one bit as an important role. No, no, that's not playing an important role at all. I mean, of course, we could also have four bits of outputs. I mean, this is the optimal one, and the same result is true, because we just use one out of the four, same result works. I mean, this is kind of optimizing the numbers, but of course, we could say four bit of input, four bit of output, the same result is true. Of course. Yes, yes. But it's a very good question. Thanks. Okay. Hi, David. I had a question. Yeah, sure. David, I had a question. Yeah, sure. When you have this memory, each time the memory gets an update, so you get a possibly different classical channel each time from the previous one. Is that a reasonable way of thinking about the memory setting? This is exactly what we are doing, exactly. The memory allows to or helps to allows to select a different channel each time. And that's why we can encode and copy. We can encode uncomputable problems here. Indeed, the, I mean, yes, exactly. This is one way to see that, exactly. So, that for each state of the memory, somehow a different channel is applied. But I mean, it's not really true because, of course, then, I mean, also the memory passes through. So, in a sense, I mean, it's not just that, no, because somehow it's not that the state of the memory, the state of the memory also changes afterwards. And of course, all this is probabilistically. But there is a good way of seeing that. And that's more or less, indeed, the way we prove that is by exactly. Is by exactly relying to this probabilistic finite automata problem. So essentially, I mean, the so I mean, a probabilistic finite automata is simply a situation in which somehow at each time step you have several controls you can apply, and you would like to reach a particular state in your memory in this case, and then somehow the somehow the probability of getting to this output state is kind of the quantity that is undecidable, indeed, with a promise on how close you can reach this state, I mean, which probability you can reach that. And then essentially the idea is, I mean, that if you reach this final state, I mean, suddenly you can open, I mean, if you reach this state in the memory, this open. This state in the memory opens a perfect channel, and in the end, the capacity is very good. But if you don't reach that state, then you have a channel which is pure noise. So, in the end, the best strategy you can do is to try to somehow make the input of the channel control which is happening in the memory to try to reach this final state that suddenly makes you have perfect capacity. And that's more or less the philosophy. The philosophy. And then, somehow, the key thing is a very nice amplification of gap technique in the context of probabilistic finite automata due to these two gentlemen here. So that's more or less the philosophy. So in a sense, exactly what you say is what we are trying to do. I mean, in a sense, the input on one of the legs, let's say, for instance, now in the input, in the channel leg, controls what is happening in the memory one, and the same, the input in the memory one. And the same, the input in the memory one controls which kind of channel one when it's applying. I mean, and for this, you need kind of two different types of bits of input here. One is the real one you are transmitting information, and the other is that you use to control somehow what is happening in the memory. And that's how the construction works. So, in the end, essentially, the philosophy is what you just said, what we are trying to exploit there. Okay, so but again, this is just to show that these objects are not. To show that these objects are not really easy. And another result also that connects this, I mean, that uses this undecidability inherent to these objects, and this connects now with the, I mean, at least the object connects with the talk here, is, okay, now these matrix operators, we can ask them to be mixed states, in the sense that we can ask them to be semi-definite positive and trace one. Okay? So if Okay, so if because of course they are operators on some L D-dimensional Hilbert spaces or a tensor product, and then when this happens, we call the Matis product density operator. So we put this density here to make it clear that that's the problem. Okay, and these objects appear, I mean, in numerical simulation of 1D open quantum systems and in numerical simulation of 2D closed quantum systems. One is done by Systems, one is done by tensor networks. So that's kind of an obviously that appears very naturally, at least in numerics. And the problem that one faces, indeed, trying to optimize in these objects numerically is that it's not easy to impose on this kind of linear map A that would have a state. Okay? Imposing trace equal one is easy. That's very easy. But imposing positivity, I mean, that is a semi-definite. I mean, that is a semi-definite positive operator, is hard. And the way it's done in practice is by imposing a local purification. So that guarantees that in the end, this A is some operator and the same operator transpose conjugate. And of course, then it's positive. By definition. Yes. I have a question. So there's two spaces at the input, two spaces at the output. Where is the positive semi-definiteness on the joint space or on the one? On the joint space or on the one of the spaces or? No, no, here, here, this is all these legs are inputs, and each one of these is C to the D, a small d, and this is C to the D tensor with itself L times, and the output is the same space, of course. So the other thing, what was before the memory, now is just something virtual, it's just part of the construction, but doesn't appear here, that's an operator. And in this tensor product of, I mean, this heatware space made of tensor products. Heat vertical space made of tensor pods is in which we just ask is to be semi-definite positive. Okay, good. So, any other question? Okay, good. So, okay, so I mean, a problem that goes back to the kind of seminal paper of finance in Natural Emperor of Final Corrective States is whether if this is always possible or not. So, is this the only way? Is this the only way to ensure positivity? Okay, and again, the answer is no, at least if one requires this, I mean, local purification B to work for any system size L. Okay. The argument is nice because it uses somehow undecidability as a tool and is the following. So, I mean, checking positivity for all L is undecidable. Okay. And that's again a consequence of this amplification. Sequence of this amplification technique I commented before of Jimbert and Oadach. This was realized first in this paper by Clis and company, but not using this result. I mean, they use something a bit weaker, but I mean, and the stability works equally. And then the second ingredient is what is called nowadays the fundamental theorem of matrix political states. And this result shows that if such B exists, That if such B existed, second positivity would be decidable. And because it's decidable, it cannot exist. Okay, so it's not a constructive proof. It doesn't give a particular A for which B doesn't exist. And as far as I know, such an A is not explicitly constructed anywhere, but it shows an existence proof. I mean, in this kind of way, it's pretty similar to this recent conterexample to the colon embedding problem. That somehow it is shown, the existence is shown by proving that if something Proving that if something were true, something undecidable would be decidable. So it's maybe the same philosophy. And this kind of fundamental theorem of matrix quote states, again, go back to Fernanda-Terali and Bernard in the 90s. Okay, the strongest version, that's the one we need here, is this one here. And essentially, it says that two matrix protein states, so two matrix plot operators, sorry, and now these colors means that they are different linear maps. I mean, one, the linear map green and the linear map orange. Green and the Linux orange are equal for all system sizes if and only if I mean they are related by some unitary, unitary inverse relation. Of course, the implication from below to above is trivial. What is not trivial is from above to below. And okay, I mean, there are many subtertities here. The crucial thing is that in order for this equivalence to be true, one needs to check equality above only for. To check equality above only for system sizes which are a computable function on the virtual dimension capital D. And it's okay, polynomial is enough. And that makes this a decidable result to check if two mattis both operators are the same object in the end. So that's more or less the idea. Okay, so this about so basically the statement can be phrased that if and only if Statement can be phrased that if and only if they're related by gauge exactly transformation, is that the same? Use this language before in previous papers. So this is the gauge condition. Exactly. So this shows that exactly. Again, I mean, there are subtleties in many things in particular in this theorem, as I in my disclaimer. So there's still some phases going on, some matrix that confuses with everything going on. It's not exactly this, but I mean, this is the philosophy. Yeah, that's that's the idea. Okay, so okay, so let's go now to the kind of main part of the talk, which is kind of the recent results, which is the result in preparation that should be out maybe next week, hopefully. But somehow it builds on two previous results that are those here. And it's about the classification of phases for mixed states, so understanding. Mixed states. So, understanding, I mean, exactly which are the different properties that one-dimensional mixed states can have that are different at large scale, so like topological type of effects. So, okay, so in order to, okay, one needs to start defining what is a phase in the context of mixed states. And okay, that's that's a very, I mean, I mean, it's not a, I would say. It's not a, I would say, a clear, or there is not a clear best way to do that. But I mean, the way we see this is the following. So we would like to capture somehow how complex a mixed state is. First of all, we need this, a mixed state should be indeed a family of mixed states defined for all system sizes. So, I mean, we are talking about a thermodynamic limit. Ah, okay, Ramis is asking. Ah, okay. Ramis is asking about if instead of positivities, complete positivity. I'm almost sure yes. But okay, I'm almost sure yes, because this result, I mean, this amplification technique is very, very powerful. Essentially, you can combine it with any property. But I'm not, I mean, I would guess yes, but I cannot tell you, I mean, whether the same will work. So we work okay. So essentially, sorry, so again, I will be a bit informal here, but somehow when I say a mixed state row one, what I mean is there is a nice, natural way to define this in the thermal mid limit, meaning for all possible system sizes, okay? Not just at the thermal limit, but even more for all possible system sizes. And there are several ways in which one can do that. For instance, be in the gift state of a local Hamiltonian. State of a local Hamiltonian or a metallation invariant or many other ways. Okay, so, but okay, we need somehow that there is a family which scales with, I mean, that is well defined for different system sizes. And then somehow we say that the state is more complex than another one if we can obtain the second from the first somehow fast. And one can again go for some more like a continuous version, essentially that there is a local. Essentially, that there is a local impladian that evolving on row one reaches something very close to row two in short time, say constant time. Or, the alley probability bounds essential is equivalent, if there is a sorted circuit of quantum channels that starting on row one gets close to row two. Of course, one has to make this rigorous talking about. uh talking about what this almost i mean this what this epsilon means and how sure what what short depth means instead of i mean as a function of epsilon and so on and so forth but essentially the idea is that uh the depth of the circuits should scale poly logarithmically with the system size and with the base of the error of something like that okay but again the the details are in the papers uh for this talk it's not that relevant okay i will explain now why i mean because Okay, I will explain now why. I mean, because in the end, we will work with the exact case in the end. So, the nice thing about this definition is that, as opposed to the pure case that the being in a phase is an equivalence relation, because of course the transformations are unitary, this is an order relation. So, there are, for instance, the trivial phase, and this is the phase where all product states live, is less complex than any other phase. Complex than any other phase. Because what I can do is just I plug a channel that destroys everything and constructs my product state in any place. And this is depth one. And no matter in which place I start, I construct a product state. So essentially, however, of course, channels of evolution by the implements are not reversible. So for instance, if I would like to construct the toric code starting from a protein state, this takes time. Starting from a protest state, this takes time. And it's known that even within bladions, it takes time, which is order the system size to be constructed. Okay, so destroying, I mean, topological correlations, global correlations is very easy, but constructing them is not. And that's why this is another relation. And then, of course, what we would like to understand is exactly which are the different phases. Within a phase, of course, are things that are related in both ways, with both arrows. With both arrows, and which phases are extremely more complex than another one. So, I mean, essentially, I have this sort of diagram, and that's more or less what I would like to understand. Okay, that's the goal. Okay, so it's a nice goal, but let's start easy. And in this talk, I mean, I will just go to the most simple possible case. Okay, so we'd like to understand which are the different phases, which are the invariants that characterize them, etc. So, let's Etc. So let's start in the one-dimensional case and in the gap case. And I will explain what this means. Okay, one-dimensional, there is nothing to explain, gap, maybe. Okay, so it's well known by now, and okay, people like Yusven and others have contributed here, that in the case of pure states, there is only one phase. Of course, there are many phases if one imposes symmetries, but I don't want to impose symmetries. I just see the pure topology. Posymetricity, it's just in the pure topological process. Okay, uh, question is: Is the same true for mixed states? Uh, and okay, a priority, I would say why not. No, I mean, mixed states are even worse than pure state because they have some sort of noise behind. But on the other hand, I mean, there are these holographic correspondences between two-dimensional systems in the bulk and one-dimensional systems on the boundary, which are mixed. Systems on the boundary, which are mixed states. So, in a sense, 1D mixed states is something between 1D pure and 2D topological. So, I would like to understand what is happening, essentially. Okay, so again, as in the pure case, it's easier to start with particular representatives of each phase, which are randomization fixed points. Since we're interested again in topological properties, pure global properties, because any Pure global properties, because any local property can be changed by some local Hamiltonian or channel. Essentially, what we would like to do is: okay, let's start blocking. And until we reach some sort of fixed point in this blocking procedure, blocking shouldn't change the phase. So in the end, I mean, this is of course something very, I mean, not rigorous at all, in each phase there should be some point which is invariant under this blocking non-mization process. This blocking the normalization procedure. And let's start analyzing those objects. Okay, whatever they are. And now let's try to understand what they are. Moreover, we would like to understand gap cases. And gap is not clear what it means here, but for us, it will mean an area law for the mutual information. So, somehow, exactly. So, we would like to understand boundary theories of two-dimensional topological models. Two-dimensional topological models, which are non-tidal, and therefore, somehow these objects fulfill an RA law for the mutual information. And because they fulfill an alia law for the mutal information, they are well approximated by matrix plot operators. So, in the end, I mean, the object we would like to study, I mean, that somehow captures, hopefully, these stars, are matrix flow density operators, which are also randomization fixed points. Okay, that's and that's. Okay, that's and that's somehow the object we'd like to understand first, um, because we hope that somehow they capture all the complexity of this 1D gap mix case. Okay, that's that's okay, that's not what I say. The argument, of course, there's nothing regular so far on that, but this is, I mean, justifies why to look at this particular family of objects. Okay, and now we start proving the group of things. Okay, so now we've Proving the group of things. Okay, so now we start with those objects and we'll try first to characterize those objects in an operational way. So if something is under merging fixed point, one can understand it as the limit of sub-blocking procedures. So essentially, it should have no type of length left in the system because if it has some sort of correlation length or any length, by blocking, this decreases, no? I mean, in the end, we. This decreases, no? I mean, in the end, it will be like zero. So, we'll consider two types of lengths. So, the first one, so we'll define a randomization fixed, an MPDO being a randomization fixed point if all correlation functions are independent of the distance between the observables. Which makes sense if I'm allowed to block that this happens. And okay, the second type of absence of length is Is the saturation of the area loop for the neutral information? Okay, and let's try to explain exactly what this means and why this is expected. So, if I have a one-dimensional system, in particular, a matrix pull density operator, and I cut the system into parts, and I assume always that L is less than half of the system, so n is bigger than L. I consider the mutant information between the two halves, and it's known, and this is a consequence of a structure. And it's known, and this is a consequence of strong supervisivity, that every matrix proposition operator fulfills that this quantity, this mutant information, increases with L. Okay. On the other hand, because we have an area law, a matrix podicity operator fulfills an area law, this mutal information is upper bounded. So if I keep blocking, I have a sequence, an increasing sequence, which is upper bounded, so it should converge. And therefore, I mean, in this limit. And therefore, I mean, in this limit of blocking, one should expect something like that exactly the area law saturates, that the mutual information of this cat is the same as the mutual information of the next cat. And these are kind of the only two operational properties we will impose in our matrix potential operator to call it a randomization fixed point. And as I said, it's expected to be the case for these objects. For these objects. Okay, good. So the first theorem we have is that indeed. David, a quick question. Sure. This is the case for periodic boundary conditions. Yes, yes, I'm always, yes, I am always considering traditional invariance and peri-boundary conditions. Yes. Thanks. Jen, do you have a question? Yes. So is it correct in the case of time? Is it correct in the case of type one to think about this as some sort of mean field limit? It's a very good question. Indeed, not really. I mean, because you could have, I mean, the objects could be very complex, could have a lot of global correlations. But somehow, if you trace out part of the system, somehow, you get a tensor-proof structure. A tensor-produced structure, but I mean, but it doesn't imply that globally they can have a lot of, I mean, I mean, some global entanglement, global, and that's the whole point. Indeed, I mean, in a sense, it looks like it would be like mid-field, but no, I mean, there are very complex objects fulfilling these two types of conditions. It's not really. Sorry, Amy to interrupt that your discussion. Isn't that Your discussion isn't that rather kind of a system with criticality? Uh, yeah, again, yes, and no. I mean, one expects that if there is some sort of criticality, the mutual information should grow type one, kind of, if, you know. Okay, yeah, yeah, yeah, yes, yes. Okay, length scale diverges. Yes, but I must excuse me. On top of that, I'm assuming already that they have an available. Of that, I'm assuming already that I have an area load because I have type two. Yes, so in this way, I'm killing these critical objects. I see. So, again, once one, yes, please, please, okay. So, I mean, the question is, did you have a model system maybe where these things are satisfied? Of course, of course, many indeed. Yes, yes, yes. For example, the bound, I mean, okay, the boundary theory of any two-dimensional topological model, I mean, say, Kitaf, Tori Kode. Secita F, Tori Code, Quantum Double Models, StreamNets, all that fulfill these two properties and they are highly non-trivial. I mean, because they encode the whole anionic theory of the two-dimensional model. Right. I must be very confused about something because if you say it's a I have to think of a gapped system, then if I have a gapped system, I would expect the correlations to decay exponentially. And so you must not be independent of the No, he cannot be independent of the distance. So, yeah, but okay, but yes, yeah, there are two, yeah. Good point. Uh, so, of course, we are in the limit, so here indeed is zero correlation, not even decaying at all. I mean, zero correlation is exactly zero, but it can be also constant because I can have something like a GSC type state in which the, I mean, the, the, the, I mean, there are long-range correlations, but they are arbitrary distance correlations. Uh, and that's totally okay, also. Okay, so if Okay. So either is equal to zero or is constant. I mean, really, at any distance. Okay. Right. And then both, for instance, the GSCS state also fulfills take one and take two. Okay, so. Okay, so, but it's a very good point. I mean, because if one looks at this, one says, okay, but this is just produce states. No, come on. No, no, but no, no, not at all. Okay. Okay. Okay, indeed, I mean, our conjecture, because of my previous argument, is that these objects really are kind of models for all possible phases of matter in one Demix states. Sorry, with an area law, I mean, on top. Okay, so non-critical models. Good. So, the first result we have is that indeed these two types of Of, let's say, absence of correlations, is indeed equivalent to indeed the existence of a flow for which the object is an RG fixed point? Because so far we have defined RG fixed points by properties that are expected because we are taking some limit of blocking procedures, but there is no flow at all here. And then the first theorem is that indeed, if these two conditions are fulfilled, this is equivalent to indeed the existence of a flow for which the object is energy. Flow for which the object is energy fixed point. And indeed, okay, I mean, you see the theorem there, no? And Mattis Potenti operator is a dimension of fixed point if and only if there exist two quantum channels, T and S, that, okay, they do the fine-grained and coarse-grained operators. I mean, T duplicates the number of tensors and S produces the number of tensors by one. So that's that's okay. So that essentially shows that exactly they are indeed remission fixed points. For some part. It from isotries points for some particular flow. Okay, so that's now the first result that I mean is nice because connects, I mean, justifies also the name dormization of this point from a clear definition. And now, yes, please, Simone. Is that theorem constructive? Yes, yes, it is. Yes, yes. Okay. So another, the, the most So now the most surprising theorem for us is the following. So, okay, such T and S exist, so we have a remission of fixed point, if and only if, if now, I mean, look, I mean, it's important because we are changing direction. So, my matrix plot operator, I mean, is lives in, I mean, it's an horizontal object, meaning that the inputs are the vertical lines. But of course, I mean, it's a square object. So now I can switch what is physical and which is. What is physical and which is virtual, meaning that I can consider another matrix both operator in which now the physical degrees of freedom are traced out and the virtuals are left open. So if I consider these objects now, they form a finite dimensional algebra. Okay, and with the dimension independent of n and some other extra properties that I would not write here. So somehow having an artificial point immediately implies the existence of an Immediately implies the existence of an algebra of matisp operators which is finite image. Okay, um, and that's how it's cool because okay, okay, this somehow justifies the title of my talk. I mean, why to study the matrix product operator as it's exactly the thing we need to understand to understand which objects appear here, like artifacts points. Okay, that's that's the Like artifacts points. That's there. But a nice thing is somehow that mattis-point operator algebras were first considered by the group of Frank, Firstette, as a way to recover with tensor networks alternative description of all possible string net models, which are supposed to cover all possible two-dimensional non-Kidal topical phases. So it's this. So in a sense, it's nice because how it's So, in a sense, it's nice because somehow it's the same object that appeared. And of course, there is a reason for that. And as I said, this is kind of holographic correspondence. One expects that in two dimensions, all the properties of the bulk of a two-dimensional topical module are somehow reflected in the boundary. And the boundary is a one-dimensional mixed state. And of course, if I consider a randomization of fixed point at the two-dimensional level, okay, the boundary should be also a randomization fixed point. And therefore, the same algebraic structure should appear, and indeed it appears. Structure should appear, and indeed it appears. That's what we showed. Okay, so that's the idea. Okay, so I will present briefly our view of matrix quote operator algebras, which is very similar to the one of Frank, but a bit different, but it means. So essentially, what we have is, I mean, a matrix quote operator. Sorry, I switched the arrows now from bottom to top. It doesn't matter. And okay, this B of X is just some matrix here indexed by some X. In the end, By some x. In the end, these objects will be representation of particular types of algebras, particular free by algebras. So these objects x will be the elements on this kind of algebras. And okay, B is just some sort of representation. Okay, that's the idea. Okay, so an algebra of matrix operators is simply something which fulfills the following property that for all L, For all L, if I multiply, I mean, right-hand side, two of these objects with labels x and y, I obtain exactly the same object with label x times y. Of course, for that, I'm assuming that in these labels, these labels already have an algebraic structure. It's an if and only if, but I mean, I don't want to get into the details now, okay? So, okay, so the important thing here, of course, is that I can multiply. Here, of course, is that I can multiply. So these objects are an algebra, but I can also add another tensor. I have L black tensors, I can add another one, L plus one. And of course, this is a co-product. So on top of the multiplication, I have a co-multiplication, which is adding an extra tensor. And in this sense, these algebra of Mattis Poe operator are usually, I mean, are naturally endowed with the structure of a pre-by algebra. I mean, the structure of a pre-by algebra. So there is a product and there is a co-product. And okay, they have nice properties. Okay. And moreover, a nice thing is that there is a duality between an algebra and the dual algebra. And in this case, it's very natural because you're simply interchanging vertical and horizontal edges. That's very, I mean, the duality that usually emerges in this biogebra is very natural here. Is very natural here. And it's, of course, related to what we did before, that we interchange vertical analysis. Okay, so our we have one theorem. Okay, a bit more, but I mean, this is one of our main theorems. So if these pre-bi-algebras correspond to Hof algebras, then the mattes potentially operator are in the trivial phase. So they can be constructed by death to circuits of quantum channels acting on a prototype state, say the maximally mixed state. A broader state, say the maximally mixed state. Okay, so this is the first theorem we have, so which is in the line of what happens in the pure case. Okay, in order to understand it, okay, I will not explain in detail what the Hof algebra is, but I mean, maybe the best, those we know that you know, probably some of you know, I mean, you know, and for those who maybe didn't hear of that before, I mean, if I put all the list of properties, I mean, it would mean nothing. Of properties, I mean, it will mean nothing. So, essentially, the best thing is to think that of algebras are a natural generalization of groups. In order to generalize them, one needs to work with the group algebra. So, if I have a finite group, then okay, in the in we'll restrict to finite dimensional hof algebras, okay, indeed to semi-simple hof algebras. Okay, so um, so if I have a finite group, I can define the group algebra, which is simply a linear combination. Which is simply just a linear combination of group elements. And okay, this is an algebra. So there is a product, which is just simply how to extend by linearity the group multiplication. There is an identity which corresponds to the neutral element in the group. But there is also a co-product, which in this case means mapping one group element to the tensor put of the group element with itself. Okay, again, extended by linearity. Again, extended by linearity, and there is a co-unit, okay, which is kind of the analog for the co-product as the unit is for the product. And there is also an object, which is the antipode, which for the group is the inverse operator. And okay, this, if one works in the group case, these objects will fill many compatibility conditions. So now a cofalge is an abstraction of those things. So you have all these prop, I mean, all these objects, product, an identity, an a co-product, a co-unit, an antipode. And a co-product, a co-unit, an antipode, which behave well with each other. For instance, the co-product is an homomorphism, etc. That also fulfills many compatibility conditions. And the next thing is that being a hof algebra is a self-dual notion. Somehow is the way to generate, to put in the same footing a group and the representations of the group. And that's kind of why it's a nice structure. Okay, so. So, if we weaken these compatibility conditions, we get a weak hof algebra. And we hof algebras are relevant because somehow they are the algebraic object behind representation, I mean, effusion categories, which is somehow the mathematical object to understand anions. And indeed, I mean, it was proven already, I mean, early 2000, exactly that, the representation theory of Wichof algebra is exactly the set of Fusion categories. Fusion categories. So, in the end, okay, our second theorem is shown that indeed fine-dimensional Louis-Hoff algebras also give rise to randomization of fixed point matrix potential operators. And I answered this question of Simone. I mean, that indeed for all these very complex objects whose representation theory already includes all anionic theories, all of these objects are examples of Romanian fixed point matrix points in terms. Okay, so these are already very non-trivial examples. Be very non-trivial examples. Okay. Okay, good. Okay, that's more or less what we know. And now, let me just finish what we what we okay. No, sorry, yeah, just one. Okay, it's very nice, maybe, okay, the proof is technical because hof algebras are technical. That's it. I mean, but the idea is very simple. And indeed, making it work for the group case is very simple. But we need to, yeah, the hof algebra. But we need to, yeah, the whole algebra case CTN is very technical. So, we want to construct, I mean, let's try to illustrate it with the simplest possible group algebra, which is the group algebra of C2. We have two elements. So, the mixed state, this normation fixed point mixed state associated to this group algebra is just this object. Okay, it's just identity tensor n plus sigma c tensor n. A C tensor N property normalized. Okay, and this is exactly the boundary theory of the toric code. So, how we construct this with a dev two circuit of quantum charge? Exactly. Okay, there are two steps. The first step, which is the blue, we just construct many copies of row two, essentially, of the smallest possible size. A dime size for which this mixed state makes sense. And that's what we, I mean, this is, of course, trivial because simply the way to do it is to trace what you have and construct this state. And the non-trivial part is what is called the gluing map. And the gluing map, what does is it glues together. So if I have, I mean, two row twos, I mean, it glues this in a row four. If I have a row four and a row five, it glues them into a row nine. It glues them into a row nine. So essentially, it's able to glue all these things together. And that is the non-trivial part. For the group case, it's trivial, or it's very easy. For example, for the Tori code case, for this C2 case, it's exactly this object, I mean, which is a trace-preserving completely positive map. It's easy to check and easy to check that it does what it does. So it's pretty splitted. For the Hofalgebar case, it's also splitted, but I mean, the proof is pretty technical. But the intuition is the same. And then. And then I will finish, because there's probably time to, about okay, for half algebras, everything is in the trigger phase. Is there anything which is not in the trigger phase? If yes, it'd be cool, because somehow it would show that in one dimension, there could still be things which are topologically non-trivial without any symmetry protection at all. And then we have a conjecture for that. A conjecture for that, and if we take the Wickhoff algebra associated to the Fibonacci function category, our conjecture is that, and we take the corresponding normalization fixed point matrix potentially operator, our conjecture is that this is not in the trivial phase. And we have some evidence for that, which is that we can prove that for such example, there is no gluing map possible at all. I mean, the gluing map cannot exist simply, which somehow shows that at least the same argument will not work. Work. And we have some other evidence that indicates somehow that somehow this is not in the trivial phase. But it's not a proof yet. We hope, okay, we have some ideas how to finish the proof, but so far we didn't manage to. And of course, happy to chat to anyone interested in the conjecture to give you more details. But I think this is maybe the best way to finish. Way to finish. I mean, just with this conjecture. So that's it. Thank you. Thank you very much for this fascinating talk. Are there questions, more questions from the audience? Martin. I just want to. I'm a little bit confused. I'm a little bit confused of like if I have if I have something 2D topological, I don't know, Tori code, anything else, then you know there is this associated modular tensor category that that is conjectured to classify it. And I am somewhat confused. So what part of it survives when I go to the boundary? Yes, yes. Clearly, not all of it, because otherwise it would not. Yes, sorry, of course, I was not informal there. So, I mean, what I would. Informal then. So, I mean, what I was saying is the following. So, if all these models, which are normalization of fixed points, the ground states are exactly exact tensor networks, exactly PEPS, breakthrough and target per states. And now for a PEPS or for a tensor network, there is a clear definition of what is the boundary state. And essentially, what you do is you have a PEPS or a tensor network, you have kind of the physical degrees of freedom, the virtual degrees of freedom. The virtual degrees of freedom, and what you can do is just do kind of a transfer operator thing. So you sandwich the physical degrees of freedom with the conjugate so that only the virtual degrees of freedom live in the boundary. And the object that remains there in the virtual degrees of freedom is the boundary state, I call it. And then, in the context of tensor networks, it's known that there is some dictionary between properties of this boundary state and properties of the bulk. For instance, we prove. Properties of the bulk. For instance, we proved, and I think I gave some, I mean, at least people like Esven or others have listened to this talk before. We can prove that if this boundary state is sufficiently local, then this is the same as saying that the Hamiltonian bulk is gapped and these type of things. So it's in this olographic correspondence with intensive networks, in which I'm talking about boundary theories, exactly. And because I'm considering states in the park. Considering states in the bulk, which are remotely fixed points, the boundary theory is exactly a matrix ball density operator. There is no approximation need or anything like that. And that's what I'm that's exactly what I meant. No, that's why I understand, but I'm confused. Like if I take a story code, it has a non-trivial modular tensor category that's associated to it. Yes. But do you tell me that the boundary state doesn't have any non-trivial topology to it? No, exactly. No, because it's a very good question. No, no, of course. Yeah, it's a very good question. No, no, no, no, no, no, no. No, exactly. Because exactly, that's the question. Because, of course, the notion of what is a phase is different. That's exactly the. So here we define phases, but something which is purely one-dimensional, which is this, I mean, having a local impladient that evolves from one state to another one, or a finite episode of quantum channels. This is not the natural notion that emerges in the boundary when you consider when you. The boundary when you consider when you consider the two-dimensional topical order in the park, even if the object is the same, the notion of phase is different. Wait, now I think I am confused. Yeah. So can we be specific about Tikitayev's story codes? Yes. So So, what I mean is that the fact that I can construct the boundary state associated to the Tori code with a depth to quantum circuit doesn't imply at all that I can construct the bulk with a finite depth quantum circuit of unitaries or with channels. I mean, it doesn't imply it at all. I mean, I can construct the boundary only, but not the bulk. Even if there are. Even if there are some dictionary between properties of the boundary and the bulk, the fact that I have a way to construct the boundary fast, let's say, with a depth to sit with, doesn't imply that I can construct the bulk fast at all. I mean, there's no, this implication is simply not true. I mean. And there's just one boundary state. It's simply to show that, I mean, this back-to-boundary shows that there are many states which are. States, which are mixed states, that encode, I mean, in the algebraic structure, the anionic theory. But somehow, for a bulk system, because these anions indeed create long-range correlations, this implies that you cannot create these objects fast. But somehow, even if these objects are encoded there in an algebra-like way, this doesn't imply that, I mean, if you see this as a mixed state. That I mean, if you see this as a mixed state, this puts a wall in the speed at which you can construct this object. And indeed, this is the whole point to understand what happens. But what is clear is that if there are non-trivial cases, that is not clear as yet, the invariance has nothing to do with the anions. I mean, they are totally different invariants, which Which we have no clue. Because, of course, there are half algebras which have a pretty complicated an unitary structure, and all of them are in the T V. I see, thank you. I think that was my question. What is the connection of the fusion category of the big veil algebra to the modular tensor category of the bow? But you say that basically none. None. None, known, exactly. That's the whole point. But the object itself is the same. Indeed, I mean, if you. Is the same indeed? I mean, if you allow me here, so this is my matrix fluid operator. If I concatenate this in this way, and this is a two-dim two-dimensional model, now the legs inside at the physical degrees of freedom, these are all possible stringets. I mean, all possible topical orders. So, I mean, in the same object, with the same object, you can construct all these two-dimensional topological models also by concatenating them in some particular way. So, all the information. In some particular way, so all the information is there. It's simply, of course, that which is the natural definition is different for both words, but somehow that's that's but the object encodes algebraically all the complexity. But of course, the whole point is exactly that, to understand, yeah, exactly. What is trivial and what is not trivial, if we see them as one-dimensional objects, not as, I mean, the two-dimensional objects, they generate with a different construction, and that is not clear at all. Clear at all, that's all, yeah. Yeah, that makes sense. Thank you. Okay, are there more questions? Yeah, I have a question. What about the 1D states, mixed states that are thermal states of 1D systems? Are they all trivial or is there anything known? Yeah, I mean, yes, yes, exactly. So, essentially, if you have a thermal Necessarily, if you have thermal states of local interacting Hamiltonians, they are expected to be trivial, yes. Because I mean, you take having those with a randomized fixed point is the same indeed as having, yeah, I mean, since a trivial operator, yes. So they are expected to be trivial. And if you impose, I mean, and this indeed rigorous, if you impose that you have a randomization fixed point with the form e to the minus beta h. Yes. So they are all trivial, yes. Yes. Any more questions? I don't see this. I don't see anybody. If I'm overlooking you, please pick up. Indeed, I mean, if I may, I mean, yeah. So, okay. The fact that all, yeah, the fact that e to the minus beta h is three. The fact that e to the minus beta h is trivial for h commuting, which is the analog of, I mean, the property of being automation of fixed point. I mean, it's true, but I mean, it's hard work to prove it. I mean, the fact that you can generate this for every temperature, for example, with the Davis generator, is a result that we posted last December, and it's a pretty technical proof that requires using operator spaces and other things. So it's, I mean, in the end, it's true, but it's not easy to prove. Yeah. Yeah, but I'll ask you later for that reference. It sounds interesting. Yes, I mean, I mean, for this particular result, I mean, yes, you can, yeah. I mean, if you Google me, the last paper I put it in, I know, I mean, there are two papers from December, which is clear from the title that we are doing exactly that. I mean, proving that the Debi generator of a commit. Of commuters and between us in one dimension converges very fast because we are able to bound the logs of constant. That's that's but I mean I can give you the reference by email later on. But it's easy to find actually. Thanks. All right. Since I don't see any questions anymore, let us thank David again for this very nice talk. And I suggest. And I suggest that we now take a break of something like ten minutes or eight minutes and reconvene at ten past. 