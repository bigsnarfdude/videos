Thank you very much, everyone, for being here. Today, we are very happy to have Joe Khalil from UT Austin. He did his undergrad and part three in the University of Cambridge, and after that, he did his PhD in Berkeley. And then he did his postdoctoral work in Princeton, and he is now an associate assistant professor at UT Austin. And we're very happy to have him here. And we're very happy to have him here. I'm sure soon he'll be associate as well. Thank you, Joe, for accepting the invitation. And his talk is on structure and point clouds via tensor decompositions. Yes, well, thanks very much for the invitation to speak. I was at Oaxaca once before in person, and it's a really special place. So maybe in the future, I could be there again. So, just at a high level, here's the idea of this talk in one slide. We suppose that we're given a data set, just a finite set of points, x1 up to x capital N, in the Euclidean space Rn. And we'll suppose that the data set has some type of structure. So it's perhaps well modeled by a mixture of a few Gaussians in our end. In Rn, or perhaps the data lies roughly on a union of low-dimensional subspaces. This is as in the generalized principal component analysis. Or for a more scientific setting, perhaps each individual data point XI is a 2D image of, say, a 3D molecule subject. Subjected to a 3D rotation, a tomographic projection, and noise. This is the setting of, say, cryo-electron microscopy. So the data set's a bunch of images. Or fourthly, perhaps each data point is a point track across frames of a video in which there are a few moving but rigid objects. A few moving but rigid objects. Okay, and we'd perhaps like to sort the point tracks according to rigid object. This is the setting of motion segmentation in computer vision. Okay, so we'll try to learn the relevant structure by forming moment tensors of the data. So the dth moment tensor, So, the dth moment tensor, I'll explain this notation in a slide or two. You take each vector xi and tensor it up d times, outer product d times, an average. This here is the moment tensor of the data set. And the goal is the idea is that this knows quite a bit about the hidden structure and will try to pull. Hidden structure, and we'll try to pull that out by computing a suitable low-rank decomposition of the moment tensor. So that's kind of the idea of this line of research, decomposition of moment tensors. So to concretize the talk, I'm going to focus almost Focus almost completely on the computation for a specific type of tensor decomposition, the one that's relevant for Gaussian mixture models. So this is in some sense the most basic tensor decomposition in the list that I had. It's the first one, mixture of Gaussians. And so there, the moment tensors will follow a so-called low-rank C. A so-called low-rank CP decomposition. Okay, so we'll come to that in the next slide. We'll see what the relevant tensor decomposition is. And so I'm basically going to be giving you a numerical method that computes this tensor decomposition. And I'll be providing results on the associated non-convex optimization landscape and telling you how you can actually use. And telling you how you can actually use this tensor decomposition in high dimensions. So, when little n in the last slide is potentially big, and when capital N is potentially big, and even when D, the order here, is potentially big. So, with the talk focused as such, this is mostly going to be, well, this is going to be joint with Well, this is going to be joint with Joao Pereira, who's presently a postdoc here at UT, and Timo Klock at Simula in Norway, and Tammy Kolda of Sandia. Okay, so all of this is joined with Joao, and bits of it are with Timo and Zami. Okay, so this is a 10. Okay, so this is a tensor. For this talk, it's just a higher-order array of numbers, for example, represented, say, by this Rubik's cube, which is showing a 3x3x3 tensor. I'll focus on real tensors, so every entry of the array is a real number, and symmetric ones, meaning that if I permute the indices, I permute the indices, I get the same number in the array. Okay, so I do this because my primary motivation is moment tensors, which are symmetric, just as a covariance matrix is a symmetric matrix. Okay, so the specific decomposition that I'm focusing on in this talk is the CP decomposition, and it's also you maybe. And it's also, you've maybe heard of it as the low-rank decomposition of a tensor. So it's maybe the most classic one. We're trying to write the tensor T as a sum of rank one terms, lambda i A i tensor D. The lambdas are scalars, and the AIs are vectors which get tensored up to the dth. Up to the dth, to the dth order. And just as a reminder, A tensor D, well, it's an N by N by N D times tensor whose I1 up to I D entry is gotten by multiplying the corresponding entries of the vector A by this formula here in the bottom right. Okay, so we're trying to write this tensor T as a sum of rank ones. Okay, so let's right so symmetric tensors are coming up as moments, like as I sketched on the first slide, a moment of a data set, but actually also derivatives of a multivariate function. If I had a function f from Rn to R and recorded all of its partial derivatives, this would of say order D, this would give me a dth order. This would give me an adh order symmetric tensor. They also come up as higher-order adjacency matrices for hypergraphs, the symmetry condition. And so doing this specific CP decomposition of trying to get the scalars lambda i and the vectors Ai has been applied to learn parameters of interest in various models. Models, GMM, which I'll sketch on the next slide, but also others such as hidden Markov models. It's also been related to learning weights in neural networks and in signal processing applications, such as independent component analysis. Okay, so here's Okay, so here's kind of the simplest case of a mixture of Gaussians and why this decomposition comes up when you look at the moments. So suppose we have our unknown means, A1 up to AR, in Euclidean space. And let's suppose we have a mixture of isotropic Gaussians centered at these mean vectors. So x is a random variable. So x is a random variable distributed by according to a kappa plus epsilon, where epsilon is a standard normal and kappa is a discrete random variable taking values between one and r. And the probability kappa's i is lambda i. Okay, so we've got a mixture of r Gaussians, and each Gaussian has a weight of lambda i. Only that, just for simplicity here, I say all the covariances of. Here, I say all the covariances of the Gaussian are the identity. Well, what are the moments? Well, the population moments of x, they have these formulas, right? So the expectation of x is just the weighted average of the AIs with the weights lambda i. All right, this is. All right, this is quite clear. If you take the expectation of A kappa plus epsilon by linearity of expectation, right, it splits as a sum. The expectation of epsilon is zero. And then just conditioning on kappa, you get this. But likewise, with the higher moments of x, so you could look at xx transpose, this n by n matrix, its expectation. matrix, its expectation is actually this. The weighted sum of AI tensor squared weights lambda i with the correction term coming from epsilon, the covariance of epsilon here. And then you can look at say the third moment of x and again you get lambda i a i tensor cubed summed over Tensor cubed summed over i plus some correction term involving lower order moments of x which would come from you know interactions of a kappa and epsilon and likewise with all the other moments the higher moments okay so we see um that the dth moment uh for this mixture of isotropic gaussians it's looking like It's looking like it's got this CP decomposition, this rank one decomposition, with corrections depending on lower moments. Okay, so you see that the CP decomposition is the relevant one for Gaussian mixture models. Right, and this is well known in the literature. And this talk is about actually And this talk is about actually computing the computational problem of getting the CP decomposition. But the question only really makes sense, or well, it makes best sense if the decomposition is unique. So if I provide you with tea here, you know, our You know, are does that actually uniquely identify the weights lambda and the vectors AI? Otherwise, if it were unique, you see these higher order moments would be up to these corrections would be determining the desired centers of the Gaussian, so we of the Gaussian. So we could actually learn these centers if we did have uniqueness. Centers. If we did have uniqueness of decomposition, and indeed there is uniqueness of low-rank symmetric tensor CP decomposition. And this is actually a result of algebraic geometry. So there's a long lineage of papers that established the uniqueness of decomposition. And I put up here. You know, I put up here the somewhat technical statement. So basically, you know, I'll slowly try to read through it. Supposing that this R, this number of terms R, the rank R, and the order D and the length N are such that the order is at least three. So we're really talking. So we're really talking about tensors rather than matrices. And the rank is not too big. It's less than this binomial coefficient, which is on the order of length to the order minus one. And ruling out three specific low-dimensional cases, well, then, you know, if we assume that T admits this exact low-rank decomposition, With our terms, and we assume that the components are generic in some technical sense. So we've got a generic low-rank decomposition. We assume that T has this. Then, in fact, T has rank R. So the decomposition we assumed is minimal, and T has a unique minimal decomposition. Okay, so the key things here are that the order needs to be at least three. You talk about tensors, nodd matrices, and the rank has some upper bound on it. And then generically, if it's low rank, it's uniquely low rank. And this is one reason I think tensor decomposition, one of the things it offers in application. One of the things it offers in applications that the tensor T actually determines these components, AI and lambda i. So if for your learning problem, you can construct a tensor where the variables of interest correspond to the rank one components of the tensor, you're actually in a good situation. You just now have to do the decomposition. So that's the uniqueness theory. Then I think that the question becomes: do we have any algorithms? So given the tensor T, can we actually compute these components? And please stop me at any point if you have questions. I can't. I can't see the chat window or anything, but please feel free. Okay, so well, the bad news is that computing this decomposition of the tensor is actually NP-hard in the worst case. And actually, most computations with tensors are NP-hard. There's a paper titled As Such by Hiller and Lim. But there's some good news, which is that there exist efficient algorithms for tensors that are low rank enough. So we have this uniqueness. we have this uniqueness when the rank is and length to the d minus one n to the d minus one but if if the rank is if the rank is less than that so substantially less than that then in fact um we actually get algorithms two okay um so for order three there's a um a simple algorithm that that is Algorithm that is working up to rank n. Okay, so for an n by n by n guy, you can actually decompose if it's rank at most n. For fourth order, you can decompose, so you've got unique, so for third order, you actually have uniqueness up to order, up to rank n squared, but you've got you've got a way of decomposing up to rank n. Up to rank n. For fourth order, there's uniqueness up to n cubed by the last slide, but we know a good sort of more complicated, but very nice algorithm working up to rank n squared for fourth order. And Zhi Wing Ni has a method for all orders. For all orders, at least three, and it's going up to this rank, which is still quite a bit under the uniqueness end of the D minus one. But so this is what we think has to be. So, so this is what we think has to be the case for tensor decomposition. So, there's actually a conjectural statistical to computational gap here, where it's a theorem that there's uniqueness of the decomposition up to rank order to the end of the D minus one, but we were only expecting efficient provable algorithms up to the rank being the square root of the number of tensor entries. So, these are things. So, these are things we expect. And I should say, there's also various empirical algorithms. You know, you could actually just try to minimize the residual squared. You take your tensor minus the desired decomposition and try to find the lambda, fit lambda i in AI. Or another thing you can do is try to find one component at a time. One of the vectors AI. Uh, one of the vectors AI at a time, uh, and then sort of deflate. And this is the idea of the tensor power method, um, where you try to maximize the correlation between the given tensor t and a vector a to the d. Okay, so this, you know, it's it's a very vast literature, but this I hope is a somewhat representative survey. Representative survey of some of the methods. And so the main thing I want to communicate in this presentation is there's a new method and it has some advantages. So it's based on a reformulation of the problem. right the problem is given t find these these these lambda i and a i's and um the reformulation is is uh that we're going to construct a subspace of of um of lower order tensors and try to find rank one points in this linear subspace. So let me explain this reformulation of the tensor decomposition problem. And I'll assume for ease that we deal here with fourth-order tensors. So the method is actually applicable to any order, at least three, but it's easiest to explain for fourth order. Okay, so it's based on this elementary lemma. You have an n by n by n by n tensor t. Okay, so maybe it's a fourth moment of some data points. And let's here suppose that it exactly has this decomposition, lambda i AI tensor 4. Well, what I can do is I can flatten this tensor to a big matrix. I can unfold. I can unfold the tensor to a matrix. So, this is how a lot of the tensor decomposition stuff goes. So, we unfold the tensor T to an n squared by n squared matrix called mat T, whose rows and columns are both indexed by pairs of indices between one and n. Indices between one and n, right? And it's just the thing that you're thinking, that you group the first indices, the first two indices together and call that a row index. So you unfold the fourth order tensor to a 2D matrix, and then the tensor decomposition unfolds to a matrix factorization. But it's that looks like this. So MAT T. So MAT T is it's a product of three matrices. The first is n squared by R and its columns are the vectorizations of AI, AI transpose. This is a so-called catchy. This is the so-called catchy raw square of A. So I take my vector A, I do A, A transpose, vectorize that to be n squared, and that's one of the columns in this first factor. The third factor is just the first factor transpose. And the middle matrix is just the lambdas diagonalized. Okay, so Mat T looks like this. Mat T actually has this factor. This. MAT T actually has this factorization. It's equivalent to the tensor having this tensor decomposition. Okay, and it's a funny type of matrix factorization that we have here, because each column has this funny structure that it's the vectorization of a rank one matrix. Okay, but this is what CPD composition actually amounts to. Okay, and the And the corollary is that generically, the column space of the flattened tensor of MAT T is actually the column space of the first matrix in this product. Okay, so this is the upshot of the lemma. I'll say it one more time. The column space of the product of this specific product generically is equal to the column space. generically is equal to the column space of the first matrix in the in the product which is right the span of of of um if i unvectorize it's the span of ai ai transpose okay so um so the upshot is that this is actually this this thing here this the span of the tensor components times Of the tensor components times themselves transposed. This is a subspace of symmetric matrices. This is actually something that we can compute given T. So given the tensor T, we can flatten it, do a matrix, and then extract this subspace just as a column space by doing the SVD. Okay, so this is this weekend. Okay, so this is this we can actually access just given t not knowing what the lambdas and the ai's are, we can get the span of ai ai transpose. And there's a result that there shouldn't be any other rank one matrices in this subspace of symmetric matrices. Okay, so let's call A calligraphic. A, calligraphic A, to be the span of these guys, AI AI transpose. So it's some subspace of symmetric matrices that we can pull off from our fourth order tensor. Right? We're interested in finding these AIs. They're rank one points in this subspace. And the good news is there's actually no other rank one points in this subspace. Okay, that's the good news. So, um. So, this tells us that our problem is to basically find the rank one points in the linear subspace A. That would give us the AIs. And then once we have an AI, it's actually quite simple to get the corresponding lambda i. You can see it's going to amount to a linear system. Okay, so this, you know, this is. I'm sorry that this is technical, but the upshot is you've got a linear space of symmetric matrices, and your goal is to get the rank one points there. That's actually the same as decomposing the tensor. And I'll skip this slide. This slide was just explaining why there should. Just explaining why there should be no other rank one points in the linear space, but I'll skip this slide and I'll jump to what our method is. Okay, so let's motivate the method. So we're get, we're starting here with this. We're starting here with this symmetric tensor T, maybe a moment tensor, and we flatten T to an N squared by N squared tensor, matrix rather. And then let's consider its eigen decomposition. So the eigenvectors will provide an orthonormal basis for the column space of Mat T. So that's a basis for our subspace A. Basis for our subspace A. But unfortunately, these eigenvectors themselves are not likely to be the rank one points that we care about. Okay, so we still got to somehow find these rank one points. Well, let's consider the orthogonal projection onto the subspace A. So QQ transpose. Okay, so this is a mapping that eats a symmetric matrix and gives us one. Matrix and gives us one, the closest one in our subspace A. Okay, so we wish to find rank one points in A. So let me take a vector x and just think about xx transpose. Okay, that's probably not in my subspace A. So let me orthogonally project it onto A. It onto A. And now let me look at the, so that's a matrix now. Now let me look at the Frobenius norm squared of this matrix. Okay, so this is F A. It's a function of a vector x in Rn. Okay, and it's in fact a polynomial in Rn whose degree is the order of the tensor T. So it's a fourth degree polynomial. Here's a very Here's a very simple sequence. So, here's a very simple inequality that underpins our method. So, notice that, so by definition, f A of X is this guy, the norm squared of P A X X transpose. Well, that's definitely less than or equal to the norm squared of X X transpose, simply because I've hit it, I've hit XX transpose with an orthogonal. I've hit xx transpose with an orthogonal projection. That's going to shrink norms. So certainly I have this inequality. And then this is equal to the norm of the vector x to the fourth power. Okay. When is their equality? Well, it's precisely when the projection didn't do anything. So that is when x transposes in A, the subspace A. The subspace A. All right, so I have a quality precisely when X transpose is in my subspace, and that's great because the only rank ones in my subspace are my desired tensor components. Right, so I've so basically the tensor components are characterized as the X's where this equality, where we have equality here. Where we have equality here. So let's use this to formulate this optimization problem. We'll try to maximize FA of X subject to the normalization that X has norm one. So this max is what we look at. Okay, so by what I just said, the global maxes. The global maxes are precisely the vector components AI and also their net and also minus AI. So our goal is to actually just maximize this polynomial on the sphere and that will find for us the components of the vector components. And the way we propose to do this is by projected gradient descent. By projected gradient descent. Okay, so we just take a gradient step in Rn and then project back to the sphere. And we call this the, this is like a subspace power method that we call. Okay, so any questions at this point? Okay, so well, I can give you some convergence. I can give you some convergence theory for this. So the beta that you're seeing is basically one over beta is the step size in our projected gradient descent. We need the step size to be small enough. So we have some, so in other words, we need the reciprocal beta to be big enough. And here's some examples. Be big enough. And here's some explicit bound. This is our iteration trying to maximize this polynomial FA and find the rank one points in the subspace A. Okay, so does it work? Well, we can show that for all AIs and so for all configurations of tensor components AI and all initializations. And all initializations x on the unit sphere, this iteration does converge to something. So you don't have very complicated dynamics. You don't have a limit cycle. It does always converge and at worst at some algebraic rate. Okay, so Okay, so that's nice. The next thing we can say is that for all components AI and almost all initializations X, this iteration has to converges to a second order critical point of FA. Okay, so that is the gradient, the gradient on the sphere is zero. Gradient on the sphere is zero, and also the Riemannian Hessian is negative semi-definite. So we avoid strict saddle points for almost all initializations. Okay, and thirdly, for generic AI and rank Not too big. The global maximizers are precisely plus or minus AI. They're precisely the tensor components. So this follows from the thing I said a few slides back, where the only rank ones in our subspace are AIAI transpose. Okay, so we're always converging. We're always converging. We're always converging to a second-order point. The global maximizers are what we want. And fourthly, we do actually have, so every AI is an attractive fixed point for this power method. I see something in the chat. Do you have any idea about the existence of non-global minima? Existence of non-global minima? Yeah, so it's a great question. So they can exist. So we can have non-global maxima, but they sort of don't seem to exist or they don't seem to come up very Or they don't seem to come up very often. So, sort of, if the AIs are random, say, the components of the tensors were constructed to be random on the unit sphere, we think with sort of high probability, there should actually be no spurious global maximum. And I'll present numerics in a moment that suggests that. And okay, and moreover, you know, we have some, we also have a landscape analysis that shows at least there are no bad local maxima with sort of high functional value. So we can prove that in a super level set. In a super level set of the objective FA, the only second-order critical points are precisely plus or minus AI under certain conditions. So I'll also come to that result soon. Okay, so the fourth thing I was saying is that actually each component is attractive, and this iteration converges at a linear rate. Linear rate locally. So that's something. Okay. So there's some interesting ingredients in this convergence analysis. And I refer you to our paper, or you can ask me afterwards. Okay, so as I was just saying, experimentally, our Our subspace power method converges to one of the tensor components almost always. So here's some simple numerics where we took the length to be 20, n equals 20 in the fourth order. We generated the AIs to be just say random on the unit sphere, and then initialized also randomly on the unit. On the unit sphere. And what we plot here vertically is the relative frequency of us converging to one of, of our initialization converging to one of the AIs. And you can see, you know, for a lot of franks, it's 100% that we observed. That we observed. So the rank 190 is special here because actually beyond 190, there are other rank one matrices in our subspace. But rather close to 190, all the way up to say 170, we observe that projected gradient descent is finding one of these components. Of these components almost 100% of the time, at least under this standard random tensor model. Okay, so here's our method. This is now an end-to-end tensor decomposition method. So I've been describing to you how to find one component, AI, but you can put it all together to get an end-to-end method. So the input is a tensor T, and the first step. And the first step is to flatten that tensor and do an eigen decomposition to extract the subspace A. And then you do this subspace power method, the projected gradient descent to find one of the components Ai. And then you deflate. So you determine the scalar lambda i such that The input tensor T minus lambda I AI tensor D has rank one less. So you can actually determine, you can determine the Lambda I that that such that this rank one update to the input tensor T reduces the rank by a closed formula. So the determining the scalar lambda i can be done in closed form once AI is available. Once AI is available, and then you repeat this process. So AI is removed, you won't hit it again, and you go for another, you do more of the power method. Okay, so this is the pipeline. And sort of maybe the most important thing about this method is its performance. So in blue, we're showing the subspace power method. We're plotting here timings. On the horizontal axis is the length n of the tensor. And we assume here, just for this experiment, that r is 2 times n. And we followed the similar, we generated the tensors by the same random tensor model as before. Okay, and so in orange is this fourth, is a provable. Is a provable method that I mentioned before. And yellow is coming from a well-known package in MATLAB. And so we observe about in this experiment about a one order of magnitude speed up here. And so this experiment was done. So, this experiment was done for fourth-order tensors, but we also have results for sixth-order and so on. The method is roughly, so far I've been discussing, I've been supposing that my input tensor T is exactly low rank. And of course, this is a big problem. This is a problematic assumption. So, one can add. Assumption: so one can add noise to the tensor T. So it's planting a low-rank decomposition and then adding, say, entry-wise noise. So you can still run our method to try to get the components with a noisy input. And we observe a stability on par with existing methods here for noisy inputs. Okay, so right now. Okay, so right, so the missing piece of this theory is the existence of bad local maximizers. And we recently analyzed the non-convex landscape of the SPM functional, this FA polynomial, and we have some results in two scenarios, assuming the rank is Is small, so like on the order of n, and some deterministic frame conditions on the components AI, or that the rank is much larger, but we have randomness in the AIs. Right, and so towards this conjecture that there are no bad local maximizers, we obtained we obtained results. And here's one of them. And here's one of them. So, assuming that the AIs, the tensor components are random on the sphere and that T is noiseless, we define this quantity here, epsilon r. So it's, and if epsilon r goes to zero as the length and rank scale, then there exists a constant C such that with high probability. Such that with high probability, the SPM program has exactly two R second-order critical points in this super level set. So you need the functional value to be at least C times this quantity epsilon R. And here, the only second order critical points are plus or minus AI. Okay, so you don't have any really high quality bad local maximizers. And this super level set is a little bit. And this superlevel set is a little bit higher than what you get with random initialization. So, this isn't quite a global result. And I should also say we went to great effort to provide a version of this result with a noisy input tensor T. So, we also in the paper coming in Neurops, we have a robust version of this result where the tensor T has noise which propagates to the subspace A. And it's a similar style of result. Similar style of result. So, just in the last couple minutes or two, I want to say that, you know, the other huge problem of tensor methods is the curse of dimensionality. There's just so many entries in this tensor, right? N to the D entries in a tensor. Sorry, this should be n to the D. So, well, my original motivation was to apply this to moment tensors. So, tensor is constructed by Constructed by a cloud of points in R to the little n. So, in that case, you know, is there a way to decompose the moment tensor T without ever forming the moment tensor, just by working with the vector, the given vectors, the given data points. Right? So it's a well-posed question. Can you find these lambda i's and a i's with just given the the original data points without ever storing or forming the Without ever storing or forming the tensor T. And Sherman and Kulda answered yes, when you apply a direct optimization method for tensor decomposition. And in work in progress, we're showing that indeed you can apply our method computing just with vectors rather than tensors. And our storage cost is linear in the length n. It is linear in the length n. So you have a dramatically less storage cost than if we were to form a tensor. Okay, so for moment tensors, we can actually sort of overcome the curse of dimensionality. And just to give you a timing, here we're looking at 10 to the four data points in R500. We're fitting to a mixture of Of 100 Gaussians, and doing that with the sixth-order tensor. Well, if you were to form this tensor, it would be 500 to the sixth. That would take a lot of space, 125 petabytes. By never forming the tensor and just running our method implicitly, just with the with these vectors in r to the 500, we can obtain the centers, the 100 centers to the Gaussian in 13 and a half. Entrance to the Gaussian in 13 and a half seconds on a desktop computer, we get an error of about six to the times 10 to the minus 3. So the error that we see here is stemming from the fact that the moment tensor of a mixture of Gaussians is only approximately low CP rank, right? I showed at the beginning that there were these correction terms from low order moments. So, okay, but we're actually more accurate than K. But we're actually more accurate than k-means, even still, even so. So, just to wrap up, you know, in this, we're interested in other types of tensor decompositions besides the CP that I've described here. You know, an interesting one is the inversion of moments that arise in cryo EM, which is a Which is a biomolecular imagery technique. And there's various, I mean, all of these I think are interesting. And so you have a different structure in the point cloud and a correspondingly different tensor decomposition. And the question is to characterize uniqueness, like is the moment tensor enough to tell you the structure? And to devise storage-efficient methods to decompose the. Methods to decompose the tensor and learn the structure. And, you know, so we applied, I applied this to real data for a task of motion segmentation. So here the points, the data are point tracks across video frames, and they're sort of rigid objects here. And we're trying to sort, we're trying to cluster the tracks into rigid objects. And our tensor decomposition method is actually. Decomposition method is actually doing better than some other popular methods for some data sets and pretty not the best but okay for other data sets. So this is an application to motion segmentation with real data. Okay, well thanks for that. I'll just end it here. Thank you. Well, thank you very much. It's so interesting. It's interesting to see all of these speedups in computation and memory. And memory savings, which allow for things to actually be run, and even these simulations with real data-that's really interesting. So, does anyone have any questions for Joe? Yeah, I was curious about, so first of all, is this implemented in a Python library? Yes. So, we first did MATLAB. So we first did MATLAB, but yeah, we're moving it into Python. Okay, great. I guess is that available already or not yet? I guess. I think it's on our GitHub. Let me get back to you and I can email you after this with a point. I'd appreciate that. That would be wonderful. The other question I had was: so for subsymmetric tensors, can you do something there? Like, suppose you have, you know. Do something there. Like, suppose you have, you know, a tensor form like AI, you know, AI tensors up to the kth component, and then it varies. Something like that. Can you do something like the decomposition there or at least use the algorithm to speed up the subsymmetric part of the decomposition? Yeah, that's a great question. So it's also something that I thought about. Thought about. So, this method currently is for symmetric tensors. We did that because I was motivated by moment tensors, but I think the same ideas should go through for partially symmetric tensors, like you're suggesting, with some modifications. So, the power method, right, would be Right, um, would be optimizing over the like a tuple of vectors, uh, probably in some alternating manner. Um, so it hasn't been done yet, um, adjusting this for partially symmetric tensors, but um, I think it is something that's doable. Yeah, we use this, this arises in our application where we use the strongly orthogonal CPD. You know, described by Colba. I think it's in the Colba paper, one of Colba's papers. But that's an issue, right? We would not have a symmetric situation, but would be interested in using it. That's the one I described in my talk. But all right. Well, thanks. Thank you very much. Thank you. And since, well, does anyone else have a question? Does anyone else have a question from the audience? Maybe while you decide, I just wanted to ask you if you could go back to your theorem where you had samples on SN, because that one went by a little bit quick. I couldn't quite. Yeah. So I don't know if you could maybe explain this a little bit more than what you thought you went through. Yeah, so okay, so we. So we need some assumptions on the tensor components AI, because we know that there are some NP hardwarest case instances. So here we assume that the AIs are random. So they're going to be uniform on the unit sphere. We can always assume that they're unit norm because we can put the The we can rescale and move the norm into the scalar lambda i. So it's harmless that they're on the unit sphere, but we'll assume that they're uniform on the unit sphere. And then we introduce this quantity epsilon sub rank, and it's equal to the rank times log of r to the eth power. E here is the Here is the order of the input tensor divided by two rounded up. So for fourth order, E is two. Okay, and then we divide by end of the E. Okay, and so our result is applicable if this quantity epsilon r is going to zero as the rank and the length scale. So that is, we need r to be like a little bit less than n squared. bit less than n squared for fourth order tensors. Okay, this is our rank assumption. And then there's a constant independent of the rank and the length, such that with high probability over the randomness in the components, the SPM problem, which is maximizing this polynomial FA on the unit sphere. FA on the unit sphere has exactly two R second-order critical points in the super level set where the functional value is at least C times epsilon R. Okay, so there are no bad local maximizers with a big functional value. There's no high quality bad local maximizer. High quality bad local maximizers, none in this super level set. And right, the level of this set is vanishing because we're assuming we're in the scenario where epsilon r goes to zero. But unfortunately, this level is a little bit higher than what you get with a random initialization on the unit sphere. We conjecture it's a logarithmic factor with higher. Okay, but the only second-order critical points are exactly plus or minus AI in this super level set. So, sort of like in particular, if you ever, if when you're running the subspace power method, you ever get beyond this functional value, then you're actually home free. You're going to converge to one of the AIs. To one of the AIs. So it's just, we just have to boost the functional value a bit beyond what you get with random initialization. And then this guarantee tells us that we're going to get one of the AIs. Okay, great. Thanks. Thanks for the explanation. Thank you. Delve deeper into that. Well, thank you very much again, Joe. I think it's been very interesting to see you. It's been very interesting to see all these developments in all these directions. And thanks for coming to the conference as well.