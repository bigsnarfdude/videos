Okay, good morning, everyone. Thank you very much for the introduction. Thank you very much for putting together this workshop. Personally, it's just great for me. I feel to be pushed on the boundary of my comfort zone. And so that's a boundary problem. It's very pleasant for me to be there. And thank to all of you. Me to be there and thank to all of you for a beautiful presentation. I've learned a lot. Uh, yeah, so now my idea is to try to explain to you, uh, hopefully, what we've been doing. Yeah, part of it is over 10 or 15 years. So it's about a priori and aposterior estimates. I think that we should be final. And yeah, there's one central tool that, in my view, does. It, in my view, just bold, and I would like to explain to you about it. And the name is this first line of the talk. So, it's a joint work with over the years with many collaborators. So, I've listed them here. And structure the talk into a very long introduction, hopefully, as smooth as possible, and then kind of the contents. The contents. So, in the introduction, I in particular try to explain all the titles. So, let's fix very specific context. I think it should be helpful for all of us. So, I will be just looking on one simple toy PDE, the Poisson one. So, I look for scalar variant function u that goes from omega to r and satisfies minus Laplacian u equals f in omega and for simplicity of presentation just homogeneous. Of presentation, just homogeneous direct boundary conditions. Okay, so omega for me just Lipschitz polytope, so light segment or Lipschitz polygon or Lipschitz polyhedron. F just L2 source there, and there's nothing more, omega and F2 data. So that's the usual weak formulation. Yeah, I've heard, for instance, this morning from Johnny. So yeah, that's that the space it's kind of sits there from my viewpoint and is. From my viewpoint, and is the intrinsic one. So, L2 functions whose weak gradients are also L2. Okay, and this weak gradients they appear here and they need to be L2 because they are integrated. So, this notation is L2 integral of this color product of the two things, and that's why the H1 seems to be perfectly natural there. So, this I think is really something we've seen many times, but for me, it's always a But for me, it's always surprising that very often people don't make what is hidden there. I'm going to uncover now, the link to the fact that much more is hidden in this toy weak formulation. In the weak formulation, we don't just learn that u is h10, but we learn something about what I like to call a dual variable. So this u would be primal variable, say temperature, pressure, or concentration, and the dual variable will be minus. And the dual variable will be minus graduate, it's the flux, the flow fields, the darshan flow, whatever. Okay, and there is a second function space that is kind of intrinsically there. It's the functions of L2 integrable objects vector-valued with divergences in L2. So that's the H did, as Johnny was saying this morning. And this line, the weak formulation of your PDE, actually states it, okay, that the graduate is in. Okay, that the grad u is in h diff. So u is h10, so it's l2, and grad is l2. So this is l2, that's the first requirement for the h diff. And this line, it's exactly the expression of the fact that, yeah, there is a divergence, and the divergence is f. Okay, so the two spaces that Johnny had in his complex, they naturally appear there. They are stick to my PDE, and they are H10 and H diff. They will be all along the top, they will be present all the time. Yeah, if I speak about potentials, I have in mind this. When I speak about fluxes, I have in mind this. Okay, so now it's about numerics. So, I will have some computational mesh, and no surprise, this will be triangulation, so simplicity, d-dimensional simplicity. Simplicies, D-dimensional simplicity, so line segments, triangles, the truss. I stick for the top to the usual notation h, the maximum diameter, but nothing is connected to uniform meshes, and all of this applies to highly graded meshes. So that's the first parameter, h, the mesh size, and the second one, p is the polynomial degree. Okay, so polynomial degree, again, one number for all the triangles, everything can be generalized for varying. For varying, and the spaces in the heart of numerics for me is this one. So it's piecewise p-degree polynomials on each separate machine element p degree, but no requirement on what are different or not on the boundary, nothing in this notation there. Okay. And yeah, the notation UH is typically the approximation of U. Yeah, okay, pictures, toy pictures, but yeah, okay. So. Yeah, okay. So, simple setting, my domain omega is integral. It's like line segment 0, 1. There are four mesh elements, all of them are line integrals. So they form the mesh. And now I have p-slice and fines here, p-slice defines there. So that's the notation p1th. And that would be a typical possible numerical approximation. Well, that one lives really separately. one lives really separately mesh element by the mesh element in in in p1 fines so the next one would kind of go with my pde which imposed h10 my notation i just intersect the piecewise parameters tap h10 and you've seen this is absolutely equivalent to say intersection c0 so intersection continuous functions and it's Use functions and it's finite elemental, so that will be always C0. I don't go higher smoothness. But I know this IGA meeting, so I put in backup slides the generalization to IGA, but I want to focus on finite elements. Okay, so that's C0 all the time. Do you mind just like resetting the camera? It should be good. Okay, I need to redo my stuff. I need to really do myself here. I'm so tempted to show with my hand, but okay, I need to use this technology. Okay, so yeah, toy things, but they are also leaders to the, yeah, I'm now jumping in front, but later on, this will be my object that will be given, and this will be its potential reconstruction, as you can kind of feel. So I reconstruct in the correct space. Okay, for the moment being, I just have to. Okay, for the moment being, I just have two examples of piecewise polynomial discontinuous and piecewise polynomial in the good space C0. Well, 2D, yeah, the mesh is triangles. So on each triangle, I've got a little parabola, nothing to do one with each other. So that would be a function in the P2. And that would be a function in P2 cup H10. And if you look, well, it doesn't fit with my notation. I put zero boundary condition, and the function is not. Zero boundary condition and the function is not really zero on the boundary. So picture is wrong. Now, the second variable, the dual variable, the flux. So if you do conforming finite elements on a simpler salmon 2D on triangles, and you do P1, the gradients will be piecewise constant, right? So there will be one arrow in each triangle. And say if I pick this. And say, if I pick this edge between this triangle and this one, in terms of physics, that's completely wrong because this arrow tells me that the flow field leaves the triangle. There will be mass going from left to right. And this one tells me, no, no, no, it goes from right to left. So that's piecewise polynomial, zero degree, but not in the good space. And yeah, that would be a drawing of the one in the good space. Of the one in the good space. So, that on the right is exactly what Johnny was showing this morning in a more abstract way. That would be the Raviyat-Toma thing. That's the definition here, but we don't really need it. It's a finite-dimensional piecewise polynomial subspace of this Hilbert space H diff. It's a bit richer than just polynomials. There's something Johnny was showing. Yeah. Cross product or dot product depends HPR HDF, but okay. Okay, so this. Okay, so these are for the moment just examples of an object piecewise vectoral polynomial, not in H div and piecewise vector valid polynomial in H div. Okay, cool. So a small theorem. I really like this one. So let me try to explain it. So U is the weak solution of my PDE, okay? We have our mesh, th, column B D P, a completely arbitrary object therein, say this one. This one, okay, and I'm interested in error. Okay, that's what numerics kind of later is. So, what is the distance of the discrete, say, numerical approximation to the true solution? Okay, so yeah, please, please don't hesitate. So, I'll explain. Here, I put just the grat UH, because if you start with, say, conforming function. Because if you start with, say, conforming finite elements, they are in H10, so you have a right to speak about weak gradient. If I start with a guy like this, it's discontinuous. There is no weak gradient of that object. But you can do weak gradient element by element, which is nothing but the true gradient. And that's the notation H here. It's not distributional gradient, it's element-wise, gradient, piece-wise polynomials, apply gradient element by element, and that's it. That okay, so for me, that's I call it energy error. Okay, that's kind of the norm induced by the setting, that's the energy of my puzzle problem. Now, this norm square actually equals to something and equals to two terms that write like this. Well, the first one is maybe not surprising. If I start with an object that is outside of the space, then kind of I have to see the Then, kind of, I have to see the distance to the space. So, this is a spurting, okay? How far the UH is from the space? Well, the other one is also written in the form of a distance. It's for this dual variable for flux, okay? Flux, we've seen it should be h diff and should have the divergence equal to f. That's your PDE, that's the physics, that's the equilibrium, yeah, the dual variable, and it is in equilibrium with load, okay. Equilibrium is lost, okay? Well, if it's suspicious, the second term also writes in this form, okay? Which is maybe better. This is the weak form. You plug in there your approximate solution. You measure how far is it? That's the residual. And if you take the supremum or the maximum, that's the norm, the size, the dual number of the residual. But they are the same, okay? And I like to stick to this one. It's convenient for what I'm going to say. Okay, so that's error equality, it's characterization. You haven't lost anything so far. Okay, so let's now turn it into a posterior estimate. So that will explain the first keyword from the type. Well, there are two minimums. So I can just pick an object in this space and in this space and this constraint, and I get a buff. Okay, so this is it. So that's our theorem of our posterior estimations. Just pick enough. Estimations. Just pick an object in the correct primal variable space. Pick an object in the correct global variable space with the good physical constraint, this is the divergence equilibrium being imposed here. And this is less than or equal. So we call it a posterior estimate because this thing on the right only has discrete objects. It's computable. It's your aposterio estimate. It's your apostrophe estimator, and this thing on the left is the unknown error. Okay, I admit, I first kind of came over it like 15 years ago, but since then, it's still the same fascination for me. I don't know the thing on the left, and I know the thing on the right. Well, it's a posterior, it's kind of part of the game. I know the UH, so kind of it's much, yeah, I know it's much deeper than a priori, it's different viewpoint than a priori. Put an apprio. A prior, you don't know UH, so that would be difficult, but a posterior. Okay, well, uh, two central comments to proceed. So, well, to be for me to really call it a posterior estimate, I like two constructive things to be satisfied. So, first of all, I'd like this objects in the correct primal space be constructed. Correct time space be constructed out of UH in a local way. So the philosophy is: I've paid everything to compute my UH. I don't want to spend more time on doing anything else. I mean, these were minimums here, so you could minimize. But this would be a global problem, and this is what you want to typically avoid. So that's the keyword I want to have these log constructions to give. And second thing, we need to make it practical. So UH was polynomial, and I would like these. Was polynomial, and I would like these two guys to be some kind of piecewise polynomials as well. Yeah, just wondering, you said this is computable on the right-hand side, yeah. But the sigma h solves this equation gradient equals f. How can this be? How can you compute that? How can this be equal? So this is for the moment completely arbitrary object in function space with a constraint. There are many objects like this that exist. many objects like this that exist. Okay, the sigma, okay, sigma h is unspecified for the moment. I'll show you the contents of the talk, suggestion how to do it for the moment. Yeah, I just tell you that there are many objects that are there. But if the gradient of sigma h equals f, is this possible in this discrete space? Because f can be something. Yeah, okay. So, well, very good. Now it comes. Okay. Do you mean this really? Okay, to make this really true as it's written on the board for the moment, of course, it would be hard. So I restrict in this for the sake of presentation, I restrict now for the slide, it will come back later to general, to F being piecewise polynomial. Of course then it will be in reach of polynomial. Yeah, very, of course. And if I interrupted you, what's the relation to CS Lema? Yeah, CAS Lemmar. Okay, so let's jump. Okay, so let's let's let's jump. No, it's fine. I have it there. So Seras Lemma says this, okay? Seras Lemma tells you that if you do, so you jump, but from if you do conforming finite elements for the possible problem, which is this line here, then the distance of your conforming finite element to the true solution is nothing but being realized as the minimum of over the whole. This is the minimum of over the whole space. That's sales lemma. Okay, the sales lemma does not speak about the dual things, about the fluxes. Okay, it speaks about that your UH by definition is a Garcin projection. In my setting, it's just equal. That's it. So you have to a constant, continuity. But in your estimate, you don't need this constant. No. Which is typically unknown. So this is. But this is Poisson. In Poisson, Sears Lammad, the constant is one. Yeah, because Passon, the binary form, is the scalar product on your finite, unfinite dimensional H10 space. And it's written here, the discrete approximation realizes the minimum. So yeah, the sales constant would come if you have some fancy diffusion and then you need to bound from above and from below the eigenvalues of. Okay, so that's sales lemma. Okay, so that's Ser's Lemma, and now we see the difference, okay, in this theorem here. Yeah, it's another equality, and the equality comes with, I mean, twice the spaces, and in particular, there is this dual viewpoint. It's not present in CSTA. There are two equalities, both of them are true, and they're different. Yeah, please don't hesitate. I mean, once again, let's spend half of the talk there on non-different. Let's spend half of the talk there on the introduction because I think that's where probably you grab the idea. Okay, so we were here. So, from here, we pick two objects, we get the bound. I was trying to motivate that I need the construction of UH towards these two objects to be local. And also, UH is piecewise polynomial. I want the thing to be doable in practice, which typically means both of these objects need to be piecewise polynomials. And then in finite dimension, they're. And then a finite dimension, everything's fine. Okay. Well, so far, yeah, sorry. So, this was the first thing I needed here, as you correctly noted. And actually, another point that is kind of necessary so that I can proceed exactly as written here. Yeah, I don't, yeah, I need some kind of link between this object here, so the gradient of your approximate solution. Gradient of your approximate solution and the load. The link is very weak, it's one equation per vertex of your mesh. Reformulation. This requirement, it's the Gallerian orthogonality of UH, but only with respect to lowest order functions, the head functions. Psi A for each mesh vertex A, head function is the function one and zero elsewhere, piecewise defined. This always there in. This is always there in your finite in your polynomial spaces. And we need this in order to construct a good object sigma h. Otherwise, it's not doable locally. So that's very important. So that's kind of also part of the questionable step. So these are the two additional things I need. And then I can this upper bound. So that's my unknown error. That's my computable estimator. Computable estimator, but now I'm a mathematician. Okay, I have two objects: a non-error computable estimator, but only one-sided relation. As a mathematician, I would like the things to be equivalent, two-sided bounds. Like mathematically, up to a constant, they should be the same, and then I'm fine. Well, this was true for n sh and sigma h. So now I need to continue. Oh, yeah, I need to do, yeah, I just need to. I need to do, yeah. I just need to grab good ones, not just any, but some good ones, okay? So, what are my structural requirements on the good ones? So, you see, this SH is typically constructed close to UH in this norm. And my requirement for a good one, the SH should be that close to UH that it's impossible to get closer to UH with any object that would be from the whole. With any objective to be from the whole unfinite dimensional space. This is a central point. These are polynomials, these are finite-dimensional. And I'm claiming now that I can do a magic. I can construct an object that is that close to this one that up to a constant, I cannot get closer in the whole H1. And similarly, here, sigma h is constructed to be close to this thing. Both are piecewise polynomials. And I'm claiming that there are ways how to do it. I'm claiming that there are ways how to do it. That actually, up to a constant, it's impossible to do closer to that guy, even over the whole unfinite-dimensional duality space, which is H diff under the constraints. Okay. So this is really the central part. It's the central thing. And yeah, I will show you what are the tools beyond. These are these are Punk Area maps and these are polymer extension operators that make it. And they only make it if the both. only make it if the both data are piecewise polynomial of the same degree. You cannot do it in a general way. So yeah, for the moment we have to believe it. If we believe it, then we made it from here to here, but okay. Yeah, so there's one last point. Again, yeah, I had two requirements where it appeared in the third one. Okay, so for that, this equality as written, okay, this object UH can be outside of H10. Can be outside of h10, and this line here for the moment tells us if I take trace from left and from right of the uh on each mesh phase, the mean value that should be zero, and then I can do the matching. Okay, so yeah, in finite context, that would mean I can do non-conforming or mixed or spaces like this, not really digital for the moment. Okay, so I made it back. This from the previous. This from the previous theorem is the error, and this is now my what I call optimal alposter estimate. So, this is unknown error, this is computable, and I made it back. So, equivalence of the two in particular, and that's my notation. Whenever you see this symbol, this means up to a generic constant that only depends possibly on space dimension, one, two, three, and mesh ip regularity. That's it. There are two big parameters of numerics: H. Of numerics, h, measure size, p point degree. Anytime you see this thing, it means independent of two basic numerical discretization parameters, h and p, which is typically not the case in finite elements. You have constants that blow up with p. But these constants will always be and it's independent of the geometry of yes, it's only dependent on space dimension and measure regularity if you have some kind of intuity. If you have some kind of intuity angles that are bounded, you have set. Okay, this is where the geometry of omega comes in. Because if omega has a small angle, then you have the but say, yeah, it's covered by the mesh. For instance, the tip is no problem. If you can mesh the omega with nice elements, it's fine. We kind of, yeah. So, yeah, you've seen these pictures, so that's kind of this is what will be the tool. You give me this as a data, so at the occasion of this turret, UH, and I construct locally an object that will be close to it, and I will use it as SH here. So that's the what I call potential reconstruction, and the other one, you give me some piecewise polynomial of. Some piecewise polynomial flow field, vector valued, and I locally construct something that is close to it. And that would be what I would call just reconstruction. So that's the notion of being outside of the space. And I bring it to the space. And the key for this second object is the word equilibrated. It means that not only did I bring it to the space, but I made it to satisfy paper. I made it to satisfy the constraint. So, yeah, so this is the potential construction, that's the flux construction, and we've kind of seen that they make the a posterior analysis, okay? So, let's now move to the slide we've uncovered a bit. Let's move to a priori. Okay, so let's stick to conforming finite elements again. Mesh, polymer degree. This is my finite element solution. We've seen it. It's a SAAS lemma or characterization, Garrick projection, whatever. Garrick projection, whatever. That's the characterization of the solution. And let me show you now in this slide that the same tools, actually, just one of them here because I don't have the non-conformity, will make me a priori error estimate. That is also something I like to take term optimum. So how do I proceed? I pick mesh element by mesh element independently and I do the same thing as here, but on the element, Garrick and projection, H1 autograph. Garakin projection, H1 orthogonal projection, best polynomial that fits the function, but only in the element. Okay. Well, that's clearly a piecewise P degree polynomial, that's by definition, but there's no reason to be conforming, okay? It will be outside of the H1 space. Well, that's how I made my reconstructions, okay? I take this as datum and I make the reconstruction. My reconstruction is. My reconstruction SH produces me out of this guy, a guy that is close to it, but yeah, makes it conforming. Okay. And moreover, yeah, so now I'm in the good space, the same space as my polynomials here as my finite element solution. And because the finite element solution is the argmin, it's less than or equal to the object I've produced here. Okay, now a little triangle inequality. Inequality, I bring the psi h back there, okay. And let's now look on this second term here. Well, sh is my potential reconstruction of this datum psi h and I've done it in the way that is as close to the psi h as it is possible on the whole h1 space. So I can, in particular, plug in their u in this second term. In this second term, in the first term, there was only u minus psi h. Okay, so yeah, that's one plus c times, so up to a constant this. And let's now write down what this psi h minus u. So that's the definition of psi h. So this is now just plugging back this definition here. And I see that the minimization over polynomials, but cut H10 up to a constant. Up to a constant is the same thing as going element by element and minimizing individually in the mesh elements. Okay, so I call this global best and I call this local best. There are constraints here, C0 constraint there, H10 continuity. There's no constraint here. And as you can feel, now you're done. I mean, this up here estimates, you're on the element, but you're fine. H over P, and that I would call optimal HP. Would call optimal HP a priori arresting. So, in particular, this constant once again is independent of P. It doesn't blow up with P is, which we have would be authentic. So, let me rewrite what we see here. Let me rewrite it in the picture language. It's always the best. So, it's approximation thing at the end of the day. This is finite element context, but let's step away from this. Step away from this and take approximation context and plot it in 1D. It's clearly see what we're doing. So I've got my omega, my mesh, the target function into H10. And in one hand, I look and how good can I approximate the target by piecewise polynomials, but yeah, C0. On the right, same function, same mesh, but I approximate it element-wise. Okay, what we've just shown. Okay, what we've just shown is that the two approximations are of the same quality up to a constant. There is no difference in how good you can approximate this way and this way. And yeah, it's my favorite approximation result. It's beautiful, right? I mean, piecewise discontinuous polynomials, piecewise continuous polynomials. And both have the same approximation power. So written as this, this is a result by Andras Weser. By Andrea's visor, but his constant in the equivalence blows up with p. And now I can prove that the constant does not blow up with p, and this is what we use here. So here we are using, yeah, constant of not blowing with p. Okay, so yeah, that was a very long introduction. Do you have questions? So actually my plan was to take you on on a bulk. Plan was to take you on a balloon trip over Paris. So that's what we just do. You know, Eisen Tower, Seine River, Sacre Caer. But the French they also have the Concorde. You know what's Concorde? It's a hypersonic plane. So that's kind of the plan for the rest of the talk. No way. Okay, so I apologize for the rest. I will completely switch gears, but yeah. Okay, so what are these two constructions? I want to show. Are these three constructions? I want to show it to you. I want to write in math formulas what you've just seen, kind of more on the picture in which I'll get back to a posterior numerics. And I would like to have a few minutes on the tools that make all of this work. So what is this potential reconstruction? I've got data and p-slice polynomial, and I end up with p-flies polynomial cap H10. This is what I call reconstruction. How do I do it? Mondi should give you a picture. Mesh vertex with the two neighbors. Mesh vertex with the two neighboring vertex star, if you want to call it like this. My data is polynomial. I play in the head function one year zero zero. I cut off that guy, which is still, so now this product is still discontinuous, but zero. So that's my cutoff. And then I produce the closest object to the discontinuous guy in the continuous world. Okay, so I formula is here mesh value. formalize here mesh vertex patch cutoff by head function and this discontinuous polynomial I try to get as close as possible to it in the continuous polynomial world so cap H10 zero on the boundary of the patch and I'm kind of capturing the discontinuity along the mesh faces inside of the patch and then I take these results on the patch combine so this is argmin writing yeah Writing, yeah, all Lagrange conditions, and this is nothing but this little finite element, say numerical approximation of a small PDE inside of your mesh, which takes this as a datum, which is a discontinuous piece of s polynomial, and I project it to continuous work. That's what I'm doing. Okay, so maybe key words that are beyond. I localize to patches, everything goes on vertex patches, I use for this the head function that cuts off everything. That cuts off everything. This datum is being projected, and I'm in H1 space. The S and two condition is directly. I put zero on the boundary of those patches, and that's why I can then glue things together. Okay. Well, there's a technical point. When I multiply by that function, I increase my polymer degree by one. Okay, so I typically pick for this P prime P plus one in a poster analysis, and we are fine. I cannot do it in a priori. Do it in a priori. So, for the moment being, I just kind of brutally cool these things down with some lag ranch interpolator, and there's more stuff hidden there. What to do? Okay, and this makes something like this. What about the H1? Oh, sorry. So, now the properties of this H1. So, the central property that follows from the tools that I would like to show you in a while is the following. So, this is the definition of my local problem in the vertex star. In the vertex star. And I've got a P prime piecewise polynomial datum that I approximate in the P prime cup H10 space. The tools, the Pancari maps, the polymer extension operators, allow me to prove that in such a setting on each single vertex patch, this is as good as forgetting about this and doing the whole H10. That's where the unfinite dimension comes in. Rephrasing. Rephrasing or putting another perspective, there is no single word of degrees of freedom in all that I'm doing. The moment I would put degrees of freedom, I'm done, I'm dead. I mean, degrees of freedom are finite dimension. I want links between finite dimension and unfinite dimension that are kind of true consistently. So you see, that's kind of here. If I go with P very high, I run into H1. If you take degrees of freedom and run very high, they don't give any meaning anymore. You cannot take point values in Lagrange, you cannot take phase. Values in Lagrange, you cannot take face normal phase components in H2. Okay, so this is really the central part. This is patch by patch. When I combine, I obtained exactly the result we needed in the long introduction. I obtained that my potential reconstruction is as close as that close to my target as any H10 object can do. Okay? Up to this temperature is fine for a moment. Fun for the moment. Well, same story for fluxes. This is my datum. This is what I want to reproduce. How do I do it? I go for patches, take vertex, all elements, head function, cut off of the datum, project the discontinuous hg non-conforming datum to hg conforming, same degree polynomial space. Now I need to do it under the constraint. I get objects that are zero on the normal tricks on the boundary, combine them. Well, we are in. Well, we are in the duality setting. Diriclet flips to Neumann and primal finite elements flip to what would be here dual or mixed finite elements. IMD that equilibrium is written here. The sigma is sum of sigma h day. The diff is the sum of the individual divergences and kind of, yeah, just flavor. You see the head functions, they sum up to unity. So that's kind of where they go away, and this is where I get my equilibrium. Way and this is where I get my equilibrium. Okay, so again, typically I can either do p plus one or yeah, kind of brutally force for the moment things back to the good quantity degree. On the picture language, I take my data, I take mesh vertex by mesh vertex, the vertex is around, cut off by head function, so zero there, still discontinuous. Now I project this discontinuous object in. Project this discontinuous object into locally H diff conforming. So normal trace continuous. So that's the formula here. Objects indexed by A combined, I get the final one. And it turns out that the final one is H diff. It's clear, but also satisfies the divergence constraint. Again, the key property for all the theoretical results is that because this data This data and this other data model piecewise polynomial approximation on the same degree polynomial subpiece of H diff after constructs the same thing as approximation on the H diff. I mean this is really the tool where you spend 30 pages proving it and you use results of others that are 30 or 50 pages. Okay, if you combine these things living on patches in total, yeah, that's exactly what we needed in the introductory slide. To slide, my flux reconstruction is as close as possible to the datum as any object from the whole h diff. Okay, and the constant is uniform in both h and p. Any other questions? Yes. Because from the potential, you take a bunch around a vertex. And for h t you also take a bunch around a vertex. Is it possible to do batches around edges? We thought about it a lot and I I think that no. I don't have that proof, but I think that no. I don't have a proof, but I think that no. So, in a sense, these head functions, the psi a, we use them also on the continuous level, we multiply by them, say, the h diff function. So it's a piecewise polynomial, you multiply h diff, you're still in h diff, okay? Kind of, it seems that it's the smallest setting you can go. The partition of unity by head function that makes cutoff sum to one and still stay in h1 if you multiply h1. In H1 if you multiply H1, or H diff if you multiply H diff. Okay, so you saw the two constructions at least. I'm happy I made it here. Yeah, maybe some more formulas in that part. So that's more what you've already seen in the introduction, but rephrasing with math. So we've seen this picture: approximation by continuous piecewise polynomials is the same thing as approximation by discontinuous piecewise polynomials. So writing it down in the form of theorem. Writing it down in the form of V or M gives rise to this beautiful VRM. I love it to say: bigger up to a constant, the same thing is smaller. So, minimization over smaller space is up to a constant, the same minimization over bigger space, or minimization over C G, continuous graphing is the same thing as minimization over D G. And this is the full formula. Target function H10, point a degree. I try to get as close as possible to it. With the constraint to be in the space, it's the same thing as without the constraint. It's the same thing as without the constraint. Once again, this has been known with a constant dependent on P, and you can prove that actually the constant is independent of P, and that's maybe the full strength here. And how do you prove it? Yeah, you just take, as we've seen, this element-wise H1 projection, take it as the input of your potential reconstruction, apply it, and then follows. So this is one slide that someone. This is one slide. So, in total, if you think in approximation theory, what does it give to you? So, D space dimension, H10 function. And you take the function, so it's highlighted here. You don't need H2 or H3 or K. The function only has H S K regularity and the S K can go down to one. We are not grabbing point values. And also, And also, this SK is k index for any mesh element, so local regularity, but element-wise. On each mesh element, the function has its own regularity. That's typically the case. Singularities are only somewhere. The rest of the mesh is fine. So that's the setting. And then, yeah, the approximation bound written in all details. So you gain the approximation power in the minimum usual between the Ptolemy degree and the smoothness exponent of the. Exponent of the current mesh element, you gain the approximation power in p, and you have a generic constant that is independent of h and of p. So that would be like a final approximation. So there's a similar story on the h diff level. So again, beer with RM, I like it. Bigger up to constant became smaller. And then you develop all the details. So let's go here. h diff function fixed H diff function fixed polymer degree approximation of this given function with a constraint on the space and constraint on the divergence is uptake constant the same thing is approximation the same target but without the space constraint without the normal component continuity and also you can forget about the diff constraint okay so we've shown it first again with tools that were yeah Tools that were not P-robust, and now we can also prove that it's actually true as a constant that is independent of the polymer degree. So that would be local best, global best in HDIF. Okay. And yeah, it's linked to what Johnny was describing, commuting projectors, but we have no time to show you all the details. Let's just skip it. I've shown the apostle estimates. Estimates. So let's just wrap up, be fast. So we've seen that for Poisson problem with a weak solution H10, yeah, if you have an arbitrary piecewise point guy that satisfies this one equation per mesh variety, kind of lowest order guardian orthogonality, then if you take side UH, apply the potential reconstruction, take this bold fake side, the broken element-wise gradient, and you apply the flux reconstruction. And you apply the flux reconstruction, then this is now the full form of the theorem, including generic f. If f is generic, then you pick up one more term here, which is f minus the L2 autograph projection of f. So it works for general f. This is your unknown error. This is your known estimator. And you can prove the converse bounds. Two estimators lie below the error that you're interested in up to a pure bus constant. And again, it's And again, it's just the results of these H1 potential reconstruction and H2 flux reconstruction. So, this is something that can be used in numerics. I would like to show you this. The application to a priori is theoretical tool, but the reconstructions are practical. So, you plug them into your computer and see some mesh, some polymer degree. You can always compute the estimator ratify, you get a number or any simulation. Get a number. For any simulation, you get a number, and the number tells you what is the relative size of the error in the simulation. But you pick up a known solution so you can see what is the reality, you can compare, divide how many times I overestimate, refining mesh size, refining polynomial degree. You're working nicely. I don't know you, but I can make you sure that up to one or Making sure that up to one or one factor, uh, yeah, this is the bound for you, it's not just a number, so these objects are P-slash polynomials. The estimators are element-wise, so I can say, well, this is what I predict as the distribution of the error in the mesh. Again, test case, I know the true solution, I can compare it. Yeah, now it's eyeball measure, but yeah, they seem to much nicer. So you can use it to do adaptivity, of course. That's kind of the typical. Reductivity, of course, that's kind of the typical use of apostrophe estimators. So you say, okay, now let's refine here, and maybe also, yeah, let's detect: should I go H or P? So you see there's nothing going on. I stay low degree, don't refine. Here I go high P. And yeah, I did not know the solution. It's a posterior, it's not approximate. And yeah, you can typically at least, there's no proof there, you can obtain exponential convergence rates of the. Convergence rates of the practical numerical schemes. Okay, yeah, I was too slow everywhere. Yeah, you just one month slide, I skipped the rest. So what is really the tools that make all of this work? Okay. Johnny, you've been showing things like this. Give me some target and let me get as close as possible to the target on the finite dimension. Target on the finite dimensional world, which is here. So that's what you've been doing for existence. Now it's stability obviously. Okay, so this is on one simplex. The datum is some polynomial living on the boundary. And I'm trying to hit it on the discrete level. And now the proof is that up to a constant, this is possible in the same way as on the continuous level. And yeah, these are the references where. Yeah, these are the references where you show how to do this. The keyword is polynomial extension operator. This is on one single element, and what we did, we extended to patches, which is, yeah, let me skip the hdiff one. Let me skip this one as well. And yeah, let's just finish with the conclusions. So, yeah, if I order things at the end of the day, what all this methodology in terms of a priori allows me to do is allows me to design peace stable local automated projects. To design peace-stable local committee projectors, if I have them, they imply these local global equivalences. I've been showing things, and if I have the local global equivalences, that implies optimal a priori HP estimates. So that's like the a priori branch. The same tools give me a posterior things. I've been showing things in Laplace. Yeah, we did, we did, and yeah, simplest meshes, we did, we did non-matching mesh, mixed meshes. Meshes, mixed meshes, hanging nodes, uh, yeah, h minus one source terms, yes, binds, and many other PDEs, especially for apostoles. And you find all the results are presented in these works. Thank you very much for your construction. Thank you, Martin, for the for the nice talk. Questions, comments? Questions, comments? How expensive is it to compute BSS? Yeah, very good question, I suppose. So maybe let's go here. This is it. So it's local, it lives on patches, and it works with polynomials. And you need to still solve a smaller plush and discrete one on the patch. So you solve many small problems. On the patch, so you solve many small problems, yes, they are completely independent from each other, they are completely parallelizable if okay, we are IJ setting, right? So the message for you, ah, that's that's and all your pictures are the same. They're just one element and the four guys around. That's it. So there are local problems, but they're all the time the same. The right-hand side varies, but the matrix is the same. So you assemble once the matrix, factorize it, and then the evaluated estimate in an explicit way. Have already estimated in an explicit way. It's a bit more tricky on simplicity meshes. Any patch is different, but also there we can reduce it from patch problem to cell problem and some kind of loop around the mesh limits. Yeah, local problems, that's the key word. And yeah, I think that's why there are key robust. It's not construction, it's not expositive construction. You need to minimize, you need to minimize, get the guy that hits the minimum. So that's why there are a lot of problems. So that's why there are a lot of problems. You have to do a pair of parallel, like several elements to one processor, several completely independent one from each other. So, yeah. Just to be completely sure, when you say an element, you mean a direct with all the naming parameters? For the element, there's the simplex, and the problem is written on patches. So, all the elements that share the element, the vertex star. The shared element, the vertex star. I don't know how long. Okay, that's what we see there on the picture. That is that is the locality. So that's the locality, but okay, commenting more, we can always reduce this locality, which kind of has all the elements sharing the vertex, into individual mesh elements. So actually, this is how the proof of this goes. Pick one, solve a problem on the one, and then move to the neighbor. It's a constructive thing. So in practical implementations on simple she measurements. On simple sheet meshes, that's the way to go because maybe you're rather expensive to go on patches and assemble things on patches. You rather do it on and by down, and it's possible. This problem is written in sport moment on the patch on the vertex star. And it is a theorem, it can be equivalently reduced to a sequence of problems serving on individual synthesis. When you do the individual one, you do all the patches adjacent to it. You have to. Is adjacent to it, you have yes, so one 2D is trivial. Pick one, pick one and just move 3D. Yeah, you have enumeration things. How do you move? Oh, I was to ask that you studied this on the parties at some point. Yeah, yeah, we did. We did the meet equation, okay. Generally, what are some of the challenges that appear in that context relative to what so? My viewpoint is as follows: What I've been concentrating about is like the discrete work with polynomial. If you speak about other PDEs, the work is elsewhere. The work is how the norm of your PDE is related to the residual. But the construction of these objects is all the time the same. It's kind of independent of the PDE. If you want another PDE, PDE. If you want another PDE, there are other things, but on the PDE level, how does the error connect to the residual? Since what changes if you go from PDE to PDE is this theorem, okay? But this theorem is kind of independent of all the methodology afterwards. Okay, so the patches and all that. It's all the time the same. And yeah, at the end of the day, I think it's people in some sense do it all the time. People, in some sense, do it all the time, right? I mean, you get some breached data and you want to kind of, yeah, what people would do, they would just average. Averaging works perfectly fine in practice, but averaging goals over degrees of freedom. So it's typically not pure robust, and we want here pure robustness. That's why it's not averaging, it's lot of problems, it's energy minimization. I think the energy reason would be quite so in the previous slide that you had. So in the previous slide that you had before, the chih, is that for the ch, is that supposed to be uh in the broken? Would that be uh? So there are just two applications. In a posteriori, this psi h would be uh. Okay. In application to a priori, we have it here. Yeah, the psi h is kind of the local best, this v1 orthogonal element projector, which is continuous. Which you do it's a theoretical tool and then you apply to it the reconstruction. But when you do the opposite, you apply to UH. And in that case, the first term goes away if you use a conforming methods. Yeah, so I've put you all the framework, but it's kind of once again here. Yeah, maybe to get a first idea, if your method is conforming, this term just goes way, way here. But yeah, I've included this. But yeah, I've included this one because this one is crucial for. Okay, if you do conforming methods, you need this and you only worry about this dual word, but for a posteriori. If you do a priori, well, this is the slide for conforming finite elements here. Yeah, the other one is crucial. And you don't see, so in a priori analysis of finite elements, I go through potential reconstruction. And in a posterior analysis of finite elements, I go through flux reconstruction. So you have. So you have P robust and H robust, of course. What if you do like in the splines and you look at the number of how smooth you are? Is there a question like that? Like if you get, what do you call this in the spline computer where you have so much content knots? Can you have it robust with respect to? Yeah, so was that an interesting problem for slides on splines, okay, but this is too long. This is too long, yeah. So, in a sense, the short answer is the a posterior done and it works. A priori, I don't know if they have a button. Okay, other questions? If not, okay, let's take a look at the stay in the middle.