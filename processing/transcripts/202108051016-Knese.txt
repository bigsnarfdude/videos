Of Vickel, Pasco, and Sola, but I think it makes more sense to give a broader talk. And so I'll talk about three papers of Vickel, Pasco, and Sola and try to find an interesting unifying thread through some of their stuff. And the higher dimensions here is supposed to be spooky. I mean, it may seem a little early for Halloween, but I did just get a catalog in the mail for. In the mail for Halloween supplies. So it is the next major holiday. And so the body of the talk will focus on these three papers of Pickel, Pasco, and Sola. Let's see, this is derivatives of rational inner functions, geometry of singularities and integrability at the boundary, which is in the proceedings of the London Math Society 2018. Then they followed that up with level. Then they followed that up with level curved portraits of rational inner functions, which occurred in this journal with a rather long name, which I won't try to say. I think they called it the PISA journal. And then the last one is where they jump up to three and more variables to study singularities of rational inner functions in higher dimensions, which I think will appear in the American Journal of Math. So, and earlier I put my talk in the chat, so people ought to be able to follow along a little bit. So, the if you get nothing out of the talk here is just a quick takeaway, and I'll sort of reinforce some of these points as we go. But the main thing is rational inner functions generalize finite Blaschku products. But I hope I'll convince you they're interesting in their own right because of. They're interesting in their own right because of connections to operator theory, connections to stable polynomials, and because they have interesting singularities. So, this is the 10-second takeaway of the talk. But with that, let's get into some actual definitions. So, rational inner functions, I'll be focusing on the poly disk. So, these will be These will be rational functions that are so Q over P, Q and P are polynomials. They'll be analytic on the D-dimensional poly disk, which is given here, just not the ball, the main thing to remember. And these will be bounded analytic functions, so they'll be bounded by one inside the poly disk, and the inner part. And the inner part is that they are they have modulus one on the distinguished boundary. And I have to say, almost everywhere, that's even easier in this rational case. So just everywhere except for potential zeros of p on the d torus. And here again, here is the d torus. So it's all points in C D with modulus one. C D with modulus one. And I do want to emphasize a couple different viewpoints. So, rational inner functions, if you, if for some reason you're not interested in them, it seems like a specialized complex analysis topic. I think lots of people are interested in related things, which is what you might call real rational pick functions. And so, if you do conformal maps, Conformal maps, switching out the disk with the upper half plane here, and switch to studying rational functions, and you apply these conformal maps to rational inner functions, you'll get what I might call a real rational kick function. So it maps poly upper half plane to the upper half plane. And it's rational, and it's real almost everywhere on Rd. R D. And so these functions, they're rational functions with all the polynomial in the numerator and denominator all have real coefficients. So it seems sort of like a very practical class of functions. And something that often happens is I like the poly disk for studying global questions because its closure is compact, but the upper half plane is very useful for studying local questions because you local questions because you can you can examine sort of what happens at the origin much more effectively in an upper half plane setting because you have this sort of flat boundary and not the detour okay and like i said in in one dimension uh rational inner functions are Rational inner functions are Blaschke products, and I guess I should say finite Blaschke products. Let me put that in here. And so they factor into these simple pieces coming from Mobius transformations. They're specified by zeros in the disk, and they sort of have these poles that are reflected. Have these poles that are reflected outside the disk, and you can convert these to real rational pick functions and get an alternative viewpoint on rational inner functions, because then you'll get a rational function. And so again, real rational pick function now in one variable will map the upper half plane to itself. And then the property that these functions have is that you have these. Have is that you have these polynomials on the top and bottom with real coefficients, but they have to be real-rooted and their roots have to interlace. And that's actually a characterization as long as you kind of throw in some normalization to make sure your function doesn't map to the lower half plane. But if you take two real rooted polynomials with all real coefficients with interlacing roots. With interlacing roots, I'll just state that, assume you know what I mean by that in an intuitive sense. Then the ratio will either map to the upper half plane or the lower half plane, and just sort of do some normalization to make it map to the upper half plane. And I guess, you know, we often emphasize that when I jump to higher variables, rational inner functions can have singularities, whereas in one variable, they don't. But it's worth emphasizing. But it's worth emphasizing that even in one variable, real rational pick functions do have boundary singularities. They'll just sort of, in higher dimensions, they'll jump up by a dimension. So in one variable, the singularities of real rational pick functions on the boundary, which is the real line, will be, I guess, zero-dimensional, whereas up here, the singularity. Whereas up here, the singularities will be, I don't know what's the dimension of the empty set. Can someone get negative one-dimensional? So there's none of that. But that changes when you go up. Okay, in higher dimensions, you can give a description of rational inner functions just purely in terms of the polynomial in the denominator. In the denominator. And so all rational inner functions in several variables will be of this form: p tilde over p, where, okay, for this to make sense, the polynomial in the bottom has to be non-vanishing in the poly disk. And the polynomial in the top has to have the same modulus as p on the detours. And one way to build a function like that is to take a reflection like. A reflection like so. So I reflect across the disk, and then I multiply by a monomial in order to make this a polynomial. And you may as well assume, or you can, oops, switch to eraser, you can force things so that p and p tilde here have no common factors. And then you get that studying rational inner functions is equivalent to studying a class of. Studying a class of what are called stable polynomials. The term stable polynomial has become pretty vague to mean just sort of like a class of polynomials not vanishing somewhere. But in this case, I'll say you have a polynomial with no zeros on the poly disk that has no factors in common with its reflection. And that makes it even better, which it makes it atoral. It makes it atoral. So P and P tilde, if they have zeros on the detours, they'll be common zeros. And so saying that P and P tilde have no common factors means, you know, they can't have too many common zeros on the D Torus. So basically what happens out of this is that the set of zeros on the D Torus. On the detourus will be common zeros. And so then you're sort of pushing the dimension down. And so the dimension of this set, it'll have pieces with various dimensions possibly, but the dimension will be less than or equal to d minus 2. So you have to go to d equals. go to the equals to to even get any singularities in this case all right and let me again emphasize that you can switch to studying real rational pick functions by doing a Cayley transform and mapping to the upper half plane and so here you'll get It's not, you can characterize these in a similar way, but it's maybe better to use real rational pick functions to kind of define some various multivariable notions. So a real rational pick function will have polynomials in the numerator and denominator with all real coefficients that That don't vanish in the poly upper half plane, but because they have all real coefficients, they can't vanish in the poly lower half plane. So they have this very strong property. And this ends up being these polynomials are in the class of what you might call real stable polynomials. And this is sort of the proper way to generalize the notion of real rootedness from one variable to several variables. From one variable to several variables. And then a real rational pick function is kind of the best way to kind of generalize the notion of having interlacing zeros for higher dimensional functions. Okay, so why study rational inner functions? I tried to say something, you know, on one of my very first slides, but let me be a little more specific and do a little name dropping to just Name dropping to justify it. So one main reason is maybe you're interested in bounded analytic functions. Seems like a natural topic. Any analytic function from the poly disk to the disk is locally uniformly approximable by rational inner functions. I think this is usually referred to as Tara Theodori's theorem, but Rudin's book on function theory in polydiscs. In poly disks proves this for d greater than one. So, this is our replacement for the fact that, I mean, I can't factor out zeros of multivariable functions, but I can approximate them with these simple bounded analytic functions. And as I showed you, there are many. There are many types of stable polynomials. The denominators of rational inner functions end up being these so-called atoral stable polynomials. The real rational pick functions lead to real stable polynomials. And these classes of polynomials have just appeared all over the place in the past decade or so. And here's sort of the name-dropping. See if I can zoom. See if I can zoom. So, if you want to kind of see a survey on stable polynomials, the main one that's been written is by David Wagner. It's in the bulletin of the AMS 2009. It surveys some work around then by Borcha and Brandon on linear maps preserving polynomials and connections to statistical mechanics and combinatorics. And I guess every And I guess everyone here knows that stable polynomials have been used by Marcus Spielman and Srivastava in the Kasman-Singer problem and their work on graph theory too. Stable polynomials also come up in studying multivariable generating functions, which appear in analytic combinatorics in several variables. And what I think Alan even found, there's a recent I think Alan even found there's a recent paper by Sarnak and Kurosov on stable polynomials as a way to generate, what is it, examples of Fourier quasicrystals. So stable polynomials in themselves are interesting and they're just these fundamental building blocks of rational inner functions. So that's why you should study them. More to the point of maybe the conference is that in The point of maybe the conference is that in one and two variables, rational inner functions just sort of interface so nicely with several important theorems in operator theory that they seem just particularly well suited in one and two variables. And let me say a little bit more about this last item, which just falls into the general category of operator-related function theory. Of operator-related function theory. So, a rational inner function in two variables, so mapping the by disk to the disk, always has a special operator theoretic representation. So, there exists a unitary matrix, so it's finite dimensional, and that's very important here, which you write in a block form as ABCD, such that phi is the Such that phi is the like transfer function associated to it, which is just this type of formula here. Just so sad to be, oh, I left out an inverse. That's important. I'm so used to this formula that I assume everybody knows it, but I'm sure if you've seen it for the first time, this is meaningless, but you could think of it as a generalization of sort of a As a generalization of sort of a Mobius transformation, maybe we throw in matrices. But it is built out of this unitary matrix and a diagonal matrix. And it has kind of an interesting history. I mean, so this is stuff mentioned in a previous talk was about Agler's theorem. Egler's theorem on interpolation on the bi-disk. And so that was, you know, Agler proved something like this, maybe for a general bounded analytic function in two variables on the bi-disk, but you'd have to allow for infinite dimensional unitaries. And that the proof involved Ando's theorem. And then later, Cole and Wormer showed that you could get kind of this version with some finite dimensional matrix and also. Dimensional matrix and also work of Geronimo and Wordeman proved something basically equivalent, which was, I can't explain now why there's this equivalence, but they showed, they proved this formula down here, which is a sums of squares formula, which there's an easy way to go between these two things. And also, Baal Sadovsky and Vinnikov kind of proved this. But then, you know, all of we. Uh, you know, all of we could have just gone and looked closer at the engineering literature and noticed that Kumer proved this exact fact around the same time that Agler proved his theorem. And we could have saved ourselves a lot of trouble, but I get a little worried sometimes that they're gonna open up a cave and there'll be a cave painting with this sort of This sort of transfer function formula on it for functions on the bid, but I think we're okay for now. Okay, so rational inner functions in several variables can have boundary singularities, and we would like to understand them a little better. I'll try to say on the next slide why we're interested, but here, just let me show you that this can happen. That this can happen. So, here is what we call our favorite inner function. So, I guess the denominator is sort of the easiest way to write down polynomial with no zeros in the bi-disk, but it has a zero on the two torus, which is at one. And I don't know, once when I was a postdoc and I was giving talks in my own institution to kind of In my own institution to kind of prepare for job talks, I was told I shouldn't lead with this example because it's sort of too much of a Mickey Mouse example or something. People won't take me seriously. But people love Mickey Mouse. So I'm not afraid to talk about it anymore. And so if you go up in more variables, you can write down something similar. And so you get a single. And so you get a single boundary singularity with this three-variable function. You can modify this one in a different way and get a boundary singularity that is one-dimensional here. So it's point e to the i theta, e to the negative i theta, one. This one isn't terribly interesting because you're just replacing a variable with z1, z2, but I'll show you later. I'll show you, you know, later. The paper I'm talking about came up with lots of interesting examples of boundary singularities. So there's no lack of examples. But these are the easiest ones to write down. Okay, so why study singularities of rational inner functions? Why not just approximate rational inner functions with the regular ones, which you could do, I suppose? Well, I think single. Well, I think singularities are where all the action is at. If you want to sort of pin down where interesting behavior happens, you sort of have to pursue the extreme examples. But other places where they occur kind of explicitly, well, they occur as sort of case studies for boundary regularity of bounded analytic functions. And a prime example of this is the paper by Agler McCarthy. The paper by Agler, McCarthy, and Young from 2012 on Carath√©odori theorem and the bi-disk via Hilbert space methods, I think is the title, something along those lines. And so you can use these transfer function formulas to study boundary behavior of just bounded analytic functions in general, but I think a lot of most examples that you study will be rational functions or rational inner functions. The other reasons might be that you should just intrinsically be interested in boundary zeros of stable polynomials. And this kind of comes from the setting of analytic combinatorics, I think, where you might have a multivariable generating function, maybe something of this form or more complicated. form are more complicated and in analytic combinatorics you try to understand how fast coefficients grow depending on singularities of the function and so you have to sort of push out to the first singularities of the denominator and understand the behavior of those and so you may as well normalize things so that you're on sort of the biggest poly disk where this one over p is analytic And you normalize so that that's the unit polydisc, and then you have a boundary singularity. And so it naturally leads to studying singularities of stable polynomials. And let's see. I'm also interested in boundary singularities because I guess I'm not supposed to talk about too much of my own work, but I will mention three of my papers. Three of my papers. I guess that sounds bad, but the singularities come up naturally in the study of this sums of squares formula, which I'm not trying to emphasize it or anything, just sort of believe me, there's a sums of squares formula that's interesting and connect to the transfer function formulas. Uniqueness. So, this formula, there may be for a given polynomial different formulas of this type, and it turns And it turns out that the uniqueness of the formula, in some sense, is governed by how many boundary zeros that P has. So more zeros on the two torus give you fewer choices for this sums of squares formula. And this line of thought is pursued in two of my papers, polynomials with no zeros on the bi-disk and integrability and regularity of rational functions. And I'm sorry, I have to mention another. And I'm sorry, I have to mention another one. I don't think this is too egotistical. Just I'm not going to talk about any. Don't turn my video off. If you study extreme points of real rational pick functions, you're naturally led to special stable polynomials and rational inner functions. And rational inner functions that have singularity. So, extreme points in sort of the space of real rational pick functions will all be related to rational inner functions with singularities on the boundary. And I have a paper on this topic, Extreme Points and Saturated Polynomials, where I prove that certain real rational pick functions built out of stable polynomials with as many zeros as Polynomials with as many zeros as possible on the two torus give you extreme points. Okay, so that's motivation for studying the topic. So how do we study singularities of rational inner functions? Let me count the ways and then I'll focus on two of them. So different ways to study singularities of rational inner functions would be to look at various Look at various measures of boundary regularity and some lines that are pursued in the papers of Bickel, Pasco, and Sola are non-tangential limits, and also studying boundary level sets. So these are also called unimodular level sets. I mean, so the boundary values will all be unimodular. So you're looking at sets where phi is equal. Phi is equal to some lambda, where lambda is on the circle. And they also pursued an interesting idea, which is to look at, you know, rational interfunctions are bounded analytic functions. And so, you know, you can't study their integrability, they're bounded, but their derivatives should tell you something about how wild their boundary behavior should be. And so they've investigated. And so they've investigated some really interesting phenomena about, you know, for which p are the derivatives of rational inner functions in Lp of the detours. And I have to use a fancy p because there's a polynomial p in the background. So you have to pay attention to fonts. I'm sorry. Other possible lines for studying singularities are questions. Questions about pursuing, you know, when classification of various numerator ideals, you can look at, I have this fancy pencil that switches to eraser all the time. Okay. You can look at all You can look at all polynomials so that your Q over P is an L2 and this forms an ideal. And you can try to characterize this because this should give you some measure of how many and how singular the zeros of P are on the D torus and other variations of this concept. So you could look at all Q's so that Q over P is in L infinity the D torus. But I'm going to focus on On these two ideas, boundary level sets and derivative integrability, coming from the papers of Bickel, Pasco, and Solo. Okay, so here's one really interesting theorem from the first two papers that I'm talking about. And let's see, I started at 11:15 and I have till 12:05, so 20 minutes, okay. So 20 minutes. Okay. So here is what happens in two variables as far as derivative integrability goes. So let phi be a rational inner function on the bi disk. So it'll be p tilde over p. RIF stands for rational inner function. And they came up with a numerical geometric. A numerical geometric invariant called contact order, which I'll talk about on the next slide associated to P and its zero set that completely characterizes the derivative integrability of phi. So for a power p one between one and infinity, Between one and infinity, these derivatives will be in L P. The first derivative, or sorry, the derivative with respect to Z1 will be in L P if and only if the same derivative in Z2 is in L P if and only if P is bounded by one over one plus K. I should emphasize that if P has A boundary zero, then so this number k will always be even, but if it has a boundary zero, so that there are actually singularities, this k will be greater than or equal to two. So these derivatives can be at best in L to the three halves. But if you have, if this But if you have, if this numerical invariant, whatever it is, which I'll talk about next, if it's even bigger, then you get even worse integrability. So what is this numerical geometric invariant that's so scary? Well, it's called contact order. And it's this interesting quantity that turns out to have two interesting but equivalent interpretations. Equivalent interpretations, and it was proven in, I think, the second paper I'm talking about that the two interpretations are equivalent. So the first interpretation is this one. It's kind of, I don't know if I'll go through formal definitions of this because I hope to just give kind of a geometric sense. It's the fastest rate that branches. The fastest rate that branches of the numerator zero set approach the two torus. So I'm looking at phi, this rational inner function, p tilde over p, and I have a boundary zero, which will also be a zero of the reflection. And suppose I could parametrize the Parametrize the zero set of p tilde. I mean, you can't always do this, but let's pretend that we can. And I'll parametrize it for zetas on the circle. So for each zeta on the circle, I'll get some zeros inside the disk in the other variable. And the contact order is going to be the fastest rate that you can get for. That you can get for how this piece approaches the boundary. So you can show that one minus the modulus of this zero as a function of zeta has some growth, so zeta minus one to some power k, and you find the branch where this k is the biggest. And you can plot this often with. can plot this often with these sort of plots where I view zeta as e to the i theta and and you look at how the the modulus of the second component is changing and you're kind of looking at how closely it touches the line y equals one say and so this sort of rate of approach of the zero set to the two torus and To the two torus, and you look at sort of the highest order approach, that gives you this number k that was in the derivative integrability theorem. But they came up with another interpretation, which they initially call order of contact, and then prove contact order is the same as order of contact. So, you don't, so it's like a, it's commutative then, I guess. Um, that uh you can compute it in this beautiful. You can compute it in this beautiful geometric way that first you have to understand what the boundary level sets of rational inner functions look like. And so I've drawn how I think about them. So I have this rational inner function with a singularity at 1, 1. And so I'm switching to kind of polar coordinates here, theta 1 and theta 2. These boundary level sets occur in what they call horns. In what they call horns, I kind of view them as sort of like they're like locks of hair. They appear in these bunches. So different values appear in they appear in these sort of bunches together. And you look at sort of the highest order of bunching. So you have all these level curves kind of in the same. Level curves kind of in the same bunch here, and you look at how closely they contact each other as you pass through. And you do this over all the different bunches that pass through the origin. So it's really amazing that you could get this concrete picture of what the level sets look like. And it looks just like this picture. I'll show you actual plots in a second. And another remarkable thing. Um, and another remarkable thing, you know, to understand these boundary level sets, they proved that they actually break up into analytic pieces. So there's no reason that you can parametrize an algebraic curve with an analytic function except at a smooth point. So, what they prove is that at a possible singularity, the zero set here breaks. The zero set here breaks up into a union of smooth pieces, which is not in general guaranteed. You may have to pursue things like Puissot series. And so there are these two interpretations of this contact order, and you can use them to address derivative integrability. I think the second one makes it easy to see visually what contact order has to do with derivative. Contact order has to do with derivative integrability. So if you stare at one of these, at this sort of these bunches that you have here, what's going on is that the rational inner function phi is kind of going through different values in this bunch. And if these curves here are really tightly, if they're If their difference has high order of vanishing, then the function is sort of changing values very quickly in this tiny region. And so you would expect that the integrability of derivatives would be affected by this. So if it bunches, if the values change very quickly in this very tight bunch, then the derivative of the function is going to be wild. Is going to be wild, and so its integrability will suffer. So it's a very nice way to see how the pictures show you what the integrability of the derivatives will be. Let me show this kind of with a concrete example and then show more pictures. So, the favorite inner function was this one, it's so simple. This one, it's so simple that you can actually write down the zeros of p tilde, just, you know, you can parametrize them in terms of one of the variables. So I can write z2 in terms of z1, and then I just do exactly what I was saying earlier. I look at the order of vanishing of one minus z2 squared on the zero set as I approach z1. As I approach z1 equals one, and you just compute this, and you get that it grows like one minus z1 squared. So the contact order here is two. And so you get that the first partial derivatives are an LP if and only if p is less than three hex. And you could do this by looking at the boundary level sets too. By looking at the boundary level sets, too. So, here's what the boundary level sets of this function look like. I've just sort of drawn them here. I think, let's see. So phi equals one actually gives you just this diagonal line. And then other values will give you sort of these curves that contact it with order two by this connection up here. There is stuff. Up here, there is stuff to worry about, which I haven't talked at all about, which is there are going to be special values that behave a little differently. And these, there's two types of sort of bad level curves. There's value curves corresponding to the value of phi, let's see, phi's non-tangential limit at a singular point will give you weird behavior. And there's also some exceptional. And there's also some exceptional curves that they discovered that, so you have to kind of do examine this bunching business generically because there's, they showed that there's like two special values that you can't apply it to. But okay, let me show you some more pictures of this phenomenon. Here's a more complicated example. It looks like one of the pictures that I drew, but maybe you're more convinced by computer plots. More convinced by computer plots than my drawings. So this shows the level curves which appear in these bunches. And in this example, I think the bunches, they're all sort of their difference vanishes to order four. And then you can also see that in these kind of moduli plots, you look at the roots of p tilde, let's see, as a function of, this is how I. Of, this is how I described it earlier, as a function of Z1 on the circle. And you look at how the modulus of Z2 changes. And so these are heading up to one at it's sort of contacting this line, y equals one with order four up here. Okay, so I finally get to some of their higher dimensional stuff. Dimensional stuff. And we could go through a little bit of proof here. The one of the cool higher dimensional theorems that they have, and I'm really just kind of scratching the surface of the third paper here, it gives you a great way to start analyzing these problems. So if I have a rational inner function in d variables. D variables. I can examine something very similar to what I was writing before. I define this distance function. It'll be a function of points on the d minus one dimensional torus. And so it's the distance between the zero set of P tilde intersect zeta cross D. Zeta cross D. So you're looking at zeros living in the D minus one dimensional torus cross the unit disk. And so you're coming up with some kind of measure of how the zero set of P tilde approaches the d torus. And I need one other definition in order to state their theorem. You define this set omega. Find this set omega x, which is you look at points where this distance is small depending on x. So I look at zeta in the d minus one dimensional torus, where this distance is less than one over x. And then you can use this set omega x and its Lebesgue measure to study the L P. The LP interval. So you get that for P between one and infinity, the partial derivative with respect to the dth variable is an LP, if and only if this just one variable integral is finite. So at least transfers the problem to a geometric question of understanding how the Lebesgue measure of this set omega. Of this set omega x grows. And let's see, I've got seven minutes, so I could, I think it's useful to see how this theorem implies the d equals two theorem. I was thinking about going through a little bit of the proof of this theorem because it's a nice proof, but maybe I don't have time, so I'll just But maybe I don't have time, so I'll just refer you to the paper. But it is something that you can sit down and read in a few minutes. So, but I think it'll be useful to see how this theorem actually implies their two-variable theorem. See, I have this sneaky way to hide stuff. So, here was the two-variable theorem. There's this notion of There's this notion of contact order, and it determines the LP integrability. And so, let's see how this higher-dimensional theorem implies the two-dimensional one. And then I'll show you pretty pictures after that. Let's see. So maybe I'll Okay, so I'll go a little quick, but hopefully I'll give you some idea because I've already said some of these points. So contact order K means that you have a point with, and I'll assume without loss of generality that it's 1, 1, where the polynomial in my rational inner function vanishes. And let's just pretend I can parametrize the 0. Can parametrize the zero set of p tilde by an analytic function. You can't exactly do that, but there are some replacements that you can use. And the whole point of contact order is you can find a point where the rate of approach of this zero is on the order of k. So theta to the k if I write z1 as e to the i theta. Okay, and then what comes in. See what comes next. So then this means that that delta function in the higher dimensional theorem is roughly one minus theta to the k near the singularity one. And then you can examine this set omega x directly. So again, it was these points where delta is less than one over x. And just from this relation, I don't want to belabor it, but I don't want to belabor it, but you can show that then this set omega x is of the size x to the negative 1 over k. So there's a little something you have to do here. But then once you know how omega x, how its Lebesgue measure grows, then you just insert that into their multivariable theorem. And then that spits out the integer ability. So, this, let's say, the derivative with respect to the second variable is in LP if and only if this function, if this integral is finite, I plug in this approximation and you just have to check when a certain integral is finite and ends up with the right bound. With the right bound that p is bounded by one plus one over k. Okay, so that's some sort of a proof. I was gonna talk a little bit about the proof of this higher dimensional theorem, but I'll skip it. But let me get to kind of the cool examples that they construct. And they had to develop lots of machinery just to understand the higher-dimensional examples. So I'm leaving out, you know. Examples. So I'm leaving out, you know, a big chunk of that paper. But I think the main takeaway from the paper is that they were able to construct and analyze lots of examples with various different behaviors. But just to emphasize some things about derivative integrability for a rational inner function, so in d equals two, they discovered that this derivative integrability is determined by a single. Derivative integrability is determined by a single numerical invariant. It works for integrability, it is the same for the two variables. They showed boundary level sets of rational interfunctions consist locally of unions of analytic curves and that the sort of bunching of the branches determines this numerical invariant k. And then they showed that the d greater than two is the wild west. Not clear how the geometry. Not clear how the geometry of the zero set of p tilde or the boundary level sets relate to derivative integrability. You can get different integrability for different variables. Boundary level sets in variables three and higher break up into different dimensional components. You can have lines and surfaces, and they don't have to break up into smooth pieces like they do in two. Into smooth pieces like they do in two variables. So I'm not going to go through computations in the interest of time, but you can build an example of a rational inner function with different derivative integrability for different variables. Here is one of the examples I wrote earlier, and they showed that the That the derivatives are in L P if and only if P is less than two, which you know, this doesn't fit the kind of pattern in two variables where you had this one over one plus K. This K is an even number, and so the first cutoff is at three halves. They found that in three variables, you can have functions that are in LP up to two and And, you know, again, it's not clear how all this relates to the geometry, but this function has boundary level surface, surfaces that are smooth except for the boundary level surface phi equals minus one. So there's sort of one bad one. But they came up with many, many other examples. I'll just sort of show you some of the pictures. They came up with an example where the boundary singularities consist of Consist of three curves and the boundary level sets, they all contain two vertical lines and they contain a surface that has a singularity and that singularity changes for different boundary level sets. So it seems very complicated, but hopefully there's some way to understand this geometry. Okay. Okay, and maybe I'll skip all the pictures and try to end. So, the main problem in several variables now, they have all this machinery to understand lots of these examples and build examples, but it's still very interesting to try to develop a coherent description of these zero sets near a boundary singularity and to understand the geometry of the boundary level sets of a rational inner function. And then, just one question I've always had. And then, just one question I've always had is: do the singularities of rational inner functions interface with any of this operator-related function theory? I just wonder if you can construct interesting examples based off of singularities alone that explain some things that happen in higher birds. That's where I'll stop. Thank you. Thank you very much, Greg. Questions for Greg, yeah, very nice talk, Greg. I haven't looked at this for a while, but is this stuff any is it useful for