Is well predicted, you can say something about the specific region. So you can say that basically the specific region is responsive to some kind of information that's captured by the feature space representation. And this framework is very flexible because you could choose basically a feature space according to your hypothesis about the brain. For instance, if you're interested in language, you can think, construct a feature space. You can think construct a feature space for syntax and a feature space for semantics, let's say. Some feature spaces, however, are hard to construct. It's hard to just build a feature space of the meaning of words in English. So that's why, and especially because of the improvement in deep learning in recent years, people have been very interested in using representations from neural networks in many domains, for example, in language. And so these feature spaces will be less well defined and more implicitly defined. And more implicitly defined. And in the domain of language, there's been literally tons of papers just following this approach in the recent years. But also, this is not just in language, has been even more work in vision, comparing representations, let's say in neural networks and brain activity, and also in other domains like audition, for example. Even though I really like this encoding model approach, I still think. Encoding model approach, I still think it has one important limitations, which we do not talk about enough, which is that it's not hypothesis-free. So, whenever you want to construct an encoding model, you do start with a specific hypothesis, like to determine whether you want to pick, you know, semantic feature space or a syntactic feature space or whatever it is you think is important for brains. Even when you pick, let's say, a layer from a neural network, from a convulsion. Layer from a neural network, from a convulsional neural network trained on ImageNet. You still have a hypothesis. For instance, if that network was trained on an image classification task, the representations it learns would be good for classifying objects in images. And so if you use that representation to predict brain activity, you have an embedded hypothesis in there. And so that might be a good hypothesis. Maybe the brain is kind of identifying objects is relevant for the brain. Identifying objects is relevant for the brain, but you might also be adding strong bias and missing something important for the brain by kind of pre-specifying the hypothesis in this manner. So another approach could be characterized as end-to-end modeling. So you can think of this as an almost assumption-free approach to reverse-engineer the brain. And so the idea here is that if you have enough. If you have enough data of people doing tasks in this manner, can you train an algorithm right away to predict brain activity? For instance, in the case of language, imagine that you had an endless stream of data of people processing language. If you train a neural network to predict brain activity right away from the text that the subjects are processing, hypothetically, what needs to happen to predict the activity optimally. The activity optimally is that you need to learn something about how the brain is combining the words together because those operations are what's creating the brain activity. So in order to predict very well, hopefully what you'll do is you'll be able to extract some of the principles that the brain is using to combine words together. And these models that you'd learn with these principles in them might actually lead to better AI models. So here's like kind of an implicit assumption. kind of an implicit assumption or bet that if you constrain an algorithm to predict green activity you it will gain some knowledge that will make it better um because uh and just an example for for language for instance if combining specific words together leads to another meaning that's not contained in any of the words the brain will react to that will react to that meaning and for you to be able to predict the brain activity optimally you will need to learn that when you combine these two words together you get this These two words together, you get this other meaning. Of course, this is like in an ideal case, but maybe there's like some considerable things that we can learn just by predicting brain activity from scratch, from the actual input. And these networks will not just be useful for understanding brain activity, what the brain is doing, but hopefully there'll be better AI models. Maybe they'll be more accurate, more efficient. Models. Maybe they'll be more accurate, more efficient, or more generalizable to different settings. So, I'm going to give you three examples of this broad line of thought, which kind of introduces training algorithms directly on using brain activity by constraining them or by directly predicting them. And I'm going to go through three different examples. The first one is by actually by Alon Fish, who's one of the organizers here. Was one of the organizers here, where she constructed an embedding that's constrained from using both text data, a text corpus, as well as brain activity. And I'm going to also talk about other methods. Like, for instance, if you have a very large network that you don't have enough data to learn, enough brain data to learn just directly, how would you be able to train it on a large corpus of text, let's say, and then. It on a large corpus of text, let's say, and then fine-tune it on brain data. And I'm also going to give an example about a network that's entirely trained on brain data from scratch and whether that is viable or not. Okay, so starting with Alana's experiment, she was interested in building a vector space model of semantics that captures the meaning of words. So you can think of this as a matrix of co-occurrence frequencies between different words in English. Between different words in English. So, this captures over here: every row here is a different word, and every column is how often does it occur with different words? So, this captures the meaning of different words because, you know, different words occur with different probabilities with others according to their meanings. However, there's definitely redundant information here because some of the words are similar to each other. And so, the task here is to learn a lower-dimensional or low-rank approximation of this matrix that can. Approximation of this matrix that captures most of the information in it. And this matrix A here would be your new latent space that captures the meaning of every word. So one way that Alana uses to draw this is to think of this matrix D as a transformation from the original space into your latent space that you want to learn. And so, for instance, you can, in this original paper, they had around 10,000 words. They had around 10,000 words in the corpus that were selected. And they learned a transformation from that corpus that had a much lower dimension. Now, the question is, would that learn space, that learned latent space, become better if you constrain it to also be relevant for brain activity? So for some subset of these words, For some subset of these words, the authors had brain activity recordings, fMRI and MEG recordings, of people reading those words. And the idea is this space that you want to learn has to capture not only the corpus data for all the words, but also this additional brain activity for the subset of words. And this is really cool because you do not need to have brain activity for all the words in your corpus. You can just use a subset of them. subset of them and using a specific set of a specific optimization procedure the authors were able to basically come up with a latent space that had very nice features for instance this particular text and brain model was actually better at doing tasks such as decoding new images of new brain activity images. So you have subjects reading words that are held Words that are held out, and you're trying to say whether the word is, let's say, an airplane or an apple, etc. And actually, the vectors that come from this text and brain optimization are better at this task than the ones that are only optimized on text. And even cooler, these brain and text factor space models are also more similar to the human behavioral judgments. So So the authors had access to a set of questions, 218 questions, that were labeling the different words in the data set. And using this kind of ground truth, they were able to establish the ground truth semantic distance between those different words. And it turns out that actually the vectors that come from both brain activity and text are actually closer to those behavioral ratings than the vectors that only come from text. The vectors that only come from text. And it's interesting because, like, text is also a behavioral measure. It's like people actually producing tests. But it turns out that adding brain activity to the mix produces better vectors of word meaning. So this was kind of the first example about that. I think Alana also has more recent work following this, the same axis of thought. But I'm going to give you another example. But I'm going to give you another example, which is also another way of constraining or changing a network with brain activity. So in this work with Dan and Maria, what we were interested in is to see if large language models such as BERT, which is this really large network that's trained on a lot of text and was back then was state of the art, whether how does it change if I actually make it also I actually make it also predict grain activity. So, here I'm not just, as I explained before, I'm not just extracting a vector and building a separate model. Here, I'm actually going to change, I'm going to add the prediction head, and I'm going to change the network in order so that it predicts better, let's say, the fMRI recordings of one subject or the MEG recording of the subject, etc. And for this experiment, we had data from people sitting in a scanner and reading a chapter from Harry Potter. And we had both fMRI and MEG data for Both fMRI and MEG data for these subjects. So, the idea is that we're going to take a model, a bird specifically trained on a large corpus, and we're going to change its weight so that it's better at predicting one of those data sets. And the main idea here is that if after we fine-tune this network, it becomes better at another task, then whatever the network has learned from the first, from the brain activity. From the first, from the brain activity is actually relevant for the other tasks. And there's been some transfer of knowledge between from the brain activity to this other to that to the network being able to solve these other tasks. All right. Okay. So I don't have a lot of time to go into details, but I can tell you at a high level what the results were. So here I'm showing you different subjects in different colors and also. Subjects in different colors. And also, I'm showing you the voxels ordered from the most accurate one to the least accurate one. And what I'm showing you here is the improvement in prediction performance after of brain activity of the fine-tuned model over the vanilla-burton model. And obviously, after you fine-tune a model to predict a subject's activity, it becomes better at predicting the subject's activity on held-out data, which is not surprising. But the cool thing is that we Pricing. But the cool thing is that we also saw this improved performance for all subjects except this one when transferring between subjects. So I will train on one participant and then use that network to predict another subject as participant's data. And I still see an improvement, even though we're not training on the same person. So that allows us to say that maybe the network is transferring some information about ephemera data related to reading. But we want to do something that's kind of more general. But we want to do something that's kind of more general than fMRI. So we looked at what happens when you transfer between from MEG. And here the pattern is not as clear. There doesn't seem to be a consistent improvement after transferring from MEG. But actually, if we look at which voxels do improve and which voxels do not improve, maybe are overfit, we find that the regions that do improve are located in the language region. So maybe this is not a noise pattern. So, maybe this is not the noise pattern. Maybe what's happening is that the language regions are actually improving due to the MEG transfer. And so, this is kind of a like a primary, preliminary evidence that there have been some transfer of information between MEG and from MEG into the network that is relevant not to specific modality, but to language processing. However, in this particular case, we did not see a transfer to NLP tasks. Not see a transfer to NLP tasks, so we were not able to perform NLP tasks better using BERT after transferring from after this fine-tuning with brain activity. So we're still not there in terms of changing a network, changing the specific network so that it's better at its task. So let me give you the third example. And so here we're going to build an end-to-end model directly without going through this fine-tuning stage or the encoding model stage. Coding model stage. In this case, this is a visual experiment. So we're going to take, we're going to try to kind of train a network from starting from images, natural images that subjects are seeing, and trying to predict the brain activity related to what they are seeing. This is joint work with Inakshi, who at the time was a student at Cornell and is now a postdoc at BPS in MIT. So in So, in this particular case, what we needed in order to train these networks, these large, I mean, the network on direct data and predicted brain activity is enough data. And so we're lucky to work with the NSD data set, which has actually 10,000 images, 10,000 unique images for eight subjects. And each of these images is actually even repeated three times. So these are like clean 10,000 images. 10,000 images. And I mean, still, you might think maybe that's not enough to train a neural network from scratch. So, this is what we ended up obtaining. I'll tell you the result in a second, but let me just give you a bit more detail. So, here what we did is we trained a specific architecture of a specific neural network architecture. Fortunately, I don't have time to tell you all the details, but I do want to tell you that we trained this network to predict the activity in different Network to predict activity in different in four different visual ROIs. So these are high-level ROIs that are selected for one category. These are extensively studied ROIs. So the FFA, this specific or the selective for facet, the visual word form area, which is sensitive to letters or the letterbox area of the brain, the EBA, which is sensitive to body parts, and RIC, which is visual. And RC, which is Plix processing area. And we're just going to individually train these networks for these different areas. And what we end up with is actually pretty good prediction performance close to the noise ceiling for the different subjects in the different areas. So these are, remember for each subject, we train a different network for each one of those areas. And we also do other experiments in which we look at transfer performance between subjects themselves. So, this is pretty encouraging that we're able to predict well the activity in those different regions, even just without pre-training on a big corpus at all, just from the images that are available from that experiment. So just a pair of image and associated fMRI activity. But the cool thing about this approach is that now that after you train, you have this network. So you don't only have, you can look at more than just prediction performance. You can actually go and invest. Prediction performance. You can actually go and investigate the network and see what is going on, what is it using to predict. If I give you a specific image, what in the image is causing activity? Or what is like the part of the image that the voxels are most responsive to? Or what kind of image, like how do we know what kind of image, like what are the properties of the image that lead to a specific activity in the voxel? And in order to do that, we used the network dissection procedure by David Barr, where the high-level idea is that The high-level idea is that for each input, for each image, we can kind of estimate based on for a given voxel, based on the model that we learn, which regions of the image activate this voxel. So you can come up with some form of a mask from the network for a specific image. And then you can look for that same image in any data set. So the image doesn't have to come from the experiment. It can be from any data set. So we use the Set. So we used a specific one here that was segmented and where we know for each pixel, like what kind of object does it have, what kind of texture does it have. So the images are labeled. And so you can take for each one of the concepts that you have, you can take like the overlap between the segmented part in the true, the ground truth segmented part, and how much the voxel was attending to it in some sense. And that allows you to get for each voxel a measure of which concepts. Of which concepts it is actually kind of selected for or activated by. And you can also do this for different nodes in your network. And here we do an analysis in which we kind of capture nodes that are selective for a specific concept, like more selective than a specific threshold. And we see that actually these networks that are optimized for these different visual areas, they're actually much more sparse in terms of the Much more sparse in terms of the concepts that they are interested in. You can compare them, for example, to those different random networks over here, which are much more distributed. And the cool thing is, I mean, this is what you expect. For instance, in FFA, you see that it's definitely a head selector, which is not a surprise. The visual word form area likes signboards, which have letters on them. You can see also it's a head selector. This is probably because there's some overlap between the voxels and visual word form area and FFA. And visual word form area and FFA. The body area likes people and heads. And RSC likes windows. This is one of the interesting things we've discovered that's for some reason, maybe windows are really important in the image. It allows you to say whether you're indoors or outdoors. I'm not sure yet about that, but it turns out to be an important thing. And you can see this also, you can go back to the original images and kind of tell for like each one of those networks which parts of the image does it like. The image doesn't like, and I mean, like, this is just cool to see that, like, yeah, I mean, you build basically what the network has become-something like a face selector. Um, and the visual words from area network is like a letter selector. The body area network is a body and head selector. And here again, this has this weird Windows selector. Something about Windows makes it important for RC according to this. Important for RC according to this metric. Definitely more is needed here to understand this. And another thing we can do also is to kind of go and do another, once you have the strain network, what you can do is you can try to optimize the image that mostly activates different voxels in those areas. And this is what we get. We get, for example, in FFA, we have these kind of concentric circle kind of shapes that activate most these. Shapes that activate most of these voxels. The RSC has very rectilinear features. The visual art form area has these weird kind of streaks that maybe look like an eight or a zero, like something like writing, which is also another way to go and investigate these networks and see what features are important for these different pain regeners. There's definitely more details about this work, but I mean, it's also nice to give a shorter talk. Nice to give a short talk. So, I like the format of this workshop, so I'm just going to keep going on, can discuss it later. But as a conclusion, here, what we're saying is that these hypothesis-free computational models not only accurately predict brain responses, but can also reveal like patterns of semantic selectivity, but also like different features that specific voxels could be useful for, could be sensitive to. And with more investigation, we can also try to start seeing like what are these different regions doing? Saying, like, what are these different regions doing? Are they being only like detectors of faces, or are they detecting a specific spatial pattern? How much information is being processed in these different regions? So, these are the three experiments I wanted to talk about. I just want to end on some ideas for the future. I'm quite excited about this direction. I think other than these image vision experiments, I think this is also. Vision experiments. I think this is also going to be exciting in language, where we have not only people looking at images, but maybe people reading text or engaging in conversation. So a very large data set of such language data will be very useful for building these kinds of end-to-end models for language. So we're definitely working on this, collecting this type of data. This is in collaboration with Alex Huth, who's also in this workshop. In this workshop. But I think just collecting the data will not be enough, obviously. We're going to have to, in general, just be careful because there's going to be a lot of difficulties along the way just because of the kind of things that end up being important in the data set. When you have a continuous language stimulus, for instance, there's a lot more that comes into play. And we're going to have to kind of separate different elements into like this. Different elements into okay, this is relevant to language, this is relevant to maybe working memories, is relevant to some other things. And for that, I think the most exciting thing about this for me is that there's this rich connection between these specific topics, like for example, the neurobiology of language, and other AI subfields, like, for example, natural language processing. And sorry, okay, so that's what makes it a very exciting time for me. And this For and with this, I'd like to thank you for listening and take any questions. Thank you very much. Lovely talk. All right. Let me just see if we have any hands raised here. Oh, Deiying. Sorry, Guillaume, would you like to ask a question? Are you clapping? Okay, sorry, I'm mixing clapping and hands. Alona. Sure, yeah. Thanks, Layla. Great talk. Sure, yeah. Thanks, Layla. Great talk. So, I also have done a little bit of work on vision because it's just much clearer how to segment an image. And then have you thought about how to transfer what you've done in the image space to language? Like, what is the parallel to the image segmenter that you've talked about or the network dissector or something? That's a good question. Yeah, I guess it would be nice to have something like that, like a dissection of topics in the semantic space, probably. Yeah, that's a great, great question. I have not thought about it a lot. I think definitely, yeah, I love the idea though. I think. I think I might have, Alex might have discussed something like this in the past also. Like once you, because these voxels, once you have a language model, the voxel, the model you end, sorry, once you're working in the language space, the models you end up learning are approximate semantic space, so you can think of clustering it in a specific way and trying to identify. And trying to identify, yeah, definitely, maybe regions in the brain that are selective for specific concepts and other language features which we not even know about. Like it might be that we organize language not necessarily semantically, but through some other features. Sarah, do you have a question? Beautiful, beautiful words. Beautiful, beautiful work and beautiful talk. Thank you. So, to follow up on Alona's question, you know, and this question will probably reflect just how ignorant I am about MRI and working with human subjects. But going back to the analogy with the visual work, wouldn't it be interesting to do segmentation of actual speech? You know, sort of because that's a big problem when people are. A big problem when people are talking and you want to translate that into written language through the segmentation. And would your brain data or your neural data have sufficient time resolution for allowing you to do that? So I don't think the fMRI one would, but we also collect a lot of MEG data. So in MEG, yes, you do have the resolution. You would have one millisecond resolution, which you don't even need that much to do the segmentation. And these kinds of things, the segmentation and these kinds of things uh like uh like local locally like things that change in time fast fast in time like big changes in time uh are things that I think are easier to detect in MEG like MEG is a good tool for such things so that would be definitely an interesting direction as well like trying to segment or rely on temporal segmentations Max? Yeah, hi, thanks. That was cool. Since this is like, you know, a dynamics workshop, I just want to ask about the, so there's some semantic content and then sort of syntactic structure and how sort of language is organized in terms of words and sentences and stuff. And I recently looked at I recently looked at some topic models that try to extract topics from text, and it turns out that the sort of standard ones now don't actually include any syntax structure. So it's just about sort of occurrence of words and not so much sort of reading from left to right or the dynamics or intonation. Do you think that this imaging approach can sort of capture the sort of dynamical Dynamical aspects of semantic relationships, I guess, as opposed to just semantic associations. Like you might have a trajectory of language that has a sort of unique signature that because of the sort of way that you intonate the words and they sort of phonetically come together or something like the liaison in French or something. Something like the liaison in French, or something that has like a unique dynamic component that helps structure words or make associations between different words more or less likely. Yeah, that's a great question. So there's definitely some constraints given, like, for example, such as, for example, you can't, when you're listening or reading a book, you only have information about what you've read so far. Information about what you've read so far. So there are some sets of models that you cannot necessarily use because they take information from the future. But given that, architectures like the transformer are really, really flexible. And given enough data and enough signal to noise, the bet is that you would be able to detect these kinds of relationships between words that are a function of the words itself. Of the words itself. So, whenever you get these kind of patterns, like a liaison or something like that, you would be in a kind of a different regime which allows you to connect different pairs of words from your recent history together. So, like the idea behind using an architecture like the transformer is that you will be able to learn these dynamic connections that happen. And by dynamic here, it's not necessarily a function of time, it's more a function of the actual word or the actual location you are in. Or the actual location you are in in a sentence. Thanks. Alona, do you have a question? Yeah, good. I was wondering if maybe you could say a little bit more about like what does hypothesis free really mean? Because there are some hypotheses baked into anytime you put them all together. So yeah, great question. So definitely I was being a little trying to get away with it, but we are. Away with it, but uh, we are using a convolutional neural network, so that's already a big hypothesis. And um, like, there's a different, it's not just that modeling choice, there's a different parameters you have to choose also. So, definitely a lot of hypotheses there. I guess I was focusing on the notion of hypothesis from that's more about the when you design an encoding model or even design an experiment. So, if instead of collecting a large data. Collecting a large data set of natural images, like the NST data set, you can start with a hypothesis: oh, I just want to look at tools versus animals, let's say, that will constrain the data you collect. But then let's say you just look at natural images. You're not constraining, you're not using these kinds of hypotheses, but you're using a naturalistic data set and an encoding model. But you use, let's say, AlexNet that's trained on objects. Um, object detection or object classification, then that is your hypothesis, too. You're saying that you know, whatever feature is important for object detection is also relevant for the brain. And you can kind of, it's kind of interesting. You can, you can change the task that you that you have in the network. And we have work with my student Arya that does that. And so, if you look at networks that are trained with different objectives, like segmenting an image or Somantic an image or classifying places or kind of 3D orientation, things like that, you will get different features in the network. So, just because you're in the network doesn't mean that the features are insensitive to the task you train the network with. So, when I say hypothesis free, I mean free of those assumptions or those kind of fixed pre-specifications. So, it's not a two-step approach where you first get the features or you train a model to get features and then you predict. And then you predict brain activity. You learn a model to predict brain activity. You do both at the same time. There's also a question in the chat, but I think it's just asking for the paper, if the paper with the face detectors has been published or if it's available somewhere. Yeah, not yet, but I hope. Yeah, not yet, but I hopefully very soon put it out on the market. Yeah, great. There's another question in the chat here. It says, is there sense to using adversarial inputs to better understand the learned representations? And what might adversarial inputs be for text data? Yeah, I guess adversarial inputs might show a limitation either in the model that you've trained or kind of some confusion in the brain itself. So I'm sure that's like a smartly designed experiment to kind of come up with that would use them. In the case of text data, I guess it would be whatever. I think that the difficulty in language is that we don't have these very nice, very selective regions that are as well identified as, let's say, the face area, the place area. We're more or less getting there as a field, but it's still kind of not as clear. So, but let's assume you had a specific region that you thought was very relevant for a specific concept. A specific concept like faces or humans, then I guess that an adversarial example would show you or would prove to you that you're actually like, let's say a region, you think a region is relevant for something like social concepts, but it turns out that actually it's sensitive to a specific kind of category of words that are very frequent in social. That are very frequent in social concepts. So, hopefully, the adversarial setup will help you identify these things. So, it would be kind of word, other sentences that capture the same thing, but in a different way. Oh, and there's another, sorry, chat question here. It says, just to make sure I understood the results of the upcoming paper. Of the upcoming paper, constraining the networks to predict EGFA neural activity and images drove them to become face detectors. Almost. So we're only predicting the neural activity, and then we're going and back and investigating the network. So we're not predicting any images, but we're predicting only a brain activity. And then when we inspect those networks, we find that they are face detectors. And the way we find that there are face detectors is we look at basically which Is we look at basically which regions in a given image most activate these networks. And that's why I was showing the images with the attention maps on. Great. So I think we're just out of time, but I think we're also out of questions, so that we're doing well. So thank you very much. This was just really, really interesting. And we'll move on to our next speaker. Speaker. So, if you want to stop your share, and I'll get Max to share. Sorry, Sue, just to let you know before we start, I will be recording again in Zoom as something that's happening with our live stream again. Okay. That will not stop at this time. I will just keep it going. Okay. And Max, are you fine with us recording your talk? Yeah, that's good. So we'll be doing that. Good. So we'll be doing that. Okay, so great. We'll move on to our second talk of the set, Max Puelma, and Tuzel, and go right ahead. One turn in. Great, thanks. So, hi, everybody.