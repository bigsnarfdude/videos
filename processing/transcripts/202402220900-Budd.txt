So I'm going to talk about joint work with Lisa and Tio, who are both here, and we're part of the Math Deep Learning project joint between Universities of Bath, Cambridge and University College London. Right, so my title, Adaptivity, Explicit, and Neural Networks. I'm going to start with talking about a much younger subject and then focus down into Focus down into a smaller area we're looking at, and that's the subject of pins. So, many of us, if not most of us in this room, maybe all of us in this room, have been working in PINs. And PINs came around, you know, the first papers were in the 2017, 2018, and so on, advertising machine learning as a way of solving differential equations. Differential equations. So, everything we've done so far, development of cutter methods, BDF methods, finite element methods, finite volume methods, spectral methods, is all now redundant. We can replace it all by machine learning. Or can we? But I mean, the thing about pins is that they offer the promise of, say, solving a PDE without having to go through all the f of formulating a mesh and a lot of the problems. The mesh and a lot of the problems that you have to do. So, to kind of state the obvious, I think for most of you, what is a pin? Well, a pin is a solution of a differential equation essentially created using some sort of deep neural net. So, we have an input x. I'll just be thinking for the moment about differential equations and spatial variables. You put in x. You put in X, you go through the neural net of linear and non-linear transformations, you come out with an output at the end, and that's the solution of your differential equations. That's the idea behind a pin. So it's nothing very new, I'm sure, for you. So let's have an example, and this is the main example I'll be looking at today. An example of the very commonly used, in this case, shallow neural net might be a Neural net might be a Rayleigh network. Yx is the sum of CAX plus B, and positive terms. And there's an interesting point here, which is really very fundamental to this talk, which is that any Rayleigh network, whether it's deep or shallow, in 1D, 2D, or however many D's, is a piecewise linear function. So if you put Function. So if you put in x, you get a piecewise linear function out the end. And so there's a natural similarity between a RAILU network and, let's say, a piecewise linear approximation using finite element. The similarity doesn't go too deep, but it goes deep enough to allow us to make some comparisons, and that really is going to be a large part of what I'm going to talk about today. So just to kind of emphasise, if we have it in one down here, that really should be a scalar, not a vector. That really should be a scalar, not a vector. If you take this Radio network, this is a piecewise linear function, and it changes its gradients at what we call the knot points, ki, which are given by, in this case, minus bi over ai. So there's this natural similarity between radio networks, piecewise linear, finite element, and this kind of partly lies at the heart of the way pins sort of work. Sort of work. Okay, so here's how a pin works, and they're basically two types of pin. They're what I might call traditional or strong pins. And these are where you assume that the output of your neural net has strong regularity, basically the same amount of regularity as your differential equation. So if you need twice differentiability, you can't use regular, you have to use something else. You can then differentiate y, you can substitute. Why? You can substitute that into the differential equation and evaluate the residual at collocation points, and then you train the neural net to minimize a loss function, which is based on the residual and the back conditions of differential quotient. So that's a strong pin. In a sense, this is very similar to the sort of collocation methods that we've been using for years in codes like Cold Cis Water and so on. But it takes Water and so on, but it takes it on further because you're using the neural net to represent your function. A different type of pin, I think introduced by Wei Nan Yi and his group in around about 2017, is what we call the variational pin, which I really like. Most PVEs have a variational formulation, which requires much less regularity of the differential equation. So RELU would typically be. Releue would typically be fine, that would have the amount of regularity you need. And then you say you might have a variational, you might have some sort of functional, and the solution of the differential equation is the minimum, the minimizer of that functional. And so you can stuff that functional into your ReLU, into your net, and minimise it, and then hopefully the minimizer will then be something like the solution of your PDM. Something like the solution of your PDF. So that is a variational or deep grits method for solving a differential equation. So there we have two basically different types of pin. Both are advertised as mesh-free methods. Neither are mesh-free. A traditional pin, you need to know where the collocation points are. A variational pin, you need to know how to integrate, and therefore you need to know where your quadrature. Great, and therefore, you need to know your quadrature points. And both, because they're using, say, with ReLio or whatever, some sort of mesh underneath, have a mesh there somewhere, even if you don't explicitly say what it is. So, these are what kind of pins are. And let's have a quick look at a simple example just to kind of whet the appetite. Suppose we want to solve the equation minus u double prime is f of u equal to x, then you can use some. Then you can use some sort of network. Typically, you might use sigmoid or tank, something with a reasonable amount of smoothness. You might have a residual, which is the sum, a loss function, which is the sum of the residual of this evaluated at the collocation points. And then you've got boundary conditions imposed over here to enforce the conditions here. So that might be a very straightforward example. And if you And if you take an equation like u double primed is minus u double primed is pi squared sine pi x, which is a very simple equation, the solution of which is u is sine pi x. Then here we operate off the pin. The loss function, which is the residual, goes down nicely. And if you monitor the L2 error of the solution, that goes nicely as well. And so everything seems to work. And here are the results of that. On that. So, so far, so good. And of course, approximately four million papers have been published on PINS from the start of this conference. And I've been looking very popular menu. But is it any good, really? Is it any good, really? So I would say it's not unreasonable to say a reasonable number of people in this audience come from a background of scientific computing and numerical analysis. Maybe that's an insult to some of you, but certainly that's my. Insult to some of you, but certainly that's my own background. I was trained as a numerical analyst. And so I start asking questions about pins. I don't want them to just be free pictures, so I can get Neurix publications how I wish. I want to ask questions like, when don't they work? I'm not sure if you can get a Neurix publication by saying when something doesn't work. But anyway, when do and don't things. Well, this is a really important question. Can we develop a convergence theory? Develop a convergence theory, or can we even make sense of a convergence theory for a pin? So, what I mean by a convergence theory might be something like: I've got a certain number of layers, I've got a certain width, one here, and there, what sort of areas of line do I expect? Some sort of training method, I know not what, you know. So, what is the accuracy? What is the cost of a PIN in terms of all these things? How does that couple up with the training method? Can we ask? Can we ask traditional questions in approximation theory? Sort of questions we've been talking about already this week. Can we ask those when talking about a pin? And here's something which, I mean, my whole life has been spent looking at adaptive numerical methods. E, in his paper on Diet Ritz methods, claims his methods were, and I quote, self-adaptive. How true is that? Here we are. This is, you know, straight from his paper. Actually, DRM's a self-adaptive method. Actually, DRMs are self-adaptive mesh-free methods. Do I believe this? To what extent are these sorts of questions? So, these are the sort of questions that I am asking myself, Tio, Levi, all of our group. We're trying to look at these sort of questions in the context of a number of different problems. And they are very hard questions, but I think they're great questions to ask. Let me give you a sneak preview of the answer. Well, not really the answer, but. Well, not really the answer, but this is a not untypical calculation. I won't go into details, but here's the application of Diet Ritz method to solve a Proton equation in two dimensions. So Laplace U equals f of x, nothing very fancy, f, not particularly difficult. We have a domain with a few singularities in it, but again, nothing particularly difficult. And we've got the number of degrees of freedom. So we had a So, we had a hero method. I think what we did was we kept the depth the same, but increased the width in a systematic way. And we looked at the L2 error of the solution. We did the same with a discrete Galerkin method with a little bit of adaptivity, because that's what I do on the side. So, here's the Galerkin method. The error goes down as the number of degrees of freedom increases. Nicely, it goes down like 1 over n. Nicely, it goes down like one over n squared in terms of normal degrees of freedom. And here's the deep Ritz method. Look at it. Yerra stays almost constant. And interestingly enough, if you look at the original paper by E on Deep Ritz method, they actually look at the error as a function of a number of degrees of freedom, and it does exactly that. But they don't make a big deal of it, because what they do is they publish pictures over here. They burn their PhD students. They burn their PhD students on looking at this region where the solution is better than the Galerkin method. So I'm a little bit cynical about this. So we have to be careful. So the question we have to ask ourselves is why is this sort of thing happening? Is that partly because I'm coding it not particularly well, I'm not implementing the matter of rights, or is this something kind of inherent in the way these things work? But this was quite an eye-opener. Quite an eye-opener. I should say this picture was produced by a PhD student of mine, Simoni Apple. Yes. Is this a linear problem? Yeah. FU equals F of X? Just check. Nothing complicated. Well, there is one complication. We're solving in an L-shaped domain. But there's nothing wrong with that because these are advertising mesh-free methods which will cope with L-shaped domains. Yes? But I, okay, so let me try to say the other. So I guess the point is: can you do? The point is, can you do your DG in 100 dimensions? No. Right, but they would claim that they can do it as well. Fair enough, fair enough. That's fair enough. But is the answer any better? Is it wrong? It can do it. Absolutely fair enough. But no, that's a very, very good point. There are obviously places the pin can go where a DG method will struggle. And that's absolutely fair. Perfectly reasonable. Okay, well, let's move on. So I'm still going to be boring traditional numerical analysts. Going to be a boring traditional New American analyst, or as one of my colleagues called me, Bolden School. So there we are. I've been asking these questions long enough. So I would argue that certainly the convergence of a pin deeply relies on questions related to how well we can approximate a function neurons in the neural net, which is very much your territory, and critically, how well we can train to that approximation. So, that approximation should be these are vital questions, and I suspect what's mostly happening here is problems with training. Problems with training. So, that's what we're going to have a look at today, using sort of a combination of mostly shallow but occasionally deep neural networks. We're going to see how, well, practical problems of approximation can work. So, this talk is going to be much more nuts and bolts to them than the wonderful theory that. Nuts and bolts than the wonderful theory that you talked about once. Brilliant all by the way, but much more nuts and bolts. So let's talk a bit about the theory, and apologise to everyone who knows this really well. One of the things which is really nice at the Vedrock of neural nets are the expressivity results that you have. If you want to see a nice summary, there's a lovely book which came out last couple of years ago by Gueta and Company. By Gueta and Company, Mathematical Aspects of Deep Learning. Some of you may have even written chapters about it. I'll just come on from the book. So we have these notions of expressivity. So here's the granddaddy of them all, the universal approximation theorem, which basically says that if you is continuous, then we can find an architecture, for example a radio architecture, which will approximate that to any desired manufacturing. Okay, so that's really good. We can find an approximation. I made an approximation to anything. That theorem didn't say how deep it was, how shallow it was, it just said you could do it. But brilliant result. Then this has been followed up by many other results. Apologies for everyone in the audience who published brilliant results because I haven't referred to them. But here's Yorotsky's theorem, which says if you put a little bit more structure on the use, it's not just continuous, it lives in some Toblov space, then if you want an error epsilon, then you can construct a new. Then you can construct a neural net where it has certain bounds in its width and its depth to give you this sort of error. And if you sort of dig into these results, you get the wonderful idea that you may get exponential decreased narrow as you increase the depth of the neural network. So, this is why these things are very, very attractive for using kind of numerical methods, because you've got this wonderful theoretical convergence results. Theoretical convergence results available to you in terms of the expressivity that you can get. So, in principle, we should be able to nail it when it comes to solving our equations. So, what's the problem? Well, these are theoretical results, but in practice, unless you're careful, and you use really careful convex stuff, whatever, compressed sensing methods or whatever, you tend not to get anywhere close to achieving these results. To achieving these results. So let's show you what can happen. And we're going to look very simply at the following problem. We're going to consider a set of target functions u of x and we're going to have a simple shallow neural net with ReLU, and we're going to try to minimise the simplest of all loss functions, the square difference, evaluated at a certain number of quadrature points. Number of quadrature points xk and we just want to minimize that last function using our favourite method Adam for example. But in practice you can use any optimizer, it doesn't really matter too much. Okay, well let's see. Well when we tried the pin earlier with sine pi x, it seems to look quite good. So let's try sine pi x into there, minimize using radio, and the answer is horrible. And the answer is horrible. This is what we call a plain vanilla pin. So you take the standard PyTorch or whatever implementation, you take the standard optimizer, you put in the radio network, you press the button, you wait for the results, you get a crappy, sorry, I've been recorded, a not particularly good approximation. You can see it's not particularly good. And the L2 error of 10 to the minus 2 is ridiculous. L2 error of 10 to the minus 2 is ridiculous. You should be getting 10 to the minus 10. So that's for, come on, sine x. Pretty easy function, twice differentiable, infinitely differentiable, with a plain vanilla bin, the plain vanilla regular network. Let's try something a bit more challenging, x the two-thirds. We'll come back to this quite a bit in this talk. It's one of my favourite functions for peculiar reasons, related actually to the deep reds that I spoke earlier. Deep death sign, which spoke earlier. Here again, substitute it in, off it goes. Awful. Look at it. Terribly unstable, really bad approximation. We've kind of gone from 4 to 2 in terms of the L2. Again, we should be getting 10 to the minus 10 if we're doing this right for this problem. So, what's going wrong? It's not the expressivity. We've got these results, and anyway, I'm going to show you in a minute that we can do this properly. It's all tied up. It's all tied up with issues through training. Okay, so problems essentially as they arise with training and also with the conditioning of the radio network. So the question I basically posed is that it turns out to be a very ill-conditioned problem. So what I want to do is let's say compare the Renu approach with two other approaches which look With two other approaches which look pretty similar. Pretty similar. So the first is the grand daddy of all methods, the piecewise linear spline method. Think of phi here as a basis spline, piecewise linear. The sort of thing you'd have in finite element. And typically in finite element, you have a mesh, you fix the mesh, and then you waggle your coefficients up and down to minimize your loss. Okay, so that's a finite element method. That's a final element method. Here's things which I love to death, so I've been studying these all my career: things called free-knot methods or adaptive methods, where you not only waggle these, but waggle these as well. So they're both piecewise linear approximations. Both piecewise linear approximations, and therefore have a great degree of similarity with radium. So let's see how we get on. So let's see how we get on. But before we get on, I just want to quickly compare these two methods. So the linear spline, so you've been around, I'm trying to think when finite elements came out, 1960s. They're not particularly expressive, but they do converge. Lots of theory on them, but you get there in the end. They're not adaptive. They can't adapt themselves, particularly around singularities. They are not equivalent. They are not equivariant. This is something which Brin often Ellen are interested in. Once you put down the mesh points, you're stuck. You can't do anything. But we've got good error estimates. And one thing which is really interesting and very important and completely missed in neural nets is that they fall a linear space. In other words, if you've got one approximation of another, you add them together, you get a third. That's certainly not true of a neural net. The Free Not Spline, much more expressive. You can get much greater accuracy. You can get much greater accuracy. They're self-adaptive, going back to this earlier thing that E said. In other words, if you give it something singular, it can adapt itself around the singularity. They are equivariant. They're invariant to Galilean transformations, rotations, scalings, all sorts of things. They're wonderful things. Great error estimates, but they are a non-linear space. You can't take two of these things and add them together and get a third. So that's a bit harder to work. Anyway, so then we have. Anyway, so there we have two things which look pretty similar to a ReLU, but not quite the same. And that's yes. So phi is always ReLU in your formulas. No, no, so yes, this is right. What I'm thinking of here is I should have drawn a picture really. So a classic one of these would be So it's some sort of combination of two values. Exactly, are she? All will happen. So, Devore wrote a very, very nice review article a couple of years ago in ACTA, where he had a kind of good look at the relationship between ReLU and these things. Basically, any 1D Ready network of width W and L is formally equivalent to a piecewise smooth three knots line with free knots N, where N is bounded by L of that form. So any ReLU network can be formally made equivalent to this. And the proof goes two ways. Proof one way, if you've got ReLU, it's piecewise linear, end of story. It's piecewise linear, end of story. I won't go into where these bounds come from. That's not really relevant for today. But end of story. If you want to go the other way, it's a bit more complicated. This thing here can be made out of ReLUs. There it is. That is the precise description of that in terms of ReLUs. And therefore, if you take a Freenot spline, that is exactly. That is exactly equivalent to this ReLU network, where the coefficients C turn out to be this. And what's kind of interesting, if you work through the algebra, that if you think of the coefficients here as being samples of a differentiable function, then the coefficients of the Rayleigh network are basically the second derivatives of the coefficients of the spine approximation. That already should start to ring a bell because you can have a smooth function. Because you can have a smooth function which has really wobbly second derivatives. And that's not quite the whole truth because that's not true at the ends of the network, you get other things as well. But anyway, there is that form of equivalence. So bearing that in mind, this equivalence and equivalence, I'm now going to walk you through some of the theory related to Free Knott splines and the expressivity theory and the trainability of those. And then we're going to walk back. And then we're going to walk back to have a look at how much that goes over to the ready type approximation. But excuse me, I'm not familiar with the three notes. So the case, how do you determine them? Determine them to be as good as you can get them. You need to have some sort of strategy. Hand on. Yeah. All the roots. But the three knots fine, yes. You've got Wi Pi I. Wi Phi I X minus Ki Wi Ki to the determined. So you've got an extra level of degrees of freedom. Just to explain that I know where equivariance comes in. If I want to translate my problem along, all I have to do is translate all the K's along and it's equivariant. And it's equivariant. If I want symmetry, reflection symmetry, I see the reflector. So you determine the case based on the properties you want? Yeah, the case is something that you find. Right. So in principle, a radio network has the same expressivity as an FKS. If I have a tiny good FKS, I can turn it into a RALU job. So in practice, this doesn't happen. Because it seems that the trainability of this thing... Of this thing is different from the trainability of a RELU network. This is what we're finding. So it's kind of interesting. So, what we're going to have a look at is the practical expressitivity of the FKS and how that maps across to RELI. Right, so okay, so we're going to look at two things and compare them. One is the red. One is the radio network where we have free choice of W and K, which is this thing, and a subclass of these, which are called the interpolating Free Knots lines, where we should say one other thing. This is one here. That's important. So the interpolating Freelanc line, if I have a function u, sets the weights here to be u evaluated. Weights here to be mu evaluated at the knot points. This is simply an interpolate, but we can choose where the knots are. So this is more expressive than that, but this is harder to train than that, because there's more to train. So here's some theorems. There's some nice theory around. It turns out you can take target functions which are awful. In other words, they can be singular, pretty nasty. Singular, pretty nasty, but providing they live in the right sort of solvoff space, you can approximate them pretty well for the free knots line. And you get results like the error in the two-norm or the infinity norm. It goes like one over n squared, where n is the number of knots. And that applies to both the free knots line and the interpolated free knots flying. Typically, this is about a couple of orders of magnitude smaller than that. But these are good expressivity results. And so they you can apply them to sine x, which is the function we looked at earlier, x is two-thirds, all sorts of functions. Right. You can do more than that. If you know u double primed, and you shout to me, but we don't know u double primed because we only know u, but I'm saying to you, yes, but we're solving the pin, so we do know what u double primed is. u double primed equals f of x, f of x. So we do know u double prime in many cases. We can actually write We can actually write the error down. Isn't this wonderful? We get this amazing result. The error goes like the fifth power of the not separation times the second derivative squared. Isn't that amazing? So you can write all this down. So that's another way of writing down the error. So it's a kind of different expression of the same thing. And we can prove, if you look at this, the error here is made. This, the error here is made up of a sum of thingies, and there's fairly deep results which say if you have an error made up of a sum of thingies and you can mag all the thingies, and you've got a certain amount of sparseness in the representation, then the optimal error comes when you equidistribute the things. Sorry, this isn't very precise mathematically, this is a bit more precise. If we equidistribute, so we make all of these the same, then that's equivalent, exactly equivalent, to minimum. Exactly equivalent to minimizing the other thing. That's a really nice way of doing things. So it motivates us to introduce what we might call the equidistribution loss function, which is exactly the same as the other one, the other one over here, where we simply try to make all these the same thing. Right, so let's say how we might proceed. How we might proceed. We can proceed then by rather than training our thing to minimize the L2, we can minimize these other things. And if we're really fancy, we could just do it directly by quadrature. And I'll show you some examples of that. And I'll give you an example of that in a second by essentially setting this thing all to be the same. So let's take my favourite example, x to the two-thirds. Why do I fascinate on x to the two thirds? Why do I fascinate on x to two thirds? If you're trying to solve a Poisson equation in a domain with a corner, which is something I have to do a lot in my life, then the singularity at the corner, if x is the distance from the corner, goes like x to the two-thirds. So that's why I focus down a lot a bit. If you take this, the second derivative is this. The equidistribution condition turns out to be this. And it turns out that the optimal place to put your knots. Place to put your knots is the ith knot is at 15 over 7th power of the location, isn't that? Super, I'm sure you were shouting at me. It's obvious, it's obviously pretty cool. It's cool enough. And if you substitute that in, you find the square of two error goes like one over n to the fourth. Brilliant. Compare that with a classic finite element interpolant, and even worse, the radio thing, where the error goes like on every instance. thing where the error goes like 1 over n to the 7 thirds volume. So n to the fourth is a lot more expressive, 7 thirds. Okay, so let's see how this might work. So we're going to try training an FKS and we're going to try training it directly. In this case we're just going to train it using the L2 error and it's kind of interesting. If you use add Interesting. If you use Adam on this and you start from a random set of coefficients, then you kind of don't get anywhere. That's these sort of things. If you start with a uniform set of coefficients, then you do start to get towards what you want. But if you start it saying a little bit of information, we know where the lot should be, let's see what else we can get, it produces this really good solution. Sorry, can I give you like two? So sorry, but do you mean like two s two stage training here? Oh, sorry, I've oh, I've jumped the gun. Tell you in a sec. Consider, yeah, okay. I apologise. Put the slide in the wrong order. And here's just a comparison between the expressivity of the FKS where you train everything. This is the interpolating FKS where you only train four knots, both going like one eighth and fourth, and this is the final element standard. And this is the finite element standard uniform where it goes like this. And that's what the approximation looks like for x to the two-thirds. It's essentially perfect. So it's able to represent that singular function really, really well. Here we come. So what's the two-level training? So two-level training is a way of training these things where we kind of try to look both at the problem of finding where the weight should be. About the problem of finding where the weight should be and where these things here should be. And we have this equidistribution error. If I go right back to this, this error here just is expressed in terms of, this is the equidistribution error for the interpolating spine, but that's fine. We're going to find these dots in the interpolating spine. It's just in terms of this. So that gives us, and this is formally equivalent to it. So we can have. It. So we can have a loss function which is this, and we can combine that with the original L2 loss function to give us a combined loss function. And the two-level, the mixed approach is to train the whole thing with this combination, say with a half, starting from something uniform. Or the two-level approach, which is kind of nice, is you say, I'll switch off the coefficients and train for these things. And train for these things using this, using the overall error as a regularizer. And then I'll swap it around and I'll train for these and train for this using now the equidistribution as a regularizer. So that's the two-stage training. And that turns out to be remarkably effective for the FKS type approach and simply works. As an iterative method, you do this as an iterative method. Well, you do one and then the other. And I suppose you can repeat, but in practice, I mean, that's what happened there, just an instance. But I'll give you some more examples. And this motivates us to say, well, maybe we could try the same thing for the radio. So for the radio, we might take a shadow radio, sort of radio L L N, compute the implicit naught. The implicit naught functions, that's the ratios of A of B, with this, train for those, find those, and then train for the ones. So this very attractive way of training already. Rather than just plain vanilla training, we can try this. And we tried this, we thought we were doing well. So this is work in progress, so please bear with us on this. And we found what's interesting is that we can find these terms quite accurately. Determines quite accurately, but still the weights, the C's in the Renu turn out to be ill-conditioned. So there still seems to be problems with training Renu. So let's actually show you how this goes. So these are all calculations that Tia did. The top is a sort of repeat of the calculation I put up earlier, where you take your plain vanilla Rayu approximation, put it into PyTorch, press all the buttons, see what happens. It gives the See what happens. It gives these very poor approximations for these three functions. These are where it thinks the knot point should be. And that's really poor. You're going to get very, very bad accuracy with that. In comparison, if you switch on this equidistribution, it actually finds the knot points very well. So it it can locate them really well. So if you were to find those and then just substitute in the interpolation points, you get then a very, very accurate solution. Then a very, very good solution. Here's a comparison between the kind of where the knots are. This is a kind of histogram for those three functions. It's trying to put them in the middle, whereas x to the two-thirds it puts them where the singularity is, exactly where you want them. And sine pi, it also concentrates them where the curvature is highest. And this is also interesting. This is what happens to your knot points when you train. In the plain vanilla one, they go all over the place, they cross over each other, they tangle each other up. Each other, they tangled each other up, it's a mess. Whereas, if you do it with the equidistribution regularization, they do much better. So, in principle, you're getting much better extra precision. That's the good news, and now I'm going to show you some mixed good news and bad news, and sort of where we are. So, we're going to have a look at a series of functions, and on the left, Chris, sorry for the interruption, can you describe what's being described? What's being displayed here? What's being displayed here? So these are in a RELU network. We're going to think of a RALU network as the sum of CI RELU AIX plus BI, which is equivalent to the sum of CI, well, scale that doesn't really matter. Review X. Ready minus Ki over Ki is minus Bi over Ai, and that's what we've got. So these things should be concentrating nicely while ending up, that's what we do. So if we have a look at going back to a little more considered, here is the thing we're trying to approximate. This is x, one minus x, this is sine x. Each curve is the evolution of one of the k art. The evolution of one or the KI? Yes. These lines, right? Yes. So here is a KI. We evolve forward in time. Yes, sorry, I should have put the labels on. Here are the KIs going forward in time. That's where they're going. Okay. So here we have the same picture for a series of things. Here are a series of functions. Here's x to the two-thirds. And here's a comparison of some different methods. There's a lot in this picture. So I'll try and walk you through it. So I'll try and walk you through it. Here is the Freenots blind with two-layer training. Totally nailed. This is the loss, by the way. 10 to the minus 10. That's as expressive as you could wish. And I can construct a Releo based on that, no problem. Okay. Bit of a wobble at the end, but that's probably a mistake in the implementation. Nothing too serious. So that's FKS. This is an FKS where An FKS where you, instead of going through this, this is more like the finite element method. And these things over here, unfortunately, are the Raylier methods. They do well to start with, but start to fade out at the end, and this is due to ill conditioning. It's due to ill-conditioning. The two-layer method definitely improves things a lot over the plain vanilla, but the ill-conditioning hits us in the end. It hits us in the end. Here's a couple more functions. This is a tanch. There's a lot of points concentrating on the singularity. Again, the FKS two-layer training totally nails it, but the RELUs, which are over here, really still continue to struggle to get the loss function down. And I'm not going to sort of state this as a theorem or even really a conjecture, but it's more of a belief that it's these problems that we're seeing with the struggling here. That we're seeing with the struggling here, even when we put a lot of effort with the ReLU, that are relating to the problems that we're seeing with, particularly the deepness methods. Whereas this is much more like what you're doing in a Galerkin, or particularly with an adaptive Galerkin, where we get much better value. Okay, just to another thing. This is to do with the old conditioning. So here's a two-layer method where we first train the dots and then train the weights. And then train the weights. If you train the knots and then do an FKS, the loss goes down eight orders of magnitude almost instantly. If you do it with radio, well, this is where we change the learning rate. Maybe sort of over here, but it's going pretty slow. And that's due to ill condition. And these are the radio coefficients. And these are the radio coefficients. They look all weird. I won't do that because I'm running out of time. Just to say, the plan is to move forward onto looking at pins with the same sort of ideas where we try to put some constraints on the way we formulate our thing. We're going to slightly bring the mesh back. And here's an example where we try it out on a simple convection problem. Simple convection problem, diffusion problem, sorry, where if you don't constrain things, it gets there in the end, but very, very slowly. Whereas if you put these constraints on, it trends almost instantly to get the results. But these are kind of beginning results. So I'm running out of time, so just finish off. In the way it tends to be implemented, and I don't want to kind of take any glory away from the correct ways of implementing it, there's this gap between theoretical explanation. This gap between theoretical expressors 2 and what you see. They're formally equivalent, and you can train FCAS really efficiently, but things seem to be going wrong due to real conditioning with the rail view, and that impacts on pins. And so I say this sort of way before we understand Roll R with some pins, and you can kind of see why it's part of the case. Beautiful. And I should say, most of the work with And I should say most of the work was done by Tio. Oh, my understanding is Tio is answering questions, too, right? Well, he said he'll have to. Okay, so does anybody want to test you? Hi, yes. So, great, great, great stuff. So, I have two questions about, like, you know, first, if you then move up with the dimension, I guess then how this analogy will help you. And also, if you go. Will help you with the answer. And I'll say if you go with that, right? So I had with that. Both are very good questions. I don't really probably know the answer to either. Going off a dimension is already quite awkward because the kind of mesh that lies underneath a 2D radio is a very, very complicated thing and very hard to get a handle on. So I'm the answer to that is that no. As regards depth, see. An FKS has no depth. What you see is what you get. So you have to ask the question and how a deep RELU might relate to that. And all we have with them is delivering numerical results. People haven't got anything to deeper parts. So lots of work. I guess with the deeper network, you still have knots somewhere, but the formulas are much more common. The formulas are very common. In principle, very done. No, in principle it could be done, but in practice it's hard. So the deep brain network is still piecewise lined up. Oh yes, it's got the knots. So it's just formula. It's just much. I mean in principle you could have a deep network, calculate the knots, put AMT loss function and off you go. And we've done something along that, but it's pretty hard to do. You have like far more numbers than you have to create a freedom. And so like in the shallow case, yeah, it's uh linear space, it's a line approximation. And so the analysis sort of transfers quite nicely, but it's not obvious. But anyway, very much further work to do on this. But for a REILU network, is there because you have a lot more parameters, so that's an advantage, but you still I remember seeing a RITLA paper that's called deep RAILU networks. Deep radio networks are shallow, is the meaning of the thing. Well, all deep radio networks are formally equivalent to a shallow network. I'll just say that to the equivalent. But formally equivalent and the same outside, not quite the same. Fair enough. Well, okay, so maybe in the interest of keeping our time, we'll move on to the next talk. Thank you again. Talk. But thank you again, Chris.