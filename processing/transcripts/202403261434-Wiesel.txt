I'm happy that we're running a bit behind time because Germany is playing Netherlands at the moment actually in soccer. Just about to be half-time. So if anybody's interested, it's 1-1. If I'm checking my phone later, then please just assume. Is it friendly or is it a phone? It's a friendly, so who cares? Do you want me to keep an eye on the score while you talk? Yeah, I'm also I'm not gonna talk about the game as well. So, I'm not going to talk about the game elsewhere. Okay, so let's turn to some statistics. So, this is short work with three people, and we actually started working on this some time ago when I was still at Columbia. By now, I moved to Carnegie Mellon. Pepper, the first author here, moved to Cornell. Sydney Rush is still holding the fort at Columbia, and Damilka is at Elf Braxton. Hello, Mike Steph. So, let me give you some motivation. So, we're really interested in a very basic setting here in the talk. So, I'm going to assume that I have IID samples, so paired observations x and y, where x is a d-value to an invariable, and y is one-dimensional coming from some joint distribution p, which we don't know. Right? And now we want to. And now we want to do best linear prediction. And if you just have your empirical measure available to you, so this is the location for the empirical measure I'm going to use, Pn, well, the classical thing to do is just to do empirical minimization, which means, right, you pick your favorite loss function L and you compare y and x transpose beta here. So for the empirical measure, a different way to write this would just be to write the sum. This would just be to write the sum out, right, and really take the average here over the observations. Okay, so far so good. Right, classical example could be the squared dots function, for example. All right, so that's part of the story, right, which is commonly known as training. But of course, we would like to have also some generalization. Also, some generalization by which I will mean the following. So, you use the empirical measure to actually train your data, right, to find your empirical optimizer. So, that's why this depends on Pn here, right? And then you use some different testing distributions to actually evaluate how well this estimator works if you move away from the samples which you have already seen. So, here I'm So here I'm using the notation q, right? This could be q equal to p, which was the underlying data generating distribution, right? Or some other distribution which might be motivated by adversarial attacks, covariate shifts, or whatever you want to think of. So in general, we might be thinking about Q, which is not exactly P, but is in some sense closed. Okay? Okay, and if we want to have good generalization, there's actually conventional wisdom in the learning community that it's always a good idea to regularize your original problem. So not to work immediately with just the empirical risk, but to do some regularization which will then yield a better generalization in terms of Generalization in general. Okay, so to make this a little bit more hands-on, I want to give you a specific problem to think of. So now I'm taking a specific loss function, which is just the half power here. So captures the squared loss, for example. And regularization, I want to understand here as taking some kind of norm of your beta. Right, so this is a So this is a quite classical setup, in particular for r equal to 2. This captures the square root also if you take your norm just equal to lambda times the L1 norm of beta. This has been well studied in the literature in the last 20 years or so. And another example would be the square root slope, where you now take a slightly different norm. You take a slightly different norm, so you take a sorted L1 norm, and you also have different lambdas available to you, which are also sorted. So, for specific statistical reasons, this is also a problem which has been well studied. So, I'm just trying to convince you at this point that our setting here, this generic setup here for different norms, is actually relevant to the To the change. Okay, but let's come back to the modic weighting questions here. What is actually the question we want to understand? And I've written it down here again. So there seems to be some connection between regularization and generalization, right? Can we actually make this quantitative? So can we actually, for a specific regularization, can we actually find the distributions where generally Distributions we're generalizing against, and the other way around. So, can we make this a one-to-one correspondence? Okay, and I'm talking to experts here in the room, so I don't have to pretend we're the first ones doing this. In particular, I want to cite the works here of Jose and Ray, right, who actually were among the first who We were among the first who found such a nice correspondence between regularization and generalization. In particular, the following holds, right? This is the problem I just looked at for i equal to 2, so the root mean squared optimization problem. And it turns out there is really a one-to-one correspondence between this problem and the distribution of the robust optimization problem, where we now stood taking in. Now, still taking in for over beta, right? But now, instead of looking at the empirical measure, so this would be the empirical risk, right? If we just look at the empirical measure, we're now looking at all Q's which are in a specific 2-Wassage time ball around the G. So again, this has been around for a couple of years and is well understood, and I think it's really a beautiful result, which explains how these two things go together. How these two things go together. Right, now I'm in the difficult situation that this is, of course, not the end of my talk. So to motivate a little bit going forward, I have to criticize these works a little bit, right? But please don't take this too seriously. So okay, you can beat me up later. So what is one problem? So, what is one problem with these works? I was talking about generalization. And if I actually want to have generalization for the true P, right, then from this formulation, it's actually obvious that I have to guarantee that my true P is at least in the Vasic temple. If I don't have this, I don't have control. We can talk about it later. I already disagree with that. Yeah, okay, for generalization, I will explain later where I think this is needed. So it's still empirical. This one is empirical, right? This is the regularized. Okay, because you kept using the term generalization, so I thought the top one was the. That's the regularized problem, right? This below here is what I. No, no, it's because you were using the term generalization that I thought was. Using the top generalization, that I thought for some reason that the top one was the tr the true distribution. It's only now that you just say there is a problem where the true, we don't know if the true distribution is in the problem that I realize it's not the true distribution. Right, this was always, so I started with the empirical measure, right, always here. So first I was looking at the empirical risk here on the first slide, and then I said, okay, I want to actually test. I follow that. So there is no generalization yet in the work that you just showed. So, the generalization is implicit, but I'm going to tell you more about this later. So, from this formulation, in a specific way, you can actually get generalization error out of this, but I'm getting there. So, give me a few minutes. So, okay, let me make this point, right? If you want P to be in the Wasserstein ball around the empirical measure, then well, you have to choose a beta at least as big as the Wasserstein distance between. As the Wasserstein distance between Pn and P. Well, if we're in a high-dimensional space, this is going to be a problem, right? Because it's well understood. If you don't make any further assumptions, this scales like n to the 1 over d, and that's known as the cursor dimensionality. So even for not too high dimensions, this will mean that I need a lot of data to be able to estimate this quantity reasonably quickly. Quantity reasonably quickly. Alright, a second problem, which is maybe more theoretical, is the following, right? I told you that you actually, to get this duality result, at least in this form, you have to use a specific to Basserstein ball. And what I mean by this is actually that this ball doesn't allow for uncertainty in the X and in the Y values simultaneously, but actually you have to fix the Y. But actually, you have to fix the y values. And that means, of course, if you have the normal distribution as your p, this will never be in any Basselstein ball around the empirical measure. Because from this assumption, right, you get automatically that any measure in this ball only has finitely many different y values. Okay, so that's a problem. Can we solve this in some sense or the other, right? Then maybe a different Then, maybe a different question is: these works here actually exploit duality a lot. We have seen this a couple of times already this week. Can we actually just look at the primer side of the problem? So can we actually give the worst case distribution and use this to get the result through? And then lastly, does this duality only hold for norms, or can we extend this a little bit further? And I have to say immediately the last point I will not have time to cover. The last point I will not have time to cover, so you can find that in the paper. But I'm going to focus on these three points here during this. All right, so let's come to our contributions here. So I was just telling you about the Wasserstein distance. So here again is one way to write it down, right, between x and y, and we use some norm in our And we use some norm in Rd plus 1 to measure this. And as I said, right, usually, if you want to have control between the empirical measure and the true measure, these are the rates you need to use. So here's an idea. If this doesn't work, why don't we just use a different metric? So we got a nice duality before, but maybe that's not the only metric which makes this duality work or something comparable. So let's try this. Let's try this. And the one I want to use here is actually, we didn't go too far, right? Is what's called the max dice Basserstein distance. So it's still a Basserstein distance, but it has one crucial advantage, which is that now we're not looking at norms here anymore, so we don't compare distributions in Rd plus 1, but we're only looking at projections onto one-dimensional subspaces. So how to read this? So, how to read this is really you fix a gamma, right, a vector in Rd, and now you project your distributions down to one dimension. So, essentially, you're comparing for this specific gamma, you're comparing prediction errors under P and under Q. And then you still do the same normalization by R and square root for one of our R here. And you take the infimum, so that's exactly as path, right? And lastly, And lastly, the last step you do, you take the supremum over gamma. And at that point, you're kind of, well, if I take gamma as large as I want to, right, this will probably blow up. So I have to normalize. And one way to normalize here is actually by looking at this specific fraction. Okay, so some fun facts about the Max Leised Wasserstein metric: it's actually generating the same. It's actually generating the same topology as the Wasserstein metric, but the metrics are not equivalent. So there is some hope. If they were equivalent, of course, there would be no hope to overcome the curse of dimensionality here. But as they are not, right, maybe this is actually going. All right. Let me give you some more motivation here, actually. If I look at the Max Leised Wasselst diametric and I take the Euclidean norm, right, then it hurts. Euclidean norm, right? Then it turns out that this is always smaller than the true Wasserstein metric. So that's the right direction for us because we were looking at this before and this had two slow rates. So maybe if we take a different metric here, which is actually dominated by the Baselstein metric, we can get better rates. On the flip side, this actually means that the balls we are generating, they're going to be larger. So maybe, again, we can actually settle for We can actually settle for a smaller radius which scales better than the Waster Steiners. And okay, how can we hope for this to overcome the curse of dimensionality? Well, the quick answer is this is basically a one-dimensional quantity. It is actually for fixed gamma just a one-dimensional Buserstein metric. Now, of course, I'm still taking supremum over gamma here, which might be screwing up my. Might be screwing up my rates, but it turns out it's actually not too bad, right? Even taking the supremum here is still much better than looking at norms in arguments. All right, so yeah. So can you take the integral? So just use slice to Washington style instead of a max slice? So not for the the problem we're studying here. Of course in general you can do this and you get even better rates. But for this one we really need the maximum slice. For this one, we really need the strong gravity if we use well. Our goal is to get generalization, right? And for this one, we then really need a maximum distance. So, okay, let's start with the results. The first result is really I've shown you the odd duality. Now I'm changing the ball, right? Now I'm looking at the max dies Wasserstein ball, but the result is still exactly the same. The result is still exactly the same. So I'm looking at the regularized problem here. This is still in duality with the DRO problem, where now I use the max dice form. So we've lost nothing here, which is good, right? Because we like the old result. We didn't want to destroy it. But we can even get more, right? I told you I was interested in the worst case distribution. Well, here it is. If you start with x, y being the x x x x x With xy being distributed according to your law of p, how do you get the worst-case distribution? Well, you just do a linear shift. So, linear shift by this number, so this random variable e here, which is defined below, which is basically, again, the prediction error, scaled by delta times some normalization factor. And so you shift x by minus e times the gradient of the norm to make it. Gradient of the norm to make it d-dimensional, right? This is just a random variable at this point. But the blue term, this is completely deterministic, right? There is no randomness in the blue term. And for y, well, I'll shift it again by e, but this time without multiplying by the seven. Okay, so let's have a look at an example here. What I've drawn here is actually now pairs x and y, and in the blue, the blue points are just the impression. The blue points are just the empirical distributions. So 100 points from some normal distribution here. And then I do OLS, right? I get the blue line. So that's all very familiar. Stats 101 for everybody, right? So now how do I get the worst case distribution? Well, those are the red points here. And you see, if the blue points were very close to the blue line, then actually the shift is not going to be big. Not going to be big. If I'm above the blue line, I'm just going to shift slightly above here as well. And if I'm below, I'm going to shift slightly below. If I'm further away from the blue line, then the shift is going to be much bigger. And that's kind of what we saw from the formula on this time. And now the red line is as through duality is either the square root Lasso or is gonna be the OLS now for the worst case distribution. So by duality, that's the same. So by duality that's the same. Yeah, so each point in the empirical distribution, right, I'm going to shift by the signal. And they all have weight one over n. All right, so that's an illustration. I hope that makes sense to everybody. So what can we now say about the generalization error? About the generalization error. I was starting with this. I was saying we had for Wasserstein distance we had very bad rates. What can we do for the maxized Wasserstein distance? Well, it turns out actually under minimal assumptions, so we just need some moments under P, and especially we need moment of order S where S is greater than twice the Wasserstein exponent. Then we already get rates which don't suffer from the cursor dimensionality anymore. From the cursor of dimensionality anymore. In particular, we get rates of order c times some log factor divided by square root of m. Alright, so these rates are actually optimal up to this logarithmic factor. You know this from one-dimensional Wasserstein theory, right? You know that one over square root of n is the best you can achieve in general. The constant c is actually explicitly computable, depends on these quantities. So we can actually. Quantities. So we can actually use this to compute confidence sets. And lastly, we can improve this so we can actually get rid of the log factor if we look at compactly supported features. So then here, of course, the support would feature instead of the moment. So you were saying for small voltage time distance, you wouldn't get something like this? We do, yeah. That's also been studied, for example, by. That's also been studied, for example, by other people at CMU. But the max slice case dominates the slice Rosserstein distance, right? So it's actually harder to get these rates. But once you have these rates, you automatically get rates for the sliced. Alright. So the norm pops up here, right? So in the moment. That's the same number here, or the door is the same. So here I've written it for the Euclidean norm. In general, it's the do a normal, right? It's completely red. Yeah. Yeah, there might also, I mean, there is some dependence on the dimension, right? Again, let me not go into the details here. If we had like one volume on the One volume on the on gamma when you took that supreme? Would you get a you know one rhythmic dependence on D, or is it always going to be some kind of one or the So the dependence on the dimension usually is square root of D. So square root of D by square root of m is again we know is optimal. No matter what the true norm is for you taking the supermarket. Yeah, so the norm features here, but not so. So, yeah, this is really the play, right? You just take dual norm on one side. Sure. Okay. Yeah. So the direction you take it as deterministic or stop passing by your slice? When I slice, so that's the normal definition, right? When I slice, I'm taking supremum here over vectors. So there is no randomness in these vectors. I'm just curious if you do the element, for example, the integral. But for example, the the heat proline, if you pick uh random directions, that helps to pick random directions, that kind of like average. Well, I mean, if you take random directions, you would really interchange supernatural, which is a completely different problem. Yeah, I think I'm I'm still having in mind the sliced one. Yeah, so for a slice one you're taking a specific measure on the unit ball and you're just integrating over this measure. Yeah, oh. Yeah, but would that help? Or you can talk that way? Yeah, this would give better rates than the maximized machine. So here is a thing that we did in that paper, but I think it's showing up in John's question. So in that paper we did not attempt to make sure that the true distribution is in the center of the ball because of course that just doesn't make sense. Instead, It just doesn't make sense. Instead, what we did is we said, okay, let's project the solutions of the problems, right? Let's project the vast extra into the space of solutions of the fails. Right. So instead of like the, and when we did that thing, the delta that we got actually scales as low, like, you know, we have one over there, you know, that the portion of this magic disappears, but the dependency is logged in. Right. Log of H. Right. Not the square root of lambda. Yeah, yeah, yeah. So this you cannot get for the max slice distance. Well, you could, I think. You could if you did exactly what I like when we did, which is... Well, I mean, if you want to solve this, really you want to find rates for the max less distance, it's not that there has to be spheroidance on D. If you want to have rates for this problem, maybe you could, right? That is not what I'm claiming. But I think for you, it was really a different question. You, it was really a different question, right? You were not asking for generalization error, you were exactly asking for what's the measure P, which actually has the same minimizer as the original problem used. So, you're really projecting down onto the space of P types. Yeah, but you could have also projected the the the solutions of the problem, right? So the projection of the of of the sol of the actual the arc in, like the so the the thetas. In like the thetas, that was just because it was more functional. But you could have said, okay, what is the smallest value of the objective function, which is the generalization now? And you would still get something that is informed by the problem. First of all, I really like this. I'm just not. But the point I'm trying to make is that the generalization by the generalization to something is not the generalization with a task, right? And this thing is optimal, but it's optimal regardless of the task. What I'm saying is that if you added the task, I think you could actually get optimal, optimal for that mass. That's what I'm saying. Yeah, that's possible. So I think here it's really the new take is to use the max size distance and you get some, as you say, right, you get some additional robustness for free, which maybe for the specific task you have in mind is too much. You have in mind is too much, right? And then you pay with this by the square root of d, which again, compared to the cursor of dimensionality rates you have for the Wasserstein distance, is maybe a small price. I guess another question, a bit similar, is do you need to have a true distribution key in the ball in order to ensure generalization? Right, that's, I mean, that's the key question, right? That's the key question, right? For at least this is a way to get generalization. Maybe there are other ways, right? For example, you can think about delta methods, stuff like that. But then usually you really just get generalization at the true minimizer. You don't get generalization uniformly over the set of betas which you consider. And maybe there is some, you have some estimation error, and the betas are numerical error. beta is a numerical error so you're not really true that you're at the true sure that you're at the true minimizer but maybe you moved a little bit away and then like these classical statistical models methods would maybe not give you give you what you actually want right so that was the idea here to actually have some additional robustness and have this supremum over beat but again this might not be the easiest uh the the only thing to do sounds like coffee break is gonna like maybe you have two minutes You're putting me in a difficult position. Germany might be losing. Well, then you level to know. Yeah, exactly. I'm just going to keep going. So just to compare, regardless of the generalization error, these rates on the maximized Wasserstein distance also improve on known results. So there is this nice paper by Daniel Badel and Schahan Mel. By Daniel Badel and Jahan Mendelssohn in 2022, which covers the case p equal to 2 for isoparametric isotropic distributions. And there's this older work by John Nilesweed and Philippe Rigoli, which was only looking at sub-Gaussian tanks. And we here extend this and improve on the rate, so we really just need some moments, as I've shown you above, we don't need some. I've shown you above, we don't need subausion. What is big depth here? It's the quanta. So they actually, I mean, these are one-dimensional quantities, right? So you actually have the Wasserstein formulation as quant functions. And that's what they analyzed. Okay, so let me wrap up. Let me actually come to the generalization bound I was aiming for. Right, so how does it work now? You use the data informed by The delta informed by your rates. So these are your rates you got from the Max Lisberstein. And now you take a Q which is in the Max Lisbonstein ball around the true distribution P of radius epsilon. Then now you can control the testing error under Q by the empirical error plus this radius of the ball around the two distribution plus epsilon times one over one plus beta. One over one plus beta norm. And as I said before, right, this doesn't just hold for the empirical beta or the true beta, but actually uniformly in all betas, which you would like to test this. All right, one last slide. So this, you see, I first gave the talk in Brazil. So this is trying to explain a little bit how. Explain a little bit how these different balls go together. Right at the center, you have the empirical measure and you have the two measure over here. And now the squares are going to be the Wasserstein balls. So this is the smaller Wasserstein ball, which has radius 1 over square root of n. And this is the bigger Wasserstein ball, which has the curse of dimensionality. And now you see here in the smaller ball, you actually don't have P inside the smaller ball in general, but it's in the Ball in general, but it's in the bigger ball. And then the max size ball, this is the blue one. So this already covers the 2p with the smaller radius. And it turns out now the worst case measure is actually an extremal point under both of these faults. So it's actually on the boundary of both of these faults, which then explains why we actually get the same duality, right? We can actually use the same worst case function. Work-based functions. Okay, so I think I can conclude very quickly. I've shown you another duality here which uses the maximized distance and at least shows one way here to overcome the curse of dimensionality for generalization bounds. I'm not claiming it's the only one, but I thought it's a neat idea to try out. So, thanks for your attention and the perversion. Thank you very much. As I said, we're going to leave the questions to the 3.30.