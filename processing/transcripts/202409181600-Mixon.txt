How are we doing? You guys took your nap and ready to go? Yeah. All right. I am Dustin. And I'm going to tell you about bilips and variants. If you don't know what those are, you're about to find out. So it's all good. There's some joint work I've done with some awesome people. Some of you know Jameson. He's pretty great. Joey's also great. These used to be, well, he used to be. Joey's also great. These used to be, well, he used to be my student. Now he's in industry. He's currently my student, about to be in industry. They're both amazing. Sorry, you're not going to be able to work with them. Okay. Let's talk about invariant machine learning. So we are looking to process data that's given to us in a real Hilbert space. When you think RD, okay? I got points in RD. I want to do data processing. So maybe it's classification or clustering. Maybe I want to do nearest. Or clustering, maybe I want to do nearest neighbors, visualization, something like this. It's important that our data is in a Hilbert space because the lion's share of data processing algorithms love Hilbert space. I don't know why, that's just the way it is. Okay. So suppose a genie or a troll comes along and tells us, hey, actually, you should be identifying points modulo the action of a subgroup of the orthogonal group, FYI. Orthogonal group, FYI. So, for example, maybe your data is really like each example is a graph, which you naturally encode as an adjacency matrix. But then when you made that choice of ordering your vertices, you've introduced an ambiguity. And that genie is trying to remind you, hey, these are actually, these look like different vectors, but they're the same. So you should treat them as the same. I want you to identify points. I want you to identify points in the vector space of three by three symmetric matrices under orbits under this group action where you're permuting rows and columns. This mess? Presumably, this should make it easier on us. Okay, we've got more information and should make it easier. Cool. So let's talk about the genie's notion of distance. The genie has told us. Distance. The genie has told us that we should be identifying points in a common orbit. So, if I have points that belong to two different orbits, what I'm going to think of is the minimum distance between points in those orbits. I'm going to call that the distance between those two orbits. That's what's going on here. Okay. This is a quotient distance, but let's be careful about the nature of this quotient. When two points have distance zero, what that means is that one Zero, what that means is that one of the points is in the closure of the orbit of the other one. And this is a symmetric relation. The closure of the orbit is just the orbit if your group is compact. For example, if it's finite, which it will be for a fraction of this talk. At some point, we're going to encounter an example where the closure is not the orbit. So you actually have to think of the closure as its own thing. This happens, for example. Its own thing. This happens, for example, in this contrived example of the group, the subgroup of SO2 generated by rotation by one radium, right? You're dense, but you're not the whole thing, and that's annoying. Okay. So the genie's notion of distance is a metric over a space. It's not quite the quotient. It's like maybe it's a double quotient. First, you mod out. First, you mod out by the group, then you mod out by like topology or something, closure. You do that thing. And so we use two slashes for this double quotient. This is the metric quotient. Questions about this slide? So, this is what's up. We're trying to do data processing in a Hilbert space because that's where data processing loves to happen. Processing loves to happen. Our genie comes along to give us extra information, but then the information he gives me twists up my Hilbert space into this non-Hilbert space metric space. And I want to do data processing in a Hilbert space. So this makes me sad. It shouldn't make me sad because he gave me information. It should help me. So there's a standard approach here, which is data augmentation. So I'm going to use the fact that To use the fact that things should be closed under this group. So I'm just going to spin up every example I have into an entire orbit of examples. But this isn't very feasible when my group is huge, right? I'm just blowing up my data set. Now my training is inefficient. I'm using the hint that was given to me, but it doesn't feel super great. We're going to take a different approach. We're going to take a different approach, and that is we're going to take this metric quotient and we're going to embed it into a new Hilbert space where data science is fun again, right? So we're going to figure out how to embed this metric space in a way that preserves the metric as well as possible. If those distances are preserved, then the hypothesis is that, well, that would be good signal for my data processing. So I'll be. For my data processing. So I'll be preserving that signal. And if the group is huge, it's not a problem like it was up here. It's not blowing up the size of my data. In fact, it's better because it's making my space smaller. Okay. Questions about this slide? So let's talk about what it means for distances to be preserved. It's something just It's something just a little bit technical. And in order to get it into your brain as efficiently as possible, we're going to have a game show. Can I have a volunteer? Come on up. Give Alex a round of applause. So there are going to be two questions. Okay. If you get both questions right, I will buy you a glass of Mezcal. Glass of Mezcal. If you get both wrong, you have to buy me. You have to buy me a glass of Mezcal and 50-50 the watch. We'll just drink. Okay. Cool. Here's the setup. Maybe stand on the other side so I can face you and you can also face this. Okay. Here's the setup. We got two metric spaces. Okay. These are not quotient spaces. They're just metric spaces. They're called the unit square. Okay. They have Euclidean distance. So that's why this is a So that's why this is a distance one across the side. I'm giving them different names just because I have an input space and an output space. They both happen to be the unit squared. So now I have two metric spaces. One's going to be the domain of a function and the other is the codomain of the function. I'm going to show you that function. Ready? Here's the function. Okay, study it. Yep. You see how it works? See what's going on? Okay, question one. Where is the this looks? Where is the, this looks like a derivative, it's not. Where is the distance quotient smallest? Let me tell you what I mean. So consider any pair of points in here, look at where they're mapped to there. The numerator of this distance quotient is the distance over there. The denominator is their distance in the original space. Where is this quotient smallest? Think about it. Think about it. You can ask the audience if you want. Yeah, yeah, yeah, yeah. But what about the blue? So, like, these two points or those two points, give me two points. These two points right next to each other. Do we feel good about that? You don't feel good? What's wrong? You don't feel good? What's wrong? You would pick these two points. Would you two please fight? Tell me something about your system. Yeah, goodbye, right? So this is an important thing. I'm talking about the distance along the ambient protamine space. I'm not talking about this goofy sub-thing. Okay. So you haven't given. So, you haven't given me your final answer yet. So, you say the two endpoints of the blue. Okay, let's see what's going on. Yeah, let's give him a round of applause. You got one right. One out of two, halfway there, halfway to Mescal. We now know that he's not going to buy me a glass of Mescal, at least not for this. Okay, so what's going on here is the infimum of all distance quotients. The minimum of all distance quotients is this square root 2 over 3, because I take that distance and I divide by that distance. That's the smallest that the distance quotient can possibly be. And now I'm going to ask you, where is it, the large part? There, I'm going to say points infinitesimally close. Infinitesimally close. Why do you think that? Then we're moving. The ambient is following the curve right there. Following the curve right there and not putting something forward. I see. So it's like triangle inequality tells me that I'm losing something. Like, oh, that's good. I like that intuition. That's exactly right. Give him a round of applause. Take your seat. I'll give you Mescal later. So here, the supremum of the distance quotients is, yeah, you got to open it a little bit, right? The supremum of the distance. The supremum of the distance quotients here is pi over 2 because you take this quotient and there you go. Okay, so what this is telling us is that the distance quotient is always between these two numbers. So that tells us something about the extent to which this map is preserving the metric on the domain. Does that make sense? So now for uglier symbols. So you take the infimum over all points in the domain that are distinct. In the domain that are distinct, so that the denominator is non-zero. That infimum is the optimal lower Lipschitz bound, the optimal upper Lipschitz bound, replaces the infimum with a supremum. You're going to be bilipschitz precisely when alpha is strictly positive and beta is strictly finite. Okay. You're going to be an isometry precisely when alpha is one and beta is one. All the distances are preserved. Is one, all the distances are preserved. And then there's a notion of distortion. You should think condition number. Okay, the distortion is the upper Lipschitz bound divided by the lower Lipschitz bound. It's always at least one. Obviously, the smaller it is, the better. Questions about this one? Our task is to bilitiously embed our metric quotient into a new Hilbert space. Okay? Bilifiously, we want to preserve the metric structure as well as possible, which means we would like the distortion to be as small as possible. So when do bilips invariance exist? I'm saying invariant because it's a function of the quotient. Okay. These functions, if you think of them as functions over. Think of them as functions over v, they're constant on orbits, okay? They're invariants. How do we construct ellipsoids invariance? And how low can we go with the distortion? We'll start with some examples. Where do I get a read of my time? Is there a clock somewhere? That's my read. Okay. I'll just ask you periodically. Just ask you periodically, how's my time? And I'm due to finish at 4:50. Okay. Boss, 4:50? Let's call it 4.50 plus 10 for questions and transition. Okay, so I'm going to give you some examples of bilips and variants, and I'm going to break them out into this Punnett square according to whether the group is finite or infinite. The group is finite or infinite, and whether the input Hilbert space is finite-dimensional or infinite-dimensional. Okay, this just helps my brain because there's going to be a lot of examples flying at you. So, first we'll look at a reflection group. This is one of the prettiest types of groups acting on a vector space, if you're not familiar. Maybe wear a hat because your brain's going to explode. It's awesome. Okay, so baby's first reflection group is the Group is the symmetric group acting on a vector by permuting coordinates. Okay, so I'll ask the audience which member of the orbit of the first standard basis element under the action of the symmetric group. Okay, now think what is that orbit? Can someone tell me what that orbit is? What are all possible permutations of correct of this? Standard basics. Standard basis. Okay, thank you. So, which member of the standard basis is closest to one, two, three? Any guesses? Zero, zero, one. That's right. So, what's going on here is you're sorting these coordinates, and those coordinates are already sorted, and that somehow aligns them as best as possible. The quotient distance ends up being precisely the norm of the difference between the sorts. Okay? Okay, and what that means is sort determines an isometry on this quotient space to Rd. So curious things about this example is that our group in this case is generated by transpositions swapping any two coordinates. Also, the set of sorted vectors forms a closed convex cone in the Convex cone in the entire set. This is the image of this map. Okay. And then also, if I were to spin up that cone, it would essentially partition the entire space. I say essentially because of the boundary. The boundary will be double counted or whatever. Okay. So these are features that generalize the general reflection groups, and you're about to hear what those are. So a reflection group. So, a reflection group is a finite group that's generated by reflections. A reflection is a member of the orthogonal group with an entire hyperplane of fixed points. Okay, so it has negative one with an eigenvalue of multiplicity exactly one. So you have some reflections, and then you take the subgroup generated by those reflections. If that's finite, you call it a reflection group. Okay, so the symmetric group, permuting coordinates, is an example of this. Permuting coordinates is an example of this. It's generated by its transpositions. Whenever you have a reflection group, you're going to have, you look at each reflection, it gives you a hyperplane. Those hyperplanes carve out space into isometric cones. Each one is called a vial chamber. Pick your favorite vial chamber, and I'm going to define a map that receives a point and finds the member of the orbit that lands in your favorite vial chamber. In your favorite vial chamber. Okay, so in the case of sort, you're looking at all possible permutations and you ask which one lands in my favorite vial chamber, namely the sorted vectors. Okay, and this is an isometry in general. So for example, you give me this point, you look at this orbit, you find the guy that lands in the vial chamber, and you map to that one. Okay. So this generalizes the sort map. Another curious fact is that it's piecewise linear. Is it's piecewise linear. Sorts piecewise linear. So just something to keep in the back of your head. Going back to our Puntnet Square of examples, let's talk about something that some of you may have heard of called max filtering. So first example of max filtering, let's say our vector space is Rd and our group is the entire orthogonal group, not a subgroup, the entire orthogonal group. And I'll ask, Group. And I'll ask which member of the orbit of the first standard basis element is closest to this guy. Now, someone tell me what is this orbit in this case. Remember, the group is the entire orthogonal group. So what is this orbit? The unit sphere. So which member of the unit sphere is closest to that point? Maybe describe it? It's not the basis vector in the third direction. In the third direction, this direction of this, you normalize it onto the sphere, you project it on the sphere. See, I'm trying to make sure it goes through your brain. Okay, we're on the same team. Okay, so what's going on here is the distance between two things is really just the difference in norms because you can align them just fine. There's no restriction. Accordingly, this norm determines an isometry in this case. Okay, and here. In this case. Okay. And curiously, if you think in terms of Chris Schwartz, what's going on is the norm is the supremum over the orbit of some unit vector, the supremum of this inner product, your vector. I'm going to get some more water while you guys think about that. I was expecting to see half of you on email. Okay. You're actually paying attention. Thank you. Okay. So if you have a finite group, but not the orthogonal group, but a finite group, then you can mimic that move. Mimic that move. For a generic choice of templates, Z1 through Zm, this map where each coordinate function is the supremum of inner products, will be bilips on this quotient space, provided you have enough templates, namely at least twice the dimension. Notably, this is piecewise linear. That soup is really kind of. That soup is really kind of a match, isn't it? Piecewise linear. You get explicit distortion estimates for random templates. You just turn the crank on random matrix theory and you get it. And we think this thing's biological even for infinite groups, but we don't have theory yet. We have explicit constants, and they're not fun to look at. And they're not fun to look at. Oh, I don't know how close to ophthalmo they are. But they give you bilaterals. Back to our opponent square. This one's curious. So I have an infinite group acting on a finite dimensional vector space. And we're looking at examples where. And we're looking at examples where the metric quotient is equivalent to quotienting by a finite group, some other space. So here, I have a metric space isomorphism between my quotient metric and another quotient metric quotient for a finite dimensional space for the finite group. I know how to bilipsly embed this, and therefore I can pull through, pull back, pull, push, whatever. Pull back, pull, push, whatever. I can combine stuff to make it work. So I have my metric, I pass through my metric space isomorphism, then I've passed through my biolifts embedding. All together is biolifts embedding of the same distortion. Here's a fun example. Consider the orthogonal group acting by conjugation on real symmetric matrices. Matrices. Someone sell me invariance for that. We'll give you a hint. Real spectral theorem. What'd you guys say? Eigenvalues, okay? Eigenvalues is a great choice of invariants. In fact, if you take the eigenvalues, that will give you a metric space isomorphism with Rm modulo, the symmetric group. Right? Because how do you order the eigenvalues? Right, because how do you order the idea? Well, sort them, and now you're getting an isonic there. Okay. Cool. So that's a that's a guy of distortion one. This whole thing is an isometric. Okay. Great. How are we doing so far? Am I losing you? Are we all on the same page? You guys are kind of acting like my students when I'm lecturing, kind of being When I'm lecturing, kind of being quiet. Not sure how I like that. There's going to be a bunch more examples that we go through the rest of the talk. This means like new stuff that we know stuff about, that means it's going to be the focus of an open question. Okay? A serious. Inconceivable. You had an assumption, right? In the first example, pilot one, you are happy with pilot. What would I need to do in the first case to get this subject? You know, you're going to find out in this talk that sometimes it's not possible to get an isometry. Okay? Thanks for that. I didn't even plant that. That was great. Okay. Okay. Other questions? Okay, so we're talking bilipsis invariance. If you throw away the first word, there's been a lot of people thinking about invariance for like hundreds of years. Like David Hilbert invented algebraic invariant theory. So you might wonder, what the heck does that have to do with what we're talking about here? Okay. Presumably, there's some connection. So their setting is not. Their setting is not more general nor less general, it's just different, and there's some intersection. So they're looking at specifically polynomial invariance under linear group actions. So not necessarily orthogonal group actions. So for example, suppose your vector space is the real line and your group is just plus minus one. It's a negation. Okay, well, let's think. Okay, let's think about the polynomials that are invariant under the action of this group. Someone tell me what these polynomials are called. Just shout it out loud. What are the polynomials that are invariant under the action of negation? Even polynomials. Okay? Right? P of minus x equals p of x. Do you remember that class? We covered this two weeks ago. So now. So now, what the even polynomials. These have structure. They're a vector space. What are they spanned by? The even monomials, okay? And the even monomials, there's more structured. Like if I have one of them, I can get the rest of it. Which one? X squared. Once I have X squared, I can. x squared. Once I have x squared, I can take his products with himself, and then I can linearly combine. So we say that as an algebra, these guys are generated by x squared. Cool. Curiously, if I have two orbits that are different, then this particular polynomial distinguishes them. Okay. You track them there? This generalizes. So Hilbert has this theorem that says if your group is nice, don't worry about that. Let's think finite groups for down. Then the G-invariant polynomials, they behave like this in that they separate all the orbits. Also, they are finitely generated. So there's a finite list of things like that that give you all of them. So what that means is if you So, what that means is if you have a couple of orbits that are different, then there's going to be some gene invariant polynomial that separates them. Therefore, they're separated by the vector of generating polynomials. Okay, so I have this injective polynomial map. It separates all possible pairs of orbits. But we're interested in bilipids. Okay, we got separating, but we want bilipids because we want. Separating, but we want by ellipsis because we want to preserve the metric information as well as possible. Let's look at another example. So take the vector space of Cn, so vector of n complex coordinates, and we'll look at this familiar group of permuting the coordinates, okay? The G-invariant polynomials, polynomials that are invariant of the actual. Polynomials that are invariant of the action of the symmetric group are these things called the elementary symmetric polynomials. You don't know what that is? Let me tell you. So based on the previous slide, there's an injective map that receives this quotient and kicks out a vector of n complex numbers. And what those numbers are, the coefficients of this polynomial, the polynomial whose roots are the coordinates of your vector, right? This polynomial doesn't change when you permute the roots. You permute the roots. It's the same polynomial. Therefore, the coefficients don't change, so they're invariant. Those coefficients are functions of the coordinates of x that are called these elementary symmetric cool. For example, if you have the vector x is one, two, three, four, all the way up to 20. Okay, so this is what they're sitting in here in the complex plane. That gets maps to the coefficients. That gets mapped to the coefficients of this polynomial of degree 20. Okay. I want a bilipschitz invariant. So in particular, it should be robust to noise. Okay. Let's add a little bit of noise and see what happens. I'm going to add 10 to the minus 6 to the coefficient on t to the 19. Let's look at what the roots become. So observe that the imaginary axis, that's two, okay? And these guys are huge, right? I slightly changed in the invariant domain, and it totally dorks with my data, okay? This was discovered by a numeric analyst, and he lost all faith in everything. Okay. So, this is a problem, right? I have this, I have hundreds of years. This, I have hundreds of years of algebraic invariant theory, and it gives me this. Okay, so the bilifius invariants that I've shown you, I kept telling you stuff like piecewise linear. I'm trying to tell you these are not differentiable. And that would spook anyone who's interested in algebraic invariant theory. Polynomials are not non-differentiable. Not non-differentiable. For example, let's consider the case where I have the real line and my group is negation. We're going to do a magic trick. I want you to think of your favorite function that happens to be differentiable and even. When I say even, I mean he's invariant. So your favorite differentiable invariant. Think of, don't tell me, think to yourself of a particular function that's different. A particular function that's differentiable and even. You got one? Okay, now take the derivative at zero. Okay? Don't tell me what you get. You got zero. So what's going on there is your function gets flat as you get close to zero, right? That's bad for lower Lipschitz bad one, right? Bound, right? Like my lower lip shits bound is zero. That's bad. I'm not lower lip shits. Therefore, I'm not bilips. That's awful. So if I'm going to be bilipit, I better not be differentiable in this case at zero. In general, I better not be differentiable at points with a non-trivial stabilizer. So I was calling the shots here is that zero is stabilized by negation. Is stabilized by negation. In general, there might be other points that have a non-trivial stabilizer. Your bilechist invariant is necessarily not going to be differential at those points. In fact, that's what we're going to do. I am conveying that the hundreds of years of algebraic invariant theory, which is dealing me differentiable invariants, don't Invariants don't cut the mustard for our bilibet story. Okay, so if they're differentiable, then they're not lower lips, therefore no good for us. Cool. So this is saying those examples that were piecewise linear, they're non-differentiable, but don't judge them for that. Okay, it's not a bug. It's a necessity. They have to do that in order to satisfy bilips. This is also telling us that for the bilipsitz story, algebraic and variants. But by Lipschitz's story, algebraic invariant theory isn't enough. We need to go beyond. Okay? How am I doing on time, Har 33. Thank you. So this is the state of things. So we had a problem at zero in the case of x squared. In general, X squared. In general, there's going to be a problem at zero because the origin is fixed by any member of the orthogonal group. Okay, he's going to have a huge non-trivial stabilizer, the whole group, he's in the stabilizer. So this is how you can fix it. Restrict to your function at the spheres, the sphere of the input. That's avoiding the origin. That's nice. Suppose you have a function that maps the sphere to the sphere of another Hilbert space. I'm going to tell you what homogeneous extension means, okay? Extension means. Okay. So you give me a point in the full Hilbert space, not just the sphere, but anywhere. You decompose it like polar. Okay. And then you shove the direction into F, and then you scale by the magnitude. Good? For example, if your vector space is the real line, your group is negation, and you remember that. Line, your group is negation, and you remember that x squared, and in fact, any differential invariant is terrible. We'll start with the polynomial invariant. You know it's bad, but then restrict to the sphere, which is just plus minus one, and then homogeneously accept. And then you get what Rebecca Willow would do. Okay, this is good. Yes. Um, so actually, in this case, this is an isometry. That's that's super good. In general, uh, you can control the optimal bilips bounds of the homogeneous extension from the optimal bilips bound of the f that's only defined on the sphere. Okay. Um, so I get the this isn't a bound. These are the optimal bilipsitz bounds for the homogeneous extension. And I was kind of surprised that I was able to get this when I was trying to prove this. I assumed I was only going to. When I was trying to crew this, I assumed I was only going to get garbage bounds. And I happened upon an identity that I have never seen before. I asked around. No one's ever seen this. Maybe you have. This could work for Euclidean distance. So forget quotient distance. The Euclidean distance between two vectors, I took this polar decomposition thing, equals the square of the difference between the magnitudes plus this. Plus this nonsense, I multiply the magnitudes and then I multiply that by the distance squared between the unit vectors, the directions. This is like some radial Pythagorean theorem. It's trivial to prove once you have it. Okay, so that's kind of cool. Let's use homogeneous extension for good. So it suffices to think about the sphere because then I can homogeneously. To think about the sphere because then I can homogeneously extend. Our group acts freely on the sphere if there are no points with non-trivial stabilizers. Okay? So that's good because I don't want a non-trivial stabilizer if I want to be differentiable on the sphere. Yes, super. If the orbit is The orbit is, nope, that's not it. So the stabilizer, I could do orbit stabilizer, if the orbit is the size of the group. So you're in the stabilizer precisely when your point is a fixed point of that member of the group. So if everything moves your point except the identity, then the stabilizer is just the identity. It's triple. I bet it was tripper. So, if I have, if I don't have any non-trivial stabilizers, I can afford to do a differentiable invariant, at least on the sphere, and then homogeneously extend that thing. Turns out, if you have a finite group, having a free action on the sphere is equivalent to there being a biliptidous polynomial invariant on the sphere. And that in turn is equivalent to getting away with 2d minus 1 coordinates. 2D minus 1 coordinates. Okay. 2D minus 1 might sound familiar to phase retrieval folks. The way to prove A implies B is you grab generating polynomials to get injective, and then you need to promote to bilipsets. So you also need an immersion. So you need to control gradients. And you do Lagrange interpolation plus a compact argument to control gradients. To get B implies C, you do a generic linear projection. Generic linear projection, and you apply some algebraic geometry. The standard moves. For C implies A, take the contrapositive. We already know that if you don't act freely, then you have a non-trivial stabilizer. Therefore, good luck having a smooth bilatience map. In the special case where G is abelian, we actually have explicit polynomials that do the job. D squared over. D squared over. Yeah, let's do an example of this. So I have V is a real Hilbert space. My group is plus minus identity. We call this real phase retrieval because that's the group under the hood of that problem. Take the outer product of your input vector. Okay, so I have a vector in the sphere and I multiply it by his transpose set. This is This is the map that is our d-squared coherence. Let's figure out what the optimal bilix bounds are. So, we're going to look at the square of the distance quotient. So, upstairs, I'm taking the difference of a square of outer products. I open the square cycle trace, I get this. Downstairs, it's a lot easier. Just open the square, and then the numerator looks. And then the numerator looks like the denominator. So there's a square and something factors. You cancel. You get that. Okay. So this is saying, remember x and y are on the sphere. This is a number that ranges from zero to one. So the distance quotient squared ranges from one to two. So the optimal bilips bounds, this map, are one and square root of two. This is true for any. This is true for any real Hilbert space, even little L2. So in max filtering, you could attempt to get biollipsis bounds, but then you'll have infinite distortion for little L2. This is Jameson's result with Pazaza and Ingrid. But you can get root two just by doing this. In the complex case, it's near-identified. Near identical. Okay, it's the same truth, the same bounds. There's another case that you probably haven't thought of. Your vector space is the complex plane in your group is the rth roots of unity. Okay, you want to mod out by that. You think about what that would be like. Here, your invariant is take the rth power. That's an invariant. And the optimal biles bounds after a little bit of calculation. Downs after a little bit of calculus involves cosecant for the first time in my research. So thanks trigonometry class. Great. These are free actions. We don't have balance for other free actions. That's okay. Questions about these slides, Bernie? Does this mean anything for the people banging their heads? Is this something like the best you can do with Facebook? I have another part of this talk where I'll circle back to that. Any other questions? Also, what you didn't tell me your name. What's your name again? Oh, not Dustin. Thank you. Testable, thank you. Um, okay, let's talk about what you want: minimizing distortion. Okay, okay. This is you're about to receive something that blew my mind when I learned it. I didn't come up with the thing that's going to blow your mind. I learned the thing that's going to blow your mind and it blew my mind. And now I'm paying it forward. Okay, this is insane. This is this is great. Okay, so let's talk about Euclidean distortion. So you fix. You fix a metric space and now consider all possible bilips maps into all possible Hilbert spaces, not necessarily separable. Okay? And take the infema. This is the best possible distortion. Cool? It's called the Euclidean distortion. Okay, here's the crazy. Here's the crazy thing. You ready for this? That is first of all. I don't know how to think about that, right? Like, I'm optimizing over big things. Okay. This is finitely determined. Okay. It's finitely determined. So what you can do is take all finite subsets of your metric space. Okay. So that's now a smaller metric space. At a smaller metric space, you can find the Euclidean distortion of that finite metric space and take the supremum of that, and that will give you exactly the Euclidean distortion. Okay? I'm a little disappointed by how not flabbergastic you guys are. This is wild. Sorry? Sorry? Possibly huge. Yeah, yeah. In particular, the way, so one inequality is obvious. The other inequality, you construct the thing using ultra powers. That's correct. Okay? Wild. Okay. Once you have a finite metric space, you can find the Euclidean distortion. You can find the Euclidean distortion by you could have written this semi-definite program down on your own. Okay, like this, this is not wild, but the point is you can actually compute Euclidean distortion for any finite metric space. Okay, it's just semi-definite. So the examples I showed you, real complex phase retrieval and roots of unity of the complex plane. Roots of unity of the complex plane. If you use this stuff, you sample the circle densely, you argue what this SDP ends up being. It's nice because you get circular matrices. So SDPs turn into LPs, which you can analyze spectrally. And you take limits, those examples all have minimum distortion. So that's square root two for a real and complex basic tree, where it's op on. Optimal, okay, over all possible bilateral maps. That's the minimum distortion. Same with that cosecant nonsense. That's optimal for roots of unity in the complex plane. Georgia. So the distortion is the quotient of the top and bottom. And bottom, forget a map. No, I'm telling you that the quotient, the smallest the quotient can possibly be, is square root two. The bottom and top are related because the target space is Hilbert space. So I can scale by any factor, and that'll scale the bottom and top. Scale the bottom and top Lipschitz balance by that factor. So the quotient is really what's calling the shots here. Does that answer your question? Great. Are we doing on time? 46. Oh, man. Okay. So now I'm going to show you some bizarre examples. So think of little L2, where it's a sequence. It's a sequence of vectors, not numbers, okay? Vectors in Rd, and my group is S infinity. So, all possible bijections of the integers to itself. Okay, I'm going to consider this quotient. You may not be surprised that in this case, the metric quotient is different from the quotient. Okay? Explicit example. Explicit example, let's look at this particular vector that's in little L2. Okay, so one, one half, one third, one fourth, you get the idea. Let's swap the first two coordinates. That's a move I can do in my group. Okay, it gives me a new member of his orbit. Let's swap the first and third coordinates of this guy. Okay, that gives me another thing. Let's swap the first. Another thing. Let's swap the first and fourth coordinates of this guy. It gives me a new thing. It's starting to feel a little like Hilbert's Hotel, right? I'm shifting everything to the right one at a time, and the first coordinate is shrinking to zero. In the limit, this is going to something that is not in the orbit of the original because the original doesn't have a zero coordinate. Okay? Okay, so I have to consider the closure of this orbit. Okay, turns out when d equals one, you can generalize what sort would have to be in order to account for the fact that things are infinite and some coordinates are going to be positive and some are negative. And that's a little annoying, but you can get a nice sonometry in that case by generalized sort. If the dimension is at least three. If the dimension is at least three, the distortion is infinite. There does not exist a bilips embedding in any Hilbert space. Reason being, this metric space contains arbitrarily large expander graphs, and those have arbitrarily bad distortion. We don't know what happens yet, too. I want to close with this seemingly fundamental example: L2 of R2 mod rotations seems like something one should care about. Yeah. Related quotient, L2 of Rn mod Rn, like translations. L2 of Rn mod Zn. Okay, do another. L2 of the Taurus, mod the Taurus. That feels similar. Similar. L2 of the finite cyclic group mod himself. Okay. I would like to know if I could biliptiously embed these things. That feels like it would be useful. That's equivalent to asking if the Euclidean distortion is finite. I don't know if they're finite, but I do know that they're all lower bounded by a single quotient, namely L2 of Z. Quotient, namely L2 of Z mod Z. This seems really fundamental. And we don't know if we can embed this in Hilbert space. So this seems like a worthy target of investigation. Is there a bilips that's map from little two of Z mod Z into any Hilbert space? If not, then you're not able to embed these guys by virtue of that inequality. If so, then maybe you could embed these. I don't know, but it's a good place to start. Lots of open problems. I'm currently really interested in this thing. I want to generalize. Generalize this notion of homogeneous extension, where I resolve not just singularities at the origin, but singularities elsewhere. I would really love some machine that takes algebraic invariant theory and fixes things to make bilips invariants that are possibly optimal. They were optimal in the case of phase retrieval. Can I replicate that? All of the distortion. All of the distortion minimizing embeddings that I've ever seen happen to be semi-algebraic maps. I mean, all of the things that I'm thinking about are fundamentally semi-algebraic. So maybe I shouldn't be surprised that the optimizer is staying in that category, but that's not obvious. And then, yeah, these examples, I'd like to know their distortions. Some papers. That's my talk. Thanks. We're going to have about eight minutes for questions. I'm sorry, what's your name? Oh, okay. You're talking about molecular structures. Okay. So I guess I'm actually very weird that it's in here. Yeah. A lot of builds on my slides. So I don't think I talked about molecules, but what I'm writing rhymes with a talk from yesterday where I care about the graph, but I represent as a matrix. Okay. If I have weights on my vertices, then I have special numbers on the diagonal. If I have weights on my edges, Diagonal. If I have weights on my edges, then these ones are other special numbers. The ambiguity introduced by passing to a weighted adjacency matrix is the same issue, the same group invariance. So this technology generalizes to beyond this binary stuff. I really just need this Hilbert space. And all of those examples land in this Hilbert space. So, I don't have optimal bilipsitz embeddings for this particular group action. This is a finite group acting on a finite dimensional vector space. Therefore, max filtering delivers a biliptuous embedding, but our current But our current bounds on the distortion are not as good as I want them to be. And then I just very confused. I guess when I've seen people try to come up, take a graph representation of the model and then come up with some sort of canonical ordering of the atoms so that they can plug it in and sign up. Then it doesn't work very well. It doesn't work very well. In particular, it's just super unstable. That's right. And so I'm wondering if the bounds that you're saying are corresponding to that empirical phenomenon, or if we think those are so I can speak to that empirical phenomenon. What's going on is they're selecting a representative of each orbit and uh And that's a different map than mapping the entire orbit to a single point. Like they're, well, they are sending it to a single point, namely a representative point, but that map is not necessarily bilipschitz. It is bilipsitz in the case of reflection groups, okay? Where you have a canonical choice, namely this vial chamber, but this group action is not a reflection group. And so that choice will end up Voice will end up being unstable, like you're saying. Is that good for Rahman? What are the two things? the two things oh the explicit example that they're working with okay how does that play into this if you take the outer product then it won't suffer from the same difficulties that they have so the data you get from phase retrieval unfortunately is of the form of a max filter so you're stuck with that So, you're stuck with that style of measurement. Our problem is instead trying to find the optimal, the minimum distortion embedding, and their distortion embedding is infinite, infinite distortion. So that doesn't set the mustard. I haven't thought about whether I could weasel some sort of phase retrieval application from this technology, but that's interesting. So the some group actions really want to be interacted with computation-wise, and they're kind of the usual suspects. So for example, if you take short fat matrices mod out by orthogonal group on the left, then the Then the quotient distance, it's like a Priscretis problem. So you just solve a spectral thing and it gives it to you. Accordingly, you can compute a max filter just as quickly because the max filter is really the cross term when you open the square of the quotient distance. So you can compute that just as quickly as you can compute the quotient distance. So that gives us a large family of examples. There are some examples. There are some examples, like this one, where it's not so easy to compute quotient distance. In particular, if you could compute quotient distance between two graphs, you can tell whether those graphs are isomorphic. And so you're solving the graph isomorphism problem. So there's a spectrum of these things. There's a zoo of examples. And some love to be analyzed, and others are more volcanic. So other questions? George? I would like to take my problem. Why? Sorry. Okay, let's go. So, if I have a vibe that's invariant, then Then he's invertible in his image. And so you can just reverse the arrow and plug in your smooth invariant and go backwards. And then diagram commutes, you can go forwards with a accurate app. So, yeah. Is there a question over here? Oh. Don't sell anything. My mind's not blown. Can you please solve that? Okay. Good. I'm just finding it. I don't know, but that would be great. So, for example, I ran this program a bunch of times for this example here, L2Z modZ, and the best lower bound. Z and the best lower bound I could get was like root two, something like this, like 1.4 something. And I was, I mean, STPs are kind of a limiting factor here. So I could only access like metric, finite metric spaces on like thousand points. And I was trying to be clever on what I was doing. And it would be nice to have some sort of theory that says. Have some sort of theory that says that for finite sets of this size, you get an approximation that's this good. Yeah, that would be nice. I don't know if that's the only thing that we're doing.