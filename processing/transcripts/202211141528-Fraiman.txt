This is the first time since before the pandemic that I traveled abroad, so I'm quite excited to be here. And I'm going to talk about the distribution of weights in minimal spanning cycles, which are a higher dimensional analog of minimal spanning trees. We'll go over that. So this is joint work with Cheyenne Mukherjee and Rugan Tope, which right now they're in Leipzig and Bangalore, but at the time they were both at Duke. They were both a duke. He was a postdoc of Cheyenne. And Cheyenne is interested quite a bit in topological data analysis, where these simplicial complexities show up. And the minimum span in a cycle is connected to dead times of certain cells in the persistence diagram. So that's kind of what got us started working in this. So, oops. So, I'm going to review very briefly. So, I'm going to review very briefly. I know everyone knows what a minimum spanning tree is here, but we have a weighted graph, positive real weights for every edge. A spanning tree is basically any subgraph that has no cycles. So it's a tree and spans all the vertices. And we can define capital T for that. And the minimum spanning tree is a tree that minimizes the total weight. So the sum of the weights of the edges here is a. So, the sum of the weights of the edges, here's a picture showing that the both edges are unspanish. So, this is a combinatorial optimization problem that is quite tractable. There's many algorithms, greedy algorithms work well, Frames algorithm, Kruskal's algorithm, which I describe here, are the most famous ones. So, you can order the edges by increasing weight, and at each step you basically add one edge if as long as it doesn't create a cycle. As long as it doesn't create a cycle. Okay, and you stop when you span the whole thing when everything is done. So that's kind of what it works, what it looks like. And you do that with oblivion distance. So it's trying all these things, and sometimes it's going to reject and not add things, and eventually everything ends up being correct. Okay, so I'm going to look at the average case analysis, so a random model for MST. There's two main models that people have looked at. One is more geometric. Have looked at. One is more geometric, so you point throw points on the square or higher dimensions, and the distance between points is given by the actual Euclidean distance. So what's nice about that is it satisfies the triangle inequality, etc. But there's also a combinatorial or kind of like a mean field model for distance that is just a complete graph where all the edge weights are IID, say, uniform zero. IID say uniform 01. What distribution is here? It's not too important. I'm going to focus on uniform because things are a little bit simpler to state, but you can put exponential, you can put pretty much anything that has a continuous distribution function, etc. Here, people have used a lot of tools related to suballitivity, it's connected to random geometric graphs and so on, but I'm going to focus on more the combinatorial setup. I'm going to I'm going to briefly recap a couple of results, but this is by no means comprehensive. One of the first results about the random combinatorial minimum spanning tree is the limit for the expected total weight by freeze that converges to C that three, the sum of one over n three, which is one point two zero two, etcetera. 1.202, etc. This was extended by Steele to more distributions and removing the finite variance condition and so on. Then Aldus in 1990 started looking at these things via local weak limits. At that time actually it was like a fringe conversions. It was not really quite a really local weak limit theory, but it's Theory, but it's more or less the same thing. And then Aldus and still later formalize it as local weak convergence. And more people have looked at this. Luigi, Alari Berry has proved also. So the local weak limit could be when you're looking at something infinite, you can wire things at infinity or not. So there's a choice to be made there. And I think the first limits were. The first limits were not free, so not wired, and then Luigi looked at this wired model. Dante-Jensen proved the CLT, so if you subtract C dub 3 and divide by the variance, you get a symptomic normal distribution. There's a lot more work. One of the more recent that I want to mention is that people figure out the scaling limit of this thing in the Bermochaus or metric converges to some compact. Versus to some compact tree that is not the continuum random tree. It's a different thing. It has a different password dimension. It has a dimension 3 rather than 2. Or 1 third rather than 1 passer. Again, this is by no means comprehensive. There's a lot of more exact formulas. Field and Steele, for example, have some nice connections of this, with the integrals of 2. Of this with the integrals of two to polynomials and other things that people probably are more familiar with. But I want to talk about what people have done in the higher dimensional setup. So what is a simple complex? We're just generalizing a graph. So rather than just having vertices and edges, we're going to have triangles, tetrahedrons, etc. This has to be closer than their boundary, and they have. Boundary, and it has the, you know, they have the nice property that if you do boundary twice, you get zero. So that allows you to define homology, right? So the image of this guy is inside the kernel. You do the quotients, you get all the homology groups. And it turns out that, so this is a little bit of notation, the dimension of a face is going to be the number of vertices minus one. A triangle has three vertices, dimension two, and kv is going to be all the faces. Is going to be all the faces of dimension of 2. But so the Betty numbers are exactly the dimension of those homology groups. And at a very high level, you can think that what they are doing is counting the different types of holes in different dimensions. So beta 0 is the number of components minus 1. Of components minus one. This guy is the number of tunnels like this here, so just a cycle that it kind of contract. And this is the number of voids. So, for example, if this tetrahedron has all the faces but not the filling, then that would be a whole two-dimensional hole and so on. Okay, so what is a spanning A-cycle? Well, instead of edges, you're going to have D-faces, so the largest dimensional faces, say triangles, and you're going to have And you're going to have, you can rewrite the spanning and acycle condition as saying that beta zero is zero. So basically you have only one component, but this component is minus one, and that you don't have any cycles. So beta one is zero. Well, so this was proposed by Gil Kalai in the 80s, and he proved something, a counting formula for this that generalizes. Formula for this that generalizes Cayley's formula for higher dimensions, and this is the natural thing to put in there. Okay, so what is the setup that we're going to look at? We're going to look at the complete d-dimensional skeleton. So, meaning I have all the edges, I have all, you know, for example, if I'm doing it on dimension 2, so I'm adding triangles, I have not only all the vertices, but I have all the edges between those, and I'm adding. I have all the edges between those, and I'm adding faces to that. I'm doing it in dimension 3. I have all the vertices, all the edges, all the triangles, and I'm adding volumes to that. The weights are going to be IID uniform, and the object that we want is this thing. So the minimizers of the sum of the weights. Okay? All right. So you're only considering the default. You're only considering the D faces. Yes. So you can do a lot more, but this is the model that people first looked at: that is, you have everything up to dimension D, D minus one, and then you add other D phases. You decide which D phases you keep, basically. The rule is you have to know D homology or D minus one homology. Yes. So um this uh I the Um this uh uh the earliest paper that I'm aware of is by Hiroka and Shirai and they proved that the uh expected w total weight uh is uh they they got the order, is of order n to the d minus one but they couldn't get the constant. That happened two years later by Hino and Kanasawa. They found what the asymptote, you know, if you divide by n to the d minus one, this converges to a constant that is sort of like generalizing this zeta of three for dimension one. One to higher cases. But these constants are not very nice. There are some complicated integrals that I'll show you later. And that was kind of our starting point, what we wanted to do. I don't understand the model. So the uniformly weighted complex. Yes, so I have. You mean you're taking all the complexes that satisfy your. No, no, no. I have the completely dimensional thing. Right. So, and every d-dimensional. So and every B-dimensional phase has a uniform? And I'm gonna keep some of those to get a spanish. So and when you say keep uniform, it sounds like you're taking all the ways of adding some of the D simplicity to satisfy your rules. And then. What do you mean uniform? Oh, uniform because the weights are uniform. Weights are uniform. But then, what if the weights. It actually doesn't matter too much if they are uniform. Matter too much that they are a different thing, but that's the name that they gave it to it. So I'm not claiming that's the best name. There are weights on these ones. Yes. The weights don't really tell you, interact with the constraints. Well, you're going to pick the thing that satisfies the constraints that minimizes the sum of the minimum split sum. Yeah. Oh, great. Thank you. Very, very. Okay, so yeah, so they proved that it's order n to the d minus one, which in dimension one basically is: okay, it's a constant, we know what that constant is, but actually once you do it with triangles or things, you have to scale properly, so it's not just grows with n. And then these guys got the right constant, and that was our starting point. And what we wanted to understand was the distribution of weights. And this was motivated because. And this was motivated because of all all the work with local uh li and weak local limits local weak limits, sorry. Uh because these proofs were, I mean they're super nice, but there are a lot of heavy formulas there, and we thought that there could be a way to get this in slightly more intuitive, at least to me, more probabilistic way. So, the distribution of weights, it doesn't look like maybe this is a local quantity, but it This is a local quantity, but if we want, if we look at this guy, oh, kind of like, well, maybe I have to rescale them, but suppose you look at something like this, then this is a local quantity because you can think of this as the weight of a random edge. So pick a random vertex, look at the neighbor, that weight. So you can turn that into a local quantity. So there was hope to approach it with local with limits. Local with limits. Delfinado and Les Brun proved something that sounds very simple, but it has to be done anyway. If you have a simplicial complex of an a phase of dimension D that is not in the complex but the boundary is, so you have all the, if you're adding a phase, you have all the edges, for example, then you can sort of classify it into positive or negative. Only one of these two things happen. Either you Whether you close a simplex and create a new void, a new hole of the right dimension, increased by one, or there was something that you couldn't contract, and when you put that phase, you can contract it now. So you kill something in the minus one-dimensional homology. The analog, and every other Betty number doesn't change. The analog of this in the graph case is very Case is very simple. What you're saying is when you add an edge, you either connect two components or create a cycle. So that's increasing, creating a new cycle or killing one component because you connect them. And then prove that Khrushchev, you know, once you have this, you can extend Kruzkal algorithm and it works and it gives you minimal spanning tree. Okay, so perhaps this doesn't look like for somebody that hasn't looked at this problem, maybe it's not too natural to look at this random complex. But there is a nice connection between the the edges in the MA in the minimum spanning tree that have weight less than P, for example, and GMT. So this is gonna be similar to that. This is going to be similar to that. So, the linear machine complex is the analogous of the Erdos-Reni graph, but in higher dimensions. Again, complete skeleton, n vertices. You have every face or edges and triangles of d minus 1 dimensions. And then the things that you add are the d faces and you add those with probity d. So keep adding and you get something. Okay? It turns out that. That one of the things that people wanted to study about this is: is there a phase transition analogous to the emergence of the giant in GMP? And it's less obvious how to even formulate that because this is connected from the beginning, so you cannot really talk about components. So, what it turned out to be that the right object to look at is called the shadow, and these are the The faces that, when you add, they actually increase, so create a cycle. Okay? When they're added in order? Like you're adding the face. So look, you know, for any complex Earth, you have this thing. You can define the shadow and say these are the faces that if you add that face, you create a new hole. Okay. In the graph case, what this is saying is these are the edges that link two components. Sorry, that they are on the same component because they create a cycle. So here you might see the connection. Once you have a giant component, you probably will have a giant shadow here. And that's what happens in higher dimensions. You can define the density of the shadow, this function. Density of the shadow, this function divided by how many things potentially you have in the shadow, and linear and pelev prove the emergence of this giant shadow. They gave an implicit formula for S and they showed that there is a critical value such that before that you don't have a giant shadow, after that you have it. And what's kind of surprising there is that you have this jump also here. Is that we have this jump also here. So, this is Erdos-Reni, right? So, at one, we know that one over n is the phase transition, and after that, it grows. Well, you know, there's an implicit formula, the survival rate of a Poisson process. Here, it's kind of the same thing, but oops, all of a sudden you start already with something kind of big. The y-coordinate on the graph? The density. So, this S. The density of the shadow. So, this is not exactly the giant component, but more or less the same. And that's the formula, what it looks like. And they did, use the tools of local convergence. What you have over there is some sort of tree that each face has potentially quite some number of faces here and here. The route is special because you have. The root is special because you have three, so there's a bit of a size bias there, but then every guy, every edge has two more. So one is used from your parent and then two more, and you have that. So you have like a multi-type branching process where one of the types is actually boring, it's deterministic. You always have two children, say if you're looking at triangles, or d minus one children. And the other type is, say, Poisson or something. And you alternate. And you alternate. So, every Poisson child gives you a deterministic child, and vice versa. So, using that technology, they figure out this thing. And this is what we found out when we were looking at the MST and MSA. So, this is a histogram of weights in the MST. And it turns out that, okay, it looks kind of close to that. Maybe not super close, but this is 400 vertices. So, believe me that if you make it with. So believe me that if you make it larger, it becomes closer. And this is sort of implicit in that paper of Aldus. It didn't quite prove this, but... Is that rescaled at all? Or you get weights all the way up? They are rescaled. They are rescaled. Yeah, I'm cheating a little bit. But this is what happened in dimension 2. And now this is like 100, but remember that you have like 100 to the power 3 phases. And we saw that. And we saw that it looks like that jump was happening there. Okay, so we tried, you know, we decided, let's see if we can prove that. So we are going to look at the empirical measure, but yes, we are rescaling like you suggested. It's not just the way to remember, you know, if you are taking the n smallest uniforms out of n squared, they are order 1 over n, so it makes sense that you rescale it like this. You rescale it like this. And this is actually, you know how many things you have in the this is the n to the minus one essentially. So you know, choosing like that. And then the shadow measure is basically one minus a shadow density. And this comes from that rerouting thing, basically. The bias of the router. In the higher dimensional paradigm you you're still at about one over n so you add a lot more but you scale them by one over n. But you scale among all the possible weights, the number you need to make up your acyclic complex is still one of random or is some other? Still 1 over n of them, or is some other? No, the number of faces that you need is 1 over n of all the phases that you can. Yeah, all the possibilities. Yes. Yes. Okay, and we could prove this. We could prove that this measure converges weakly in, well, it's a random measure, so it converges weekly in some probabilistic sense. We can do it for an LQ, for any Q. For uniforms, if you put a For uniforms, if you know if you put other weights and they don't have moments, then you can prove weaker things like maybe improbability, but you could prove that. And I want to tell you just briefly, this is not how we did it, but this should at least give you an idea of why it should be true. So, you know, we're looking at weak conversions, so we take a test function and integrating with respect to this measure is basically just the expected value. So you're evaluating the So, you're evaluating the F at those delta points, this scale. And you can make this a deterministic sum by just putting the indicator of the phase being there, take that out. And this is one of the crucial parts. If you condition on the weights and on the phases that you've seen, the next phase that you're seeing, I'm ordering the phases, I forgot to say that. I order the phases by increasing weight. If you condition on If you condition on the previous weights and on the phase, this is equally likely to be any phase. It's exchangeable. So I can separate these two things. This is basically the probability that a random phase is there. And then maybe it's not too surprising that the probability that a random phase is there is essentially doesn't have to be in the shadow. So that's where the one minus shadow sort of shows up. And now, you know. And now, you know, it's relatively easy to believe that this looks like a Riemann sum. You know, there's stuff that, you know, this is not the easiest way to prove it, actually, so we did it a different way, but this should be very close to xi, where you are, you know, taking the expected value. This is the density, so, and this looks like this integral here. Okay? The d minus one comes from the fact that. The d minus one comes from the fact that you need still the delta x here. So you take one of those here, and you're left with, you know, that one. Okay. That's great, but you know, weak convergence means that we can do this for bounded functions, and one of the ones that we might want to integrate actually is x, because that will give us the total weight. So it's pretty easy that if you have something that resembles uniform integration. Something that resembles uniform integrability, it's not quite the same thing because you're actually truncating the integral inside, but it's more or less the same. Then, if you have this condition, then the integral converges. And with that, we can recover those results of Kino and Canada and Freeze and Kino and Canazawa. So Fries proved that this converges to C dot 3, and if we put x of x equals to x, dimension 2x, dimension 1, this integral is precisely this. Well, okay, there's an n and n minus 1, but in the limit, that doesn't matter. So these two are the same, have the same limit, and that this is a calculation that I think appears on Albus and still, or something similar appears here. You have to use a couple of tricks, but you get that this is CW3. Okay? And then And then in higher dimensions, I told you these guys proved, well, they didn't only prove the convergence of the sum, they proved the convergence of weighted sums as well. That was done for the MST before, but it's still. And this is their ugly constant. I'm not going to even try to read it. But it turns out that if you take x to the alpha, this is still integerable. Uh um integrable, uniformly integrable, and you get the same limit and that is equivalent to the to the to that integral. And it to me it looks a little bit happier. Okay, okay, so maybe I'll briefly mention one more thing. So when we rescale that thing, there is a little bit of mass that sort of well, not mass, but there's a little a few phases that sort of escape to infinity, because we are uh multiplying by n. We are multiplying by n. Those extremal ones are too little to actually matter in the sum, but we wanted to, you know, but they are interesting for some reasons because they are related to, for example, the connectivity, like isolated vertices around the threshold for connectivity. So we look at the shifted countant measure where we scale by that, but we shift by d log n. So most of the bulk actually. So, most of the bulk actually now goes to minus infinity, and I only see the extremes here. And it turns out that that converges vaguely, so as point processes to a Poisson point process with e to the minus x intensity measure. And that, in particular, recovers Erdos-Reni isolated, you know, a number of isolated guys at the trust hope for connectivity. We know that it's positive. For connectivity, we know that it's Poisson with parameter e to the minus c. Well, that's precisely equivalent to looking at that, because if you're isolated, then you're still not connected. You have to add that edge to the MST to connect to that guy, and you're going to add the guy with the smallest weight, which is that. And similarly, Carl and Pittel did the number of isolated B phases at the threshold for vanishing homology. For vanishing homology, which is kind of the equivalent of connectivity. And we also recover that by looking at this thing. So isolated here means touching it or it means it's... Right. So for example, suppose you have this edge, and there's a lot of triangles that potentially could be there. None of them are present. And then you're gonna add the one that has the smallest weight, basically, to kill that and for the MST. There's nothing else, no nothing there already that shares a D minus one face, or nothing there that shares any vertex when? I mean, you have the skeleton, right? So you have all these triangles, but they are empty. Right, right. Yeah. Okay. All right, so Alright, so since we are going to have the open problem session, this is not really an open problem maybe that I could present, but I think it would be nice to do some of this work in the geometric case. Things are a little bit harder, we don't know what the constant is, we have sub-aditivity and other techniques, but what if we do the same thing and the weights are now given by volumes? Volumes. If you have the points, you have the triangle. What is the area of that triangle? As far as I know, this hasn't been looked at. And it should have some connection to the Vietnamese Ribs complex the same way that the Euclidean MST has some connections to the random geometric graph. Does Kruskal's work for that? There's one? Kruskal's algorithm worked for that? Yes. Yes, that works. Alright. I'm used to a term, but I wasn't familiar with vague convergence. Oh. Yeah, so that is usually the notion of convergence of point processes. So this is weak star convergence in the space of measures. So bounded and converged. Measures, so bounded and bounded support. Your test functions are functions of bounded support. I just want to see if I understand, right? When you're adding these in minimum order, for a while you're adding basically everything because there's no big shadow. Right. And then all of a sudden, there is a really big shadow, and so you're only adding a fraction of it. A fraction. A fraction of them. And then that fraction continuously. But the thing is that those are the ones that matter the most because they have the heaviest weights to some extent. So everything that you put before, it weighs so little that it sort of doesn't matter here. So everything that matters is at the scale of where the shadow emerges. Oh, okay. So let me see that picture, the picture with the drop in it. Uh that one. That one. That one, yeah. Yeah. So it looked like there that you added a bunch of stuff where you were adding almost everything. And then it dropped and you're adding some more stuff here. You're saying even, you know, all this, if I take the moment of this in the picture, we still haven't gotten anything yet. It's all out on a bigger scale. Or what? No, no. So when I say, okay, these are still in the right scale. These are still in the right scale. So you first add stuff that it doesn't even show here, that's tiny weights, like one over n to the v. But eventually you start adding stuff that matters at this scale. And yes, you have a piece where you still don't have the giant and you add everything. So you're still adding everything. Yeah, and those matter. Those do matter. Everything in this picture matters. And then there's something that doesn't contribute at all to the mass that's the same thing. That gets the extreme process. That gets the edge. And that doesn't contribute to the moment either, like to the average X value, or it does? No, it's hidden in the uniform integraD. So it doesn't. But it doesn't, yes. Looking for a critical window here makes sense, or because of the topic doesn't matter. It doesn't. Yeah, so I think it would be super interesting to see in the linear Meschulen complex what happens. I think their techniques are far from being able to say that. In fact, they really need to move an epsilon apart to sh so I drew this, but you have to be an epsilon apart. But they cannot get in the window. So so I would suggest actually