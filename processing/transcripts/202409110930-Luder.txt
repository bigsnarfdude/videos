Today, right, my talk will be easy, quite easy in, right? Why? Because I do talk about databases, right? Database construction that is mainly used then for machine learning, especially for the development of kinetic energy density functionals. Density functionals. So, why do we need this? Because I think the community would benefit eventually. What do we have? We have crystal structures computed, lithium, aluminum, magnesium, and a few other elements for which quite reliable superpotentials are available. We have Potentials are available. We have a few ground state properties as well as of equilibrium properties computed, right? Of equilibrium here related to atomic displacement and lattice displacement. And the data set compared to other data sets is particularly tailored for the needs of Orbiter 3DFT developers. Well, at least that's what I hope. Well, at least that's what I hope. Why is that? Because we include electronic density, the kinetic energy, total charm potential, and a few other ingredients that might be pretty useful, right? If you want to machine learn the kinetic energy density functionality. The application of this data will be at least in parts later covered by Sergei tomorrow in his talk from 10 to 10.30, if that is still the special. 10:30 if that is still a schedule to have a quite widely covered chemical space. And I will talk you through the program flow I implemented to handle UpperNet, right? Which in principle should be quite easy. UpperNet is a so we are back here. I close the shed. Also, we have the program flow and UpperNet. So in principle, UpperNet is a fantastic tool. Is a fantastic tool, right? It's reliable in those cases, or other. I personally encountered a few times difficulties using lookup superpotentials, in particular and volume relaxation or the SCF cycle. So that needs to be handled. So I wrote a proof on flow around that, but that takes care of that. Let's see. Switch the slide. Well, the slide is not switching. Normally just clicking, right? Normally, just clicking, right? Oh, here we go. So, as Sarah mentioned, the machine learning development of kinetic entity functionals has picked up in recent years quite a bit. Yesterday I did a very, very quick Google search, right? There will be a little bit dependence on my browser history, but first thing I see is here a paper from Snyler and Boko from 12th-12th, and then actually one with Sarah on it, then ours, and a few others. And a few others. And I think the last two I mentioned here, actually, maybe you will see a lot of as well. Let's switch for that. So, looking into machine learning for kinetic energy density functionals, what it is normally using is supervised machine learning, right? So, we need data for either GPR, Gaussian process aggression, neural networks, deep learning, or whatever there is. Whatever there is. Then, when we need data, what are the requirements on the data? Well, the data must be representative on the problem we want to solve, on the relation between the data. The quality must be ensured, so we shouldn't have any systematic errors or cooling errors or whatever inside them. And we must have a sufficient amount of them. So, then is the question: right, how much data do we need? How to sample vocabulary. Need how to sample the chemical space sufficiently, right? How much data does it give this? How do we handle the amount? Maybe we can reduce the amount a little bit. Maybe we need to increase it. I don't know. How to ensure that the quality is good? That's a difficult question, really. And how do we have easy access? And the last two questions are actually something I will address later as well. One not answer, I will just address it. So, again, right, so machine learning for kinetic energy density functionals. If you look into the formulation of the kinetic energy density, commonly what we see is the Conchambre functions here, right, or the gradient of that. We have here the Thomas-Fermi contribution. We have the squared gradient that takes the gradient of the density as well as the density. We have the squared. We have the sphere Laplacian, taking the Laplacian and the density. So tau, rho, the gradient of rho, right of the density, and the Laplacian of the density seem to be pretty important if you want to do supervised machine learning for KEDF development. There was also quite a nice talk from Paul Oryer earlier, right, commenting. Earlier, right, commenting on the sixth-order term, that might be difficult to handle. So I'm still quite happy that we haven't included higher-order representations here. That still might be sufficiently okay. So if you think about now you want to construct a database, right? Where do you start from? So the first thing you normally do is you start in what is out there, right? And if you look into material. There, right, and if you look into material databases for crystal structures in particular, so I have not here included the molecular space, right, only the crystal space. Then we have, for example, a materials project that is one of my favorite tools because it's quite easy to use, to be honest. And in recent years, I encountered a few strange things, but that might be just, I mean, that might be some other issues. I'm not sure yet. I'm not sure yet. There's Aflow. There is CUD as well. What is generally available? Of course, lattice constant. We have the lattice phases, phase ordering. We have a few pipe properties like pipe modulus or the engineering tensor sometimes available. Density, band structure, DOS. What is harder to get is the density. What harder to get is. The density. Matar to get is the quadrant of the density, then a plausible, right? These are all the things you may want to consider when you want to machine learn the relation leading to the kinetic energy density functional. So we need some kind of workflow, right? High throughput tools to Tools to use what is given that would be here, the lattice constants, and then use that tool to get what we want, would be the density and so on. Still, there was a question, so which database to start with? My favorite here, Materials Project, Aflow, I tested as well. The last test I actually did this morning, right? Because I wasn't sure do I use it correctly or not. So, my test case is an alloy, lithium, sodium, magnesium with arbitrary stoichiometry, right? X, Y, Z. I plug this in. I get a few structures in my Turitz project. I plug this in in A flow, right? You see here, it's very, very small, but that's there. Lithium, sodium, magnesium. I don't get any results. No results far. Don't get any results. No results found this morning. I did it again. I clicked all A4, right? So maybe I do something wrong. There's still nothing here. I get an error message. So I was like, okay, great. Stick with Materials Project. And then you can use the API in Materials Project as well as the other given information, DFT-based, that is the affirmation energy, convex hull energy. Convex cell energy and whatever you may need in your workflow to control the reliability of results you obtain with low pseudo potentials. So that is missing. Mentioned already a few gradients density, a gradient of the density, Laplacian, kinetic energy density, and so on. But this is all accessible in or through AppleNet. So we can actually now So we can actually now move on and ask ourselves: so, how do we define the chemical search space? Well, that is, in our case, restricted somewhat to the existence of easy to use local absolute potentials. I want to be careful here saying easy to use because some potentials I don't really get to work yet. For some very Work yet are for some very, very practical reasons. There are some neural network holograms potentials, for example, that are given in a format UPF that is generally used for quantum exposure. I have not managed to convert them to the PSP8 format and other format in UpperNet. So I couldn't use them. There are some high-quality local pseudo-potentials. They mainly cover transition metal. Mainly cover transition metals. However, you want it to stay in the S and P block. So I am stuck with the BLPS from the cutter probe. I used that back in Singapore when I still was a postdoc with Sergei and we developed the TIN pseudo-potential. I also included another pseudo-potential he developed with his former PhD. His former PhD student flow of sodium, right? But the rest are the BLPS, right? Which gives us 11 elements, limiting our chemical search space a little bit. So then we naively just try to see what can we get now for materials project with this restriction of 11 elements in different combinations. In different combinations, right? We can start with one element at a time, two, and then take different permutations. The simple for loop around the API core gives already 1,700 plus materials for five elements. However, I have a note here, of course, you try six then to see what you get. However, the API always dies on me when I try six, so I stuck with five. So I stuck with five. If you are at five, maybe I can try three and four as well. It's not that of a loss. You go down to 1700 plus materials. However, it's a bit lopsided towards magnesium and silicon phases, right? So one-third of that 1700 plus is already some magnesium-silicon compounds, which is a little bit well, it might tinder you in. Well, it might hinder you in given some bias in your sampling of database. Anyway, so we thought about how do we balance this a little bit? How do we limit the chemical search space to make our exploration of the machine learning kinetic energy density functions a little bit more efficient? Well, we came up with the restrictive stoichiometry where A, B, and Capitals are elements. Capitals are elements, A, B, and C small letters are the numbers of the stoichiometry. And we say for like three elements, or up to three elements, we have one smaller than the sum of the ABC should be smaller than 4 plus 2m, where m is the number of elements. What does it do? It filters out, for example, a lot of phases that are similar, right? And it gives Right, and it gives a little bit more flexibility when we go to a large station matrix. Over that, we go down to 760 materials. Um, however, of these 760, there are some that are a little bit more exotic, a few lowered structures as well. For materials you wouldn't expect to be layers. We apply convex silver node, allowing it to less than 700. And then, remember, we want to do a DFT calculations and upper net. At DFT calculations and up in it, right? And up in it is good, right? But it's not the fastest good, at least not my compilation. So I limit it to elastic sizes in any direction to be less than 10 angstrom, right? On our conventional standard unit cells. So down to 435 materials. I mentioned already we encountered a few layered materials. We also want to get rid of them. Of one as it hinders a little bit the machine learning of the kinetic energy density functional in some cases, at least. So, apply a density filter to detect the layered materials, and we're down to 433 materials. So, it's a database. The properties need to be discussed at least a little bit. Sampling is one of them. Is one of them. And we have here linear, binary, and ternary compounds included. I resolve this to the element that is in that stoichiometry. And from uninary systems, it's pretty easy. We have a total of 58 unique compounds. Binary ones, be very careful how you read this table. So the total is the total of binary systems. If you sum up these numbers. Systems. If you sum up these numbers very quickly, you get something like 600 plus. Obviously, 600 is not 300, but this is because you have like double counting of materials in, for example, lithium and sodium alloys. There will be counted twice, one for lithium, one for sodium. Okay, what we in the end want is some kind of more even distribution, right? Because this data set was used for machine learning in the end, right, with random sampling of some phases. So, either way, it's So, either way, it's more homogeneous. There is a slight bias towards magnesium, indium, and lithium phases that make around 50% of the data for binary and tanary compounds, right? It's not too bad, right? But it's also not like perfect. Now, a quick check when we did the calculations, however, on the 435 data set, the mean. Said the mean and median volumes of the full relaxation are not too bad, so it's still okay. But so, how do I get the volume relaxation? That's the next question. Considering the overall workflow, right, we have step one, get the structures, step two, get the ground state properties, lattice constant, and then I mentioned you some off-equilibrium geometry. Of equilibrium geometries, right, and properties, which would be step three, and last, it would be step four: collect the data. So the volumes obtained through, of course, volume relaxations. I use the block flow resolve for step one and two. The colors, right, from dark to bright, step one and two, and so on. So we have basically no problem for step one where we define the sort. One where we define the search space, query materials, projects, API, and pre-filter some phases. All right, no problem. The next step one is set up some volume relaxation. We restrict ourselves to isotropic relaxations. For this, we take a very, very plain uppermint input, right? So nothing too fancy, and just change the essential properties there. Essential properties there, right? We set some values that is basically resulting in equal sampling of the PoloNzone. We set up the local pseudo-potential library link and pass this then to our SLOM scheduler and execute the DFT program. No problem, right? Then we think at the end you should check your calculation. That is what you would do as union as well, right? You check for normal termination, you check for conversion. Termination, you check for conversions, right? And then, well, if A or B fail, you check a few other things. I will talk about this in a bit, just to give you at least one example about what are issues that could be fixed. And then you need to fix, you need to repeat as you would do as a student or whatever. And if the success error, you extract the prime state information and you go to step three, set up the signal point. So a common issue I actually have in AppleNet with local civil potential. Net with local specific potential is related to the lattice dilatation during the relaxation, right? And there's a special parameter that allows you to handle the changes in the G-space a little bit. And it always tells you when it fails after a few ionic steps, right? What you do here is insufficient. You should increase, right? An adequate value would be 1.35. That's what the program tells you. And the documentation also tells you. You and the documentation, however, also tells you the value should be not larger than 1.15, right? So, this is this is like a loop I encountered in UpperNet that is just there, right? And that is one of the barons, right? Now I have a better solution outside UpperNet to handle this a little bit better, but that took a little of time. It's one of the common issues I had, just in case you want to repeat these calculations. Okay, so when we have fixed the volume relaxation, we can go to the strain calculation. We can go to the strain calculation. These are the off-equilibrium data. We can define the strain lattice here or strained lattice as lattice times the strain matrix, where normally we have lattice vectors given here in what is it, rule valves. Good. So we set up in step three of the workflow the single point isotopic strain calculations. We again use the same. We again use the same module to handle the schedule, all right? Submit, execute, collect the results, and then extract the grid-based information. I mentioned earlier, that would be density, gradient of the density, Laplace, and so on. Again, we need to check the proper termination of the calculation, right? One, two. And there are a few other checks that need to be done. You need to check that the density is properly represented. Is properly represented, so we probably should get the well, only positive values here, right? In our repo, we should have the number of electrons should be reasonable, right? What we give, we shouldn't lose electrons, there shouldn't be more electrons. The kinetic energy value should be somewhat agreeing between when we integrate the kinetic energy density and our conchant kinetic energy density. Now, using APOMIT, I cultured this is not always true. It's definitely not. It's definitely not true when you, or quite often not true, when you use non-local pseudo-potentials. Here's an example, right? So I feel the total kinetic energy density, right? Given Hartfield or Bohr, I integrated 2.2. Then the Kuncham kinetic energy is 4.4. So something is off, and this is actually the that in this case it was linked to the number. Linked to the non-local part. There are, however, other issues that we encountered that is more related. Well, I believe still it's related to the MPI version that was used at that point. So we collect the data, right, in a reasonable format, hopefully, that is shareable, not accessible. Contained is the MPI ID, right? Just we can really talk about the same. We can really talk about the same material, the energy cutoff, right? A few details of the calculations, like the K-point sampling density, the type of pseudo potentials, very important, right? Different pseudopotentials will give you slightly different results. We need to have the cell volume, the FFT query, right, to reorder the volumetric data, the kinetic energy values of DFT, and then here's something cut off: the total energy this is supposed to be. The strain value we can give, I totally be used off equilibrium conditions, and of course, our voluminatrix data. From there, we can process a few features, right? But I think Sergei will tell you about this a little bit more. So, so far, so good. Last thing I want to note about the database is: I store, I break. But I store, I bully file a little bit how the qt files are generated, which are used to store volumetric data. Normally, it should be a 3D data structure where qt files are like rows of five, right? And then you have a break. It's easier, however, to handle them as a, well, for me, as a 1D column, right? But you can easily restructure this in a grid, right? 3D crit. Here's a code snipplette. A cool snippet. A few pictures for once, density, kinetic energy, and so on for quite a simple material. And then you're basically ready to go. Now, right, what we have done, the details are here. 500 eB cutoff. That's quite small for kinetic for local pseudo-potentials. Okay, our K-pan sampling corresponds to 20 points in general for per. 20 points in general for per reciprocal lattice unit, 40 for some metal phases to capture the frontier states a little bit better. And we have isotropic lattice relaxation. Now let's see the results. I still have like five minutes, no problem. And it's really true. Of course, the first thing you would consider is discussing about the lattice constant, right? So how good is the lattice constant that we compute? I give That we compute, I give quite a simple plot. So we have here a materials project lattice constant, we have here a DFT local pseudo potential lattice constant. For some 10-value alliance, we compare the Z-value materials project to our A3 lattice vector length. So if it is the same, it should be on this diagonal, right? And if it is not, it's off the diagonal. And here the red bars are some relative error. And you see the relative errors should be. Background, you see the relative error should be less than 4% in most cases, many cases less than 2%, which is pretty, which I think is all right. Now, if we go to the entire data set, I limited to the 433 case, resolving the inary, binary, ternary systems, and looking at A1, A2, and A3. Yeah, I think it's alright, right? Our square values are high. Are high, the relative errors are normally around 2%, right? They're getting a bit larger for in some cases here in the ternary Z component. So usually lattice constants are not the problem in the data quality at least. There's a caveat to this, but could be a discussion. Next thing, a little bit more. Next thing, a little bit more challenging for local absolute potentials, maybe, or for materials project, I'm not sure yet. Maybe a third data set would be nice. Is the reformation energy relative for each chemical composition? Again, optimally, the values are on this diagonal. We see a large spread. This case is for magnesium. It contains linear, binary, and tannary phases, a resolve field. Phases, I resolve here the mixture for the binary phases, right? We see it's quite evenly distributed. There might be some differences depending on the element, but it's not too good. What else I need to say? A random materials project at the bottom, DFT on the side. The green data points are the entire data set, and the blue ones are just the ones with magnesium inside. Museum inside. Okay, too good. So, if you look into the 11 elements, there are some trends. We have quite well working aluminium, AS and sodium, right? L-Iron and Tin. Solithium and tin are rather poor performing. The tin I made myself, right? So it's me to blame, maybe. Or maybe it's the combination, the incompatible The incompatibility with magnesium, because you see there's a lot of magnesium in my Tim phases. For lithium, right? So the Carter group offers two different lithium. I chose one of them. I haven't done the calculations with the other one, so maybe that gets better. Right. Anyhow, so there is, in those cases, it's some non-linear trend. There are some exceptions, right, where the data are just far off what. Are just far off what materials project gives, so to be confirmed, maybe with some quantum expressor all electron calculations to get some more confirmation. Right, I think I'm somewhat in time. Why does it matter and what was it done with? So, of course, it matters with, I hope, bringing a little bit, maybe the orbit of 3D FT. But maybe we orbit a 3DFT machine learning community together because we all share one interest, and nobody needs to recompute all the data for all the phases all time. It's much better, much more efficient. So, but what is it now? We have, how much data we have? We have 18 strains for each phase of 133, which is like 7,800-ish structures. Last time I checked, it was like 250 gigabytes. Check it was like 250 gigabytes, but depending on which data I shall write, sometimes it's a 500 gigabyte, depending on what you include, right? After the pre-processing, post-processing. Was the chemical space sampled enough? Well, we think so, right? We hope so, at least for what we did, depending on the availability, the choice of original database and materials project and what seemed reasonable to us, right? Seemed reasonable to us, right? With the phases we sorted out. So, how to handle the amount of data? I had like one mode available for this, right? So, I had Python available, so that was Python and patients. Meanwhile, I have the schedule right hand, so in principle, it could be parallelized even nicer. It would be the long-term error, right? One push and then not long a bit. How to ensure quality, all right? Check, check, check. But this is also the point. But this is also the point where the community is, I think, very useful because we have different codes, right? We have different computers, we have different compilations, right? If we just collect, compare, evaluate, at the end we make sure that everything is in place, we could help each other. We can learn from each other and in the end have some very robust community data sets. I definitely want to share the data I have because they are. I have because they are not very useful if I don't use them. Anybody else has access to them? So maybe I look into materials cloud, COG, or GitHub seems a little bit small for them, right? But I want to follow the third principle, which is findable, accessible, inter-operatable, right, and reusable. So, and why does it matter? I just quote here in text from the abstract we have, right? So, we had some recent work, and there seemed to be some. And there seemed to be some promise about the kinetic energy density functional development, especially in terms of improved sampling through smoothing and averaging. But I think Sarwe will mention this in his talk more tomorrow. Is that all? Oh no, there is one more here. Summary and conclusion. So I hope I presented some high-quality data set of metals, alloys, and semiconductors containing uni-robinary temperature structures. Containing unierabenial terrible structures. I hope it is relevant for at least some of you. I'm grateful for Professor Manzo's including you this, right? It's fun. I learned something, I can contribute something. What should we do? If you vary what this database could be for you, feel free to contact me. I can even share before. I, however, think because other Think right because other people do machine learning, we could over time now we could set up a common space, right? Agree on where to upload, right? So people all use each other's database that we used in machine learning for orbital 3DFT, right? Playing together the results of several codes with several and overlapping setups, right? Energy cutoffs, zoo potentials, k-points, whatsoever. And of course, find the common format and sharing. Of course, find the common format in sharing. So, I have, for example, now SPADE the one-dimensional 1D cube file format, but you can always choose something else. And with this, I already can conclude my talk. I think that is now half an hour-ish, right? Okay, thank you for your attention.