wait all right uh jose thanks for the introduction and uh thanks uh um everyone in the uh organization committee to organize this uh great conference i um start looking into the deep learning for a while and then really start working on that more concretely more recently i find this is very exciting topic and there are many potentials particular for the genomic data um so today i'm going to talk about this So, today I'm going to talk about this biologically informed neural network for single-cell RNA-seq data. And here is an outline. So, I start with some introduction about gene annotation, graph neural network, and then talk about the microwart. It's mainly a graph neural network approach, try to exploit the gene annotation information when we analyze the single-cell RNA-seq data. And then the result part we have. And the result part, this is really very much ongoing work. So we're still looking into the detail. We have analyzed a few data sets, like a couple COVID-19 data set. So I'll talk about some of those results. And feel free to interrupt me if you have any questions, or we can also discuss at the end of the talk. So I think many of you already know. Uh, many of you already know this very well. I will try to go briefly here the motivation. The deep learning we know is very effective approach to learn the low-dimensional representation from single-cell RNA-seq data. So when we look at single-cell RNA-seq data, each cell is kind of a point in a high-dimensional space, right? The dimension is the number of genes we measure. So it could be 10,000 to 5,000. And we want to find a low-dimensional representation for each cell. So you can do a PCA. So, you can do a PCA, and it's quite often it can work okay. And then the deep learning method, at least many of those work in this area try to find the low-dimensional space, and which is often reflect underlying cell types in the single-cell data. So, like earlier work, DCA and ICVI, and those are very powerful. Powerful and a very pioneering way to look at the neural network in single-cell RNA-seq data. And then the deep learning model can also be trained with the transfer learning. So the part of transfer learning also make the deep learning more popular and more useful. For example, Silver X by Jin Shu and Nancy, who are also participating in this conference, they have done this transfer learning. Transfer learning using the deep learning. So, my motivation here can be considered one way is to, when I start looking at this question, I'm thinking compare an image versus a cell. And then the reason is we know deep learning, particularly convolutional neural network, is really successful in imaging data. So, we look at the image, what they are. They are a collection of pixels, and each pixel Of pixels, and each pixel itself is not really interpretable. You have to put them together. You need to consider each pixel and its neighborhood to interpret the image. And the reason that convolutional neural network is very successful on imaging data is because some particular property of imaging data, including this swift invariance. So basically, if I want to say whether the image has a cat or not, I can flip the image. Or not, you know, I can flip the image, I can put a cat anywhere in the image so it doesn't change the conclusion, doesn't change the label. And then the local connectivity, basically the pixel connected with each other. So you need to look at the local connectivity. The compositionality means the image they have, like the structure, you know, you have very basic structure, a line, a curve, a corner, and then put them together become a higher level. Higher level, more meaningful image. So that's why the convolutional neural network is successful for imaging data. And then, if we look at a cell in terms of single-cell RNA-seq data, then we think about a cell as a collection of genes. So somehow we can make a connection between like compare a pixel versus a gene. And then each gene is interpretable. Okay, it's not like a pixel. I mean, the pixel is kind of exchangeable because it doesn't. exchangeable because um it doesn't matter you know where the where the the the cat is in the image but the gene is not they each gene has their own meaning it's not uh exchangeable so there's no swift invariance but there's some level of local connectivity and the compositionality like the local connectivity you can think about a jinjing connection so they have some functional annotation to give us that and the compositionality is like the um uh um The gene can become a function group, right? A pathway of gene set. And then, so a meaningful question asked when you look at this is how do you define the local? So that's where I talk about the gene annotation. So, here I'll mainly just use the protein-protein interaction. I think we have a nice talk yesterday by Joseph talk about deep learning to infer the protein-protein interaction. So, here I'm now using there, just using the known annotation, like I got the annotation from bio-grade to look at the protein-protein interaction, then to define gene gene interaction. And then, here the plot is just an example of protein-protein interaction for schizophrenia. So, it's pretty complicated and then. Complicated, and then we know it's informative, and then we also know it's very noisy, particularly when you think about a particular system at a particular TC or particular stage. Many annotation may not be accurate. So, we want to keep that in mind when we try to use such information. Okay, now we talk about the graph neural network. So, we start with this analogous with the image. In fact, the convolutional layer can be considered as a special case of graph neural network. As a special case of graph neural network, basically, when you have a convolutional layer, you apply the same operation to all positions. On the left, you can consider this is like the image. And then each node or each point here is a pixel. So when we look at one pixel, we look at its neighborhood. So basically, the convolution filter, for example, a 3x3 filter, basically look at the neighborhood of each pixel. So it does a weighted summation on all pixels in a real. The summation of all pixels in a region, and the weights are shared across regions, so that's the convolutional layer of image. And then, when you look at the graph, this is like a graph. So, there are several challenges. First, the neighborhood are different from node to node, right? So one node may have four neighbors and the other node may have one neighbors. And the size of neighborhood are different. And then those neighborhood, they don't match, right? For image, you can do the same operation. You can do the same operation across all sliding windows, all pixels. And then, because the neighborhood has the like kind of consistent interpretation. And then for the graph, it's not clear, you know, how do I match the neighborhood of one node to the neighborhood of another node? And particularly when we talk about the Jinjin network, I mean, the nodes are also different. So, what we do, I mean, the What we do, I mean, the graph neural network, a particular popular part is graph convolutional neural network. So we kind of apply the convolutional operation for the graph. So what it does is two steps. First, aggregational neighborhood. So you want to use the graph information. So you kind of want to smooth the data. So now if you want to put this in a concrete example, you can think about we talk about graph gene gene network and then each gene. And then each gene has a data, like attributes. So that's the data gene expression, say, thousands of cells. So you want to do the aggregation of neighborhood. So basically for each gene, I may kind of smooth the gene expression data of this gene's expression and all of its neighbors expression. And then you want to learn the representation of each node based on its attributes. So we have somewhat similar. So we have somewhat similar to the imaging or deep learning typical problem, like for each gene, now we have thousands of cells. So it's each gene is a point in high-dimensional space. And then we want to reduce that data. We want to reduce the data from all those cells into some low-dimensional representation. So the aggregation part is predetermined. So as we mentioned, you know, the neighborhood. As we mentioned, you know, the neighborhood are different, the size and the meaning are different. So, usually, there's not much to do for the aggregation part. Like, you can just take a weighted average or just take average of the neighbors. You can put a higher weight on the node itself and then put a lower weight on the neighbors. And then, the convolutional part is to learn the representation. So, here is a very simple formula. So, you have H. A very simple formula. So you have ht as the hidden nodes in the t's layer, ht minus one is the hidden nodes or the data in the t minus layer, and the w is the weight. So that is shared across nodes. So think about you have, say, 5,000 cells, and then you want to do a reduction of those cells. Suppose your reduction is five cell types of those 5,000 cells. So you can do that reduction. You know, you can take a weighted average of. You know, you can take a weighted average of all the cells within one cell type. So, that's the W. That defines how do you transfer the high-dimensional data to low-dimensional, and then you do the same thing for all the nodes in the graph. And then you can introduce the non-linearity by adding the f function. So that's where the neural network is more powerful to introduce the nonlinear transformation. Okay, so those are the background. And then now talk our method. So, what we want to do. Method. So, what we want to do is use graph neural network to learn the low-dimensional representation of single-cell RNA-seq data. As I already mentioned, the graph here is the gene network, and then we use the protein-protein action from Barograde. And we think that's informative, but they are far from accurate. And then here, each node is a gene attributes, so each node is the expression across cells. Expression across cells. And then the goal is to find a latent component where each component is a gene set. So you can think about this as a way like I want to cluster genes. This is type of work actually pretty popular in gene expression data, even start the day. So when we analyze micro data, you have gene expression data. Have gene expression data, you have gene annotation. Then, can you use the gene annotation to better cluster the genes? Right? So, that's kind of like an older problem. And now we try to bring the neural network here to see how it could help. So, we want to find latent component where each component is a gene set. And by using the single-cell RNA-seq data as well as the annotation. And then for each gene, instead of using the observed gene expression data, that's what The gene expression data that's what the old method do, right? When you do clustering, you just use the gene expression data here. We first do a low-dimensional representation by a graph neural network. And then this is kind of an overview of the framework. So here, start with the upper left corner, we have this gene cell matrix. So you have the gene expression data. That's typical. You have a matrix, each row. Typical, you have a matrix, each row is a gene, each column is a cell. And then here, our framework can be used for either supervised or non-supervised learning. I'm actually mainly focused on supervised here. I feel the supervised learning somewhat pushed the neural network to learn more information. And we have also tried non-supervised learning. It also worked, but it seems the advantage is not that. The advantage is not that apparent. So, here, suppose we consider a problem, like we want to compare COVID patient with a severe or mild disease, and then we want to find which gene sites are different. So, think about the problem, you know, you have single-cell data, and then you have tens of thousands of cells with a severe, tens of thousands of cells with mild. It's actually pretty easy to find some differentially expressed streams because the number of cells. Differentially expressed streams because the number of cells is huge. There's a question, you know, the number of cells or number of individuals. But if you do the comparison on cell level, it's easy for you to find the differentially expressed genes. But the question here is we want to see whether, because in that situation, because sample size is huge, the small p-value doesn't mean much. Actually, you want to find a more meaningful interpretation. So here, we assume you first can select the gene. We assume you first can select the genes interest. Here, we select the differential express genes because we want to do supervised analysis. And then we estimate the gene assignment matrix by graph neural network. So the graph I saw here is kind of like each node here is a gene. And then in the bottom here, you guys can see my mouse, right? In the bottom, we have the latent components. So each component is kind of one latent component. And then here, Component. And then here, the genes are connected by graph, by edges. And then in the graph, based on the protein-protein action, you may have kind of something like a module. So for example, these three genes mainly contribute to this latent component, and then this three mainly contribute to this latent component. I'm going to talk more details how do we do this part, but basically we end up with something like this: right? This each row is a gene, each column is a component. So we have the assignment. Components. So we have the assignment genes assigned to different components. And then combining this one, and this is actually what we got from the graph neural network. But if you want to do supervised analysis, the testing is actually again on cell level. So we actually need to come back to get the cell level data. So basically, we use the raw data and then the dimension reduction. So we reduce the data to like each row. Data to like each row is a latent component, each column is a cell. And then we can compare those cells using a like feed forward neural network. Okay, so that's the framework. And then now let's go in more details of the graph neural network. So we first reduce the dimensional data on cell level. For example, we start a thousand genes and 5,000 cells, and then we reduce that to, say, 1000 genes to 32 dimension. So from the cell to latent dimension. And then we use the In the dimension, and then we use the graph neural convolutional network with the scape connection. So there is a RayLu activation function. And then the first part, the HL is the output of the else hidden node. The A delta is a normalized connection matrix. Basically, if you use A times H, it's like smooth the data based on the graph. And then after smooth that, you multiply the weight matrix. So what we do here, we also add a scape. So, what we do here, we also add a scape. So, basically, you have the edge directly multiply the weight matrix without smoothing. So, that's saying I want to give my own attribute higher weights. So I can automatically learn those. And then the next step is the pooling called the graph pooling. Basically, now we want to guide the latent components so we can have a basically learner assignment matrix, say. Assignment matrix, say, a thousand times 20 to assign genes to the 20 components. So, this part is the particular popular research area. There are many actually graph pooling algorithms. What we use is a particular one, a spectrum clustering, which is the basic idea is if you have the graph and then you want to find a cluster, so it's kind of like spectrum. So it's kind of like a spectrum clustering. You have those connections and then you want to find the clusters. The spectrum clustering itself is harder to compute. Then we can put the objective function, slightly modified objective function of spectrum clustering as the loss function for the neural network. And with additional loss that try to balance the size of cluster. So yeah, so the loss function for supervised learning, we first consider a classification. We first consider a classification accuracy loss, and then we also consider how well the kind of clustering accuracy, how well the reduction of genes to latent component capture the graph information. And then so the implementation is just we use some code in PyTorch geometric. Okay, and now let me talk about the example. It probably helps you to also pin. Pin down some ideas. We analyzed a couple COVID-19 data. One of the data sites with larger sample size is this publication from Sue et al. So they collect this number is number of samples. I think the number of individuals is not that many, but like they have 200 samples from mild or moderate and 52 from severe. So what we want to do is So, what we want to do is, I'm not sure whether people want to talk about COVID now. We are living at past. I think for most of us, really infection doesn't matter at all. But for some high-risk population, the question is whether there is a risk to develop severe disease versus mild. So, here we want to compare the gene expression of CD8 T cells between severe and mild, particularly the cell. The cell response is probably something to protect us, also. Although we got a spreakthrough infection, so the antibody doesn't protect us, but the immune cell, the T memory, T cell memory could protect us. So here we split the data half, half for training, half for testing. And then among the training, we take 10% of cells as validation just for the training process. And in the training data, we select top of thousand genes that are differentially expressed between sphere and Express between severe and mild COVID-19 patients. And then we run our analysis, and then this is the gene assignment matrix. It's actually pretty clean. This is after a few run adjusting the loss and then to see where we go. But so the x-axis is a thousand genes. The y-axis, the column, are 20 latent components. So basically, you can see there are like 400, 300 genes. Hundred 300 genes that we don't really have clear assignment. And for the other genes, so the darker color, I mean, this basically we have soft max activation function at the end of neural network. So we got the thing between zero and one. You can see basically each gene is unambiguously assigned to one cluster. So here the dark blue is the scar equal to one and then the white is One and then the white is scar to zero. So we assign those genes to different groups pretty clearly. And then this here is another way to look at this when we look at the testing data. So here each point is a cell, and this is typical the TCNE projection with the Leyden clustering. And this is the cell label, whether the cell from mild or severe individual. Individual. They're not that much separated. So, I mean, this is a type of problem. If you just do unsupervised analysis, you may not see that. I mean, there is a signal between mild and severe, but you actually need a supervised learning. And this one is the score in cell LIVAL for each cell. So in the testing data that we haven't looked at all, and then we apply our neural network. And for cell LIVO, we look at the cell from severe patients. That's the blue one. What's their scar? One, what's their scar? And the cell from mild patients, so they're labeled zero, and what's their scar? So it's interesting, you know, for patients who are severe, you got higher scar in severe, but you also get a lower scar. So that's expected because the patient is severe, but the cells, it's a mixed population, right? I mean, it's not like all the cells are different. So some cells in the severe patient are different, and some of them is more similar to the male patients. But after we got the cell level prediction, After we got the cell level prediction, we just take an average and then we assign a scar for each individual. So, this is the scar for individual level. You actually can see severe and mild is very well separated in the patient individual level. Okay, I think I'll wrap up very soon. So, just some discussion. So, since I'm talking about the biological informed neural network, this is actually a topic that has been studied by many groups, like DCL, PNITE, XAMAP, and their approach. XA map and their approach is well, I got this annotation, so I use the annotation to decide what's the structure of my neural network, and then we are using a totally different approach. And then basically we say, we got annotation, we want to use them, but we allow you to choose how much you use the information. Okay, I'll skip that. And then this is really just work with me and one student who is just a master student. And Zhu Xiao, who is going to start his PhD in Berkeley actually very soon, but has done a tremendous amount of work on this. All right, I'll stop here. Thank you. Thank you. Thank you so much. It's a really nice talk. Interesting. There are a couple of questions. Michael Hoffman, you want to ask something? I thought. So Michael is asking how many samples. I don't know on the real data or what? Yeah, on the CDA T cell. T cell data. Yeah. Yeah. Because I remember the sample in terms of the sample they collect, but they collect two samples in each time point for one individual. So it's about half. I would say it's about 100 to 150 individuals. I forgot the exact number. And then we split them in half. Let's say 120. We split them in half. 60 individuals. All the cells. 60 individuals, all the cells in the 60 individuals as training, and then the other six individuals are testing. Any other questions? I think people, you can just, you don't need to use a chat. You can just ask. Yeah, I see Amin has a question on how sensitive the performance in the choice of the network? Yes, the reason I'm asking this question, by the way, Gary Talk is that some. Talk is that sometimes, as you mentioned, these networks are quite noisy and also missing values. I was wondering, especially like the gene-gene networks like BioGrid, did you try other networks, let's say String or DIP, to see whether this is uniquely useful? There is a signal in BioGrid, or where the performance basically comes from. Is it really the signal in the network, or it's maybe a regularization? Fork, or it's maybe a regularization effect that it has in improving the performance? Yeah, yeah, it's a great question. So, we try actually different annotations. First, like we try this protein-protein reaction at the early stage, we also try just gene ontology. Like, given the gene ontology, you actually can calculate the similarity of two genes, and then you can put a threshold and go the graph. So, it doesn't help that much. Doesn't help that much, and then it makes the procedure more complicated. So, we just use a protein-protein interaction. And then, does it help? I think it does. We try to train this without the network. It has been well, we have trained different versions. The performance is either slightly worse or very similar, but I think the advantage here is also make the gene size. advantage here is also make the gene site more interpretable because later when you look at the gene site, I don't have time to go through that. So you kind of follow similar genes together. So that's part of all the interpretability. And then we also try to change our loss function. Like our loss function has supervised loss and then we can just unsupervise loss, like which emphasize on the annotation. Remove the unsupervised part, performance slightly worse. So the unsupervised So, the unsupervised laws also help, try to encourage you to find the things that align with the annotation.