Be able to present this result today because we actually pushed it to the archive on Tuesday. So it's very fresh, and I'm talking about a joint work with Noila MÃ¼ller, who's in Eindhoven right now. So if we look at this title, maybe four questions might arise. So I guess the first question could be, what is pool data? The second, what is spark? The second, what is sparse? Then what is near optimal? And maybe one might be interested in how it actually works in the end. And in the next roughly 20 minutes, I want to answer those four questions. So let's start with the first one. What is pool data? So suppose we have n items, and each of those items has a specific weight, which is either zero. Weight which is either zero or some integer weight which is non-zero but bounded. Then we call the configuration of those weights, so the assignment of the weights to the items, the so-called ground truth or the signal, and we assume that this is drawn uniformly at random. Furthermore, we assume that we can pool items together and measure the whole pool of items. And measure the whole pool of items, so measure the weight of the complete pool. This is sometimes called the additive model. And last but not least, we assume that all measurements need to be conducted in parallel. So we are only interested in non-adaptive strategies to this problem. Here's a small example with seven items and five measurements. And for instance, in the first measurement, you have one item of weight one, one item of weight two. Item of weight one, one item of weight two, one item of weight zero. So the measurement in the end renders the number three. Okay, in this setting, or the setting is clearly a special case of the so-called compressed sensing problem, where this weight or the signal vector could be real. And in this talk, I will restrict myself to a special case, namely, if we Special case, namely if we set d to one, and this is called quantitative group testing. And the quantitative group testing problem is frequently studied since the 60s and of high interest also today. I know that some people are in this room that published on the quantitative group testing as well. Okay, so this is full data. What is sparse? Pull data. What is sparse? I call a vector or a signal sparse if it consists of, say, k non-zero entries, and those k non-zero entries have to be sublinear in n. More precisely, we actually assume that k is not just sublinear, but n up to the theta for some parameter between 0 and 1. So it's really smaller than n. And sparsity is not only important. And sparsity is not only important in the quantitative group testing problem, but general in many interference problems. For instance, the theorem we see here is maybe one of the most famous results in interference problems, namely that compressed sensing is actually solvable efficiently, so by a convex optimization problem, if the signal is fast enough. Okay, so what is your optimal in the quantitative group testing? Well, we want to achieve to recover the ground truth with as at least measurements as possible. And a very simple counting bound shows us how many measurements we at least require. So we know that there can be a We know that there can be at most k plus 1 up to the n different measurement results. But we want to distinguish n over k different configurations, so sequence of zeros and ones. So it turns out that m has to be linear in k. And of course, this argument is very simple, and maybe there are sharper bounds known. And the best known upper bound, lower bound. Yeah, upper bound, lower bound is actually by Yakov, which add us, which increases it by a factor of two. So it's really linear in k. And actually, we know that there are exponential time algorithms like XORF's search that succeed with that many measurements. So the underlying measure. Yeah, principle measurement scheme or pooling scheme is very simple. Any measurement just chooses half of the items uniformly at random. And this was proven quite recently by independent proofs of Hayek and B. Busch and Gruer of Frankfurt around Oliver Gehard. Okay, so these are exponential time constructions, but what is about efficient? Efficient algorithms. And here, the state of the art is a bit different. So, how can we possibly approach this problem? So, one idea would be, well, we could apply some of those more general algorithms for contrasensing, which is basically basis pursuit or its refinements. And this takes me a K log n measurements. But that's not very. But that's not very confusing because it's, of course, a much more general problem. And then there are some works by a group of Kalimi around Kalimi that use sparse parity check codes. And again, this requires K log N measurements. And of course, one could, in principle, dismiss the information how many items of weight one are in a measurement and only ask the question. And only ask the question if there's at least one, then this would be called binary group testing. And there are a lot of algorithms known for them. Matthew will present us three of those tomorrow, I guess. But in the end, they require K-log n measurements as well. Then there are two approaches studied explicitly for the quantitative group testing problem. One is a thresholding approach and one is, yeah, Approach and one is somehow a relaxation of the quantitative group resting problem. But it turns out that both of them require K-log n measurements as well. So one might conjecture that there is a computationally hard regime. So there is a gap in between efficient constructions and exponential time constructions. But actually, it turns out that this is not the case. So what we managed to prove is So, what we managed to prove is that there is a randomized polynomial time construction that comes with a polynomial time inference algorithm that allows to reconstruct the ground truth by no more than m is O of K measurements. And it is written down in that way because this two one minute theta over theta k is just the information theoretic law about. So, you cannot be better. Lower bound. So you cannot be better than this, even in exponential time. And in front, there's a very moderate multiplicative constant. But the most important thing is, well, the log n factor is gone. And how do we do this? Well, basically, we equip a clever version of the thresholding approach from the last slide with a so-called spatially coupled pool. Spatially coupled pooling design. So, spatial coupling is a technique that actually was invented in coding theory. And the main idea is to partition the set of items into two groups, namely the seat and the bulk. So, the seat is red in this picture. And the important thing about the seed is that there are very, very few. Is that there are very, very few items. So the number of items in the seat is asymptotically vanishing. And most of them are items, they are therefore in the bulk. So what we are also doing is we add a certain type of geometry to this pooling scheme. So in the simple construction I told you previously, any of those measurements would just have taken a certain A certain number of items uniformly at random. And this is still the case, but this measurement cannot choose the items from all items, but only from those in this compartment, this compartment, and this compartment, and this compartment. So it's locally, it looks very much alike than the normal construction, but there is this, yes, somehow. There is this somehow ring structure around it that should allow inference better, and I will explain why. So the first idea is that we first decode the seed. So the seed and the seed we have in our setting roughly n over square root k items. And we know that out of them there are at most roughly Are at most roughly square root k of weight one. So now we apply an algorithm of our choice that is already existing, like basis pursuit or the thresholding algorithm or whatever. And we see that we require then roughly square root k times log n measurements in the seed. But this is, of course, sublinear in k. So in the total number of measurements, In the total number of measurements, this is negligible. Okay, so if we look back at this picture, we have already reconstructed the weights of all the individuals in this red compartment. So this individual, this item, sorry, this item is in this red compartments. If we now want to reconstruct the weights in the fourth compartment, so in this one here. In this one here, they are contained in measurements here, here, and maybe here. And if we look at a measurement in this compartment, then almost all of the items belonging to those measurements are already reconstructed correctly. So we know basically everything or almost every weights that contribute to those measurements. That contribute to those measurements, and only very few are unknown. We can now hope to infer the weight of the items in this compartment based on this information. And to this end, maybe the so-called, let's say, unexplained neighborhood sum of an item might be a good measure. And this is basically just sum up all the results of the measurements in the neighborhood of an item. Neighborhood of an item, but subtract from it everything which is already known. And this unexplained neighborhood sum itself is, of course, a random quantity, but it turns out that it's very easily analyzed, so it's binormially distributed. And importantly, it is increased by the degree of the item if and only if. item if and only if the weight of the item is one. So maybe we can distinguish items of weight zero and weight one by this quantity. It turns out that one needs to be a bit more clever actually, because there are two effects coming in here. So the first effect is that the information in very close compartments to the current To the current item is much more valuable because there is much less fluctuation, we know many, many more. So the information is somehow more valuable. On the other hand, we find that the unexplained neighborhood sum itself in this compartment here is much, much, much smaller than in further apartment compartments because. compartments, because there are many in further apart compartments, there are many more items that are currently not reconstructed. So the quantity is much higher. And if we would just sum up those quantities over the single compartments, the contributions of the first compartments would just vanish. So instead of just calculating this unexplained neighborhood sum, we calculate, yes, somehow centralized and normalized. Somehow, a centralized and normalized and weighted version of it, where we account for exactly those two effects. So closed compartments contribute more, and due to the centralization and normalization, the quantities are comparable. This weighted sum, which comes out now, is still increased if the item has weight one. The item has weight one instead of weight zero. Not by the degree, but it depends on the degree. And so, if enough measurements are conducted, we can actually really separate the distributions of these quantities between items of weight zero and items of weight one with high probability. Let me summarize it. So, spatial coupling was So spatial coupling was previously, at least to our knowledge, used to optimize constants, as was, for instance, done in the binary group testing problem by Amin. And we actually used it to decrease the order of the required measurements. So this is new. And what one also sees is that the simple thresholding one would One would maybe normally do is not enough. And actually, we tried it, it only improves the constant. But this normalization is basically the key to reduce the order. Then, of course, we have in our theorem this four times this information theory. And actually, we cannot get much better in the design we studied. In the design we studied. So the information theoretically, the optimal design is the one where each measurement chooses half of the items uniformly at random. But unfortunately, if the graph becomes so dense, we couldn't control error terms in concentration results that are required. So we needed to sparse the graph a bit. And in this much more sparse. And in this much more sparse graph design, actually one can prove that we at least would require A times this one man theta over theta K measurements. So at least for small thetas, this spatial coupling hits this information theoretic bound exactly. Okay, so this is everything I wanted to say, and now I'm of course happy to discuss or answer. To discuss or answer any open questions. Thank you.