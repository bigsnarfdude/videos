It works when this second one is a follow-up from the first one. So the setup is the following. Suppose that we observe a sequence of tokens, like discrete data. This could be words or ngrams or URLs or IP addresses. And for simplicity, here I'm just denoting different symbols with different shapes and colors. And so this is our sequence of observations. Our sequence of observations. And in the applications that we want to consider, we have that data set is very large. So it's so huge that we cannot store the whole data set in memory, but not even a frequency table. So the number of distinct symbols is too large to store in memory a table that tells you the blue dot appeared one billion times and the red star appeared five times. And so, in this kind of situations, there is a well-established. Situations, there is a well-established field in computer science that deals with this kind of problems through what is called data sketching. So, a sketch is just as you would do with a painting. So, a sketch is some approximate representation that tries to preserve all the essential information of this data. And, in particular, the sketch that we are dealing with are built from a random hash function. So, we have h. So we have H that is a hash function, and it's random. It obeys some properties that hash function obeys. It goes from the universe where symbols take value, capital S, to the set of integers 1 to capital J. And in our computer, we store a vector, CJ, that is the sketch, which is a vector of capital J integers that is initialized to zero. And once you see the data, you pass each observation to the hash function, and it will give you a number from 1 to j. So, for instance, h of x1 is equal to 2, and you go in position 2 of this vector, and you add 1 to the value that was there. You do the same for x2. So, for instance, h of x2 is equal to capital J. So, I go in position capital J and add 1. And then I see x3. And again, I have h of x3 is again equal to 2. H of x3 is again equal to 2. So here I'm adding 1 to this cell, and this is where troubles start to happen because in the original data set, I have seen three different symbols, but in my sketch, I have just the information that I've seen one symbol here once, but this cell or bucket, I've seen it two times. So I'm losing some information in doing so. So, anyways, this is the data that we have. Once we have processed all the data, we have this vector or sketch where its elements are denoted by buckets typically. And the inferential problems that we want to answer are what are called recovery problems. So, the first one is the following: the frequency recovery problems. So, suppose that you observe a new observation like word XN. Like word xn plus one is hello, and you wonder how many times did you already see it? So its frequency f of xn plus one. So of course, it would be a trivial question if we had the possibility to store the whole data set in memory, because we can just go back and count. But from the cartoon that I previously shown, if, for instance, xm plus one is the orange pentagon. Orange pentagon, then we are in trouble because we are confounding it with the blue dot. And the second question that we want to answer is kind of related and it's the cardinality recover. So we want to have an estimate on how many distinct symbols are there in the sample. So for the interest of time, I will talk about just the frequency recovery problem. And if you're interested, you can go to our papers and see about the cardinal literature. See about the cardinality recovery. So, the thing is that since we are conditioning only on the sketch, then we need to be very careful in doing these estimations. And well, as I said, this is a very established field in computer science. So, there have been many, many approaches to the frequency recovery problem. And probably the most famous one is this count mean-sketch algorithm by Kormode and Mutukrish. Algorithm by Kormode and Mutukrishnan, where they consider a slightly different sketch from what I presented you. So it's basically the same sketch as before, but different rows are obtained using independent hash functions. So instead of using only one hash function, you use capital M independent hash functions as you have this table of m rows and capital J columns. And once you have XM plus one, you want to output its Plus one, you want to output its cardinality. And this very simple algorithm just outputs an upper bound for the cardinality. So, what it does, it goes through the first hash function and it queries what is the value of h1 of xn plus one. So, for instance, here is capital J. And so, I go to the first row, I take this number and I put it here. So, it's 19 here. And I do it both. Is 19 here. And I do it for all the ash functions. So, for instance, here for the mth hash function, I go to hm of extent plus one, it gives me the number two, so I go to row m, column two, and it gives me some and 30. So I have a set of capital M numbers, and I just take the minimum. And this is guaranteed to be an upper bound on how many times did I actually see x10 plus one. So, of course, there are some downsides. So, of course, there are some downsides to this. Well, the upside is that it's very simple and it's almost immediate. So, it's efficient for a live streaming application. But the downside is that we are outputting a deterministic upper bound. And the interesting thing, at least to us statisticians, is that this kind of algorithms treat the data as a fixed sequence. So, there is no randomness coming from the data. No randomness coming from the data that you can leverage. And of course, a statistician, well, the first thing that you want to do is to consider a model for the data. And this is what Bayesian non-parametric count means sketches to. So why Bayesian non-parametric and not something else? Because it's very natural to talk about Bayesian non-parametric when you have a sequence of discrete data, such as a sequence of data from a Diricle process, species sampling, and so on and so forth. Species sampling and so on and so forth, they fit exactly the kind of data that we are interested in. And so, in Bayesian non-parametric counting sketches, on top of the sketching procedure, you just assume that data are a sample from an almost surely discrete random probability measure. And this approach was pioneered by Diana Kay and co-authors, where they assumed that P is a Dirichlet process and then extended by Dolera and co-authors. By Dolera and Cotors, by assuming that P is a Pitt-Major process. And so, in this framework, what you want to learn is the posterior distribution. And in particular, these authors focus on this kind of posterior distribution that conditions on the same sampling information that goes into the count-mean sketch estimator. So, the count-mean sketch estimator just takes the minimum of these numbers. The minimum of these numbers. Here I'm conditioning on this number and I look at the conditional probability that I've seen xn plus one exactly r times. And then since the ash functions are independent, they write this factorization of the post-zero distribution. And so the main object of interest is this conditional distribution that is the probability that I've seen xn plus one r times, given that. R times, given that with sketch m, so with just one sketch, I've seen the value into which xn plus one gets mapped to, c times. So the first question that we wonder is: what is this conditional law? So, is this a real posterior? And intuitively, it should not be a real posterior, meaning that. not be a real poster, meaning that now our understanding of what is the posterior distribution, this is the posterior distribution, the conditional law of some parameter given the whole data set. While here, even assuming that this factorization holds, here I'm taking my nth row of the sketch, it has capital J numbers, and I'm throwing all of this information away except for one number. And so, in our first paper, we show that the full posterior, so a condition on the whole row of the sketch, is equal to this single bucket posterior if we assume a Diriclet process prior. So, this is a very good news because this single bucket posterior is very easy to compute, and it also validates the approach by chi encoders because it's fully Bayesian, because it's based on the posterior distribution. Posterior distribution. But unfortunately, we can also characterize the Dirichlet process as the unique random probability measure for which these two conditional laws coincide. And most importantly, in all other cases of interest, like the pit-measure process or normalized random measures with independent increments or Poisson-Kingman priors, you get intractable distribution. So the full posterior has a cost. As a cost, a computational cost that scales exponentially with capital J. So this means that no one will ever use this kind of models in actual applications. And so, and what is even more puzzling is that this factorization that is at the first step of these papers is actually wrong because, well, they argue that since the hash functions. They argue that since the hash functions are independent, then the posterior should factorize. But in reality, these hash functions are applied to the same data sets. And so since the data set is the same, the row or the sketch are not independent, and so the factorization is wrong. And so to overcome these drawbacks, which make Bayesian non-parametric kind of cumbersome, we try to be frequent. Cumbersome, we try to be frequentist in this in the second paper. And so we assume that data are just a sample from an IID sample from an unknown discrete probability measure. And then we follow basically the steps of good. And we first assume that P is known, and we compute what we call an oracle estimator. This half hat of capital P is what is the best estimator that you can get if you know the sketch and you. If you know the sketch and you know P. And this is the same idea of the seminal work by Good, with the only exception that in the work by Good, then at some point he manages to replace these quantities that depend on P with some empirical quantities while we cannot. And I will show you why. So because these probabilities here, these pi j's, depend on this ratio. Depend on this ratio here, that is basically the conditional distribution of what is going on inside one bucket of the sketch. So you take all the data that for which h of x is equal to one, and you wonder what is their distribution. So basically, this is something that we are explicitly forgetting in the way we are saving data. So our oracle estimator does not have an empirical counterpart that is easy to. Counterpart that is easy to compute. And so, a way to remove the dependency on P then is a minimax or worst case analysis. And so, we performed this, but we had kind of bad luck in this because we can actually find in close form what is the soup, so the worst distribution here that realizes this worst. Is this worst risk, and it's a probability distribution that gives mass only to one specific atom. And in this case, so if we plug it in this very specific and weird distribution, we get that our estimator coincides with the original algorithmic approach of the country sketch. So basically, now we are back on where we started because we started from the country sketch. We tried to be Bayesian, and then we saw that, except for the case of the Dirichlet process, which has its own. Case of the Dirichlet process, which has its own limitations, we cannot actually compute the estimators that we get. Then we try to be frequentist, but then we had to get rid of somehow this dependence on the unknown distribution. And then by doing so in a worst case fashion, we recovered the original approach. And so what we need actually, what we realized that we needed, is a convenient, computationally convenient way to put prior information. To put prior informations on P without being fully Bayesian. And so, what we do is again take ideas from Good and propose this class of smoothed estimators. So, in particular, we take the oracle estimator, which depends on P, and then we just simply take the average over all possible P's distributed as a sort of prior non-parametric prior. Of prior non-parametric prior distribution. So, here through this, we are placing assumptions on p, but we are keeping basically the same structure of this simple estimator. And in particular, if we assume a normalized random measure prior for p, then we get that everything is computable in closed form for any class of random measure. So, again, let me remark what is the Me remark what is the difference between the BMP and the smoothing approach is that the BMP approach computes the posterior expectation with respect to the law of P given the sketch of the quantity of interest, while here the smoothing is kind of a prior expectation, but not of the quantity of interest, but of a kind of oracle estimator. So these are very, very different approaches. What is more interesting is that if you assume a direct More interesting is that if you assume a Diricle process prior for the smoothing, then you fall back to the original B and P estimator. So there is this very nice bridge between smoothing and being Bayesian non-parametric. While if you assume a normalized generalized gamma process, then you get this mooted estimator, which is very simple. It's just multiplication of one element of the sketch times some quantity. While if you were to look at the Bayesian non-parametric To look at the Bayesian non-parametric estimator in this case, this would be like a few slides even to write down the expression. So it's kind of impossible to deal with this. And so we have some sort of numerical illustrations in our paper where we basically show that being, well, using the normalized generalized gamma process improves upon the Diraclet process, so there is some advantage in using. Is some advantage in using this muted estimator over the Bayesian, the fully Bayesian parametric one when you use a Dyrical process. The second question that we tried to answer to was to deal with multiple hash functions. So as you recall in the original BMP formulation, they were just saying that, well, you just deal with one and then multiply the resulting posterior because everything is independent, but it was actually wrong. Is independent, but it was actually wrong. And so we wanted to be able to deal with this case, even to understand, also to understand if there is actually an advantage in using this sketch obtained with m hash function as opposed to using just one larger hash function. And of course, the factorization does not hold even in our case. So analytic computations are very, very hard. And so we realized we needed. And so we realized we needed some heuristic. What we did was to borrow ideas from this multi-view learning. So, multi-view learning is a paradigm for statistical learning where you have data with multiple views, such as you have an image and its caption. And you have very good models to analyze the images, you have very good models to analyze the captions, but hybrid models might be harder to find. And so you want to. So, you want to treat this model separately and then somehow aggregate the inference just at the end. And in our setting, we see the sketch obtained with a single hash function as a view. So we have capital M views of the same data. And we for each of uh view, we can compute with our smoothed estimators the probability that we have seen xm plus one r times. x n plus 1 are times. And then we have, so we have m probability distributions that we need to aggregate. And we consider two rules. The first one is the product of expert, which basically just multiplies this probability. So this could be the same as taking capital M independent variables. Each one is distributed according to this mutant distribution. And we say that the probability that we see in xm plus one r times is the probability. R times is the probability that each is proportional to a probability that each of these independent random variables is equal to r and the second one is the minimum of expert rule that we take ideas from the original algorithm and I don't have much detail much time to go into the details but it's just another heuristic way to aggregate inferences. So we don't have any theory on supporting why Have any theory on supporting why any of these two rules should be valid. What is nice is that if we apply the Diraclet process smoothing, then this product of expert rule goes back to the same posterior of chi and co-authors. While if instead in the minimum expert aggregation rule we apply a worst-case analysis separately on each sketch, we go back to the original CaldMeSketch algorithm. And so we have, well, much more than these numerical experiments in our paper. Here I'm showing some errors, so lower is better, where data are generated from a pit manure process with different parameters according to the rows. And here I'm just binning the data differently. And on the row, I'm varying the width of the hash function such that the total. function such that the total memory occupied by the sketch is constant. So what I'm so there are several comments here. The first one is that in purple there is the original algorithmic approach which is the worst one every time. So it always performs worse. While in colors and different textures that I don't know if you can see specifically there are our smoothed estimator with the two different aggregation rules and what is Rule. And what is clear is that the normalized, generalized gamma process is always the one that achieves the smallest error. And moreover, as you increase capital J, so you increase the width of the hash function, but reduce the number of hash functions, the error increases every time. So it's actually convenient to use our heuristic with this ad hoc aggregation rule. With this ad hoc aggregation rule, overusing the standard approach that uses only one very large hash function. So, summing up, our two papers provide a statistical framework for the analysis of this sketch data, which is a non-standard type of data. And, well, since this kind of data are used in Kind of data are used in computer-intensive applications, we needed to find a computationally convenient way to add hypotheses on the distributions that generated the data to improve the accuracy of the estimator. And, well, to the best of our knowledge, this mooted approach which combines frequentist and Bayesian non-parametric approaches seems to be the only framework that is computationally feasible. In our papers, you can see the details about the cardinality recovery problem and some very important thing that I did not tell you about, which is how to estimate these parameters of the smoothing distribution. So, this is actually crucial, and we have an empirical-based procedure to do this, which is not too complex in practice. In practice. And we also have some extension to feature and trade allocation problem, which, surprisingly, at least to me, result in a simple estimator. So I think I'm up with the time and I'll be happy to take any question. I have one question. Question: I was just wondering when you did a reference of the work of good and you use an expectation of an estimator. Would it make sense to be thinking about maybe a median there or like a good discussion problem? Ah, yeah, this is a very good question. So, we take the idea from good because he takes this. Because he takes this expectation and then basically, so up to here, basically, this estimator is the same one of good. It's just that he has information to replace this pi j with some empirical quantities, that is the empirical frequency in the data. Taking the median would be for sure possible. The median would be for sure possible at some level because, yeah, it would be actually possible because, yeah, thinking on this, we have a smoothed version of this pi j, so we have this distribution and we just take the median of this distribution. This shouldn't be too hard, but we didn't investigate this. Thanks for this very Thanks, thanks for this very interesting question. Any further questions? There's no more questions. I mean, if you include time, let's think again, Mario. Many thanks, Mario. Very nice stuff. Thank you.