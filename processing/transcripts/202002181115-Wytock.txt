Thank you for all your patience right before lunch. I'm here to talk about my work on using machine learning to map transcriptomics to phenotype. There's two references, one that came out last year in the PNAS and one that will be coming out in the next month in Science Advances. I'll be talking about both of them. They follow kind of the same workflow, but I'll try and illustrate the differences as we go along. And so the central research question that both of these The central research question that both of these approaches are trying to do is how do we model a complex phenotype? And I'll just by way of introducing the notion of a complex phenotype, focus on two examples. One being growth rate in single cellular organisms, the other being cell type in humans. And the idea behind these complex phenotypes is that they require the interaction of multiple cellular processes. They're not just one single circuit, but you really need to understand how they interact together in order to understand. How they interact together in order to understand how the phenotype is melted. And others have already highlighted the challenges that, you know, in building models where you're interacting, where you're trying to do large-scale modeling of intracellular networks, it's hard to get in vivo estimates of parameters. It's possible that you could have unknown interactions, but at the same time, we have a couple things. At the same time, we have a couple things that are turning in our favor in terms of the massive amount of computational power that we have. So, as I'm sure many are familiar with Moore's Law, we have doubling of our computational power every, what, five to seven years, if I remember correctly. We also have something similar in terms of the amount of bioinformatic data being produced and deposited into publicly available repositories like the sequence. Repositories like the sequencing read archive. So the motivation behind this work is that let's try and take advantage of large amounts of computational power, large amounts of bioinformatic data, and see whether we can leverage that to map some of these low-level effects to larger cellular phenotypes. So there's, I'll tell you about the data, just give an overview. There's five major data sets. Major data sets. The first two are in single cellular organisms. The first column is the number of expression profiles, and each expression profile you can think of as a vector where each entry in the vector is a number of counts of that gene. And these are genome-wide, so it would be for about 4,000 genes in E. coli, about 6,000 genes in yeast. The number annotated are the number of experiments for which you have a paired growth rate measure. Have a paired growth rate measurement for these single cell organisms. So you'll notice right away that there's somewhat smaller amount of data in order to map the gene expression to the growth rate. And of course, with the single cellular organisms, we consider those as being a single cell type. The references are in the paper. They are probably too small for you to read, but they are. But they are there. The human data sets, we have three different approaches to measure the gene expression in humans for the data sets we considered. The first is data taken from the Genome Tissue Expression Consortium, which has roughly 10,000 measurements, all of which have an associated cell type where the measurements are not treated with drugs. There's 26. There's 26 cell types in that group. We also have a microarray data set. Roughly half of these have drug treatments, so they're not used to infer cell type because when you perturb the cell, it's not clear what state they should be in for the purposes of training. There's up to 100 different cell types, including normal and cancer cells. And finally, the last data set is high-C data. Hi-C data, and this is chromatin conformation data. So, different from gene expression, you're actually measuring the physical structure of chromatin contacts in the genome, and we're trying to see whether you can infer the phenotype from that. This is a much smaller data set because it tends to be more sequencing intensive to map out all of these connections. So, what does annotated mean in this sequence? So, annotated, yes. So, in the case of the single-celler organisms, it means that we have a genix growth rate. A gene growth rate associated with that gene expression measurement, where we know the state of the cell. In the case of humans, it means that we know the cell type and that it's not perturbed by, say, a gene disruption or a drug treatment. It is just a measurement of that cell, and we know what the cell type label is. So the approach, oh, go ahead. Approach oh go ahead. All this is supposed to be very condition dependent I guess. So how do you map well how do you aggregate different conditions that might be annotated differently or? That's a good question. So one of the things that we're so yes, these cell types would be somewhat conditioning dependent. Right now we're excluding cases where they are treated by drugs because potentially if you do over Potentially, if you do overexpress several transcription factors, maybe you're in an in-between state where you're trying to de-differentiate a cell. In this case, though, usually the thinking is that your cell types are reasonably robust to perturbations. So, that because you're at least in normal function, your cells don't spontaneously. Cells don't spontaneously change their phenotype, even though they're exposed to varying conditions throughout your physiological states. So the approach that we're going to adopt is we're going to use the correlations between genes as a proxy for the intracellular networks that are processing the information. We'll apply a K-nearest neighbors model, which is effectively a voting model. Effectively, a voting model to assign the phenotype to predict the phenotypes. And there's the rationale for using correlations rather than, say, a direct network reconstruction is that we're trying to avoid the massive labor of reconstructing the network. You also need a lot of experiments to resolve whether two things interact or not and to fit the And to fit the parameters. So we're trying to sidestep some of that by taking advantage of the fact that we have a lot of data of just the cellular states. So cartoons to highlight the approach basically. We have an orange cell and a blue cell type. We would apply sequencing to them and we get transcriptional profiles that are represented by the boxes, each the orange and The orange and blue boxes. The small rectangles inside the box are genes: red and green. Red is off, green is on. In the actual data, you would actually get a count, but for this, we're just representing it visually, it's off and on. And once you have this for a large enough number of measurements across a variety of conditions, you can get an estimate of how. Estimate of which genes tend to be co-expressed. And if you decompose that matrix, you're approximating the information processing network, which we're representing in one cell type. We have all of these black arrows are dashed because this circuit is off or it's not being used. Whereas in the blue cell type, we have part of the network that's on, which is represented by the solid arrows. Which is represented by the solid arrows. And so once we have, we now re-represent the sequencing data along the principal components of this correlation matrix that we're going to call eigengenes, just to refer to them as they're linear combinations of genes. And once what we will do is we project them into the high-dimensional gene expression state space and estimate. Space and estimate the boundaries or the regions where the cell types are in this space. For example, the orange and the blue cell type in this model. And then finally, we'll use the machine learning, the K-Near's neighbors, to do the prediction on this. So just to be perhaps too explicit for this audience about what an eigengene is. About what an eigengene is. You can imagine you have a correlation matrix, which is what's up top. CAB is the correlation between gene A and B, right? You would construct one of these for all the genes that are measured in your data set, and then decompose the matrix, and the eigengene is really what we identify as just the eigenvector of these matrices. Now, what I haven't, so the first step is we find the eigengene. So the first step is we find the eigengenes, but now in order to employ them to do learning, we want to select ones that are most relevant for our phenotype of interest. Some eigengenes are going to be show lots of difference between cell types or growth rate. Others are not. So we're going to do what's called in machine learning feature selection. Or in this case, we're going to select the eigengenes that are most relevant for our phenotypes of interest. Yes? Are you already assuming you don't? And are you already assuming you know the labels of the two types before you because this could this could have all been unsupervised? This is a supervised approach here, yes. Yes, you could imagine, yes. I've considered doing the first thing we tried was the supervised approach, and I've considered options to do some of the unsupervised approaches when, for example, you know, if you're doing an experiment on something, you don't know what the result's supposed to be, you might want Supposed to be. You might want to have an unsupervised approach to say this looks like it's clustering in a different region of the space. But I could talk more offline on my ideas for doing that. So just the motivation, this process of selecting, featuring selection is sometimes called regularization. We're trying to both avoid fitting noise and also reduce the complexity of the model. And we're going to use. And we're going to use in the growth, there are two slightly different strategies between whether we're trying to predict growth or cell type. And they're related to the fact that growth is a continuous variable where cell type is a discrete label. So they're slightly, they're related problems, but one is a regression problem, the other is a classification problem. So in growth, we're going to talk about state space occupancy, like how well is the data representing the The data representing the possible states that you're considering. And in the cell type case, we're going to apply a rule of thumb that measurements from the same cell type should be more similar to each other than measurements from different cell types in general. So this is a cartoon of what the state space occupancy matrix looks like. I have eigengene expression represented, just two eigenes of the total represented on the axis. The total represented on the axes. There's a centers of the circles are the region, would be the Eigen expression. The radius of the circle is the growth, so representing three variables here. In the background is the frequency or how dense the number of experiments are in each bin for the eigengene frequency. Now, there's an experimental limitation to how well you can resolve express. Well, you can resolve expression of an eigengene or a gene for that matter. And if everything is close to clustered really close together as it is here, you can't distinguish reliably the expression levels. So this would be an undesirable set in terms of our, for the purposes of our feature selection, whereas a set where the things are spread out with more Are spread out with more, show more differences in the expression as well as with the growth rate, would be more desirable. So we're going to encode that in our quality function when we're selecting our eigenchates. So this equation is the quality function. It's a little bit nasty, but f is basically a sum of two terms. The first term here. Of two terms. The first term here is the predicted growth rate minus the actual growth rate squared, and taking the square sum of the squares of differences. So you can think of this as the accuracy of your prediction in the first term. The second term has to do with the occupancy. Another thing that I'll mention is that this S here is the set of all features. Of all features. So that means we're going to start with cardinality of s equal to 1. Select the best first feature, then fix that one, try the remaining n minus 1 features, fix a second one, and so on to construct our model as we go through in selecting the eigengenes. The lambda here is a regularization parameter that balances how much you. That balances how much you care about accuracy versus how much you care about state space occupancy. And the state space occupancy is based on the number of bins that you possibly could have versus the number that are actually occupied by the data. So the denominator is in Ng is the number of bins of gene expression. N sub nu is the number of total bins of all the eigengenes you've selected. So you could imagine you break each eigengene into So, you could imagine you break each eigengene into, say, 10 bins. Each time you add another feature and make your model more complex, you're getting 10 more bins out of that. But this number of occupied states by the actual data is going to saturate at a certain point because you can't occupy more states than the number of data points that you have. So, you want to have enough that you're sampling, you're trying to look for an optimal. You're trying to look for an optimal sampling of the possible number of states that you could have in this case. Why the argmin? I'm sorry. So, oh, so the argmin of this is the, you want to take the feature, s, that minimizes this function. That's, the argmin is basically the, so the argmin is just the feature that you end up selecting is to take the selection, the feature, the actual value of this. Value of this quality function would be just tell you, once you reach the minimum, that tells you when not to add more features. That could be a little, go ahead. And this is in the validation stage, so you train a model, then you do selection features on a validation stage. Yes. That's right. And then we try and test it on the other data set. And so the idea is that we would have. The idea is that we would have, I mean, this is a kind of a cartoon of the three regions. You have a region where, for the first feature that you add, your prediction is of the growth rate is not particularly good. It improves as you add more features. At a certain point, you start getting, say, your prediction gets good, but you're now, as you add more features beyond that, now you start to either fit noise or you have a much larger, you're making your multi-larger. You have a much larger, you're making your model much more complex than it needs to be, in other words, in this region. And we see something like this for our actual results for both yeast and E. coli. You get to about eight features that you select for E. coli before it starts to get less accurate, and about 18 features for yeast, and then it starts to tail up, but much more slowly. I'll briefly talk about I'll briefly talk about the same thing, how the cell type homogeneity criterion works in human cells. Basically, we have the blue cell type of blue triangles and the orange cell type orange triangles. And we're comparing the pairwise distance of all pairs of interactions for the blue cell type and between the blue and the orange cell type. And the rule of thumb is if measurements. rule of thumb is if measurements of the same cell type are more similar than measurements between cell types, for a fixed percentile or fixed area under the curve, the pairwise distance should be smaller for the within cell type distances than for the between cell type distances. And then this actually has an interesting impact on the shape of the cell type regions in space in that if you Space, in that if you here we would imagine that we know what the regions are in space through some of the modeling somehow. You would actually, if you connect two of measurements of the same cell type by a cord, and you sampled along that cord and applied the model to predict what the cell type would be, in this case, what you would have in the middle region is you would actually predict some things along this cord as being a different cell type. As being a different cell type, which is actually undesirable. You'd rather have something that's relatively convex so that you could say that if you're on the interior of this some space in high-dimensional gene expression space that you have some confidence that you're actually belonging to that cell type. By applying the homogeneity criterion, you actually tend to reduce the number of times that you see this. So that's something that we tested. So that's something that we tested to show. And that gives some confidence that when you are kind of getting close enough, you're more likely to be long to that cell type. We see, so different from the E. coli and yeast data where I showed kind of the feature selection. Here we're plotting accuracy. We're high, one is good, zero is bad. And so again, we have the underfit regime where if you only have zero to one. Only have zero to one to three features selected. You don't do so well. It slows down from about four to six features, and then beyond six features, you're now wiggling around depending based on the noise somewhat. So let's show some results on the validation here. And so, what we would do is we'd train the model, and now each of these experimental sets that we have. Experimental sets that we have. So each profile, the gene expression profiles are grouped into experimental sets and deposited into the databases. So usually it would be, you know, if I did a set of 12 experiments to figure out measuring in several different conditions with, say, six technical replicates of two conditions, I would deposit those things independently. I would deposit those things independently into the database. So, as a way to test, say, unseen data, we would take that data out and try and predict the growth rates of those experiments on the model based on the gene expression taken from the database and see how well we do. And we do this for both E. coli, and here the different colors are representing different strains in the data set. Note that the Note that the R squared here is capturing roughly 80% of the variability in growth rate here. The gray outline would be, say, the, say, roughly 5% error of the dashed line, which would be perfect prediction would be everything falls exactly on the line. I'll talk about how this compares with other methods at the very end. We also do the End. We also do the same for yeast. And in this case, the colors are different carbon sources. So this is the same, more or less the same strain because these were generated at different times, but kind of by the same, mostly by the same lab. And we actually do get closer to 90% explaining the variance there. Now, the cell type validation is similar in that instead of Is similar in that instead of doing a regression problem like we did up here, where the outline. Sorry, Palmer. Yes, yes, please. So if you mean E. coli would take just the expression level of ribosomal genes as your feature has been reported to scale linearly with growth rate. How well would you do? So the challenge is that oftentimes people deplete the RNA in these gene expression databases. Well, okay, so if you had a metabolomic measurement of the ribosomal proteins, you're saying? Or okay, so you're saying like the RPL. Okay, I think. So we did try to do a comparison with, say, the precursors of biomass. I could look at the ribosomal proteins to see whether To see whether you do as well on that, I've not done directly that problem. The reason we're interested in things outside of just the ribosomal gene is because there's a particular if you wanted to, the goal that I'll talk about at the very end is what we want to be able to do eventually is take transcriptional impacts from, say, a drug, input them into the model. Input them into the model where you change the transcriptional state, and then see if you could, say, optimize the growth rate. And that may or may not, it might only be, does that drug impact the ribosomal genes, but it may not on the outside. But it's a good, I mean, this is a good thing to consider, especially for the microorganisms, right? Because we do have a lot of physiological information about the ribosomes being important for growth. Ribosomes being important for growth. It's less clear in the cell type case whether there's something that is a unifying signal for cell type. I mean, yes, you have, in the developmental context, you know, certain factors will be on, but in the differentiated cells, it's less clear what you would need to change to go from one state to another. There's a whole industry of biologists defining models. Biology is defining marker genes or single genes. Right. That is true. The problem with a single gene method is you might, it's kind of the difference between your measure and the behavior, right? So the marker, you're trying to link the behavior to that measurement. However, it could be that when you're perturbing the cell, you turn on that marker. You turn on that marker, but you've broken the connection that was established as the biomarker connection. So that's why we're trying to stay with the whole cell case. And in fact, if you had a mismeasurement of a single gene in a pathway, you could end up having like so miss like a false positive in terms of your biomarker that could be mislead you. Whereas if you're taking a broader Whereas, if you're taking a broader scale, you can kind of detect and not be confused by, say, a single mismeasurement in a gene. So, that's the other rationale there. All right, so I just wanted to show the validation for the cell type experiments where we have the 26 cell types. And so, these are aligned. It's kind of, it's probably hard to see the actual cell types, but they're ordered from the most. That they're ordered from the largest number of experiments to the smallest number of experiments, predicted on the y-axis, the measured on the x-axis, and they are in the same order, so things along the diagonal mean that you're predicting accurately. The fraction predicted of one is the dark orange, and less than one in 100, or less than 1% is white. And what's interesting, it's not only that the prediction accuracy, which is around 9%. Accuracy, which is around 93%, is good. We also see that the places where we have missed predictions, they're actually related functionally. For example, the colon, large intestine tissue, and small intestine tissue are like one of the highest values. Another case is we have breast tissue and adipose tissue. So we're seeing some biological. So we're having seeing some biological relationships between the areas that would be confused in this sort of in this validation scheme. And so returning back to the data sets, we have kind of roughly rules of thumb of what previous methods can do versus what we were able to do in terms of predicting the phenotypes. And the And the in E. coli and yeast, where you might E. coli, there's been a lot of work on metabolic modeling that seems to do reasonably well, but we were still able to improve upon that with some including all of this transcriptional data. In yeast, the linear models that predicted for, say, one particular cell type or one particular state quite well, that set of genes. Well, that set of genes did not go extend to another context as successfully. Finally, in the RNA sequencing versus the microarray data, we see kind of the power of RNA sequencing and the increased sensitivity to low expressed genes. Interestingly, for the high-C data, we don't do that much better than others. And there's some interesting things to talk about there if we. Things to talk about there if we get to the time of an extra slide, but the short answer is we find that the local structure is less important for predicting cell type than the long-range structure. And a lot of high-C experiments focus on the local looping of DNA rather than whether these things are packaged, perhaps, where you have part of DNA on one chromosome near DNA on a far away point, either on the same chromosome. Far away point, either on the same chromosome or perhaps even on the inter-chromosomal contacts. So, I do want to talk about applications. So, I've talked about kind of all this work we've done to do prediction of the state, but what do we want to use it for? And the goal would be to take the transcriptional response to a perturbation and then be able to test that in silico to see if that would give you the desire. Silico to see if that would give you the desired phenotype. And so this is a cartoon again of measurements in a high-dimensional expression space. The filled circles are unperturbed states. The empty circles would be, say, a gene knockout. And so we would just calculate this, measure this vector that I'll call B sub K, which is the response to, transcriptional response to a gene knockout. And or, say, knockout. Say knockout B sub L, and you would collect the data from a database, you've seen where you've seen these knockouts applied. And the most naive thing to do would simply be to add them together, tips to tails, which we know because the cell is a non-linear system, this is not expected to work in general, but this gives us the first order kind of approximation of what would happen if you combined a large number of single knockouts together. Of single knockouts together, and you could see whether you're pushing yourself in the direction of your desired phenotype. That's, for example, this boundary line here would be the boundary between the red and the blue cell types, where the red is the undesirable cell type and the blue is the desirable one. So with that, you can find, look for our upcoming paper and science advances here. You can see the results on the single cellular organisms in PNS. Single cellular organisms in PNIS from last year. There's also code available on the GitHub, and I open it up for questions here. We've got a little bit of time. Yes, please. I mean, compared to something like a random forest. So what we did is we actually, in this most recent paper, we tried a canier s neighbors on the eigengenes. On the eigengenes. We tried a random forest and a support vector classifier. We found that the K-Nair's neighbors does seem to work slightly better, but we tried a few different things to see whether there was one that, you know, with the idea that maybe one works better. Correct. Let me see. Yes, you would do the I'm trying to think of whether we had what we did in terms of that. In terms of that, I'm not sure if it's a night. Let's see here. I guess in the random forest case, I'm trying to think of how we would, because my understanding is usually when you're applying the random forests, you could select only certain features that could be selected, or you just, as opposed to, like, because usually it's like. To, like, because usually it's like you take something and you say, all right, does it split which direction? So I'm not sure. We did run the, we did not do anything to augment the random forest beyond kind of some of the packaging there, but we did allow it to fit to the data in terms of taking it out of the box a little bit. Other questions? Yes, please. Do you think you do much better in yeast than in E. coli because you have only one cell type that goes into the cell? Possibly. I think there has to do with there's less variability of the data. These are the experimental data set that we were able to obtain with growth rate was more somewhat more limited. It was grown on different conditions, but it was not, say, many different strains, for example. Many different strains, for example. That's what I would attribute it to in that case. Oh, Elizabeth, sorry. So it seemed like you were doing this always in the context of one data set. Do you gain something by mixing different data sets together, looking at FIC and templates? This is something that would be interesting. What's hard to do is hard to build training sets because often Hard to build training sets because oftentimes they're not. I mean, the goal here was to try and get a large data set of at least one data modality. But we would be interested in trying to incorporate, say, two complementary modalities, right? Because you get different information from the gene expression than you do from chromatin conformation or methylation data, for example. Data, for example. All right, thanks again.