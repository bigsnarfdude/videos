For the invitation, the opportunity to give this talk. Also, I would like to say that I really like this format of the conference, like presenting some others' work. In fact, as I was building this talk, it turned out to be more like a survey rather than speaking about a particular work or article. I think you will find it interesting anyway. And please, please interrupt me if you have any questions at any point. So before, I mean, I will talk about artisobola spaces in the unit ball of the complex dimensional space. And I mean, before going to the motivation and all that, let me introduce some just Use some just a definition of how this hobble space is stored, so we're on the same pace. So I denote by B D the unit ball and D sigma is the normalized surface measure on the boundary of the unit ball. And I mean, you are all familiar with the Hardy space, H2. One other thing that I want to mention is that in this talk, I will only talk about Hilbert spaces. I mean, obviously, there are HP Hardy spaces, but I want HP hard spaces, but I won't only talk about Hilbert spaces. So the okay, the hard space is just this space of holomorphic functions in the ball, which are uniformly square integrable on all smaller spheres, spheres of strictly of radius strictly less than one. And then sorry, I think. We introduce also this fractional differentiation operator. So, here S is some real parameter at the moment, then we will specify it better. And it acts on a power series, I mean formally like this. It takes a power series and it multiplies by this weight to the S the monomials in the power series. Monomials in the power series. I mean, here I use multi-index notation, right? So this Z is actually one. And similarly, alt. And then I can define my Hardy Sobel spaces, H2S, which is just a space of holomorphic functions such that this S functional derivative of F belongs to H2 and the norm. I can choose of many the Define, I can choose of many the equivalent nodes and then define it in this way. So, I mean, this has the first advantage that I can put many of the spaces that are particularly interesting for operator theory or function theory in one family of spaces. So, for s equals zero, of course, this is just the identity operator, so it just gets back the hardy space. But then we have some. But then we have some more interesting spaces, in particular, for this be a d over 2, I get the so-called Dirichlet space. Then for D minus 1 over 2, I get the Drury-Arverson space. And then for greater S we have some algebras, and then we have some mysterious spaces in between, and then some Bermuda-like spaces. Then some Bermuda-like spaces. Okay. That was just the definition. But I would like to talk about the motivation. I mean, why? Ah, yes. Before doing that, let me also mention that these are reproducing kernel Hilbert spaces. And for some equivalent norms, not exactly this norm, but some equivalent norm, we can write the reproducing kernels explicitly. So this is the reproducing kernel for H2S when S is strictly less than p halves. And the reproducing kernel of the Dirichlet space is this kernel with the logarithm, this logarithm that typically appears on one Dielsless space. So yes, as I was saying, I mean, I would like to talk about why these spaces are. Why these spaces are interesting, why one should care about these spaces. And I want to look at it from two different perspectives. I mean, one is especially for operator theory. I mean, Dury-Arberson space is probably you know much better than me. It's of fundamental importance. I mean, and essentially this is um starts with uh Duri's von Neiman type inequality, which you also know, but let me Which you also know, but let me restate it. It says that if you have some d-tuple of operators which commute and act on a Hilbert space and they satisfy this inequality, so they are, as we say, row contraction. Then, for any complex polynomial of d variables, the norm of the polynomial applied to The norm of the polynomial applied to this detouple of operators, the operator norm, is bounded by this, let's say, multiplication norm of p. So it's the norm of p times f, the supreme over all f, which have a norm less than one. Norm here, I have to be careful. It's a norm which is not exactly the norm that I defined before. It's equivalent to it and can be defined in terms of the tailor for this. Another view is a geometric approach. From this approach, I think the Schlespasses is a more, it's very interesting because, I mean, it's a theorem which, if I'm not wrong, for dimension one is due to Arazian Fisher, and then for all dimensions is due to Mark Coppelozzo, which says that the Which says that the Dirichlet space is, in some sense, which can be made precise, the unique Hilbert space of analytic functions, which contains constants and it's invariant under compositions with holomorphism of the uniform. I mean, invariant in the sense that the norm of a function composed by holomorphism is bounded by some constant and the norm of f, and this constant does not depend on phi for f. constant does not depend on three for every three holomorphic possible in fact i wanted to mention this this result because in fact um for one dimension you probably all know that this comes from a seminar which is exactly the area of the image of f as a remote Of f as a Riemann surface, or as we say, counting multiplicities. But I mean, although there exists an invariant seminar in all dimensions, and there is an explicit formula due to Marco Peloso, I don't know any geometric, let's say, interpretation as the one that we have in one dimension. So, what is what this quantity represents? We just know that it's invariant under by all of them. So, I think it's a very interesting problem. It's a very interesting problem to find the geometric meaning of this. So, pretty much that was about the motivation. I mean, one can say much more about this basically. These are two highlights that I wanted to mention. So, the idea was to talk about this about two quite separate but intertwined topics. One is Carlos on measures and multipliers. Measures and multipliers, and the other one was interpolation. So, multipliers, I mean, the multiplier already appeared when I talked about Drury's inequality. So, let me now introduce it formally. So, the multiplier of H to S is just, as you know, the space of all functions f, such that if I multiply it by any function g in. In H to S, I remain in the same space. And it's about algebra equipped with a multiplier norm. So you recognize this point that it appeared already in Durham's unique one. Now, this makes it quite interesting to estimate at least or calculate the multiplier norm of a function more explicitly, let's say. Let's say. And it can be proven that the multiplier norm is equivalent to the supremum norm of F times a constant which comes from Carlosian measures. And in the next slides, I want to emphasize on this constant that depends on correlation measures defined by F. Defined by F. So let me look at this constant more carefully. So, first, we need to introduce the notion of Carloson measures. A Carlson measure is a positive Borel measure in the ball such that our hard sobel space is contained in L2 of this measure. So, in fact, by the closed graph theorem, you know that this is equivalent to writing this inequality. I think it's inequality. It is controlled by a constant times the number and which is equivalent to saying that the identity operator is bounded. So we define this Carolson constant of mu, let's say, to be the norm of the To be the norm of the identity operator acting between these two spaces. And the Carlesson, this quantity associated to F in the previous slide, it's comparable, comparable, I mean it's bounded below and above by some multiplied by some constant by the Carluson constant associated to this measure. I mean, you don't have to read the example. Have to read exactly what it is, but it's a quantity that depends on some derivatives of the function f for some m bigger than s with respect to some weight. And this d lambda d is the invariant area volume measure in the ball, which is I think some fifty minutes. So, I mean, what essentially I want to say in this slide is that if you want to understand the multiplier norm in Hardy-Sobel spaces, you have to understand the Carlesson measures of the Hardy-Soblet space. And this, it turns out, that it has close connection to a notion of capacity for sets in the in the D dimensional sphere. The dimensional sphere. So here now we restrict to positive s less or equal than d halves. So for a measure, a positive Borel and so on, we define this potential, S potential of the measure, to be just in some sense the convolution of the measure with the kernel of the space. I remind you that this kernel is Kernel is a absolute part of one over two t minus two s right then we can define an energy for our measures which is just the the integral is just the integral of the potential of the measure with respect to the same measure. The same measure. And finally, for a set E, so E is a subset of the boundary, let's say compact. We can define its capacity, which is the maximum measure you can put in the set, such that the energy of the measure is. energy of the of the measure is uh stays below one here by m plus i mean just positive uh positive bored measures so that's the capacity in fact that's the that's the square root of the capacity um and i mean we introduce also this um uh these balls here which i mean in dimension one Here, which I mean, in dimension one is just some kind of Carlos boxes. So if Z is a point in the boundary and R is some number strictly less than one, then this area here is just the Carlism box. And this point here is just Z times one minus R. And IR of Z is just this boundary set. Just this boundary set. I mean, this is dimension one, but analogously works in all dimensions. And I mean, we have the following theorem connecting all what I was saying about capacities and Carloson measures, which says that for this range of S, so between the Drury-Arverson space, sorry, between the Dirichlet space and the Drury-Arveson space excluded. Space excluded. A positive Borel measure is Carlesson if and only if for any points in the boundary and for any numbers for any numbers are k strictly less than one, the measure contained in the union of these boxes is controlled by a constant times the capacity of the union of these boundary arcs. So let me draw a picture which is very helpful. So you take any configuration So, you take any configuration of sets of the form QZR. You consider all the mass that mu puts here. And this has to be controlled by a constant times the capacity of the set, which is just the union of these boundary intervals. Okay, this was resulted by Stegenga. Was resolved by Stegenga. It was proven for the Dirichlet space in dimension one, the classical Dirichlet space, and for then Ahern and Kohn for all artisoble spaces in this range. It's very important that this result does not hold in the whole range. In fact, if S is less or equal than D1 perhaps, so we're in the left of the Durial. Left of the Durial-Epson space, then this condition is sufficient but not necessary. And I think this is very interesting and it's worth spending some time to understand why macroscopically the proof works and why this is not characterization for the Durian Russian space and the other spaces in the scale. Spaces in the scale. So, in fact, the idea of the proof is the same for all dimensions, and it's the following. So, you have a potential theory defined by the kernel, and it happens that the reciprocal of the kernel is what we call a quasi-distance. So, this is, I mean, the reciprocal of the kernel is just this quantity, right? Sorry. Sorry, it's d minus 2s at 4. And of course, this is a quasi-distance because, I mean, it's some power of the Qurani distance, which is a true distance. So it's a quasi-distance. Then we have a very abstract potential theory due to Adams and Hedberg, which says that. Adams and Hedberg, which says that if you have a potential theory, which is defined as I defined it before, potentials, energies, capacities, and so on, then this potential theory satisfies what is called the boundedness principle, which is close to the maximum principle for harmonic functions. It says that the supremum of the potential of a measure is controlled by some constant times the supremum. Times the supreme of the potential just on the support of measure. So if you have a measure supported on your boundary, then it has a maximum value in the set that is supported, and then it decays outside the set. And then, again, by some abstract machinery that we don't, I mean, it works generally, we get some what's called a strong faster inequality, which is Which is, I mean, a bit technical to read, but essentially it says that the capacity of the set where your potential is greater than lambda integrated with respect to d lambda squared is controlled by the L2 norm of the measure. I mean, when I write L2 norm of the measure, I mean that my measure is absolutely continuous with respect to the surface measure and take the L2 norm of the Normal thing, but on continuing, and this, so I mean, just by the fact that we have a kernel which is a quasi-distance, we arrive automatically to the strong quester inequality. And then the proof works quite easily because you pick a function in the hardest Hobbes space, take, let's say, it's Space, take let's say it's a real part and it's the same thing. You can prove that it has a representation as a potential and the norm of f of phi in L2 sense is bounded by the Hardin-Sobel's norm of capital F. And then you just take the integral that you want to bound, you write it in the distributional sense like this, and then And then you use the fact that your measure satisfies this sub-capacity inequality. So, that the fact that if you take unions of such sets, the measure is bounded by the capacity. I mean, here implicit, you have some maximal function argument, but I will skip it. And then, by the strong capacitor inequality, you know that this is bounded by the L2 norm of phi, which is by construction bounded by the By construction by the hard sobbling form of x. So, I mean, if you have the moral of these two slides, is that if you have a kernel which satisfies the quasi-triangular inequality, then quite automatically you get the Carloson measure, not characteristic sufficiency for Carluson measures in this way. So, what was wrong in this? So, what goes wrong in this argument when D is less than D minus one halves? So, in the truly argued side? I mean, in fact, nothing goes wrong. The argument works. Only that it gives only a sufficient condition of taking the absolute value of the kernel. It turns out that it's too crude. In fact, I will say it now, but it will become more apparent. Will become more apparent in the next slide. It's only the real part of the kernel that plays a role in embedding theorems. And it just happens that for this range, the real part of the kernel is pretty much the same as the modulus of the kernel. I mean, you can make this because what happens is that I mean When you have this inequality, this exponent here is between zero and one, so this turns out to be equivalent to the real part of the scale. It's simple. But why I'm saying this, it will become clearer later. So, in fact, in the critical case of the Drury Arvees space, there Um, the real part of the kernel, which is the part which is interesting for Carlos and embedding theorems, is still positive. So, in some sense, we still have some hope. While for S strictly less than d one halves, you have a signed kernel and things are even worse. So, what do we know about the Drury-Alveson space? Consider the following experiment. Take a Carloson. Take a Carloson measure and you have an identity embedding and take theta to be its dual. Then you can explicitly calculate the dual by using the reproducing kernel. Here I wrote the alpha to denote the Julia Russian reproducing kernel, which is just as you probably know. And then you flip to the other side. And then you flip to the other side the adjoint, and you get this expression for the adjoint of the identity. And then you test theta star theta, which now goes from L2 to L2 of mu, on characteristic functions of this Carloson type boxes. And of course, since we started with a Carloson measure, you get a necessary condition for Carlesson measures, which says that. Which says that if you take this double integral of this real part of the kernel, then it must be bounded by the measure of this Carloson box. I mean, if you have not seen this before, it's quite a strange condition, but it's what is called T1 testing condition. The surprising fact is that as Is that as Potsy Roffberg and Sawyer proved, and then Sundia reproved in a different way one year later? Is that in fact a measure is Carloson for the Drury-Arberson space if and only if satisfies this testing condition and you have this simple growth condition. So the measure of this curves and boxes grows linearly, let's say. So this So, this now probably understand why I was saying that the real part of the kernel is the only one that counts. Because, I mean, this inequality involves only the real part of the kernel. And it's still sufficient and necessary to characterize the Carloson measures. So, this was the part about Carlos and measures. I mean, essentially, I talked about these four papers. One was the These four papers. One was the first one by Achern and Kohn about exceptional sets for the hard disobedient spaces. They do it for all fields, strictly less than one. Then Kohn and Verbitsky developed a quite refined non-linear potential theory. Non-linear applies to the fact that P is not equal to two, so we're not in a Hilbert space, which causes some extra difficulties. Some extra difficulties. And then we have the paper of Parkozi-Rothberg and Sawyer for Carlos measure for the Drury-Arbene space and the paper of China. Now, I have some more time and I want to talk about also interpolation problems, which are closely related to the Carloson measure problems for the Hardiskoblock spaces. So, very generally, I mean, interpolation. I mean interpolation problems you have some you have some data which are let's say complex numbers you have some nodes and you want to find a function in some preassigned space X such that it takes on these nodes takes these values, right? Notes takes these values, right? I mean, here I mentioned the classical elementary fact that you can interpolate n values in n points by a polynomial of degree less than n. But in our case, of course, this space from which we draw our functions is going to be a Hilbert space of functions, and in particular, it's going to be And in particular, it's going to be one of these Hardy Sobolev spaces. So, let me say a few words in general about interpolation in the context of reproducing kernel-Hilbert spaces. So, suppose you have a reproducing kernel-Hilbert space in the unit ball. This is, I mean, in a while, it's going to be again this one of these hardest cobalt spaces. Then take a sequence and we construct this weighted restriction. Construct this weighted restriction operator, which just takes a function and restricts it to the sequence weighted by the norm of the kernel. Here I read, of course, that it maps well too, but it's not true. I mean, a priori might map everywhere. If, in fact, it happens that the operator of restriction is on on top, so if On top, so if tz of h is the whole L2, we say that the function is simply interpolated, also sometimes onto interpolate. It's called optimolating. So explicitly means that for every sequence of data in little L2, I can find the function in my space h which takes these values on these points z i. Z pi. In particular, if Tz is also bounded, then we call this sequence universally interpolating. So write it in this notation, Tz of the space is exactly L2. It's an easy exercise to see that the boundedness of the weighted restriction operator is equivalent. Is equivalent to the fact that this measure, which is just a sum of direct deltas weighted by this producing kernel squared, is a Correlation measure for H states and L2 of this measure. So, this just some comments, immediate corollaries from the definition. And a condition which is more geometric and it's necessary. And it's necessary for interpolation is what is called weak separation. So it means that if you define this metric here, which is just, I mean, more intuitively, it's just the absolute value of the sinus of the angle of these two kernel vectors. Then, if a sequence is interpolating, then all points are bounded below in this distance. Are bounded below in this distance by some epsilon positive. This is necessary for interpolation. And in Hardisoble spaces, so when S is not the half, so we're not in the Dirichlet space, then this is in fact equivalent to the separation with respect to the Bergouan metric. For the Dirichlet space, the separation is a bit more intricate. So, there is this theorem which says that, so in this range now, it's exactly the range that we had before, but it includes also the Drury-Alverson space, then a sequence is universally interpolating for H to S if and only if it's weakly separated and the measure is a Carloson measure. So, in fact, these two conditions that I mentioned that are necessary together are also sufficient. And this theorem has a lot. And this theorem has a long history. I mean, first, for one dimension, for the hardy space, for the hardy space in one dimension, it was proved by, I mean, I say Carlson and Sabir and Shields, but in fact, Carlson proved an analogous result for the multiplier algebra of H2, H infinity. And then together with the work of Shapiro and Siles later, Later, the result is equivalent to this theorem for d equals one, s equals zero. Then again for one dimension, but for the whole range of S and all the way to the Dirichlet space, the result is contained in two preprints independently proved by Bishop, Marcel and Sandberg, both in. Marcel and Sandberg, both in 1994 and still unpublished. Then another argument by Boe in 2005 approves the result for all S, but it doesn't quite work for the Drury-Alverson space, so he has to exclude this case. And then, in fact, for all S and B in the theorem, For all S and B in Ethereum, it was proved by Alleman, Hartz, McCarthy, and Richter in 2017. And then again, another proof by Hartz in 2020. In fact, the way they do it is they prove it for a much larger class of spaces, known as spaces with a completely valid problem. Yeah, so that that was what I wanted to say about interpolation in this deterministic setting. In this deterministic setting. And I mean, in the last minutes that I have, I just want to mention a new take that we had in interpolation recently, which is, I mean, random, we call it random interpolation. And in fact, instead of a sequence, you consider a random variable and A random variable and a point process, and you ask with what probability this point process is an interpolating sequence or not. I mean, the idea is that if you randomize the problem, you get a feeling of what situations are generic, I mean, what a typical sequence of points behaves like. So, there are, of course, many ways to consider. So, there are of course many ways to consider random sequences of points. But we start, we will talk about the simplest possible way. What you do is that you choose a sequence of points zeta n independently, randomly, on the surface of the sphere, distributed according to the Lebesgue measure, so uniform on the sphere. Problem in the sphere, and you choose deterministically you fix a sequence of radii, and then you consider this random sequence of points, lambda n, which is just the point zeta n with this modulus. We call it the Steinhaus sequence. Now, of course, we want to ask with what probability these sequences are interpolating, but the first thing to Sequences are interpolating, but the first thing to notice is that a sequence being interpolating or not, either universal or simple, as we mentioned before, is a tail event. So it depends only on the eventual behavior of the sequence, and it's independent of any finite number of this random variable. So you have Kolmogorov's 0-1 theorem, which says that the probability of this sequence being interpolating is either 0. Been interpolating is either zero or one. So we expect to find a condition on Rn, because remember Rn are fixed, which says when this sequence is interpolating with probability one and when this condition fails, the sequence will be interpolating with probability zero. zero. And I mean, we had, I will break the rule and talk about one or two results that I also did. So with some collaborators, Artman, Karen Kelley and Red Wick, we looked into this problem and in fact it is convenient to introduce this counting function. So this function measures how many of your Measures how many of your points have a per invariant distance, so this beta, you can think of it as the Bergman distance, from the origin between n and n plus 1. And then we can formulate our results as follows. So we worked in dimension one, and there's an interesting breaking point. So up until one-fourth, so this fractional is Remember, you had the scale of spaces here, which are the sl so in the middle, you have s one-fourth, where in dimension one. Up to here, this sequence lambda is universally interpolating for h to s with probability one, if and only if this summability condition on the square of this counting function holds. Now it's interesting. Holds. Now it's interesting because, first of all, this condition does not depend on S. So in fact, independently of S, all these spaces have the same universally interpolating sequences. And of course, if this condition does not hold, this event has a probability zero. This is to be expected because of Kolmogorov's. To be expected because of Kolmogorov's 0, 1 0. Then in the other half of the spaces, so if S is between 1 fourth and 1 12, then the condition changes and it depends on S and it's a summability condition not on some power of n, but rather on the counting function itself. So it's just a summability condition on the distances of. Some ability condition on the distances of the points to the boundary. And we have this characterization. Of course, if this condition fails, the probability is zero. And for the Dirichles space, which is the only remaining case, we have again a theorem which describes interpolating sequences. And now, of course, you don't have two to the minus something. You have the logarithm of this because we are in the Dirichlet space. Because we are in the restless phase, so you have this model in this. And it was interesting because recently another paper came out by Dian, Wyek, and Wu, who studied also the same problem in higher dimensions. I mean, in their paper, they have did a lot of work also in the poly disc and so on. But in particular, regarding this problem, they studied. This problem, they studied interpolating sequences, random interpolating sequences for higher dimensions, and then the strange thing happens that for all S in this range, so Druze Arguson included, but not Dirichlet, you have this law, Kolmogorov 0, 1 law, which depends on S. And there is, I mean, there is no breaking point. Uh, I mean, there is no breaking point that appears in dimensional. Uh, also, I don't know what happens in the case of the Dirichlet space. I mean, I expect to be some analogous result, but I don't think there is something in the work. So, that was all that I had to say. Thank you for your attention. Thank you very much. Thank you very much. Let's give Nikasa a round of applause. And let's turn to questions and comments. So, perhaps I could ask about this. Perhaps I could ask about this. I'm sorry, I have a question. Okay. For the Druyavsen space, or those spaces, are the simple, as you call them, simple interpolating, the onto interpolating sequences. Is there a theorem about those? So for simply interpolating sequences, so just to remind to everyone, it means that for any data. It means that for any data you can find in the function, but maybe the operator is not bounded. So we know, first of all, I mean, it's not even clear that they exist. So in fact, in the hardy, in dimension one, for the hardy space, if a sequence is simply interpolating, then it's automatically universally interpolating. It's included in the theorem. Carlos. It's known that for the It's known that for the only space that I know of that we have constructed simply interpolating sequences which are not universally interpolating, it's the Richlet space in one dimension. So s equals one half in equals one. And there is no known geometric characterization. So I had the results some two years ago characterizing simply interpolating sequence. Simply interpolating sequences for a Dirislet space under the additional assumption that this measure is finite. So that this series converges. Under this assumption, there is some potential for characterization of such sequences, but there exist even some sequences that this measure is infinite, and they're even more difficult to characterize. Difficult to characterize, and I don't know how to do for all the other spaces. I don't know even an example of simply interpolating sequences which is not universally interpolated. I don't know if you can. Thank you. So I think we have a question from John. Yeah, I have a question about your the capacities you sorry, I don't know how to put my hand down here. There we go. About the capacities you talked about at the beginning of your talk. Does it follow that Does it follow that you have boundary limits quasi-everywhere with respect to those capacities? Yes, yes, yes. For these spaces, the exceptional sets are of capacity zero for all S between, in this range. Yes. And by an exception set, you mean a set where you don't have a non-interactive. Don't have a non-production, yes, yes, thanks, yes, but exceptional sets are a big mystery for all other S's, even for the true algorithm space. Yes. Oh, you're saying it's because that's the only one where you have matching necessary insufficient conditions that you can say exactly what the exceptional sets are. Yes, exactly. Thanks. Okay, do we have any uh Okay, do we have any other questions for Bernie Kos? All right, well, if not, let's thank him again for a very nice talk. If you could upload your slides in the chat, that would be great. Thank you. All right. So we now have a slightly longer