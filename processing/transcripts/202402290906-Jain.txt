We're on perfect timing for MIT time team. So yeah, it's great to have IHJ here. Tell us about some cool connections between curriculum class, SKA, and probably not left. Thank you so much. First, big round of thanks to our wonderful organizers for organizing such a like amazing program. I've learned so much like in last three, four days. And just we have like a like nice nature, that doesn't work. Nature that doesn't work. Anyway, so I'm going to talk about this new assumption called dense bars learning parallel noise. And yeah, I have another title here. Don't look at it for now. It won't make sense. It's like obfuscated. We don't have to go into detail about it. Okay, so the whole agenda is around new assumptions. As you know, like cryptography, we need assumptions to build things, and assumptions are really the fuel for everything we do. Unfortunately, right now, there's a big shortage in assumptions, especially Big shortage of assumptions, especially for doing post-quantum cryptography. And in particular, post-quantum cryptography beyond your basic notions of like public key encryption and things like that. So really advanced post-quantum cryptography. So the first thing, imagery that comes in mind when I think of like remarkable success story of cryptography. So picture like this. I mean, this is sort of soil where you have all these amazing hard problems. And then on top of this, you have a bunch of things you can build. And now I picked some notes. Can build. And I picked some notions here, sort of to represent the selection of topics we do. So it's invariably biased out of my own interests. Yeah, I just want to gauge a sense of how many of these things you might know or you might have seen this. How many of these notions were you aware? Just to get a sense of what kind of cryptographic background should I assume from this audience. So, how many of you would know, let's say, three of them? Or what they're saying? Not bad. Okay. Not bad. I was not trying to embarrass you, I promise, but not bad. Okay, so anyway, we won't t touch on like these notions in any detail or whatsoever. And like most of the times I will not be talking about any crypto notions whatsoever. But if I do, and if there's some sort of like question, just ask right away, don't wait for it. And this talk is going to be very accessible and low level, so sorry, high level, so yeah. Anyway, yeah, okay. So Okay, so now let's just change the game and let's ask what can you do quantum or what can you do against quantum computers, secure against quantum computers. So you have this other set of assumptions which are commonly studied in cryptography and you can do pretty much everything I mentioned, the people side except like I mean with permutations, like in distinguished cryptography oblivion, rest of it. And the major player, the really elephant, is learning with errors. From learning with errors, Errors. From learning with errors, turns out, I mean, that's really the reason why we have so much progress. Classic example, we can do fully homomorphic encryption. Thanks to breakthrough words by Winode and Friends, you can base it on LW. So LW really has been very, very successful. But let me now change the game again. Remove LW from the picture. What do we have now? Wait. Yeah. So the things in write. Yeah. So the things in Red, we don't know. This is starting to get a bit worrying, right? Into the game again. Now. Sorry, why do we want to remove LWE? So we don't, I mean, we want some sort of diversity in our assumptions, and right now, quantum LWE is the only game in town. So I'm trying to make a point that if you want to do things from other assumptions, we are severely lacking. So these are the things that go away. Pretty much these things go away only if you want those. Go away only for only if you want those quantum. Post quantum care. Okay. Okay. Because we're parallel quantum. Yeah. Why do we care about just this? I will answer all about permutations, my concern. One way permutations seem like they use this thing. For some things, we are cryptographically like in some sort of protocols, you don't need guaranteed strings to have a promise that they are in some image of some functions. You might want to use it. Anyway, so pretty much everything goes away. I'll again change the game further. Change the game further. Now, I'm going to ask only for things which you can base on sub-exponential hardness. Remember, like you know, there are problems like that technique and so on and so forth. They are broken in quasi-poly time. You do not want to construct primitives that are quasi-quality broken. You want to construct primitives which are secure against 2 to the n to the c time adversely for some c. So then we are left with like nothing almost. Pundit functions, public key encryption, and this very fascinating class of new derivatives, homemarket secret sharing. So, market secret sharing, which I was hoping you all would talk about, but I guess he didn't talk much about something else. Anyway, so Rachel is going to talk about that in the next talk. Okay, so this is the straight, and it's actually quite worrying. So, let me ask, why should we think about diversifying portfolio? Okay, I'm not giving you any investment advice, and don't listen to me for any investment advice. I give you bad advice. But I'm talking about diversifying assumptions. I'm talking about the diversifying assumptions. How do you do that? Here, we need new hardness assumptions or new problems. We've tried enough, like, to leverage those things, and we are kind of stuck. So, we need new assumptions, but there's hardness everywhere. We're looking for assumptions that are usable. We can actually do stuff from it. And not only that, we want assumptions that are either well studied or they can be well-reasoned in some sort of known ways of reasoning about hardness, which you guys. Of reasoning about hardness, which you guys are experts of. So, this is where I feel like there's a great potential for creative work. We are looking for an algorithmic approach to cryptography. And once we have new assumptions to play with, we have lots of tools, we are experts at taking the maximum amount of mileage from how to use those assumptions. We can do wonders once we know this assumption is usable. So, this is where cryptography. So, this is where cryptography has been hiring with complexity theorists' approach. And I feel like there is a lot of potential in symbiotic effort between the two communities. And let me answer now Yvonne's question. I don't know if it's really your question, but why should we care about another assumption? I mean, LW is great, gives rise to worship-based case connections. Post-point up secured has got a tremendous amount of selling. Why should we care about it at all? At all. There are two-part answers. One is my personal answer, and another answer is what I would tell a reviewer or like on conference paper or something, more diplomatically correct answer. I feel like creativity is the reason. There are connections that we need to, if we explore, there is a lot of potential. I'm really looking forward to all those things. Now, the thing with this is it may not pay off right immediately, but I'm sure that in I'm sure that in some form there will be rewards to all this effort, maybe in terms of bypassing barriers where we are stuck at, or maybe letting devising assumptions to do things efficiently, such as what you all talked about, like there's an efficient notion of secret sharing scheme and so on and so forth. So this is what makes it exciting. And secondly, there's actually just no reason. At least I don't understand any particular reason why we should just be fixated with playing with few tools. I mean, the whole point of cryptography are new is I'm sure. I mean the whole point of cryptography are new assumptions like assumptions and using that. So we kind of have to go at it from all sides. Okay, so this is my motivation, but also equally important motivation are actually in human history, it's not too out of the line to expect a few breakthroughs in cryptoalysis. Just 30 years ago, we had Shur's algorithm, which completely changed the landscape. Which completely changed the landscape of how we view cryptography. So, this is not kind of like unreasonable that we will have some rate-looking technologies to hedge against those. And finally, we are also just stuck in terms of techniques. For a long time, I and like a non-significant fraction of community, major fraction I would say, expected that we could construct Irofarm LWE. Like FHE had just come around and just reasonable to expect that something like LWE should be helpful for doing obfuscation. Helpful for doing obfuscation. We are actually quite struck there. But there are other assumptions which start you. So you need to get unstarted. Anyway, so this is just a salesman pitch on why we should care about new assumptions, but let me go to the subject matter. This work is about new assumption, but it's going to be about a code-based cryptographic description. Okay, so codes, error-correcting codes, as you know, have a long history of study, and we understand really their algorithmic tools. understand really their algorithm toolkits very well. And what this means is that typically for code-based assumptions, we have very good tools to like at least understand how secure or non-secure the assumptions are and that's why you feel comfortable working with them. This kind of is captured in what's called as the famous assumption of learning paradigm with noise and this assumption captures the hardness of decoding random linear codes. So just to remind people, I mean most of you would know this, but if you don't, then this is the assumption. Then this is the assumption. When you sample a random matrix A of dimension n plus m. So think of it as a really wide matrix, short and wide matrix. M is some polynomial in n. And you sample a secret S, multiply it with this matrix A. And I ask you, can you solve the secret S for me? This is all done over Def2. The answer is obviously yes. You can apply Gaussian elimination. You have redless equations. So you can solve for the sequence. Now let me change the game here. The sequence. Now, let me change the game here. I'm going to add a sparse error noise vector here. I'm going to corrupt some with epsilon probability of these equations independently. And I ask you to do the same thing. And this learning paradigm noise assumption says that this is actually hard, depending on what noise property you're going to add. We're going to talk about what are good regimes and what are not. But this is roughly the hardness of the problem. And yeah, I'm going to also take notation. Yeah, I'm going to also notationally refer to them sometimes in equation form, which is like Ai S, Ti S plus E I, which is just, I'm writing them in equation form. Anyway, so in cryptography, we don't care so much about search assumptions as much as distinguishing. So what I would require is that A as A plus V is a distinguishing A are uniformly chosen vector. Right? So that's the distinguishing variant of non-mirror A. What does this sign? None method. What does this sign mean? It means that no tea time attacker, let's say I want things to be secure against tea-time adversaries, no tea-time attacker can tell a card these two with a noticeable probability. What does noticeable mean? It means cryptographically significant and it means inverse polynomial. So no adversary should be able to gel apart these two with more than inverse polynomial. Okay, so that's LPF resumption. And in the past, just because LPN has been really well studied and like the coding algorithms we understand really well, the decoding algorithms, in the past, for cryptographic utility, people have proposed variants of LPN, and we will also be proposing variants of LPN. And what are these variants? These are essentially the same living adoption, except this A is going to be chosen from some special distribution. It may not have to be uniform, it can be from some special distribution. From some unit special distribution and it helps us in like utility, basically. And I mean, one variant that you might very well be aware is just sparse learning parity with noise, also known as random kxor constraints for expansion. So that's an example of this, and there are many other examples. So we will be actually building a variant which gives rise to a non-significant part of advanced cryptography, which we previously did not work on, any post-quantum assumption. Did not work on any post-quantum assumption barring algorithm. Okay, so that's the thing. Now, let's talk about how I kind of want to tell you about hardness generally because we'll be reasoning about our assumption and what's the security parameter. So, let's just try to understand how like how hard LPN is overall. Anyway, so there's a lot of work over uh for medic pattern noise coming from various communities. Um Various communities. Turns out there's a very simple attack that sort of almost up to like our understanding tells you how much running time it takes to solve LPN as a function of the error probability in those equations. So very, very simple DAC. So the typical DAC is actually exponential. The exponent depends on the error probability that you use. The exponent that you use, sorry, the exponent that it turns out, the tag. The experiment that it turns out, the tap, the best-known algorithms, they can be approximately figured out by this nice strategy. What's the strategy? It's like simply guess, let's say, n errorless equations. So you know that only epsilon fractions are corrupted. We're going to guess like n equations which are all errorless, and then just apply question and loop. I'm assuming polysamples throughout. Yeah, so this thing's uh change the picture changes a little bit up to lower order terms. Bit up to lower order terms when you have more samples, but I want to focus on only on samples. So this nice strategy works at least in telling me roughly what level of security to expect. Yeah, so this single iteration will not work, and you expect to repeat it many, many times, and this is the amount of time roughly you might have to repeat. So there are four important phase transitions in terms of errors, and that's because of application. That's because of applications. These are like these four-body transitions are when you use constant error probability, and when error probabil drops down to 1 by root n, that means roughly root n samples are corrupted out of n equations. Then there are aggressive regimes where, let's say, roughly out of n log squared n equations are corrupted and so on and so forth. So these are important conditions, and roughly this attack corresponds to exponential time here, to time here. Every equation is not represented in programming. Every equation is not represented by the one minus epsilon, so one minus epsilon to the yeah, so those many try roughly. Yeah, exactly. So the top one is just one minus epsilon to the n. Actually, this is roughly two times n times epsilon. Two times two to the power O tild of n times epsilon. No, I get it, but it's also like one minus epsilon to the, one over one minus epsilon to the, sorry. That's like exponential and same thing. I mean, it's the same thing, but one of them has submitted. Yeah, yeah, yeah, yeah. Yeah, sure. Okay, good. So, and at what point it's broken? When you work with a parameter like log and pyr, meaning that if I kind of log in equations in any equations, that's actually polynomial time broken. Why did I do so much? Why are these points important? Turns out that the phase transitions happen with respect to applications. This is typically what's called as the minigrip regime. Meaningless regime. You can build one-way functions from it, suit random generators, and like anything that the one-way function implies. Basic things, really. We don't know a lot of advanced stuff here. There are some interesting applications in Eval's HSS line, which I'm going to skip, but nothing as close to a full-blown public key encryption. 1 by root n is a very special point at which this classic work of Aleknovich, which designed a public key encryption when the L probability drops below 1 by root n. Below 1 by root n. And here you know a few more things. And so these two things summarize pretty much an exponential, sub-exponential security regime, whatever we can build for up here as of now. What happens when you drop down below log squared in the n? This is actually a very interesting regime. We have a lot of different interesting advanced formats. Not a lot, but like significantly more than these two. You can build collision-based hashing, you can build identity-based hashing. Hashing, you can build identity-based encryption, you can build like different kinds of mildly human secretives. So, this is a very interesting regime to study for applications. Unfortunately, the problem is that this is quasi-polybroken. So, now we want to ask, can I come up with a LPN variant where you can kind of have same things, potentially even more, but with some exponential time security or with inverse polynoids, not like log squared n by n. Not like log squared n by n. You want like 1 by n to be something, and something needs to be less than 1. Okay, so that's what we're going to do. So in order to understand why we are stuck at log squared n by n barrier, I want to focus on just one primitive and promise that's going to be the only primitive I'm going to tell you about. But it's an interesting one. So, collision resistant hash function. We're looking for functions which compress, so L bits to L prime bits. So they compress, that means they are collision. Bits, so they compress, that means there are collisions. But we want to design them in a way that for any polynomial time attacker, coming up with these collisions is hard. Okay? Now, why care about collision-distant hash functions? For you guys, turns out, it has been observed, and this is very heuristic, if you are able to build collision-distant hash function from some assumption, and let's say it also implies, let's say, PKE of the encryption, typically it happens that it implies a bunch of other things. Happens that it implies a bunch of other things. So it becomes interesting. And there are lots and lots of examples for this, like just that primitive, which typically, there's no formal connection here, but if you are able to build collision distribution hash function, then typically it will have more interesting consequences. So I know like some of you are trying to build PKE from TensorFlow PCA. I mean, I really hope you succeed. If you do, pick up. You succeed. If you do, take a moment and try to see if it also implies closure-resistant hashing. Should closure-resistant hashing be easier? I think harder. Okay, so if you attempt to end something like that, potentially it's interesting we aren't just picking. Maybe you should that the construction of collision resistance helping requires some structure. The provable construction requires some structure, and this structure also helps the other verification as well. Right, so compression. Compression. Yeah, exactly. So, yeah, I guess you've all told me to correct this, but yeah, so collision distant hash functions CRHF, I wrote CHRF from LPL, and this goes to very cool work of Winod and others in 2019. So you're going to see really why are we stuck with logs by NBA and error and how to construct it. Just going to help us solve it all. So this one is actually in. This one is actually inspired from lattice-based construction. I don't know if you have seen sys hash function, essentially similar. But the idea is that you're going to sample a random matrix A, again short and wide, so it's an LPN matrix. Then what you're going to do, the input domain to this hash is going to be a vector, which is an n-dimensional space, but it's going to be sparse. So it's going to be like n over log n sparse. Why this comes up, I'm going to tell you later. The hash computer. Value data. The hash computation is simple, like just a times x. So what's going on? I mean, the hashing is just this. I chose a sparse vector x and multiply with an a and I get some vector y and the hash is just exactly really simple. Now apparently, like why is it connected to LKN? It's not clear because there's no error. I mean, so we're going to see what's going on here. So first, let's identify why is it compressing because we need collisions, we need compression. So it turns out, So it turns out the number of t sparse inputs is mtc and number of outputs is 2 to the n because that's the total space where y lives in. And all we want is that those inputs to be more than number of outputs, then we expect collisions. And this turns out exactly when t is n over log n. If it's like more than, if it's more than n over log n, then this goes through. But can be like n to the 0.99. n to the 0.99. It's exactly at n to the n by log n where you expect polygons for this function. You can show this. Simple combinatorial logo. And that's really why this will turn out to be quite important to say that why we are stuck at not being able to base this on LPN with error greater than log squared n by n. Okay, so I will show you the reduction just because I think it's nice. Just because I think it's nice. In detection, I'm going to use this piling up lemma. I'm sure in Boolean functionalysis, this comes up almost routinely. So, what does this say? It studies these sums of random variables, x1 through xt, sum over f2. Each of these xi's are like Bernoulli variables, but they're still not probability. They're independent. Now, we want to understand how does this behave if I sum too many. Now, if I sum too many, then it's going to be close to uniform, right? Going to be close to uniform, right? But at what point is it like one by inverse poly close to uniform? Right? So this says that when I add like roughly log n by epsilon of them, if each of them are epsilon highest, then I will have inverse poly distinguisher between this and the uniform. But if I go above this, let's say omega into omega of this, then it's going to be less than any polynomial, and that's not cryptographically distinguishable. And that's not cryptographically distinguishable. Right. So, this we are going to use as a simple fact. I mean, I'll send this before. So, how do you build a show reduction? The idea is really simple. Let's say an adversary is able to find a collision of this function. So, what you can do, you can find two inputs x and x prime, both of them sparse, and then their hashes agree, which means that you have a vector v, which is n by log n sparse. Which is n by log n sparse such that a times will be 0. Right? Both of them are sparse, so therefore b is also sparse. This actually helps us distinguishing LPN samples from random, provided the error probabilities log squared n by n are below. So the idea is that if you have an LPN sample like this, you want to distinguish a, random, just multiply it vector b, a times b vanishes, and all you are left with is e and v. e is a Bernoulli. And V. V is a Bernoulli distributed random variable with probability log sparring and v is n by log n sparse. So filing of the says that this is going to be biased towards zero with the inverse file times. So that's that. Now why can't we hope to get better error probability here? Because we're guaranteed that these b's must be at least n over log n sparse, and so this error probability must be below log squared n by n. Below dog squared n by n, my filing number, right? So that's where we are stuck. So, what we want to do is basically unstuck this condition. Yeah. Also, I want to give you this principle which we might refer. Turns out this argument is fairly general. It applies for all LPN variants. There's nothing about random may be used. So, if I want to distinguish S A plus E for any A of your choice from random, all you need is to find like. All you need is to find sparse B such that AB is 0. Sparse and the error probability that you can hope to distinguish from is log error. This is known in the learning, like learning sparse parity is known to be harder, like using few examples is known to be hard, equivalent to learning parity. Like this is an old little learning theory result. I see. Yeah, I mean, this is not a result. This is just like a, yeah, I'm just saying it's, it just almost follows. Almost follows immediately from this. Right, so it applies to this. Anybody use this anyway. So, this is collision recent hashing, another connection to statistical DK that I mentioned. I'm not going to do that in super detail, but roughly there is this informal way of measuring expressivity of assumptions that have been gaining properly in cryptography, which is this complexity class of statistical zero knowledge. It's a structured complexity class of PM into Pro-AM. Class of PM intercept Pro-M. And turns out that assumptions that fall into this class end up being quite expressive. So take your favorite assumptions, LWTDH, value for maps. They're all inside, or like can be broken inside this class. I'm not going to go into particular just in the interest of time. But for LPN, which is an assumption we're talking about, even though, like, About. Even though, like, LPN with 15210 probably implies public encryption, it's not known to be this secret. This one, LPN with error probably 1 over 210. So, what's known? Again, due to Winode's cool work, log squared n by n error, if the probability error of error drops below that, this is the only regime we know inside a C. Right, so this is LPN is some sort of an except exception in terms of being in SDK. Being in SDK. So this is the only, I mean, probably has to be very small. So again, we want to ask: are there some exponential time secure variance in SDK? Just used to clarify what you mean, you're referring to a promise problem that has yes it says no instance. Yeah, it's a promise problem. Okay. Yeah, actually can I ask a little bit about that? I was a bit confused about it. So LPM is usually, I guess I think of it with a planted problem, but like when you're asking the COAM. Distinguishing. It's a distinguishing problem. One with random and other with like... The COAM means like you know some other problem, some statistical certificate that the short interactive proofs of non-satisfied data. Right, okay, good. So let me touch So let me cut straight to our assumption. We define this new assumption called then sparse LPN. And what it does is it mixes both standard LPN and sparse LPN. The mix is informal. Let me make this precise. So our assumption is we'll have an instance like this, A S A plus E and then A uniform. What's A? It's going to look like this. So we have a dense matrix, short and wide. It's exactly like dense RPN. Its dimension is going to be, let's say, n over Dimension is going to be, let's say, n over 2 through n. It's almost square, but it's not going to be full rank or dimension n. So it's n by 2 plus n. And n by 2 is not important. It could be constant times n. And then we have this sparse LPN matrix here. Sparse LPN, as you know. So there's a constant K, and each column vector is exactly K sparse. So think of them being chosen right. So, think of them being chosen randomly from this distribution n2 space. How many columns do you choose? It's the number of equations you can afford in a sparse LPN assumption. It needs to be less than n to the k by 2. I'm going to use like something dealing with delta, delta is some constant, n to the 1 plus k by 2 minus 1, 1 minus delta. Note that when delta is 0, this is n to the k by 2. When delta is 1, this is n. Right? So it's somewhere between n and n to the 2. It's somewhere between n n and n to the k over 2, and it's actually polynomial lesser than n to the k over 2. Okay, so this is our assumption. Error rate we are going to use is inverse poly. Okay, so just to give you a sense, the error rate we will work with is n to the 0.5 plus some constant that depends on how many quotations you give out and it also depends on k. Good. Everything clear? Is everything clear? Okay. Why did we choose T here, a dense matrix here, and why did we choose a sparse matrix here? Well, if there was no sparse matrix, it's the same as LPN assumption and we have to put point stuck here. Why did we choose a sparse matrix here? Why could we not just work with a sparse matrix instead of dense times sparse? This is an interesting reason. Turns out sparse LPM with that inverse pi-noid probability is broken. That inverse point noise probability is broken in file over time, which is what we're going to talk about. Okay, so I'm going to, I'm assuming all of you are familiar with sparse LPN. If that's not the case, then we're going to do like really a quick survey of it. So sparse LPN, these are just LPN equations where each coefficients are constant sparse. So you can think of them by sampling this bipartite graph. There are m vertices on the right. There are m vertices on the right, n vertices on the left. Each vertex on the right is connected to k vertices on the left. So that's how you construct the matrix A. And error, of course, is some porno. Now this has been studied in the context of CSP retradition for a very long time. And typically, what you're interested in, you guys are interested in, is being able to find a certificate that if I gave you a random V instead of something that looks like this, that it's actually uncertain. Is that it's actually unsatisfiable. So, in terms of what's known, this has been a central target for SS refutations for long years. At n to the k12, it's absolutely broken, even a spectral certificate. When the number of equations drops down polynomily in like less than n to the k12, the best known type refutation size is like sub-exponential. So, n to the delta degree SOS will. Delta degree SOS will not work with when a number of equations is below this number. Now, something that's kind of important in this and also in the distinguishing problem is existence of cycles because remember I told you that if you can find sparse vector in the kernel of A, that means that you can distinguish these samples. For such a graph, if you choose them randomly, there is a with high probability you will have cycles of size some polynomial to the delta, the delta comes. n to the delta, the delta comes from here. If you choose a worst case graph, you are always guaranteed that there will exist something of n to the delta log n incised cycles, the so-called hypergraph movebound, if I'm not mistaken. Prabhish, correct me if that's that. So you're always guaranteed that there will exist a cycle of n-to-degener logging size. But if you sample them randomly, good chance it'll be at least this much. Okay, so that's for sparse LPN. So that's for sparse LPN, but in cryptography, we're using it as a distinguishing problem. Here, as I said, error probability becomes really important for applications, also for security. Also, how you choose this graph. I mean, we care about very low probability error. We want that there should exist no short cycles with very high probability, like negligible or like less than only inverse polynomial. This is not known how to do how to sample graphs with like these kinds of expansion. So that's a great open question. I'm not going to. Greater perfection. I'm not going to talk about that. So, the Applebomb alone approach that generates with negligible scale probability. Great question. So, there are very exciting work of Applebomb and student, Aliran, right? And they had like such a construction. It does not give you this. So, it gives you a unique neighbor expansion, which is sufficient to do lower this cycle. Expansion, which is sufficient to rule out this cycle. And the thing is, it doesn't work all the way to the KO2. So there are some gaps there. So we probably need to improve on that paper. But yeah, this is only one paper I'm aware of, and it's a great paper. Anyway, so for cryptography, recently sparse LPM has turned out to be really amazing. It's useful for obfuscation and kind of plays a critical role here. But in the paper, we wrote it as a good PRG, but essentially it's sparse LPM. Also, as Rachel is going to talk about, there's this notion of homorphic secret sharing scheme, which I mean, the version we built currently sparse LPN seems to like, you don't know that from any other assumption. Okay. The reason why sparse LPN roughly plays an important role is this, that when you look at LPN, you're stuck at one by root n er probability for designing PKD schemes. Now, this famous work of Applebom for Harvard. Work of Applegomb, Rehart, and Winderson, which showed that you can, if you rely on sparse LPN, you can basically an error probability that's much, much higher. n to the minus delta, you can choose this constant, provided the number of samples exceed n to the like something closer to n to the power t. You can make this tiny, as tiny as you want. Now, naturally, this should be the reason why it should be more applicable for other things. So, we want to ask what are the phase transitions for application. Are the phase transitions for applications here? First pass alphin naturally 1 by 2 delta seams. This is what Apple Bumberson gives you for public encryption. Right, so this is what the picture looks like right now. Below 1 by delta, we don't know any public encryption, and that's a great open function. We don't know how to build it. That's where the interesting application I pull to be live out alike. At above, when I probably drop below 19 delta, again, I want to say that the Again, I want to say that the number of samples is this much. Then we can get PKE and stuff. But that's that, as I understand correctly, we don't have so-called SDK regime known yet. Like where you have expect collision resistance hash functions and other applications. Threshold is for every delta or this. So fixing M as number sample. M as number of samples, you get some delta. So now we want to ask: is there a collision-distant hash function regime? And if so, is it secure? By the way, the running times of a best known attacks is like just brute forcing over the cycle, which is of size enter the data. Both these cases. Improving this is also a great question. So looking ahead, it turns out that you can base collision resistance with inverse polynoise. Idea is really simple. Words polynoise. Idea is really simple actually. So this is your sparse matrix, the same recipe to design like polynomial signature, multiplied with a sparse x, and you get some y. Now, how sparse does this x need to be so that you have compression? Sorry. We want this. Now, notice that x is t sparse. Each column of a is k sparse. Y is going to be k sparse, y is going to be roughly k times t spars. So when you work this out, m choose t, which is the number of possible x's, m choose k over t, which is the number of possible y's, and these m choose t must be significantly greater than m choose k t. When you work this out, lo and behold, you get something, funky looking expression t to the k minus 1 n by k by m. I don't know if you have seen this before, but But it corresponds to something polynomial. t is some polynomial. So just to show you, if I do it n to the k over 2 1 minus mu samples, this comes out to be n to the k over 2 1 plus mu mu by k minus 1. If k is very large, then this is n to the half plus mu by 2. Right? This is not n. It's not like n by log n or anything like that. It's a polynomial. Or anything like that. It's a polynomial. And yeah, I did some calculations, and if the number of samples are, let's say, K over 4 with varying sparsities, you get some polynomial. Sorry, Aisha, I'm just having trouble following. So what's the significance of the fact that this is polynomial? So when you, this is collision-related, let's say this is what can it be based on? It can be based on sparse LPN with error probability 1 divided by. A probability one divided by this. I see one over t. Yeah, because that's the size of the cycle collision. Yeah. Does this make sense? I just want to make sure I'm not losing anyone. Please ask right away. I can quiz you. Okay. I'm assuming I'm making sense. Okay, so that's the point. That's the point. Like, now you can, whatever you can, we're hoping to do with dense LPN with probability log squared n by n, you can potentially work with an inverse polynomial, probably being slightly less than 1 by root n, right? I mean, whatever this number is. Okay, so that's that. So sorry, it's just like, let's read the off. Okay, so you're just saying, like, suppose that I'm just below the refutation threshold. I'm just below the refutation threshold slash uh like recovering from the refutation threshold. Then like luckily it turns out that t is less than one over root n. Oh sorry one over t is less than one over zero. One over t is less than one over root n, yeah. Okay. This is the equation that needs to be solved. I understand. I don't want to solve any equations. I just want to understand qualitatively why I'm happy about things. About things. So, like, like, okay, so MB, but like, I really recognize I love Spar Selfian, and I like know something very special happening players, so that's good. Okay, now you're saying that something good happened in T. Can we just recap? Yeah, so, okay, in Den Salpian, what happened was when I wanted to find collisions, at what level this function a times sparse x equals to y, when you admit collisions, uh you admit collisions when x's are n by log n sparse. n by log n sparse. And now this needs to be n to the 0.5 plus something sparse. And that something is less than 1. And it's not good that it's less than 1 or it is good that it's less than 1. It's good that it's less than 1 because if the coefficient... Yeah, if it's n by log n, then in the reduction, we want to base it on L3 and the error probability would be like reciprocal of that. And now the reciprocal of one by And now the reciprocal of one by this number is still the inverse polynomial. Which is bad. Which is good for security. Okay, but we want to be in, I thought we wanted to be in the SCK regime. So right, so SCK regime is the regime where you expect compression. So okay, the thing, okay, let me reduce the monitor. I guess this is the assumption. And in order to distinguish, in order to show the reduction, if someone finds a collision If someone finds a collision at a collision for A, you're able to find like a vector p sparse such that AB is 0, right? And this lets you distinguish this from random, right? Because this E times B V is biased. But what error probability should this be? This error probability is really roughly like 1 over t or like log n over t, right? Something like that. This is the maximum error probability you can distinguish from. You can distinguish from. And here, if this is n by log n sparse, then this is log squared n by n. And that's very bad aerotography. I see, I see, I see. So it's good. Like the fact that we're sparser. Yeah. The fact that we're much sparser is good. Yeah. The collisions are sparser, it's good. It's going to give you a good reduction to LPN. Thanks for asking. Okay, so we do that. Unfortunately, the issue is, this is what we expect the ST regime. The SCK regime. Unfortunately, the issue is that this is broken in polynomial time. And that's actually surprising because parse helpine is really, really popular in cryptography. But yeah, I don't know if this was known at all. So why is it broken? Oh, it's broken. Yeah, I'm going to talk about that. So it is really a very, I mean, the most underrated algorithm of all time, Russian elimination. So you're going to find like a set of small size, let's say. Small size, let's say little r, such that you have r plus one equations on those, it's a shrinking set, but there are many, many equations, like r plus one of them. And once you have that, then you can, it's a linear dependency system, like there is a linear dependent system, so you can find an order or size cycle. And if you work this out, when can you expect such a phenomena to happen? It matches exactly the number that I showed you. So, really, what I'm saying is big. Really what I'm saying is pick, the graph is chosen at random, pick any set of little r variables of your choice. Little r is something you'll work out, but pick any variables. Now count how many equations depend only on those little r variables. So this is the expected number of equations, and you all only want to set this to be more than r. And when you work this out, it comes out to be exactly the threshold for collisions, which means that you can anywhere find these cycles. You can anywhere find these cycles, it really is in polynomial time. Right? Is the L-garding clear? Okay, really straightforward. So just look at any set of R variables and try to find equations that depend on it. There's a good chance that there will be too many of them, and then you can find separate variables. So are we profosing over all set of R or R variables? No. Uh it will work probabilistically due to the high probability. No matter what set you're gonna choose, there's a high chance that there will be many equations depending on this. There will be many questions depending on this. Random RPM. Yeah, random set will work. So I want to leave this as a question. Does this work for worst-case matrices, or is it like a phenomenon that only pulls for spark random matrices? That's one. Wait, sorry. Wait, you mean, is there a worst-case matrix that will avoid this so that we can avoid the purchase system? Can you go back to the previous slide? I just want to get the hand up. People have to do that. Feelings meaning? Yeah, that one, that one, that one, that one. The one with the lighting start. Yeah, that one. Good. Great. So, again, there is a regime where you can get collisional system hashing and the problem is possible, and it's not a problem. Yeah, that's a high-level point. There is a regime where collisionation is possible. And can you repeat your question? Yeah, there is a way to construct a collision consistent, actually, from sparse side. To collection system hashing from sparse LPN. Yeah, of course, the knowledge invariants in this algorithm. Yeah, okay, good. How is this different from the state of affairs for LPN? You know, in LPN, again, there was a very narrow window where the problem is presumably hard. Yeah. And yet you can do the colours from a system. Yeah, but it's quasi-polymer broken. At least the reduction. So that resume is quasi-polymer broken. Here it is. I mean, just with parsimeters, it will be broken because of the attack highness. Be broken because of the attack I described. But what we will do is modify that assumption essentially by multiplying it with a dense matrix so that this particular attack, at least I don't know how to execute it anymore. And then that matrix potentially has a reduction of inverse polymer noise. I see, but okay, but that part we haven't seen yet. Yeah, that part. Okay, that would just want to make sure. Yeah. Regarding the next slide. And uh regarding the next slide, Ayush, how does this bound that you the right here compare to the expansion bound? It's more. Expansion happens with like n to the delta or something, and then this is like n to the point five plus delta by tube or something. Yeah. But if this attack works, then what? Good question. ABW uses an error probability that's one by n to the delta, and this this cycle is of size This cycle is of size and with the 0.5 plus delta. So when you multiply, the bias is like exponentially small. So the cycle has a particular size. It's not like for LF delta cycle, you can actually find the deficiency. For a sufficiently larger cycle, you can find the default. For sufficiently large polynomial size. So that's that. And yeah, so there are lots of questions. Can you show it for worst case? Can you show it for worst-case matrices? Can you improve the size of the cycle? And this is just some trivial attempt. Maybe you can shave off some other factor. So, I guess sparse, the larger question is sparse LPM with inverse polynoise rate. We are beginning to use cryptographers in our assumptions, but clearly it's like there's a gap in our understanding of what are secure regimes. Okay, so this is our assumption. Basically, we patch it up with a matrix. Basically, we patch it up with a matrix. It's almost rectangular. Why is it not squared? Something wrong happens. I might tell you later. But with rectangular, at least I do not know of, and we tried really hard for the various sort of combinatorial ideas I could come up with. At least we do not know how to attack this kind of thing when you have a dense matrix which multiplies, which is almost square, but not square. Okay, so this is the assumption, and naturally it gives rise to collision resistance almost trivially. Why? Because the compression properties do not depend on T at all. It's just a dense projection. If M is compressing, which we saw it is, then TMX is also compressing. So something that's applied on top, right? So so th that's really the idea. So collision is, I mean, happens almost by definition with Happens almost by definition with the like when once M admits coll collisions at some sparsity level and we saw that it starts admitting at this sparsity level which is some polynomial then TMX is also going to admit collisions and so you can base collision resistance on probability log n divided by the risk t is given to me I know what t is or not. So yeah you have you know that this is a sparse matrix and You know that this is a sparse matrix, and you know that for this sparse matrix, collision starts occurring once you have Ax for this particular t, which is the assumption, right? The assumption, right? I mean, is given to t or so the collision, yeah, so t needs to be some number which is bigger than root n and some and less than n. Big t. The matrix t. Ah, no, you're not given t. The product, t times. The product t times. You're only given t times l. If you're given t then it's it's regardless. Oh actually yeah that uh how obvious to me it's telling you. Yeah. Uh it's actually it's actually gone. I I can tell you offline. If if I give you T, it's it's it's broken. Yeah, anyway, so this you get collision resistance. But actually, what I want to say is that our assumption is like, unlike dense alpine, it's more expressive than collision resistance. You can do far more stuff. And roughly, like the idea is that collision resistance just, collision resistance hash function just barely compresses. But you can think about compressing D-factored information. So what I want is that. Information. So, what I want is that I want to choose this x to be sparse enough so that Ax, which is t times m times x, these dense m is sparse, reveals only like a fraction, a little fraction of information about x, information for m. One really sharp compression. And these are constant that you can choose. It turns out we can do something like this. Right, so you work out some parameters, and it turns out no matter what D you choose, you can But no matter what D you choose, you can come up with a level of sparsity polynomial such that you can compress d factors of pin information. This turns out to be crucial for what we design next, which I guess we did not know from any other assumption, which is post-quantum, barring LWE. But we did not even know it from LPN with log squared n by n n. So it's called lossy traveler functions. I'm not going to say what it is, but it implies a bunch of things, and that's the reason why. Implies a bunch of things, and that's the reason why we chose to work with designers. When you think about an assumption, you want to design something, which gives you lots, lots of different applications. So that's why we chose lossy regular functions. Now, this is for cryptographer fans here. I still do not know what all our assumption can do. I guess we still don't understand what it can do and what it can't. So it's a really interesting question if you can extend the set of applications for this. I know some. For this, I know some things that it cannot do, which I guess I'm happy to talk offline about. Okay, so I guess I will skip this part. I want to focus more on like cryptanalysis and questions. And that's where we will end this talk. So again, this is where I guess your expertise is highly valuable. Because typically, I would think of more combinatorial strategies, whereas Combinatorial strategies, whereas clearly there is something that has like analytical component, the sparse LP inside. So, yeah, can you use some of those techniques to attack our assumption? I tried combinatorial approaches and I can just report roughly what like rough findings. So, it turns out when you think about any new LPN variant, you want to rule out, broadly speaking, very informally, two kinds of strategies. One is this guessing attack. One is this guessing attack that I told you. So that depends only on the error probability, so that's not quite that interesting. The other one is how efficiently can you find sparse x so that Axis A. Right? So those are the two strategies that typically we care about. And this is true because most matrices in the coding theoretic language are not good at records or we do not know efficiently coding algorithms for those. So typically, these are the two attacks we want. These are the two attacks we want to rule out against. Now we can come up with artificial examples, like let's say the matrix is read-solomencore or something. You know that there is an efficient algorithm for decoding from large errors. Here, this like the strategy that I'm saying, you know, kind of reasoned quick analysis does not work. So, if your matrix has any algebraic component, yeah, I mean, all bets are off. Okay, so the point is that you want to safeguard against two strategies just to give some sort of reasoning to. To give some sort of reasoning to the assumption. And I'm going to talk mostly about this part, how efficiently you can find this. So, here's just a quick question. So, in both of that, you're actually solving the search problem. You're basically decoding, right? And here you just need to distinguish this type of LPN instances. We don't have search decision reductions. That is true. So, what's your question? So, my question is: aren't there any Aren't there any potential attacks that only break the decision or something? This one is going to break the decision. Okay, but this is like a simple decode. No, I mean you just find as part of the over-determined regime. Okay, good. So the idea is that in our case, what's going to ha happen is just because M is has a cycle of struct size entity delta, T Mx is also going to have strike, right? T does not really mess up the cycle structure. Mess up in the cycle structure. But the part is, which is the worrying part, is that you can work with cycles that are bigger than n to the delta. So the number that I said to you, you can work with the error probability something like this, which is really above n to the 0.5. So even if you find cycles which are bigger size, this assumption is known. Now, without this t, I know how to do that, and I told you about that. That and I told you about that. With this tea, I don't know what to do. Sorry. Okay, I'm not sure that I'm totally following, but another thing you could try to do is you could try to factor the matrix A, right? Great question. That's what I'm going to talk about next. Yeah. So if you can peel off this T, you're, again, you're done. Let's try peeling off T. So if it's invertible, then what's going to happen? Let's just try to develop understanding. If it's invertible, what I can do is. What I can do is I can try to find this D. Basically, I'm going to find this matrix Z, which is supposedly T inverse. I'm going to try to find it. So, Z times H must be equal to what? A sparse matrix. Right? And now, if I look at, I want to find the first row of Z, what do I do? I'm going to find it so that Z by A is the first row of M, and I know M is really, really sparse. M and I know m is really, really sparse. It's almost like zero. There's a constant k over n chance that each term is non-zero, so it's really, really sparse. So, like, if I pick n elements of the first row, k of them are going to be non-zero, rest of them are going to be zero. In other words, this is a thing where I can just guess, like when I set this equation, I can just guess which coordinates are non-zero in pulmonary time and just solve for each of the rows of C. Each of the rows of C. I think but I think in the not this might be called like in dictionary learning, this problem. I can't tell if it's exactly the same. Finite field? Finite field? In teacher. You're working more in teachers? Oh, Z2. Okay, yeah, finite field. Finite field. Finite field first. And things become messed up when you... Yeah. So, yeah, again, great question. So, I don't know if there are other kinds of more interesting things you can do. But, yeah, coming to your question. Isn't there like a Luclis and like version of Buclis that is based on this? No. They are previously all algebraic stuff. Yeah, no, my question is whether it was not at that. There aren't any. They use algebra really in a crucial way. I don't know. Here, because it's not algebraic. Because it is a McClilles style thing, right? Semi-McCillis, because McCillis works with a square matrix. I'm working with a rectangular one, but sure. All you tell it should be McClellis. I'll do tell it to be McKinley. Like, it's that's exactly what we're doing. So, what if things are not squared in this case? I mean, when you try to find out the inverse of the first block, you get identity times something dense and then matrix M. Now, this is going to be, like, every column here intersects with this dense part with a constant probability. So, what you end up getting, the first row is going to be like constant factor dense. And now, this Halpion-ish attack. And now, this LPN-ish attack, the non-zero guessing attack that I was going to tell you, takes exponential time. Yeah, so this is, I guess, I really don't know how to, and I can actually prove things secured when this is like really flat. I can also do this. So, it's like, yeah, this is a regime we don't understand really well. What's like if you can peel off T. Okay, so yeah, I mean, we can also work with almost square matrices. work with almost square matrices and minus n to the new terms. Okay, I won't take much of your time. Let me end up some open questions. So yeah, new attacks on our assumption. I mean it's interesting that we don't understand the part where sparse LPM becomes easy, right? So that's another great question. So you can investigate precise algorithmic thresholds for sparse LPM for the distinguishing problem. Also, our assumption looks like 10. Also, our assumption looks like tantalizingly close to both sparse alpine and dense alpine. I mean, is there a regime where we can expect some sort of reductions between sparse LPN from sparse alpine and dense LPN? Also, like, again, in Minot's classic work with worst-case average connections from LPN, that's the only known result which shows an LPN assumption, which is exponentially hard, is more secure than a worst-case assumption. Than a worst-case assumption, which is an LPN-type assumption, and that assumption is actually quasi-polybroken. So there's a huge gap in the quality of reduction compared to LWE where these reductions are high. So one of the issues there in those arguments is that the same issues that I pointed out with like getting collision resistance for LBN. Can we potentially use these insights to get better worst case average case deductions? Better worst-case average-case assumptions for LPN or LPN type assumptions. Okay. Next is like, yeah, sparse LPN. I've talked enough about it. And yeah, Rachel's talk, it's all about that. And finally, again, I want to emphasize how exciting this journey of new assumptions is. As I said, we really are in a shortage of usable assumptions. Of usable assumptions. So find out which, when you're working on some problem, if you find an assumption and you think it's cryptographically useful, yeah, just do a little bit of study on how it can be used cryptographically. Hopefully it's not a significant effort on your time. And yeah, if you find out that your assumption gives you PKE, and if you find it it's also useful for collision resistance, it's going to be cool. So take one step forward. And that's it. And that's it. Thanks for listening. Perfect. You had an open question about distinguishing sparse LPN. What's the question here? I mean, it is just this error probability. I mean, typically when you think about LPN, the live intuition is it's broken when the L probability is below log n by n and it's secure when sets like log squared n by n and above. Run sets like log squared and line and above. Here it's broken at some inverse poly property, and who knows that the inverse poly is like much better than what I did, right? I threshold like with air power, what I was worried about, do you think this can be used for public PHP encryption? Because in terms of Because I'm curious how it compares with RSA in terms of how efficiently it can be used. Ah, you're talking about a different game altogether. You're talking about concrete parameters? Yeah, like for 2 to the 128-bit security, what's the dimension? What's the size of the ciphertext and things like this? Yeah, maybe I guess so. I guess I'm curious. I mean, if actually it was solved tomorrow, then like would there be like a two-hour equipment would be. Would you go to this for our next public GMP36 scheme or the last basic target team or something else? I guess concrete security is a different beast and something that I don't understand very well, but there are people in the audience. There are standardized list variants. I mean, Lattice-based scriptures is generally more efficient, but you also have efficient versions that are like locked style and police style. They use MVPCs, there are all kinds of medium density channels. Medium density packages. There are all kinds of packet packages of this. But most one security is the main advantage of the RSA. RSA is terrible. RSA is the one RSA exponentiation is at the level of microseconds, 10 to the minus 3. And if you look at the LW or min L W sort of operation, it's at the level of milliseconds, 10 to the minus 6. Yeah, nobody. People don't use RSA. Like uh for key agreement they use like a gentle columns, key agreement, and then it's also fast search. The things that equipment are still clustered. So to my understanding. To my understanding, elliptic curves are like security-wise the best because uh n size n-bit size can be broken into the n by two time and that's the best of all the samples. Well thanks to the finish. I would start right at 10.30, so at least we got there at 10.30. No theory voting whatsoever. No theory of voting. Yes, definitely at least the same time. I used to do that. Oh, it doesn't sort of be a short time. Maybe it's a good thing, like. I would just like you know if S comes subscribe, then we move on to the information of this choice to make it work. This right here is a bit of a notion of advance of saying I was like if you like some inviting electricity. Yeah, exactly the right way. Yeah, then that then exactly this. Maybe talk about it. Yeah, I was going to talk to you about that as well. Same with Nazi, but something of it. Okay, so so you know we find okay, so the question is: can you for the same viscosity level for the matrix M, right? For M or T times? Not T times M. I think I... Okay, for the... So you have a matrix which is a product of a sparse matrix and a white kind of ultimate matrix, right? T is the right kind of ultimate matrix. M is a problem. Okay, so for the same sparse matrix. Okay, so for the same sparsity level for M, right, I could say, you know, like raccoon sparsalpian versus this variant of sparsalpia, right? I could look at both problems. This is how sparse helping, right? Is it because the matrix is the underlower sparse? No, no, I know, I know, I know. But this variant of the problem where the underlying matrix is K-sparse versus regular sparse is K-spars, right? So there are these two problems. Can you sort of show that the new problem is no? Problem is no easier to solve than the older problem for the same sparsity level. That's the very best question. I totally ask. Because easily, what is happening is writing up the secret now is S times T ball. It's an argument vector chosen from the subspace, right? Now, if you don't tell me anything about the subspace, then it's an argument vector. That's it. Because I cannot hopefully be able to be with the subspace. No, I know, I know, I know, I know. I know. So there are two extremes, right? I mean, one is you don't. Two extremes, right? I mean, one is you don't tell anything about the subspace, which is exactly as possible. Now you're telling something about the subspace. In the form of T times. Yeah. Right? Now, you're saying if you told me the whole subspace in its entirety, then you said. Yeah. Yeah, it's really easy. I can say. So then it becomes fast LPN where the secret is just from a known subspace. Right, but I think it's really not that far. So I guess what we are looking at, so this is what you're given. So, this is what you're given. Sure. Let's take the first column. And you don't really care because you know T. So you're given an. Oh, yeah. Right, that's the setting we are in. No, but this is not square. In your question, right? No, no, T is public. T is public, but it's rectangular. Ah, okay, I see what you mean. Fine, okay, sure. So all you want to do is just guess and choose each column, and then t times you can guess what are these. But like, can you have a singular randomness assumption? Can you have a similar randomness assumption like the TM is indistinguishable from here? Yeah, that's what we also claimed. But yeah, I didn't want to go into the talk about that. I see what you mean. So then that would imply security from Sparse and this plus that. Yeah, so this plus regular. But sort of isn't the sort of analogous assumption itself sort of like a sparse, not LPN, but sort of like a binary SVP is what we called it in the work with Evolve, right? I mean, so you take a matrix multiplied by I mean, so you take a matrix multiplied by a sparse vector, look at the output distribution, and you say, well, it's sort of random. It's going to be random if m is more dense. You can use like... Yes. Yes. So there's a bit of a trade-off. It's in case I'm right. Yeah, yeah, yeah. Then here you're not giving me T. Yeah. Let's think about it as KSUM or binary SVP or whatever. T will be given and T times. T will be given and t times secret sparse vector, we would claim that it's a random right. So, here t is not given. T is a bit okay. I was thinking about a case when this is not even a case. I am actually giving you T and M separately. So, T and M separately, and I draw from S. And I uh I draw from S times C times S. So the only problem is that the secret here is possible PLC is drawn from the subspace. This I would bet is good, provided the error probability is large enough. This I'm giving you strictly more information than here. Yeah, I think it's all the game is all the error probability. So I guess my question is this, right? So for the same sparsity of m and for the same error probability, is this version of the problem, can you show that it's at least as hard as uh At least as hard as it's possible. So we know the statement for LWE. Yeah, I don't think so. Yeah, but I also want to say that the error problem is quite crucial because. I know, I totally understand. I totally understand. Because you might get some, I mean, even this reduction works out. Even this error probability can be less than that. So you want to show that first LPN is as secure when you choose the secret from. Choose the secret from subspace in such space and this is a sequence that we know for L. But there you need to do something with the matrix. How do you prove that you do? Left over actually in the. You don't even need that, right? I mean, like, you know, you can you can sort of pull the subspace into the matrix and some that sort of works. And I bet it works for LPN as well. As far as the LPN is. As far as the LPN is too small. Yeah. I mean, what I'm going to say is, even if this reduction works out, it might. The seed actually works out, it might then sooner it will go to the broken regimes of Sparsa Pn. No, I no, I totally understand. But I'm saying, it's interesting, let's be in a non-broken regime of Sparsa PN. Does this like the problem easier by transfer subspace? Does it actually make it? My intuition about this first variance is that, say, if you look at collision resistance, that like the hard collisions to beat are the ones that just differ in one bit. Like if I flip. bit like that like if I flip one bit of X I see so if you can rule those out that should be like if you can say that there are no such collisions that they don't exist then I think that would be some other chances such collisions would be easy to find yourself right or one or constant or maybe even like one bit yeah that