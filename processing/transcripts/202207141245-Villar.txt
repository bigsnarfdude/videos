Thanks. So I'm going to talk about enforcing exact group equivariances in machine learning. And maybe I don't need to justify to this audience why do we want to enforce exact group equivariances. You talked about graph neural networks in many, many talks. That is one way of enforcing exact group equivariances, is equivariance with respect to the action of permutations by conjugation or Conjugation, or also you talked about deep sets. That is also functions that are invariant with respect to the action of permutations by multiplication. There are other actions of groups that one can care about. Translations, that's something that is enforced by using convolutional neural networks. The convolutions are translation equivalent. And also, depending on how you define your filters, there's the idea. There's the idea that you may be enforcing some rotation equivariance as well. And I may go into the details of that. Just so this is kind of like a generalization, if you want, of graph neural networks. It's other functions that are equivalent with respect to other group actions. And so in particular, we are going to consider invariant functions that are functions. So if you act in the input, then You act in the input, then the output doesn't change. So f of g times x is equal to f of x for all g and all x. So for instance, if I have this image classification problem, if I rotate the image, then the classification doesn't change. So that's an invariant function. And also want to think about equivariant functions. So if I act in the input by a group action, then the output is like acted on the output with the same group action. The output with the same group action. So, well, by the same group, the group action doesn't need to be the same, and you may be familiar with that. So, for instance, when you're learning an encoding of the nodes in a graph with a graph neural network, the group action in the input is conjugation, the group action in the output is multiplication. It's the same group acting in both spaces, but they act by different means. But in this example, I have a particle simulation and I have a particle simulation and I predict the particle simulation in the future. And if I rotate the particle simulation, if I rotate all the particles, then the predictions rotate in the same way. So my initial conditions are positions and momentums, and my predictions are positions and momentums after a certain time. So if I apply a rotation to my inputs, then the predictions rotate in the same way. So the question is, how can we parametrize the space of functions that satisfies that? Space of functions that satisfies that, and what can we analyze the mathematical properties of that space of functions? So, applications, images, graphs, you already talked about it, and then particle systems. So, I'm going to go into more detail into the particle systems. And there are many approaches. I would mention two approaches, the one based on irreducible representations and the one based on invariance. And they have some applications into cosmology and vegetation dynamics. Cosmology and vegetation dynamics that I will show at the end. So, why equivariance is an invariance in machine learning models? One reason is because they provide the right inductive bias. So, if you know that your physical system satisfies these properties, then if you want to learn how your physical system behaves, you need to enforce the constraints that you know that physics has. You can prove that it improves the learning. You can prove that the number of samples that you need to learn the Of samples that you need to learn the same function is going to be smaller for a fixed error if you enforce the symmetries, just because you are constraining the class of functions. And you can think about it in terms of out of distribution generalization as well. I'm going to describe that at the end. And it is a source of interesting mathematical questions. The question that I'm going to address today is how we can design algorithms that exploit the invariances and equivariances. The invariances and equivariances. But there are other mathematical questions that are very interesting. One is: how can we learn the invariances from the data? And how can we make a machine learning model that learns invariances from data and exploits them in like an end-to-end fashion? That's a very interesting question that I think has not been solved yet. And of course, the mathematical question of the generalization bounds and the RF distribution generalization balance that you can get from these models. Okay. From these models. Okay, so in this talk, I'm going to focus on the symmetries of physics, classical physics, because the laws of physics satisfy all these symmetries. So it makes sense to enforce them. And the symmetries are known. So particularly, we're going to talk about symmetries with respect to orthogonal group, the special orthogonal group, which are rotations, and the orthogonal group is rotations and reflections. The Euclidean group. The Euclidean group, which is translations as well. And then this is the Lorentz group, which encodes the groups of special relativity, sorry, the symmetries of special relativity, and the Poincar√© group and symmetric group as well. And also we are going to talk about units equivariance, which is equivariance with respect to the scalings that come from the units transformations. So, how can we implement that? And classical. And a classical theorem from Amy Neuther in 1915 says that to every differential symmetry generated by a local action, there corresponds a conservation law. So in particular, in this example that I had with this particle simulation, the fact that we have a space translation symmetry corresponds to the conservation of momentum. The fact that we have a symmetry with respect to Orthogonal group corresponds to the conservation of angular momentum. And the fact that we have the time transformation. And the fact that we have the time translation symmetry. So, if I observe the system now and has certain positions and velocities, and then I observe it 10 years from now, and it has the same positions and velocities, then the predictions don't depend on the time. That is in one-to-one correspondence with the conservation of energy. So, the groups that I'm going to discuss today, and I already said this. Today, and I already said this, but just so if you have any question, please ask. But you may have seen this before: the orthogonal group, the special orthogonal group, the Lorentz group, and maybe I can tell you how you define it. It's kind of like the orthogonal group, but with respect to a different inner product. The inner product corresponds to the Minkowski inner product that takes the time component, multiplies the time components, and then subtracts the inner product in this. And then subtracts the inner product in the space component. So, this, like, the first thing is like the time, and the other things are the space. And then the permutations, which you know, they act by conjugation or they act by multiplication. And all these things, you can just think of them like the Ortogona group and the Lorentz group. You can represent them as matrices acting by multiplications on your vectors. So, you may have seen how these symmetries are implemented in the Symmetries are implemented in the machine learning literature. The first way that people do this is by using data augmentation. So basically, if you know that if you do a perturbation, like if you rotate the input, then the output should rotate, then you can add that to your training set. And then you can hope that by adding that data to your training set, then it learns that behavior. And there's some statistics. Behavior. And there's some statistical results showing how data augmentation performs for learning these kinds of functions. Another way that one can think of doing this is by doing some loss function penalties. Since we know that many of these symmetries come from conservation laws, you can have a penalty function that penalizes not obeying this conservation law. And then that could be one way of enforcing the symmetry. But I'm going to focus on the architectural design. So basically, I'm going to restrict the learning to a class of functions that satisfy the symmetries. And bonus point, you get bonus points if you can implement all the functions that satisfy the symmetries. And this is related with what Chris was talking about with this WL hierarchy. Like in the case of permutations by conjugation, you cannot separate the orbits with a reasonable architecture. You cannot have a class of functions that like That expresses all the orbits of the action of permutations by conjugation on these n by n matrices. So you do some form of relaxation where like you identify some things together and then you make it, you refine it by being able to separate more orbits. But in this case, since these groups are like nicer, we're going to be able to find architectures that are universally. Architectures that universally approximate the class of invariant functions or equivariant functions. And I'm going to show you the difference between invariance and equivariance. Okay, so yeah, so let's see the classical approach, or not the classical, it's just like the first, I think this is kind of like the first approach of equivalent machine learning that I've seen. This is by Condor and Maron. They came up with this idea of They came up with this idea of, like, okay, if I want to implement a class of functions that satisfy these symmetries, what I can do is I can just take a feed-forward neural network, which is defined as the composition of linear layers and non-linear pointwise activation functions, and I'm going to say, I want to make this equivariant. So I'm going to restrict the linear layers to be equivariant, and I'm going to choose the activation function so that they don't break the equivariance. The issue with The issue with this idea is that there are not that many linear equivalent functions. So, if I'm thinking about rotations, like if you're going to, if you want to be like invariant with respect to rotations and you're a linear map, then the only thing that you can be is just a constant. So that doesn't give you the expressivity. It would be like very bad. You are not going to be able to separate the orbits in that way. So then what they did is, and then the other issue that this approach has. The other issue that this approach has is that how can you find out which ones are the compatible activation functions with the group action? So, in the case of graph neural networks and the action by permutation, then it's easy to see that if you apply whatever function pointwise, then that doesn't break the equivalence. But if you're working with rotations, you cannot, you have a vector, you rotate it, and then you apply a square to each of the entries, that's not equivalent anymore. So you have to do something smarter. Have to do something smarter. So, how do you do that? So, the way they solve this issue is by extending the action to tensors. So, basically, instead of considering that the entry is one vector, they consider that the entry is a tensor of the vector with itself many times, k times. That's the input. And then the output is another tensor. And then they extend the group action to this tensor space so that the group acts in each of the That the group acts in each of the components, like in each of orders of the tensor in the same way. And now they say, well, now we're going to find the linear equivalent functions on this tensor space. And then there's more than one. There's many functions in that case. So you can think of it, heuristically, you can think that a tensor of order k can express polynomials of degree k. So you're looking at what polynomials of degree. Polynomials of degree k that are equivariant you can express, like obviously there's a connection like that. So, how can you parameterize these linear equivariant functions in these tensors? So, the answer to that is to use group representations. And you may already be familiar with how this works, but I'm going to give you like a very short introduction of how this works. The idea is that, okay, you have a group action, the group action. You have a group action. The group action has the property that if you act like you have a group element, and this from this group element, you get a bijection from the space to the space. And when you compose this group action, this bijection, then the composition of these functions corresponds to the multiplication in the group. That's the definition. So, what is a representation of a group? So, what is a representation of a group? A representation is an action of the group onto GLB, which is the linear maps from a vector space to itself. And you may think, okay, this is a weird definition. But actually, this is a nice definition because it allows you to understand the group multiplication as matrix multiplication. Because the linear maps from a vector space to itself, you can think of them as linear transformations that you can express with a matrix. Transformations that you can express with a matrix if you choose a basis in your space. So, basically, a group representation allows you to understand the multiplication of group elements as matrix multiplication. And that's what you need. I mean, that's something very useful. So, then there is the notion of irreducible representations, which is basically the idea of like a representation is irreducible if you take, if no, like, If no, like if you cannot take like a subspace of B here, so that when you restrict the function, the action to that subspace, you still belong to that subspace. So basically, just like something, it's kind of like when you think about prime, like numbers, you can decompose it in primes, and there's like these prime objects that you cannot decompose any further. But the same things happens with these representations. There's this decomposition that allows you to have like this. This decomposition that allows you to have like these prime objects that you cannot decompose any further. And there are this very nice fundamental result that is called Schurz lemma that tells you that a map between two representations is either like zero or a constant times the identity. So basically, that's basically what it says. And you can see. And you can see that by looking at the definition of what a representation is, is that a map between representations is in a one-to-one correspondence with linear equivariant maps. It's like something like if you have seen the notion of this diagram commutes, if you have taken an algebra class, this is what it is. I didn't go into the details, but I can go into the details after if you want. But basically, the approach is the following. You have a group. You want to find all linear equivalent functions from this tensor space to this tensor space. You take the representation of that group in that tensor space, and then you decompose it in irreducibles. And then you can find a parametrization of the linear. Parametrization of the linear equivalent maps by just looking at the maps between these representations, which is given by the maps between the irreducibles, which are like easily parametrizable. And that's the idea of the papers by Russikondor and many of the later papers that do molecular dynamics and maybe some of the things that Hannes will talk about tomorrow. Hannes will talk about tomorrow. Implement how do you implement linear, how do you implement equivalent functions on molecules and things like that? This is basically what they do. So they're actually, and they have very nice software package that implement that with Nintelsoflow and in PyTorch. I know two, one, Fuchs is the first author and the other one is Thomas. And basically, one of the issues with this approach. The issue with this approach is that, given a broad representation, you need to find the decomposition in irreducibles, and that can be computationally complicated to do. It depends on the computation of something called the Klefs-Gordon coefficients, which are unknown for like some specific groups. But then if you go for SO3, this thing works very nicely. But then if you go to other groups, then even if. Like, even if you have SO7, then it may be more complicated to implement. And note to universal approximation. There is this paper by Dimon Maron in 2021 that showed that this approach universally approximate all SO3 equivalent functions if you allow the order of the tensors go as high as you want. It's kind of like a version of stone bias trust, which if you've seen, that's also something that they did for graph neural networks in the past. Something that they did for graph neural networks in the past, and they showed that in that way. I don't know exactly how it's the dependency on the order of the tensor, but yeah, it may be bad. On something else that I think is worth saying, that you don't really need to represent the tensor as an object, because if you have like a As an object, because if you have like a K tensor, how much memory does it occupy? It's ridiculous. So, what they do is they see that these tensors are representable in terms of the spherical harmonics, and then these papers implemented in terms of the spherical harmonics. So it's easier. I mean, it's like tractable. And then the order of the tensor corresponds to the degree of the spherical harmonics. Degree of the spherical harmonics that they use. Okay, so I'm going to talk about the approach that we've been working on with my postdoc and other people, Ben Bloom Smith, who is the invariant theorist in our group. So basically, the idea is to use a different approach to parametrize the invariant and equivalent functions. First, there is a classical theorem which is called Classical theorem, which is called the first fundamental theorem of evaluation functions. And you can see it in, it's very old. You can see it in a book by Baile in 1946, but there's like probably older versions of this. And basically what they show is that for OD, for instance, or in for simple G groups, you can do this. You can find, you can show that if you have a function that takes n vectors in Rd and outputs n vectors in Rd and outputs a scalar, a real number, and that function is all d invariant if and only if you can write it as the inner product of the input vectors. So basically, so I have n vectors in Rd, and then if my function is rotation OD invariant, then when I compute the scalar products, Compute the scalar products and I apply a rotation to the vectors, the scalar products don't change. So that will show one direction, like every function that is a function of the inner product is invariant. And in order to prove the other direction, what you can see is that if you have the matrix of the inner products, then you can compute the Cholesky decomposition of that matrix. And then you can recover the vectors up to O D. And that's, you recover the vectors up to O D. And that's you recover the vectors up to O D. And therefore, you can, like, if you can recover the vectors, then every function can be written in terms of the inner products. So that's kind of like the way that an applied mathematician would prove this theorem. And an invariant theorist will prove it in a different way. And a physicist will say that this is obvious. I have a collaborator in this paper who is a physicist and said, like, this is obvious because you can write all these scalars in Einstein summation notation. The scalars in Einstein summation notation, whatever that means. And so, this is for OD. And for SOD, you have a similar characterization, but it's a little bit more complicated because like you need to take into consideration the determinants of the D by D submatrices of your N by D collection of vectors. So, the characterization would look like this: is the inner products and then the D by D submatrices. The D by D submatrices that you can have. And for the Lorentz group, it's the same as in OD, but you replace the inner products by the Minkowski inner product. That's what I told you. You multiply in the time coordinate, and then you compute the minus the inner product in the space coordinate. So that is a characterization of all invariant functions. So in order, so one question that you may ask is like, I had n vector. Like, I had n vectors in Rd, so I had n times d features, and now I'm going to compute all the n square inner products. So I go from n times d features to n square. Is that like scalable? And okay, I don't know if it's scalable, but you don't need all the inner products because you can reconstruct the same thing. You can reconstruct the vectors up to OD by from a subset of order of n times d plus one scalars, and that's in connection with the. Scalars, and that's in connection with the literature in like low-rank matrix completion or rigidity of gram matrices, if that rings the bell to you. So yeah, so this also can be combined with like translations and permutations in a simple way. So if you want your function to be apart from like being OD invariant or OD. Variant or OD equivariant. Sorry, I'm missing some slides. Well, oh, yeah, this one. Sorry. Okay, so I'm going to describe how the OD equivalent vector functions look like. Yeah, I cannot go to translations if I don't do the OD first. So we proved in this paper. We proved in this paper that if we have a function that takes n vectors in Rd and outputs a vector in Rd, this function is OD equivariant if and only if we can write it as a linear combination of the input vectors with coefficients that are OD invariant scalar functions as before. So functions of the inner products. And the way the way I, so what happened was. The way I, so what happened was like, I was like, I can prove this. It's a very easy thing to do because if I have a node equivariant function, then the output needs to be in the span of the inputs. Because if it's not in the span of the inputs, you can construct a rotation that fixes everything else and fixes all your inputs and flips your output, which is not, right? Like, if your output is not in the span of the inputs, just construct a rotation. Span of the inputs, just construct a rotation that fixes your inputs and moves the output. And so that's not a possible option. So that's the way I thought about it. But then I was like, okay, but this should be something that is known in the invariant theory literature. There must be a mathematical way to prove this other than just like a geometric intuition. So I'm going to tell you how you can see that in a couple of slides. Slides. But just to tell you, this suggests an approach, which is a universal approach to write equivariant functions with respect to the ordonal group that basically take the inputs, compute the scalar products, and learn these coefficient functions f1 through fn, which are functions of the scalar products, and then just do the linear combination to produce the output. And that parametrization is very easy. Is very easy and is much simpler than using the Klefschordon coefficients. And you don't have to go to like high-degree tensors because it kind of like, since you have the basis of the invariants, which are just the inner products, then an MLP would give you a parameterization of all equivalent functions with respect to the OD action. But yeah, I didn't write the stone-bias theorem for this, so that's. For this, so that's that would tell you where the rates are, but the rates cannot be worse than in the other case, I believe. Okay, so if you look at SOD equivalent functions, it's not as easy because you don't know that all SOD equivalent functions lie in the span of the input. The output lies in the span of the input because you have like, like, you know, this like thing, like the cross product of two values. The cross product of two vectors is SOD equivariant, but not OD equivariant. So then you're going to have extra terms that come from that, which are these terms that come from the fact that you can construct, you can compute the determinant of the D by D matrices, and that will give you something that is S. SOD invariant, and then you can take D minus one set of vectors and compute the cross, like generalized cross product, and that would give you a vector that can be SOB equivalent. But other than that, it's not too bad. So, this is where I show you that there exists a general theory that allows you to go from invariance. To go from invariance to equivariance, and it's kind of like shows why this simple proof that I had for the OD case, OD equivalent case works. So the idea is the following. So we say that you have the homomorphisms from a vector space B to W, then you can see that this space is actually in a correspondence, in like a one-to-one correspondence. Correspondence in like a one-to-one correspondence with the homomorphisms of B times the dual of W to R, and you can also see that the homomorphisms that are bilinear between just the Cartesian product of B with W star to R. So this tells you that there is some kind of connection between the invariant functions on arbitrary copies of the space to the equivalent functions, right? Because there's like this duality that goes from one space. Duality that goes from one space to the other one. And I'm going to show it how it works for a specific example, which is the OD example. So the idea is the following. Okay, say that we have a function f that takes n vectors in a vector space, maybe I should have said Rd, and outputs a vector. And this is an equivalent polynomial. So now I'm going to fix a vector in V that I'm going to call it V star because it kind of comes from the dual, but I'm just going to. kind of comes from the dual but i'm just going to say that it's a vector in b okay so now i'm going to consider another function that goes from b n plus one to r and this function what it does is just it takes these uh n plus one vectors the n vectors from the original space plus the new vector that i added which is fixed and outputs the multiplication of this invariant function sorry equivalent function that i had before uh in a product with Had before in a product with the star. This function is going to be invariant just by construction. And so if this is an invariant function just by construction, then it's a function on the inner products of B I transpose Bj, right? Because of what I proved. It's a function of the inner products. It's an invariant function, so it's a function of the inner products. So what inner products do I have? I have the inner products of the previous vectors and the inner. Of the previous vectors and the inner products of the previous vector times my new vector, right? So, this function over here is a polynomial, it's an homogeneous polynomial of degree one on B star. So, then basically what then what happens is like this function can be written in terms of these objects. It's going to be an invariant function, which is going to be a polynomial in this times that guy, those guys. guys the inner products that and and then then since this holds for all these star then i can just cancel out these v stars yeah and then that gives me the form that i had in the theorem so basically there's like an uh like if you if you know your algebra you can work it out without doing the geometric uh interpretation of the of the space Interpretation of the space. Okay, so moving on. Okay, we know how to parametrize OD equivariant functions. Then you can also make it translation equivariant. That is easier. Maybe you can just consider the differences between, say, the vectors and the center of mass, and then add the center of mass at the end. The center of mass at the end, something like that. And also, this can be extended to permutation invariance and also OD equivariant. And the way that we prove that basically a function is OD equivariant and permutation invariant, if and only if you can write it as basically kind of like we had the same, a linear combination of the input of the input vectors, but now. The input vectors, but now the functions that we have, the coefficient functions, have the property that are all the same. Before, it could be a different function per input vector. And it's the same function, and it's a function, it's a function of the thing that is multiplying, and then everything else as a set. So this can be implemented as message passing, right? So And then there is an extension of this. But well, we did this for the Orthogonal group, but then we realized that actually this can be done with more generality for any groups by understanding like how do you generate the invariance for the group of invariant polynomials, basically. How can you find the generators for the group of invariant polynomials? So if you have a generator for the So, if you have a generator for the group of invariant polynomials, then you can use that to parametrize the invariant functions. And then you can use the trick that I showed you before to go from invariant functions to equivariant functions. So, the question is, how is the group of invariants? And is this like a silver bullet, I guess? Can we use this for graph neural networks? So, basically, the idea is the following: the group, the algebra of invariant polynomials, is generated. Invariant polynomials is generated by objects called primary invariance and secondary invariance, which are polynomials. And then you see that every invariant polynomial can be written as a polynomial combination of the primary invariance times the secondary invariance. And that's something that you can give you, it gives you a parameterization of all invariant polynomials. And so in the case of OD, the primary invariant. The primary invariants were the scalar products, and the secondary invariants are the uh just their trivial. So it's just that. So everything can be written as a polynomial of as a polynomial of the scalar products. So basically that's it. Like this is like a general case of the thing that we just show. So I asked him, okay, can we do this for graph neural networks? Graph neural networks. And he was like, Yeah, yeah, I can give you the primary invariance of the secondary invariance for this. And he came to me with like a step that was like, well, first you need to know every, all the class, all the isomorphism types of graphs. And so, and then you construct, you construct your, your, your, so the primary invariance were something simple, I don't remember what it was, but then the secondary invariance were like one polynomial per isomer. One polynomial per isomorphism class, or something like that. And then he was like, well, actually, if you do this trick, then you can get something like n to the k, where k was like a huge thing. So anyway, so what this tells you is like for simple groups, then you may be able to find a parametrization of this form that is computationally tractable, but in some cases, this parametrization would not be computationally tractable. It just turns out that for the groups that Just turns out that for the groups that one cares about in physics, this worked out. So I have some numerical example, which is basically a toy showing like you have this double pendulum with springs. And so this double pendulum with springs has some parameters like the mass and the lengths of the pendulum, the lengths of the spring, and then the constant of the spring. And then you, but you don't know that, you don't know what they are. That you don't know what they are, and there are the data is the positions and velocities at certain times, or maybe the position and momentums of center times. And then, if you see the kinetic energy plus potential energy gives you the Hamiltonian, which is a conserved quantity. And if you look at how the energy is expressed here, all these terms are, you can write all these things in terms of inner products, right? Right? So, yeah, so it shows up in physics, like the inner product showed up in physics. And something that we think is like, okay, this double pendulum is not equivariant with respect to O3, because there's the gravity. So it's equivariant with respect to like rotations in the plane, but it's not equivalent with respect to rotations in like the vertical direction. But if you add the gravity as an input to your learning problem, then To your learning problem, then it is because just yeah, because if you rotate the pendulum and you rotate the gravity vector, then nothing changes. So we learned this as an O3 equivalent function, having the gravity as an input. And then we did some like neural dollar, and we did something like a Hamiltonian neural network because we know that the system is Hamiltonian. So instead of integrating any differential equation, Of integrating any differential equation, you can just learn a Hamiltonian and integrate that Hamiltonian, and that will give you another symmetry of your problem. And so we can show that some numerical examples were like the neural ODE doesn't perform as well as the Hamiltonian, which is expected. And then the groups that you use, like here you use O3, but then you use in the original paper by Fincy and Andrew Wilson and Max Welling, where we took the code and we took this example of the double pen. Code, and we took this example of the double pendulum. They didn't use O3, they used O2, SO2, D2, D6. So the smaller the group is, the fewer symmetries that you're imposing, and the worse it performs. So that's also like an indication that imposing symmetries in this problem makes sense. So another symmetry that we may want to implement in this case of this adult penalty. In this case of this Abel pendulum, is symmetry with respect to units because this Hamiltonian has units of energy, and energy you can write it as kilogram meter square divided by second square. So if I do a transformation and put everything that has units of kilograms in units of pounds, then the learning should be able to be equivalent with respect to the Be equivariant with respect to the transformation. And so we have this. So basically, how do you implement this symmetry with respect to this units transformation? So first of all, we see this units equivalence as the action of a group. The group has to do with, it's like the number of units. The number of units. So it would be like it would be like R plus, so non-zero entries to the K, where K is the number of units and the group is multiplicative group, right? And so the way you can implement the symmetries with respect to this group action is the following. There's a classical theorem. Theorem in dimensional analysis that is called the Buckingham-Pi theorem that tells you that if you have your inputs, the inputs have units, you can construct a transformation to dimensionless objects. So for instance, the first mass had units of kilograms, the second mass have units of kilograms, and then M1 divided by M2 is dimensionless. Two is dimensionless. You can construct a basis of dimensionless features, and then you can learn in the space of the dimensionless features, and then the output would be you learn something that is dimensionless, and then you transform it into the right dimensions using just a fixed scaling. And so you can write like finding these dimensionless features, you can think of it as like the multiplication of your input features with some. Of your input features with some exponents so that the units cancel out. And then basically, it looks like this: it looks like a product of your input features with some exponents, and the sum of the exponents need to be like, there's like a combination of the exponents that need to be zero. And you can write that down. And yeah, and it's basically just solving a system of linear equations over the Of linear equations over the integers. Like the system of linear equations is the combination of the units of each object, linear combination equal to zero when you multiply them. Anyway. So that gives you like a way to implement another symmetry, which is the symmetry with respect to scalings or units. And what we did is And what we did is we took this double pendulum example again and we showed how well this units equivariance performed. And we found out that actually it didn't perform as well as the one that didn't implement the units equivariance. And the reason why we think that that didn't work out that well is because when you construct these dimensionless features, you have to divide by the features. By the features. And so sometimes, if some of the features are close to zero, then, like, for instance, the momentum can be close to zero if the particle at some point has velocity like zero, like if it switches direction. And so then it makes it unstable. So you have to be careful and how do you construct these dimensionless features. But what we see is that it performs well in like out-of-distribution samples. So for instance, Samples. So, for instance, I can train my model in masses that are like uniform between one and two, and then evaluate my model in masses that are uniform between like one and five. Or maybe I can think about like uniform between zero and one and the other, the test could be like uniform between zero and a thousand. And then, when I when I construct the features in the feature space, Features in the feature space, the distribution becomes much closer. So then there may be something to say about out of distribution generalization and equivariance, just because you can look at the orbit of the group action in your space and it covers much more. And this is not compact, so you are not going to be able to sample from like any possible measure. Well, definitely not a uniform measuring. Definitely not a uniform measure in the entire space because it doesn't exist, but just like it gives you the option to learn out of distribution. And I think that's something that could be useful in practice. And so we have a couple of applications. One application is to vegetation dynamics in collaboration with Bianca Donitrascu, who is in computational biology. And basically, we And basically, we chose this example, which starts with like you have a pattern of soil, and this is like in an area setup, and you want to know the evolution of this vegetation at a certain with time according to a lot of parameters. And the reason why we decided that this was an example where we could use units equivariance is because all these parameters that describe Parameters that describe the problem have a lot of units. So it made sense to do a learning that was unit equivariant. So, for instance, the rainfall or the infiltration rate of the soil, etc. And so, there is a dynamical system, which is a PDE, which is called the Ritkirk model, that tells you how, depending on these parameters, what is the evolution of this system, of this. This system of this vegetation. And we basically, what we did was we did some simple regression problem. And then we compared a baseline with implementing no units equivalence versus the one that implements the units of covariance. And we obtained that it performed better. And I can show you like the dimensionless features that we obtain, these guys are just like a product of this. A product of these things so that the units cancel out, basically. That's how they look like. And so you reduce the dimension of the space and then you do this regression. And so something that I think would be interesting is to see if we can learn the dynamics from data in a way that is equivariant. So it would be like something like an equivariant ODE. Equivariant ODE on images with like, or yeah, PDE on images. And then other equivariances that you can implement in this model are like rotation equivariance, translation equivariance, and units equivariance. Yeah. And another application that this is in collaboration with Kay Story Fisher and David Hall, who are astrophysicists at NYU. At NYU is about predicting galaxy properties from dark matter simulations, which I don't know much about, honestly, I must say, but that's why I have collaborators who actually know their stuff. So basically, the idea is that they have a dark matter simulation and they want to predict like the mass of the galaxies there. And so this is a problem that is equivariant in nature. Is a problem that is equivariant in nature because these cosmology simulations are equivariant. So, if you apply a rotation, then well, the galaxy, like whatever, if you apply a rotation to the dark matter simulation, then the galaxy inside it should rotate in the same way. So, and the properties that they want to predict are actually like invariant properties, like the mass of the galaxy. And so, they do the implementation that they do for this learning process. That they do for this learning problem is based on scalars that are basically constructed from vectors and tensor of this dark matter simulation. So they construct the scalar features and then they learn the properties as a function of the scalars and they show that they beat the baseline in that problem. So, how much time do I have left? Nothing. I think I would be more than happy if you cut it his time. Okay, so I have one more thing that yeah. Okay, so I have one more thing that I think is quite interesting, which is not my work, it's actually a paper by El Seri on stadium. A paper by El Sedi and SEDI in ICML 2021 that that's a very nice way of characterizing how much do you gain by imposing symmetries. And I like it because it's very like a very sleek computation and it's just like a very easy to understand kind of thing. So the idea is the following. So say that G is a compact group acting, say, in Rd, and then you have a measure where you sample your data. Where you sample your data, and this measure is G-invariant, right? So, your data is sampled from a G-invariant distribution. So, then you have a regression problem, which is you have some X's sampled from this distribution, mu, and then you have some y's that are f star of some of the xi. So, there is some ground truth target invariant function. Target invariant function which is invariant, and then maybe it has some noise. So, then the question: first, you define the risk, as you know, is the expected value on the distribution of your data of your predicted f of x minus the truth y. You take the norm square of that. And so you're asking for what is the generalization. Asking for what is the generalization gap, you define the generalization gap between f and f bar to be the difference between the risk of f minus the risk of f bar. And so if f is any function, and f bar is the projection of the function onto the space of invariant functions. So you take your function that you do a regression, and then you take the output of your regression and you project it onto the space of invariant functions with respect to the Variant functions with respect to the G action. So then this risk is always greater or equal than zero and is equal to the norm of the projection of the function onto the orthogonal complement of the invariant space. So, how much do you gain exactly by projecting onto the space of invariant functions is what Functions is whatever you drop, you gain in terms of generalization gap. So, and this, and so something interesting is that this is the norm of the function with respect to the measure of where the data is sampled from. So, I like that very much. And the key property that they use to prove this result is that you can consider like two kinds of projection. Consider like two kinds of projection, like the projection onto the space of invariant functions. The projection that comes from the measure, which is like the closest function that is invariant, that minimizes the distance with respect to the measure. So the L2 distance with respect to mu, which is this one. So the projection of F would be the minimizer of the norm of F minus H, where H is invariant and the norm is with respect to the The measure where you sample the data from, and you can also define the projection as one would do if you are an algebraist. Like you would define the projection, like this is called the Reynolds operator, which is the averaging over the group, like of f of g times x dg. Right? So basically, if you want to find the projection of the function. The projection of the function onto the like I don't know permutation invariant functions, then you will sum over all permutations of f of the permutation applied to your input. That's the Reynolds projection operator. So in this case, the reason why this math turned out so beautiful is because in this case, both projections coincide. They're the same projection. But that is not true in general. So, all the assumptions that are here are kind of needed. Not all, but I mean, you need that the measure is basically what you need is that this operator, the Reynolds operator is self-adjoint with respect to the inner product coming from this L2. And yeah, so for instance, I try to use this for the units equivariant, which is non-configuration. Equivariant which is non-compact, and actually, it's not true. Uh, so I was kind of bummed, uh, but it's not true because, not just because the group is non-compact, because it's true if you use like scalings that are complex. But the reason why it's not true is because there's no, like when you apply these projections here, you get that everything, like any polynomial, like you get that. Polynomial, like you get that monomials need to be different monomials need to be orthogonal, and there's no measure that makes like x and xq orthogonal. Right, because like the measure would be no L2 measure that makes x and xq orthogonal, because it would be like you need that the integral of x to the 4 d mu is equal to zero, which is not going to happen. So, yeah. Going to happen. So, yeah, so that didn't work out. But something that I think would be interesting is to understand how much you can extend, like, what are the limits of this theorem for what kind of groups this holds or like what kind of actions or whatever. So, finally, so sorry, you have to give your talk. I'm going to say that, okay, the goal was to enforce exact symmetries in machine learning because. In machine learning, because you can have better sample complexity, and I can show you like LSEDI and SEDI show you that you gain something by imposing these symmetries, at least in some cases. And well, I didn't talk about GNNs, so that's, but you know that GNN, and like you can characterize the expressive power of GNNs via the graph isomorphism problem, which is what Chris did. Um, like Chris did, um, and um, and also, well, the symmetries in classical physics. When I show you the simple characterization of all the covariant functions with respect to these symmetries that are given by simple leak groups, and I extended it to two units equivalence, and there's some open problems that I told you maybe during the talk. And these are my papers. These are my papers. And so, for instance, I will acknowledge my collaborators. And I like that these collaborations are very interdisciplinary. So, David Hogg, which is in both papers, is an astrophysicist. And then Kate is also an astrophysicist. Wei Chi is a PhD student in statistics at NYU and did all the implementations for all these things, or most of these things. Or most of these things. So, yeah. And Ben is an invariant theorist, and then Bianca is a computational biologist. So, yeah, that's it. Thanks.