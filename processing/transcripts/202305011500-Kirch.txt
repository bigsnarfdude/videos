Because in Germany still cold. So, this is joint work with now. I don't know what, how do you do this in Apple? I have no idea with Apple. So, this is joint work with Philip Klein, who this was actually part of his PhD thesis, and he's now a postdoc. And Marco Meyer, who was a postdoc with me on this and has now a permanent. On this, and has now a permanent position in Hanover. And you know, with these conferences, you always have some person there who doesn't really take the title too seriously. So basically, I decided to just ignore those two words and go for statistical challenges for complex images. But it's not brain images, it's actually an application from engineering. Yeah, and this is a 2D slice of what I'm talking about. Of what I'm talking about, we have this big picture here. And this is actually an image of concrete in a scanner. Actually, it's an actual picture, except for this crack here, which has been artificially added for this example. And what we want to do in this project is detect those potentially dangerous anomalies or actually, in the long run, also understand. In the long run, also understand better. This is what the engineers want to do. They want to understand better how these cracks develop because then they can use that, of course, in building stuff. And apparently, right now, what they can do is they can watch it on the surface, but they would like to actually be able to look inside the material. Also, this is part of a bigger project with both the Fraunhofer Institute and the University in Kaiserslauten, and also the University of And also the University of Ulm. So, this is the type of anomalies we want to detect. But as you can see here, concrete is a heterogeneous material. So, there is not only salmon and water, that would give rise to stationary kind of picture, but there's also other things like various aggregates, which let's just call them natural anomalies, like this could be air or it could be cruel. Air, or it could be cruvel, and could be other things as well. And of course, you don't want to detect these things, you want to detect the fissure. And those, they need to be discarded. Now, you may want to say there are probably machine learning algorithms around to track the fissure. And that's true. As a matter of fact, this is what the Kaiser Slauter and Group did these past three years to find out what are methods that are around there and how to. Are around there and how to adapt them to this situation. But they already knew before they started it, and this turned out to be true, that this is going to be computationally too difficult for the kind of or too slow for the kind of pictures that they have in mind. And what they have in mind is this. So actually, the motivation for the grant proposal at that time was this Gulliva project from the Fraunhofer Institute in Kaiserslauten. In Kaiserslauten. And at that time, they were kind of hoping that they were able to build it. By now, they are building it, and it's supposed to be operational this summer. So what it is, it's a CT system, but with a significantly stronger X-ray than medical equipment. So this doesn't say anything to me, but I'm pretty sure it does say something to some of you. So this measures some nine mega electron volts. And basically what it can do, it can screen reinforce. Do it can screen reinforced concrete components with a diameter of like 30 centimeters and the length of up to six meters. And the goal, and this is, I mean, of course, they want to use this for other people with other goals as well, but their original goal was to understand the crack development during some four-point bending test. What it generates in each experiment is something between 120 gigabytes and two. 120 gigabyte and two terabyte of image data. So, really, you, I mean, you really need to know what you're doing. And basically, what you need is you need to have a local area where you adapt your machine learning algorithms to. And you need to have a starting point. Because if you start your machine learning algorithm, for example, at this point, it will basically do the circle here. So, this is what our goal was to find. What our goal was to find some simple methodology, local methodology to detect a pixel that contains a fissure at least with a high probability so that they can use it as a starting point for their algorithms. And let's first start into modeling. So basically, so the theory is very general. It's for p-dimensional images. The pictures I'm showing you is two-dimensional and Is two-dimensional and of course later it should be three-dimensional. So basically, we have this t times t random field, let's say, and we identify the voxels with grid points like this. And then what this means is just this grid point, basically, notation. And then what we assume is that what we observe here is some signal term plus the noise. Signal term plus the noise term, where of course the noise needs to be centered. We also need to have an existing variance and some moments and the number of moments that we need depend on the dimensionality we have. So originally we started with the IID assumption, but by now we are working on the M-dependent assumption, finalizing that, so to say. That, so to say. So basically, if you think of it in terms of a test, then we want to test the null hypothesis of a stationary random field with an unknown mean. So we don't know the baseline mean, versus the alternative that we have some kind of anomaly, be it the one that we are looking for or the natural one, which kind of means we have some region where we have a different mean. And in this case, actually, I formulated it one. I formulated it one-sided because we know Fisher is black in those pictures. So the pictures we converted them to color scale because it's easier to see, but they're of their grayscale normally. And therefore, you know, if it's a fissure, it has to have a low value. But of course, the theory is general enough, you can as well use two-sided alternatives. And what we aim for is a class of test statistics that's flexible. Test statistics that's flexible enough to allow for different shapes of this F. So to find the anomalies, both natural and unnatural ones. But also allows for unified mathematical theory. And what we use is basically just scan statistics based on scan windows. And this could be just one scan window with this as an anchoring point. And then, of course, you can shift the anchor. And then, of course, you can shift the anchor ring point and therefore shift the whole window as well. And then you can kind of scan the whole picture. And of course, it doesn't have to look like a circle. It could be like this as well. And you can shift. Now, if you have any point in space, what we do is we basically map it, let's say, to the lower left corner in the grid point. And then we take the corresponding circle. And then, as scan sums, we just take the sum over. Sums, we just take the sum over all boxes that are within this scan window, and of course, as the scan average, this would just dividing by the number of points. And it could look like this as well. Now, there is literature, even old literature on this, but so basically it focuses on all. So, first of all, the image is a hypercube. That's fine. Is a hypercube, that's fine. But then what they are using, they are also using smaller hypercubes with the same orientation. And this is too strong because, of course, the fissure, it will be able to change around. So we need in the very least to have the hypercube be able to move it, to turn it, and to consider it simultaneously. And that doesn't work with this theory. The other thing is they always consider known mean. Now, this is not the biggest. mean. Now, this is not the biggest problem, but it's also slightly different. And you see, there is an old paper. Basically, what they do is they assume that their smaller hypercubes are of the same order in each side as the bigger hypercube one. And this is why they end up getting a functional central limit theory. So for this, they need IID error, or they don't need, but they use IID errors and finite variances. But latter, they do need. There are some newer. There are some newer works, and they use smaller windows, meaning they are shrinking, they are of smaller order than the big one. And then if you do this, you end up with extreme value asymptotics. But this is done also for the IID case and for Gauss noise only. I said it's not suitable mainly because we have this problem with the changing fissure. This problem with the changing fissure, changing direction, and we want to be able to turn it. We also want to move away from just using hypercubes. Yeah, so we want to have different shapes as well. The other thing is that I really, really wanted to work with local contrasts rather than just the global contrast. So the other works they work with the global contrast, but I do want to work with the local contrast for various reasons. So, for example, if you think that the mean slowly changes. Think that the mean slowly changes over time, then if you use a local contrast, it will still find the thing that you're looking for and not the slowly changing mean. But if it's a global contrast, then no, I mean, even if there's nothing there, it will then find the moving mean. So it's kind of like a two-sample test versus a one-sample test. So basically, what we are looking at, and okay, we are adopting this first situation here where the wind. Situation here where the windows are of the same length as the bigger hypercube, which is probably not very realistic for the simulation, but it was already complicated enough. I mean, for the application, it's already complicated enough. So for now, we stick to this. And then, but we want to use finitely many scan windows with a fairly general shape, and we want to be able to combine those with some function. And it turns out what we need is Lipschitz continuity. So, in future work, it would be nice to get a corresponding result for the shrinking case. Okay, so let's get back to the example. So, how could we achieve this? Basically, the idea is to use window shapes based on the geometric properties of the different types of anomalies that we are looking at to distinguish between them. And let's start with the fissure. Now, locally, this looks like, let's say, a thin rectangle. Looks like, let's say, a thin rectangle here. Yeah, so basically, and it has a high contrast to both sides. So, basically, the idea is that we are using a scan window like what you see there, a circle kind of inscribed with a rectangle. And then we look at the minimum difference between this gray inner segment and both outer segments. Segment and both outer segments. Both because you want to have a contrast to both sides. So it's really just a small thing here. And this is one-sided formulation. Remember, if it's actually a fissure, then this one will have low value. So this will be large. We started out with the two-sided case, but this works a bit better. Now, this is actually with a fixed angle. Is actually with a fixed angle, so I need to consider this for a well, you know, also as a function of the angle, and then, of course, this works very fast with Apple, too fast. But I don't know if you could see this, maybe I can just go back. Basically, what we then want to do is we consider it with different angles and go through the picture with all different angles, basically. Now you can see it moving. And then maximize over multiple. And then maximize over multiple angles. Yeah, I need. Yeah, well, yeah, we need to decide on the shapes, of course, of the scanning window. That's the decision of the scanning window. So you have to decide on what scanning window to use. Sure. And then it's fixed, but you can use several of them as long as it's only finitely many, that the theorem works. Now. Yeah. Okay, and if you do this, this is what you end up getting. So you see, you clearly get the fissure, but you also get, so in this case, it's not so. If you do the two-sided case, you get shadows around here. You get also the edges here. Here, you see, like those these light points, they get some kind of strange structure. You see that in the middle of the bubbles, this is cancelled because. Bubbles. This is cancelled because there's also no contrast, so that's not surprising. The edges they are with the two-sided case clearly visible here, not so much because this is darker. Still, it makes sense to find a way to find the edges of the bubble. Because if we find the edge of the bubble, we can subtract this from the statistic. How do we do this? So basically, at the edge, this has a high contrast. This has a high contrast to both sides and little contrast inside. So, if we want to find the edges, we can just use this window circle split in two semicircles and compare. Now, this is a two-sided test, the two mean values. And then again, maximize over multiple angles. And then, this is what you get to really get emphasize the edges of these angles, but you also get the shadows here. You also get the shadows here. But it's not enhancing the actual fissure, which is good because we want to subtract this from the other statistic. So this is one F and B. One is for one-sided, then F is for Fisher, and NB is for no bubbles. This is combining the two, and this is what you get. So it's actually not bad. But clearly, what you need next is you need to have a threshold. Yeah, how do we get? How do we get the threshold? Well, getting back to the math theory, basically. And what I have here is a statistic as a function of finitely many different scan sums. So I said we are dealing with the simplest situation. By the way, in the extreme value case, where they have this just the oriented ones, they actually deal with a whole range. They actually deal with a whole range of rectangles between two lengths, basically, both of them shrinking sufficiently fast, and then they still get the extreme value result. So you can take this another step, but this is already complicated. It's getting more and more complicated, obviously. So as you can see here, this is our finitely many scan sums, and we want to have a joint functional central limit theory. And then actually, what we yeah, I Actually, what we are entirely what we do, we want to get a functional central limit theorem for a combination of them via this function g. And we assume m-dependent errors. This is, as I said, it's still the final details are still work in progress. G needs to be a Lipschitz continuous function, which is fine in our example. And those scan windows need to fulfill certain regularity conditions. Now, we are currently also. We are currently also changing them a bit, but bounded convex sets fulfill this assumption, and this is what we are dealing with here. Then we get our functional central limit theory and the limit process. Of course, it depends on the long-run error variance, but other than that, it depends on the Lebesgue measure of the intersection of the scan windows. So you can write down the formula, yeah? Of course, and you can also simulate from it because you know. Also, simulate from it because you know how to construct it. But yeah, so and then of course you get the result for this particular statistic as a corollary because it fulfills all those assumptions. And then this threshold also controls the family-wise error rate uniformly over all those pixels. But of course, for stationary random fields, it doesn't take the theorem you have before. Or yeah, the central idea. Uh, what's increasing? The um, so basically, you are a grid, so you have your fixed thing, and you are the grid is getting uh yeah, it's not written down like that, but but if you want to think of it like this, yeah. So, because it's not infinite asymptotics in the sense that we don't assume there is a fixed random field continuous that we look at uh discretely, but we actually At discretely, but we actually have this plus IID noise situation, which is also often done in regression. Yeah, again, on information, exactly. So then, of course, you want to know what it does under alternatives. And what we looked at in detail is if you have a rectangular anomaly that is actually longer than your circle diameter, you can allow the width to be. We can allow the width to be smaller than the rectangle in a thingy. And for simplicity, we assume that the angle is actually equal to one of those scan angles. Then you need to have signal strength. So this is the signal strength. And now this is 2D. So this is basically one less than our dimension. This is the usual, so the square root of your data points, basically. Data points basically of your sample size. So it's the usual thing. If that goes to infinity, then you have an asymptotic power one test. Now, you can do this with all other kinds of misspecifications if you like, because in the end it comes down to just looking at the signal plus noise. We understand how the noise works, and you just need to work out the signal, which basically needs it involves the back measures of the intersections of the scan sets and the anomalies. But you have to make a decision. And the anomalies, yeah, but you have to make a decision, you cannot write it down for everything. Um, okay, and if we threshold this, this is what we get, and I think it's not bad. There are more difficult things as well, like in this example, here you see it's a much thinner fissure. And so it's obviously also not so visible here. There do remain some significant points, and they are actually where they are supposed to be doing, which is enough, because then we can start. Enough because then we can start our machine learning algorithms. And this is another more difficult example. So it looks now very gray and you don't see any bubbles, but they're here. So there is bubbles and over there. And the reason why this is so hard to see is because this is a steel which has the concrete which has steel fibers in it. And steel fibers on the scanner will be really light and make everything else really dark. And then the other problem is that steel fibers The other problem is that steel fibers locally look like a fissure, so it's not a problem with the one-sided test so much because there, of course, it's a light fissure, or you know, looks like a light one, not a dark one. But you have on the boundaries of them, there are typically some darker guys, and they locally look like a fissure. Therefore, I mean, at the moment, we are just showing you the same statistic as before, and therefore you get more artifacts. So, you would need to think of another way of. To think of another way of contrasting this. And this is what you get after threshold. So it's not entirely bad, but it's also not as good as the other one. In all examples, we somehow need to estimate the variance, and we use the global interquartile range and the normality assumption. Quantiles were obtained by Monte Carlo simulation, and we use nine equidistant angles. Now, there is a nine always works well. Nine always works well in the simulations and also in all the data examples. And there were some longer simulation results that were actually looking at how much do you need to still get a reasonable power for a given signal-to-noise ratio, because you can look at this. I think in the actual data, this is why, even with the thin one, three ones, you still found some significant points. And even with just three angles. And the reason I think is because the fissure is moving around. The fissure is moving around. So you are likely to, at some point, at least get the angle right. Yeah, it's computational. Of course, basically, it's how big your window is. Now, if you ignore the application here, you can improve over NAVE implementation in certain situations if you store the previously calculated means and use them. So at best, for rectangular windows, you can think about it. It gets linear in a number. It gets linear in the number of boxes, even if you have these linear scanning windows. But then you need more memory. So I don't think really this helps for the application. Now for this very large image data, you can do some ad hoc adaption because we don't need the full thing. We just need to find significant points. So for starters, we could just actually calculate the thing for voxel with a small enough gray value because the fissures are black. And then we could kind of even go ahead and Kind of even go ahead and, for example, if you want to use different windows and things like this, we can just go ahead and kind of do this sequentially. So start with maybe with the angles. Let me explain it with the angles. Start with fewer angles and a more liberal threshold. And then if that is significant, I continue on to more angles and a less liberal threshold. And therefore, of course, this saves a lot of computation time. Yeah, and you can do this also with like thinner and smaller. Of it like thinner and smaller in striped windows and things like this, which I would call multi-scale, for example. Yeah, currently, the computation at bottleneck is actually the simulation of the threshold value. We have some ideas on how to simulate it on smaller scale and then kind of try to get from this the bigger thresholds. Maybe for the application, you can also do something hand-waving. We will see. We will see. So, hopefully, I have convinced you that you can use scan statistics based on local differences to detect anomalies as long as you understand what you're looking for in image data. And we do give theoretic justification with statistical guarantees because they have this generic limit result. And in future work, I mentioned some of these things. For example, it would be interesting to work with shrinking windows. To work with shrinking windows, it would be very interesting to work with multi-scale case. We did this in change points, and this is where the idea came from. So, this is, you know, both of this actually, and this is what we wanted to do. But as I said, it was already quite complicated. What happens next is that Philip will spend the next couple of weeks in Kaiser Slauten at the Fraunhofer Institute. They have their specific software that they use to analyze their image data, which is called Tulip. Is called Tulip. He already did implement the 2D case, and then it was much faster than R. And now the next goal is to implement a 3D version. Where I think the critical point will be how many angles you need, yeah, because of course in 3D, it's getting much more. Although actually, there are machine learning algorithms. They were working with because it's a 2D slice, but in fact, the 3D version of it, it was like a slice fissure. It went all the way through, it wasn't like a pencil. It wasn't like a pencil, but more like, oh, sorry, let me crap this. It was more like this, not like that. And then, of course, this, in a sense, in 3D, is easier than this. Yeah. So, okay, we will see how that goes. Yeah, and those are the references. And thank you very much for your attention. Yeah, so I noticed that sometimes when you had the long fissure that there was like gaps, there would be a gap in it here and there. Can you use neighborhood information for you? Yeah, I mean that is also something I think that is something that might be able to help with the steel example. Steel example, or you could because you had some other artifacts, if you can, but you have to do it quickly, yeah, because that's the problem. So we haven't really thought about it yet. We have to check how fast this goes. And if you end up with having, so because also, of course, this doesn't show you of the significant points, which ones were the one with the darkest value, because maybe if we take those, then we throw already out everything that is not important and then the rest the other algorithms can take care of. But yeah, certainly that would be interesting. certainly that that would be interesting yeah and the gaps typically come and when there is a when there is a curve then of course it doesn't detect it anymore or at some equation there was a lighter circle inside it and then of course because of this lighter so there was actually one of the bubbles inside the fissure and therefore this couldn't detect it but also if it changes direction too quickly then exactly where it changes it this won't be able to find it so we weren't too worried about To find it, so we weren't too worried about it, but yeah, you could have a rule that your neighbors, as long as the surface are overlapping, because the batteries, the directions are the same, it could be different and there would still be. Yeah, but your signal gets smaller because if you have your if this is your if this is your signal and then you put this in here, yeah, you have this guy, the half of it has the signal, the other one doesn't. And then also there's One doesn't, and then also there's some contamination here, and therefore you have much less signal. That's a problem. Yeah, just curious: why did you need higher order moments as the dimension goes up and why the end dependence is just not just big block, little block? Yeah, yeah, exactly. So, so, so, first question. Um, I was wondering the same thing. So, and the quick answer is it actually came out of the theory, we just needed it. Out of the theory, we just needed it for the theory to work. The thing is, though, so I don't remember exactly, but I think Philip ran some simulations and he found out that there were some, it looked weird as soon as you had less moments. So at this point, I'm thinking maybe it's not just a proof thing, but it's an actual thing, but I cannot guarantee you this. And the second question was. Ah, well, we started out with the IID errors, like all. Started out with the IID errors like all the other papers. And then our collaborators said, Well, but you know, there will be dependence. Yes, of course, there will be dependence. And then maybe it's just start with the simplest thing with M-dependence. So this is what we did. And I think it's getting really more much, I mean, as soon as you can, of course, probably also extend it to some weak dependence situation, but it's getting much more complicated because you cannot just work with the big blocks, small blocks, but you also need to have some adaptation and things like this. So, you know, we decided to just. This, so you and our we decided to just go for the M-dependent case, but at the moment, not beyond it.