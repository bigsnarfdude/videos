So the organizers tasked Matt, Eric and myself to talk about inference. So we tried to figure out what that meant because it's a very general topic and I don't think we're supposed to talk about in the first day to talk about our own work on this, but rather we thought it was an opening, participating in this general conversation to establish a common language and vocabulary. Language and vocabulary. So, first among ourselves, the three of us would know each other from before, and then hopefully you will participate and feel all these generalities with your own words and concepts. I think you will find much of an overlap with things I was shown earlier today, which is great. So, I try to point to that, but please do if I forget to. Okay, so inference, yeah, well, it's a very general thing, so we're trying to infer what inference. What inference was and what inference was supposed to talk about based on the context of where we were supposed to talk about it. Anyway, so rather than talking about inference in general philosophical and mathematical terms, so we divide the work, why we'll speak some generalities, try to make it a bit less general, but still general. Matt will talk a bit more specifically about kind of tools you use in Matt. Kind of tools you use in machine learning. And Eric will speak a bit more about reinforcement learning. So that's the plan of today. If I ran out of voice, it's because I have been flying for too long. Okay, okay, so there we go. So, inference, so as an instance of inference, just to be a bit more concrete, let's talk about assessing the effect of a treatment. Yeah, and so we use that as a vocation to talk about various things that we think relate to inference. Again, I don't attempt to define inference, but just to more or less talk about it and see if we can define it together. So, general goals, so could come in many flavors, and this will talk about in much more detail today than I can do here. But one possibility is to But one possibility is to try to assess the effect of a treatment. So, if we apply this treatment to this individual, what will be the health state afterwards? You can develop a control system to continue treatment over time, right? So that will come more with the reinforcement learning that Eric will talk about. It may come with another goal with understanding, so try to figure out where. Figure out where this condition comes from, what are the mechanisms behind it. So, these kind of very general goals. And the variables may also be extremely general. The variables you are after are completely undefined, these holistic variables of health state, right? Because you may measure whatever the sugar in the blood, but you are not really after that. You care about the whole implication of everything, how is the person doing in general. How is the person doing in general? So, that the health state, the treatment also maybe is somewhat better defined, but still, it comes with treatment, it may imply a change in habits, it may imply so many things. It's again a very general concept. The individual, so we are addressing the needs of a particular individual. What is an individual, so what characterizes this particular person, and also in Seven Environment, which also. And also inside environment, which is also something that is somewhat hard to define. So these are the true variables, if you would like, or true lack of variables that you are after, very general ones. But then you need to somehow quantify them. So you have some variables where I'm trying to install some notation, at least for this talk. So let's call X some quantify. So let's call X some quantifiers, like for instance the cholesterol level or sugar in the blood or pressure or whatever. Then there are factors called those Z that may affect the outcome. And that could three factors are characterized as the patient, the age, the weight, even who is the patient, so the patient himself, or him or herself, themselves, whatever. themselves, whatever, in their individuality is one factor, some sense that goes with this idea of individual. Also, the current conditions, the past conditions also the history of this particular patient. And some variables may be measurable, some others may be latent, right? So let me call all of those Z. And then there is some action. Also, Z, and then there is some action A that will go with the treatment, right? Which is another factor, it's a factor you have some control over, and you would like to make some decision on. That would be the only difference between A and C. And then there are the observations, because they're O in general, but which are typically, it could be some of these, so ideally you're observing ZX. Observing z, x, and values of a, but maybe you're observing other things that are easier to observe, right, like all of you were talking about, and somehow you translate these observations into factors that are a bit more meaningful. Okay, that's all very general. So, those are general goals, if you like, and the variables you may be talking about. Again, please do stop here. Again, please do stop here in a second. The idea is to have a conversation. I'm talking to experts, so I won't be offended if you correct everything I'm saying. So, general tasks. One could be to build a model, so to figure out how an action, how a treatment will affect the outcome X under condition C. Condition C. And that could come in the flavor of some regression or classification. Can come in the flavor of estimating the probability, so being more general, like given a number, you give the full probability distribution of the outcome. Or you can simulate that probability distribution. Typically, you will not give, except in very simple models, you will not actually provide a closed form for the distribution, but actually we provide some. We provide samples of it, right? That will allow you to simulate alternative futures and the various treatments. Another task could be to uncover parameters. So that will go into fine in this way, for example. Latent factors, right? George, you were giving a name to those. Those were the. Well, the phenotypes was the word I was talking about. Phenotypes was the word I was talking about, but anyway, so you were doing it. So I forget because sometimes I forget the words, but you were talking about them. Anyway, anyway, so anyway, you can be looking for some hidden state that has underlying the data. That can be done through whatever. It can be done through tubatial introduction, through clustering, through something more general, we call that function. Through something more general, we call that factor discovery. So you can think that that is, in a sense, takes your so takes x is an ace and known values of z and gives you hidden values of z. So there are these latent variables. For instance, I was just saying that groups of people react differently under otherwise similar conditions to different treatments. Another one is to figure out where we are now, right? So that will go under data simulation. So you have data from the past up to now, you figure out, but you are not measuring the complete state of the person right now. So you figure out where you are. And also, where you are is also to provide diagnosis. So you are given data and figuring out what this person has. This person has. It goes somewhat under the similar category. And the other one, yeah, steer the treatment, so optimal control, if you would like. So propose an action, taking into account that you may modify that action in the future, right? And you can even take into account what you will learn from your action. And then you are in the reinforcement learning kind of setting. Are you more or less with me there? No objections? Will millionaire? No objections? Okay. Okay. So some... I know today is not the day of... supposedly for the tensions and challenges is the day for the but anyway you have to talk about something so let me talk a little bit about some tensions and challenges and I think some some of you will be talking about more tensions and challenges in infants tomorrow yeah well and in fact many of them have already been spoken about today Have already been spoken about today, like interpretability, right? The question of where you would like a model that is accurate and gives you somehow very sensitive, robust answers, but you don't understand at all. Or would you like one that you understand? So there is merit for each of those things. So there is a tension going on there. In that spirit, model versus data-driven inference. So would you like something that is based on biology or something? Is based on biology, or something that is based on just some past data. And again, both have their virtues, but it depends on what you are looking for. And of course, ideally, you would like to combine the two. I think there's no that doesn't believe that you should combine knowledge and data. But on the other hand, it's a very gray area. It's much easier to work in either extreme. Either to understand mechanistically, you propose mechanistic models for how things work. Mechanistic models for how things work, or take the data and do whatever machine learning from them. To do the two together, well, we all agree that it's great, we all agree that it's somehow, somehow it's always the other person's task. Anyway, yeah. Okay. Some other, another one, electrifiability. So I think. So, I think David wanted emphasized that he wanted to speak about it, so I have it there. So, you may have, if you know a lot about the field, you may have built a model that has many, many parameters. And some of those parameters might look very nice on the model, but maybe you cannot pin them down from observations. So, you need to be able to have a tool. If you do a model and you want to use it, And you want to use it, well, you need to have a model where you can tell how changing parameters affect the outcome. The outcome, the observation that you can make. There is big data that's always a tension in the sense that the more data you have, so we live in a world that is becoming fuller and fuller of data, and the more data you have, in principle, the better, right? Why not collect as much data as you can? On the other hand, if you don't use that data right, If you don't use that data right, having too much data may make you lose the tree in the forest. So that becomes a challenge on how to use data so that the more you have, the better, and not the other way around. And then some solutions to some of these problems, right, you can say, is to use regularizers, so not to fit parameters to a model, but somehow pay. But somehow penalize them to enforce regularity, to make them more interpretive, for instance, to enforce sparseness of the number of non-zero parameters and things in that spirit. Always don't forget cross-validations and always keeping a training popul training and a testing population to make sure you are not overfitting your data. To solve to solve some of some of those problems and others. Some of some of those problems are there. Okay? Then, set the old one about observational studies and randomized experiments that of course becomes very told this way, it looks very openly simple after the talk of Naomia. And inner, right, of course, you can put lots of things around this, right? So now you can not only have a clinical trial. A clinical trial or an observational study, but you can have questionnaires, you can have people entering their data in an application, you have all kinds of things. So you have some way of intervening and not in the collection of data, biasing or not, right? And you have data that is, you can get data that is much broader, but also somehow more biased. So what to do about that? Yeah, not just again, just intention, not not not proposing solutions. Not proposing solutions. I would propose some solutions, small solutions in my own talking today, but that's a different story. And another gateway between you would like the position to be individual, you would like to apply to this patient a treatment that is good on the general patient, right? Not and if you can, you would like to tailor the treatment specifically to how this person reacts to treatment. But on the other hand, you don't want to just use data. You don't want to just use data only from this individual because that's too little data. So you need to aggregate the data from everybody, people from different conditions and so on. On the other hand, without losing the fact that you want to make predictions for this individual patient in these particular circumstances. Then, of course, Naomi spoke a lot about the variability of data type. So I don't need to say anything more about that, but it's definitely a challenge. challenge what do you what what do you do how do you combine yeah what the medical what the what the what the clinician writes on a piece of paper how do you combine that with all the information and makes get something robust out of that there's the fact that you that if you have much data well it can be unreliable so how do you use the data yet not get contaminated by the outliers you know and there is fairness that I cannot say That I cannot say much about it that I will not be underqualified for, right? But anyway, but it comes in many flavors, right? So, having data that is biased towards a particular population to misinterpret results as assign causality to the wrong cause, just because, for instance, a person with a particular social environment may have some illnesses and some treatment in some way. Some illnesses and some treatment in some particular hospitals, and you may misassign what is due to what, and many other things. But, anyways, part of that general category of things. But I think I did my share. Anyway, so anyway, clap on board or something. I leave things too much. Any comment about this first part? Okay. Okay, so I'm gonna talk about something that hopefully people will be awake for if you can't. People have been saying the words machine learning a whole bunch, and we figured it was the inference people's day one job. Inference people's day one job to say what the heck that is, at least in some kind of accessible place. And so I'm going to make the pitch to you that if you guys are all scientists, you already know how machine learning works, at least for the most part, because it's just all these things that you've already practiced doing and learned how to do. It's fitting curves. And so it's not, most of the time, it's not some crazy thing that you can't wrap your head around. We're just curve fitting. We're just doing regression quite often. Although Noina did talk about. Quite often. Although Naiena did talk about some unsupervised stuff, but I'm not going to go there. So, what's the big deal with ML? Okay, I'm going to focus specifically on inferring a function that performs a task. And there are a lot of ways that you can do this, but we're going to scope it out just like curve fitting. That's a little small. Okay, so let's define some tasks. So let's say that we're looking for a function, f dagger. Anytime you see a dagger, that's the true one that we're looking for. Say I want to find some function, f dagger, that maps input. function f dagger that maps inputs x to some outputs y. Okay, I'm going to define a function space where that lives. It's somewhere. So that's the function f dagger. Cool. Now because I want to approximate it, I want to go find something like it. I don't actually know what f dagger is. I want to find something similar to it. So I'm going to hypothesize some class of functions that might be related to the class of functions f dagger. I'm going to call it f theta. Maybe it's like a parametric function class. So examples of these might be linear regression. might be linear regression, they have some parameters. Another example would be neural networks, they also have some parameters. And there are classes of functions that actually can be approximators in, say, like continuous functions, if F-daggers continuous functions. Other awesome models are Gaussian process regressions, random feature models, polynomials, you name it. They're cool, right? Okay. This fits for all of those things together. Okay, so now let's talk about how far away some particular Talk about how far away some particular function is in my approximator class from the one I was looking for. So you might define a distance. Here I'm defining an L2 distance, and it's an expectation over some distribution of inputs for the function. So it's basically, you can think of this as mean square error if you're not happy with L2 with respect to a distance. And okay, now what's the problem with this? Is if I don't actually know what F dagger is, so I can't measure the difference. Know what f dagger is, so I can't measure the difference between my candidate function and the thing I'm trying to approximate because I don't know what it is. But I do have some observations of that function f dagger, right? And so we're going to call those inputs xi and some outputs yi. So I see it in some places. And so I can measure an approximate distance. And you can think of if those data are sampled randomly from mu, then basically I have a Monte Carlo estimate of this L2 distance, so some kind of thing. And so on average, Some kind of thing. And so on average, it's going to be right, but you know, it'll have some variance. So I can try to guess what that distance is. Let me just define some things in this graph. So notice that this f dagger is a minimizer of this true loss function over all of the functions. So this is the best one according to this loss because it is actually defined that way. If you subtract f dagger minus f dagger, zero. Great. Okay. So that's the minimizer of that. Now this is smaller than I hoped. Now we're going to define Note, now we're going to define F hat dagger. This is the minimizer of our approximate loss. So remember, I only have some finite data. And so the minimizer with respect to just the data of this loss is going to be f hat dagger. And in fact, that's the closest I can get. If I have a particular data set, even though it was generated by this thing f dagger, I can't actually know the difference between f dagger and f hat dagger because I only observe some examples, right? Because I only observed some examples, right? Okay, and then let's define f theta star. Okay, f theta star is just like f hat dagger, except it's the best one in my model class. Okay, so what ends up happening is you have some idealized function, f dagger, you observe some data, and now the best one that you could even possibly hope to recover from that data is f hat dagger. And then what you do is you project this guy onto your You project this guy onto your approximated class. And that's f theta star. But then one more thing happens because I have to go and find that f theta star, and I might mess that up. So I might try to minimize this distance with respect to my data, but who knows if I actually do it perfectly. This is the optimal one because it's the closest, but I end up with some f theta at the end. So when you get f theta and you started with data that came from f dagger, this is what machine learning. This is what machine learning is. And your train error that people haven't talked about is the distance between the model you come up with and the data-informed function down here. And so that's okay, because you may worry, how do I evaluate then how far away I am from F dagger? Because all I see is this F hat. They could be super far apart. Well, you appeal to statistics and you say, well, I happen to know that F hat. Well, I happen to know that f hat dagger came from a distribution with mean f dagger in some sense. And so if I just randomly sample data again, I get a different F2 hat dagger. And I can measure that guy's distance from F theta. And this is your test looks. So when you split data independently, you get train and test error. And this is the kind of thing that you use to evaluate your data. And so you want, so how do you solve some of these problems of like, Solve some of these problems of like, first of all, the generalization gap. Like, you observe some finite amount of data, and your train loss might be smaller than your test loss. So as you collect more data, this distance shrinks down because that distribution in orange contracts. So they think of like, you know, law of large numbers and central limit theorem, right? That as you collect more data, you're going to converge to F dagger. So that's how you deal with this guess. So that's how you deal with this gap. You just collect more data. Maybe you can't, but this is how ML people deal with this, right? Then you might worry about how far away the F theta star is, the optimal ML model. And the way that you close that gap is you just more parameters. We defined an approximator class. So the better approximator it is, the closer the edge of this green ball is to that function. That function, and then you just try and optimize the heck out of it. So you say, oh, well, I didn't get the optimizer, right? Just keep running and you hope, right? And that, I mean, it won't get worse. So you'll get closer, right? So with optimization, you try to run different optimizers. So that's kind of the high-level picture that I wanted to paint for machine learning. And maybe I'll stop here and just see if there are questions about. Stop here and just see if there are questions about this crazy diagram. Yes, sir. Yes, it looks like you're promoting overfitting. I don't think that's what you mean to say. No, well, so I'm not actually, because what I'm doing is I'm asking for a, so the final evaluation is the one with the smallest test loss. So that's how, that's the model that I want. So I want an F theta that has the smallest test loss possible. Ideally, F theta is this close to F dagger. And the way that you can Close to FDA. And the way that you can learn whether or not it is is by doing this train test. But I'll talk about why this might look like overfitting and how to deal with this. So again, this is the overfitting, right? So if this is far, then you may overfit. So you want the train loss and the test loss to be the same. If they're not, then you're in some kind of biased regime. You're in some kind of biased regime. I think this might be one of those two different terms for the same thing, things. Are you thinking already like the overfitting in terms of too many parameters in your regression? Yes. Yeah. So if what you're saying is that, you know, if I have some, if I have a curve, right, and if I sample more and more data on this curve, it really doesn't matter. Curve, it really doesn't matter whether I use 10 to the 6 parameters or 10 to the 1. If I fit it perfectly there, and then I resample data around this curve, and it's fit perfectly there, and no matter how much I resample, it's still perfectly fit. Then I didn't overfit it. So there's an empirical evaluation of overfitting. I think a good diagram of overfitting is from F dagger to F theta star. Theta star because f dagger to f dagger hat doesn't really have a concept of overfitting. You fitted it perfectly as far as you can tell. There's nothing else you can say about it because you don't know anything else about it. It's only when you move to your green sphere that the overfitting means that where you didn't happen to hit that point, it sends it to the wrong place because you're not really. So no, so think of f theta as the same. Think of f theta and f theta star as the same. theta and f theta star is the same. Because oftentimes the optimizer is not the problem. I wouldn't recommend thinking of it that way. So think of it as f theta star and forget about the optimization error for a second. That yes, at the end of the day, the real error is the difference between these guys. But you can never observe that, right? So all you can ask for is some kind of statistical evaluation of those distances. And that comes from And that comes from using data that are independent from this error. So, this is what I'm saying: you start with something, the finite data brings you over here, and then you land over here, right? And then if you sample other finite data, you can probe this distance. So, when I resample, I may end up over there, and this gives me evidence that I was actually further than. That I was actually further than I thought. But you can never compare F dagger hat to F dagger hat 2 because they don't have a concept of what happens between the missing points. It's only F theta star and F theta star 2, the second one not shown. And those are the two you can compare. So I can compare these guys and these guys, because this guy comes from Dab, and this guy comes from Dab. I can never know whether F2 Dagger was on the left side. Whether F2 dagger was on the left side or whether it was right next to it. But in the large data limit, these two things will be close. Okay, I'll go with that. I can explain later what I mean, but it doesn't matter. I mean, my brain is pretty fried, so I may have said all the wrong things. But yeah, it probably won't get better at this point, so I should just keep it going. No, no, it's wandering around and saying. Wandering around and saying, I can use it. I have kind of a theoretical question. I need to define overfitting. So I think that it's a overfitting is when the training, so overfitting has to do with data, right? So it says that you observe some data and then you make some decision that works better for the observations than it does for the true. Observations than it does for the true function overall. Right? And so the true function overall is f dagger. So if you make a decision that better fits f hat dagger than f dagger, then that would be overfitting. Let me just say, I really like this diagram. I never thought of dividing it up like this, so I really like it. I think we could think further about it, and I'll explain later on what I mean by what I said before, but I really like. really like this division between F dagger hat and F theta star. But I think F dagger hat has no concept of what there is outside of the points that are measured and that's why you can't compare one F dagger hat to F dagger hat two. You have to move to a theta in order to do a computer. Yes, I can't measure them, but yeah, that's right. Measure them, but yeah, that's right, that's right. Yeah, I'm not saying that they're far or close. They're just independently sampled from orange, if that makes sense. And so that allows you to get an estimate of the test loss that isn't biased by the training loss. In other words, that can show you whether or not you're overfitting. That in a statistical sense, that the test loss and the train loss should be the same. And if they're not, then you are probably, you have it, you're likely to be overfitting. Likely to be overthinking. Okay, let's keep going. Okay, so let's just talk. So clearly, data was super important over there, right? Because the way that you deal with overfitting, one way to deal with overfitting is to just take more data. An infinite data limit, you probably can't overfit, right? But that's not a great answer for this room, right? So what else can you do? Well, so if the data covers the space sparsely, but somewhat you Space sparsely, but somewhat uniformly, right? So, like, you know, this is a classic example, right? You don't know what the oscillations are. You need to regularize or limit the complexity, and this is what Artie is getting at. So, you know, do I want to fit this or this? You need to make a decision about regularity. And this is a very fundamental thing, and it is a really valuable concept. But a lot of times, it's worthwhile allowing for a lot of expressivity and then demanding. Expressivity and then demanding that there's a limited, or that things are regular in some way. If, on the other hand, you have data that covers, say, a subset of a domain, like you see some really prominent features on one half of the graph and then really sparse on another part of the domain, that maybe it's great to fit with a lot of complexity on one part of the graph and then have the the function you're learning be really boring away from most of the data. Away from most of the data. So you can think of, on one hand, regularity, but regularity doesn't quite capture distance off of data. I didn't observe anything over here. I shouldn't be saying something really fancy. So it should be linear or constant or whatever your kind of prior beliefs are. So think about that. If you're trying to learn a function and you know that you're going to observe some high fidelity data in some regions, allow yourself to express there. And then when you're far from it, don't do anything. Have it be zero, whatever. Be not informative, say? Uninformative, yes. Uninformative, yes. What else? Right, so don't be afraid of expressive models, just learn how to regularize them, I think, is a recommendation. Other challenges: latent variables. Okay, I just said, you know, there's an x and a y, but what if our input spaces, what if we observe a subset of the relevant inputs to this function? That's a problem. So you may need to try and look for things that account for this discrepancy. What if you have noisy inputs? This is like a really simple case. Inputs. This is like a really simple case, right? Just take if I have this input-output system, but the inputs are corrupted with noise. If the function I'm looking for is non-linear, that noise is not guaranteed to average out. You can learn a bias model just from noisy input data. Bruce wants to say something. Yeah, I just don't like the word if. Sure, great. There is an amount of noise combined with an amount of nonlinearity. Of nonlinearity that produces an amount of bias that you may not be happy with. So you may need to incorporate denoising. So sometimes you don't do it, and it's because those amounts don't matter, and sometimes they do. What about non-shift stationary? Sometimes the data distribution shifts, sometimes F dagger changes over time. You might have to calculate that. I think this is everything. Okay. Eric, yeah, yeah. Is there ever a time when you don't think about non-stationary? Because the way you kind of describe it at the end, it was like PS. Was there ever a time where you don't think about that? Yeah, I mean, to be truthful, I mean, you know, I'm not working in many applied problems at the moment, but I think that, well, here's what I should say: that I think that problems should be scoped out so that they are stationary. You induce stationary. Are stationary, you induce stationarity in some way, which might mean that you need to expand the dimensions of the system that incorporate the thing that's changing over time. But I mean, if you've got cats and dogs, like, I don't know, they're always just cats and dogs. You know, if you're doing like image classification or things like that. But there really are a lot of settings where, okay, here's an example. Any of these mathematical models that we've seen, like these ODE systems, right, they're mapped from initial condition and parameters. map from initial condition and parameters to a trajectory of solutions. As written mathematically, that does not change. That is a function that maps parameters and initial condition to another function. That's not a stationary, so that map is stationary. And in fact, if I wanted to and seemed worthwhile, you could try to do a whole bunch of machine learning and learn some kind of surrogate model that maybe more quickly evaluates some features of the model than you would get. Features of the model than you would get from integrating them, or things like this. So there are lots of cases where if you can scope out that there's always these inputs and then there's some function that is always applied to them in the same way, then there are these outputs, then that is a stationary thing. So yeah, I think it's fine. I think where non-stationarity shows up is when we really restrict the complexity or the things that we keep track of. And then there's something behind it that's actually. There's something behind it that's actually moving, and that's the most important part. So you just need to maybe include that moving piece, and now it becomes stationary with respect to that extra stuff. Eric? Eric? Do you want to share your screen, Eric? I mean, because I can do this, but maybe I should just. What would you prefer? Yeah, why don't we just put it? I'm not going to share my screen. I think I'm probably going to. I know we have a nice funny thing. There we are. Oops. Alright, and how are we doing at time? I think the schedule is. Where we're by doing an hour or something. Blue. But just go. That's good. Yeah. Yeah, but see if you could shave off a minute, that'd be really helpful. Get us back. Well, thanks. I'm really sure I can be there. I made it as far as Minneapolis. And now you got a call that one of my kids had tested positive, so I had to turn around. Luckily, I hadn't left the country. I'm negative, I guess. That's no problem. I figured it's best not to risk it. So, as Matt Bob said, you know, our goal was to talk about, or we thought we should produce some of the. Or we thought we would introduce some of these concepts to just sort of set a common language, and my task was reinforcement learning, so I'm going to do that. But at first, I thought I would just talk about choices. So dad always thought laughter was the best medicine, which I guess is why several oldest type of rigid things. I just figured it's so close to the day low. Okay. So what is reinforced learning? First, it's using data to make better decisions. Using data to make better decisions. It's a subfield of machine learning or AI or quantitative analysis that focuses on making decisions under uncertainty. And I think it's distinguished by this approach of, I mean, essentially all statistics in analyzing forming decisions in one way or another. We do an analysis, we use it to make some kind of decision. But in our own, we try to take a holistic approach where we really try to put into the decision system what we care about, whether that's profit or patient outcomes or some measure of efficacy. Or some measure of efficacy and constraints on side effects, whatever it happens to be, and then optimize the best decision strategy directly as opposed to maybe doing some summary analyses or some modeling and then informal decision making. And so the goal is to generate information and to use that information to maximize utility over time. So I don't want to generate and use, but we want to make the best use of the information we have, but we also want to think carefully about how we're going to generate information. About how we're going to generate information and improves our models and makes better decisions. And so that's this tension that sometimes calls exploration and exploitation or earn versus learn or ethics versus efficiency. But it's this notion that there are times when it's going to benefit us long term to spend to sort of do something sub-optimal in the short term. And as a discipline, it uses ideas from a very wide range of disciplines, so external designs, special efficiency theory, causal imprints, optimization, and so on. And so, you know, at the inner loop of any And so, you know, with the inner loop, I mean, our algorithms are watching supervised learning. Also, it's all sort of usual kind of regression and classification type ideas. And of course, people think of RLs. Many, many people think of these high-profile examples where we look at Alpha Zero or AlphaStar, or more recently, AlphaFold, which is this sort of protein structure identification problem, or gaming, or robotics. And I should say, Or robotics. And I should say, maybe I should have introduced myself. So I'm actually robot. I'm at SaaS form at Duke. But I also spend one day a week at Amazon, where, so I do ORL in the medical space, my professor job, and then I do ORL in other spaces like retail selection and robotics on the Amazon side. So I can see a little bit of kind of both parts of RL. Okay, but I would say that RL in the wild, I put a footnote here to just remind me to tell you. I'm here to just remind me to tell you that, of course, the things I say are heavily biased by my own experience, and so apologies if I'm sort of giving something that's working on short trips, it's just my lack of familiarity, and I didn't want to talk about something I didn't know much about. So here I'm making the distinction. These are problems where there's very high costs. So either it's patient risk or it's retail costs or something that there's really a significant expense to collecting data and to making decisions, which is distinct from, say, chess or Which is distinct from, say, chess or Atari, where, of course, efficiency is important, but you can generate an arbitrary amount of data at pretty low cost. And so things like adaptive clinical trials. So how do we adapt treatment assigning over time based on the responses we see so far? And things like dynamic treatment regimes or adaptive treatment strategies, adjust-in-time adaptive preventions, and mHealth. Things like system-level triage or team-based care, we might look at how we would utilize. You might look at how we would utilize registered nurses over time in part of a team-based care solution. Visit recalls. We talk about how often she go to see your clinician and see your advantages. Again, these are the kinds of RL problems where data are scarce and expensive and they have real costs. Public health is another area where RL is being applied. Things like public health messaging, where you adaptively send messages to get somebody who's screened for colorectal cancer or maybe to get Maybe to engage in safe behaviors related to SDIs and HIV, resource allocation. So we've worked on distribution resources for upgrades like Ebola. And then in the retail space, advertising is the obvious what everyone thinks about it. You go to Google and you search for something and they show you some ads. Recommender systems of pricing, but there's other things like what should go into a store. So what items should we put in the store? Where do we locate the store? So where do we build? Locate the stores, so where do we build warehouses to distribute to those stores and so on? Looking at fraud detection. So there's a lot of areas that are kind of new to RL that have traditionally been made by building predictive models with ML that have experts sort of digest that information and then ultimately make the sort of final judgment call. But now we're seeing more automated sort of solutions. I think that's exciting. So I think we're all familiar with these ideas, but I thought it would be worthwhile to just start from. I thought it would be worthwhile to just start from scratch. So, the RL model of decision-making is kind of displayed, well, a toy version of it is displayed in this diagram. This idea that you observe the status of the system. So, that could be the health status of a patient, it could be your sales at the store, or it could be all the cookies that are stored for an individual who goes to your website and searches or something. And then you take that statement, you use it to select an action or make a decision or select a treatment, and then you observe some consequences. Observe some consequence. So that could be the health test, the next visit, it could be whether they bought something from your website, it could be total revenue in your store or how much wasted food you had in your restaurant, whatever it is. And then the system transitions to another state and the process repeats. So the idea is in systems like this, you want to learn from your experience. So to learn from our experience, you want to not only study how state and action relate to reward, but how state and action relate to next state. Relate to next state. And we also want to be really thoughtful about how we select actions, not only to maximize reward and utility, but also to generate information that improves our models. So lots of examples exist in different areas, and some economic ones here. A randomized clinical trial is a very simple model of reinforcement learning where you do all of your sort of learning up front and then you do exploitation, right? Of course, we don't say that in the medical world. We don't call exploitation. That doesn't sound so great. We talk about patients, but we do talk. It's not so great we talk about patients, but we do talk about sort of optimally generating information and then optimizing it. So we randomize patients into treatments and then we estimate the best thing to do for those patients, and then that's what we do. Then there's slightly more sophisticated, which is like bashed learning. So you have alternating phases of learning optimization. This is very common in industry. So you push a system into production, and maybe you have a small substream of traffic that's randomized, and then you use that randomized. And then you use that randomized traffic to update your model and you push that into production, and so on. We also see that with different sequences of trials. We might have an expert-dried policy, we tested a health intervention, we're doing implementation testing, then we take that data, we improve it, we do some randomization, we improve the model again, and then we keep deploying stochastic versions of our learned model, but we freeze it in between these updates. And so that's sort of batch learning, not to be confused with. Not to be confused with batch data. And then there's fully online. It's the idea that we are updating our models every observation and we're kind of jointly optimizing learning as we go. So these are kind of degrees of exploration and exploitation. Okay. So I thought, you know, again, in the interest of just kind of setting ideas, to just set some notation for these really simple decision problems. So the simplest possible decision problem is a batch one. Possible decision problem is a batch one-stage problem. This is like a clinical trial. You go and you randomize patients to treatments, and then you do all your estimation and inference afterwards. So here the data would be something like XAY, I equals 1 to N. So here I is indexing patients. These are IG triples. X is patient information or your decision context if you're talking about some other decision problem. A is the action or decision. It could be a treatment or an intervention, or it could be an ad that you showed or a product that you displayed. And then Y is the measure of. And then, why is the measure of utility? Again, it could be patient response, it could be clicks, it could be verbs, something else, and code it so that higher is better. The goal then is to construct a rule that maps X to A to maximize the mean of Y. That's the idea. So there's lots of ways to do this. First, I just want to note that kind of implicitly, we talk about RL, there's this notion of feasible actions. Actions. So there's usually a function psi that maps the context space x to the set of allowable actions, so the sort of power set of A. So any given context X, psi of X is a subset of these actions that you are allowed to take in a given context. So maybe you can't send somebody a notification on their phone if they are driving or they've muted notifications or they're supposed to be sleeping at that time. And a policy in this context is a matter of. Policy in this context is a map from context to actions such that pi of x is in this feasible set. So, another example, a lot of people talked about, or several people talked about type 1 diabetes. So, if this were an insulin dosage, then of course there would be guidelines for what it should be based on the patient's current status. So, under pi, the decision maker will select action pi of x, taking pi delta x. And we'll define v of pi to be the expected utility if you assign actions according to pi. You assign actions according to the pi, and then the optimal policy makes that expected value as big as possible. So that's how you might formalize a very simple decision problem. And then in terms of estimation, it's kind of two main ideas, and these we'll see kind of pop up as decision problems get more complicated. The first is based on regression. The idea here is to just estimate the mean of y given x and a, q of xa. So you can estimate this using whatever your favorite machine learning regression model is. That gives you some estimator q hat. That gives you some estimator q hat q. And then the estimated optimal policy pi hat of x is just the value of the action a that maximizes your estimated function. And so what's nice about this formulation is you can apply this with all sorts of data structures, whether the text is our images or functions or anything else, sensor data, anything we can do regression for, you can do this. On the other hand, you're doing maybe more modeling than you have to, and so that's why a lot of people sort of try to. Why a lot of people sort of try to propose or support policy search, but here the idea is I don't want to go through this regression model to get the optimal decision. I would rather estimate this marginal mean outcome under pi directly. So for any policy pi, the idea is to construct an estimator of v hat at pi, which is an estimator of v at pi, and then give us some class of policies, capital pi, just compute the argmax of v hat over pi of the class. And this is an introductory talk, and you've already seen a lot of Talk and we've already seen a lot of ideas and notation, so I'm not going to describe the estimators that we have. Just I put a reference done here if you interested. Okay, so some things that are still open problems, even in this sort of toy example, it's not a toy example, but the simplest possible example, is going to scale and complexity. So if you think about something like assortment selection, so what goes into a store, if you have n things in your catalog and you click k of them in your store, just n choose k possible assortments, right? Sort of, right? So that's a massive space. Something like Amazon, deciding what goes in the course selection, you might have a billion items and you choose some enormous number, right? So these are astronomically big selection problems. You also might have a high-dimensional complex feature space. You might want to make a street recommendation that's based on text data or functional data, imaging data, technical data, and so on. So we have very high-dimensional data. And then there's the experimental design problem. So, how should we assign actions in a way to maximize efficiency? In a way to maximize efficiency related to pi-hat. Obviously, there's a lot of work on maximizing efficiency in estimation of a parameter, but if we only care about decision robots the optical design, and how do we do that if we still want to ensure power for secondary analyses like model diagnostics, for example? There's also a real important issue that shows up in medical decision making, which is dealing with multiple outcomes and patient preferential heterogeneity. So we have scientific. So we have side effect profiles, we have different measures of efficacy, and one set of outcomes for you might look very different to me. So we have different life circumstances and so on. And so what we find is that even though the outcomes may be identical, we'll use them very differently. So how do we get at those patient preferences and incorporate them in decision making? But then there's this secondary issue of stated versus revealed preferences. So somebody may say, I don't mind having that side effect. I'm okay with that. Side effect, I'm okay with more efficacy in exchange for more side effects, but then they experience a side effect and they realize, well, actually, it's destroying my life, right? Okay, so if you want to go up one step in complexity, we can go to online one-stage decision combined. And so it looks superficially very similar. This is what's known as a contextual bandit setting. Again, we observe these triples, x, a, y, but they're no longer at ID. So the 