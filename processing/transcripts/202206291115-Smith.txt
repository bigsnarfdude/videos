So, I always used to say that this works if I just speak into the microphone here, right? That should work. Yeah, okay. I always used to say that a good statistics talk should begin with an interesting data set. So, here is my data set de jour. It's annual maximum daily maximum temperatures in Kelowna, British Columbia. And I'm sure the climate records must go back a lot earlier than 1984, but this is what I was able to readily put. But this is what I was able to readily pull off the web. And this is a long enough series to illustrate the statistical points I want to make. I thought, by the way, when Gabby started talking yesterday, she was going to come up with exactly the same example, but she had a slightly different example, but based on the same meteorological event, which is the heat wave that hit the northwest a year ago and the actual date of this. Oh, I need to. Sorry, I should use my window here. Window here. So, the actual date was a year ago tomorrow. And I think most of the present audience are probably more familiar with temperatures in Celsius than Fahrenheit. So it was 44.6 degrees Celsius, 112 degrees Fahrenheit. So we're interested in questions like: well, can we characterize how extreme that event was in terms of a return level or something like that? That and but also, um, the attribution question, which is related to the causality question: can we say that such an event is more probable in a global warming world than it would have been, say, 100 years ago? And I'm going to answer yes to both questions, but um, let's let's see a little bit about the ideas behind it. So, you're all familiar by now with the generalized extreme value distribution, three-parameter distribution. Distribution, three-parameter distribution. The shape parameter psi reflects how long-tailed the distribution is. And with temperature series, we usually get a negative psi, which corresponds to a finite upper bound. And as Gabi pointed out yesterday, this actually causes some problems for the analysis. So if we just do an MLE for the GEV distribution in this case, you can see I convert sigma to log sigma, it makes the To log sigma, it makes the optimization slightly easier. But the critical parameter Ïˆ in this case comes to negative 0.42. So that's a very short tail distribution. And according to that model, the upper limit is 39.8. That's the estimated end point based on the data up to 2020, which, if you interpret that literally, means that what happened one year ago was impossible. Okay? Now, we Now, we want to relate this. We want to add trends to this because one possible objection to that analysis is just fitting a straight GE that doesn't allow for the trend. So we can add, in this case, I'm just modeling mu as time dependent. I could model all three of mu, sigma, and psi. I'm probably should, by the way, but I'm just for simple analysis, I'm just modeling mu as a function of a covariate and the covariate. Covariate and the covariate I'm taking here is the global mean surface temperature. Now, I don't actually think this is the right covariate for this analysis, but I'm actually following a precedent that I'll come to later in talking about that particular covariate. So if we fit this linear regression, that's statistically significant against the no-trend model. The psi in this case comes to be negative 0.29, so it's still a substantially negative psi. Negative psi, and the estimated end point for 2022 under that model is 41.6, which is still less than the observed value. So simply throwing the trend in there doesn't solve that problem about zero estimated probabilities. That, by the way, is the temperature series I'm taking. The climate data experts in the audience will know exactly what I mean by HADCRUP5. It's the British construction of the global mean temperatures. Global mean temperatures. And you can see that, and it's expressed in terms of anomalies, as is usual in climate data sets. And you can see there's a sharp rise from about the 1970s onwards up to present day. But you all know that. So there are certain senses in which this zero probability phenomenon doesn't actually make sense. I mean, it doesn't really make sense from a meteorology point of view. Meteorologists do study. View meteorologists do study things like what's the maximum temperature that's physically possible, or what's the max rainfall that's physically possible. Francis Weirs has a very nice paper on how you would integrate that sort of physical constraint into things like a GEV analysis. But it also doesn't make sense statistically because we know that mu, sigma, and psi, the MLEs, they're only estimates, and there have to be points within sort of Be points within sort of the cloud of the confidence regions for mu, sigma, and psi where this estimated probability is indeed positive. And one way we can take into account the uncertainty of the estimated parameters is to do a Bayesian predictive analysis. And by the way, one of the early papers on this approach was actually about the Vargas data set that Anthony referred to. It was a paper by Stuart Coles around 2000. Coals around 2000. So, you know, thanks to that idea, and I think these ideas have become more established recently. And so, I've done the same analysis using an MCMC to produce a sample from the posterior. And I think Jean-Shen Joubert yesterday also referred to this. The adaptive metropolis algorithm works very well for this kind of model. And what we find, so now what I'm effectively doing, I'm generating a thousand simulations of the A thousand a simulation of length a thousand from mu sigma and psi from the posterior distribution. And I can go through those thousand values of the parameters and associate with each one a probability of exceeding that critical value, the value that we actually observed. And indeed, the posterior mean is a positive number, but the posterior distribution still has a large atom at zero. And I'm mostly going to express this in terms of. And I'm mostly going to express this in terms of return values, which is one over the probability of exceeding the value. And what you can see here, I had to make a caveat about this slide because I realized in going through the talk this morning that these numbers I'm quoting for the return value, they are very, very, very unstable. If I repeat the MCMC, I get a totally different answer. But I think I can solve that particular problem. I can't solve some of the I can't solve some of the others, but I can solve that particular problem in doing a much longer MCMC. So, what I'm going to do is when I get back home or by the end of the week, I'm going to revise these slides and I'm going to put an updated set of slides on my webpage. So, you know, feel free to look at that. But as things stand, I'm getting under the straight GEV model, that was a one in 70,000-year event. Under the trend model, it's about one in 2,000. Trend model, it's about one in 2,000 years. Okay. So, this style of analysis shows how we can relate this to a global variable. And that sort of partly answers the causality question because there's a lot of analysis of how global mean temperatures are causally related to greenhouse gases. And so, at least this provides. And so, at least this provides a connection. But to make it a quantitative connection, we need to look at some actual climate models. So, let's now come to a climate model. Now, I've just chosen for this analysis one run of one climate model. And really, this should be a multi-model analysis that takes all the models into account. But just to make it simple, I'm just using one model, and that string of letters and characters on the top, Daihi will understand this, Gabby will understand this. Understand this, Gabby will understand this, probably the rest of you don't. But this is the HADS GM model, the British climate model. And that 585 at the end of the string of characters, that indicates the scenario they're using for future projections. And this 585 scenario, it used to be called the business as usual scenario, which I think is probably a bit of a sick joke these days, because I think when people first use it, Because I think when people first use that phrase, nobody really believed that we would literally be doing that 20 years later. But there we are. The model that essentially assumes that greenhouse gases are going to continue to go up and without any serious constraints. And so that has a similar shape to the real data I showed you earlier, but this is projected through 2050. And I think the projection, so this is taken from the website. So, this is taken from the website for CMIP6. That you can go on the website and you can download the data. That's how I constructed this data set. And there are many other data sets there. I just chose one to use for illustration. So, I've repeated the previous analyses with global mean temperature from that model in place of the observational data. I shouldn't use the phrase real data because, for me, climate model data is real data just as much as anything else is, but it's not from observations. Observations. And for a counterfactual run, I use data from 100 years earlier. Now, that's not the ideal way of doing it. I prefer to use an actual natural forcings model where you continue to mimic things like solar fluctuations and volcanoes. But those aren't on the public version of the CMIP6 data set. I'm certain such runs exist, and I'll probably talk to some of my climatologist friends to get hold of them sometime. Friends to get hold of them sometime, but this is what you can download from the web. Um, and I should also make one caveat about this approach: when I talk of using the model data, the model data were actually generated in 2015. They weren't generated last year. So I'm using sort of a projection of model data as a covariate. And that might be problematic if there was a real discrepancy between the model data and the observational data. And in some analyses I've done of this thought, there really is a. Analyses I've done of this thought, there really is a discrepancy. And I already said the quoted return values are highly volatile, but they give an idea how the estimated runs go with different models. And in this case, if I take the left figure here, it's the same figure as I showed earlier, but using the model data from the so-called historical climate run, which means it takes into account the actual values of greenhouse gases. And what I'm Gases. And what I'm calling pre-industrial is actually the data set from about 100 years earlier. And what you can see is the return value is about 10 times larger, which would imply that this event is 10 times more likely to occur in present-day scenarios than it would have been 100 years ago. That's the way to interpret it. But again, don't take those RV numbers too literally. Numbers too literally. And finally, finally, for this part of the talk, we can use these climate model projections combined with the GEV model to estimate future probabilities. I actually think this is, in some ways, I think this is more interesting than just agonizing over the attribution question, because we can certainly take the right-hand half of that climate model picture, feed it into the GEV model, and project. Model and project what the probability of this extreme event would be in future years. Now, this curve I'm showing you is actually very jagged, which is partly because of the way I did the Bayesian analysis with only rather short MCMC runs. But the primary reason this curve is so jagged is because I'm just using one run from one climate model and individual climate models go up and down at random. Go up and down at random, just as the real data does. A better way of producing this would be to basically combine data from as many climate models as we can. And at some point, I'm going to do that. But what this shows is that, again, if you take these numbers literally, the probability of such an extreme event goes up by approximately a factor of 10 between now and 2050. And that's on top of the fact. And that's on top of the fact that it's already gone up by a factor of 10 since pre-industrial time. So that's giving you some measure of how global warming is affecting these extreme events. Now, I said that I was following a precedent here. There's been a widely cited paper by this so-called World Weather Attribution Group. They specialize in getting a paper. They specialize in getting a paper out within a few days of the event, and this paper was published sometime. Oh, it was put online, it hasn't been published in a journal even now, I don't think, but it was put online within about 10 days of the event. And I'll just highlight a few of the things they said. They actually came up with, they said, well, this is about a one in a thousand year event in today's climate. I'm going to dispute that in a minute, but that's actually consistent. Minute, but that's actually consistent with what I said. Um, they, I think this is probably the same data set as Gabby was showing, so it's not actually a data set from one station, it's an integrated data set from a reanalysis over a chunk of northwest North America. Um, they, I said, there was a precedent in using GMST as a covariate because that's exactly what they did. And in fact, they were doing something similar to what I've They were doing something similar to what I've just done, which is comparing 2021 with projections from 100 years ago. I want to highlight the one technical point of what they did, because this is very related to some of the things Anthony Davison was saying in his talk about that Vargas data set, because they also noted that there was a essentially this extreme event for 2021 was beyond the range of the GEV distribution. The range of the GEV distribution as they fitted it. And they had a couple of fixes to that. And this is critical to where that one in a thousand number comes from. One of their fixes was to constrain the GEV so that the end point can't go beyond the observed value. That was one alternative analysis they did. And another analysis was to actually include the 2021 event in the GEV. Event in the GEV estimation itself. And that's precisely the point that Anthony was arguing about with regard to the Vargas data. Now, I don't know whether Daihi remembers this. Do you remember that I once had an argument with Gertian von Odeberg during an IDAG meeting? Because I actually did one of my analyses in this way, where you include the extreme event in your GEV analysis. Analysis. And my rationale for doing that was: if we're interested in future projections, future projections, we should take account of all the observed data, including the very outlying data. And at that point, Kurt Jan von Odeburg, who very sadly died about a year ago, but he's one of the real inspirations behind this whole type of analysis. And in fact, what he did, he reversed what he was arguing with me those years. He was arguing with me those years ago, he was actually doing it the other way around, at least assuming he had some say in this particular part of the analysis. So I thought that was rather interesting. But that's why I don't quite trust that one in a thousand estimate that they came out with. But it's all about how you handle these zero probability events. So I've got about five, no, I've got about eight minutes left, and I'm And I'm going to go back to the advertised topic of my talk, which I wanted to go through all that because I think that relates to the theme of the meeting and how you can do these sorts of analyses with univariate time series. But what I actually said I was going to talk about in my abstract was spatial models. And I want to talk a bit about spatial models for this kind of data and show how to relate that to the sort of topics I was. Relate that to the sort of topics I was just talking about. So, one of the analyses is based on Hurricane Harvey and the amount of precipitation there. Those are some pictures that were shown at the time of the city of Houston being flooded. And the statistical methodology here is the methodology I'm following in this paper is a slightly different notation, but it's still a GEV. Slightly different notation, but it's still a GEV and allowing both the mean and the location parameter and the scale parameter to depend on covariates. And this is actually following very closely an analysis that was done by Rissa and Wiener. And the main covariates we were using here is sea surface temperature over the Gulf of Mexico. That's what that SSTT refers to. And we get some estimates. So we can do a similar analysis based on just one station. But what I want to show here is an extension to a spatial field. And this is a paper that was published in Envirometrics. And the lead author here is Brooke Russell. And Brooke, if you don't know him, he was a student of Dan's. So there's a thread here that goes back about 20 years in work of both. About 20 years in work of both mine and of Dan's. And in this data set, we took 326 stations. We fitted, well, we're using ETA and Tor rather than mu and sigma. Not everybody uses the same notation for these things, but we used ETA, Torr and Psi. We initially estimated them individually at each station, but then we tried to smooth over them by modeling it as a five-dimensional spatial. It is a five-dimensional spatial process using a technique for multivariate spatial processes due to Wachenagel. It's called co-regionalization. And there's a two-stage estimation procedure that allows for spatial correlation among individual measurements. I'll give a bit more detail about that if there's time at the end of the talk. But this is what came out of these pictures taken from the paper with Brooke Russell. And the top left picture here is sort of. Top left picture here is sort of showing it's not exactly pre-industrial conditions, but a low SST condition when you get a mild increase in the, I guess the variable being plotted here is the precipitate, let's see, it's the estimated probability, okay, estimated probability of an annual maximum seven-day rainfall exceeding 70 centimeters. And the point here is you can observe the space. The point here is you can observe the spatial variability, and you don't get much of a probability in the low SST scenario. You do get a substantial increase, but particularly focused on the Texas coast under a high SST scenario. But that particular year, 2017, the SST in the Gulf of Mexico was unusually large. And that's what's driving the different shape of the third picture. So by putting So, by putting results together like that, we're combining information over a lot of stations. We can see how the high SST affected the whole Gulf Coast, not just the city of Houston. And although we didn't address this directly in our paper, I think this sort of spatial modelling is also useful. We had a bit of discussion earlier in the week about the sort of selection bias issues. We're not proposing a solution. We're not proposing a solution to selection bias. I don't think there really is a solution to selection bias, but at least these spatial modelling gives you some idea of how these probabilities vary over the coast. Now, if I've got time left, I've got about three minutes left. I'm going to say a little bit more based on a very recent project that has been started by a student of mine by the name of Brian White. And this is a very important thing. Is um, this is joint with a guy called Rick Lutik, who is the director of UNC's Institute of Marine Sciences. And Rick is the author of a program called AdCirc, which is a model for projecting surges in hurricanes and other extreme weather events. So, if you live anywhere near where I live, you turn on the evening news. You turn on the evening news about this time of year, and the weather station meteorologists will say, Well, there's a hurricane coming in, it's now somewhere in the neighborhood of Cuba, and we predict that in three days' time there will be a 10-foot surge in Wilmington, North Carolina. That would be a typical weather forecast around about this time of year. That 10-foot surge probably comes from Adzirk. So, that's what he's done. That's what he's known for. But, what the For. But what the current project is about is trying to understand historical records. So, what they've done is taken weather records from ERA5, which is one of the reanalyses, and you run ADCERC historically to produce historical surges. And he's comparing them with a rather limited record of measured surges that are from a network of titles. A network of tidal gauges set up by NOAA. And there's the network, 26 stations along the eastern coast US. And one of the things we're interested in is how would you adjust the ADCERC model to adjust it because you're sort of assimilating the NOAA data. And we're doing a model that's the same model as. It's the same model as Russell et al. I don't know whether I've got time to get into the real technicalities of the model, but it's the same idea that the Y vary, you know, mu sigma and psi, we just have three parameters in this case, vary spatially. We have a spatial model for the, this is now a latent process. So theta includes mu sigma and psi. It's a latent model. So that allows for one component of dependence. For one component of dependence between different stations, but there's a second component of dependence, which you know, this sort of model for just allowing new sigma anxiety to vary from station to station, that doesn't allow for the sort of temporal correlation that may say you have one big hurricane coming in and producing a huge, you know, a huge surge at a lot of different points simultaneously. And one approach to that model is through that. To that model, is through that sort of problem, is through Max Stable Processes. I don't think anybody's actually mentioned Max Stable Processes the whole week, which I'm a little bit surprised. I'm sure Anthony would have done if he had had a second hour for his presentation. But the approach we're using is something that I started working on about 20 years ago, where you model, you know, the theta hats are the MLEs from the individual stations. You model that as your smooth spatial process. That is your smooth spatial process, and then an error term, epsilon, and the epsilon term itself, you try to measure the covariance of different stations. Well, that's my predefined limit of time, but let me, I'm very close to the end of what I wanted to say. We do use this to come up with a projection. Now, this is still very preliminary. Brian, did this. Preliminary. Brian did this in about a week, but he's continuing to work on it. It's a projection for what this projected 100-year return level surface looks like. And these blue clouds you see at the top and bottom of the picture, they're meant to represent the limits of confidence intervals site by site. So let me conclude. I've talked about univariate series and spatial series, and I think with the And I think with the univariate series, you know, there are these problems about zero probability events. The spatial models, even if you're ultimately interested only in one site, they do provide a way of capturing the information from multiple sites. And although we haven't verified this aspect yet, I think using spatial models is a way to at least reduce the severity of this zero probability problem. So I'll quit. probability problem. So I'll quit there and be happy to take questions. Thank you.