Well, hello from North Carolina. And thank you, Leonid, for organizing this great meeting yet again. Really enjoyed it last time, and I'm really enjoying it this time. Yeah, so over the past few months, I guess, inspired by the amazing amount of genomic sequencing data we have for COVID and other viruses now, my group has been thinking a lot about how we might design optimal sampling strategies for genomic epidemiology or genomic surveillance more generally. Or genomic surveillance more generally. And just so everybody's starting on the same page, generally the goal of a lot of genomic epidemiology is to try and figure out or reconstruct transmission pathways in terms of the sources of infection or who's transmitting to who in a given environment. So just to start off with a concrete example, we might be interested in a healthcare facility figuring out whether new infections are occurring locally within, say, a given hospital award. Within, say, a given hospital ward, or if new infections are being imported from the general community. And so, what we'd like to be able to do is, based on the genetic similarity of different sequences, is figure out the actual direct transmission pairs in terms of who's infecting whom. But even if we think we might be sampling everybody, we rarely actually can. For example, there might be asymptomatic individuals that we just simply miss because we don't know they're infected. Don't know they're infected. And this incomplete sampling can create all sorts of biases when it comes to actually reconstructing transmission patterns. In this simple case, I've just highlighted example where, for example, if we missed this focal individual who infected these two individuals in our local hospital in blue, we might erroneously assume that these two infected individuals were infected by somebody within the hospital when, in fact, these infections were actually. These infections were actually imported from the external community. And this is one example of a far more general phenomenon. I think I would maybe even argue that this is the core problem that we're facing in genomic epidemiology as a whole, where we've made amazing advances over the recent years in developing all sorts of new and really great statistical inference methods to infer transmission dynamics from genomic data. But we really haven't put in the same amount of thought or effort. Haven't put in the same amount of thought or effort in terms of designing optimal sampling strategies that would actually allow us to learn what we want to learn about transmission dynamics from genomic data. And it's likely, I think, in all these settings that our inferences about the sources of infection or who's infecting whom are going to be highly biased. And another way of saying that, I guess, is that our inferences about transmission dynamics are likely always to be highly contingent or dependent on exactly who we sampled. Exactly who we sampled. So, we've been trying to think about very general ways in which we could design optimal sampling strategies for genomic epidemiology that would allow us to maximize the amount of epidemiological useful information that we can get from genomic sequence data while minimizing the cost of genomic sequencing. Arguably, now we have the ability to basically sequence as many pathogen genomes as we could want, but there might be better uses. Want, but there might be better uses for that money other than genomic sequencing. For example, in the U.S., we might want to use some of that money that's being dedicated towards sequencing now to, you know, maybe fix our crumbling public health infrastructure instead. So the approach that we've tried to use to learn these sort of more general ways of designing optimal sampling strategies is to use reinforcement learning. And so, if you're not familiar with reinforcement learning or RL, as I'll call it, Or RL, as I'll call it. The general goal of RRL is to design computational agents, or we can just think of these as models, that can learn how to make actions which maximize their long-term rewards simply by interacting with their own environments. So what makes RRL different from many other forms of supervised machine learning, in particular, is that in supervised machine learning, when we're training our learning algorithms, we often provide the learning algorithms with the Provide the learning algorithms with the truth of all the values or all the data instances in our training data set. You know, for example, if we were trying to train a classification algorithm based on a training data set, we would provide the algorithm with all of the true classifications for each data instance in our traded data set, in our trading data set. But in the case of RL, what we want to be able to do, because it's sort of more symbolic of what we're actually trying to do in the real world, is figure out optimal. Is figure out optimal strategies or how to play optimal actions by simply having the learner interact with its environment. And so in order to reformulate our epidemiological sampling process in terms of a reinforcement learning problem, we sort of recast our sampling problem as a Markov decision process on possibly incompletely sampled or observed transmission trees. And this has a And this has a distinct advantage then that we can use sort of the standard mathematical tools that we use to analyze all sorts of stochastic Markov processes when we're playing around with any sort of model of dynamical systems to study sort of this sampling problem. But just to give you the core components of these Markov decision processes or MDPs as we call them, we start by writing down the state transition probabilities of the model transitioning between any Transitioning between any two states, and in our case, our transition probabilities or transition densities just govern the dynamics of a stochastic branching process, right? So we can write down the probabilities of having a given event happen at a given time in our branching process, and then we can simulate from that model. But what makes these MDPs different from a classic Markov process is that we also then have to. Process is that we also then have to make a decision about how to act, or in our case, we have to make a decision about how we want to sample. So, for example, over each sort of decision-making interval, we'll need to decide who in our population of infected individuals we want to sample for genomic sequencing. And then the final thing that we need to complete our specification of the MDP is we need some sort of reward function that's going to tell us how we reward our learner. Reward our learner for playing particular actions from particular states. And for today, I'll just use the very simple example. We want to reward our learner for sampling direct transmission pairs in our transmission tree, because by sampling direct transmission pairs, we would arguably learn the most about who is infecting whom in the actual epidemiological dynamics. So in here, in this case, we would assign the learner two arbitrary points for sampling these two transmissions. Sampling these two transmission pairs in this given training interval. So, then the way that we can actually train our model or we can train our actor to sample optimally is we can formulate our action or in this case our sampling policy as a neural network model. Where in this case, the inputs into our neural net would be our observations about who is infected at a given time. This could be, for example, the number. Type. This could be, for example, the number of infected individuals or even how many infected individuals we've already sampled. Those inputs would get processed through several layers of the hidden network or the hidden layers of the model, and then it would output some decision about how we sample. For example, the outputs could be the sampling fraction or the number of individuals that we want to sample in each population. And so by formulating this in terms of this neural network model, we can then train the neural network to sample. Neural network to sample optimally by having it interact with a given simulation environment. So we can play many, many training instances. For each training instance, we sort of update all the parameters and weights of our neural network. And over time, what we hope for after many rounds of training is for our neural network to converge on something that sort of produces the optimal sampling policy, given that we're in any given sort of sampling state or any given state of the model. Of the model. This is what's known sort of as a policy gradient model in reinforcement learning. What we now know about this approach is that it generally doesn't work all that well. And that's for several reasons. The two that I'll highlight right now is that in most cases, learning is highly associative, right? So there isn't some universal best sampling strategy that will work across all time. Will work across all time and all space for all epidemics, right? Generally, who we want to sample will depend a lot on the given epidemiological dynamics at a given time, as well as who we've already sampled. And another thing that makes reinforcement learning very hard is that the rewards that we get from sampling are often delayed and very sparse. So we might have to sample over a long period of time, maybe throughout the course of an entire epidemic before we'll. Epidemic before we'll actually can learn whether we've succeeded in the task that we want to perform. So, the one solution that's sort of been proposed in the RL community is to instead of trying to directly learn the action policy that says how we sample, we sort of take a more circuitous but ultimately more effective approach where we try and predict the rewards that we'll get if we play a given action. We'll get if we play a given action at a given particular time. And this is known as a Q-value strategy. And so, to complete the model, I'll just say that this idea of needing to be able to predict the rewards that you'll get from playing a particular action is sort of what has allowed reinforcement learning in general to take off in recent years. This is just a quote from Richard Sutton and Andrew Barto, who are sort of like the experts in the field where they The experts in the field, where they say, you know, the most important component of almost all reinforcement learning algorithms is a method for efficiently estimating rewards or reward values. The center role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades. And if you're interested in RL, I would highly recommend this book because it gives sort of a great, really majestic overview of how reinforcement learning has been developed from the theory of Markov decision processes going all the way. Markov decision processes going all the way back to the 1950s and 1960s. So, what we need then is we need some sort of action or what I'll call a Q-value function that allows us to predict the rewards that we'll get by sampling in a particular way, given that we're in a particular state at a given time. And what these Q-value action or these Q-value functions actually try and predict is the long-term reward that we'll get for sampling in a particular way and continuing to use that sample. And continuing to use that sampling strategy thereafter. So we try and predict the long-term expected reward when we're predicting these Q values. And the strategy that we've used is that instead of representing our action policy as a neural network, we try and basically implement our Q-value prediction function as a neural network. So just as before, we sort of input our observations about the Of input our observations about the state of the system into this neural network. But what's output is instead of the actual actions that we play, we try and output what will be our long-term reward for playing a particular action or sampling in a particular way at a given time step. So this is just a very brief snapshot of how this type of what's known as deep queue learning might work in practice. Each training iteration, we would run some sort of Training iteration, we would run some sort of stochastic simulation. And then at each step of that stochastic simulation, generally what we would do is we would play an action or a sample in a particular way that maximizes the expected reward that we would receive based on our Q-value prediction. But with a certain probability epsilon, what we do instead is play a random action. And this is basically. And this is basically just to help our algorithm keep from getting stuck in sort of a local optimum where we might think that we've already found the optimal sampling strategy and we might want to just want to exploit the rewards of finding of using that particular policy from then on. But playing random actions every now and really helps us explore alternative policies as well. And then I won't go into the depths of how we actually train the neural network. I'll just say that we can. Neural network. I'll just say that we can use very standard gradient descent approaches that are often used to train neural networks in modern machine learning, where the general goal is to minimize or optimize some loss function that will depend based on basically the difference between the rewards that we expect based on our Q-value function versus the rewards that we actually received. So, over the long term, what we're trying to do is minimize our To do is minimize our error in our predicted rewards from our neural network. David, could I ask a quick clarifying question? Sure. I wasn't quite clear on exactly what the reward is. Early on, you said that it's not based on getting the sort of truth correct. But then I thought you said it's about getting certain transmission pairs correct. So, how do we know? Can you just explain how the reward works? Is this Aaron? This sounds like an Aaron question. Sorry. So I've left the reward function. So I've left the reward function sort of completely unspecified so far, but like in my simple example here, I guess I was arguing that it could be something as simple as just giving the algorithm sort of like arbitrary points for finding each direct transmission pair. But I'll explain in a more concrete example in a few slides. So what we've been doing to see if this Q learning strategy actually works is we've been playing Actually, it works is we've been playing around with simple almost toy models where we've been using stochastic, susceptible, infected, susceptible, or SIS models to see if this actually works. And so in these SIS models, what we assume is that susceptible individuals become infected and symptomatic with a certain probability sigma, but then with a certain probability one minus sigma, they become asymptomatic. And for simplicity, we'll just assume that all symptomatic individuals are. All symptomatic individuals are observed and therefore eligible for genomic sequencing, but asymptomatic individuals need to be tested first, or they needed to test positive before we would know that we should sequence them. And we'll allow individuals to be tested while infected with a given probability p-test. And then for all individuals, whether they're directly observed to be infected or whether they tested positive, we'll allow a certain fraction of S of those. Of those individuals to be sequenced. And so, this is what Erin was just talking about. In order to actually get this to work in practice, we need to be concrete or actually fully specify the rewards and cost of a given sampling strategy. So again, we're going to keep things simple here. We'll just assign a reward based on the number of transmission pairs that we've actually sampled. And here we're using a relative award where we'll assign a reward based on where we'll assign a reward based on how many direct transmission pairs do we sample versus the total number of direct transmission pairs that we could have sampled. And then we'll do something very similar for our cost function as well. So we assume that the cost of testing and sequencing each individual has a certain per unit cost. And so the total cost is just the sum of the total number of tests that we run and the total number of individuals that we sequenced. That we sequenced relativized or normalized by the cost of testing and sampling everybody if we could. And so, then the final thing that we need, what our ultimate goal then is to maximize the net reward, which we can think about basically as just the difference between our reward and the cost. This is just an example of how we can make this more interesting, how we could use more complex reward functions if our goal wasn't. Reward functions: if our goal wasn't to do something as simple as just finding as many direct transmission pairs, but say we want to do something more complex now, like infer a given epidemiological parameter, maybe R0 or the transmission rate between two different populations. In that case, our reward function should sort of quantify how much information we've gained by sampling in a given step. And so, if we think about this as sort of a classic Bayesian learning problem, what we can do is. What we can do is we can use the callback library divergence between our prior distribution, which basically says how much information that we know about our particular parameter before we've sampled more, and then how much information did we gain when we update our posterior distribution based on new sampling. So what the KL divergence basically allows us to quantify is the integrated difference between our posterior and prior distribution, which allows us to actually say how much information we have gained by incorporating. Much information we have gained by incorporating new samples. But for today, I'll just go back to our very simple and very stupid model, where we're basically just rewarding our model for finding direct transmission pairs. And so even without any fancy reinforcement learning, we can just begin to use this model to explore optimal testing and sampling strategies. So first, we'll just take a quick look at what the optimal testing strategies are. Strategies are, we can see that very quickly testing everybody becomes prohibitively expensive because, in order for the testing probability, the probability for someone to be tested while they're infected, we have to do more and more tests. And so the cost of testing everybody grows exponentially because we have to perform exponentially more tests to catch everybody. Meanwhile, the reward for testing in terms Reward for testing in terms of how many transmission pairs we ultimately are able to find only grows linearly, such that the overall net reward shows diminishing returns in terms of the testing probability. We can do the same thing then for finding optimal sampling strategies, where again, I'm just using sampling as sort of a stand-in for sampling for genomic sequencing. In this case, it's sort of the opposite as we Opposite. As we increase the sampling fraction, our costs increase linearly because we're assuming a same per sample cost, but our reward in terms of how many transmission pairs we find grows exponentially with the sampling fraction. And I hope this is somewhat intuitive. The reason why it grows exponentially is that if we've only sampled a very small number of individuals already, the probability that any new sampled individual will fall in a direct New sampled individual will fall in a direct transmission pair is very low, but as our sampling fraction grows, the probability that an individual, a newly sampled individual, falls in a direct transmission pair increases exponentially. And this means that our net reward or total reward also grows exponentially as we increase the sampling fraction, but with an important exception that I actually think is kind of interesting. So if we can consider different relative costs, Consider different relative costs of testing versus sequencing. And in this case, our cost ratio reflects what's the relative cost of sequencing an individual versus testing. We see as if the cost of sequencing grows more and more, we can wind up in this weird parameter regime where funky things start to happen. And actually, the lowest net reward we can actually receive occurs at intermediate sampling fractions. And so that means that. And so that means that if we start off at low sampling fractions, which I would argue is sort of the most relevant regime for most of us, you can only sample a very small fraction of total individuals. What will happen is that as we actually begin to increase the sampling fraction, our net reward will actually decrease, meaning that our sampling strategy is actually becoming less and less efficient in terms of rewards versus cost as we begin to sample. Cost as we begin to sample more. So, this is just a very basic insight from a very simple model, but it does suggest that if our goal is to actually find direct transmission pairs or find pairs of individuals that are very closely related to each other and the underlying transmission pair, sequencing more individuals, if we're starting from a very low sampling fraction, might actually lead to much less cost-efficient strategies. And the only way of sort of sequencing And the only way of sort of sequencing through the sort of dip in our net reward function would be to actually begin to sequence and sample, sorry, sample and sequence almost everybody in the population. But we wanted to use this model to just test to see if our reinforcement learning algorithm was actually able to find or return truly optimal sampling strategies. So, in this case, Maddie Brissel, who's a first-year bioinformatics PhD student in my group. Informatics PhD student in my group used a simple SIS model to first just systematically explore all possible testing and sampling fractions. And since we can define our sampling model just in terms of two parameters, we can do a systematic search parameter space and actually find the testing and sampling strategy that is truly optimal. In this case, testing an intermediate number of individuals, but then sampling them all is sort of the optimal. Them all is sort of the optimal strategy. And so we can see if our reinforcement learning algorithm is able to find this optimal strategy as well. And so what Maddie found was that even just by training our reinforcement learning algorithm for a very short number of iterations, our loss function very rapidly declines. Where again, our loss function is basically the error in our predicted rewards that we would get from playing a particular sampling strategy. From playing a particular sampling strategy, it almost goes to zero, suggesting that the algorithm is able to learn very efficiently, is able to learn very efficiently what reward we'll receive for playing a given sampling strategy. And then we can also see how our net rewards increase over time. And here over just like 200 iterations, again, we can see that the algorithm converges on some sort of local plateau of high total rewards. High total rewards, we wouldn't necessarily know if this is the actual true optimum or not. But in this case, of this very simple model, we've actually know what the true optimal testing and sampling strategy is. So we can look at the sampling strategy that the reinforcement learning algorithm has employed in, say, the last 20 iterations of the training algorithm. And we can actually see that the algorithm does, in fact, converge on the truly optimal sampling strategy. So, what I've hoped to show you today is that although reinforcement learning is new for us and we are sort of scared of it as well because we come from a much more traditional likelihood-based statistical inference world. I think using these types of machine learning algorithms actually allow us to very effectively and efficiently explore different sampling strategies. And so far, we've only really explored these extensively in terms of trying to do very simple things. In terms of trying to do very simple things like sample as many direct transmission pairs as possible, but what we're turning towards now is trying to use these same algorithms to figure out how to sample if our goal is actually to infer important epidemiological parameters like R0 or the transmission rate between different pairs. And so, if we use reward functions like the one that is described earlier in the talk, where we use something like KL divergence to actually quantify how much information we gain. Information we gain by sampling at a given time step, we can actually use these Q-value learning algorithms to, I would argue, predict what we actually or learn what we actually really want to predict, which is how much new information will we get about an important parameter value by sampling in a given way. And the final thing that we're trying to do now, which I think is really interesting, is that, of course, right now, before we deployed this in the real world. Before we deployed this in the real world, we'd actually have to make sure that we could train the model using simulations that reflected reality and we had a good model of epidemiological dynamics. But if we're exploring optimal sampling strategies for already well-sampled pathogens, say like COVID, we already have access to huge data sets. And that might allow us to use these reinforcement learning strategies to explore optimal sampling strategies. Explore optimal sampling strategies simply by sub-sampling from already very large data sets. And so, for example, for COVID, we could ask how well we would do, or could we find sampling strategies that basically perform just as well as if we had sampled everyone that we had already sampled, but at a much reduced cost. And with that, I will thank the organizers again and thank you all for listening. Okay, thanks very much, David. And we have time for a couple of quick questions, I guess. So anyone either in person or online? Oh, there's some movement in the CMO room. The microphone is moving around. Hi guys, can you hear me? Yes. Okay, great. Thanks, David. That's a great talk. Just wondering how easy it is to add in sort of additional demographic variation where your cost-reward ratio would be different, I guess. So, like, versus comparing sampling within a hospital versus sampling in the community, it might have great reward. It would have great reward identifying transmission pairs in the hospital versus transmission pairs in the community. So, can you sort of, how easy is that to include in the model? Yeah, so I guess the nice thing, but also the really scary thing about this framework is the choice is really ours in terms of how we want to reward our learning algorithms. So we're completely free. We're completely free to specify different rewards for sampling or identifying transmission pairs in different communities. I guess it's really up to sort of the designer of the reward function to figure out what their actual values are, what they're actually trying to optimize. We've thought about this a lot in terms of how we would sample for healthcare acquired infections and there, because we actually want to infer who's infecting who. To infer who's infecting who, we actually do really want to sample direct transmission pairs. But instead of weighting the relative reward for sampling transmission pairs in the community versus the hospital, we have been thinking about this in terms of different costs for sampling in the community versus sampling in the hospitals, right? Because if someone's infected in the hospital reward, they might just be sort of down the hallway from, let's say, a large sequencing lab. So they might get. Lab. So they might get readily sequenced. But what we rarely do is actually make the effort to, you know, go out into the general community and start testing and sampling individuals there. So my assumption would be that the cost of sampling in the general community would be much greater than sampling within a given healthcare facility. Thank you. Okay. Time for one more? Yeah, sure. So, this is another Aaron question. I'm sorry. I don't know if that's a good thing or a bad thing. But, Dave, really nice talk and very, very interesting stuff. So, it occurs to me that like a lot of times, you know, sampling for information is closely tied to actual intervention in an infection system, right? So, either the sampling, not only does it give you information about the epidemiology, but it also helps in the diagnostics or the individual who's sampling. Or the individual who's sampled gets moved into contact precautions or something. Can you say a little bit about what would be involved in factoring those kinds of things into the mix, right? Because I suppose, you know, removing somebody from the infectious pool by sampling them has an impact on what you see subsequently, but it also has a benefit of, I don't know, I can't think it all through. Can you help? No, that's a really good question, Aaron. So I think it's a good Aaron question. So I think it's a good Aaron question. And I mean, the thing that I really like about these formulating everything in these reinforcement learning problems in terms of a Markov decision process is that we have the ability to do exactly what you just described. So in the classic world of Markov decision processes, which were originally used to design sort of like optimal control of dynamical systems, the transition probabilities The transition probabilities might depend not only on the state of the system at the previous time step, but then what action we played at that previous time step. So in a case where sampling might be tightly tied to direct intervention strategies, we could imagine a case where testing and sampling a given individual might also mean that they're put, say, directly on anti-retroviral drugs or any kind of antibiotics. Drugs or any kind of antibiotic drug. So, actually, by sampling them, we might remove them from the infectious population well. And that's completely allowed for in this sort of MDP world. And then, in terms of our reward function as well, we might not only want our reward function to sort of reflect the informational rewards that we get by sampling in a given way, but we might want our reward function. But we might want our reward function to sort of also consider the advantages or the benefits that we get from directly intervening as well. So maybe we get one point for identifying a direct transmission pair, but we get an additional, I don't know, billion points for actually preventing new infections, right? Like if you can completely, it's completely up to the user in the end how they want to specify the reward function as well. As well. Great, thank you. Thank you very much, David. We will leave the other questions for the break out.