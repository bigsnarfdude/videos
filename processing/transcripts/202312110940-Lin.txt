So, I will share with you this work, which will appear in GISSP. So, this work was led by my former postdoc, Yao Wu Liu, and so he's currently a faculty at the Southwestern Financial University in China. And so, let me give you a little motivation why we are interested. Why we are interested in this. And so, this is an overview of the whole genome sequencing timetable. And so, about 15 years ago, over 23 years ago, that was the completion of the human genome. At that time, only one person was sequenced. The cost was very, very high. And about 15 years ago, that was a completion of the 1000 genome project. And though at that time, Project, and though at that time the cost was still high, and the depth was pretty low, and though it was only about over 2,000 people sequenced. And in the last couple of years, the sequencing cost has dropped dramatically. So multiple large-scale whole genome sequencing programs have been launched. And so in the US, the most well-known one is called the Top Medical Scale Trust. Is called the top math use called transomics concision medicine program by the National Heart Lung Blood Institute. So we have a sequence of about 200,000 whole genomes and with many different heart lung related diseases such as cardiovascular disease and also the traits such as the lipid. And another large-scale whole genome sequencing program in the US is called the genome sequencing program. So we have sequenced about 350. We have sequenced about 350,000 people and with many different diseases as well. In the last couple years, several large ball banks have been launched. The most well-known one is called the Euclid Ball Bank. And so it has integrated the genotype data with the electronic medical record. And so it has released the GWAS data and just two weeks ago. Data, and just two weeks ago, it has released the whole genome sequencing data of a half million people. And so, the other one called the Medium Vision Program that planned to sequence about a million people. And the third one is called AlphaS program that also planned to enroll a million people and also sequence a million people. And so, the Alphas program used to be called the Precision Medicine Program. Medicine program. So that is where this term came from. And so it was announced by Obama and Nutrition. So basically, there are lots of whole genome sequencing data being generated and so in the next few years and we are going to see millions and what tens of millions of whole genome samples. And so therefore, we need to be ready for this type of analysis. So and also makes the analysis scalable. So give you a sense. So, to give you a sense how big the data we deal with, and so this is a top math risk. This has sequenced about 185,000 cogenomes. So, that is 185,000 samples, subjects, and with different diseases and traits. So, before the QC, there were about a billion variants. And so, in China, In Chinese, it's about Xi Yi. And after the QC, they are about 845 million variants. And the majority of the variants are very rare. So like about 45% of the virus are single-ten. That means among those 200,000 people, only one person have that variance. And 16% are double-ten. That means two people have that variance. Have that variance, and only less than 1% of variants are common variants, which are covered by the GWAS chip. And so, therefore, to analyze those data, the scalability is critically important to us. And so, we want to scan the genome quickly to identify the variant set, or we call the mask associated with the disease and treat. So, we are not interested to whether. interesting whether to whether the whole genome associated with the diseases that has no practical meaning. So one want to see what the variant set or the mass associated with the diseases. So basically the functional category of a particular gene, the variants in that functional category is associated with diseases. And we do this a lot of time across the genome to cover the whole genome. And to set this up, so this And to set this up, so basically the statistical framework is not that hard to write out. So basically the why is outcome in genetics called the phenotype. So for example, think about LGR. This is the bad cholesterol. And so lower is better. And then C is a co-virus, age, gender, and ancestry, PC, and X is the P genetic virus. Genetic variance. And so, in a set, for example, if you think about FOE and there's a promoter region, suppose there are 100 variants, and we want to see whether those 100 variants are associated with the LDL. And so the variants are likely to be correlated, which can be illustrated by this LD map. So that means if the two points here are kind of dark, these dark, that means these two ones are correlated. Are correlated, and so I'll debase this test station called the correlation in genetics called the latest disease equilibrium. And so, so basically, the design matrix is very sparse. And so, if you look at this example, suppose you have 100,000 people, and then you have 100 variants of FOU promoters. And so, for each, you can see for each column, and so the values are 0, 1, 2, basically, the genotype. True. Basically, the genotype have one from mom, one from dad, and the numbers indicate the number of minerals. And so, if they dark, that indicates the values of zero, and if they bright, that indicates the values of one or two. So, what this tells us is like many of the values are very rare. And so we need to analyze them in a set and based on the biology we define using the mask, like using the The mask, like using the promoter enhancer or the loss functional variant, and so on. So, the model is not hard to write. So, basically, suppose you have a continuous trait like LDL, then you regress LDL on the covaris and also the genome code. If you have a binary trait like cardiovascular disease, you write out the logistic model. And so, the goal is to test there's no association between the variance in mask and the And the L D over the tree. So basically, the null hypothesis of beta equal to zero, alternative beta not equal to zero. So, therefore, this is the testing is the hypothesis, the traditional composite alternative test. And so the dimension of beta generally is not small, but it's not huge. Generally, 100 or 2000 and depends on how big the mass is. And so this basically. And so this basically presents an issue that how can we develop a scalable and powerful test for this type of problem? And so traditionally, one would think you can do the likelihood visual test. But the problem is that in this type of setting, there's no uniformly multiple partial tests, as we know. The UMP doesn't exist. So if the dimension P is not that small, and so suppose the P is 100, and so what. P is 100, and so 100-degree freedom test is may not be that powerful. And so, so the so how can we create a more powerful test? And because the UMP test doesn't exist, and the likelihood we show test and it's not that powerful across the range of alternative, especially the dimension is not that small. So, the question is, how can we improve the power over the likelihood ratio? Is a likelihood racial type of test, a water water or score test. And to set it up, so we can start from this kind of a burden test, which is a traditional way that people use in the whole genome sequencing study. And so pick some example, if you have y requests of y on x beta. And suppose you assume that the beta have the same sign. And so the beta, I think all the variables in the So that means all the variants in a mask are all protective or are harmful. So that means the beta are either all positive or either are all negative. And so generally, this can be true if one considers the loss functional variance of the gene. And so in order to set this up, and so the test is not that hard to do, one assumes that beta is the same. So if the beta is the same, then you take the beta. The same, then you take the beta out, then this becomes a one-degree freedom test, and this can be easily constructed using the spore statistics. And so basically, that is a correlation between X and Y. And you just calculate the sum of the marginal spore statistics. And this is one degree freedom test. And that's pretty easy to construct. And so, but the problem with this is that if the betas are in different Are in different directions. And so some beta are positive, some beta are negative. And then it loses power. And so to overcome this issue, so we propose over 10 years ago, we call this kernel machine test and this basically is a stat test. So that allows the beta to be in different direction and some beta can be negative, some beta can be positive, and assume the signal to be weak. Generally, that is true in genetics. Weeks generally that is true in genetics. And so this test basically sets up a distribution for beta with a new zero and a variance tau. It doesn't need to be a normal any distribution. You just need to specify the first two moments. And so to test this setting, and this converted the multi-dimensional testing problem to a single-dimensional testing problem, testing the balance component equal to zero. zero. And so the challenge here is the null hypothesis tau equal to zero is on the boundary of the parameter space. So then by doing this test, the press statistic is not that hard to derive. It is suppose you have the marginal score statistics and then this test basically is the sum of the score statistics squared. And it doesn't follow a regular chi-square distribution because chi-square distribution because x are correlated and they follow mixture of chi-square. So this can be easily calculated and using the Davis method. So this step method has been since we proposed over 10 years ago has been widely used in the analysis of real virus in the whole genome sequencing study. And now after 10 years and we ask ourselves and so can we do something better? For example, suppose For example, suppose we know the beta are in the same direction, and is the burden test the best test we can do? The answer is no. The reason is it assumes the beta to be the same. Suppose the betas are different, which is generally true. Even the beta have the same direction, they are 100 beta all positive, or they are all in the same direction, but the magnitude they are likely to be different. Then, how can we create a task and Can we create a task and which really matches this type of reality and it's better than the burden test? What is the burden test the best you can do? What the burden test can be allowed a choice? So the question is, how can we develop powerful tests and if the beta is the same sign with the next different value? So you can see this is a constrained testing problem. So basically you constrain all the beta d top. Constrain all the beta be deposited and then to derive the magnetic racial test in this type of context, it is complex and because of constraint maximization problem. And also the scalability is critically important to us. So this is just not scalable. And so then we ask ourselves, can we do something fast and scalable and quick? And also it has to be accurate in the pill. Has to be accurate in the tail because we do this test a million times across the genome, standard genome. So generally, the type of error rate we are interested in is like 10 minus 7 is small. And so now let's look at the learning problem. If we think about prediction, and so there are two approaches to be part of a learner. One is that to find a single best learner, and so generally it's a mathematical And so generally it's mathematically difficult and also computationally intensive. The other approach is to do ensemble approach. Generally, this is very effective in practice and computationally much faster. But those type of learning problems mainly focus on prediction. So we are not interested in prediction. We are interested in the inference in particular testing. And so we ask ourselves, can we find a simple best test? A simple best test that can improve the power in this type of settings. And but the betas in the same direction, but the magnitude are different. And so theoretically, finding a simple best test generally is mathematically challenging and also confusionally expensive. We cannot afford it and for something too expensive because we have to make everything quick. So this basically motivated us. So, this basically motivates us to explore an ensemble test for interest, in particular, for testing. So, how can we do that? So, let's review ensemble learning. So, ensemble learning is a classical machine learning method for prediction. And so, the core idea of ensemble learning is to create a bunch of phase tests. And so, then use those face tests to construct a finite. Base type to construct a final ensemble learner. So, the basic idea is the basis, the base learners are generally weak, and however, the final learners are strong. So, basically, this basically is equivalent to this Chinese idiom saying that the wisdom of the Maces is seed that is the wise. And so, this is a traditional example of ensemble learning. Of ensemble learning with the random forest. So basically, random forest, and so every tree is a weak, the tree generally is a weak learner. So in order to construct a random forest, so basically when you do the bootstrap and then when you do the bootstrap, generally for each bootstrap sample, suppose you have 100 variables. And in order to construct the tree, generally one doesn't use 100 variables. Use well-hattried variables. And the reason is we need to make the tree sufficiently independent. If the trees are too correlated, and then the prediction performance will not be good because the variance is too good. So generally, one selects the root n variable to construct the truth. So if you have 100 variables, you choose the 10 variables. And so for each randomly selected 10 variable, you build a true. Then afterwards, you are sample. Then afterwards, you ensemble. So, in order to ensemble, you basically average the tree. Suppose the B is 1,000, you average 1,000 trees. And so generally, this random forest strategy is better than backing. And so the backing basically assumes the M equal to P. So if you have 100 variables, you lose 100 variables for each tree. And so the backing basically makes the trees correlated. Trees correlated, and so the variance is too big, and so therefore, the prediction performance is not very good. So, the question we ask ourselves is given the success of this ensemble method for prediction, how can we develop a method for hypothesis testing? So basically, we want to go from the ensemble prediction to ensemble inference, and in particular, ensemble testing. So, if you just look on the If you just look on the face, and this we need to answer two questions. And so basically, we needed to construct base tests and also we need to ensemble the test. And so the question is, how can we construct the base test and how can we ensemble the test? So in order to do that, let's review ensemble learning. So how can we do base learner? And so in random forest, so basically what you Forest and so basically, what you do is to do the random tree as a base liner. So, the random component is you go through the bootstrap and also select a subset of the feature to construct the tree. And now, when we do the ensemble texting, and then how can we construct the base test? And so, the idea is we are going to introduce the random width. week and so then for different problems and the um the base tests are different and the um so we'll be problem dependent i will illustrate that and so basically the random um the base tests are we are going to introduce the random weight and so in the test and so construct the base test and now how can we ensemble so to so to start So, to start, let's review our sample learning. So, in ensemble learning, the unnamed following is to ensemble to a tree. So, basically, what we do is we average the random tree. And so, for the regression setting, we basically do the average the tree. And for classification, one can do majority both. And now, the question is: how can we ensemble the base test? And so the And so there are a few issues. And first, the test statistic for each base test, you can construct a p-value. Suppose we do this base test a thousand times, then we are going to have a thousand dependent p-value. And so generally, most of those phase tests and the random widths are going to be long. And so if we, then that means many of the p-values are likely to be big. So if we just add. fit so if we just average those p-values that would be a lousy idea because only a small number of those random weights are likely to be the right weight and so and also those p-values are calculated from the same sample so they are correlated then how can we ensemble those correlated p-value and so what we are going to do is we are going to use this a cad method basically this causing combination method to ensemble correlated To ensemble for it to the p-value. So, this will give us the final ensemble test basis. And so, what is the cause, the ACAP, this causing combination method to ensemble the correlated p-value? And so, this is the, so let's review this the p-value combination method. The traditional way to combine the p-value is to use. The p-value is to use a fisher method. And so, suppose the p-values are independent, and then when you do the minus log p, and this will follow a chi-square distribution when they are correlated, when they are correlated, when they are independent, and then the feature combination method will have the degree freedom to be. To p and the chi-square distribution. However, if the p-values are correlated, the fission method will become much more complex, and this is not a good way to go. And so in many settings, and so when you deal with a composite alternative, generally, because we don't have UMP tests, and we can now try different tests to see which test gives me the smallest p-value and pick that one. Pick that one, and this is a cheating. And so, this costive combination method really creates a beautiful way to combine probably the p-value and the computationally very quick. So, the basic idea is you have a p-value of any test you want to use. And so then you take the time transformation, this will become a cost-e-regularity. And so, one of the most of the time we don't like costing, it doesn't have a We don't like cost, it doesn't have a mean, it doesn't have a virus, and it has a high lead field distribution. But because this highly field distribution has a disputeful property that no matter how correlated those p-values are in the tail, and they still follow a proxy distribution. And it doesn't, you don't need to worry about the correlation because most of the time the correlation among those correlated p-values are difficult to calculate. And also the choice distribution are difficult to calculate. Also, the joint distribution is difficult to calculate. By doing this Causy configuration, and so we just need to use the Causi distribution to calculate the value. And so this is really, really convenient and also computationally takes less than a second. So very quick, especially in the field, calculation is very, very, very, quite accurate, especially to 1096, 1097. And so this And so, this APAD was first proposed by my former post-doll Yao Wu Liu and in our American Journal Human Genetics paper in 2019. The later on, the theory was republished in the JASA paper. So, this is very convenient method if you want to find different correlated p-value if you don't know which edge guys. And now, the natural question is: how can we choose the feed? And for the ensemble test, And for the ensemble testing, and then obviously what you do is if you are interested in prediction, and you just select the feed and the number of bootstrap in random forest until the testing arrow becomes stabilized. So in the testing problem, so basically what we do is we choose the number of phase tests until the ensemble the p-value becomes stabilized. And so then that will be clear. Now, how can Now, how can you construct the base test? And so let's start from ensemble burden test. So recall the problem with y equal to x beta. And then I have the marginal score statistic. So this problem, and so here we assume the betas are in the same direction, but the magnitude can be different. Suppose I tell you the true beta, then I can think of the true signal. Then I can think of the true signal direction. And if I know the true signal direction, and then I can project the score statistics, the one-dimensional score statistics to the true signal direction. And then this is a UMP test. And so in reality, this type of Oracle test cannot be constructed because I don't know the truth of version. And now, if I think about all possible And now let's think about all possible directions. And so, basically, all possible direction for beta that basically will be in the first quadrant. And so, that basically on the unit circle in the first quadrant. And so basically, if I pick a direction in the first quadrant, if that direction is close to the true signal direction, then I'm going to have the best projection. Have the best projection, then the test will have the best power. And so many times I don't know the true signal reaction. The idea is I'm going to randomly simulate the wind in the first direction. And so let's take a look at this burden test. So what this burden test really does. And so basically, if the true signal directions are known, suppose I randomly pick a direction. And so then that basically will be the first quadrant. Basically, it will be the first quarterly. Suppose all the data are positive. So, what is the burning direction? The burden direction, assume all the data be the same. So, that basically the 45-degree line. This is this blue line. And then you project your score statistic to this 45-degree line. And so, if you reject it of the burden statistics large, and so the natural question is whether the burden is a good choice. Burden is a good choice. And so, definitely, you can see if the true direction is this 45-degree line, and the burden direction will be the best direction. But if the true signal direction is here, the true vector direction is this way, then the burden will be off, and then it's likely to lose power. And so, then the question is, when does the burden have no power? Even the beta have the same sign, both beta are positive. Is that possible that? Is that possible that burden will be a lousy choice? The answer is yes. If the correlation between the axis and negative, and even the beta in the same direction, and then the burden test can have little power. So to show you, suppose that we do a rotation, and so we rotate the true signal direction to the horizontal line. And so this is the line here. And so, this is the blue line here. And then, the suppose we have 22 signal direction after the location, you can see that if the correlation between x1 and x2 are negative, and then the burden direction can be quite far from the true signal direction, even the beta are positive. So, then you can see in this situation, the burden will have little power. Then, what can we do? And so, because in the act. And so, because in reality, we don't know the true signal direction. So, what can we do? And also, it's difficult to find the true signal deduction analytically and quickly. So, what we are going to do, we are going to ensemble. So, how can we ensemble? So, basically, what we do is that we first simulate the W, a simulation called a thousand W, and in the first quarter. And so, how can you do that? So, basically, you simulate normal zero, one. You simulate normal zero one, and then you normalize it, and then basically then you will have the W that basically will be the on the circle. This is the polar polar coordinate on the circle in the first quarter. And so you can see many of those random directions are likely to be wrong, but that is okay. And then what we do is we project the score statistic to each random direction. And so we do that. And so we do that a thousand times. So we, then most of those four statistics are likely to be lousy, and because most of the random direction is likely to be wrong. But as long as one or two, a small number of random directions are right, and then we will be good. And so that is where this A-card comes in. And so you can see for each projection and the random projection, we can copy the random projection we can copy the p-value all those p-values are correlated because they are populated from the same data set and so the a cat basically they just use the causing combination method to confine confine those 1000 p-value so a card combination is similar to the minimum p-value of test statistic and so so basically it tried to find out the which p which one Which one did you use the smallest p-value? But if you use the minimum p-value as a test statistic, generally its p-value is difficult to calculate because you need to know the correlation. But by using the A-cut, I don't need to worry about the correlation. And so that is the beauty. And so basically, you basically find out which projection gives you the smallest p-values. And then this is what it adds in the combination. And so, therefore, you can. And so, therefore, you can have among the thousand random projected score statistics, many of them are lousy, but that's okay. As long as there are few directions, close to the true signal direction, it kind of will basically catch it. And then that basically gives me the final p-value. And to show you, so here is the correlation between the, suppose I have a p causative p and I have a changeable correlation, if the correlation is positive. Correlation is positive, then you can see the ACAD under the burden have a similar power. But when the correlation have some negative, even you can see the small negative correlation and you can see ACAD, the ensemble test really help a lot, especially for small ARPA, which ARPA level, which we care about in the whole genome sprint, you need to do the multiple comparison adjustment. For comparison adjustment, you can see the power improvement is significant. And so, also, it has a nice statistical property. So, basically, we show that it has good, to use this ensemble version test, it has good efficiency. So, basically, if we let the ARPA level go to zero and then the version tasks and the And the power difference is very little. And so basically, it approached the oracle test with high probability. So that is good. So basically, ensemble burden test is near, but we are optimal. So that is nice. And to give you an insight of why this works. And so this. And so, this basically is the spirit is similar to the random ensemble learning by reducing the balance. So, basically, you can see if you do the base test and you project the source statistic to those assaults in the random direction, because many of those random directions are likely to be wrong. And so, therefore, you can see this base test a weak test. And so there has a Test and so that has a large variance. But when you assemble them using the AP path, and you can see the values much smaller, and so that improves the power. So that is good. And so basically, to do the ensemble burden test, you first generate, say, a thousand random directions in the first quadrant. And then you project your score statistic to that random direction. Statistic to that random direction. And then you do this a thousand times, but each time you calculate a p-value, and then you ensemble them using the APAP. And so this approach assumes the beta are in the same direction. So what happens if the beta are not in the same direction? If that is the case, the second step will lose power because some scores are positive, some scores are negative, signal cancels out. So basically what we need to do. So, basically, what we need to do is to replace the second term by the statistic if the betas are in different directions. So, we basically project the score in the whole circle instead of the first quadrant, basically. And so you can see that basically replace the second term by the quadratic statistic that follow a mixture of chi-square distribution. And also, this is the random projection as well. And then the rest of the step will be the same. Then the rest of the step will be the same. And we can replace this random weight. And this is a diagnostic by incorporating biological information. And in genetic, generally, we incorporate a new frequencies with functional annotation. So both the burden task and also the stat task are powerful when the alternative affects. Alternative are dense. And so when the alternative are sparse, so basically, then we need a different test. And so basically, if one considers the dense regime from the from under the sparse regime. And for the sparse regime, so basically the alternative is you assume that you have majority of the parameters are zero. But you don't know which one are zero and you don't know how many. Are zero, and you don't know how many. And so, one approach to handle this type of sparse alternative is to do the higher critism test. But higher criticism, the convergence rate is really, really, really slow. It's log minus low. So it is too slow. I need the P to be super, super, super large, like it's a 10-seventh. But in reality, we don't hear that huge P because in genetic study, we hear. In genetic study, we hear about modeling the p because we want to study the genome to find out which gene was genetic and what the mask of the gene associated with the diseases and the treat. And so therefore, the asymptotic hierarchism is not useful in practice. And so we are interested in the P model. So therefore, what we do is we are going to do a random subset test, ensemble subset test square test. set by square test. So basically suppose I have a piece of 100, 100 values. And so each time I randomly select a pin. After I select the 10 and then what we do is I construct the scorecast using these 10 variables and then I do that a thousand times and then I have a thousand p-value sample. And so you can see computationally this is very quick. So then the natural question is how can we So then the natural question is how can we choose the M? And it turns out if you just take the square root of P, it works pretty well. And if you take square root of P, so basically this separates the dense regime from the sparse regime. In reality, we don't know what the true number of the signals is. And so suppose that is the true signal is sparse. It's sparse, sparser than square root of a p, how much power are we going to lose? It turns out it doesn't lose much power. So if you look at the example on the right, suppose the p is 100, I choose a 10 variable. And the x-axis is the true number of signals. So if the signal is really, really small, the number of signals is very, very small. And so like one, only one of them is a signal. So you can see the right curve. So you can see the right curve is a high-criticism test. You can see it is powerful when the signal is really, really extremely sparse, but lose power quickly. And then this green curve is the chi-squared test. And so generally, this is powerful when you have the dense signal. So you can see when the signal is really sparse, chi-squared has lost power. But signal become denser, chi-squared has gained power. And this purple curve is the And this purple curve is the Berkshung test. You can see that it's better than the higher quedition test when the signals are moderately sparse. And this blue curve is ensemble chi-square test. You can see it has the fastest power and most robust across the range of the number of signals. So that is good. So we applied this under to the RIC coaching. The RIC whole genome sequenced study analyzing the LAPO protein. And the population sample size is 2,000 with African Americans and 55 million variants. And the covary include age, sex, and 3 ancestry PC. We focus on the real variants with a loose frequency less than 5%. And we do the sliding window of the genome. And then in order to get And then, in order to decide how many random tests I should do and to choose the key, and so we increase the number of these tests until the ensemble test becomes stable. And so you can see it in this curve. And then when the B equals 250, it becomes pretty stable. And so we choose a B equals 1000. So why do we have two curves? And the reason is in genetics. And the reason is in genetics that call real valence analysis, and generally we upgrade the real values and the conquer the common values. And so we use all of frequency, the beta function of a loop frequency at the width. And so basically, if the beta one is one, that's basically we give all the variance equally. The beta 125 will give the more real values a higher rate. So here's the finding. And so let's look at this one. So let's look at this one. And so if you look on, if you use burden test, and you can see that if one does ensemble test, you can basically increase the discovery by almost five times. And if you do the SCAD test, you can still increase the number of discovery by 50%. And the other test I do not have time to talk about is the remote. Have time to talk about is the remote test. And so this is the minimax test. This paper was passed in JASA about two years ago. And so this already has some kind of confined test spirit. So therefore, this ensemble test improvement is smaller. To summarize, ensemble tests is an alternative to the direct approach of analytically finding a single best test. A single base test to maximize the power and it provides a flexible and flexible, robust way to boost the power of the weak base test. So basically, the idea is to introduce the random weight in the base test and ensemble them and using this APAD, this cost combination method. So this is quite convenient and really fast. And I hope. And thank you. Thank you.