I know that it's a former speaker. Okay, what is a multi-index function? Uh you will never know because this is not working. Okay, so now we're going to do it the old way. Alright, so this is it. You know, you take a vector in Rd, let's call it z, and this is what you're allowed to do. You project it with a matrix, which is d times r in r dimension. In R dimension, and then you apply a function. A function that goes to R R to R. So you are taking this R direction and you're outputting one factor. It's not a neural network, by the way. Because of course I can take these two and multiply them and do all that kind of stuff, which I could not do. So that is a multi-ended function. And well, let's think about. Well, let's say now I'm giving you a data set, giving you many vectors. Of course, I will take them Gaussian. I'm giving you many such y. And let's say you are working the limit when d is large. Can you learn the direction? Can you learn the w's? W stars. Alright, so just for concreteness, I can consider the single index example, so only one direction. So very simple example. Let's say z star is the projection. So we'll use always these notations. So z star is a projection of your vector, projected on W star, the matrix, or in that case a vector, because in one direction. So, okay, a linear model is a good example. Phase integral, you know, the projection in one direction, and when you take the absolute value. It takes absolute value. Actually, this is not phase retrieval because it's a real problem, not a complex one. Perception, that we all like. Okay, so you can even have some noise. This function can be stochastic. That would be the sign of this star. And then if you have multi-direction, the multi-index problem, well, you have the I star, so let's say you have three directions, z1, z2, z3. So you project on these three directions, and then you do whatever you want. You do whatever you want. Z1 plus actually value Z2, Z1 star plus 2, Z2, whatever, polynomials, or you don't have to stick to polynomials. You can even have something like a neural network of noise. Okay? So it's an arbitrary function of a bunch of linear combinations. Yeah. And the goal is just to find the linear combinations. Well, yeah, you can also try to learn entirely. You can also just try to learn entirely the function, which is also super interesting, but I will be very modest today. Just learn the direction, learn the surf space. Actually, once you do that, the rest is the easy. Okay, so how many samples do we need to learn W star? So there are many, many words of these, just citing a bunch of them. And I mean, if I would be standing here three years ago, I will tell you all about information. Tell you all about information theoretic bound. Let's say that we know this function, and the only thing we have to learn are this direction. Well, you have to be Bayesian, and you compute the expectation, the posterior distribution on W given the data. In fact, I did this years ago. Some of you probably know this work. For single-index model and multi-index model, and you can compute all this stuff, you know. All this stuff, the morale of the story is that essentially you can order this support, necessary, and for any function I could think of sufficient. And you can do this nice stuff that many of you are also familiar with, where you can compute exactly the best overlap you can get with any direction. That for the perceptron, this is for phase retrieval. And then there are all these discussions that we have a heart phase, what kind of Phase, what can algorithm do, and so on and so forth. Okay, so very nice. But that's not what we are going to do today. Alright, so assuming the function is known, it's maybe a bit too much. And you all know that computing this post-disale arbiter is super hard. I mean, of course, in practice, we have IMP. And that's probably why I'm here, because I did this work. But come on. Come on, it's 2024. Who cares about BestCap? The only thing my student cares about is neural network and gradient descent. And even Kokosh now is doing gradient descent. So of course, what you want to know is if you just use valid gradient descent, not any of the stuff we learn in CS. Does this work? Right? So what would you do? Well, take maybe not you. Take, maybe not you, but certainly my students will say, Well, of course, you're not going to learn any of these crazy algorithms, we're just going to use a neural network. Okay, so Z is going here. Now you have a matrix W and you project in P dimension or P neurons. Of course, you don't know what R is, so you're going to take P very large probably. And uh well, and you're going to try to predict in the second order. Going to try to predict the signal error. And how are you going to do this? Doing gradient descent. Of course, it might be just one sample at a time. So let's forget about the second layer, okay? Because we just care about the first one. So let's say this is fixed. So we just do gradient descent on the doublet. Okay, so S G D one sample at a time, or if you want to be fancy, you can even have patches. You can even have patches, size and b. So before just updating, you are collecting your gradient with 20 or d squared, d to the cube, whatever, and you're updating. All right, so that's what we want to do. You have the teacher or target function, which is a simultaneous function in R you know, from R to one, and we are trying to learn it by just doing random descent with uh an element. With an element. Okay, so for future reference, that will be important. Turns out, perhaps not very surprisingly, that what is really important to understand what's going on is Hermite decomposition of the target correlation star. And so if you decompose, well, not in polynomial, but in Hermite polynomials, which is almost the same thing, the role of this coefficient. The role of this coefficient of your target function plays a fundamental role. And in particular, there is a notion that was introduced by our friend here, and that turns out to be fundamental, which is a notion of information exponent, which is essentially, if you have one direction, what is the order of the first non-zero coefficient? Okay? And that was generalized to a multi-direction. It's called the leap impact. It's called the leap impact. Okay, so what do I want to tell you? I want to tell you a little story with three parts. Part one, what happens if you do one kadi step? Part two, what happens if you do many kratz steps? Part three, what happens when you do many kratten steps with a measure plot twist? keep you entertained. Right? Okay, let's go. So, first one Learning with One Giant Steps. It's based on this work which we got on archive last year, but you know, I need to acknowledge that most of the story, and it was very English, you know, that was where we took the ideas, is coming from this two nice set of work by From these two nice sets of work by Alex Damien, Jason Lee, and Medien also by Paul People. Okay, so here is the first thing. You do only one gradient step, okay? But of course, you're not doing LGD with one, just simple. At this time, if you just do just one gradient step, you probably want to have a large batch. And everything will depend on how much you are waiting before doing update. Alright, so this is what's going on. This is a Right, so this is what's going on. This is the size of the batch, and this is what is uh is going on. First of all, uh if your batch is too small, little O D, you're not learning nothing whatsoever. There is no learning. If you look at the overlap between weight after one step and the truth, basically Basically, they have learned nothing, and if you normalize the overlap norms, you have this easy. No projection towards the teacher. Okay, what if you do something larger? Okay? Well, if you are larger than OD, then suddenly you have an overlap. You are learning something. But interestingly, what you are learning is limited to OC. You're learning is limited to a single direction. You cannot learn two directions. And if you're thinking at three directions, maybe you're just learning a single combination. So it's still maybe not that impressive. And you really need to go beyond this. You really need to go to a larger than d square. It's required in order to learn more about one direction. So then you start to learn more direction, and some direction can be tricky, right? And so this is where these information exponents are interesting. If you look for a particular direction, the order at which it has a first non-zero coefficient, let's call this information or leap exponent, well, it turns out you need d to the l in order to correlate to this direction. So we are really happy. To these directions. So they are really hard directions. It's really, really a lot of data if you want to learn them in one gadget. By the way, you might think it's too big to learn in one gadget instead, because why would you do this? Well, you might want to do this because it's efficient on a computer. On a computer, it's very easy to distribute the computational gadget on billions of computers and then have a radar. People do this. Right? Distributed computing. That's how you train an LAT. So it's not completely crazy. But it turns out that, you know, some directions, the one that have is. Some directions, the ones that have this nasty information exponent, require really, really big gradients. And essentially, you know, individual direction with coefficient L require a gradient B to the L. Okay, so that doesn't look very efficient, but that's what happens when you do just one step. Okay, so that's the mathematics. That's the mathematical version of what I just told you. What we are saying is a statement about these overlaps. And if you are not learning, M divided by Q is small, otherwise they are large. And you can even be full rank and learn everything if you have one radiant if you have a batch large enough. Alright, so you can learn this stuff in one step, but you need a really huge. step but you need a really huge really really really huge constant step a number of batches so to have a good devat so that looks very inefficient okay so of course nobody will do that that's not entirely true certainly my students who recode this in my machine learning 101 class will never do this they will say no I'm doing many steps right many many steps and averaging gradient and I'm repeating gradient descent many many times and that will Many, many times, and that will certainly work better. Okay, so let's see what happened in this case. Again, it's based on two sets of works. And again, the story here was mostly solved in a series of two interesting papers. The first one by Gerard Deza and Lokoche for single-index model, and then later on by Emmanuel Ab√©, Enrique Boisera. Alright, so now, and this is crucial for the rest, I'm doing many steps, but each new steps I'm doing into a new fresh batch of data. I'm never repeating anything. So ever I'm doing one sample at a time, and so you give me a new sample and I'm updating, I'm never repeating it. I'm never repeating it, or I'm doing it with large batches. So let's say NB is order D, like a big batch. But every new batch is completely new. I'm never reusing anything. Why am I doing this? Because it's easy. I mean, it's not easy, but it's easier than the rest. So there have been a huge amount of work on that. And I cannot list that, yes. I know it's easier to analyze. Does it also improve? Easier to analyze, does it also work better? Don't spoil my talk! Oh, sorry. Oh, damn! Thank you, Chris. Now, it's obvious that this is a good idea to do this first, as I would say. Right? And let's do a fresh batch of that. So, um, there's been a huge amount of work on that. Uh, and you know, uh, starting from uh work in physics in 95, mind you. Uh In 95, mind you. So let's see what happens. So let's start with one sample at a time. And it turns out that these exponents, where I came from, by the way, appear here as well. That's a very informal description of the work of Gerard, Rokoche, and Raisin. So if you have a single index model, it told you. If you have a single index model, it all depends about this information exponent. So you expand the function and you look, you know, how many steps you have to do in the Hermite decomposition to have something non-zero, and you call this information exponent. Okay, so if the first Hermite coefficient is non-zero, well, it's good. Just order this sample and you are going to learn the direction. Better than random. Okay, maybe you're not lucky. Okay, maybe you're not lucky if you're doing like something like phase retrieval. Phase retrieval is the first term coefficient, which is essentially the linear part. So if you're quadratic, for instance, well you need d log d. If you have something harder, so for instance if you take the third airbit coefficient as a teacher, which is x to the three minus something, three x one or 3x1 or something like this, right? If you take this, well, then the first non-zero coefficient will be 3. So you have that kind of thing, and you will need d squared sample, and in time you will need d squared, because time is number of sample inches. Okay, so you may wonder how this changes if you take large fresh batches. So every time new data, completely fresh, but instead of being greedy like Rockershake, But instead of being greedy like a coach, I'm not rushing to update my weights every time I have a new sample, I'm waiting until I have enough of them and then I'm updating. Of course that will change everything. Not so much. So this is what we have been doing. The paper should be out soon. Well, you see it looks kind of similar. So let's put everything on the table so that we can see. So that we can see what's going on. Okay, so here we have L D G D, one step of gradient, or many steps of gradient descent, but with large batches. Okay, so let's compare. So that's L D G D and then one step gradient design. So already you can see that if you really want to do everything in one single step, well, there is a price to pay. In terms of time, of course I win. Time, of course, I win, because I do everything in one step. But in terms of sample, there is essentially you're losing a factor of D with respect to what you get. That still can be okay, by the way, in practice, if you can parallelize what you want to do, it's fine, but there is a price to pay. Okay, what about many samples, but with large batches? Okay, so this is here. So that's interesting, at least. So that's interesting, at least in our own. Now, in terms of computational complexity, it's the same. You're not losing. And as long as your sample is not too large, okay, so it should be order d to the l minus 1 at most, so take the square root of d, d, d squared, depends on l, you can get an acceleration. Okay, so this is nice. Not really surprising. You know, if you distribute things, you view. You know, if you distribute things, you expect. But I was slightly disappointed. I thought, you know, maybe you would need this stuff. But you don't. But you don't. So it didn't help. So large fresh batches and distributed computing, if you want, it's acceleration in terms of iteration, but in that problem, it doesn't start better in terms of cooperation or complexity. Okay, what is change? Okay, what is changing for multi-indexes? Not too much. Basically, you have the same kind of stories that was investigated in detail by Emmanuel Abbey and Krotov. Essentially, now you have to distinguish all directions and you have the same picture in all directions. So the hard direction, you take time, time, time. So it looks a bit boring, except there is one thing very interesting for me index model that they uh they found. Index model that they found is a staircase mechanism. And let me describe this to you because it's very, very cute. For multi-index model, they are special functions, but you learn very, very fast. That's very interesting. Right, so let's be, to be concrete, in the case I'm doing, large fresh batches of size order D. Okay? So what happened is the following. So, what happened is the following: at each new step, you learn all new directions that you have not learned if they are linear conditions of what you have already learned. And if this is not clear, I will make bubbles and rave my hands, and it will be clearer. So, these are four cases and four steps of algorithm. So, let's see what happens. Let's say that your function is z1 squared plus z2 squared plus z1, 2, z3. Plus z2 squared plus z1, z2, z3. This is not linear. It has no linear part. So in any finite number of steps, since I'm doing order v, I'm not learning anything. That's right. I need to go to at least this step to start to learn this stuff. It's a quadratic problem, the information exponent is 2. I need these steps and this squared data. Okay. Okay, what about this function, function b? Well, here I have z1. Well, here I have Z1 plus something which is here and here. So at first steps, oh, it's linear in Z1, I'm learning Z1. Now the second steps, well, look at that. I have Z of Z1 plus Z2 squared. Well, condition on knowing this, this is quadratic, so I'm not learning this. And here, well, I should learn Z3. So my plot is actually here. Learned Z3, so my plot is actually missing. Oh no, it's G of Z2, but I have not learned Z2, so I'm not learning Z3 either. So nothing changed. Let's look at C. Now something is interesting happens. Okay, I'm learning Z1. But then condition of knowing Z1, Z2 is linear. The function is linear in Z2. Well, you learn Z2 in the second step. But knowing Z2 now, Z3 is linear here, condition on Z2. And at the third step, you have learned all direction. Okay, so they call this in this paper the Stierke mechanism, which is kind of a really cool mechanism where, you know, for some spatially designed function, you have some kind of sequential learning, which is something you see a lot in machine learning. And this is another example here, uh, where you can learn everything in easy steps. You can learn everything with easy steps. So, this includes not just multilinear functions, but multilinear functions of arbitrary linear combinations. Because that can include other sorts of monomials, right? Yeah. I mean, it's really good stuff. So, I was really, I mean, I found very, very, very interesting at the same time. Okay. So there have been many, many works on this topic, and just flashing talks, you know, the staircase properties, how a hierarchical structure can guide deep learning, because since these are the only functions you can learn in finite number of iterations, that's kind of cool. Right? I mean, they must be very special. And many, many works on that. And the question is now to come back to Chris's questions. Is now to come back to these questions. At the beginning, one. That was for gradient descent with every time a completely fresh batch of data. But what about multipath gradient descent? Because I would be tempted to say people don't do this. That's not quite true. Actually, when when people train large equation models, these days they go only once and they are life too short and after you spend six months training your model it Six months training your model in the wait six months to go twice on the data. But you know, most people do gradient descent, they do gradient descent, right? They take data and compute the gradient, they go somewhere and they compute the gradient again and so on and so forth. You do multipasses. So how this thing is changing in multipasses? So in in in this uh recent paper, um you know, Emmanuel Enrique and and Theodore looked at the situation. And Teoda, what do we have? And say, well, a natural question is: how all changes if you do ELM with several passes? I agree with that. ELM setting is, however, harder to analyze, but as Chris is saying, it's not because we didn't go to the moon because it was easy. Right? So we consider that an important question. So, okay, I read that and I said, good, let's look into this. And they have a And they have a guess, which is interesting. So they conjecture that you can beat the d to the l minus 1 bound, and they conjecture it's actually d to the l of the 2. Okay, so why did they conjecture this? Turns out this is a CSQ double bound, the correlation of the statistical correlation. And in fact, there are good reasons to think that something is, you know, a lot of things are happening. Something is, you know, a lot of things are happening here. There was a recent paper by Alex Damien and co-theirs who, still using fresh batch, they use a modified loss. It didn't use a square loss, it used something a little bit more clever. And they show indeed that you can get an acceleration and reach this. Right? So it's, you know, maybe if we are optimistic, multiple gradient descript will give you less. Right? Okay. That's it. Does it? Okay, so what's going on if you are doing multipath gradient descent and you go multiple times, does it change the story? Yes, it changed the story. But really dramatically. Like, you know, really, really a lot. Like, it turns out you don't need L to the square, you need two steps. Okay, so that's quite a big challenge. Okay, so that's quite a big change. So, since I'm a physicist, I first started by doing simulations. This is a problem with information exponent one. It's a single index function, the parabolic tangent of z. The first coefficient is z, so it has a linear part. This is an overlap after six time steps. And every time, you know, every time my batch is ordered, big batches, I iterate once, twice. I iterate once, twice, and so on and so forth. Well, basically, they do the same. They learn both single pass and multipass. By the way, you see that, you know, single pass is better, it's actually learning faster. Because probably you are getting new data, I have not seen them, so we have more varieties, more information. It's not stupid to think that single pass is good. Right? You avoid overfitting and all that stuff. Cool. Stuff. Cool. Okay, but look at this. This is a third degree Hermit function. This is something with an information point three, and so single path doesn't learn anything in any finite number of iterations. But, well, after one iteration, I didn't learn anything. And suddenly, with multipath, second iteration, I learned lots of things. So it is different. It's dramatically different. So, somehow you don't feel the curse of information exponent. And the same thing is happening if you look at multi-MDX model. So, this is a staircase function, Z1 plus Z1, Z2. Of course, first step, you know, I'm learning Z1. So, this is overlap with Z1 in both case. Second step, I'm learning Z2, of course, because of the staircase mechanism, and both algorithms learn Z2. No problem. But if you do something But if you do something like this, it's a ballooning tangent of Ermite 1, Z1 plus Hermite, it's not Hermite, it's Hermite. Sorry, I need to defend the French pronunciation. H3 of Z2. That is not a staircase function, and indeed, you are running the first direction, but not the second one with a single-pass algorithm. Again, multipass algorithm is one. Okay, so the pointer simulation and the line, you will see what they are later. They are later on. Okay, so clearly it's very different. Why? Okay, so let's look at meridian descent and let's see why there is any learning at all. Five minutes? Two, three? Two, three. I will do two, three. Okay, so the gradient is given by this expression. Okay, so you have the function, because this is your label, and the sigma prime that comes from the neural network. But come from the neural network. Now, what is important is the projection of the gradient to the teacher vector. Okay? This is how it looks. Come on, give me a break. You are summing over a D sample divided by D. I mean, this will concentrate. This is the quantity. Okay, so the gradient is basically given by the expectation of the projection of your data in the direction of the truth. Time. Well, again, the same thing. Well, again, the same thing. And what your neural network is estimating. Now, why doesn't gradient descent work at first? Well, because at first the projection on the truth and you estimate are completely incorrect. So these two things factorize. And that thing is the first terming coefficient. It's zero. Okay? So learning means you need to correlate. To correlate your projections to the truth. Alright, so here is the thing that tells you what's going on. How are you correlating stuff? Well, you can correlate because your weights, W, are loading the true weight. And that's what is going on normally. Obviously, that would make correlation, and that's new. But that's not what is going on here. What is going on is that you're learning your Is that your weights are correlating with the samples? And that cannot happen if you are using a fresh sample every time. Because if I'm correlating with the sample now, at the next step you are giving me a new sample I'm not correlated with that. So this can only happen here, and this is enough to break. Okay, let me skip the terror once. Basically, we proved it. Theorems, basically, we proved it, but two. Well, yeah, I'm here for a week. Theorem, this is true. You can learn basically, you know, this is a condition for learning. And they are still functions. They are connected to symmetries. But I will be happy to discuss this. The last thing I want to say is, you know, how could we prove this? And that's interesting in itself. So there is this old set of There is this old set of work in Spring Last theory. It's called dynamical mean field theory. It is a set of integral differential equations that describe the dynamics, even if you reuse batches. It dates back to the 80s. And it's not Anafin mathematics. In fact, Gerard and Alice and Gerard, Alice, and Amir proved these equations for forced glasses. And very recently, we managed to You know, we managed to, well, first channel channel in Montanari in the case of singular index model, and then we looked at this for a mixed index model, we managed to prove this equation rigorously. And you know, you can just use them, and this is what you find. You know, the overlap at time two, well, let's say this is zero plus something, and so the condition is if this thing is non-zero, you learn. Okay, and so now I can reveal you the truth. The line. Can reveal you the truth, the lines were the prediction of DMFT. So it's cool, it's the first time we use DMFT to prove anything, I think. And so I'm really happy about this. Okay, so Naomi is giving me the black eyes, so I need to finish. So, you know, many things. First, well, the batch size matter. It was the first part of the talk. I think it's interesting. Not that much, but a bit. You can accelerate stuff, don't win it. Accelerate stuff, don't we need complexity? Reusing sample matters a lot. And in fact, many things that were concluded over the last few years by studying gradient descent with fresh batches, they tended to point into some directions, for instance, the importance of CSQ lower mound for gradient descent, and make us believe that there were something that were really intrinsically hard for gradients metal. For gradients, methods for multi-index model, they are not. They were hard for single-pass algorithms. So there is a very big change between the two. Okay, and I have a bunch of very interesting open-minded problems connected to these questions. But yeah, we can discuss this later. Thank you. One question for if you have too much data compared to the size of the model, would that create a problem for correlating with the data where it holds it no matter how much data you have? So you're saying if n is much, much, is really loud? Yeah, like you have a lot of data and a small number of parameters. Like is it important the model memorizes the data inside? No. No, no, because the function is too simple. The VT index is not very complicated, right? So it's okay. Thanks, Forma. The dining room's open for dinner, I think, from 5.30 to 7.30. And we'll push the lightning talks to 8 because we're already half an hour late. Meet here at 8 and give the lightning talk, especially if you're not seeing a regular program. Does it have to be about computers? Does it have to be about like computer science or math or anything? What's in your mind? So grading descent is great as long as you reuse your samples and you don't start ranting graph desktops. That's what I'm going to take away from this. If I need to get out, I'll say just remember: don't start with the empty set and reuse your map. Oh, okay. Oh, okay. Yes, yes, also. Go back and talk to you. I can explain the ideas are fairly simple. I can explain our scale like that. I know there's a problem. So every time you have things like something. How do I because y to the fourth? Well, okay, so x squared is a multi-legged lunch. Y squared, yeah. White square minus four. Yes, exactly. So the thing is, you have to be given an idea. No, no, no, let's go. But essentially, at the given time, you just minimize everything as you need that one. And then you look at your video. When you look, it's your way there. Well, I know, but I'm just saying as an algebraic question. Yeah, it's really annoying. It's really annoying. I mean, I agree. Actually, I don't know. And this is related to some of the... But I agree, because I've written z1, z2, z3, but who cares, right? Because as you said, it is x1 squared minus y squared, it's actually y minus y times this. And so... Well, x squared plus y squared is also x plus. x plus i y just like this one     