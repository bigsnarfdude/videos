And also, since 2016 there have been three FISA designs. Since 2016, there's been three fly steps of what are the neutrinos. So we'll just go ahead. It's all going to be compressed into the clock. Impressed in the run talk. So Ed Acknowledge has a tough job ahead of him from Montreal College and K-10. So take it away, Ed. Yeah, thanks, Bob. So as Bob mentioned, there have been a series of five stats dedicated to neutrinos. I really recommend people going away and looking at these nice summaries that Louis and other people prepared over the years. And I've been so bold as to suggest that there might be another one in the next 10 years or so, which I think would be really valuable for the community. There's also a really nice There was also a really nice talk by Christoph not so long ago at a workshop on systematics. So go look at that talk as well. He gives a bit of a broader summary than I do. And a couple of disclaimers. This is very focused on TTK and the TTK3 flavor oscillation analysis. There are other analysis techniques out there, so Nova sort of approach things differently. There's also a completely different approach that sort of Dune and HyperK will probably use called PRISM. I'm also not going to mention various other I'm also not going to mention various other interesting areas in neutrino physics, and also this is very much a Bayesian perspective, so sharpen your pitchforks. Okay, so what are neutrino oscillations? In a nutshell, especially for statisticians who might not even have heard of neutrinos, basically they're these particles which have got this strange property where they mix between their mass and weak eigenstates. So, what this means is that as neutrinos propagate, there's basically a probability for them to change between A probability for them to change between these weak eigenstates. So you can see here as a function of energy for some fixed baseline, you get this oscillatory behavior where you can see sort of the probability of a neutrino staying the same flavour and you can see the probability of it appearing as a different flavour here. They disappear here and appear as something else. And this is what we're trying to measure. So there are six parameters which describe um these oscillations. There are three mixing angles and then two mass splittings. And there's also this very interesting complex phase. This very interesting complex phase, which has got lots of sort of physics implications and is sort of the hot topic. There's also sort of an ambiguity in the mass ordering. So basically, this delta m squared 3, 2 can be positive or negative. That's still unknown. And sort of one of the challenges that has always been around for neutrinos is that they only interact via the weak force. So you really sort of don't have a lot of data on these things. And also, we've only known about them for about 20 years or so. So they were discovered in 1998. They were discovered in 1998, and you can see that a huge amount of progress has been made on the knowledge of these six oscillation parameters since then. So you can see, sort of, particularly for some of these, delta M squared 2, 1 and sine squared 1, 2, these are really quite well known. And sort of the experiment I'm talking about tried to measure these bottom two and these top two. So for the minute, those two experiments are TTK and Nova. IceCube also makes some statements about them as well, but their experiments are very different. Experiments are very different. And there are going to be future experiments as well. So these are being built now, HyperK in June. They're going to be much bigger, they're going to take much more data, and they're really going to make precision measurements on these oscillation parameters, and in particular, try and make some five sigma statements on particular parameter values. So understanding our systematics is really going to be key. Okay, so what's the experiment look like? Well, in a nutshell, what we do is we produce an intent beam of neutrinos and fire them a very With neutrinos and fire them a very long distance and see how they change from one place to the other. So, this is a cross-section of Japan. So, on the east coast here, we have the J-PARG facility where we produce an intense neutrino beam. We then fire this across Japan. The neutrinos oscillate along their way. We measure how they've oscillated at this far detector. So, it's the far detector which actually gives us sensitivity to these oscillation parameters. But the problem is that it's far away. So, because we don't perfectly Far away. So, because we don't perfectly collimate the source, you can't. You have a lot fewer data events. So, for TTK, we have about a thousand selected data events that we use for our oscillation analysis. There are also some higher statistic samples that don't come from the beam, but come from atmospheric neutrinos. So, they don't come in this way, they come up and down. But I won't really talk about those. And then the other part of the experiment, which is really key, is the new detector. So, this is right up close to where we produce the neutrinos, so you have a really sort of So, you have a really sort of a much higher statistic sample, and also this is effectively a control sample because your neutrinos haven't oscillated here. So, this is how you characterize your neutrino beam and also how they interact, and that sort of thing. So, for the TTK near detector, there are about 200,000 selected events. So, a lot more than the far detector, but probably if you work on the LHC, there's something that still doesn't make. Okay, so how do we predict these number of events, or what's it depend on? Don't take this equation too. On, don't take this equation too seriously, but just to give you an idea, you know, if we're looking at the number of events in the particular energy range, it will depend on the signal. So, this is some oscillation probability, which depends on those six parameters. And then it depends on all of your nuisance parameters. So, what are these sort of different sources? You've got the neutrino beam, so that's basically what energy neutrino did you produce, how many of them, what was their flavour, that sort of thing. Their flavor, that sort of thing. Then you have the cross-section or interaction model, so that's the probability of a neutrino actually interacting in your detector. But then, also, sort of, what energy particles did these interactions then kick out? Because it's not the neutrino that you're actually measuring, it's these particles that they produce in their interactions which you're seeing in your detector. Then, of course, there's detector systematics, so acceptance efficiency, momentum scale, et cetera, et cetera. And these are different for your near than your far detectors. In your far detectors. So, just to give you sort of an idea as to what this looks like, you can see in orange here, this is the data for one of the samples at the far detector. You can see this dip, that's neutrino oscillations. And then you can see there, sort of behind it, there's this blue band. So, this is sort of a density of what happens if we throw our systematic model from our post-fit. So, you can see this is sort of the sort of range that we throw our systematics from. You can also see sort of this is less than the statistical uncertainty in a lot of. Less than the statistical uncertainty in a lot of our pins. So at the minute, we're not systematics dominated, but as I was saying, for these future experiments, we will be. And yeah, we factorize out all of these different nuisance parameters, and in total, we have about 700 parameters, six of which are signal. So we really have a lot of nuisance parameters. So going back to this picture of the good, the bad and the ugly. So the good ish, for our beam systematics, For our beam systematics, we actually have a set of dedicated detectors to measure this at TTK. We're producing the beam, so we try to understand it. No one else has our beam. So that's sort of where we get all of our information from. So we have beam monitors, we then sort of have a dedicated near detector to just measure the beam. Then we also have some really great external data sets where basically sort of NA61 and SHINE have got a copy of this target, which is how we produce the neutrinos. So it really sort of reduces systematical. So, it really sort of reduces systematic uncertainties, and we sort of put that into our analysis. But in the actual oscillation analysis, we don't sort of use these physics parameters, we don't have sort of the hadron scattering in there, instead, sort of the beam group take all of the data they've collected and take these external data sets as well, and then come up with a model or estimates of these physics parameters. We then throw them and then we produce a distribution of sort of the expected. Produce a distribution of sort of the expected number of events for different neutrino energy ranges. So, from that, we then get a covariance matrix with the error in each one of these bins and also the relationship for different energy ranges. So, we end up with a covariance matrix for the beam parameters. Then, similarly for the detector, you know, we have calibration and control samples. We don't directly fit these either. So, again, we do these sort of toy throws to come up with distributions of the expected number of events in the bins. Expect a number of events in the bins in different analysis bins, and we shove them into the analysis using a covariance matrix, again, describing the correlation across different bins. You can see there are 500 of these nuisance parameters. That's because it's sort of one for each analysis bin, and our analysis has grown a lot in the last couple of years. Uh so we're actually moving away from this method because five hundred nuisance parameters is just a pain, so we're going to uh go to sort of fitting detector systematics more directly. Detector systematics more directly, or implementing detector systematics in our fit directly. And then the bad and the ugly really is sort of neutrino interaction systematics. You know, neutrinos weakly interact, there's not a lot of external data sets, and often sort of these experiments are set up in very different ways. They might have different detectors, they might measure neutrinos at different energies. So even when there is data, it's tough to sort of always use the data sets because you might operate in a different regime. Might operate in a different regime. And also, it's tough then to sort of compare different experiments. And you really don't know who's right and who's wrong. So that's also a problem if you're a theorist. So a lot of the time you have theory predictions which disagree, and sometimes they agree with one data set, sometimes they disagree with another. So deciding what you do with that theoretical uncertainty is really difficult. So we have quite a few systematics which basically interpolate between different models. And then similarly, we have quite a few And then similarly, we have quite a few ad hoc uncertainties where we know that there are differences in external data sets or particular regions where there are some problems. So, some examples of these, you know, if you're a physicist, you know, we have uncertainties for nucleon form factors and also nuclear effects. And in particular, I thought it was worth highlighting that for neutrino interactions, you know, you have your initial interaction on a nucleon, but then this is happening within a nucleus. So, as well as sort of modeling sort of that initial interaction, you don't have to worry about. Interaction, you don't have to worry about all of these particles going through your nucleus, and there's a lot of complicated nuclear physics in there as well. If you want sort of a longer discussion on neutrino interaction systematics, then there's a nice summary paper here. Okay, so now the actual likelihood function in our fit. So, you know, we do a Poisson likelihood ratio, we'll have one of these for our near detector, and if we're jointly fitting the far detector, we'll have a turn for that as well. For that as well. So, O in this equation are our signal parameters, and then F are our nuisance parameters. So, you just have the Poisson of the observed and the predicted, and then the observed and the observed in the ratio. We sum across all of our analysis bins, and then you also have all of these penalty terms. So, these are for our nuisance parameters, but also this is a Bayesian analysis, so we also have a prior for our signal parameters. It's worth mentioning given present concept. It's worth mentioning, given the present company, that there is also an MC statistics uncertainty built into the near detector term here. But, yeah, so from this likelihood, there are then two different techniques, or two approaches, examples of parameters, and then we marginalize across all of our nuisance parameters to produce credible intervals in 1D or 2D. This analysis can also forget about the far detector and run near detector-owned bits. Then the other approaches to Then the other approach is sort of to do a sequential fit. So in this scenario, we have a near-detect data fit where we use minwe, so gradient descent. From this, we then sort of expand around the local minimum and get our covariance matrix describing the correlations between all of our nuisance parameters. And then we basically take this constraint from the near detector and pass it to a hybrid Frequentis fitter. This fitter then uses. Fitter. This fitter then uses this matrix to throw marginalization toys and then they build confidence intervals using the delta chi-squared method and they also apply Feldman cousins corrections. I work on this analysis so I'm very much MCMC focused so that's my bias and there'll be a couple of specific points on MCMC later. So what does the near detector constraint actually look like? So here you can see these are So, here you can see these are two plots just of some of our nuisance parameters, with this sort of pinkish red being our constraint before we've actually fit the near detector data, AKR prior. And then the crosses are then our actual sort of post-fit results. So, these are sort of what we're propagating to the far detector in that sort of sequential fit scenario. So, you can see most of the time things agree pretty well, but you do get some pulls. Well, but you do get some pulls in some of your parameters. So, this is for the muon neutrino beam flux. So, this is across different energy regions. And then, up here, you can see for cross-section, a lot of the time you can see that we've considerably reduced our error or uncertainty on parameters compared to our priors. You can also see that for a couple of parameters, though, we actually never had a prior. So, if we don't really have an idea. If we don't really have an idea as to what our prior belief on something, then we just rely on our near detector to come up with some good estimate. So you can see for these two normalization parameters here, that's the case. And also, I should say, we don't actually fit all of our nuisance parameters at the near detector. So, some cross-section parameters we don't really trust our near detector to pin down very well, or we don't think that it should be able to. So, we don't fit those and then we propagate that full. Fit those, and then we propagate that full uncertainty to the file detector. And then you can see sort of the result here in the post-fit correlation matrix between flux and cross-section is that you get some pretty strong correlations and anti-correlations between these two regions. So the flux is sort of pretty correlated in the prior, but yeah, post-bit you really sort of get a lot of these a lot of structure here, and it's quite tough to sort of disentangle. So, in particular, sort of when you're looking at these 1Ds. Particular, sort of, when you're looking at these 1Ds, it's quite hard to take anything away because they're highly correlated with many other parameters. Okay, so then near detector to far detector. So the near detector really significantly improves our measurement at the far detector. So here you can see a prediction of one of the samples at the far detector. You can see the systematic uncertainty band here in blue before a near detector fit. Then in red you can see afterwards. So you can see, particularly in this area, Can see particularly in this area here, which is where we measure neutrino oscillations mainly, you can see that the error is really significantly reduced. So, this means that our measurement on oscillation parameters is a lot more precise. There's quite an interesting thing between the two analyses that I mentioned. So, when you do this joint VIT or the simultaneous NDF DFIT, which you can see here in red, that's sort of the credible intervals from this MCMC analysis. CMC analysis. And then, if you compare that to doing the sequential fit where you've taken this covariance matrix from your near detector and then constructed confidence intervals, you see this difference here. So, that's the black here. So, there's some sort of difference going on here. But then, if you take this hybrid fitter and do the marginalization toys, rather than it being from a covariance matrix, but if you do it from a Markov chain, so an MCMC fit to your near detector only, so instead of sampling from this covariance matrix, Sampling from this covariance matrix, you're sampling from many 1D posteriors, essentially, you can see that suddenly the two analyses agree. So, you know, we talk about sort of the far detector being statistics limited, but you can see actually how we propagate that near-detector constraint. So the constraint on our nuisance parameters actually has an effect on our contours. So this isn't really as systematic as such, and we officialize both results. And we officialize both results and just put them both out there, but it's kind of interesting. So, you know, we were talking about asymmetric uncertainties earlier. Maybe this is it. Maybe the covariance matrix just isn't picking up on a lot of these asymmetries. So we get a slightly different result. So then I want to talk a little bit more specifically about how we implement some of these nuisance parameters in our theatres. So in general, there are three different types. Different types. So you've got normalization parameters. So this is where you basically just weight up a Monte Carlo event in a particular bin or a particular kinematic region based on some uncertainty. And then we also have splined response functions, which I'll talk more about. So that's where something maybe has more shape. And then also we've started doing actual shifts on kinematic variables in the fifth. So this is where events migrate between bins in our analysis. And probably the And probably the thing to highlight here is that because we don't have large data and we don't have ginormous Monte Carlo, we can actually do this all event by event. So for each change in our nuisance parameters, we loop through all Monte Carlo events and re-weight each one of them. So quickly on supply and response functions. So basically what we're doing here is for a Monte Carlo event, we want to know what it would look like if we ran our simulation with a slightly different nuisance parameter. So you can imagine if you've got a Parameter. So you can imagine if you've got a cross-section, you might change some parameter, and that would give you a different cross-section for that event. So then we build up a spline, which basically evaluates this change at known points, and you get a weight at each one of these. So you can see on this plot here, there are some little crosses here. These are the points that we actually evaluate at, and then we interpolate between these. So in this top case, you can see that there's quite a lot of ringing. This is for a cubic spline. This is for a cubic spline. The red here is actually where things have been evaluated incredibly finely. So that's like what we should be interpolating. So there are some differences in some places. So one way we're trying to get away from or around this ringing is we've just moved to monotonic qubits blinds. So you can see here now you don't get sort of this strange ringing and things generally looking better in treatment. But yeah, so for your splines, you know, you have these for many different You have these for many different systematics, and for each event, you then basically have two choices. You can bin these responses and come up with an average response and apply that to groups of events, or you can keep all your splines for all your events. So, at the near detector, we actually evaluate every spline for every event. So, we've got about 2 million Monte Carlo events and about 30-bit splines for every event, which is about a gigabyte of RAM. And we're able to whack this on a GPU and basically evaluate. This is on a GPU and basically evaluate them all very fast. So you can do it in 0.05 seconds. So again, every time we change a systematic parameter, we re-evaluate all of these splines. And at the minute, that's not really a problem for the scale of our analyses. But in the future, when we've collected orders of magnitude more data and we've got orders of magnitude more Monte Carlo, is this feasible? We might have to go to a more binned approach. And at the fire detector, we're less worried about. And at the far detector, we're less worried about averaging splines because the fire detector doesn't really constrain systematics. So we bin things up. So we bin them in different analysis bins. We split that then by different energy regions and by different interaction types. But we are basically averaging over some binning scheme. And then, sort of, slightly more interestingly, or maybe more uniquely, is that systematic slight momentum scale and also some nuclear effects. Nuclear effects, we've actually started to implement these by directly modifying a particular Monte Carlo event sort of kinematics. So you can imagine a momentum scale has an uncertainty, you know, plus or minus whatever in MeV, and we basically vary this parameter and directly modify an event's value for that. So, you know, the implementation of how you do this, it could be some Gaussian throw, but you could go and, I don't know, interpolate between various histograms to come up with some way of Histograms to come up with some way of changing some variable. So you can see what this means is the individual Monte Carlo events will actually migrate sort of across analysis bins. So here's an example of a nuclear effect, so a binding energy. So this is what happens if we change this parameter by one sigma. So you can see that events have basically shifted in momentum. So that's the variable we're shifting in this case, momentum. So red is where events have increased, and then blue is where events And then blue is where events have reduced. So you can see these bands here again, as I said, just because we're moving in momentum. But what this means is that we have to keep track of which bin each event is in, which is quite computationally expensive. And also, sort of the interesting thing here that actually Nick mentioned early on is that for some of these parameters, you get very discontinuous likelihoods, right? Because you're shifting events. Likelihoods, right? Because you're shifting events into a different bin, you're going to get a sudden change in your likelihood because your Poisson has changed. So if you're a gradient descent algorithm, you really struggle here, right? So you have to come up with some different approach. So Christoph in his talk mentioned regularization of bin widths. But if you're running Metropolis Hastings MCMC, you don't use the gradient, so you just don't care. You're just doing the ratio of a proposed likelihood compared to where you currently are. So if one of To where you currently are. So it's one of the nice things with MCMC, or at least Metropolis Hastings, that we can just sort of move events about based on some uncertainty on some reconstructed variable. Earlier in the week, there was quite a lot of discussion on how we should deal with sort of discrete model changes. So we have this problem in neutrino physics as well, and DTK has basically come up with a homebrew procedure to do this. So we call them fake data studies. So, we call them fake data studies. I don't know what other people call them. But what we basically do is we create a fake data set. So, we'll change our Monte Carlo based on some different model which isn't in our systematic model. So, you can see sort of an example here. This blue here is sort of the fake data at the far detector, whereas green is sort of what we would nominally predict. So, we do this at the near detector and far detector, we then fit the fake data at the near detector, and then we see. Detector, and then we see what that propagated constraint from the near detector looks like compared to our far detector result. So, you know, if our near detector gets things correct and isn't biased, then our far detector and near detector will agree. But you can see here, this one sigma uncertainty band doesn't really agree with this blue. Okay, the number of events is small, but so in this example, you can see that things don't overlap so well. So, if our analysis is robust, our systematics should be able to soak this up. Our systematic should be able to soak this up. But if not, then this is just going to appear as a bias in our oscillation probability because we've changed the number of events, and the parameters which do that are our signal parameters. So, yeah, we sort of have come up with a procedure to try and decide whether something is critical or not. So, basically, we calculate the systematic uncertainty, and if our result in a fake data study is changed by more than fifty percent of the systematic uncertainty, Percent of the systematic uncertainty, we take action, whatever that means. So, in some cases, we might add in an ad hoc parameter if things are really bad. So, this would basically just inflate our systematic uncertainty and describe exactly the difference between the near detector prediction and the far detector itself. But then also, we might smear our final contours. One sort of caveat here is that whenever we're calculating systematic uncertainty, we're Systematic uncertainty, we're taking our total uncertainty and just subtracting the stat only in quadrature. This is a bit sketchy, especially for some parameters which we know have got physical boundaries. So, this is sort of not ideal situation. So, if people have ideas as to how to calculate systematic uncertainties when physical boundaries are there, then please chime in. And also, as part of this criteria, if any of our statements on physics. Of our statements on excluding particular parameter values change in affect data study, then we just wouldn't report them. So if we said that we saw delta CP equals zeros excluded at three sigma, if this was true in our data fit but not true in some of these fake data fits, then we wouldn't report it. And we've got into the habit now of reporting all of these biases that we see at various fake data studies that we do. This is just the first half of a table. This is just the first half of a table taken from a recent paper. There's many more going off the board. But yeah, that's sort of the approach we've been doing. But again, sort of these percentage changes on some of these parameters is a little bit difficult to understand because they're not Gaussian and they have got physical boundaries in some particular places. Finally, and this is very MCMC specific, but I thought it was interesting to mention, one way. Mention. One way that we can sort of look at impactful systematics post-FIP is by basically changing our prior shock. So once we've run our Markov chain, we've got this high-dimensional fit, you know, 700 parameters. We've got the parameter values at each step. And what you can do is you can, for two parameters, so in this case, we've got a signal parameter on the x-axis, and on the y-axis, a nuisance parameter. On the y-axis, a nuisance parameter, what we can basically do is we can see what would happen if we shrank our prior significantly. So, the nice thing is with MCMCs that you can do this, just you don't have to rerun an analysis with a different prior. You can just wait up steps based on the ratio of your new prior from 30 or old. So, as I say, it's basically asking what would happen if this constraint was significantly higher. So, you can see this is sort of some shrunken and slightly pulled change where we've re-weighted a particular part of our posterior, and we could then. Part of our posterior, and we could then evaluate how much our signal parameter changes. So, this is quite a nice way of investigating particular sort of systematics and how impactful they are on our signal results. Okay, I think I'm out of time now. But just to summarise, so neutrino oscillation analyses are in this sort of funny situation where our near detector has high statistics and constrains our systematics, but then the actual parameters we want to measure are statistics. Premises we want to measure are statistics limited. But even though we're statistics limited, we really have to be quite careful and we spend a lot of time worrying about how we're propagating that constraint to the file detector. We tend to treat systematic sort of event by events, or change our Monte Cole file prediction for every event that we have. And then in the final result, we marginalize over a large number of nuisance parameters. We've also highlighted a couple of post-fit studies that we do on systematics, so fake data studies. Systematics, so fake data studies to check our model is robust, and also these sort of shrink and pulls. And again, I'll sort of highlight that in the next five to ten years, we're going to have these very large experiments, which are going to take a lot more data. So, we need to make sure that our statistics or our statistical techniques and the treatment of our systematic are really up to the challenge. And I've just stolen this from one of the summaries of a previous FileStat, which seems very relevant. Should I go ahead and hit the button twice?