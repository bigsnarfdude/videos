That you know, which has been driving our interest, my interest in this, is also trying always asking the question, you know, what's mechanism? And this is something which I'm going to be actually posing to all of you again and again, because I think one of the problems with these fields like ours is people always talk about causality, right? And so, are you just picking up correlations in a way? So, how do you know that what you're picking up is really causal? And so, the first question which we had, of course, is that what on earth is this really amazing accuracy due to? Accuracy due to what is it seeing, what is the algorithm actually seeing in the data that is, you know, that is giving allowing it to distinguish between this cell and the other. And we played a lot with things like just dropping out variables in the ANNs, but it really didn't give us very much insight. So we did know that we needed both the shape and textural information. If we remove textural information, as I showed you earlier, there was a significant drop in accuracy. If you use one And accuracy. If you use only textual information, you couldn't do anything at all. So shape was necessary. So by that time, there were already a few toolboxes out there that people had tried to interrogate CNNs to see what was behind the accuracy of CNNs. So we decided to maybe try some of those. One of the reasons why we had not played with CNNs before was simply because we have very small data sets and CNNs typically use very large ones. But then, again, by this time, techniques of transfer learning were well developed. Techniques of transfer learning were well developed, and so that's what we could use. And so, transfer learning is a way to actually repurpose a large CNN trained on a large data set, or I think in general, any machine learning algorithm trained on a large data set on a smaller data set. And for CNNs, the way it works is this. You pick up a CNN. For in our case, we wanted something which has already been trained on images. And so you kind of assume that because it's been trained on images, the convolutional layers have already picked out. Already picked out the right filters for interrogating, for understanding images. So, for example, things that pick up edges, things that pick up various kinds of curves. And so, you leave all the convolutional layers unchanged. And you essentially just retrain the fully connected layers on the small database which one has. And of course, you change the output corresponding to the classes that you have. This is again all supervised learning. All supervised learning. One nice thing we did here, which is that we actually mixed up classes as controls. We are worried that these large algorithms would just pick up artifacts in our data. And so one of the things we did was we just, for any binary classification task, we actually mixed together cells of the two types and made two random classes and also gave it to the same algorithm as it controls. And the thing which was pretty exciting for us was that CNN showed even higher. That CNN showed even higher accuracy, kind of straight out of the box. I mean, it wasn't quite straight out of the box. My students, this is all, by the way. Thank you. This is all, by the way, I should say, work of a very brilliant undergraduate of mine who graduated last year, Sidney Alderfer. And so, you know, so, but it was fairly easy once we got everything to work and we had to play around a bit with, you know, with. You know, with methods to prevent overfitting, but it was we ended up getting really, really beautiful numbers, really beautiful accuracies for classification tasks. The algorithm could make out normal cells from transformed cells with over 90% accuracy. It could make out transformed cells from each other with an accuracy of 80 to 87 percent. And really, and you know, even in some ways, even nicer, accuracy on the control databases was about 50 percent, little better. Basis was about 50%, little better than random guessing, right? And that kind of made us really happy. We could do multi-class single-cell classification. Overall accuracy again was between 80 to 89 percent in the ballpark of the second type of between transform cells. And we probably can push this. This probably would improve a lot if we had larger data sets. We just had hundreds of cellular images, you know, this is this was mostly a single graduate student, a single undergraduate, uh sort of uh that was the level of funding we had project. That was the level of funding we had project. And so these are some of the numbers we get when we do multi-class imaging. And this was all published last year. So, one question which we were interested in is how important is acting organization? To answer that, we used the same, you know, we ran exactly the same machine learning method, but we used binarized images. And we saw that there was a significant drop in accuracy. And assuming that 50% accuracy is equivalent to no information about About 45% to 65%, depending upon the cell line, is contributed to APT and organization. And it's interesting, of course, that it's dependent on the cell line. So then we were interested in asking this question which I raised, what does the algorithm actually see? And we use this method called LIME, Local Interpretable Model Agnostic Explanations. It's one of the popular methods for interrogating CNNs. And what it basically does is it replaces parts of the image by superpixel. Parts of the image by superpixels. And these, it basically forms superpixels in the image and replaces them by background. And then it runs the algorithm again and again till you see a significant drop in accuracy. And one of the nice thing about LIME is that you can also get some confidence using LIME that what your CNN is actually working on the data and not on artifacts of the data. So for example, in one classic case, when people use LIME on the classification exercise they were seeing, they saw that the results were coming straight from Results were coming straight from features in the background rather than the features themselves. They were actually looking at different species of dogs, and it turns out that you know, huskies always had snow in the background. So, the nice thing about us was that line only, well, our background was black, so it would have been very sad that if, yeah, you had a question. Yeah, what's the receptive field of um your top level classifier with respect to like an image like this? How much of it the image is it's How much of it the image is? Oh, we are giving this entire image to it. So the images scale down. How much is like in top-level classifier? I can't answer that question. Maybe you can explain it to me afterwards and then I can see whether I know the answer. So when we ran live on the data, what it spits out is it spits out regions like this, which is regions, it's flags. Which is regions, it's flags, which is uh, which leads to a significant drop in accuracy. And these are these are some of the examples of what Lyme spits out. The interesting thing is that Lyme, most of the time, actually gives us the whole sellback. But in some occasions, and unfortunately, not a very large number, but I think for every classification task, for about maybe 10% of the cells, it gave us some flagged regions. And so now, you know, you look at the regions and you say, can I? You look at the regions and you say, can I interpret them with my eyes? And is that interpretation really what is relevant or not? And you're kind of making a guesswork there. So we should look at area of the cortex. You should look at the interior of the cortex, which of course you can't see sitting from where you are. But there are several of these. And so I've just given you two examples of what I see with my eyes, which may or may not be what the computer sees when it's doing its analysis. And so, for example, in ARPA-E19, almost Example in ARPYE19, almost all these cells, almost all the regions show a thick cortical actin band, that's a band on the outside with sparse actin intensity in the interior. For the AKT mirror cells, actin fiber structure, in all of these cases, if you zoom in, you'll see that there's one, like this is a little clearer, I guess, in this one or in this one. You'll see that there's strong pneumatic ordering, which is not universal over the cell, but it's in one region. There's strong pneumatic ordering of the Ordering of the active fibers with a few crisper cross at almost perpendicular angles, just a few thick fibers. Now, are these real signatures of these particular genetic insults? Are they just things that I'm seeing? I don't know, but that's, I think, a really interesting question. Because if they are real signatures, then that means that they are things like morphological signatures, which are similar to the kind of signatures people see in gene expression or in sequencing patterns. Or in sequencing patterns, right? And it would be, I think, really cool if we could see morphological signatures of things like genetic insults. But you would say that like 90% of the time you just said the entire cell. 90% of the time it just gave the entire cell, right? And so this is something that we really want to know how to do better. And that's another question for the machine learning experts out here. Are there ways of actually, other ways or other techniques of actually doing this which would give us additional insight? Yeah. Yeah. Yeah, and so your observation, you can quantify them and look at them and compare them with game controls and read it itself. So there's no reason not to do it. So there's no reason not to, yeah, you're absolutely right. We could also try to play around with, and in fact these are ideas we're trying to implement, play around with giving drug treatments to these cells, see if these features vanish and whether, and then sort of one. Whether and then and then sort of one CNN again. It's still actually meant to just quantify them. So if you talk about kinematic order, you can just come up with an algorithm. Yeah, to sort of, but not all of that is easy to quantify. Like, how would you quantify the first one? It's again one of the questions which I'll come back to at the end. But what are appropriate morphological parameters that pick up structures like? Parameters that pick up structures like this, like how to quantify something like this. It's obviously implicit in things like textural measures, it's implicit in some kind of density distributions, but there's no clear set of parameters that actually picks up these different kinds of structures that you see in actin. Or at least if they are, then we haven't got to them yet. So, anyway, I hope I have convinced you that you know cell morphology, though this audience I guess doesn't need convincing. Audience, I guess, doesn't need convincing that cell morphology is a biophysical property originating from the active mechanical properties of the cytoskeleton. Morphological features contain significant information about the physiological state of the cell. In this little cartoon here, which I don't have time to go over, I've just sort of listed some of the molecular biology that we know behind different features of something like spread cell shape. And I think there is a single cell frontier. Cell frontier of biological big data being sort of on the horizon, as it were. I'm calling it single-cell morphomics, but not what it's going to be called when it finally sort of becomes more mainstream. But I think it is there on the horizon, and it's something which there's more and more, as you'll see also from this week, I think there's more and more work in saying that there's something very deep about morphology and what it's doing. So, what are we doing now? We have a bunch of new. So, what are we doing now? We have a bunch of new collaborations and some new directions that we are exploring. One of the questions we are asking is: are there other, probably, more mathematically informed morphological parameters that can give additional insight? So, you know, on the lines of what you are asking. So, we have a collaboration with Khan and Nina. And please listen to the talk by Wong Shen Li on Thursday, in which she talks about some of our work using the elastic metric. Elastic metric in distinguishing between different kinds of cells. And we have a new collaboration with King Shukra Bhosh sitting at the back out there and Lily, his student. And we are basically asking the very general question, can we turn geometry into dynamics and vice versa? And Lily is giving a talk on Thursday again at 2.30 p.m. on classifying using morphological parameters or looking for morphological parameters that can classify polynomial networks. Parameters that can classify polynomial network dynamics. And what we are doing in our wet lab is trying to see whether we are doing a bunch of new experiments in which we are trying to see whether we can predict how to measure cellular mechanical properties. So I just wanted to end by kind of zooming out and giving you a sort of physics analogy, if you will. If you think of cell morphology as a biophysicist, what it is is a morphology of an interlinked cytoskeletal polymer mesh inside a thin, elastic plasma membrane. Elastic plasma membrane, right? If you, you know, in physics, spin order parameters, pneumatic order parameters, in many cases, you can think of them as morphological parameters. Like imagine looking down at an Ising model, you know, from above, it's a spin-order parameter is basically a morphological parameter. Order-disorder transitions, even you know, even dynamics, can be quantified and understood using these variables. We know in many soft matter systems, morphology is intimately. Soft matter systems, morphology is intimately related with macroscopic properties. Foams, for example, is a good example. And so, in a sense, what questions we are asking are very biophysical. What morphological variables are relevant for the cell? What properties of this object can we read out from its morphology? And I think, even more interestingly, can we build dynamical models using morphological parameters? And that's not we, I haven't talked about that. I think you'll hear one talk about an attempt to do that, and that's something that we are also trying to, we also have to do. To hear also time to do. And so I'd be very happy to take more questions, but just wanted to acknowledge all the collaborators and all the people who did the work and are doing the work and are funding sources. Thank you for your attention. Maybe you can have the second speaker start uh setting up to the speaker. Just keep the recording. Just keep the recorder. I think you should stop with Race Robin. Oh, you think? Yeah. Because they really did. Well, she said they didn't want to edit. Oh, they didn't want? Okay. Go ahead and ask the question, yeah? I was wondering if you also.