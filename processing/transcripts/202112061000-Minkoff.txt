I wanted to thank you organizers. I haven't gotten a chance to introduce myself, but I will just mention that 25 years ago, I went to a Women in Math Connected to Industry workshop at the IMA, and it had such an impact on me to be in a room full of women. It's more fun in person, but I can still remember that experience. And specifically sitting at a table at dinner with Margaret Cheney and Joyce McLaughlin. Cheney and Joyce McLaughlin and Margaret Wright, and it really impacted me. I, you know, was really inspired by that. And I can still remember the actual conversation I had at dinner with Margaret Cheney. It was one of my real inspirations. So I think this is a great thing. And I want to thank the organizers for inviting me. So I am going to give a talk on sort of two different things. I'm hoping to connect them. Hoping to connect them. But the first part of my talk is joint work with my PhD advisor, who I'm working with again for fun, Bill Symes, and then a graduate student of mine at UT Dallas, Hui Yi Chen. And the second part of my talk, so that part will really be the talk about extended source inversion, and it's going to be in the context of a very simple problem. And then I'm going to segue into talking about Talking about doing micro-seismic source estimation as the more interesting application that we hope to extend this idea to in the future. And that work is joint with a former PhD student of mine, Jordan Catterly, a former master's student of mine in geosciences, Matt McChesney, and then Matt was co-advised by myself and George McMeckan. So it's sort of a talk with two pieces, and I hope to connect them a little bit, although the second part is future work, a little, you know, what we hope to do. I'd like to acknowledge support from an industrial research consortium that I am part of at UT Dallas called the 3D plus 40 seismic full waveform inversion research consortium. Okay, let's see if I can do that. Okay, so here's a quick outline. I'm going to spend Here's a quick outline. I'm going to spend a lot of time talking about this idea of imaging the subsurface of the earth. So, we want to understand what's under the ground for a variety of reasons, maybe to get oil and gas out of the ground, maybe to map where faults and fractures are, to try to understand the location of an earthquake, to inject large quantities of carbon dioxide into the ground to get. Into the ground to get it out of the atmosphere. So, lots of reasons. And these days, people that are in that business are doing primarily something called full waveform inversion. And I always laugh at this name because all it really means is least squares estimation, where you use the wave equation to model the propagation of waves in the Earth. So that seems mathematically like a really obvious thing to do, but for many years, especially. But for many years, especially at oil companies, people didn't want to do full waveform inversion because they knew about a problem. And the problem is that the objective function has so many local minima. And so this problem, which I'll explain in the next slide, can also be referred to as cycle skipping. So we want to overcome this cycle skipping problem, and I'm going to talk about one possible way to do that, which is using extended That which is using extended inversion. I'll talk about what the source extended objective function is and why it helps. And then you have to think about the algorithm you want to apply to minimize that objective function. And I'm going to talk about one particular algorithm, the discrepancy algorithm, that has some pretty cool properties and show some numerical examples. And then I'll switch to the second part of my talk. And in the second part of my talk, I'll be discussing what are called micro-seismic. What are called micro-seismic sources? We want to estimate what are called microseismic sources, which are just little tiny earthquakes that occur when you inject large quantities of fluids into the ground and there's a stress release in the form of an earthquake. Okay, so I'll start by talking about this cycle skipping problem. And again, when I say full waveform inversion, which in the oil industry is what people refer to. Refer to. But when I say that, all I mean is least squares inversion. So it's the standard least squares minimization of the L2 objective function. So I've got a cartoon here that is from a well-cited paper now by Jean-Verrieux and St√©phane Umperto that shows cycle skipping in a really simple way. So in the middle, in the So, in the middle, in the dark black line here, we see a seismic trace. We see a seismic waveform. And it has three periods shown, the nth period, the n minus first, and the n plus first. In the top, we see, so this is in the middle is the data you would like to try to match. That is the observed or recorded data. In the top picture, we have In the top picture, we have a shifted waveform, and the shifted waveform comes from trying to predict the data, solve the wave equation with a wrong Earth model. And in this case, the wrong Earth model has shifted the waveform so that it's shifted by more than half a period. And if you did the optimization, the full waveform or least squares optimization using this predicted data, you would match. Predicted data, you would match the n plus first cycle with the true data, but you'd be matching the n plus first predicted cycle with the nth for the observed data. So that would be a mismatch. That would not be a good thing. So that is essentially getting caught in a local minimum. And that is the cycle skipping problem. On the lower part, we see another waveform. In this case, the prediction. In this case, the predicted data is off by less than half a period. And now the predicted data's nth period will line up with the true data's nth period, which is a good thing. So, this in essence, this one picture shows the cycle skipping problem. And it is a well-known problem for full waveform inversion that people are spending a lot of time and effort now trying to get around because now people at oil companies and Because now people at oil companies and other places have accepted using the wave equation to model wave propagation in the earth and to try to do the optimization that way. So there are different ways you could try to overcome this cycle skipping problem. And one is by using extended methods. And there are a lot of different extended methods. There's model extension and other things. I'm going to focus just on one of those types, source extension. Source extension and the extended methods all have in common adding degrees of freedom to the modeling operator. So, I'll talk about the wave equation in a minute, what we really are interested in, the Earth model parameter that describes the wave propagation through different materials in the subsurface. But we're going to add some additional degrees of freedom that we hope will allow us to bypass some of these local minima. Minima. Eventually, you want to get rid of those additional degrees of freedom because they're not physical. And so I'll talk about that too. So I'm going to talk in the first half of my talk, I'm going to discuss a really simple inverse problem. And it's so simple that you can do everything, you know, on paper almost. If you can't do it on paper, you can code it up in MATLAB and it's a simple numerical exercise. So in that context, we can actually justify. Context, we can actually justify things theoretically. There are a lot of extension methods, and people tend to say they work because they have applied them to some problems, and they seem to work, but they are not really carefully analyzing when and why they work. And what's nice about the simple context I'm going to talk about is you can actually do that in this context. The other thing is that even though it's a simple context, it still illustrates. Context, it still illustrates the basic cycle skipping problem that you see in more realistic problems. So here is the simple experimental setup. The Earth is this three-dimensional cube on the left. It's a homogeneous cube of data. So the Earth, of course, is not homogeneous. It's not a single material that waves would propagate through. It's made up instead of many different rocks and fluids, and they're often in complex. And they're often in complicated mixtures and things like that. But we're going to assume the Earth is instead made up of one material, this blue material, and the velocity of waves propagating through the Earth in that one material is 2.5 kilometers per second. So that's the first simplification. The Earth is just one material. The second simplification is we're going to assume that we're going to generate a wave from a single source, and we're going to A single source, and we're going to record the wave at a single receiver. And so that's the second simplification. Normally, you would have many sources, and you'd have many receivers. In this simple context, you can see on the right for a typical seismic source, a typical seismic source is a Ricker wavelet, which is often used, which is the second derivative of the Gaussian. You can see this source is centered at zero. This source is centered at zero and it has small support. On the bottom, we are able to actually write down what the solution of the acoustic wave equation is. In other words, what the data is. And in this case, for a homogeneous medium, the data is just a scaled shifted in time version of the source. So it's really a simple problem, but there's Really, a simple problem, but there's a lot of nice things about working with simple problems. So, the wave equation we're looking at is the acoustic wave equation, which is listed here. The acoustic wave equation has two parameters that could describe the materials in whatever medium the wave is propagating through. In my case, the subsurface of the Earth. It's got a density, but I'm going to assume the density is constant at one, so it doesn't even show up here. And then there's Show up here. And then there's one over the velocity, the sound velocity squared in front of the time derivative term. We are going to invert instead, not for the velocity, but the reciprocal velocity, which is called the slowness m. So in this picture, I show you the true velocity is 2.5 kilometers per second, and our target slowness that we're going to look for is 0.4 seconds per kilometer. Seconds per kilometer. And that's going to be the target in our numerical experiments that we're going to talk about. So, as I said, in this case, the acoustic pressure, the solution to this wave equation is just a scaled time shifted version of the source. And the source here is the right-hand side of the wave equation. It consists of two pieces in our experiments. It's a Experiments. It's a point source in space, a delta function, times a time-dependent function that people call the wavelet W. So that is our data. Our data is a version of the source. And the only other thing I should mention here is that other than the reciprocal velocity, the slowness m, we've also got r here. R is what's called the offset, it's the distance. What's called the offset, it's the distance between the source and the receiver. And in our experiments, it's going to be taken to be one kilometer. Okay, so here's the inverse problem on this slide. The inverse problem is that if we specify a tolerance for solving the inverse problem, epsilon, and we specify a region in which our source is non-zero, so our source is going to be non-zero here in a region from minus lambda. In a region from minus lambda to lambda, then the inverse problem, the full waveform inversion inverse problem, is just, in our case, we've normalized it, but it's just the least squares misfit between the data D that we observer predict at the seismometers, the receivers, and the data we record at the seismometers and the data we predict from our estimate of the slowness or velocity and the wave load. I'm sorry, I sort of messed that up, but it's a data. Sort of mess it up, but it's a data misfit term. On the bottom, you see a picture where we've plotted the objective function j, the full waveform inversion objective function versus slowness. And you can again see here the minimum occurs at 0.4. But this is a really amazing picture because not only do you see some local mins here, local minima at these two points, but every place the objective function is flat. The objective function is flat, the gradient would be zero. So those are all stationary points where my cursor is. And so you've got an infinity of stationary points, places where you could assume you'd solve the problem, even though you really were very far from the true global minimum. So this is cycle skipping in essence. And so we would like to come up with an objective function that potentially would allow us to use some. Potentially would allow us to use some local gradient-based optimization where we don't have to have a really good initial guess and we would still manage to find this target slowness of 0.4 seconds per kilometer. And it's pretty hard to do with this objective function. You can see that you could pick a lot of places for initial M's where you would not be able to recover that target slowness. So we're going to add degrees. So, we're going to add degrees of freedom to our modeling operator F to avoid these local minima if we can. And the first thing we're going to do is we're going to say, okay, well, what we really care about is to find the parameter, the slowness or reciprocal velocity that describes the way waves propagate through the ground because that correlates to the rocks and fluids that are actually under the subsurface in the area you're interested in. But we're going to throw in inverting for the source. Throw in inverting for the source W, the wavelet, even though we don't really care about that, it doesn't really tell us that much about the Earth, but it allows us to extend the modeling operator. So that's our first extension. The second extension is, as I showed you here, our source generally has some compact support, and we're going to assume it's compactly supported around zero. So the second extension in this extension. The second extension in this extended modeling operator is we're going to drop that assumption that our source only has support around zero. So with those two extensions, we believe that we can bypass local minima, and I will show you a picture to indicate that's what happens. Here is the extended source inversion objective function. It's still got this data misfit term, the first term, between the data we record at the seismometers and the data we predict from whatever. The data we predict from whatever estimate of m and w we have. But now we've added this penalty term, and the penalty term has a few pieces to it: it's got a penalty parameter alpha that we'll have to somehow decide what to use, and it's got what we're calling an annihilator, capital A, acting on the wavelet. And so, capital A in this case is going to penalize energy away from zero. Away from zero. We think that our source physically should have compact support around some point, say the origin. And so we're going to penalize the energy when you're away from that point. And so, you know, one possible annihilator in this case is multiplication by t. And then if t is close to zero, then this term, the penalty term, will be small and you could potentially have a bigger penalty parameter as you get close to the true solution. As you get close to the true solution. So that's our extended source objective function. It's got these two pieces. And here's a picture of the same plot you just saw in blue, the full waveform inversion objective function versus slowness, and now the extended source inversion objective function in red versus slowness. And there are a few things to notice. One thing is that we seem to have the same global optimum for both objectives. Global optimum for both objective functions, so that's good. But even more importantly, the red curve is convex, so we only have one minimum. And that's in contrast all these flat places for the full waveform inversion objective function. So it certainly looks like the red curve would be something that would be more amenable to local gradient-based optimization, and that's what the hope is. One thing to mention is that if we want to go to Is that if we want to go to this other objective function, then we have to worry about the fact that we're actually inverting for estimating two things: slowness and the wavelet. And the sensitivity of the modeling operator to those two parameters is really different. And so a method that was suggested about 20 years ago is the variable projection method, in which you do a nested optimization. And in the inner minimization, you're going to estimate the Minimization, you're going to estimate the wavelet. And in the outer minimization, we'll estimate the slowness. In the case that I've been talking about, this simple toy problem where we've got a homogeneous Earth and a single source and receiver, actually you can just solve the normal equations analytically to get the wavelet. And so you don't even need to do an inner minimization. But that's the way we're going to do things. We're going to have this nested optimization for the two parameters we're interested in. Parameters we're interested in. So, this is a really interesting, intriguing picture. And I can hopefully explain in one slide why it is that the red curve only has one minimum. So here it is. So if you do a little bit of paper and pencil work, you can actually write down the extended source objective function in this simple case. And I won't try to explain where this comes from, but it's not too hard to. From, but it's not too hard to get. The more important line of mathematics is the gradient of the objective function, which is the second equation here. And what I want to try to talk about is the integrand. So the integrand has sort of three pieces. It's got a wavelet squared, so that's non-negative. The denominator is also something that is positive, or definitely it's more than non-negative. It's positive. It's one plus and square. Positive. It's one plus some squared stuff. So those two pieces give you something positive. They're multiplied by the third piece of the integrand, which is the time t. And so if we consider a couple of cases for an estimate of the slowness, different estimates, if the slowness m is bigger than the target slowness, plus this lambda, which tells you the region from minus lambda to lambda is the Minus lambda to lambda is the region where the wavelet is non-zero. And then we divide that by this offset r. In our case, r is always one. If you have a slowness m that's bigger than m star plus lambda over r, then what happens is that this term m minus m star times r minus lambda, that's actually positive. So if we look at the lower limit of integration, the lower limit At the lower limit of integration, the lower limit of integration has a minus sign on that. So this lower limit is negative. Similarly, actually, the upper limit ends up being negative. So all the time values for m's bigger than m star plus lambda over r are negative. So we get a sum, an integral of some negative numbers. In front of the integral, we've got a minus sign times this offset, which is positive. It's a distance. Is positive, it's a distance, times something squared. So we've got something negative coming out of the integral multiplying something negative. So, in fact, the gradient for those values of slowness is positive. It's a negative number times a negative number. So, it's a pretty simple calculus problem. For the other set of values for the slowness m, namely m is less than m star minus lambda over r, you have the opposite. You have this m minus. This m minus m star quantity times r minus lambda actually being negative for the lower limit of integration. You multiply that by, then it's got a minus sign in front, so it becomes a positive quantity. Same thing is true for the upper limit. So you've got time taking on positive values in this integral, and then you multiply that positive number by a negative number, and you can see that for all the m values less than. For all the m values less than m star minus lambda over r, the gradient is negative. So, this is a very simple explanation of why there, in fact, is only one critical point for the extended source objective function in this simple case. And in fact, unlike the full waveform inversion objective function, which has all these local minimum values far from the global minimum, that is not the case for the extended source objective function. The case for the extended source objective function. So, Bill and Hui and I have been writing a couple papers. The paper was long enough that we split it into two pieces. And the first part of the paper is the results plus numerical experiments that my graduate student did. The second part is primarily Bill's proving of some of those results. So, here's one of the results that I want to talk about. The nice thing about this very simple The nice thing about this very simple setting of a homogeneous Earth with one source and one receiver is that there are results that you can actually write down. And that has been missing from this community's discussion about these extended methods. So the first result, which I'm actually going to illustrate with a numerical experiment, is that if we've got data that potentially has some noise in it, and if the noise is bounded above by about Bounded above by about 0.6. So we don't have more than about 60% noise. You can actually show how far all the stationary points are for the extended source objective function from the target slowness via this bound. And the important thing is if we look at a special case where f of the noise, eta, is zero, in other words, data where you don't have noise, then you can say that all this. Then you can say that all the stationary points for the extended source objective function, all these m-values, are within basically a multiple of that maximum lag for the source wavelet being non-zero over R, but R or R is taken to be one. So you can't get into a situation like this one with the blue curve for full waveform inversion, where you could have a critical point, a stationary point way over here. Way over here, which is very far from the target slowness you're looking for. So let me illustrate that with an experiment. So this is one of Hui's experiments. We're showing the data on the left, and the data is data with noise. In fact, we have coherent noise here. You could have, you know, uniform or Know, uniform or random noise. But in seismic experiments, coherent noise is actually more likely to occur. Here we've got basically a copy of the data that's been shrunk and shifted in time, and that is our noise. So it's about 30% noise. And in the middle picture, you see the two objective functions again. You see the least squares or full waveform inversion. Least squares or full waveform inversion objective function in blue, and you see the extended source objective function in red. And it looks from this picture like still the extended source objective function has a global min or its minimum at 0.4. If we zoom in on those objective functions on the right, you can see that due to the effect of the 30% coherent noise, the minimum of the Minimum of the extended source objective function, which might be pretty hard to see, but I'm trying to point with my hand here, is shifted a little bit. And I'm sure you can't read this, but it says X is 0.4013 something. So in fact, the global min has not exactly been captured by our extended source objective function. That's not surprising since this data has noise, but we've done pretty well the distance between the stationary point of the extended source objective function. point of the extended source objective function and the target slowness is about what I said 0.013. The bound in the result on the previous slide would indicate that all stationary points from the extended source objective function should be within about 0.06 of the target. And so clearly that's an upper bound. And in fact, this upper bound is sharp. You can change some things with where the noise is in time. Things with where the noise is in time, and you can basically approach this upper bound. So that's one interesting result. We can actually say something about how far the stationary points are from the target based on the amount of noise in the data and this maximum lag in the wavelet. But one thing I have shoved under the rug and not mentioned is when we talked about the extent. Is when we talked about the extended source objective function, I said there was this penalty term, and the penalty term has a penalty parameter alpha in front. And one of the issues with all of these methods is trying to determine what a good value for alpha is. So in the experiment I just showed you, alpha was fixed. It was constant. I believe it was value one. One thing that seems promising is not keeping that alpha penalty parameter constant, but allowing it to dynamically increase. Dynamically increase as the optimization proceeds. And this is a pretty cool picture that shows the objective functions, extended source objective function versus slowness for different values of the penalty parameter alpha. So the blue curve in the bottom here is a really small value of alpha, alpha equals 0.1. The red curve is a slightly larger alpha, alpha equals 1. Alpha, alpha equals one, and then we keep increasing the value for the penalty parameter. The yellow curve is alpha equals 10, and the purple is alpha equals 100. And the value you choose for alpha has a big impact on the rate of convergence of this variable projection algorithm. So, what we see with the blue curve is that it's pretty flat. So, the objective function is pretty flat. That means almost any of these points. That means almost any of these points would give you a good estimate, potentially, of the true slowness in the sense of looking at trying to make the gradient small, right? We want the gradient to be zero at a stationary point. It would be very hard with this blue curve, this small value for alpha, to tell whether we'd solve the inverse problem at our true solution of 0.45. True solution of 0.4 for slowness or at 0.5 or 0.3 because the gradient is so small for all those values. As alpha increases, things become a little bit better and easier in terms of doing optimization. But eventually, what happens is you get back to the full waveform inversion objective function with lots of flat places, lots of local minima, local minima. So the purple curve. So, the purple curve looks a lot like the full waveform inversion objective function. And so, you would have to be at a good place in the optimization close to the target of 0.4 in order for you to get there. Otherwise, you'd get stuck. So, the way the discrepancy principle that I'm going to show the next experiment using works is you start with, you know, you can actually start with a penalty parameter value of zero and something very small. And something very small. And then, as you start to approach the solution, you increase this penalty parameter in the hopes of finding the minimum you want. So here's how the discrepancy algorithm works. You have to specify a couple of things. Obviously, you have to specify some tolerance for the gradient to tell whether you're at an actual stationary point. You also have to have some idea of a range of errors. A range of errors. So, my error here is just the data mismatch. And you've got to say, okay, I want the data mismatch to be bounded in some range. And that might sound weird, but one thing you might notice is that as alpha increases, the error increases. In other words, these curves all move away from an objective function value of zero as alpha increases. So you'd kind of like to balance the idea of finding a station. Idea of finding a stationary point and increasing alpha so that you're getting closer to the target you want, maybe back to the full waveform inversion solution, with the fact that you don't want the error to increase so much. And that's where this range comes in. The other reason it's helpful is that you generally don't really know what the noise is in the data, especially for real data. So having a range of potential values. Values for the data mismatch make some sense. And so, all the discrepancy principle does is it says, okay, give me a range of acceptable errors. Give me a tolerance for the gradient. And let's alternate back and forth between fixing the slowness that we're really looking for and updating the penalty parameter and holding the penalty parameter fixed and updating the slowness. Updating the slowness. So you alternate back and forth between these two criterion until both the gradient is small and our error is in an acceptable range. So I'm going to look at the same experiment I showed you a minute ago, which had 30% coherent noise, and I'm going to try this discrepancy algorithm. For that experiment, I held my alpha penalty parameter fixed. Now I'm going to allow it to dynamically be updated. And a couple of Updated. And a couple of important things: the tolerance for the gradient is 10 to the minus 2. That will tell us whether we've got, you know, critical point. And the range for the errors is between about 0.027 and 0.11. So here are a couple of tables showing how this works. In the first instance, we're going to start with an initial penalty parameter value of zero. So if you don't have any information, that's a great place to start, right? Great place to start, right? It's something that, you know, is kind of easy to guess, and it works with this discrepancy principle. So we start with an initial estimate of the slowness of about a third, 0.343, and we do some updates to the penalty parameter alpha, holding the slowness fixed. And you see that alpha started out at zero and it's gradually getting bigger. As it gets bigger, the error, the data mismatch gets bigger. The error of the data mismatch gets bigger as well. But we stop when the data mismatch is in this interval. So between about 0.03 and 0.11, which we are here. And then we switch. We switch to holding alpha fixed and trying to update what we really care about, the slowness M. And you see, I don't really want to go into all this in much detail, but we did some local gradient-based optimization. Local gradient-based optimization using something called Brent's method, which is a combination secant bisection. And so, actually, that's why things look funny in the M column at the beginning. You have to have a range from the bisection part. But what you notice is that we started with about an M of a third, even though it doesn't look like it, and you get to target, you get close to the target slowness of 0.4 in 11 iterations of this algorithm, and you start. Algorithm, and you stop here because now the gradient is below our tolerance of 10 to the minus 2. Unfortunately, you haven't satisfied both criterions, both criteria, because the error now is outside the acceptable range. It's 0.018. You want it between 0.027 and 0.11. So now we switch back and we update alpha again, holding M fixed. We only have to actually do one iteration of the alpha update and then. Alpha update, and then our error is in an acceptable range. And then the last set of updates is for M. And after this set of updates, the gradient is below 10 to the minus 2, which was our stopping tolerance, stopping criterion, and the error is in the acceptable range. So that's how this dynamic updating of the penalty parameter can work. And I'll just quickly Can work. And I'll just quickly mention a couple of results from that experiment that might or might not be illustrative. I don't know. Remember that ultimately to solve the inverse problem, we want to go back to a physically realistic wavelet, which has support around zero, maybe in some region from some minus lambda to lambda. In this, we assumed that our lambda interval was from point minus 0.025 to 0.025. And here are pictures of the wavelets that we estimate. So, on the left are the estimated wavelets. On the right are some truncated wavelets where we set the wavelet to be zero when it's outside this region from minus lambda to lambda. The blue curve is our initial wavelet. Remember, we started with the wrong slowness, and so our wavelet is also incorrect. And the black is the target wavelet. The black is the target wavelet. The red and yellow curves, which you can barely see because they're under the black, are after updating the slowness in the first round of updates and then in the second round. And so this is kind of interesting mainly because of the next slide. And the next slide, I won't show the wavelets, I'll show the data. So note that the initial wavelet, the blue curve, is really far from the true target wavelet, the black curve here. And our initial amp. And our initial M was wrong. But look at the data match. The data match is shown on the left. The black is the target data. The blue is the data predicted by our initial wavelength and initial slowness M. And again, the red is after the first set of updates of M and the yellow curve is after the second set. But they all match the data. And this is the whole point, right? If you have two knobs to turn, both the way. Knobs to turn, both the wavelet and the slowness, you can fit any data. So, with this extension allowing for the source to also be estimated and for the source to not have to be compactly supported, any value of the earth parameters will allow you to fit the data really well. So, that's the whole problem we're trying to get around here. On the right is the data when you fuse the truncated wavelets. You fuse the truncated wavelets. So that's the end of the first part of my talk. And what we want to do now is try to actually take this nice, somewhat simple problem where you can actually show some things theoretically and illustrate why things work or don't with these extended methods and eventually get to applying them to a much more realistic setting. And so now I'm going to talk a little bit about microseismic source estimation. This is the realistic setting we're starting to work on. Realistic setting we're starting to work on now. So, to get there, I'm going to give a little bit of background about hydraulic fracturing. So, if you don't like hydraulic fracturing, you can think about pulling carbon dioxide that's being emitted from power plants out of the atmosphere and injecting it into the ground to try to reduce greenhouse gas emissions. That is still injecting large quantities of fluid into the ground, and you can still have these same little earthquakes that occur. So, this is a picture. So, this is a picture of U.S. oil production over the last 100 years. And you can see it sort of peaked in about 1970, and then there was a decline. And then over the last 10 years, there was a big increase again in production. And that was entirely due to trying to get oil and gas out of low permeability shale rock. In other words, breaking the rock using hydraulic fracturing and creating the ability to produce oil and gas from shale. Oil and gas from shale. And so, what is the problem with shale? Here is a two-dimensional slice of a piece of shale, and this is an electron micrograph scan of the shale. The gray is the shale, and the black is the oil that you would like to get out. And what you notice is what I just mentioned a minute ago, that the black blobs are not connected. This is a low permeability medium. A low permeability medium. Permeability is just the ability of, you know, if you have a sponge, how connected are the pores? How easily is it going to be to get the fluid in that medium to actually move to, say, a well? So clearly, shale is interesting to people. There's a lot of hydrocarbons in shale, but they're isolated pockets of oil and gas that are very difficult. Of oil and gas that are very difficult to produce from. And so, what people have been doing, and actually, people have been doing hydraulic fracturing for a really long time as an enhanced recovery, but it gained a lot of momentum since about 2010. And on the right, you can see a cartoon of how hydraulic fracturing works. So you have a pump truck on the surface. It's going to pump into this vertical well fluid, namely brine, at very high pressure. Very high pressure. The gray horizontal line here is shale, meant to be what's on the left. And you've got a horizontal well in the shale. And if you inject water or brine at high pressure, you can break this shale rock and create flow paths so that the fluid, these black blobs that aren't connected, can actually flow to wells. So that's the whole idea of hydraulic fracturing. When you inject the water at high pressure, you then create. You then create, you open fractures or you create fractures. You try to keep them propped open by injecting sand so they don't just close back up under the weight of the material above. And when this process is occurring, when you're injecting fluid at high pressure, you're trying to open the two sides of pre-existing fractures or create new fractures. And there's a stress release in the form of little tiny earthquakes. The form of little tiny earthquakes. And these are what are called micro-seismic events. The micro-seismic events are incredibly useful to people because if you know where they are and when they occurred and something about the distribution of the fracturing, you can tell a lot about how effective your stimulation process was. And this is kind of a fun slide that I'll just mention quickly. So the magnitude here is the Richter scale. If you know about earthquakes, you know where. If you know about earthquakes, you know, we're always hearing on the radio about a big earthquake somewhere, Haiti, or Mexico, or Japan, or wherever. And we're often hearing about earthquakes on the order of six to seven on the Richter scale. These micro-seismic events, thousands of them occur, lots and lots of them occur during hydraulic fracturing, but they're really tiny in magnitude. They're often below zero in magnitude. And an event that has the energy that's below zero. Event that has the energy that's below zero is similar to opening a bottle of champagne under the earth or shooting off a rifle. You often don't even feel these earthquakes until the events have a magnitude of three or greater. So there are a lot of these microseismic events that occur. They're usually not felt, they're very small, but they can act as a sort of a light bulb under the earth to illuminate what's down there and allow us to see. Illuminate what's down there and allow us to see things that we want to see. And so, how does this connect to what we talked about before? Well, it connects because I want to talk a little bit about how you would want to estimate these micro-seismic events and a little bit of past work where I did that with some students. So I just want to just three minutes, Mark. Oh, okay. Can I go a little bit over like five minutes to finish? We can. Okay, we can. We can. Okay, we can. Okay. All right. So let me talk very quickly about an experiment that Matt McChesney and Jordan Ketterley did. This is meant to be a flat two-dimensional slice down in the earth, an XY slice. We've got a horizontal well in black, and we've got two pre-existing fractures that intersect that well. So we're going to inject water at the place where the well and the fractures intersect, and we're going to record seismic data at these two receiver array lines. Two receiver array lines. I'm going to focus just on the purple fracture, which is angled at 60 degrees relative to this well. I'll ignore the green one. And what we actually did was we did modeling of injecting the fluid and the propagation of the fracture. And we used that information to define when micro-seismic events had occurred. And then we tried to solve the inverse problem with the data that's generated from those microseismic. That's generated from those micro-seismic events. So that's the roadmap here. And I don't have much time, but I'll just show you on the left here is for the 60-degree fracture in purple. There was a confining stress keeping that fracture closed. So it took a long time, about 1,100 seconds of injecting pressure for the two sides of the fracture to open enough that there was a space for the fluid to fill in, the water to fill in. On the right, I've zoomed into that picture with a short. Into that picture with a shorter horizontal, vertical axis. And you see what's happening is that water is injected to high pressures, nothing happens for a while, and then the two sides of the fracture open, fluid fills in, the rate of injection increases, and then the fractures lock up and the rate drops. And this repeats. This is called stick-slip behavior. And it's a physical phenomenon that occurs. And we have defined We have defined micro-seismic events to occur when not only does the fracture open, but the two sides of the fracture slide relative to each other. And when the sliding velocity of the two sides of the fracture is great enough, we say that a micro-seismic event has occurred. So all these black asterisks are micro-seismic events. And we are going to choose one of them. We've simulated this. I'm going to choose one of them. We're going to stick that in as a To stick that in as a source to the elastic wave equation. I won't go into this much. And what you see is that we can then say, okay, we can model the seismic data that occurs from these micro-seismic events at these two receiver arrays, one above the well, well, to the left of the well, one to the right of the well. And that's what's shown here. This is receiver data, seismic data at the receiver array. At the receiver array at 350 meters in Y, and the one at 50 meters. And the actual sliding of the two plates we modeled as well. And here you see the wavelet. The wavelet that I talked about before was a Ricker wavelet. People in earthquake seismology often use a Gaussian for their wavelet, but actually the true wavelet we get from the simulation doesn't look Gaussian at all. It's very low frequency, and you can see this stick-slip behavior occurring. And so our goal. Behavior occurring. And so, our goal with all of this was then to take the inversion methodology and say, can I work backwards from the seismic data that I've generated synthetically and try to invert for this source that we get from the true hydraulic fracturing process? So the red curve is our target wavelet. The blue, we started out with an initial source estimate of zero. The blue is what we get after one iteration of full waveform inversion. Of full waveform inversion, inversion, full waveform inversion on the right after 50 iterations of full waveform inversion. And at this point, the inversion stalls out and doesn't go anywhere. And so you notice some interesting things. You notice that we kind of get the peak time of the wavelet correct, but not the amplitude or the shape. And so I'll just quickly mention why. This is one of the big problems, again, in full waveform inversion. On the left, I've got the frequency domain, and you see in red the frequency content. In red, the frequency content of this true red wavelet that we simulated from hydraulic fracturing. The blue is the data. The blue is the frequency content of the data. And you see there's a sizable zero hertz component to the true source that we're trying to estimate that isn't in the data. And so it's very hard to recover the true wavelet here. If we instead filter this source we're trying to get by the Source we're trying to get by the frequency content of the data, we get this black dashed curve. And on the right is back in the time domain, you see the true wavelet we're trying to estimate in red. You see our inversion estimate in blue. And in black is what you get if you take the true source and filter it by the energy and the data, by the frequency in the data. And you see that we actually did pretty well in the inversion. This is also really. This is also related to this whole cycle skipping problem. And so, the last thing I want to say before I quit is that all of this work that I did with Jordan and Matt for microseismic source estimation assumed that we knew the Earth. We knew what the velocity of waves propagating through the Earth is. And in fact, we weren't doing acoustics. We were doing elastic wave propagation. You have compressional and shear wave velocity and density as the parameters that describe the propagation of waves. And if we look very quickly at an example, And if we look very quickly at an example where you try to estimate, say, a Ricker wavelet starting from an initial estimate, an initial guess at the wavelet of zero. If we don't know the correct Earth model, if instead we perturb the compressional wave velocity and the shear wave velocity by a little bit and assume the data was generated by a different Earth model, then here's how the inversion does. And the inversion doesn't do great, right? But it doesn't do that badly because the perturbation wasn't too off. Wasn't too off. So the blue is the inversion result. The red is the true wavelet we're trying to estimate. So the big meta point of all of this, going back to the first half of the talk, is that you don't know what the Earth model is. You don't know what the velocities of waves propagating through the Earth are. Could we now use this extended source inversion idea to estimate both the Earth model, the wave velocities, and the wavelet in this more interesting application of microseismic source estimation? Of microseismic source estimation. And so I've basically said all of this, and I think I will stop since I'm over. But thank you very much.