So, what I'm going to talk about is something that I learned over the pandemic, you know, being in an isolated situation. We decided to try to do something new. And it's a joint work with a former postdoc of mine, Jun Sai He, and a new student, Louis Liu, and my colleague, Rachel Ward. Research is supported partially by NSF, and they are over. They are all over. So, this is my attempt to connect to this conference. So, in general, let's say that we have some model, and this model gives you some knowledge about function g, which is a function of x, and g might satisfy some constraints like PDE. There's also probably space that tells you the distribution of points or whatever when you discretize. And classically, in machine learning, this. Learning, this is how I kind of put everything relates to different components for different research communities. So there are people who specialize in approximation theory that tells you essentially what is distance between one approximate function space to the ideal function space. And this distance typically depends on the dimensionality. And there are people who try to understand what kind of function and what kind of function space would not have to suffer too severely. Not have suffered too severely from the curse of dimensionality. Then there's also, you know, the objective is to construct an approximation of g of x under this kind of model from, let's say, in the supervised learning setup, from some kind of learning example, data, and typically the data is a finite number of data points, and n number of data points paired together. So you have the input x, and then you have the, some people call it the target. People call it the target or the label, like that. And so you choose a function space that you can work with, like a certain neural network, combine with the data through some kind of optimization formulation, and then you choose some kind of optimization algorithm, Q, some variance of SGD, and then you arrive at your function, your learning function, which is defined by Which is defined by a set of parameters. And when I put a star over here, it means that it's the optimum parameter. And these parameters will depend inevitably by the data as well as your training algorithms, because usually the optimization is not guaranteed that you always converge to global minimum, and it's not also clear that there is a unique global minimum. So, anyway, the point is that the function that you create. Is that the function that you create really depends on the data and your training algorithms? And then, in the end, ultimately, what I think is that it would be nice to have a certain kind of theorem, approximation theory, that kind of tells you that through this process, combining features of data and optimization, you created your approximation and how far it is. And perhaps, you know, we say how far it is from the target function that you try to learn on average, on average, with your. On average, with respect to a set of optimized parameters, that's found by your chosen optimization or things like that. So what I'm trying to say is that it would be nice to have some kind of theory where that tells you that the error of your approximation, how it depends on, let's say, your optimization algorithm and also on data. So in this talk, I attempt to try to study what are the What are the influences of data to these functions in a very specialized setup? Hopefully, it's still kind of relevant. So to start with, let's take a look at this kind of mean field, the handwritten digit, MNIST data set. So it has the idea of these kind of digits. Everybody knows that. So the original data set consists of points, and each point is actually corresponds to. Corresponds to a 28 by 28 pixel image. So this 28 square is 784. So each of such image corresponds to the point in the Euclidean space, R784. So these are all scanning people's handwritten digits. So it comes from, in a sense, a set of people writing handwritten digits. A set of people writing handwritten digit and then somehow digitize it and embed it into R784. And I could also ask the question: suppose I have a high-resolution camera, I can choose to digitize each one of the handwritten digit to be 3000 by 3000. And the same set, finite set of points, would then be embedded in this way by choice to our 3000 square, their high dimensions. So this smells a little bit funny to me because it seems A little bit funny to me because it seems to be a little bit arbitrary. And why do we embed it into our Euclidean space? It's because, let's say, the common neural network structure, it's just convenient to have an input arrange as a vector. So then we started to try to see: so, this arbitrariness of embedding space, providing that the dimension is high enough in some sense, does that come into the performance? Does that come into the performance of your neuronal robot? Okay? Okay. So indeed, there are many things, many people try to study, assuming that the real-life data most likely is going to be somewhat concentrating on some lower dimensional manifold that's embedded in some Euclidean space. That's embedded in some Euclidean space. Starting from, for example, this kind of manifold hypotheses. There are famous people working on it, publishing papers related to that. There's also a manifold learning algorithm, starting with the simplest singular value decomposition-based PCA to study, to study the geometry of the data manifold. So, by the way, when I say data manifold, I'm talking about a manifold in which x is sampled. So, then for each x, you have a response. So then, for each x, you have a response g of x by that. And then, of course, there is also this kind of classic, by now classical theorem, Johnson-Linden-Schwarz theorem, that tells you that if you have a finite number of points in Rd, then you can project it, you can find projections of these points down to a lower-dimensional subspace, linear subspace, that roughly preserves distances. So, all these are some kind of evidence that, you know, if you have a finite point set, maybe you can idealize that these point sets. Maybe you can idealize that these point sets come from a lower dimensional manifold, but you need to somehow properly understand what this manifold is. And of course, in the context of deep learning, there's also this kind of latent space and encoded structure. Input vector and then is mapped by some affine map and composed with the activation into successively lower dimensional vectors until you get to the smallest space that somehow depends. A space that's somehow determined by the user. It's called latent space. And somehow the assumption is that this latent space captures all the two things, right? The essential thing, the intrinsic thing about the data, which describes the function that you want to learn, and also the function, the certain feature of the functions that you want to learn and how it depends on x. And hopefully that this and the assumption or the hypothesis that typically you really Typically, you really need a smaller dimension to describe the task that you want to learn. Okay, so we do the simplest thing just to try to have some more solid hands-on understanding about this data set. So, if you throw in this data set to a PCA and you look at the output of the singular values, the distribution of the singular values, that tells you basically the value. Basically, the variances of the distribution of these set of points, how they spread along the subspace in each one of the subspace, the directions defined by the singular vectors that you found in your singular variety composition. So the larger the variance means that your data implies that the data somehow lies on that direction more. And the smaller the variance means that More, right? And the small advance means that your data actually doesn't deviate, you know, kind of just not too far, you know, it doesn't spread into those dimensions. So you can also think that the data tend to concentrate on the orthogonal complement of those directions like that. So here you see the kind of the statistics of, no, not the statistics, but this is the singular values, yeah, the histogram of the singular values corresponding to the single single single Singular values corresponding to the digits zero. And this is the same thing, but for all digits. What you discover, and these are how small the singular value is, the vertical axis. So if you select something that's kind of small, let's say I could claim, for example, for both, you know, this is a smaller data set, this is a larger data set. It seems that I can just select 40 directions, 40 orthogonal directions. 40 orthogonal directions. So, and define a 40-dimensional subspace, and it seems to capture most of the areas of your data set. So this kind of gives, for me, at least, a strong kind of indication that, yeah, indeed, the distribution of this data set seems to have some kind of concentration on a lower-dimensional space embedded in this kind of, in this standard setup. But then at the same time, you also realize that if each one of these images are strictly zero and each pixels in each one of the images are strictly zero and one, then each image actually lies on the vertex of this L infinity ball. So how do you, how, you know, it's a little bit strange, right, to think about R40 as a subspace and then, you know, the distribution of points on some kind of polytopes versus polytopes. So at least to me, naively, maybe. At least to me, naively, maybe, this suggests that, well, you know, in the 40, the way they distribute in the 40-dimensional space is not really kind of flat. Maybe they're kind of band around. There should be some kind of curvature going on like that. So what are the influences of the only dimensions for different characters? The dimensions are different ones, right? Yeah, they might correspond to different substates. Yeah. But even within, yeah. Because if you do S V, the single vectors are all single vectors. Yeah, but if you think about the distribution of all zeros, it's not on one vertex, right? It's also on some different top. So there must be some kind of notions of a servo notion of curvature related to that. And yeah, anyway, so we can talk infinitely long about this. It's very interesting. So, okay, so evidence that, you know, at least for this data set, there seems to be some kind of lower dimensional feature, a concentration of lower dimensional space. And then we were also working on some sampling algorithms that's closely related to, I don't know to say boldly, some X-ray transform that's applicable in high dimensions. That's applicable in high dimensions to sample discrete bone sets. If you apply this kind of techniques, so if you have good eyesights, you will see that these are all kind of wacky hand digit written digits. They are not very common. In fact, you know, they are the, one can argue that they don't appear in the, so characters, drawing or writings like this don't appear very frequently in the NLS data set. Okay, but they got picked up. Okay, but they got picked up. So this means that you can also somehow somehow define these some sort of curvature from the discrete distribution and also some notion of boundaries to this data manifold. And these characters and say that these images corresponds to points lying on the region which have somehow higher curvature. Because the theory goes, higher curvature points will get sampled by this algorithm more frequently. More frequently. That would be another topic. If you invite me next time, I will tell you. So now, let's just try to anchor down the notations and just further reduce the problem to something that we can handle. But first, so this is the function that we try to learn. It is a function, let's say formally, it's a function on a lower-dimensional manifold. In the first half of this talk, this will be just a linear subspace, lower-dimensional, and it's a real value function. And it's a real value function. And this is the neural network. It's just a function that's formally defined on Rm, on the entire ambient space. It's a function of the entire ambient space. And so also a real value, just for some simplicity. And this is a notion of the loss function based on L2 and the density or the distribution of your data points. And you want to minimize this to construct your approximation. To construct your approximation, F theta, suppose you select something, and assuming that nothing goes wrong, then you will get the optimum gravity set f sub theta star. And let's say that's putting it into an ideal situation that this training function has good generalization property, which means that the generalization error of this one is controlled. And classical notion of generalization error just means that this function. means that this function here, you know, when you integrate in the continuum setter, it's small. So then we can, the only thing we can say at this stage is that when we restrict the function at points on m, then it's sufficiently close to the function g that we try to learn. That's not the question we're going to address. The question that we want to address is: okay. The question that we want to address is what happened address is what happened after training I'm going to evaluate my f theta star at some points that's outside of the distribution that's slightly away from m and that's be reasonable that's you know that point is still close to m so we can formalize this by saying that we're interested in evaluating finding out the error of our approximation on m prime which is in some suitable sense close to the data manifold m. Like that. Of course you can write it in this kind of language. In this kind of language, suppose the generalization error is controlled, right? And your data manifold is lower-dimensional, then what can you say about the outer distribution generalization error when n prime is sufficiently close to m like that? And of course, strictly speaking, g is only defined on m, so we need to make some kind of uh assumption on how to extend g away from m to make this integral s to make sense out of this integral. To make sense out of this integral. And here we just assume that g is a constant extension along the normal directions of this manifold, like that, to create a design function. But of course, more generally, we can consider just a g which is globally defined in Rm, and then it's restricted like that. So that's another important point. Now, let's further reduce the problem to something that I can handle. So discrete data pairs like that, and then so let's assume that. So let's assume that you know, let's do a change of coordinates so that without noise, so you see the sigma here denotes the noise. So for now, let's assume that sigma equals to zero. So your data, then our data will be assumed to be concentrating on R's. So let's just do R2. So concentrating on the x-axis. And when there's noise in the normal direction, then you will have a, you know, the noise is described by that, and sigma obviously describes the variance. Sigma obviously describes the variance of the noise in the codimensions, in the normal directions, like that. And whatever analysis I'm going to present is invariant under this unitary transformation anyway. So it's very convenient to look at just our two situation. And then these are some standard notation assumptions about the distribution of your data points, that it has to be invertible. Right? And it has foray, which means that you cannot further reduce the data manifold to an even smaller data manifold. So this is m is already the smallest one. The x-axis is the smallest one. You cannot reduce it further like that. And then we're going to use some notation. So the noise, if there's a noise, and it's just a Gaussian noise. And then this notation that's commonly used by certain communities denoting the sample means or empirical means. Denoting the sample means or empirical means, and then it converges to your expectation of Z. Now, the simplest thing we can think about is linear regression. And I tried to see what we can say about linear regression, right? After linear regression, you define a linear function. And how would this linear function perform when you evaluate it outside of the x-axis? Okay, so this function has two weights, wx, wy, and these are the coordinates of the data points that you throw in. So this is the standard, very simple linear regression. And through optimization, you're going to find optimal parameters of wx star and wy star like that. Now, the first thing we notice is very simple, right? When there's no noise, then you cannot recover w sub y. Your least square problem is actually ill-posed. Problem is actually ill-posed. We don't have any information in the white directions. However, if you try to solve this optimization problems by stochastic gradient descent or gradient descent, then you will find a value of wy. And in fact, because your data doesn't have any noise in y, you don't see the variance in y. So the wy that you get is how you initialize wy. Is how you initialize WY. It stays constant along your training process. And that's sort of the arbitrariness that shows up. You initialize your weights and you train with some gradient information, but the gradient doesn't allow you to change that part of the parameter. So that part of the parameter set is not trainable. So we say that it's untrainable. Now, the next thing is that suppose you have a small variance, sigma. Okay? And then you can estimate, you know, What would be the solution to the least square problem? You have a unique solution to the least squared problem. X parameter is how it should be up to a suitable perturbation. The interesting thing is that the Y parameter takes this form like that. So, in particular, the blue part comes from, you know, obviously the 1 over square root of n comes from multi-colour integration, and assuming that your y has zero. And assuming that your y has zero mean, right? So then there's a balance. So there's this term 1 over the standard deviation times square root of n. So this says that if I fix the data set, and if I fix the variance of the data set, fix the distribution, if I'm allowed to keep drawing new points, enlarge my training data set, then I can control the performance, the behavior of my linear function, how it behaves in the y direction, and so on. So this tells you that if n is a small o of 1 over sigma squared, then my learning function will be essentially constant in the y direction, thus minimizing the error for generalization to out of distribution situations like that. So that's kind of interesting, but at that time I was like, well, this seems to be very straightforward. Well, this seems to be very straightforward, and we're so afraid that we suspect that this is known. But it seems that nobody has raised this question or pointed out this observation before. But from the numerical analysis viewpoint, in a sense, it's not that surprising if you take two steps back. But the distribution of this y is a small variance, it's a very sharp Gaussian. And of course, you need many points to resolve this thing. Solve this thing. Okay, so now this is about linear regression. And now let's say, can we say something about a two-layer value network? How would this low-dimensional feature influence the training or the performance or the stability of a two-layer shallow network? So here I did, so you can write. So, you can write it out, right, in this very simple setup. Alpha is the point-wise activation. In our case, it's RELU. And the input, you have two weight sets, and then a bias. And then this corresponds to the weight sets like that. And then they got mapped into the neuron in the hidden layer. And then, you know, after that, you get output to a particular node in the Particular note in the, you can kind of continue this picture by, you know, doing the same picture through many different neurons, and then you go on like that. Let's take a look at the activation, so the information that arrives at a particular neuron at the output layer this way. Okay, because ultimately we're thinking about analyzing value deep neural network. Okay, so in here, the notation So, in here, the notation goes: whenever you see a bar on top of this weight set, it means that that's already trained by your favorite optimization or gradient-based. Because again, if you look in your training data, you don't have any noise, it's concentrating on the X dimensions, then this W1Y, this weight set is really untrainable because your data doesn't allow you to see it. It doesn't allow you to loss. To see it. Doesn't allow your loss function to see it. So this is untrainable. And this is the arbitrariness of your embedding that comes in. And it will come in and kind of influence this one and deactivate it like that. So the task is how do you evaluate, how do you get a sense out of it or get some kind of inequality out of this? And you can say that this is some kind of posteriori error estimates because w bar is already trained, right? That's your algorithm. Already trained, right? That's your algorithm. And now you just have a different input and you want to see how your algorithm behaves. So it turns out that if on this layer here you have more than one neurons, then you add them up this way. And we call this function, you know, so the information here, so this EI, let's digest this EI a little bit. So EI. This EI a little bit. So EI is the error, E stands for the error at the ith neuron in the final output layer. And it's just the difference between when your input doesn't, the y value of your input is zero, right? And now you have non-zero y, and they are activated like this, and then you, and of course they are multiplied by this trained weight. Course, they are multiplied by this trained weight sets like that. So, the first task is to evaluate EI. What does EI look like? But if you stick with Rendl, it's actually pretty simple because this bias, together with a non-zero value related to your random weight sets and the input value y, provides a shift of the original functions like this. And this shift is in high dimension. And this shift is in high dimension, it's shifting the higher dimension. So you can think of it as there's a hyperplane around which, on one side of the hyperplane, your activation is linear, and then on the other side, it's just zero. And then the combination of these two terms provides, you know, just describe the shifting of that. And so then essentially what you do is in this high-dimensional space, the space is partitioned into three components. Spaces are partitioned into three components, three regions. In one region, everything is zero, EI is zero. In the other region, both are activated. And in the middle region, one is activated and the other one is not activated. So if it's red blue, it's easy because when it's activated, you just replace alpha by one. So this is very easy. Then what you can prove, then, you know, and because in the type of In the type of deep neural network that we can analyze, you can just write it as in a recursion relations like that, very easily. And starting with the F2, because this is the two-layer shallow network. And what we have done is we analyze the output or the error in the output of the second layer shallow network. And that would become the input to the subsequent layers. To the subsequent layers. And the influence of this, of the EIs that we see, will be just, you know, can be estimated by analyzing the recursions and use a lot of inequalities. And in the end, we can define this quantity, the change of f when you have a y equals zero or y equals non-zero, can be bounded in this way. Layer by layer, like that. And then you trace it all the b all the way back to F two. All the way back to F2. Now, then you can prove the following theorem. So there's a, so this kind of, we say that this is the stability indicator of out of distribution error. And it's bounded above by two factors, multiplication of two factors, train weights. So we concentrate on the colored one, terms, here, because they are related to whatever. They are related to whatever white components that you want to input and the set of untrainable weight sets, depending solely on how you initialize. And then what's rather interesting is that you have quadratic dependence over here, and then you have cubic dependence over here. But on the same time, this h over here is rather interesting because the gradient, so this is a function which is just, you know, kind of the output related to the. Related to the output of the second layer of the first two-layer shallow network, the output of the first two-layer network. Hij will be denoting different correspondence. And this gradient is with respect to the input. So the smoothness of this function with respect to the input somehow matters to the stability of such kind of network. So you need A to be nice, right? Otherwise, it doesn't work. A? Doesn't work. Uh, A? Where's A? Yeah, the activation function. Yeah, so here all these analyses are solely based on value. Oh, okay. It's absolutely important because you cheat, right? Then after the shifting, as I said, you know, either both are activated, then you can just subtract. Or one is activated, one is not, then you just ignore one and the other one becomes just the input arguments, or both are zero. So it's more importantly that this map. That this map from one layer to the next layer is stable, otherwise. Yeah, yeah, and that we don't assume to be, that's out of our control because that's assumed to be stable because it's trained by, for example, one of the algorithms that Mole developed. So the assumption is that when y equals to zero, you have a reasonable question. You have a reasonable problem to do. And maybe you can do it as posterior if you have a posterior. Posterior kind of estimates. For W, yeah, that we didn't do. Yeah. So far, we just assume that those are, you know, whatever the training algorithm you get, you get. You know, we just, we were obsessed in estimating the influence of these arbitrariness. Okay, and you know, I will just go through. So what are the strategies of improving the stability of if you are given such kind of If you are given such kind of network, what you can do is, of course, you can add some kind of regularization, weight decay, but since I'm a purist, I don't want to add any regularization, or I try to do something different. So we can say that you can augment your data in a suitable way. So for example, you can add noise. You can introduce noise in the white directions. If you know the manifold geometry, you introduce noise in the normal directions. But then you know from the previous analysis that you probably need to Analysis that you probably need to pay for using more data points to balance that. Okay, so I don't think we will go through this. This is just supporting the theorem, theory that, so yeah, let's go through then. This is the this delta F, the magnitude of delta F, and this is the noise level that you get. So, and then you see that for a fixed number of data points, you know. You know, you when you have a higher level of noise, then this error decreases. And here is for a fixed noise level, fixed sigma, then when you increase the number of data points, then this generalization or this stability indicator also decreases, just following what we discussed in the previous slides. Now, in most situations, suppose you don't have a good knowledge about your data. Have a good knowledge about your data manifold, then one strategy is just say, I'm going to add a small amount of noise in every single directions. But then you see that you add a noise and the noise may make a perturbation in the tangential directions. So therefore, you will have to pay. This is the additional error introduced by the noise, by the fact that you don't know your data metaphor that well. That well, like that. So, there's this kind of accuracy and stability trade-off. I think two minutes. So, for linear regression, I tell you that n has to, when you have noise in the orthogonal complement of the data manifold, I told you that n should square, should scale with 1 over sigma squared. But for deep Revel network, from our empirical From our empirical observation, it seems the situation is even worse. The scaling, you need to have even more data points. And there seems to be, so this is the number of dimensions, so the number of y dimensions, co-dimensions of your data manifold. And this is the number of layers in your neural network. But you see, at least to my eyes, the number here is this scaling beta over here. So the smaller it is, Here. So the smaller it is, the more data points that you need. And it seems that there's a gap between shallow network and deeper networks like that. The shallow network actually stays around 0.3738. And then here it's actually, as L go increases, it seems to be approaching 1.5, like that. We don't have any theory so far, but we're working on it. The last thing I want to show you is what happens if you have curvature. Is what happens if you have curvature? I got asked this question too much, so I said, okay, let's try to see what happens. We skip that. If you want, I can show you in private. So far, what we can do is to analyze the least square problem. So in this setup, let's say that this is the linear function. We try to find the optimal weight parameters. We minimize the least squared function. And in the end, what you need to do is you need to. What you need to do is you need to invert this matrix to find the weight set. But now our assumption is that xi are independent samples from some density, which is concentrating on a lower dimensional manifold that may have curvature. And so what would be the curvature effect to this question? How can we generalize the second slides of this talk? Now, R2, warm up, you rotate after. Warm up. You rotate after suitable rotations. You can just model your manifold as the curve, as the graph of a function, and k then is obviously the curvature. So then you invert this one, you work a little bit harder, what you find out is, I think it's interesting, when your target function is linear and the curvature is not zero, you can recover the linear functions when the data has no other noise in the tangential direction. No other noise in the tangent direction. But when there, of course, when there is no curvature, the least square problem physical posed again. Now, when your target function is nonlinear, then the non-zero curvature comes in in a very strange way. So you have one over curvature dependence of one over curvature for the white parameters, and then you have this curvature multiplied to the second derivative of. So anyway. So, anyway, when your target function is non-linear, then the curvature effect comes in. And that may be a non-negligible perturbation to your optimal weight sets. You generalize this to, say, hypersurfaces in higher dimension, meaning that surfaces, manifolds has one co-dimension, that you can do. You can generalize to and You can generalize to, and you'll have more complications because you have many curvatures, etc. You generalize to higher co-dimensional manifolds. The simplest thing we can try is curves in 3D or curves in any D. When you have curves in any D, there's something very elegant that shows up. This matrix that you try to invert has a very nice structure. You can define curvature and torsions and generalization of that. Torsions and generalization of that, and they come in a very beautiful way. And you can, basically, the theory goes the same. There are some kind of non-negligible effects from the curvatures. Now, the ultimate question, what happens if you have manifolds with higher code dimensions in higher dimensions space like that? This is something that we're currently working on because, in this case, it's rather difficult to define a suitable curvature system. A suitable curvature system. But then you can still say that the nonlinearity in your manifold comes into, it will introduce non-negligible perturbation to your optimal weight set. And if one of the, if your data, so yeah, so I think I can stop here. So as soon as your data set, the manifold has, is flat in one of the tangential dimensions, then your Dimensions, then your at least square problem becomes your post. So I'm curious, why would you have to add noise just to the normal direction?