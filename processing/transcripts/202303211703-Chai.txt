Different applications. I don't want to miss all the names here. So first we started from the Fuhshima nuclear expedition and we use that to build the inverse modeling system using high-speed. Then apply that to the volcanic ash and well fire and the similarity between those two we are using the satellite observations. So I'm going to probably just give like a brief Give like a brief introduction about volcanic action, not talk much about the welfare. And then we use Captax, the control release field experiment, to update the model, improve the models, because we know the exact image. So you can do a lot of things to evaluate your system. And then another topic is about SO2 emission estimation using aircraft observations around. Aircraft observations around several three power plants. And just recently, we're about to share something with you. Probably a lot of people mentioned a H3 model. And some people may just think that's a trajectory model, but actually you can run in the dispersion mode. In the dispersion mode, it can be 3D particles or path or hybrid of them. So that by looking at the particle transport, you can actually get a concentration similar to your Larry models. You can get almost everything from this Lagrange model. And we do a lot of different applications, such as the example I'm going to show. The example I'm going to show. And here is another example. We showed a simulation of Ohio derailment happened this year. So they provide the prediction of the concentration. So the first example we did is using the Fukushima accident. And here is the estimation. Estimation, you never had the exact emission, so it had to be estimated using concentration and depositions information. And here you can see the emission profiles, they are keep updated. And the first year, 2011, when it happened, it has some kind of estimation, then it's get more information and get correction. So we are going to, for this study, we are going to use 2015 updated emission as a reference. And this one is actually using a lot of local concentration and local deposition, so it's pretty accurate. What we did is we don't want to use a local operation because the whole problem we want to solve is for the US application. We assume we don't have access for. We assume we don't have access for those local observations. And the observation we are going to have access based on the global network from CDBT is called Comprehensive Nuclear Test Band Treaty Organization. They have, actually their stations are not already working. So at that time, only 14 stations provided observations. Observations, samples, all the samples are 24-hour samples. But we have a lot of information from the Euro, so you can see pretty dense network there and also US EPA and some extra stations. So altogether we have more than 100 stations and they provide a 24-hour average of error samples. So we have more than 1,000 samples available to do the To do the immersed modeling. So the basic methodology is similar to the others. It's a 4D bar framework. They have a cost function to minimize, basically measure the difference between your observation and your multiple computations. And here is just having another term with a smoothness term. We are going to show why we have that. And here is the visualization for the HMIC. H matrix, we call that TCM transfer coefficient matrix. Actually, it's just source receptor sensitivity, or you can call some other names. But we inherit TCM because at a high speed, we have that package, so we just call it TCM. And here, basically, you can see X is a release time to the emission at different hours. Different hour actually is we have here is six hour segments. And on the Y is a station response from for each emission segment. So you can see different stations, you get different response. And it's pretty dense because I for each station I did the average for all the samples with more than one sample. Modern sample for this. So it's not exactly the H matrix, it's just a visualization of that. And because that's the first time we apply the inverse modeling, so we want to make sure everything works alright, we started from the train experiment to build the pseudo-observations by adding a random model and adding random error with caution. Error with Gaussian distribution. And here we just want to show that for 20 experiments, we want to test how to choose your control and metric variables. It's basically we can use the original emission or the log emission. And we can, for the observation comparison, we can use the concentration directly, or we can use a log concentration to compare. compare. And it turned out all the cases, we got pretty good results for the first about the first 10 days. So the reference, we use the reference to generate pseudo-observations. But later time, because you don't have much observation to constrain those as early times. So you don't get as good results as early times. But we found that if you use a low constraint, That if you use a log concentration and the log emission, you get almost the best results. But the most other cases you can get, you see the correlation coefficient is like one for two cases when we use low concentration as a metric variable. So then we apply the confident like the method works alright. So we apply it to the real data and on the left it just shows the time series of the The time series of the emission compared to we got and compared to the reference one. So we see a lot of the emission segments are not constrained by the observation. So they fall back to the first guess here. And that's a reason why we wanted to use, that's not realistic. We wanted to use Smoothma's penalty terms. Penalty terms. So on the right, upper right, it shows like two cases with two different smoothness parameters we get kind of like realistic results compared to the reference. But actually, because the reference emission, they have very like six hour resolution for the emission. But for our measurements, we only have 24 hour. We only have 24-hour measurements, and our number of observations are so many. So, we probably cannot really expect to get the best results. So, what we did, then we just compare, well, we did the running average of the reference emission. Let me see much better agreement from over immersion results and theirs. So, you can probably see the red one and the brown one. Red one and the brown one, if we use running average, we can get almost identical results for the first 10 days. But later on, because you don't have much constraints, so your results are going to get worse. And also for the other pair, R03 with the green line match pretty well at early times. Okay, so go to another application using satellite observations, and here is Observations. And here is for the volcanic ash application. And for this, we have the volcanic ash product, especially for this application. So we have not only for the mass loadings, we also have the ash claw top height. That information available. So on the left, we show the mass loading part, and on the right, we show the cloud top height. So we have more information. And here we show two granules. Show two granules because it's basically coming from the modus. So it's a polar orbiting. You get a second granule almost half a day later. Actually, for the methodology, a lot of times you can construct from your model simulation using different ways. One way is just integrate from the From surface to the cloud top. And then, because actually, all the volcanic ash clouds are not very thick, and sometimes they don't come down to the boundary layer. So that probably like we have two other options. One is assume the thickness is only one kilometer for the ash crop, and the third one is three kilometers for the ash crop. Three kilometers for the astral top thickness. So one kilometer above and one kilometer below in the center, the top height. And we found that using that option, we get almost the best results better than the first two. So here is just using that to do the inversion and estimated the emission actually as a function of. Actually, as a function of time and also the release height. So we use that estimated immersion results to predict or to get the predicted emission, predicted methods. And here on the left is the observation for the three different granules. They are all used of this. So that's just rare analysis kind of. Is kind of. In the middle column, it's using the GDAS meteorology input. And on the right, it's using the European data. So you can see a little bit difference, but the two meteorology data set are also consistent. And the results really match the observation pretty well. Although you can see the difference, of course. And then next, we want to see whether the image. The immersion results give you capability for you to forecast. So, this we use estimated emission. You just use three granules and trying to do the forecast for the next two granule four and five to see whether we can get pretty good forecast. It turned out that not bad. So for the data. So for the GDAS European data, they're pretty similar and but not as good as before because we assimilated the data. But still you almost got the pattern and method in the magnitude pretty good. In this table I'm not going to talk too much, but it's basically we're trying to answer a question about when trying to do the forecast, you To do the forecast, for example, if we want to forecast for the granule 4, that's your target. And you already have the granule 1, 2, 3. But sometimes, maybe when you use granule 3, you can get better results than using all of them. But it turns out that the system we have here, if we use all the granules in the task, they all contribute. I got we got better results by using all of them. And similar, like if you have a target of granular file and also just using all three, you get better results than just use the last one or last two. You don't throw away the old information. So, as I mentioned, the welfare application is very similar to the volcanic ash for satellite information, but it's just Information, but it's just LOD information. And we built a system. We have the prior coming from the USDA Forest Service. They have the blue sky to estimate the emission. So we use the geostation satellite AOD information to estimate, to do the inversion to estimate the fair emissions. And then we use adjusted fair emissions to do the forecast. The forecast. We are already building this system, and this is one case study. So, actually, if you look at the left one, one biggest difference between this application and the volcanic ash is you have a lot of hotspots. So the emission can come from everywhere when you have a fire, but for the volcanic ash, you only have a single location. Have a single location for the release, although you can release from different heights. So it makes the problem a little bit more difficult to solve. Obviously, you'll get pretty good results. And here, I'm not going to talk too much. It's basically, if you are interested, you can see the reference here. So next, we use the control release because we don't have a lot of this kind of like Of this kind of like release experiment available. So we go back to 40 years ago, that's a CATEX experiment, happened in 1983. So for this experiment, two release locations, one in Dayton, Ohio, the other one in Canada, seven releases, but one of them didn't work as well as the other, so we are not going to use that. So you have 84 different stations available. Stations available. For example, for release two, we have 400 measurements available for each measurement since like six hours sometimes three hour average. So first I wanted to test this one is whether we can use a dynamic model uncertainty. So when we say dynamic, it means one It means that if you adjust the emission, your model uncertainty should be changed. So every time, when your emission is adjusted, your model simulation is going to be adjusted as well as a result. So when you compare adjusted prediction with observation and your uncertain Your uncertainty terms should be adjusted well. But it's caused one problem about your cost function, it's going to be kind of a moving target because that term keeps changing when you adjust your exhaust term. So we have to, this on the left, the figure shows if you don't do anything, that moving target could cause you to have some like a spurious. Have some like a spurious solution, like out zero solution and the two minimum points. So, the our solution is to add in a vector to just normalize the cost function is to go back to have the same magnitude as your first guess using the first guess part. So, it turns out that by doing that, the method works pretty well for this release. Pretty well, for this release two experiment, even if we have different model uncertainty parameter configuration, we almost always get very good results here. You can see the table around 60. So, using the same method, we can apply it to all six different releases. And here, probably your first. And here, probably first look at the accumulating that column is assuming we know the release location. And it also, I didn't describe how we got the source location, but maybe later on I can explain. And we can also use the method to just estimate the source location. Source location. But if you don't know the source location, your Q prime is not as good as you know the location for sure. So we get the Q prime is assuming we don't know the location. So we can estimate the location. It's basically you have a matrix of candidate locations and you do the forward runs to see which one matches the observation the best. The best. So that's like the lowest cosmometry is going to be our solution for the emission and emission rate and location. Actually recently we just, well that work already been published and recently I just did some other tests to check the consistency between the forward and backward sensitivity. Sensitivity. And by seeing the forward sensitivity, we just run the model forward and get sensitivity. We are source receptor sensitivities. And we can also do the backward run because a lot of people like footprint, we are all doing the backward runs. So for this case, we still use this captain because we know the exact solution. And we did 121 for 21 forward runs, and we also from release 2, we have 400 measurements. For each environment, we do backward runs. So we compare them up. We got more than 48,000 sensitivity calculated each way, two different ways. Then we compare them. So that's a visualization of that matrix, like 48,000 sensitivity. 8,000 sensitivities. And on the left is from the forward run, and on the right, from the backward runs. You can see basically it has the same feature as you expect, but definitely some difference. If we plot them as the scatter plots, you see probably no buyers, but a pretty big sprite. A pretty big sprite. And you notice that's a long scale. So a lot of sensitivity calculated two different ways may be pretty different. So that's kind of a concern. But we can still use the backward sensitivity to calculate, to do the same E-MERS modeling. Here on the left, it just shows. Here on the left, it just shows inverse modeling using forward one, getting the location, source location for the release tool, and also the emission rate. On the right is using the backward. So you see the location error is 40, around 40 kilometers for the backward, using the backward sensitivities. And for the forward run, it's 13. And also the emission rate calculated. Also the emission rate calculated using the forward run is only about 2 kilogram per hour. It's about below 5%, but for the other one, it's about 20%. So the forward run will give you better results. And backward run, I think probably backward run, maybe we need some special treatment for long-working data. For boundary conditions and some other things. Although, I haven't probably need to look into that whether we can improve that. So the last example is about immersed modeling to estimate the SO2 emission around those three park lands. So the location is close to the border of North Carolina and Virginia. Here we show the competition domain. Show the computation domain. The inside domain has a three-kilometer wall run, and we use that to drive the SV model for this application. On the left, it shows the flight track in the morning on the top and in the afternoon. So you can see like three pork plants: Ross Borrow, Citair Ross Borrow, and Battles Creek. Barosboro and Battles Creek. So, those two are very close to each other. Actually, here we show the TCM separated for the three different programmes. And you can see they are pretty sparse. And for this problem, it's actually a good thing because this problem, that means this problem can be decoupled. They are not going to interfere with each other. That's because the flight. Because the flat is very close to the core plants. So, and another thing is we found that for the morning and afternoon, we can also separate that. So, it becomes six separate problems for the three power stations in the morning session and afternoon session. So, for the plumes or the emission from the power plants, probably plume rise calculations are very important. Very important. It's probably discussed in the morning, and it requires a lot of parameters from the biology model for this sturdy parameter. But one of the biggest uncertain part is about the power plant stack heat emission. How much to calculate that, we need the exit temperature. I thought that was pretty easy to get, but we can't. Pretty easy to get, but we contact the power plants, they don't provide us that. It's not required by the EPA. So we don't have a good information about that. And on the right, we just show how much the emitted heat is going to affect your plume rise calculation. So if you look at the red lines, Battles Creek power plant, we use the emission heat of 50 megahertz. Heat of 50 megawatts and 100 megawatts. So you can see they almost doubled when you increase the emitted heat from 50 to 100. And same thing for the CPR of sport, the time series of pump rise. You can see if you have the fifty megawatts, it's more than doubled uh most of the time for the when you use uh twenty megawatts. It's expected, but Expected, but we don't know the emitted heat for this application. So, what we decided to do, we checked the literature and we know the possible range of the emitted heat. So, we just have 15 different candidates, all the possible heat emissions from the power plants. That's you can pull it on some more runs, but it's kind of different because I No different because you need to pick the best solution. It's not like you can average them together. So the optimal solution you can pick by looking at the correlation between your model results and the observation. You can also calculate the rooming square error between them. In terms of some of the results are pretty consistent. Results are pretty consistent. For example, Ross Borrow Plume. For the morning session, 70 megawatts give you the least looming square error and highest correlation. I didn't list the correlation here, it's about 0.8. But for the CPI spore, you can see the they're pretty different. So uh to give you an idea about uh the difference, You an idea about the difference. Upper two panels correlation-based results. The bottom two, rooming square-based results. And here, snapshot the prediction from the dispersion model at 800 meters above ground. And on the left, volume flat. The model is at 17C, and on the right. 17Z, and on the right is for the afternoon flight, is the 19Z. We also overlay the observations here, but first let's look at the difference between the correlation-based results and we'll describe these results. If you look at the here, so that's a CPI rock sport. So you can see a big difference. That's like a That's like a correlation very high, and then it's very low. And some of them are identical here, as I showed earlier, like in the table, some of the results are identical. And also, there are some difference for the CPI loss for and although we have the observation here, but the observation are from different height and different height, so that's not a direct comparison. But here you can see the plume. But here you can see the plume captured by the model, but a little bit shift. So, in order to do the real comparison, we have to have the curtain clause here. If you are not familiar with the curtain clause, it's just a time series of vertical profiles. And here we have the morning flat on the CO on the left and the optimum flat on the right. But you can see the transaction really narrow. So, what do we to let you see better? To let you see better, we enlarge some section of that. So, here on the left, modeling flat, you can see full trendex, and still you can see some identical results for the correlation-based and looping square-based. But you clearly see some plumes here, you can see the difference. And for this, the C means the CPI rock score plume originated from there. And for this one, you can see probably. And for this one, you can see probably the remaining square based results can be better agreement. And for the afternoon, you can see even bigger, more bigger differences. And the poom is much higher for the correlation-based results and a lot lower for this. But when you compare the aircraft observation with your model, you actually, both cases, you see very good agreement. Agreement. But you don't have observation to verify which one is better, actually better. So that means probably at the next time, when we have the flight content, probably we need some spiral to resolve this. Okay, that's a problem with this play. But actually, if you use two different methods, all of the results seem contradicted. The results seem contradict to each other. Basically, most of the time, we get pretty good emission estimation. And here is a compare with the continuous emission monitoring system, CX. They are considered almost like within 5% of error or 10% of error at most. But you can see for the morning session and afternoon session for drugs for you get pretty good results for all the two different methods. All the two different methods. The only difference here is for the correlation-based results, probably. For the CPI rows model, it's not as good as the only square-based results. But basically, you don't have observation to verify this. Okay, that's a summary. Let's see. What we learned from all those applications, one thing is we found that. We found that for the in-sectron measurements, like the nuclear application, or even for the flat observation or the field experiment, we use a lot of concentration and we get better results. But for the satellite observations, you better use the original variable to compare with each other because uh satellite observation has a lot of errors. Probably it's not wise for you to to log. For you to log conversion there. And another thing is immission-dependent model uncertainty and cost-function normalization really work for the field experiment captax case. And lastly, we showed some like a remaining problem. Although we can get a very good emission estimation for the SO2, but we don't know whether we get the good estimation. The good estimation for some bad reasons, but not like at least the contradictory results there. And also, we demonstrate sensitivity calculated for the using a forward and backward run can be very different. So something worse for the task. I think that's all I want to see. Are there any questions? I don't understand why you use the question and the connection. There have been graduates, for instance, square grader. So yeah, question about uh this one, right? So here that's uh H, I mean the high-speed model here and all mean observation concentration, M is the index of the measurement, and here divided. And here, divided by the uncertainty terms, the error. That's a combination of everything, observational model and the representative error. And here is your emission first guess. But we don't use the priority because we don't have a good priority. I think the question is about the penalty function. Uh the penalty one. Okay. Uh the penalty terms is basically you don't want uh sudden change. So the Sudden change. So that's a second directory. So if you penalize this sudden change, you introduce this curve. So you get a pretty small result. Most of the time, we used the screen radius and not the location. Well, that's that's mostly a term of the majority of the ma So that's Muslim term I used in the past for some other application, they use that word pretty well. And here it works really well. I know you can achieve the same thing using some other formulation, but this one, I think basically you don't want a large gradient. So this works. That's uh the second derivative basically is uh like uh your gradients uh minus the next gradient output. minus the next gradients are going to change too much. So it's penalize, you cannotize this kind of like sudden changes. I'm sure I missed that point. But what is your action if the model proves this version of the trajectories not fit the observed ones in terms of spread and in terms of direction? Spread and in terms of direction? Yeah, sometimes we have that kind of problem, but when it happens, we try to improve the metallurgy input. So we believe if you have good metallurgy input, you should be able to have a good simulation. Otherwise, if that pattern is not so good, you probably So-code, you probably you are not going to get all the results. You are so is there some action like, for example, as the part of the filter people do by sequential importance resampling of certain trajectories, or there must be some systematic way to address this. No, actually, we don't we don't use hello. I know for this one, I know all the particles, we just All the particles, we just go into sample like a regular ball. So it's not. So it's a high-split problem, not. No, it's a metallurgical problem. It's not a high-speed problem. Most of the time, I think... Oh, you mean the problem? Are you talking about like forward, backward or some like parts? Components of the dispersion. The components of the dispersion is not met with the observations. Well, I think here that's we when you use a real data, you never get a perfect match, right? And we for this dispersion model, most of the time that's due to the metallurgy input. And when we improve that, when we get better match, and if the result is not going to be If the result is not going to be good, and then probably just maybe we reject that part, we're not going to do analysis for that part. You had another slide where you had a metric that contained a convoluted Suyanov term in there that looked really unusual. Can you discuss that and give some intuition behind it? Of which one? It was a slide where you had a table at the bottom. Okay. Yeah, this one. Can you discuss that rank metric? Well, that is for the dispersion model. A lot of time we use this rank. It's a combination of four different metrics. First is correlation coefficient. The second one is a fractional bias. Then it's a critical. Then it's a critical success index. And last one is called the common signal parameter KSP. It's basically the largest difference between the cumulative distribution for two. And so the largest value you can get for the wrong is 4. And here you can see sometimes most of the time we get close to 3. So that's considered pretty good performance. I've just never seen that metric. Is that something very specific to this metric? I think for the dispersion, for high speed, and even for a flex part, most of people use that. It's a combination. I don't think that's the best metric, but that's popular in that community. What do you think the best is? Well, there are like some other metric, including Like some other metric, including another factor, it's like a file. It can be like but I don't know.