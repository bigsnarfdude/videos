The organization of the COL conference. So, today, I'm going to talk for an hour. So, I'm going to probably talk to you about kind of a story having to do with learning data manifolds and then learning flows on them. So, one kind of high-level motivation that I keep in the back of my mind when I'm thinking about this kind of stuff is that we have in science, we have very highly complex entities, like biophysical and biochemical entities. Biophysical and biochemical entities. And they're very complex in a very standard sense of the word, that they have a lot of interacting components. And so the question is: now that we have all of this data, can we actually learn anything about this underlying entity? And, you know, the data can take various forms, but to us, it's all a matrix or perhaps a tensor. So single-cell RNA sequencing, as an example, is a kind of data type that you can think of as a modern data type. Modern data type that has modern data problems. So there's a lot of observations. They could be thousands to a million observations. And there are also a lot of features, anywhere from 20 to 30,000 features, depending on how you count the different gene variants. And so this is a data type that comes with a lot of challenges, as do these others. So molecular data, so people can now map out the structures and folds of a lot of molecules. There's data that Molecules. There's data that has been around for a longer period of time, like fMRI or patient data. The difference now is we just store all of it. So there's huge databases of fMRI data. And every time you go visit your doctor or hospital, everything is all stored in an EHR. So this gives, makes these data sets really, really large. And so some of the challenges that are associated with this data, as I said, it can be very, very noisy. So it's very high-dimensional, but usually most biomedical measurements have limited sensitivity and have. Measurements have limited sensitivity and have a lot of noise. And that just the sheer dimensionality of the data makes it difficult to intuit anything unless you represent it in a form where you can understand something about the structure or the state space of the data. And kind of the main coherent thread through my talk today is this idea of learning dynamics and then reasoning about the dynamics that these high-dimensional entities go. That these high-dimensional entities go through. So, let me talk about that just a little bit more. So, when you have some of these biomedical data sets, by necessity, you measure some kind of biological entity at some point in time. And that gives you a static snapshot of data. But really, this entity persists and continues. It's a very dynamic entity like you and I are. And so, you're necessarily seeing a very static snapshot of a dynamic entity. So, the question is, can you? So, the question is: Can you actually learn the full dynamics? And will that tell you anything about how the system is interacting? And so, some of the things you might, so here's sort of a typical setup of this kind of data. Let's say we have single cell data and we measured it at one week, two weeks, three weeks, and four weeks through some cultured condition. Okay, so then this is a long gap to lead between these because cells are constantly changing. Cells are constantly changing. So, you might want to interpolate what is happening at a time when you didn't measure. What does the population look like? And on the other hand, you might want to say, oh, the cell is an individual entity. What is its fate? What would it have done if I hadn't killed it by sending it through my sequencing machine? So these are like typical questions. So in this sense, in this field, we often don't have time series data and we want time series data. So there's a lot of machine learning you have to do just to get the time series data that you can do cool, cool stuff with. That you can do cool stuff with. And so I'll talk a lot about how to get the time series data and a little bit at the end about what we could potentially do with it. This is going to be the kind of outline of my talk. And I guess I'm missing one bullet here. So first, I'll talk about how the structure of this data really helps us with most of the challenges, including the dynamics of the data. And that's the idea that this data. And that's the idea that this data does, to a large extent, have manifold structure. The second thing I'll talk about is the idea that you have densities that are changing. So here is one blob of data or population of data, and it changed, and now the population is distributed differently over the cellular state space or manifold. So we'll talk about how to do density estimation in this space so that you don't have to do density estimation in some type of. In some type of 20,000 dimensions, I think that's what's making the noise, 20,000-dimensional space. And then a little bit about how we actually solve this problem is we try to transport this population from one to the next to the next. But not just any sort of transport, potentially a biologically plausible transport. So we'll lead up to that about how we would transport these densities or populations. Transport these densities or population distributions from one time point to the next. And that would bring us to our manifold interpolating flows work that me and some of the other co-authors here have just completed. And finally, the missing bullet is that after that, I'll talk a little bit about if you actually have dynamics in a time series form in biology, what kinds of analyses could you do? And there, like Liz was showing yesterday, you can use a lot of topology. Showing yesterday, you can use a lot of topology and other things defined on here. So, let's talk about the first part of this: the idea that data has manifold structure. So, the kind of picture you want to have in your head is that of a surface or a sheet that's in a higher dimensional space, but the actual data is restricted to this sheet. So, you can think of points when you get point cloud data, like the point clouds that I've been showing you, as points. Showing you as points that come from a manifold. But of course, they're usually like splayed off the manifold a little bit. But actually, just thinking about them as coming from a manifold will maybe tell you how to denoise this, which is you have to restore it to the manifold. And so I'll talk about that a little bit. So, and just to give you a rough definition of what a manifold is, manifold in a mathematical sense is a space that locally resembles a Euclidean space. Resembles a Euclidean space in each point. And this actually does help us because we have a whole bunch of measurements here. So, locally, we're usually able to use Euclidean distances, but not so globally. And of course, in a Riemannian manifold, this kind of metrics comes from the tangent space inner products that are defined that give you an infinitesimal notion of the distances. So, Alyssa, you might be asking, okay, this is a really great assumption, and it seems to immediately. Really great assumption, and it seems to immediately give you a lot of things to do. But why is it true at all? And so, first, I'll say the proofs in the pudding. Whenever we've made manifold models, they actually predict things about biology. So that's a little proof in the pudding. The second thing is you can kind of reverse engineer the logic out. Okay, so the thing is, whenever you're measuring a biological entity like a cell, you're measuring different components of your cell. So let's say you were measuring different components of like my body parts. Components of, like, my body part. So, if you measured where one of my feet were, my other foot has to be really close by. It can't be somewhere else, right? That's because my two feet are coordinating to move me. Similarly, the genes in a cell make proteins, and these proteins have to coordinate in order to do anything. And so, by necessity, the features are not uncorrelated like this, but rather, rather than saying something linear like correlation, I'll say they're mutually informative of each other. And this restricts. Informative of each other. And this restricts the state space a lot. And I also said this space might be smooth. So why could it be smooth? The reason is because biological entities sort of usually gradually transition. We all gradually transition from single cells to whatever trillions of cells we are. And it didn't happen like instantly. And so, if you measure a lot of cells, you'll see cells in different phases of this kind of transition. And that gives you something smooth to estimate. So these are the reasons. To estimate, so these are the reasons why this kind of data might actually have this structure. So, how do you learn it? This is similar to what Tommy showed in the last talk if you guys were there. So, usually we learn the underlying manifold by turning the data into a graph. And this is one typical way, although there are other ways. One of the ways is to take your observations by features, which I'm calling cells by genes, to gauge. I'm calling cells by genes to get you to think about the scientific data again. And you turn them into distance. But remember, distances are not globally valid on manifolds. So we have to localize them. So when you localize them, we typically pass it through a kernel. And for example, if you had a sparsified kernel, you get something like this, where the local relationships are preserved. But usually we go one step further than this. Further than this. And what we do is we reestablish global relationships on this graph as random walks on this graph. And this is really why we use diffusion, because now this cell is no longer connected to this cell, but there is a way to get there. You just have to go through the manifold of the data, which could hopefully mimic differentiation or some kind of response. And so that's really what the diffusion operator does. So we mark off normalize this matrix, and then we usually. This matrix, and then we usually power it. And powering it to T gives you a T-step random walk probability of going from one data point to another data point. And so the diffusion operator could also be created on any graph for if, for example, you got a data set as a graph, which we sometimes get, and you would do the same thing. Sometimes you add laziness to this. The other interesting point that I want to mention about the diffusion operator. About the diffusion operator, is its eigenspace. So, Dobby already talked about the fact that this creates the classic diffusion map embedding where the distances are diffusion distances. But really, what I want to highlight is that these eigenvectors are the same eigenvectors of a similarly normalized graph Laplacian. And so these creates harmonics on your graph. And these harmonics actually allow us to do things like density estimation, filtering, denoising, and things like that. So this is. Like that. So this is a Markov matrix, and the eigenvalues go in magnitude from one to zero. This would be the trivial one. And next to the trivial one will be a low frequency signal over your data graph. And then a signal that's further down, like the second to last eigenvector, will be a fast moving trend. So again, thinking back to the canonical example of, oh, we have data on a manifold. Now, how do we denoise it? You probably want to remove these high-frequency vectors because this corresponds. Because this would correspond to a high frequency noise. And that's actually what we do. And I'll talk about it in the first method that I go through as an example. And the final thing is that it usually helps us also when we want to do any kind of learning. So if we want to learn to predict the label or anything like that, or if we want to generate data, we have had a lot of insights arise out of manifold regularizing the latent space. A latent space and then learning the prediction because you're coming up with an abstract representation that's good and is more likely to generalize than just immediately classifying. So we've seen this work in many, many instances. So we have different pieces of work that show visualization, course screening, both of which I won't talk about, but maybe Guy will tomorrow. But today I'll maybe focus on the top quadrant here, where I'll talk a little bit about. Here, where I'll talk a little bit about imputation because it introduces a certain kind of filter, and then I'll talk about density estimation. So, my actual first foray into this kind of manifold learning and why it helps with this high-dimensional data was, I don't know, sometime around 2015 or 16, and then it was published years later as the biological clock of publication is different than the CS one in 2018 in Cell. In 2018, in Cell, and this is the algorithm that we called magic. And I think we called it magic because our collaborator was like, Oh my god, this is just like magic. And I was like, okay, we'll have to make an acronym for magic. And so the main idea is to actually use this powered Markov affinity matrix and actually multiply by the original data matrix. So in simple terms, what this does is it replaces every feature in every Replaces every feature in every data point as an average of its diffusion affinities. That's sort of what it does. But if you actually look in the frequency domain of what it's doing, it is creating a filter where it's eliminating those high frequency eigenvalues. So it's low-pass filtering the data in the graph Fourier domain. And so with this, we can get very drastic results with this kind of data where we picked a data set that's bulk and we sparsified it. We sparsified it like single-cell data. We just dropped 80% of the values, and you can re-impute it and denoise these types of trends. So, this was really promising to us. But at this point, I maybe have semi-convinced you that these manifold models are useful for single-sale data sets. But let's go to what I was talking about: multiple data sets, collected over time or in different conditions or whatever. So, this is kind of a problem where. This is kind of a problem where you have to lift some of these notions. So that's why density estimation helps from a single data point to a whole set of data. So initially we dealt with just two sets of data. And this was work that we published, I guess, last year in Nature Biotechnology. It's called MELD. And the idea is that we can density estimate over the manifold these two conditions. These two conditions, and we can really pinpoint where they're different. And we'll show you why that's an issue and why we had to come up with a solution for it. So, in this kind of data, you often have a control and a treatment, or a healthy person and a sick person, and you're measuring their blood, or you're measuring something, and you really want to see how it's different. But it's really difficult to see exactly how it's different, it's not a simple number. So, for example, the brown is Brown is how the stimulated cells fall in this fate plot. And blue is how these unstimulated cells fall. And you can see there's quite a lot of overlap. And then there's some areas where one seems enriched and the other doesn't seem enriched. So it seems like the right thing to do is actually just estimate their density and compare them to one another. And that's actually the idea we have. The idea we have is that these come from a common manifold because they're describing a common system. Because they're describing a common system. There's the same cells, they're stimulated or unstimulated, but the stimulation somehow changes the density on the manifold or whatever the condition is. And we want to get an idea of what is the difference between these two densities. And when we get an idea of the difference in densities, then we can say, you know, what's really different, what cell types are different, what genes are expressed differently. By contrast, most of the state of the art. Yeah, go ahead. 20,000. So, when the state of the art, even probably to the present, is just to take your data and cluster it, right? And then say the cluster has more representation in this condition than this condition. But you guys, being data geometers, might understand that those clusters might have nothing to do with the response, right? So, what we're suggesting is to put off clustering until. Suggesting is to put off clustering until you actually discover these densities and how to compare these densities. And then, if you cluster with that signal in mind, you'll get some clusters that are more indicative of the real truly affected cells. And that's what this is illustrating. So how did we do that? So again, we have this data graph that I already described to you. And we just created indicator signals. So we combine the cells from both the samples. We created these two indicator signals. Two indicator signals, and as you can imagine, if you just have these two indicator signals, they never overlap. So, that's not what you want, that's not what you want to compare. You have to smooth them. And the way to smooth them is to do some kind of kernel density estimation. So, we do a type of kernel density estimation on this graph where each cell is sort of treated as a bin. So, to formulate the kernel density estimation, we actually just analytically saw. We actually just analytically solved this optimization here. In this optimization, here Z is the density estimate. It agrees with these indicator signals as much as possible, but it's also smooth on this data graph. And L is the Laplacian. And so you might be wondering, what is L heat? So, you know, why didn't I use just the regular Laplacian? And the reason is because this heat Laplacian is related to the heat kernel, which has been. Related to the heat kernel, which has been shown to give us stable kernel density estimates. And that was proved in 2010 in the continuous setting by Botev et al. And so we transferred that insight. And so we get very, very stable numerical estimates under Chebyshev polynomial approximations of the filter that arises from this. And so what that filter does is also a low-pass filtering. So just to make that idea a little bit more clear, you have cells. More clear, you have cells, these cells have some kind of gene. And here, instead of the gene, we have indicator signals on each cell. Which condition did they come from? Did they come from the control or the experimental condition, black or blue? And what we're doing is akin to Graph Fourier transforming them using the eigenvectors, modulating the frequency components, and transforming them back. So that's basically you. So that's basically what that filter does. It modulates components in the frequency domain. And from this, what we could see, and I don't have a lot of time to go into this perhaps, is that first off, we can easily tell which areas of the manifold are enriched in the control versus the treatment, purple versus the blue. But another interesting thing that just kind of came out of it was you can also tell which areas are transitioning because you can actually look at the frequency also. Because you can actually look at the frequency also of this resultant signal. So, this is clearly a transitioning signal, so it'll have a low frequency component, whereas this is not, this is just an area that's completely unaffected by the treatment. So, this has a very, this just has high frequency components. So, these markets, what do you want these microscopes? They're encoded by a graph. Yeah, so I don't have dimensionality reducing, I'm just turning them into a graph. So once you have these two densities, actually we came up with a likelihood score based on this uninformative prior we had. And now we have a score for each cell that says whether they're in one, they're associated with the treatment or if they're more likely to be coming from the control. And this is kind of, however, proving Is kind of, however, proving that this is useful. So, if we look again at this cell stimulation experiment, this is a type of immune cell called a T cell, and it was stimulated. And then you can look at after MELD, you get this likelihood signal. So, what you can do is take the cells that are really enriched in the treatment and really enriched in the control and see what cell, what genes they're expressing. And when you do that, what we saw was we get sensible gene annotations with a lot. Annotations with a lot higher significance than we would if we had clustered the data first and then just measured it. And so that was just kind of a sanity check that our sort of density estimation method is saying something about biology. So that was two. Let's move further and see how we can compare many samples against each other instead of just two. And so now instead of describing in great tedious detail what's the In great tedious detail, what's the exact difference between those, which you could do with Meld again if you wanted to? You just get, we're just looking for getting a distance or an Earth mover's distance. So what we do is we actually use the density estimates that I just spoke to you about, but we use them at multiple scales to derive an Earth mover's distance. So I think most people know what Earth movers distance is, but it's just the amount of work required to transport one distribution to another if you use. One distribution to another, if you use an optimal transport plan. So, this usually involves finding a joint probability distribution that's called a transport plan and then penalizing it by the ground distance that you move here. And this will become important because here you see the ground distance that's implied by this distance matrix is Euclidean. So you're just looking at the length from here to here. But I just got done telling you how the manifold is really where the single cell. Is really where the single cells live. So you'd want the ground distance to be a manifold distance. So that's something that we work towards. So the other thing is, since we just want the distance and not necessarily currently the transport plan, we are trying to derive a witness function. A witness function is just a function that has to be one Lipschitz, and it is a function that really exposes the difference between these two distributions. So it's very different expected value under one distribution than it does. Expect a value under one distribution than it does another distribution. And of course, this idea comes from the Kentorovich-Rubinstein duality. And usually, how you actually do this is a little, I think of it kind of like a game. Whenever I look at a dual form of an Earth mover's distance, what I usually see is two histograms of data that are computed at different scales, potentially, or that are computed at one scale. Are computed at one scale. First, you take the difference between these two histograms, but this is not a valid witness function that doesn't have to be smooth and Lipschitz and all that. So you make it smooth by decomposing it with limited scales of wavelets or something like this. So this is the intuition behind how a lot of these dual form Earth move resistances work. So we use this inspiration to come up with a diffusion Earth move resistance. So the idea, just like in MELD, was very similar. We have indicated. It was very similar. We have indicator vectors over these graphs, but we use different scales of filter. Here, actually, we use the diffusion filter instead of the heat filter we had derived, but it works with a heat filter too. And we use dyadic scales. So we take these indicator signals and we're diffusing, we're applying the diffusion operator at dyadic scales to these signals. So now what you have are different scales of a density estimate. Different scales of a density estimate. Now you can just take the difference between these scales of a density estimate. So these are the different scales of diffusion. They're dyadic scales from zero to k. And you have a limited number of scales because you have your graph has some diameter. And then you just take the difference between these to get a dual form of an Earth mover's distance. Now you can have distances between all these distributions and you can again embed it. And so you have this distance matrix that's created from these diffusion EM. From these diffusion EMD, where each entry is a pairwise Wasserstein distance. And we were able to show that this is actually the discretization of a continuous result. So this, if you had manifold ground distance, then this is a diffusion distance that uses the manifold ground distance. And the diffusion operator actually converges to the heat operator in the limit of infinitely many points. And what we saw here is this. And what we saw here is this notion is A differentiable, but B also pretty fast to compute. What the difference that you kind of see is that you have greater accuracy in terms of the 10 nearest neighbor finding on ground truth distributions. So actually what we tried here was we had a thousand distributions that we sampled on a Swiss roll. So we had a distribution centered at every point on a Swiss roll. And then we were looking to see where the neighbor distributions. And then we're looking to see who are the neighbor distributions. So, not the neighboring points, but the neighbor distributions. And that's how one of the ways we're measuring accuracy. And you see, it's as accurate as Sinkhorn, which is primal form, but it's much faster. But it's less fast than some of the dual versions, but more accurate. So you're getting something in between the really fast dual versions and the best known, fastest known primal version. And here we tried to And here we tried to embed each dot. Here is a COVID patient. And if you, there were about 300 of them. The COVID patients who are colored in red are unfortunately the ones who died. And the COVID patients that survived are on the right. And there's a whole spectrum of them. And you see diffusion earth movers distance organizes them by just looking at their blood cells. And so this is a very simple organization because I didn't want to get deep into immunology, but you can see the other embeddings maybe don't have as much of. Embeddings maybe don't have as much of that structure. But you do need a lot of data points to compute this accurately. And so we had like 80 million data points from 300 people. So now finally, now that I've talked to you about optimal transport and the manifold assumption and density estimation, probably I can now talk to you about how we actually learn the dynamics and the data, which was kind of a guiding thing that we wanted to do. And this is. And this is, of course, joint work with Guy, Alex, who I think is online, and some others who didn't come here. So, this is a couple of neural networks, and these neural networks actually try to learn the dynamics in data by means of optimal transport using a manifold ground distance. And so, let me try to tell you a little bit about. Try to tell you a little bit about this kind of time course data just to make it more specific. So, this is an embryoid body differentiation system. It's actually the same one that we showed in the fate paper. And here you have data points collected every three days, and they're kind of grouped together like this. So we have five data points, but these are the human embryonic stem cells. You can kind of see they're differentiating into these different branches, but you really don't know how. You really don't know how. So, what you actually want to do is: so you have these cell by gene measurements and you want to infer a flow that goes like that, right? Somehow through these. But you don't know which cell exactly goes where. Though I will say there have been other things that are in roughly this space. One of the most popular ones is something called RNA velocity, where you can discover an instantaneous velocity vector for cells. This is some artifact of the Cells. This is some artifact of the gene expression measurement. But what we would say is that instantaneous velocity is not the same as the whole dynamics, and especially in areas of the data space where it's not available. And the second one is people have tried doing vanilla optimal transport on this. And then you get this kind of jagged thing, which is not necessarily what the cells are doing. Like they're probably not moving that way. And what we want is sort of these realistic transport. So, but you know, the key property. So, but you know, the key property cells have is that the cells have to transition through the allowable parts of this state space. So, in particular, that's an implausible path. So, nobody should be going that way, even if it's a shortcut to going to that data point. So, the way we sort of go about this is at its base, we train a neural ODE. We train a neural ODE. So, a neural ODE is a neural network that looks like this. And I know this is different than the picture that's in the neural ODE paper. I find that picture super confusing. So, this picture is, this is a neural network that computes a derivative, okay? And it puts it through an ODE solver to integrate it out to different time points. If you have data at any time point, you can penalize it. But the reason you had to do this stuff is because you don't have access to the derivative. Is because you don't have access to the derivatives. You just have access to what it integrates to at specific time points. And so that's what the neural ODE does. And this ODE solver is allowed to repeatedly call this, depending on however it likes to sample or whatever ODE solver you put in there. And this does ordinary differential equations, starting from an initial condition up to some system time t. So this is the typical initial value problem that's solving, except this interval. You problem it's solving, except this interval is being computed by your ODE solver, uh, in your and the F is being computed by your neural network. And what the authors of the original paper say is that there's some analogy to the Euler's method and residual neural networks. So by analogy, they think of it as an infinite depth network where you're not doing the even sampling. So the other key trick about neural ODEs is they avoid backpropagating through this ODE. avoid backpropagating through this ODE solver by using this adjoint sensitivity method that allows this effectively the derivative to skip from here to here. And then you have to back propagate through the rest of it as normal. So that's the gist of the neural neural ODE solver. So neural ODE solvers have been used for what are called normalizing flows. So normalizing flows are, you know, usually you start with some simple distribution. Distribution, and you apply invertible transformations to go to another distribution. These arrows are reversed, so you can train it the other way. You can train it from the complex distribution going to the simple distribution if you want to. And then you can use a change of variables to calculate this. And then what they showed in the neural ODE paper is, this is for some tutorial slide, that what they showed in the neural ODE paper is you can have deep normalizing flows, and then you can make them infinitely deep to get continuous normalizing flows. To get continuous normalizing flows. But we feel there initially, we had a paper like two years ago called TrajectoryNet that tried to use continuous normalizing flows to solve this problem. But we ran into many issues there. One is that these normalizing flows are, as I showed you, designed to flow from a Gaussian distribution to a more complex distribution. They don't naturally care about the path that they use, which we were able to solve and I'll show you how. But there is You how, but there is at their base, they're not able to solve that, and so just the way they're set up is much more suited for data generation than interpolation because you have to go every time from a Gaussian to whatever time point you have. But instead, what we want was one population at one time point to be transported to another, and we wanted to follow those manifold paths. So in order to address this, we decided to go away from normalizing flows. Away from normalizing flows. So, what we're doing instead is we're using these two neural networks. And the first neural network is called the geodesic autoencoder. What the geodesic autoencoder does is it's a normal autoencoder, but the latent space is penalized to be this multi-scale diffusion distance that I showed you when we talked about diffusion EMD, where you have these multiple scales and you're treating each point as a Dirac distribution and you're computing. Distribution, and you're computing this multi-scale distance. And so the loss function, you know, uses comparison to the geodesic distance with the embedded distance. That's what the geodesic autoencoder does. So we put our data through the geodesic autoencoder, and then we put it through the neural ODE. So what is the neural ODE trained to do? The neural ODE is trained to match what the predicted population at any time point is with the ground truth. Is with the ground truth to ground truth, and we put in you know one data point at a time. But because we already encoded our multi-scale manifold distance in the auto encoder, we can just do regular optimal transport, whatever the fastest method is we have here. So one of the problems we'd also encountered with the continuous normalizing flows is they create kind of these circuitous paths because we're not penalizing what the paths should look like. And so these might not be biologically plausible. Biologically plausible. So, what we actually do is add something called an energy regularization that penalizes the path length. So, this is the, you're looking at the magnitude of this derivative and how it's changing, and you're trying to penalize for too many changes. And so, what we were able to show is with this penalty in mind, you get an optimization that's like this. Get an optimization that's like this, where the marginals at zero and one agree with our initial and final distribution. And it has been shown here that this is equivalent to something called dynamic optimal transport. So this is a theorem from our paper that shows that if you have sufficiently large penalty so that D, your discrepancy between the penalization that we have, that this is going to give you this regularized flow actually gives you dynamic. Regularized flow actually gives you dynamic optimal transport. So, just to help you understand it, with the regularization in both the CNFs and our manifold flows, can give you a dynamic optimal transport. So, how do you transport from this to the S? You should just go where in the S you belong instead of sliding down the S. And so, with the energy regularization, in either case, you can fix it. But the manifold flows actually helps. Manifold flows actually helps you stay on the manifold and have you not start with a Gaussian distribution. So, anyway, we've already, so this is already dynamic optimal transport. It's pretty cool. It could do population migration and disease spread and all sorts of other things. But actually, we know even more structure about this system, right? Oh, I have a repeated slide here. We know that there's some additional properties of cells. So, cells aren't just transported from one time to another, they can also divide and they can also. They can also divide and they can also die. So, this leads us to try to make this an unbalanced transport formulation. And as I mentioned before, there's a very popular measurement called RNA velocity that's available on it. So, actually, we try to include these in. So, if you try to include these birth and death rates, then basically you have to learn a rate for each cell. And we actually use an auxiliary neural network to learn these rates so that you get some level of. So, that you get some level of unbalance if it helps your optimal transport penalty. An additional manifold penalty we use is a density penalty. So, again, the reason to use manifold penalties is not to have these paths. And so, we want our paths to go through dense areas. So, this is actually a simple K and N based neighbor penalty that's a density penalty. So, we have these additional like structures in. And so the RNA velocity typically looks like arrows over this. And whenever it's possible, Over this, and whenever it's possible, whenever there are arrows, not over empty space, we'll try to agree with it. And this is just a kind of like angle angle regularization. And so, with these, you see that, you know, what we want to do is have things transported over a manifold and then branching. And you see the base models kind of getting pushed, and then the velocity and the density regularization help it kind of go through and diverge. This is a newer picture with our media. Picture with our Mia flow model, which threw away normalizing flows. The original trajectory net is not able to diverge correctly, probably because it has to start from Gaussians all the time, unimodal Gaussians. But MioFlow is actually doing a good job of immediately diverging and following the paths of these flower petals. So the dynamic is like that. And so when you look at it, it also kind of looks a little bit magical because you're. A little bit magical because you're transporting these dead cells over time. If you say it that way, it sounds magical. And then you get these cellular trajectories. Okay. So most recently, something that I've been kind of excited about is some of my students, including Alex, who I guess is technically formerly my student. He's now at Mila with Guy and others, are studying diseases. Are studying diseases because diseases are totally characterized by dynamics. So, in almost any disease you can think of, and the one we're focusing on is breast cancer, there's relatively normal cells that go through some kind of dynamic transition to become a cancerous cell. And then cancerous cells go through another transformation to become these invasive metastatic cells that can kill you. So, if you want to understand these dynamics, we hypothesize that a really good way. Uh, hypothesize that are really good ways to collect temporal single-cell data and then understand these. So, our uh, you know, our collaborator, Christine Schaefer, um, who is in the Government Institute in Australia, collected these time points of what are called these mammospheres for us. Um, and the way we use it is trajectory net and now MioFlow is we use this data and we learn the trajectories, and the trajectories give us access to gene trends. And this can allow us to apply causal network analysis. Allow us to apply causal network analysis to get gene networks. So the landscape or the state space, roughly speaking, looks like this. There will be, you know, just transformed cancer cells, but they're not very aggressive. And you can watch them in a plate become more aggressive. And this is the state from which they can become metastatic. So, hypothetically, if we understood these dynamics, we could even reverse it and make them go back to this other stage. And this And this is not confidential. I had to write this here because we entered a Yale Innovation Summit, which actually we won. So that was good. And that was a format for that. So we have these five time points of data here. And what you actually get when you run trajectory net is you have these lowly aggressive cells. Oh, actually, these are the lowly aggressive cells. These are a little bit more highly aggressive, and you see them transitioning. See them transitioning. One transition goes to like a metastatic type state, and one transition goes to like an apoptotic state. And you want to learn what is different about these transitions. So because you have these single-cell trajectories, you can see on average, that's why these have these thick bars, what happens to these different genes. And we'd use some kind of imaging to validate, you know, what's happening to this gene, like Zeb, right? Like it's lower at day 14, gets higher, gets higher still, this kind of thing. Still, this kind of thing. And so, then what we did was we took all the genes in some of our clusters and we ordered them like this. And we got five gene clusters based on the dynamics. And after we got the five gene clusters, we applied Granger causality to derive a causal network, which our collaborators pruned using transcription factor targets and literature. And this actually gave us a few targets. A few targets. But the things, and we haven't tested these targets, so we have not cured cancer yet. So, but we've done some mouse tests on them. But what we see are downstream are other genes. And these genes can't be used as drugs, but these genes can be used to see if the idea or the network works. So, in particular, the Zeb inhibition leads to a smaller number of those spheres growing into this metastatic state, which we were able to show. Able to show. And also, my student Monik was able to show that when he downloads TCGA data, that the genes that we indicated have higher expression in multiple cancers. So that is a little bit about how we infer dynamics, a little bit about what we can do with the dynamics. And in the last like 10 minutes or so, I'm going to talk about what else you could do with the dynamics, especially if you had very rich, rich dynamics. And so this is a paper that's actually in BioArch. So, this is a paper that's actually in bioRxive. I mean, one version of it, it's in bioRxive. And this is a way of differentiating between different types of biological dynamics using topology. So here, we're actually using a different system where we can just get the time series. So, we have these cycling stem cells on your skin that are using calcium for signaling. And so, these calcium signals look like this. Let me try to show it to you. Okay, so they look like stuff is flashing. And these are cells that are being imaged by some kind of imaging. I think it's two-photon imaging by our collaborator. And some things that is apparent is these, this kind of signaling, it seems kind of random. And people really don't know why there's calcium signaling like this that's just flashing on and off in epithelial cells. Your neurons use calcium. Your neurons use calcium to communicate with one another, but that's very different than this kind of random flashing. We want to know is there coordination also? So we created a pipeline that uses a lot of our know-how. So first we created a cell-cell graph. Then we used these calcium signals as calcium signals as signals on the graph. Then we used multi-scale scattering transforms. Scattering transforms. And then we used dimensionality reduction and topology to show that these are different. Just to go through these steps, the graph creation here is very easy because we have spatial locality of these cells. And so the calcium is now the signal on these graphs. And then we take these calcium signals and we use these diffusion wavelets, which Michael, I think, talked to you about. These are differences between two scales of diffusion, just like you saw in the density. Scales of diffusion. Just like you saw in the density estimation, we use different scales of diffusion. This creates what's called a diffusion wavelet. So we have several scales of diffusion. So if you have a signal, you have different scales, and then you can convolve your signal with that. And when you take that, you get a second order signal, and then you can do the same thing over again. So this is like a deep scattering transform. And sometimes it's written like this. And then we take the scattering transform of every time. So, we take the scattering transform of every time point because we have signals patterned in space and in time. So, the spatial patterning is captured by the scattering, the time pattern is captured by fate. So, we see these trajectories. So, you see in particular, for example, that this is actually a pretty smooth trajectory. So the reason we use fate, of course, is that it doesn't shatter the trajectories as some other popular methods do, and it actually shows the curvature of it. Fate, of course, also uses. FATE, of course, also uses diffusion geometry, and maybe Guy will talk about that. And so the way the FATE plot emerges out of this is you get a vectorization of the signaling pattern at each time point with scattering, and it starts to develop the dynamics using fate. So then the fate dynamics show you when you're returning to a similar state as you had before, and things that would be very hard for us as humans to detect. Know here says humans to detect. So, and the final thing we do just to feed it to a machine learning algorithm is we compute the topology of these loops that we're getting. And we currently were just using persistence homology. Maybe Liz can tell me more fancy things to use here. But these are actually pretty good for classifying between different kinds of dynamics, at least that we have. So now you can see, you know, does fake connect everything or into a smooth trajectory? To everything or into a smooth trajectory? No, actually. If you look at any kind of signaling in the brain, it looks very different. It doesn't have this kind of smooth coordination. So, and so, as a result, the persistence diagram looks very different. It has many off-diagonal values like this that are being created at many different scales. Whereas this really smooth dynamic has those features that are way at the top right. So, they clearly looked different to us. And so, then one thing we were able to do because that's a sanity check. Do because that's a sanity check. Neurons are very different from epithelial cells. So, then what we did, what our collaborator did, was they took parts of the mechanism that transfers calcium signal from one cell to another and perturbed them using genetic knockouts. And when you do genetic knockouts of this, you lose this coordination and you start getting more chaotic trajectories from this. And so, there's a couple of things that they saw. They saw that cells in G1 are really important for maintaining this. Important for maintaining this, and they saw that certain proteins in the cell that regulate from the cell cycle of the signaling are also really important in this. And so we saw that both the genetic controls of different conditions looked pretty clearly different. So, from there, they were able to connect Connexin, which is a protein, to cell state and show. Two cells stay and show that this connects in sort of binds or accumulates around G1 cells. And so that causes G2 cells to be the main signaling factors. And they saw that there's still local bursts of activity, but to get this really kind of more smoother trajectory, you need connexin and G2. So, what it seems like from all of this, me not being a stem cell biologist, is Cell biologists is that the cells actually have a very smooth coordination and they have a program in mind where they have coordinated signaling that performs some kind of homeostasis thing. And this is not the case in most other tissues. And if we were just looking at those random flashes of cells, there's no way we could tell that. So it's really the tools of manifold learning and topological data analysis that really like unveil this to us. So that's sort of where I want to end. So see if you guys have any questions. See if you guys have any questions. We, of course, have lots of other works, but I just want to show you our GitHub. That's where we have most of the code for this. So, if you want to play around with our code, go for it. And I'm going to stop there and take any questions.