Of the day, which will be at XBLAP, and he will talk about the global convergence of the baseline breakhand flow of Coulomb discrepancies. Okay, so thank you very much for the invitation. I'm going to discuss the work for the PhD students in John de Fappan. And hopefully, the Fappin is coming soon. So, let me give you a quick introduction to the problem. I'm going to I'm going to study an interaction energy on the space of probability measures, whether on a Riemannian manifold which is compact, whether on Euclidean space. This energy will be quadratic. It's an energy with respect to mu. Mu is a probability measure. And essentially, okay, so what is mu? Mu is a target measure. It's a fixed target measure. Target measure. Target quality measure. Essentially, what I do is simply to take the L2 norm of the difference between mu and mu with respect to the square root of the kernel k if it exists. Okay, some sort of L2 norm of a smooth version of mu minus μ. And in order for f of nu to be positive, to be non-negative and to be a sort of norm square on the space of probation, On the space of probability measures, I need to have some assumptions on the kernel k. And natural assumptions is k to be symmetric and k to be also conditionally positive definite. The definition is coming soon. What I want to study is a kind of super boring kind of evolution equation, which is the gradient flow of this energy. Energy with respect to the Vashtein geometry. And so, what does it do? It's a continuity equation which drives mu, which would drives mu to the equilibrium measure mu. And what I do is simply I take the gradient of a potential. The potential is simply k controls with mu minus mu. And the question I want to address is the long-time behavior of this. is the long time behavior of this partial differential equation. Okay. Okay. And so what is a conditionally positive kernel? It's almost like a Gaussian kernel, almost definitely positive, but in fact it's only positive definite up to this condition. So what is a kernel? K takes as input two points on R D or on your manifold and it shoots And it should satisfy this condition for every such collection of vectors P. And standard examples of this kind of kernel are positive definite kernels. Essentially you take Fourier transform and it should be positive in Fourier. But the the kernel I'm super interested in is called the energy distance kernel. called the energy distance kernel and this is minus the distance between x and y. So in Euclidean space you simply take minus x minus minus the norm of x minus y it gives you a conditionally positive kernel. And there is also the Coulomb kernel but be careful the Coulomb kernel here it blows up along the diagonal okay and what is nice is that And what is nice is that in all these cases, at least for probability measure such that this energy is finite, then you have f of mu is zero if and only if mu is equal to nu. Okay? Okay. So now I can explain why I am motivated by studying this problem. So there is one first motivation which comes from machine learning, and there was a And there was a recent line of study for the global convergence of gradient descent flows for shallow neural networks. A shallow neural net is simply the composition of two linear maps and in the middle you put a point-wise non-linearity. So a priori, minimizing a relation function like this is non-convex with respect to theta two. Respect to C ta2. And complex relaxation was proposed by Baron, a statistician in the 90s. And the idea is, as usual, you overparametrize your optimization variable using a measure, probability measure. And by doing this, the parameterization with respect to theta becomes linear and mu. Okay? Linear mu. Okay? So super interesting because now the problem here, when you relax it to measures mu, is quadratic in mu. So it should behave nicely. But the trick is you are not minimizing a quadratic functional because in fact what people are doing is gradient flow with respect to theta2, gradient descent, okay, gradient descent. Okay, gradient descent. And gradient descent, when you read it at the level of, it's a vast harsh time gradient flow. You push μ with respect to a map, to a vector field. The second motivation comes from machine learning again and also imaging. You want to measure discrepancies between probability measures. Probability measures. And so there was this maximum mean discrepancies which were introduced in machine learning and in statistics, which simply amounts to measure mu as an element of the dual space of, let's say, space of super smooth functions like sobol or others. And one key point in this business. Point in this business is that the statistical complexity is super nice. It can be estimated in parametric rate of convergence, 1 over square root n, where n is the number of samples. So let me be clear. So if you want to estimate the distance between mu and mu, which you have only access to via independent Access to via independent samples. You can estimate the event distance between the independent samples using Dodge norm. And this gives you an estimate on the true distance between u and u in one over square root n. So it's not curved with the dimension such as optimal transport is, for instance. Okay? And And an example in imaging, or let's say bipeomorphic surface matching, or in machine normalizing flows, what you build is the following model. You are looking at, so your variable here is a time-dependent velocity field V. And this is your variable you're going to optimize upon, okay? And you start at identity, and you have a loss that you want to optimize. Loss that you want to optimize. So the loss can be relative entropy or it can be MMT. And you want to minimize the loss without any regularization, which is usually done in machine learning. So what you do is you build phi at time one, you compute phi at time one, you push forward mu zero, which is your reference density, let's say Gaussian measure, and then you want to minimize the discrepancy between your target measure and mu zero. Between your target measure and mu is your data. Then, if you do it by grade and descent, you you'd like to know if you have global convergence of the system. And there is a very simple simplification you can make for this problem. Instead of having a flow, you're just having one step of a flow. And if you do one step of a flow, you have a gradient flow on the space of measures. But the key point is in these two The point is, in these two models, I need to specify the norm with which I am going to take the gradient set. And here, what I'm going to do is simply to specify the L2 norm. It could be another norm, it could be so called F norm, but here L2 norm. And if you take L2 norm, you get the gradient flow I was describing earlier. Okay, any questions so far? Here? Yeah. So the properties of the energy is as follows. So it it's an energy which is convex in a standard sense. So you when you interfolate measures, it's super nice, it's convex, quadratic. However, in general it's rather non-convex in the Bashline geometry. So you cannot conclude to global convergence in general. And there is one nice exception. In 2D, the energy distance kernel. In 1D, the energy distance kernel. The energy distance kernel is actually convex in the Bassostein sense, so you get a global convergence. And the question I started with was how to extend this kind of result to any dimension. And the deeper question I would say is to understand this dynamic dynamical system with respect to the kernel. So you would like to classify I would like to classify uh when it comes when it when it double converges on. When it double converges on with characteristic from the camera. And just the there are a few key properties which are of interest for us. So I was super interested for the energy distance kernel, because in numerical simulation it behaves super nice. And this kernel satisfies first singularity along the diagonal. And if it was not singular, And if it was not singular, it would not globally converge in 1D. Because if the kernel is smooth on the diagonal, then Dirac masses stay direct masses along the floor. You cannot match one Dirac to a diffuse density. So I have two directions in which I can go. Whether I choose this energy distance on RD and I put this. Energy distance on R D, and I put things on that. Whether I use the Coulomb kernel because the two coincide in 1D. And I was not able to prove we were not able to prove good results on the energy distance kernel, and so we decided to study the Coulomb kernel. But let me show you some simulations. Really, uh the the in in applications the choice of the kernel matters a lot. kernel matter a lot. And if I make this gradient descent from the blue point, so the blue points are moving along this gradient flow, I choose the Gaussian kernel and you see that it matters a lot because a priori what I would like is the blue points to coincide with my other points. And it does not happen. Or at least for as long as I I was able to run the gradient set. And so if you look at the energy disconcern, it really stands out in terms of kernels. So I don't know why exactly. I don't understand why. But still, in the end, you get a good match. But just be careful. In fact, there is one key aspect of the energy distance kernel. It's a refulsive kernel. It's a refusive kernel. So if you start with a Dirac mass, the Dirac mass will be split into parts. And a priori, I have no reason to think that mass is not going to spread at infinity. Okay. So the contribution of the talk are the following. In the case of the Coulomb kernel, not the energy distance, which was my initial goal, but in terms of the Coulomb. Initial goal, but in terms of the Coulomb kernel, we are able to prove exponential convergence for smooth data and smooth target measure. And so what happens if the initial data is not smooth? And in this case, we are able to prove that the landscape is nice. What I mean by the landscape is nice is that in fact there is no local minima but the global one. Okay? I will give some details. Key works on which we based our work are this paper by Bertoz Hilo and Méger. It's more than ten years ago. And also there are many works by Cariot in this in this for this kind of systems. Um there is a nice uh there are nice papers by So there are next paper by Safati, which study the limit of putting more Dirac mass to see if it converges to, let's say, the mean field interaction system. And there is a technique we are going to use, which is a flow interchange technique introduced by Matcan, Matez, and Savare more than 10 years ago. And the topic has a recent interest in the Has a recent interest in the theme by Geraldine Schleiter, in particular by Johannes Herkrisch. And in their paper, they ask for global convergence of these schemes. So let me show you how to prove exponential convergence in this situation. I want to prove global convergence. I am on the space of probability measures. I have a non-convex. I have a non-convex functional. So I cannot rely on standard convexity stuff. So there is maybe now maybe more well known, but a few years ago it was not so well known, the polyaklo Yazemich condition, which I'm going to use. What is the Polyaklo Yazimich condition? It's probably the weakest condition you can find in order to prove exponential convergence of a system. Convergence of a system. So let's assume that you have a function from a space into R, and this function satisfies the following inequality. What is F star? F star is the minimum of F. And so if there exists such an inequality, so you bound the loss by the gradient squared of the gradient of the function, and it's a two-line proof to prove exponential convergence. To prove exponential convergence. And the key point is that you only need to measure the length of the gradient in this inequality. So you don't need to work even on the Riemannian method. But the problem now, the counterpart is that you need to come up with a proof of this polyophilia Sebit inequality. And most of the time people use actually complexity. So it's not given, it's not for free that you can get such. It's not for free that you can get such an probably what you can write, not so qualified inequality is a sort of example for Podiaclier's image. Okay, so I am not able to prove Podiaclias image inequality on the Euclidean space, but it's possible to prove it on a Riemannian manifold, which is compact. Manifold, which is compact. And using the car deduction, so let me first denote by phi mu minus mu the potential associated with mu minus mu. So I am solving this Laplace equation. So I take the inverse of the Laplacian, apply it to mu minus mu. This gives me a function on the Riemannian manifold. I take the L2 norm of the gradient, Norm of the gradient integrated with respect to the volume measure. And this gives me the energy I'm looking for. Okay? So this is the really this is a rewriting of the initial energy. And now I have to write the Versochtime gradient norm, and I simply have to replace the volume measure with mu. And you clearly see that the Polynesia-Semitic inequality is super simple to obtain. Super simple to obtain if μ is bounded below. Okay? So if the evolution stays in this nice ball of measures which are smooth enough and bounded below, then I'm in business. I have proven global convergence. So I am left with proving that the lower bound is stable along the flow. That's not clear, that's not. That's not clear, that's not given. You have to prove it. And if you write the continuity equation, assume that everything is smooth first. If you write the continuity equation, you specialize it at critical points of mu, let's say maxima and minima, and you show that they are stable. And what is nice is that this term disappears, and you are left with this equation. This equation is a simple This equation is a simple back-to-equilibrium dynamical system. And you can write it like that. So, for instance, consider measures mu and u, take the eponym, make the gradient flow with respect to the Fischer-Row metric, and you get this system. And it it's a point-wise evolution equation. Uh you can give it to uh to uh to a constructant, he should be able to prove converter. Convergence. So, what I was able to do is to reduce the question of global convergence into a regularity and long-time existence problem of the corresponding PD. So, first of all, I have to prove local welfareness of the P D. Okay? And so, this is classical. This is classical in order to prove local existence in. Local existence in Norder or superlet space, what do you do? Usually you pass to Lagrangian coordinates and you write the system into a sort of OD system. So what is this OD? So I introduce psi, which is the flow of the vector field associated with Laplacian inverse mu minus mu. I have the continuity equation here, all the same. Equation here, all the same. And then I introduce new variables, which is essentially f, which is mu composed with the flow, to obtain this formulation. And this formulation is particularly nice because the second equation is essentially algebraic, so f squared plus f times nu composed with psi. And so this is smooth with. Psi. And so this is smooth with respect to f and psi if μ is sufficiently regular. And here, for the first equation, I have an operator, differential operator, which is a change in the coordinates, which is the sort of conjugation of of this differential operator. And in fact, it's a well-known result by Evin Marzel in the seventies. Then in the 70s, that this operator, maybe it's not written in this way, but this operator is smooth in sovereign space as long as S is sufficiently high. And it has been, so there is a nice paper by Bronze Gola which extends this kind of ideas. Okay, so we get local existence for free, but in order to prove local existence, In order to prove global existence, I cannot rely on sober space because the control of the norms are not so easy. And it's much nicer to work in an older setting. And so what we are able to prove is that if you start with older regular initial measure and target measure on Euclidean space or on a closed Riemannian manifold, then we Remain and manifold, then we are able to prove global conver uh to global existence and the key points. So that maybe there are one so there are two key points. So first, the control of the norm of F in older metric. Since it's a banner algebra, we have this nice control over the F squared term, which is a non-linearity which can cause blow-up. Go up. But we know a priori the upper bound and lower bound on F because I've shown that Polyaklo-Yazewich was, the argument that I gave for Polyaklo-Yazewich applies to this term. So here, for the first part, it's exponential growth in time. And the second part, I need to bound. And this can be found in the paper by This can be found in the paper by Bertozzi, Laurent and Egel. And this is fine, I mean, this is maybe probably classical potential theory estimates, but actually kind of subtle. And using their inequalities, we are able to apply this kind of Grombal lemma and obtain global resistance. Okay, so we've solved. Okay, so I I've I we've solved the problem for uh an Oriemanian manifold which is compact. We have exponential convergence for data which are in C1 alpha essentially. I think it can be relaxed but at the moment it's kind of nice to have C1. Sorry. So you say you have exponential convergence, you mean you have global existence because those are two different I have global existence in time of the PDE, but more than that, I reduced the question of where was it? But here, I reduced the question of exponential convergence to the question of long-time existence. And why? Because if my evolution is smooth, then I have this simple argument, this simple. Then I have this simple argument, this simple one-line calculation shows that this lower bound will be stable. And the lower bound I need it in order to get this Polyaklo-Yazewich inequality. So Polyaklo-Yazevich inequality is stable along the flow. But unfortunately, I need to start with an initial mu zero, which satisfies Polyaklo-Yazevich. And you see that this Polyaklo-Yazevich in equality. That this polyakuli has available inequality, this constant, it grows up when you are interested in singular measures. So it doesn't tell you anything at theory, or at least I don't know how to do, when it comes to singular measures. Does it answer your question? Yes. Okay. And so for general measures, as I said, I have nothing to s I I I I I'm not able to to to I'm not able to reuse the result I got in the smooth setting in order to conclude something to global convergence. Because essentially, I don't have stability of the flow, like this is the case if you know about ambrosio-GP Savare theory. We don't have sort of EVI inequality because it's essentially rather non-convex. So we are able to prove that there are no local minima and so. No local minima. And so first I have to define what is the gradient flow. Because I'm not in a smooth setting. So for that, I rely on standard definition by Ambrose Orgibisavare and probably Des Giorgi before, which is a minimizing movement curve. So what you do, you're going to define the flow as a discrete process, and you're going to define this discrete process as an implicit gradient step. As an implicit gradient step with respect to the Vassarstein geometry. If you don't know about the Vassarstein geometry, just replace it with a quadratic norm. And then this time-stepping scheme depends on a parameter tau. It gives you curves. You can interpolate in time. It depends on tau. You take tau to zero and you. You take tau to zero and you say, okay, any limiting curve is a candidate for gradient flow. So existence is easy to obtain in general, but uniqueness, you have no guarantees. And so the definition we use for critical points is that the implicit gradient descent saturates at your current derivative for all tau sufficiently small. And we are able Small. And we are able to prove its classical analysis that if you start from this definition, it implies that you have this Lagrangian critical point, meaning the following. The gradient of the potential is zero on the support of mu. And in particular, if you are able to take a divergence here, it means that mu is equal to mu. But I am able to take a divergence only on the interior of the support. Okay? And this is so what it tells me. And this is uh so what it tells me is that critical points in my evolution will uh will will will probably have singular parts, or at least you know, okay, maybe mu is a singular point if it does not coincide with mu on a closed set of empty interior but which has positive measure. So, some sort of awful behavior. And the statement of the result goes as follows. Of the result goes as follows: For any point, any measure μ which is not equal to μ, I am able to find a path which is older-continuous in Vassar-Shein sense such that my energy is decreasing for short times. And the basic basic idea is super simple. The kernel is repulsive, so probably if you use a sort of diffusion equation, well-chosen diffusion equation, Well-chosen diffusion equation, then you will decrease the internal energy. Problem is the interaction energy between u and u can not decrease. Okay, here I'm going to use full interchange technique. I mean, not that much. Okay. And I'm going to conclude with this formal argument, which I find nice. So it's super formal, but we make it rigorous in the paper. So I'm going to be So, I'm going to be interested in the diffusion equation, the heat diffusion equation. I want to see the variation of my energy function along the heat equation. So, I take the derivative of E with respect to mu. It's essentially the potential against Laplacian of mu. And you see the conciliation of the Laplacian inverse and La Pacian, and this is why it's super easy to conclude. And we find actually the similar terms. You see, similar terms. Similar terms, you see, similar terms than before. And it's some sort of back-to-equilibrium equation. And key point is that the energy only depends on the difference between u and mu. So I can use Algebra decomposition of the measure in order to assume that mu and mu are singular. And formally, mu, a scalar product with mu will be. Mu, the scalar product with mu will be zero because they are similar. And you are left with this quantity, which tells you that along the heat flow, if mu plus is non-zero, and it is the case if mu is different from mu, then you should end up with decreasing your energy. Okay, and with that, I think I want to conclude. In increasing order of interest. So, this has a low interest. Global existence in sober space. I was not able to make it. Older spaces was much, much easier. Global convergence for any kind of data on a compact manifold. It seems to be true. It would be super remarkable if it's not. Because it means that, okay, there would be a convergence to some. Okay, there would be a convergence to some sort of very singular measure. The Euclidean case is completely open. Why? Because I am not able to prevent mass spreading at infinity. And my real interest is about this energy distance kernel, which in practice behaves super nicely. Yeah. And the point is that the techniques that I presented, they do not apply, at least in an easy way. Not apply, at least in an easy way. And the reason why it was easy for Coulomb flow was because of this Laplacian inverse, which, for instance, for the heat equation, cancelled the evolution. So it really relied on algebraic properties for the Council Kernel, which we don't have for the energy distance, or at least in such a simple. So any comment about the bits production? About a bit kernel. This kernel is the class of kernel which encompasses. Yeah, I mean the result it proof is for colour only for the cooling kernel. Yeah, do you do you think that is possible to generalize for it? Like for example the variable. My only interest for RIS, I think my only interest would be energy distance, which is a particular case of RISC kernel. And I don't know. I don't have the tools. I think it could be the case. Not completely clear. I think so. I don't have the tools. And I don't have the technical arguments. I don't have heuristic arguments. Heuristic arguments. Any other questions? Yeah, go ahead. So you mentioned the PL condition and convexity and so forth. Do you know of any work where degeneracy is built in? So degeneracy in the sense that what I talked about, the G times the gradient, has a non-trivial kernel. It's not your kernel, the kernel of that operator. I'm not sure. So here, here, the metric is the Vassarstein metric. So in some sense, there is no kernel in it. But in the actual kernel I'm using, so for instance, translations, translation Translations, the translations are in the in the class. And it explains why the it explains somehow white was behaving so many. Yeah, so that's a different degeneracy. This is a different degeneracy. This is not the same. I continues. Yeah, so I don't think it exists. I've not you're asking about essentially about the moving the mass of vibration, right? Or volume. Yeah, so basically your vector field has a, can't hit hip subsurfaces, basically. And so the question would be, can you get around to vaccinity or other things in order to vaccination exists on that surface? You don't know about it? So I mean I don't want to I mean it's natural to measure convergence in Wasserstein but have you ever considered trying another method in which you can measure convergence? In fact we have convergence in both energy. Okay, convergence in energy is given by Pierre and we have also convergence And we have also convergence in pass over time. But we haven't we are so probably for the smooth case, maybe you have much, much more, much more stronger convergence. Probably I don't. I was just thinking whether you cannot find cooker, probably a non-standard metric in which this interaction potential becomes convex in the end. So that for that you would get exponential conversions for free, but You would get exponential conversions for free, but you would have only an obscure metric. So on the Euclidean space, even if you assume that your measures, like your initial measure, coincides outside of a compact set with the other one, you cannot make outside of a compact set, you mean? Like you start with something where you are kind of already converged nicely. Kind of already converged nicely outside of a compact set and you have all these things going on. Can you then reduce it to the I haven't looked at this case but the problem is you need to be sure that when it where it coincides it is not going to move due to the diffusivity of the kernel, due to this fact. So so it it will not stay in a compact set and uh and even if you have equality of mu and u Even if you have equality of mu and u outside the compact set, maybe it will be deformed. It will be deformed. So the compact will not stay stable. You have to enlarge more. Sorry, I think in this example like a massive uh stay in a compact set. So can you create an explicit example stay in a compact set? Stay in a compact setup. Where is this? I have very few insights from the mathematical point of view on what happens. For a collection of points and the energy kernel, we are able to prove that the evolution is bounded in a compact set. The problem is the gradient flow, even in this case of the energy distance kernel, is diffused. It's diffusive. So, dirac masses, they do not stay in irreprocess masses. They immediately diffuse. And this was shown in the work of Garriel Steidel and Kertrich. Any other questions? If not, let's thank our speaker. And we have a coffee break until eleven until 11. Thanks.