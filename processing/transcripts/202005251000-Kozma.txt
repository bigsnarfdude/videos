He will tell us about some recent advancements in neurocritical and critical percolation. A couple of minor announcements before that. So this week we will have Gaddi's lectures Monday, Wednesday and Thursday. So next week we will have a course by Mina Gantert and the following week a course by Ivan Kovin. Both of them will have some supplementary short. Have some supplementary short lectures. So, the schedule is on the website. Now, this lecture is being recorded and also streamed live on YouTube. So, if you do not wish to be seen in the recording, then make sure to mute your video and turn off, to mute your audio, turn off your video. And otherwise, you can also always just watch live instead on YouTube. The videos will be. The videos will be posted online afterwards. So, we ask that all participants mute themselves in light of the large number of people around. So, if we do want to encourage discussion and if there are any questions, please ask them. So, you can ask questions on the chat, and one of the moderators will flag the questions for Gaddi. For Gaddi to answer. So, in terms of the format, so the lecture will be about one hour long. Were you going to have a break in the middle, Gaddi? Yeah, we can have a five minutes break at an appropriate time. Yes, sure. So, at some point in the middle, we will have a short break where we will also be able to have more time for questions. Time for questions, and after the meeting, there will also be questions, those will not be recorded. Um, so if you are interested, we will also have breakout rooms and free discussions in breakout rooms after discussions. You can stay for that if you are interested in participating afterwards. So, we also have an online forum on Zulib. So, if you go to the OOPS website, there are links there on how to join it. Join it. The forum is available also outside the time of the lecture. If you have any follow-up questions afterwards or any issues or difficulties, you can always ask there and not just talk. So with that out of the way, it's a pleasure to have with us Gaddi Kosma from the Weizmann Institute. So usually I find when I invite him to take part in some conferences. To take part in some conference and speak somewhere. He says that it's too far away, or there's some film festival that prevents travel. So now those things are all taken care of. And we have Gaddy with us. So Gaddi, please. So I'll you share your screen, let me spotlight your video and let's get started. Okay, okay, nice to be in Canada with. Okay, nice to be in Canada without having to take this awful, awful 15-hour flight. And thanks for organizing all that, Tomer and Luigi. And I guess the others are organizers, but I don't know. But anyway, thanks to all the organizers. And I'm going to give a course on critical mini course. Well, on critical decoration, it's a big topic. And just a minute, why is this? But and I don't know, but okay, so let's let's start with some basic definitions. I hope at least most of you have seen them. But let's start with that anyway. So, so we are taking some graph, but in this talk, we will talk only about lattices. About lattices. So, our graphs, even though the definitions in this page certainly can be done for any graph, our graphs will be the like you see in the picture. You take some lattice in Z D. I'm actually mostly interested in dimensions 3, but this is hard dimension 2. And you take edges, vertices are the elements of Z3, and the edge. Three and the edges are between any two nearest neighbors, guys which have L1 norm one. Okay, and now you pick some probability P and you start erasing edges randomly, each edge, randomly and independently. Each edge you keep it with probability P and delete it with probability 1 minus P, and you get some random graph like in the picture. Picture. Now, here's something which is standard, and I'm not going to prove it. It's just a simple corollary of, by the way, people who are not experts in percolation, you can look at the book of agreement for most of the proof of things that I will, for basically all the proof of things that I will skip. It's just called Percolation. It's a very readable book, and I highly recommend it. Even it might be slightly outdated, which is Slightly outdated, which is why I'm giving this mini course. None of the things I'm proving here is in the book. Certainly, not anything in the next two hours, maybe in this hour, a few things, yes. But let's see what is the result. The result is that if you ask that there is a sharp transition, what do I mean? There is some number which is known as P C or the critical P, such that if you take Such that if you take your probability of retainment to be smaller than PC, then all the components, right, the graph breaks into connected components, which are called clusters in percolation slang. They are all finite. Well, if P is bigger than P C, there is a unique infinite cluster. Cluster. The only thing here, which is a theorem, is the okay, the existence of PC is some simple corollary of the zero, one law and the monotonicity. And the uniqueness is a theorem which I will not discuss. Okay, so let's believe that this is understood. And let's go on to the next line, which is what this mini course is about, is about what happens at About what happens at PC. I told you what happens below PC. All components are finite. I told you what happens above PC. There is an infinite component. I didn't tell you what happens at PC. Is there an infinite component at PC? This is not known in dimension three, four, five, and six. It's known in dimension two. There is none. And it's known in dimension. And it's known in dimension sufficiently large, basically bigger than six, with some additional assumptions. Let me not get into that, that's not the topic of the course. But even this basic question, whether there is an infinite cluster or not, is not known in intermediate dimensions. And also, we will be extremely interested in statistics of the finite clusters which serve as a very, which have. Which have very peculiar properties in all cases that we know. And this is the topic of this mini course. There were some recent, rather modest, but still progress on the behavior at criticality. And the most and the center, I think, is this argument of Iserman, Kestin, and Newman, which is an argument from the 80s, which was originally used to put Which was originally used to prove uniqueness, but after that, simpler proof of uniqueness were found. But the argument is still important. And in recent years, especially after a paper of CERF in 2015, it will become more and more clear that it's a really important piece to understand what is happening at criticality. And I'm going to discuss this argument in details, but not in this hour, which is more preliminary. Okay? Preliminary. Okay, so I hope you get at least a feeling of what this course is going to be about. And let's go to the body of the content. And I think what is most charming, at least for me certainly, what is charming about percolation theory is that you have to tell something about what is happening at PC without. C without knowing what is the value of this number. This is just a number, but we don't know its value with the exception of dimension two, where it's not to be half. This is the result of testing from the 80s. But as I already said, I'm not interested in dimension two. I'm interested, for example, in dimension three. In dimension three, we don't know what is the value of PC. We could crank our computers to get it, get maybe five. To get it, and get maybe five digits. Okay, and that looks like just a number, you know. No, and I don't know how much you can learn on a number from the first five digits. And still, and this is, you know, think about it from a combinatorial point of view. You have some quantity from a counting point of view, you have some quantity which has an exponential and a polynomial correction, and you want to understand the polynomial corrections, polynomial correction. Corrections, polynomial correction without knowing what is the exponent. This looks almost impossible, but still, a lot of things can be learned about the behavior at PC indirectly. And this is what I want to show. And I'm going to start with a very simple example. This result is classical. It's certainly not from the 80s at least, if not before. I actually don't know what's the history of this. I actually don't know what's the history of this, but let's verify that everyone understands the notation. Okay, so let's see what is the notation here. PC, we already discussed. This was fun. This is expectation. I mean expectation at PC of something. What is something? I take zero. I will use zero for a point in the lattice, which has all coordinates zero. So this is just a notation. And C of zero. And C of zero is the cluster of zero. And these vertical lines are the number of vertices in that cluster. So I'm saying, I'm going to prove to you that the expected size of the cluster is infinity. I don't know. It could be that this is because with some positive probability, it's actually infinite. I don't know if that's the case or not. But even if it's always finite, its expectation is. Always finite, its expectation is infinite. So, under the normal conjecture that it is, in fact, always finite, it means that despite that it's finite, it's a thick tail variable. It's a variable that has no first moment. So this is already an indication of the second reason why to study percolation at criticality, because there are lots of phenomena which are quite surprising. Which are quite surprising happening at criticality. And then we will get to more of those. Okay, but let's just do the proof. Okay, so let's fix some p and this is just a notation. Again, we have exactly the same quantity, so the expected size, and let's give it the name, high, assume that it's finite, and under this assumption, pick some number, positive, certainly, smaller. Positive, certainly smaller than this. D is again the dimension, right? We are in Zd and this is a dimension, though this is not so here. It's really just the degree, half of the degree. Okay, what we will show is that at p plus epsilon, there is no infinite cluster. Okay, so let's understand why this is enough. Once we show this, the theorem will follow by a contradiction, why? Contradiction, why? Assume this is finite, add a little epsilon. Now there is no infinite cluster even above at p plus epsilon, so p plus epsilon must be smaller or equal than P C and P must be strictly smaller than P C in contradiction of our assumption that we are at P C. Okay, so I hope this is how the theorem follows from The theorem follows from this claim, which we will haven't shown yet, certainly, is clear. Okay, so let's say, let's try to, so now I'm going to prove this claim. Okay, so the claim is proved by a very, very useful trick called sprinkling. What does it mean? It means that I think about p plus epsilon percolation, right? That percolation where every edge is kept with percolation with probability, sorry. Calculation with probability, sorry, p plus epsilon. I'm thinking about it as if I'm constructing it in two steps. In the first step, I'm just taking calculation at p. Then in the second step, at each step, I'm looking at edges which are not, which have been erased and adding some of them. Now, you have to be a bit careful here with the claim with probability epsilon. It's not, you know, it depends on, you know, it's probability epsilon. depends on you know it's probability epsilon not conditioned conditioned probability epsilon over one minus b but i think you understand what what what i'm saying here let's let's see how to use it so so what i will have here are in fact three types of edges edges which is just out edges which are in and edges which are sprinkled edges are in with probability p edges are out with probability one minus p minus epsilon and edges are sprinkled with probability And edges are sprinkled with probability epsilon. Okay, so here is a bit of a mouthful. Let's see for a vertex x and a sequence of directed edges, I am denoting the following event. Now, what is the event? It's not completely written. Don't be surprised that there is no point here. Okay, so what is the event? Zero is connected to X by a path which does the following. What does it do? First of all, What does it do? First of all, there is a path of edges which are not sprinkled, which are usual, which are p-edges, let me call them, from 0 to the E1 minus. This little minus says that it's the first vertex of E1. E1 is an edge, it's a directed edge, so it has like, so I'm E1 minus will denote one side, and E1 plus will denote the other side. Now I want the edge E1 to be sprinkled, not to be P. Sprinkled not to be p and then I want to have a path gamma 2 of again p edges from the e1 plus the other vertex of e1 to e1 e2 minus the first vertex of the second edge and so on. So let's see a picture. Okay, I'm sorry, the picture didn't appear when I expected it, but here is it anyway. I have to come back later. I have to come back later. Okay, so you have a path from zero to the black here are the p-edges. A path from zero to E1, then a sprinkled edge, then a path to the beginning of E2, then a sprinkled edge, then a path from to E3, then a sprinkled edge, and so on. Okay, so, and here is the claim that I'm saying that zero is connected here is. That zero is connected. Here is the point. Let's look at this sentence, which is the last sentence that was added. Zero is connected to x in. This would be in, not ease. In p plus epsilon percolation, if and only if there are some ease could be known such that this event holds. Okay, and this is really clear. I'm not just writing it because I'm a mathematician and I like the word clearly. And I like the word clearly, it's really clear. Why? Suppose it's connected in terms of some p plus epsilon. You take some path, you can assume that the path is simple, that it doesn't use the same edge twice, and you just call E1 and E2 the edges that are linked, sprinkled along this path. This is all there is on this clearly. It's really a tautology. Okay. Okay, so what does it mean? Okay, this is just all these two lines are just what we had in the first transparency slightly compressed. And this is the conclusion that the probability at p plus epsilon, okay, now we have a new notation, but a very natural one, that this notation means that zero is connected to x. So again, Connected to X. So again, I remind you: this is percolation. So we are asking about the existence of a path of edges which have not been removed. So this is a shortcut for there exists a path of edges of both types, both regular and sprinkled, from 0 to X. And the claim that we had before was that. That we had before was that this is okay, that this is this, it was the same as the union, and then using a union bound, I'm saying that this is smaller than the sum of probabilities after you sum on all possibilities for edges. So on all n, which could be zero because there could be zero could be directed connected to x directly in p percolation. And then if n is bigger, then on Then on all possibilities for E1 and up to En. So this is a sum over n edges of the lattice Z D. Okay. And now I need to use the Vandenberg-Hesten inequality. So I think at this point I need to do a little a little bit more. A poll. How many people here know what is the Vandenberg inequality? I think you should have in the participants' screen, you should have an option to raise your hands. So raise your hands, all those who had this option, who found this option. Okay, that's not so bad. How many? Okay. How many? Okay, wow, okay. I see that even the raising of hands is a process that takes some time. Okay, good, good, good. Okay, people found out. Okay, now, please, everyone, I can, okay, I think people understood that, so that's good. I have an option to remove all hands, right? They should have that. Yes, okay, I'm removing all hands. Good. Okay, now people who want to see what is the Vandenberg. Want to see what is the Vandenberg-Kestin inequality? Please raise your hands. I want to see how many people are. Yes, okay, so that's okay. That's certainly an impressive minority. Okay, it's still an impressive minority, but okay, I got the okay. So let's understand what is the Vandenberg-Kesten inequality. It's an extremely useful inequality in all branches of calculation, not just in criticality. Recognition, not just in critically. And let's see what it does. And I'm afraid the definition is a little bit confusing, so let's concentrate this point. So we have an event. Oh, dear. So let E be an event on a plus minus one to the end. So plus minus one is just a way to denote, right? We are, I'm moving here to a finite number for this discussion, I'm moving to a finite number. For this discussion, I'm moving to a finite number of edges or finite number of events. So, this inequality is not really about calculation, it's about just independent bits. So, suppose we have n independent bits, and an event is just some subset of the cube, plus minus one to the power n. Okay, so we say that a subset of the indices is a weakness. What does it mean? This is an event. What does it mean? This is an event itself, which, okay, it's defined here, but before I do the, okay, maybe we start with the example and then see the definition properly. So let's look at the event zero connected to X. This is an event that, you know, there is a path. What is a witness? So a witness, now it's an event, it's just all the confirmations. Now, an event is just all the configuration where there is a path, but the witness is the path itself. Okay, so let's understand now the formal definition. It's the path, so it's an event. So what we want is we say that A is the weakness for omega. If any other omega, omega is an element of this. Omega is an element of this space of plus minus one n. If any other omega, which is the same for all i in A, is also in our event E. So let's see how this works for this example. In this example, you have omega is some configuration, some collection of bits for which there is a path. A is the path itself, and any other bits which are the same on the path means that this path exists. means that this path exists. So the event 0 connected to x would also hold for omega prime. Omega prime would also be in the event. So I hope this definition of a witness is clear. I know it's confusing to see it for the first time. Now let's see. Now let's move to the next paragraph. If you have two events E and F, then this E. Then this E circle F is the event that they have disjoint witnesses. So, what does it mean? You can find some A which is a weakness of E. Remember, it's a variable itself. You can find some B which is a weakness of F. Again, it's a variable and you may have more than one option. And A intersection B is empty. Okay, for example, if these are events If these are events, 0 is connected to X and Y is connected to Z, then the event E circle F would be that 0 is connected to X, Y is connected to Z, and you can choose disjoint paths. And then the inequality just says that this is smaller than the product of the probabilities. I know it's confusing to see it for the first time, but it really. Confusing to see it for the first time, but we will immediately see an application. So, I hope it will be slightly clearer after the application. And the last line is just the references. You have, it was proved in 1985 by Vandenberg Hessen, who gave it, that's why it's called the big inequality for if E and F are monotone events, if they are kind of improving whenever you turn edges on, we'll get back to increasing later. It's not important. Later, it's not important. And they already in 85 conjectured that it should be took for arbitrary and F, and this was open for quite a while and finally was proved in 1997 by David Reimer. But I don't think we will actually need it. Okay, so I hope this gave you some feeling of what is going on. Now let's look at our example. Okay, let's maybe go back to the picture. Look at this picture. The paths, the zero. The paths zero from E1, this path, this path. You can see my cursor, right? Oh no, you can see my cursor. Yes, we see your cursor. The path from 0 to E1, the path from E1 to E2, and so on, all these four paths, they are all disjoint. So they are disjoint witnesses for these events. Now let's go back, which means Which means that this event is really the event that, you know, there are path and our disjoint. Look at this word here. It's a crucial word in the argument. So the event that there exists this disjoint path from connecting our points to one another, they are actually disjoint witnesses of these events. That zero is connected, E1 is connected to E2. Connected, E1 is connected to E2, En is connected to X, and so on. Okay, and therefore we can use the Ketzen inequality to claim that this probability is smaller than the product of these probabilities. Again, let's say just for a second the statement, the probability of having these joint witnesses is smaller than the product of probabilities. Product of probabilities. Since this event means that you have disjoint witnesses for all these guys, then the probability of this is smaller than the product of the probabilities. And there is another epsilon n is just from the edges that are sprinkled. Each one has probability epsilon. So actually, using this Vandenberg-Kersten inequality, not n times, but actually 2n plus times. Also, for the probability that the edges are sprinkled, it's also. sprinkled it's also it's also they are also using different eras I hope I hope that okay I hope that it's reasonably clear at this point this is certainly the the only geometry in the proof the proof from now on will just be a straightforward calculation so it should be not so so it should be more familiar types of arguments okay so let's Okay, so let's finish the proof. Now we sum over all x. So summing over all x on this side gives us high of p plus epsilon. Remember that high is this expected size of the cluster. The expected size of the cluster is exactly the sum of the probability on all x of the probability that zero is connected to x. This is pretty straightforward. So this is all I did. This question. There's a small question on the chat, by the way. After the first inequality, I guess in the first line, whether this is P with respect to P plus epsilon or with respect to P. This is P with respect to P. I'm sorry. You see, it's written here and here, but somehow I missed it here. This is probability with respect to P well formally, you mean Geddy, formally, you mean? Yes. Gaddi, formerly you mean with respect to P, but you also have you have information of omega P and omega hypothesis. Yes, actually, yeah, you are right. Maybe it was correct for me to not write it P because this event is, let's remember what is this event. It's an event that takes, that knows the difference between the P and the epsilon. This is the event, let's see, that there exists gamma i, which are in. Which are in P percolation. This was written in the previous slide. It's not written now, but that are in P, and the EI are sprinkled. So this event is defined in the language where we are giving each edge three states, P, epsilon, and closed or missing. Okay, so, and this probability is also on this joint probability. A space where every edge has three states. Okay, this is the sprinkling trick. Okay, these are now, but after, well, technically you could say that these are also in the, oh, I see there is a penising here, but okay, you can imagine it. I'm not so good at putting my piece. Okay, so here there is really a penising. Here there is really a PMC. So these are, you can think about them also in the combined spaces, but these don't look at sprinkled edges at all. So they are only p events really. While this doesn't look at the p this last term, the epsilon to the n, doesn't look at the p edges at all. It only looks at the epsilon edges. So it's just a number, epsilon to the n. Okay, so I hope this answers the question. Okay, so let's. Okay, so let's. I'm not erasing that. It's the same inequality as above. I'm just moving it up to have a bit more space. Now, what happens when you sum over all x? x appears only in the last term. So we can take it out of the tar of the of so this sum, the sum over all x is just the sum over all x of this last term. This term is exactly high of p. And I might say that I moved also epsilon. I might say that they moved also epsilon into the other side, okay, but you can survive that. So, this is just high of p, and we can take it out of the sum, and we get sum of epsilon n, this was still here, this was here before, and a high of p. And the product now ends in the penultimate term, which is the probability that the second vertex of e n minus 1 is connected to the first vertex of e n minus. Now, let's be slightly careful. Let's be slightly careful here. Even though En plus does not appear in the term, we are still summing over it, right? Because we are summing over all edges E n. So given E n minus minus, there are 2D possibilities for E n plus. So we have to take this 2D out, and now it will be just the sum of E n minus. And then we can sum over En minus and get another Q term. And we take them both out. And we take them both out, and we get again a simpler. So we took the 2d, which is the sum of possibilities for n plus, and then another high, which explains the square here, which is a sum of probability for en minus. And we get the sum of a smaller number of terms. And we can continue this way, taking term after term outside until we get 2d to power. Until we get 2d to power n and 3 to power n plus 1 because we had the n n n sprinkled edges but only but n plus 1 sides because x there was also x. This is explained the difference between why there is an n here and an n plus 1 here. Okay, so this is just applying this argument over n times. This argument over and over n times. And this is finite by our assumption. It's an infinite sum. We have epsilon times 2d times high. Epsilon times 2d times high is smaller than half. So all these terms are bounded by... Okay, last thing on the chat if you sum over the choices of E n plus and you need to choose that's why the other terms You need to just. Yes, that's why there are other terms 2D. This was explained before. Let's repeat. Here it is. EN plus 1 has 2D possibilities. Is was that answered the question? Maybe I will look at the chat a little bit. The first question was: how does chi of p appear, which I think is related to what you just said? You sum over en and Yen, and the second question is to give to provide a reference for this proof. Okay, uh, I will don't have it on off the top of my head, but it's certainly in a Grimet's book. But let's let's explain again. So, how does high appear? Let's look at the first time it appears. That's the simplest one. Okay, it appears because we have this X here and we are. Here and we are summing over En plus one is some vertex. Okay, can you hear me? There was some notice about my internet connection. Okay, everyone can hear me? We hear you now. Yes. Okay, good. So when you sum over all X's, this term, you get high. Let's actually this was explained here. This is because this is because the expectation. This is because the expectation of the size of the set is the sum of probabilities that various elements are in the set. So if you sum over all x, you get the expected size of the cluster of n plus. But we have translation invariance. So the size of the cluster of En plus is the same as the size of the cluster of zero of any other vertex. So it's high. That's how the first high term appears. Now about the two. Now, about the two deep terms, let's explain it again. Let's look at what this sum is over. This sum is over all possible edges E n. What is an directed edges? E n. Now, a directed edge is an edge. It has, you have to pick one vertex who will be the starting, and then to choose one of its neighbors, there are 2D neighbors. Okay? Now, EM does not appear in the sum at all, it disappeared. It disappeared. And so removing n plus 2 from the sum just leaves us with 2d. Okay? And then the argument repeats. I hope this answers these two questions. So every time I remove a vertex at the end of a double Of a double connection like that, I earn a high. Every time I remove, I remove the end of a directed edge, I earn 2D. Okay, and this explain and now, certainly, if the expectation of the size of the cluster is finite, then certainly there is no infinite cluster with probability one. If an infinite cluster would appear with probability, I don't know, one of the Cluster would appear with probability, I don't know, one over 10, then the expected size would be 1 over 10 infinity, it would be infinite. So, this means that p plus epsilon must be smaller than pc. And this, as I already explained, is a contradiction. So, let's take a five minutes break and hopefully during this break, I will also give a reference for this proof. Okay, so five minutes break. Five minutes a week. Thank you. So we'll resume after a short break. If there are any questions, please do raise them in the chat. And we have a first question as soon as Gaddy comes back.  Okay, I'm back. Frankly, it's not realistic to find maybe it would be a better use of the time to if I answer questions than if I look for a reference. And I can send you the reference anyway later or the next hour. So let's see if there are any interesting. See if there are any interesting questions. So there was a question about why this is a contradiction and a short discussion. Well, it's a contradiction to the assumption that this expectation is finite. It's true that you can think about the proof noted in a contradiction. Think about the proof noted in a contradictory way, also. And this was also explained by people in the chat. What I really proved here is that the set of P for which this quantity is finite is open. This is really what the proof shows. Okay? Because what I showed, I showed if this is finite, then it's also finite for P slightly higher. For P slightly higher. And of course, this is decreasing. So if it's finite for some value, it's finite for all values below. So what this proof shows, this is a different way to understand it, is that the set of P's for which high of P is finite is an open set. But it must be on the boundary of that set. So it cannot hold for. Hold for PC. So I never used this assumption that this is finite until the very, very last end of the proof. So you don't have to think about it as a proof, but it's not like a classical proof by contradiction where you make some assumption and then use it several times and so on. Tom says that maybe this version of the argument is not in agreement, but it's certainly known. Okay, Tom certainly knows his stuff, so but if but okay, maybe it will be a problem to get. Okay, maybe it will be a problem to get okay. I don't know. Okay, any other interesting questions in the chat? Not at the moment, so perhaps we can have one more question. Does this generalize? Does this generalize to all transitive graphs? Does this proof generalize to all transitive graphs? Yes. Yes. This is completely generic algorithm. Transitivity was used. Okay, I see that there are too many experts in the audience, which is funny because they should all know this argument. Transitivity was used. Transitivity was used here at this important point. No, sorry, at this point, where we claimed that this sum does not depend on EN plus. Here, transitivity was used. But other than that, that's the only property of the graphic used. So, people who like, you know, calculation on groups can. Can take this home as a that it works on any group. Yes, and Tom is giving here an even stronger version. Yeah, so as Tom remarks, all you assume is that the susceptibility is uniformly bounded or subcritical. Yes, it proves that at criticality it's unbounded in at least. Criticality, it's unbounded in at least one, it's infinite at least one point. No, it's not bounded, it's not bounded. Yeah, perhaps we should continue then with the complaint. Okay, so I hope you got a bit of a feeling from this proof how we can learn something about PC, a population of PC, without knowing its value. That's why I Without knowing its value. That's why I started with this proof. To give you a taste of the kind of arguments, oh, good. Oh, Gabo saved me. Okay, so everyone who wanted the reference can find it in the chat. Gabo is giving a reference to Ribet's book. Okay, so, but I'm sure. I'm showing this for two reasons. First of all, it's the basis for lots of stuff that will come later, but also because I'm starting with this as a teaser of how to learn about what's happening in PC without knowing its value. You argue by some kind of contradiction, and we will see other examples, at least one other example of this behavior, of this kind of argument that show you that if Show you that if something does not hold at PC, then you change P slightly, you get something which cannot hold in this side of PC. Okay, and there is a remark here that about sharpness. Let's skip it. Okay, here's our second result, which is exactly what Hugo was referring to in the chat about some phi with a mysterious comment about phi. Let's see what is. About phi, let's see what is what is the result. So, for a set, let's S be an arbitrary finite subset of S, and let delta S be the set of vertices which have a neighbor outside of S. Basically, the boundary. Okay, this is typically used for boundary in topology, and we will use it also for a discrete boundary. This is a very, very common use, and here. And here's the second result, which says that S is that if S is any finite set that contains zero. Now let's see again. There is a slightly new notation here, and I will explain it. The sum over all x's in the boundary of the probability that zero is connected in S to X is bigger. To x is bigger than one. What does it mean connected in S? Remember that this notation means that there is a path of p-edges, of edges which are open. By the way, I never explained that, but we refer to edges which have been retained as open and to edges which have been removed as closed. Okay, so there is a path of open edges from 0 to x, which is all inside S. So, I hope. So I hope this notation is clear. And this is the statement of the theorem. And this should hold, this certainly does not hold below PC. And this, so let's move to the theorem. This I will only sketch. Anyway, I only have, I have like 12 minutes or something like that. Is this correct? I will only sketch it because it's somewhat similar to the previous argument, and also because in this case I did prepare a reference. Case I did prepare a reference in advance, but let's see how it goes. So, suppose zero is connected to x. If it's connected, then okay, maybe we'll go directly to the picture and then go back to this thing. If it's connected to x and the ellipses are our is our set, so s is the ellipse. So, you draw an ellipse around zero, you look at the draw an ellipse around zero, you look at the first time the path, you look at the path that connects zero to x, you look at the first time that the path left the ellipse or reached, to be more precise, reached the boundary of the ellipse. Then you draw an ellipse around that point. You get the first time that the path exits that ellipse and so on. So let's see what do I want. I want to find some paths gamma. find some parts gamma i again disjoint and some points yi these are exactly the points that were denoted in the picture such that here is the most important thing gamma i is from yi to yi plus one and is contained in the translation of s so s is our shape and we are moving it Moving it. This notation is just a usual notation of a set plus a point where you take all the points of the set and to add to them a yi to each point, which is basically meaning a translation of the set. So you take yi plus s and translation of our ellipse, and you want that the gamma i is completely contained in it, and that there are these terms. Okay, and again, you have the picture, so again, this is gamma. Picture. So again, this is gamma 1. It's from 0 to the boundary of the ellipse. Then this is gamma 2. And this is y 1. This is gamma 2. This is y2. And so on. Now, not only that, we must have a big number of ellipses because the ellipses are finite. Ellipses, because the ellipses are finite, it's a finite set. So if you want to go very far, then you need many ellipses. If the ellipses of size 100, then to go to distance 1 million, you need 10,000 ellipses. That's all disclaimed. So it's that the number of ellipses is bigger than this is now this is a norm of x in any norm that. Of x in any norm that you like, for example, the L1 norm, right? It's a vector in Zd. So let's say it's sum of, so L1 norm is just sum of coordinates in absolute value, with some number, which is basically one over the radius of the ellipse, but we don't care about it, just some positive number. And now we do a very similar calculation like we did before, right? The parts are disjoint. Parts are disjoint, so we can use the van den Berg-Kesten inequality. We sum from the end to the beginning, and we get at each time we get this sum. When we sum over the y's, we get exactly this guy because the paths are contained in the ellipse and we just some. And we just sum them from the outside, inside, exactly like we did in the previous proof. That's what I'm only giving a sketch. I hope people will be able to complete the argument at home. And if this value is more than one, then this probability will decay exponentially in x. Essentially, in X. Y, okay, this number is smaller than 1. You take it to power n and you sum over n, which are bigger than some number times x. Okay? So if this number was 0.99, but n would be bigger than a million, then the whole thing would be bigger than, say, 100 times 0.99 to the power of a million. It will decay exponentially. It will decay exponentially in the distance of x from our starting point, from x. And this is not possible because of the previous theorem. Because this would mean that the expected cluster size is finite. Here we do, this argument doesn't work. Here we do need that the volume of in Zd is polynomial because if the probability is the k exponential. Because if the probability is the k-exponential, but in a Bolofredius n, there are only n to the d vertices, then this means that the sum would be finite. It will be basically some like n to d minus one times e to minus some constant n, it would be finite. The expected side of the cluster would be finite, and we would and we would reach a contradiction to the previous theorem. Okay. So if that was too dense, then you can read the full argument in this very nice paper of Hugo who is in the audience. I don't know if Vansan is in the audience. Yes. I mean, you don't really need the finite susceptibility, right? Because if it was smaller than one, it would be smaller than one for. So you don't need exponential here. Yes, yes. Yes, yes, yes. So, yeah, you're right. The way I presented the argument, not, you know, I understand that this argument works for any people. Okay. So Hugo is correct, of course, that this argument can be really very slightly modified to work in any group. Now, Hugo and Vansan used it to give a new and much simpler proof of this theorem. This theorem, which is originally was proved in the 80s by more or less, I wouldn't say quite in parallel, but more or less in parallel, in the standards of the 80s in parallel by Emenshikov and by Isaeman and Barovsky. And let's see what, I hope you remember the notation. Ah, here it is, explained again. So Hippi is the class of. So what we showed before is that at Pc it's infinity. And what Mensikov is and And what Menshikov, Eisenman, and Barski, and after them Hugo and Van San prove is that if P is smaller than Pc, then it's finite. You see, one is not a corollary of the other. This is a far deeper theorem, but I don't plan to go into the details of the proof. You can read it in this paper. It's certainly better than to read it. This particular result, I certainly recommend you to read in the paper and not in the book because the book. And not in the book because the book of Dimit has the other proofs which are significantly more complicated. And I think that's more or less, I mean, you know, I still have a few minutes, but I think that's enough for today. I see that I will not be able to do anything really useful in the remaining three minutes. Usefully, the remaining three minutes. So let's call it today and continue next time, and I will take questions. So as we've been doing in the past, I will first unmute everyone so that we can thank Gaddi Cosma and then stop the recording and we will have time for questions. Okay, so we will stop the recording now and we can have questions. Okay, so Okay, so