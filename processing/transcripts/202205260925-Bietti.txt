Some cases and something about deep architecture, that is, not very deep learning, but something about architectures. Typically involving images. And so the way that uh convolutional models work is typically that we look so each layer kind of looks at a localized lab neighborhood in an image at a different scale. Neighborhood and an image at a different scale. So, in the first layer, typically you look at maybe a small patch in the image. So, this is the image of a pigeon. You just look at this area. And then, after doing these kind of convolutional operations, you might do some local pooling operations where you locally average and try to gain some invariance at the next layer. Then you redo the same thing, but at different scales. You change the resolution of your image, reapply with convolution, and other sub-sembling versions. So, the hope is that. So the hope is that through this architecture we're providing some useful inductive bias, quote unquote, so something to learn more efficiently when the data consists of images. And so pooling, you can kind of feel that you provide some translation invariance, because at the end you only have something very small. So hopefully you're invariant to larger areas of the image. But maybe the fact that you have multiple layers also helps to gain some other types of inductive biases. I feel like I haven't asked it. You're the first one who asked me this, but no, I should where is it? Fukushima Fukushima. Tell me to run just a picture. This is just a picture. I can cite somebody else, but picture anything. Okay, that's it. Which one? Which one? Fukushima? He is the relevant. That's true. I'll add that up next time. Thank you. And more recently, so this is what more modern CNNs, still not that recent, this is what they look like. So you still have kind of similar convolutional and pooling operations, but more complicated architectures. Okay, so the setup that I'll be taking is kind of a we study non-parametric regression. We study non-parametric regression and we use kernels. So think of, we'll make some assumptions about the data generating process, which is we just have labels that are some f star functions, a target function f star, tied to some input distribution x, and then add some noise. And we'll just use simple estimators, a rich regression here, so where we have some kernel k and then the corresponding RTHS, and we'll penalize the functions using the RTHSR. So here, this is kind of a simplified model. So, here this is kind of a simplified model, but if you think of grading linear models and doing optimization, you can think of this as being a similar kind of estimator. And then the questions that we'll be tackling is, so first of all, what are the right assumptions to make on this F star? So we have problems where X is very high-dimensional and has a lot of structure, such as images, audio signals. So what is the reasonable model of functions F star to consider? And the second question. Consider. And the second question that we ask is: when we have certain kernels that have structure, such as something coming from these architectures, how can a good kernel exploit good properties in X star to get a better sample of complexity? These are kind of the goals. And maybe it's also related to what Matush talked about a few days back in terms of open problems. So trying to understand really the benefits of architecture. But we don't look necessarily at deep learning, but just a simplified kernel scenario. Scenario. Okay, so specifically, we'll try to construct certain convolutional kernels that are constructed based on convolutional architectures. We'll study their RKHS and try to get some of their benefits. Benefits of the architecture. And specifically, I'll talk about two main things. So one is invariance. So you can think, if you're trying to classify that these are images of a car, then the location of the car doesn't really matter. So you want to be invariant to translation. Into translation. And hopefully, this can bring you statistical benefits. And the second part will be more about two other aspects. So, one is locality. Maybe you don't need to look at the entire image to figure out that this is a car. Maybe once you see that there are two wheels, like these kind of localized blocks, it's enough to figure out this is a car. And what I mean by long-range interactions is that maybe you have two different of these local things, but the relative position of them also matters. It's not just like, you're not just counting how many wheels to climb. Many wheels do have to take into account something more than just the local patches themselves. Okay, so why kernels? We're not trying to explain deep learning here, but I argue that kernels can still be useful. So the key thing that I'm going to try to use is the fact that they have a very nice, elegant theory that is kind of well understood. So optimization is not really a problem with kernels. It's just complex optimization. We know what the manipulation is, we can study things properly. And if we start this, we can study things properly. We can study approximation. So, a lot of kernels have universal approximation guarantees, so you can approximate any function. And for many problems, such as estimating smooth functions or certain types of structured functions, we have kind of well-understood statistical properties. So we understand if you have a function with a certain number of derivatives, this is the number of samples that you need to estimate correctly, or these are the statistical rates. You can check if it's optimal, et cetera. So it's kind of a clean setup to study. So, it's kind of a clean setup to study these things. And in deep learning, I must say that you don't always have all three. So, a lot of the theory on approximation, for instance, kind of shows that there exists a neural net that does, you know, that can approximate a certain compositional function, maybe benefits of depth. So I've got a bunch of papers from people, including people who are here. But usually this doesn't come with algorithms. So we don't know if you can actually find such architectures using data or using tractable algorithms. Tractable algorithms. And in fact, many of these actually do study benefits of depth for convolutional architecture. So, this Mashkara and Padjo paper, for instance, considers architectures where each layer kind of looks locally at certain neighborhoods, kind of like these locality patches at different layers. And they show that if the function has such a structure, then having a deep architecture can be exponentially better in terms of approximation. And two others, so also this one actually, my teacher's friend. So, also, this one actually, Matush's friend Shumir Kieper, has something kind of related where in each layer you maybe localize on certain variables. And there's also a paper by Nadaf here who studies expressivity benefits with different architectures based on tensors. But also kind of, so there maybe it's a bit closer to algorithms because you can think of tensor decomposition, but still there's no attractive algorithms that are currently known. Although perhaps his later paper here kind of tries to study also optimization properties of that. I tries to study also optimization properties of that. And then I also, so the other aspect, usually when we understand optimization well, we might not have universal approximations. Often we have something like polynomial networks, which do not have universal approximation properties. So there's kind of this oftentimes when we understand optimization aspects, we may miss out on some approximation property. That should react to this if it's not if you're not doing it right. And finally, so kernels can be a starting point to understand CNN. So it's a bit like studying the features in a model before studying sort of sparsity promoting algorithms. Maybe we're doing least squares and then somebody else can study sparsity-inducing effects like the lasso. And then the final point is that these kernels actually work pretty well in practice. So at least on CIFAR-10, So at least on CIFAR 10 you can get something like 90% accuracy if you train the exact kernel. So there's not many toy models that do this well so I think it's still interesting to study. From the paper you mentioned on kernels, they have they built the same kernel or there are different. No, so this is the convolutional kernels which use typically exponential kernel Gaussian kernels patches. Whereas these so this one is really NTK. It's from the Really NTK. It's from the Sanchez Bararas group. And they have like many layers and some complicated pre-processing. This one uses simpler pre-processing at about 10 layers. And what I'll talk about is kind of like a super simple two-layer model that does the same thing. So yeah, very different architectures. And some of them really are inspired by the NTK thing, some of them not necessarily, but they use similar kind of architectures and structures. Yeah, kernel with oh, so this this one actually was trying to approximate the kernels. So you just do some kind of random features or nice kernel approximations, and then you do like oh, these ones are just kernel regressions, like a pupil thing. Huge pupy. Oh, and then use like an S P tile that's like that's all that's what I do. Let out, but that's all. That's what I do. You have to be a little careful with the labels. You do regression and you put like, instead of one, zero, you do minus one and point nine for classes, but you do this for the place of payment. So the training part doesn't take time. What takes time is computing this huge kernel matrix. This one is like really fast. So this one you just use an approximation of the kernel and in ten minutes you have a Curiosity do you think it's a I think better solvers you mean better algorithms for the kernel? Oh, yeah, so it seems like when you do approximations, you typically get a little drop in accuracy. So it's probably fine because this is like much more convenient to do. This is more, these are like, okay, so computing this kernel matrix takes like, for me, I mean, I've been running it on like a thousand chords for like 10. On like a thousand chords for like tennis. It's not like fun to do, but you might as well just run on each of GNOME separately. But so this is a bit more proof of concept, I would say, but maybe there's some ways to do better kernel approximations. It does seem like you need to make your approximations bigger and bigger once you do it for models. I view them more as a theoretical test button. Okay, so I'll start by talking about this invariant instability, which is joint work with Luca Vinturi and Joan Gruna Medina. And this was a year of 2021. So remember again that if we're trying to recognize cars, maybe all these four images help us, you know, they're kind of the potentially we're still seeing them as an image of a car. So we would like our classifier to be invariant to this so that perhaps. Be invariant to this so that perhaps we get some more efficiency. So, if you only see one of these colours, then you can generate the other ones and try to get more information from that. So, the question is, can we get better statistical efficiency from such properties? And just to define what invariant stability means, so we'll say that a function f is invariant, and if you take a transformation of your input, so the input x, to a function f, then the function doesn't change its. The function doesn't change its outputs. So think of it as maybe we have x and then we do a translation of x to move the pixels around, maybe a rotation or something else, and we want the function to be the same. So we'll stick to permutations for this section here. And so what I mean by permutations is you have, so f is a vector, and then you take your permutation sigma, which means that when you apply the permutation at the location i, you're actually looking at a different location. Action of your orientation. So when you think of translation, this is we're just kind of permuting, shifting all the pixels around. So permutations are just in a cyclic group, but you could have like other types of groups. But we'll stick to kind of discrete groups of permutations. And so group invariance is that for any x and for any sigma in the group, then f of x is equal to f of the transformation. We'll also consider something. We'll also consider something slightly looser. So if you think of only wanting local translation invariants, so maybe you don't want to be invariant to the whole group, but only something local, then you can't enforce this, because then you can keep applying a local shift, and at some point you become invariant to the entire group. So you want to relax this somehow, and we'll call this geometric stability. And so, how do I going to study this? So, for simplicity, the kernels that we'll think about, you can think of these as two-layer networks, infinite width. So, a row here is maybe the RELU, WI is some first-layer weights, the I second layer, and we'll use this kernel scaling here. And for simplicity, let's just assume the first layer weights Wi are fixed. So, you get something like a linear model with this phi of X feature map. Feature map. And this phi of x looks like this, and it's n-dimensional. So when you take m going to infinity, you have some like basically infinite random feature model. Some early references on that. So here, random features, typically you assume w i are just random Gaussians, and the kernel is just this inner product between the features. And when m goes to infinity, you get this expectation. And this is actually in variants, as long as x and x prime are on the sphere, you get something that only depends. On the sphere, you get something that only depends on the angle between x and x prime. It's in this case, row is a relief, then it's like an R cosine kernel that comes from time, but often pops out in all these NTK time kernels. And okay, I'll also talk about NTK, but for the rest of the talk, you could just kind of forget about NTK. So, NTK would be if you train all the layers and you get a slightly more complicated kernel, but the ideas are basically similar. Okay, so how do we build in invariance now in our models? So we have, remember this feature map thing? Okay, so we have feature map phi of x, m dimensionals, as many as features. Typically, how you might encode invariance in your models is by a pooling operation. So instead of just having phi of x, you take the average over all these permutations. Over all these permutations in your group. And this is your new feature map. And then you do a linear model on top of this. And this is, you can call this a one-layer or one-layer convolutional network with pooling. So what this pooling does is basically takes all the possible transformation of your input and you kind of average over them. And this is the kind of kernel you can expect from that. So if you take this new representation with the pooling operations and you construct the corresponding kernel, then you get something. Kernel, then you get something like this. You would have two sums, but because you use a group, then you get only one sum. Essentially, you're also doing this pooling of transformations. J is a subgroup of the permutation. Yes, a priori to encode the variance. So you have to know the group. Okay, so when you're on the sphere, you still depend on these dot product kernels, but you have kind of a more involved transformation. Alright, so this is the Alright, so this is the kernel. And just for one result that you can show here, so we'll assume again regression on the sphere. So we'll assume data is uniformly distributed on the sphere. Labels are generated from some F star. Then we'll use kernel ridge regression, but we'll compare these two different kernels. So one is the invariant one, one is the non-invariant one. And this is kind of what you can show. And then later I'll compare to some prior work of Theo. So here, So here we'll assume that F is invariant, so invariant to the actual group that we care about and has a certain number of derivatives. If you look at the rates that you get here, these are kind of standard statistical rates. So when your function is s times smooth, you can expect a rate that's like 2s over 2s plus dimension. Here we're on the sphere, so dimension is d minus 1. And then the main difference here is so you have the same constant in both cases, but you have this 1 over g that comes out on the left. So if you use the right kernel, out on the lines. So if you use the right kernel, you get benefits and sample complexity. So instead of seeing, it's as if you saw like g times n samples instead of n samples. You get a benefit by a factor g. The problem is that you have this O of 1, so it's asymptotic. So is this, I mean another way to think about this in hands is I can just take do data augmentation, take my training set and and then I get g times n examples, although they're not IID. They're not AID, right? So it's actually the same estimator as as uh So, it's actually the same estimator as Theo's paper shows. So, if you do a global invariance and you do that, or the data annotation with for each sample you take exactly all the elements in the group, you get the same estimator. Same estimator as this? Yeah, exactly. So, this is, so this kernel is equivalent to doing... Is that what I'm saying correctly? Yeah, so you have a stellar kernel matrix. This kernel is equivalent to data mutation. Yeah, so you have an n by n kernel matrix instead of, in the data mutation case, you would have g times n, which is fine. In the data mutation case, you would have g times n, which is much bigger, because you pay g times. It's not good. But it's the same. Well, okay, but then you have to calculate this. You're just chubbing it into calculating it, right? Because you have to. Well, here you only pay G. Like basically you have to have G squared. Yeah. I think that's the only method. So okay, we have to do that. That also uh because uh yeah it's uh sometimes uh easier to compute a kernel, like you can buy a kernel. Kernel, I can buy a kernel. Yeah, I can do that. No, as you said, also just to know it's a kernel for analysis of other services. But yeah, so one issue is actually computationally this is pretty annoying. I mean, if G is something larger than the translations, then maybe you have to compute this. Okay, but so this is kind of the result you can show. Okay, so asymptotically, so this little O is just in N, so dimension is fixed. Below is just in n, so the dimension is fixed here. This tends to zero. So asymptotically, you can get a gain by a factor g. And if you consider even the set of all permutations, then g could be exponential indeed. So at least asymptotically, when n is very large, you can get gains by a factor exponential in the dimension. And then, so, okay, another result we show is that at least this constant C D is optimal up to constant factors, at least in its dimension, depending. Factors, at least in its dimension dependence, so this is like minimax optimal. So this is not just a fake gain, it's actually a real constant deviation. But when I say minimax, so this one is the bound that you can expect over invariant functions under the smoothness assumptions that you make, and this one is without the smoothness assumptions. One actually like instant-dependent load bound. Okay, so the way this is shown is usually these dot product kernels, you can decompose them and kernels you can decompose them and so CT is not the norm of F star in the space of spin space yeah so it depends oh so it can be independent of the time yeah so yeah yeah but I if you ignore because the norm will depend on dimension so depending on dimension sure but you also have like basically But you also have like basically these are based the C D basically depends on these like sourcing capacity constants. So it could be independent dimension. Sure, but if you look at the like smoothness setting, then typically you have some dependence on dimension. Like the eigenvalues of your kernel on the sphere, then you have some linear credit. Okay, so this is so these kernels basically you you can typically represent them in the spherical harmonic basis. them in the spherical harmonic basis. So we represent functions on the sphere in terms of their decomposition in this basis. So think of this as kind of the Fourier basis, but in higher dimensions. So when the dimension is two, this would be basically sine and cosines at each frequency. And when the dimension is larger, you get more of them. So the number of harmonics that you have at each frequency kind of increases. So each row here is in a fixed frequency. And you have a certain number n dk of them. And then when you do pooling, basically you're projecting each row. Basically, you're projecting each row down to a smaller subset of invariant harmonics, which also will form a basis of the invariant functions. And so the key thing is just kind of studying the ratio. And here we show that sort of when k goes to infinity, the degree goes to infinity, this tends to 1 over g. Then we can control the rates in this little of 1, depending on some properties of the permutations. Okay, so the way this works is... The way this works is, okay. We use a formula in this paper by Theo and colleagues at Stanford, which kind of characterizes how you can write this ratio in terms of something depending on some Legend polynomials. Then it turns out that you have a sum here of all the elements of the group. For the identity, you have exactly 1 over G, and all the other ones you can show that they decay somehow. And so eventually, when the degree is very large, the other ones vanish. And this all depends on kind of studying the density of. On kind of studying the density of some random bear. Okay, so just to compare to the paper here, so they actually focus on a high-dimensional regime where they study this, how this, like for a fixed degree k, how it develops sort of d going to infinity. And for specific groups, you can kind of show certain dependencies here that are polynomial in the dimension. And if you get something like this with the alpha power, then this is kind of the what you can hope to gain in terms of some complexity. What you can hope to gain in terms of some complexity. And so, in our case, basically, the group, so this one doesn't have to be the size of the group, basically, in grouping. In our case, basically, for any group, whatever group you have, you can expect this to hold. So, in this case, potentially you can have for any group. It can be anything. So, I guess you have to study the formula explicitly for each group. But at least, so one thing is if you have an exponentially large group, our thing can tell you that at the end, you get. That at the end you could get this, but presumably, if you're in a polynomial regime, maybe you probably cannot get any exponential gains. I mean, basically, if, yeah, no, like you can't get exponential gains. Right, so it's, yeah. So our thing is a bit kind of dreamy to say that you can get exponential gains, because maybe it'll take like exponential samples. At least asymptotically, this is kind of the scaling that you can go to. Okay, so another thing we do is we extend this to when G is not a group, so maybe those low. Extend this to when G is not a group, so maybe those local translations or something like information. And in this case, pooling, so remember I said pooling kind of projects down these spherical harmonics to a smaller dimension. When pooling, when G is not a group, it's no longer a projection, but you get some kind of low-pass filtering type operation. So you can still define some smoothness assumption that is not invariant, but something like a source condition that encodes this smoothness. Smoothness, and you can still get bounds which have this effective sample size, n times the size of the set of permutations gene. Show this for like a simple deformation model, which you know, deformations, it's kind of a useful inductive bias or prior. It's not really meaningful in terms of permutations, but you can kind of juggle and have a toy model for that. But yeah, so the main issue with this whole section here is that you have these cursed rates. You have these cursed rates. So if the function f star is non-smooth, then you pay, you have an exponent here that's still cursed by dimension. So it's not because your function is invariant to some large group that you can gain in terms of the rates. So you need something else if you really want better statistical efficiency, especially if your images or whatever data is very high-dimensional. Okay, so this is where we don't know Francis's work. Maybe what counts is maybe you only depend on certain directions of. Maybe you only depend on certain directions of the data, you don't depend on the whole space. And the way we'll do this next is kind of by having more structured architectures. Any questions before I move to the next section? Okay, so the picture, so first I'll talk about locality, which means depending on these small patches, instead of maybe, you know, you don't have to look at the entire image, it's enough to look at localized neighborhoods. So locality is maybe if you can identify. Is maybe if you can identify that this image has two wheels, then maybe you can tell that this is a car. So, how do we study the benefits of a car? And here, so instead of having these global convolutional nets, I'm going to look at convolutional architectures, but on patches. So, x here is our signal, input of dimension d, or the domain here is omega. I'll call it omega. So the number of pixels is like cardinality of omega. X of u is just. x of u is just a pixel at location u. And then these x u will be patches, so kind of a small vector centered at position u. Dimension p. So what an actual convolutional kernel when you have these small filters does is it actually look at these random features but only on a given patch. So you take these, let's say, random convolutional filters, infinite number of them, and you get this feature map. And when you have this, And when you have this, the convolutional network then takes a sum of these small neural nets at each position, with different parameters v at each position u. Take each patch, you map it to some space, and you take a linear model out. This is a convolutional, still one layer convolutional network, but with small filters. And the corresponding kernel here is, so we'll still define this kernel on patches, which is similar to what we studied before. Which is similar to what we studied before, but now you actually take the sum of this over each position that you have in here. So these X and X prime are two images, and we're comparing them patch by patch. So there's no kind of, you're never looking at two different patches in two different images. What you can do then is kind of add a pooling filter. So this will kind of make some information and provide some invariance. And so here, instead of looking at a fixed, instead of looking at a fixed u, you'll VU, instead of looking at a fixed U, you look at a local average nearby U with filter H. So H is just kind of a pooling filter, and you're taking the local averages of the features, and you get this new convolutional network with pooling, and this is the kernel that you get when you apply that. Formulas. When you do global pooling, so just the filter is a constant throughout, so you're doing global averaging everywhere. Throughout, so you're doing global averaging everywhere, you get this kind of network and this kind of kernel. And in the extreme case where the filter is just a direct delta, then this is the kernel that you get back to the first kernel. So once again, you can kind of study these two different kernels. One is the global pooling one, one is the one without pooling. And for this slide, we'll assume that the target is kind of this additive function, whereas the same function is applied on each patch. So you have some kind of Patch. So you have some kind of invariance to all the patches. So this is even, so it's translation invariance, but it's even beyond that. Maybe if you shuffle around all the pixels, it doesn't matter. All the patches, it doesn't matter, so you just copy them. Translation invariant, if you want. Okay, so what you get here, you assume that G star is smooth, again, certain number of S derivatives. We will assume actually a data distribution that each patch is uniform on the sphere. Patch is uniform on the sphere and non-overlapping, just for simplicity. And then what you see here is, okay, so first. Alberto? Yeah, no, like it's oh, here then none of the. Oh, the patch is overlapping or not? So in the theory, no. But then the kernel you can define it regardless. But then the invariant can invariant. So yeah, what it was meaning here, so if these are if they're not overlapping, I'll just stand invariant. But if they're overlapping? If they're overlapping, you only have translational invariance, potentially. Invariants is potentially, but it's but then it's this. I mean, if we just want translation invariant, it's easier. I mean, there's more. I could get it just with a rotation group, the translation group. So it gives the same thing? Yeah, this is the same as what I said before, I guess. It's kind of like you take your kernel and then you take an average over all the transactions. Maybe I'll sorry, I'll just. I mean, okay, so it's not the I mean the okay, so it's not the same construction. As before, no, because we have these localized things. Like in particular, this depends on the size of the patch, right? The kernel, yes. Right. But it but if the invariant, no matter what the size of the patch is, the invariance is just a rotation invariance or a translation invariance. Yeah. Can you get more when you have overlap? You might have something a little richer than translation invariance. You only have half. I mean, it's not the same kernel, right? I mean, it's not the same kernel, right? That I get if you just apply the translation invariance construction before. But the pooling operation is basically the same. So it's just that now we have these. This is basically the kernel that you get. So you're kind of averaging, you are taking this thing, average. So in the end, you have something that looks pretty similar to what I constructed before. It's just that the kernel here is actually localized. If you view this kernel as something that depends on the full image, then this thing is actually. Then this thing is actually the same kernel applied to a shifted image. Yeah. Maybe we'll go back to this. I'm sorry, I don't want to interrupt you. Yeah, but typically you would think of this as just translation. Yeah, no sorry. It's like if you take a full batch I mean full uh batch size. It's exactly the same. But if you take uh part of the so so here like you you you won't have translational. You won't have translational lines. Won't have? Why not? When you take global pixels? I mean, you'll have. I mean, you can't shift by one pixel, you have to shift by a multiple of. Oh, because they're not. Because they're not overlapping. No, no, no. If it's not overlapping, I think the picture is very true. What I'm confused about is what happens in the past. Yeah, that's a good point. If you have non-overlapping patches, then it's clear. Overlapping is what I think is. Yeah, like there's one example where you take. Yeah, like there's one example where you take uh pixels on the hypercube where you can actually diagonalize the kind of yeah what you get is uh yeah. Yeah, so when you actually have overlapping patches. So the problem here is I was studying this for like the sphere, which is hard. If you have overlaps it's hard to figure out what the distribution is, but in high dimension you can kind of or in general if you have like either the torus or the hyperfuel. So the torus and kernels that are translation invariant then you can diagnose. Kernels that are transition invariant, then you can diagonalize things even with overlap. So, here, anyway, so I'll talk about the one with no overlap. So, the patches are uniformly distributed. Each patch is uniformly distributed. So, you have basically a product of sphere data distribution. And these are kind of the rates that you can expect. So, the main thing is that the rate here is smaller, just the dimension of the patch instead of the full dimension. And then you still get this benefit from pooling, which is like without if you had the Without if you have the s the no pooling kernel you pay like omega, the number of patches. And if you have the fully invariant kernel, you pay one. So it's still the same kind of benefit. Another thing I want to say is if you have filters that are localized, maybe like a small Gaussian pooling filter, then as long as you don't have this global pooling filter, then you can approximate functions that even have different g's in different positions. You don't have to, each position can have a different You don't have to, each position can have a different function. And what pooling does, if you actually study the RPHS norm, so the implicit IS representation norm or whatever, then you can see that the pooling will actually penalize the frequencies of your function that you're estimating. And if your function is globally invariant, then basically when you have a localized pooling, you'll interpolate between one and omega via this like L2 norm of the, okay, assuming the filter is L1 normalized, then you pay the. L1 normalized, then you pay the L2 norm, which for a direct is one, and for the other case, one of, I guess, one over omega. So it's more like one over h kind of, you put one over h squared here instead of one. Danny mentioned, yeah, so if you want to diagonalize things with overlap, you can do things either on the hypercube or on the okay, so this model is kind of This model is kind of simplified. So we said you can look individually at each patch and apply some function. So maybe you find a function that detects wheels and you apply it all over the image. That's great. So you see this wheel, you see another one, you just count how many wheels there are. If there are two, there's probably a car. But if there are two wheels that are completely in different positions, maybe it's not a car, and yet you're thinking it's a car. So it should matter also where the relative position between these two different batches. So we care about long-range interactions between these different local parts. How can we do this with kernels? So add more layers. The kernel version of add more layers is what's called hierarchical kernels. So if you have, or the simplest case, so think of this as these kernel feature maps. And let's think of kernel feature maps of dot product kernels so that the inner product here, phi2, is just the inner product of the inputs. So this is a Inner product of the inputs. So this is the first dot product kernel on phi2, and then this is exactly the dot product kernel on phi1, kappa1. So you can kind of take compositions of these kernel functions, and that corresponds to kind of compositions of feature routes. And this is how you usually define multi-layer kernels, and also when you study NTKs or whatever, this is kind of what pops out. You get these infinite widths, different layers, and you end up with composition of kernels. So the way we can so. So the way we can so, okay, one thing, if you only study kernels like this, you know, it's it's not very rich. You basically just have a different function here, but it's still a dot rather kernel. So you get very similar spaces. But when you have these more rich structures, then potentially you can capture something richer. And in particular, so we'll study, we'll start with the kernel that I described before, just one layer on patches, and then the first layer of pooling. But then we add a second non-linear layer and a second Layer and a second pooling layer, H2. And so the idea here is that if you have something non-linear, like the second layer, when you grab patches of the second layer, then this potentially can look at interactions between these two pixels, which then can propagate down and take two different patches in the initial image at different positions. So you get these long-range interactions. And what you can do is you can study the simplest possible kernel here at the second layer, just a quadratic kernel. Here is the second layer, just a quadratic kernel. And this is what would come out. If you have like a second layer which has infinite width quadratic activations, you would basically end up with this kind of kernel. If you do have this and then you do this hierarchical kernel thing, then the function space actually will contain these interaction terms. So these g uv are basically functions that depend on two different positions u and v and are applied to two different patches. Two different patches. So there's two different things to look at. First is the distance between the patches will be some kind of receptive field that you're allowing, which depends both on how wide the pooling is and how large the patch size is at the second layer. The second thing is each of these terms here has to belong to some specific space, which is actually a tensor product space between the first layer. So this first feature map of the first layer is like some kernel, maybe like a. Some kernel, maybe like a R cosine kernel, which defines a kind of sobo-like space. And so, what you learn here, these terms in the RKHS, when you have this two-layer kernel, are actually spensive product spaces, which, so this is basically the kernel that you would get with a product of two kernels. Think of it as something like this: just the product of two kernels. So, if each one of these is like a Gaussian kernel, then the product of two Gaussian kernels is basically a higher-dimensional Gaussian kernel. If you have, if this K is like a subolev kernel, like the arcosine kernel, which turns out to be something like that, then these are, this is something called like mixed smoothness space, if anyone's heard of that. Basically, it's a smaller space than a full circle. Smaller space than a full symbol of space in higher dimensions. So it kind of lies with many more derivatives, but it's studied as well. Okay, so this is kind of the function that you can hope to expect when you have these two layers. And another thing that you can study, and I do in the paper, is basically you can study the RKHS norm, write down basically this representation cost. And what you see is that the norm, when you have these pooling filters, so the different layers actually penalize different things. So the first layer penalizes. The first layer kind of encourages smoothness of these terms. So when I say smoothness here, I mean that these functions are enforced or encouraged to be similar. So when u and v change slightly relative to each other, then these terms will be encouraged to be similar. The extreme case, if you have this global pooling filter, then basically only depend on a fixed position and not on the relative position. The first layer controls kind of relative smoothness, whereas the second term, the second term. Smoothness, whereas the second term, the second layer encourages kind of, if you fix the two positions, kind of a global shift. So you have these two different scales. You're controlling things at two different scales. Okay, so this is how you can simplify model with a two-layer kernel. And now let's see if it actually makes sense to study these simplified models where you just have polynomial kernels at the second layer. So here you can try this out on CFAR-10, kernel rich regression. Kernel-rich regression, very large matrices, and what we see is that if you have this two-layer kernel, so with certain architectures, so exponential here is kind of a Gaussian kernel on the sphere. So if you have exponential exponential, you get 88.3 accuracy. If you have just polynomials, you actually drop, have a very little drop. Basically, no drop when you have at least order 3 or order 4, and a small 1% drop when you have order 2. So order 2 basically captures these character. Group two basically captures these pairs, almost done. This pairwise interaction, almost three, four captures higher order. So you can look at three or four patches. So this kind of, and okay, if you have linear kernel at the second layer, this is basically a shallow architecture, so just one layer, and the drop is huge. So you really need depth here. But it does seem like, sorry, I also want to say that this performance 88.3 is actually state of the art for kernel. So there's this other paper by Benrecht's group. Paper by Van Recht's group here. They have a 10-layer called Merchl kernel, and it does 88.28%. You add data augmentation, you get horizontal flips, you get something like 90%, which is comparable to AlexLant, I think. And yeah, so basically, you know, the simplified model, just two layers, turns out to be enough to get good accuracy on C410, which suggests that maybe it's capturing useful stuff in that data set. Interesting kind of functional space to capture these things. Similarly, you can study kind of generalization benefits. Here we consider again just an invariant function, which looks at all pairs of patches and has the same function g star. You make some assumption on the distribution that things are decorrelated when you're looking at different patches. And then we'll look at different architectures. And here, what you see is that when you have global pooling at both layers, so the most invariant architecture. Layers, so the most invariant architecture, then you get a rate. So these are just slow rates, so I'm not looking at decays, kind of just a simple bound, which is basically a trace of the covariance divided by root n times the RHS norm of F star. So you can compute both of these quantities for all these architectures. And for the most invariant one, you get something like this. For the least invariant, you get this big polynomial dependency on the number of patches. Basically, using the right architecture can give. Using the right architecture can give good gains. If you had even higher-order interactions, these polynomials would get even larger. So the sample complexity benefits would be even larger. And one thing to note here is that if you have non-overlapping patches, you can kind of satisfy all these assumptions. And the architecture here actually looks like what's called deep sense. So I'll finish here. So we've seen simple invariant architectures where you can get benefits by the Invariant architectures where you can get benefits by the size of the group. And then we've seen when you have more structured kernels of two layers that work well in practice, then you can capture these kind of rich functions that are interaction models with certain orders, and you have a bunch of invariances amongst the terms. I've been useful on this journey. And what's missing? So sparsity adaptivity. So the hope is that also these functional spaces can tell you a bit what's needed. Spaces can tell you a bit what's needed to do better, so to have a little more sparsity, more adaptivity. So maybe at the first layer, you really want to get these like Gabor filters, something like that. But at the second and the next layers, maybe you have all these interaction terms and you want to figure out which one's matter. So if you're studying teacher learning. And then, of course, something beyond, I mean, studying transformers is also an interesting direction. It does seem like on vision, transformers, the first layer still kind of does something similar to here. Kind of does something similar to here, but the way they look at interactions is a little more.