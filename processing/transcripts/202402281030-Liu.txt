We have CPU from Dymex and IAS from where C. Okay, cool. I guess people are still totally. Yeah, so I will wait. But it's okay. I was trying to find this. Or me. Or you can. Thank you very much. Okay, how about now? Okay, yeah, okay, let's get started. Okay, cool. So, hi, everyone. Today, I'm going to talk about the detection and recovery problem of random graphs with latent geometry. So, I should say that this is not about a particular work, it's more like a survey, and many of them are not my works, and some of the people who did it are actually in the audience, so probably I feel some of the audience have like better comments than I could have. Have better comments than I could have. So I feel like I would welcome any comments and discussions. It was such a self-deprecating intro. I don't know. I just want to set the expectation. But anyhow, we'll see. She's done some awesome work in the last several years as one of the world experts on this model. Yeah, thanks guys. Thank you. Okay, I will start with the definition of the random geometric graphs. The definition of the random geometric graphs. So, this will be actually the object we'll be talking about in this talk. So, a little bit on the history. So, as far as I know, this model is first studied in 1961 by Gilbert. So, he was studying this because of interest in radio networks. And the model he was considering was actually sampling random points from a Poisson point process on just a Euclidean plane, and then, of course, connect to. And then, of course, connect to vertices if the corresponding layton feature vertex are close enough in Euclidean. Okay, so after you do this, you just look at the graph itself and forget about all the coordinate information and then study properties of such a graph. Okay, so this model later generalized to more like general metric space. So, what you will do instead in this more general setting is that these like geometric, like these like location vectors. These location vectors will be sampled actually from the measure on this metric space instead of on just the Euclidean plane. And you would connect it, again, you would connect to if they're close enough in whatever metric underlying this space. Okay. So this problem has been quite well studied, or at least people have been studying this for a long time, at least in low dimensions. So many of the applications like wireless networks. Many of the applications, like wireless networks or like protein interaction, and even like things like citation networks, something like that, they all have the property of like it makes sense more in low dimension. So people have studied many of these models in low dimension. Particular spaces that have been studied include like the Euclidean cube, 0, 1 to the D, and also like the unit sephere in low dimension, and the torus in low dimension, and some compact Li groups. And as I was mentioning earlier, this has been applied to network analysis, protein interaction, motion planning in robotics, and also topological data analysis. Even in math community, I think people do study some relevant model under the name of a system module. So that's something. Okay, so this is the roadmap of the rest of the talk. So I will start with talking about some recent development in detecting geometry of like. Detecting geometry of like this random geometric graph model, but specifically in the space of unit sphere. Okay, and later I will talk about some like the recent works on recovering the geometry in different variants of this random geometry model. So, okay, I should say that the first one, the first question was in high dimension, because low dimension detection is pretty simple. As we all know, like cleaner graph looks pretty different from just the Graph looks pretty different from just a random graph we will see. So, the first one is actually a high-dimension. The second one is a bit mixed. There are some results in high-dimensional, some results in low-dimension. And they'll end with more like open directions related to random geometry graph model. Okay, so let's get started by introducing again like this model, but specialized on the units of here. So, the distribution we're sampling these layers. We're sampling these latent location vectors from is the uniform distribution over the high-dimensional sphere. So we're sampling V1 through Vn on the Sophia, and then we already could predict, we connect to if actually the inner product is large enough. Inner product being large translates here to two points being close on the sphere. And maybe there's one geometric intuition I want to say is that essentially two edges, two edges. Two edges, two points are connected if one falls under a small sephere cap around the other. Because that's essentially saying their inner product is small. And we choose this threshold so that basically the marginal probability of two uniformly random points on the sphere having an edge connecting them is exactly. Just as this was illustrating. Maybe at this point I want to make two comments about this model. So we picked this ratio tau as we see like a As we see, like by enforcing, like basically, by it's a okay, it's a function of p and d. It's picked so that basically the edge probability is p. And without telling you actually what this tau explicitly is, I will just make the comment that when you fix p and take the limit of d going to infinity, this correlation, this tau threshold actually approaches zero. So, in some sense, what that is saying is that. sense what that is saying is that if we're if we're considering geometric geometric graph drawing from higher and higher dimensional sphere what would happen is that even though like we know like some like geometric information like two points are connected on a sphere but this doesn't give us much more like a correlation between the two location vectors vx and vy so in some sense this is saying like us dimension Like as dimension goes to infinity, what would happen is that this graph wouldn't really look any different from a graph where every edge is drawn independently because this underlying geometry is lost. Okay, so that's intuition I want to talk about here. And another thing, maybe another definition I want to give here is the Grand Matrix. The Grand Matrix is just the inner product matrix. Is just the inner product matrix of all these location vectors for the vertices we're having. And we can just simply view the adjacency matrix that we obtain from this graph model as applying a threshold function based on this, applying the structural function to this inner product matrix. Is the model clear? Now, oh yeah, okay. Now, uh, oh yeah, okay. Just bother it, I would just use this notation, uh gnpd to denote this random geometric model on a d-dimensional. It has one more coefficient, sorry, one more parameter than like our shredding. So I guess the question, the number one question for people in this community, like doing statistical inference, would be like, okay, when is this graph an interesting model? In a sense, like, when does, when does the random graph drawn? When does the random graph draw from this model actually retain some of the underlying geometry? So, this is this really cool animation done by my collaborator Sedance. So, when you actually secretly throw away only left with the graph structure, can you still tell it's coming from a random geometric model or not? So, to formalize it a bit better, like it's essentially asking: can we distinguish? It's essentially asking: can we distinguish a random geometric graph from a random erdotranic graph? As I was trying to hint at it before, this question basically depends on the parameters M, P, and D. And the D will play an important role in this problem. So let's trace the history. So, the first attempt in this question is Attempt in this question is by Bubekting, Aldern Rutz in 2016, where they actually show an efficient algorithm for the specific task I was talking about, which is when D is small enough, specifically smaller than this binary entropy P to the cube times N to the cube, then there's efficient algorithm. And this algorithm is actually very nice and intuitive. The algorithm is basically trying to compute this polynomial. This polynomial is just a you first decide. This polynomial is just that you first center the adjacency matrix you get, and then count the number of sine triangles in the centered matrix. So the intuition here is that because of like the underlying geometry, we know that if a vertex have two neighbors, then it's higher probability that its two neighbors are connected than like, let's say, in the case of two neighbors of a vertex in the Hardushani model. So it kind of makes sense that this could. It kind of makes sense that this could potentially distinguish. And I want to make a comment that actually there's this recent work by Euro and the guy, which actually pinpoints the low degree hardness for this distinguishing problem, in which they actually essentially show that the sine triangle count is optimal among the low degree tests, low degree functions. Okay, so the negative result. The negative result that's shown by them is that when d is much bigger than n cubed, so we're going to a regime, like a very high-dimensional regime, then essentially the two distributions are indistinguishable information theoretically. So maybe I don't want to talk too much about how the proof goes, but I think that there's one comment I can make, which is they actually show this indistinguishability of graphs by showing indistinguishability. Of graphs by showing indistinguishability of this inner product matrix. Okay, so you might ask, okay, Erdogani graph doesn't have an inner product matrix. Well, but you can imagine like Erdős Reni graph being like applying this rational function to a so-called inner product matrix which it's just symmetric and have Gaussian entries. Just Gaussian. Yeah, just Gaussian entries. So they're not truly inner product, but they're kind of fake inner product. Yeah, so. Yeah, so right, so essentially what they do is they're showing that this inner product matrix is exactly like a, is indistinguishable from this centered, like, okay, recentered and re-normalized Gaussian entry matrix. And therefore, then, when you apply the structural function and get the adjacency matrix, they're also indistinct. So that's the approach. So not to be able to kind of confuse. We will transpose this PSD while a random Gaussian matrix. So you need to re-center, I think. Yeah. You need to. Diagonals are like self-loops anyway, so you just like add a big identity. Yeah. Yeah, yeah, that's right. And also re-normalize it. Like delete the diagonal because you're not going to see it in the graph. So, okay, so we can see that this could potentially be wasteful because, really, like, we don't really see any of the Like, we don't really see any of the inner product matrix information, so there's hope that this could be improved further. The further improvements are due to Brandon Bresler Magaraj in 2020, where they actually improved this indistinguishability side of lower bound to this min between entropy, binary entropy of P and cube and this HP cube into the seven halves. So, okay, to make this a bit more like straightforward. Bit more like straightforward. Maybe I would just directly compute what this implies in different regimes of sparsity of graphs. So in the dense regime, nothing changed. In the dense regime, actually, it's tied. Maybe I should have emphasized in the previous slides that in the dense regime, we don't have a gap. On the one hand, there is the algorithm that works in cube when our edge probability is constant. On the other hand, there's indices. Constant. On the other hand, there's indistinguishability resulting. Okay, so in the relatively dense regime where you have average degree polynomial, small polynomial, then this would actually get you a bound of n to the three halves plus some little two epsilon and polylog n. So this improves upon the previous result by n to the three halves, essentially. And then in the relatively sparse regime where we have like an Sparse regime where we have like a most polylog number of average degree, then this further improves it to exactly like three halves. So I want to maybe for those of you who are interested or curious about the intuition behind the improvement, I will say a couple sentences about this proof. So essentially, what they do is they're able to reduce showing this T V distance vanishing. This like T V distance vanishes to showing that this specific conditional distribution actually converges to this Bernoulli random variable very fast. So what is this conditional distribution? It's like given this graph, but we mask an edge 1, 2. Let's say we pick the edge 1, 2, and we mask the edge 1, 2. And now we're given a, now we're curious about the probability. Now we're curious about the probability of this edge occurring in this matrix, condition that I know the structure of every other edges in the graph already. So it's just a marginal distribution of this specific edge one, too. So as we see, if it's in the Erdos-Randy graph, this distribution is clearly just Bernoulli P. But in the random geometric graph, it's more complicated because we already kind. We already kind of derived information about the vertex 1 and vertex 2 from the rest of the graph structures. So they're doing a carefully coupling analysis to actually show that this converges to the Bernoulli distribution fast enough so that actually when D is beyond this structural, then they can show a T B distance magnitude design. So And then in a joint work with Sedant Mohanke, Selio here, and also Elizabeth Yang, we were able to further improve this indistinguishability lower bound to HP squared and cube. So what does that really mean? So it does instill the dense cases type, so nothing changed, but improved the relatively sparse and the relatively dense regime by a factor of. Regime by a factor of spirit n. I don't want to talk about the exact approval of this, but there is one comment I want to make, which is we're able to make this kind of improvement because of some correlation decay phenomenon in the graph. And I will try to explain where that correlation decay comes in in the proof. And for the rest of the proof, I will just not really mention. I would just not really mention much about it. Okay, so similar to the previous work by Brendan Bressler and Nebaraj, we also do this reduction of reducing showing this TV distance vanishing in the two-graph model into showing a very local distribution vanishes in TV distance. Okay, so what is this local distribution? This local distribution is the following. Is the following. Given that I know, like, given the subgraph, so okay, maybe I should imagine like sampling this random geometric graph in a very sequential way. I sample first a random geometric graph on like n minus 1 vertices. So I know fully the graph on this n minus 1 vertex. And then to actually get a graph on n vertices, the only thing I need to know essentially is what is the neighbor of the n's vertex I'm going to sample. So essentially, the The distribution that we're curious about is actually this conditional distribution, which is a condition on. I know the structure of the n minus 1 vertex, a graph on the n minus 1 vertex, what is the neighbor of my n's vertex? So this is the distribution I'm curious about. So in Erdograni graph, we kind of clearly know what is this distribution. It's just a binomial distribution. The independency of edges. Edges. But in random geometric math, again, this requires us actually to work harder. So here the conjecture is that entropy of P cubed, m cubed is the right bound? Yes. Okay. So is the correlation decay property, does it fail beneath h of p squared? Or is it believed to go all the way down? Okay, that's a good question. I think in the model we consider. I thought that, like, I'm a little bit confused actually, because to me, like, I thought that the correlation decay was only important when we were getting the HP cubed and cubed for P really small, right? Yeah, that's right. Yeah, I was going to talk about that, which is that I'm kind of just giving a forecast here, which is not entirely accurate. Yeah. But about the correlation decay, whether it vanishes in the middle part, I don't exactly know, I would say, because in the middle part, where there Because in the middle part, we're at the regime where we have many neighbors. So, um, yeah, it's unclear to me. Yeah. Like, I feel, okay, so my sense, so, so, so, like, in the HP squared and heat regime, we're, we're using, like, we actually aren't taking, we're not able to take advantage of correlation decay for all values. Some version of correlation decay probably holds for all p, but we can only do it when p is really. Do it when P is really small, and when we can show that correlation case homogeneous, we can get to NQPQ. Right. So yeah, the correction of the statement should be the next slide, which is actually, yeah, this is the thing Sleal was referring to. So we're able to actually further improve the bound, the general bound from last slides, into a better bound in specific. Into a better bound, and specifically in the regime where we have a very sparse graph, like average-degree constant graph. And in that regime, we're able to improve the previous lower bound of n into like a true polylog n. And that is precisely due to we can get a better hold on this distribution of like the neighbor of the n-s vertex condition on the rest of the graph. So, and this is actually where And this is actually where correlation decay kicks in. This is a regime actually we saw this kind of phenomenon. We actually analyzed this kind of phenomenon. Sorry, can you tell me more about this correlation decay of the solar regime? Yeah, that will be next, coming up next. Yeah. Okay, so before talking about exactly why correlation decay matters here, I want to give a bit of a geometric integration about this distribution. Integration about this distribution of neighborhood of the int's vertex condition on the rest of my graph. So essentially, we can sample the neighborhood of the int's vertex in this two-step process. First, we can sample the labels of the first n minus 1 vertex based on the fact that these n minus 1 vertex generate a random geometric graph that we're given, which is n minus 1. And then the second step would be actually sample sample. actually sample sample Vn. Sample the label for Vn. So the label Vn will actually tell us exactly what its neighbors are because we already know the labels of the previous one. So essentially what this is saying is that, okay, this is only approximately the truth, so it's not entirely true. But roughly what it's saying that, is that to know what the neighborhood of n looks like, it suffices for us to know like It suffices for us to know, like, okay, sorry, let me correct my sentence. So, the probability that the neighborhood of n is exactly equal to some specific set Cs, S is a subset of 1 through N minus 1. This event is essentially just a function on all the labels for the vertex in S. Okay, why do I say that? Okay, why do I say that? It's because we said, like, n has neighbor J if and only if it falls in the sephi cap around j. So if we know the sephir cap, we know the center label of vj, then we're able to tell like what is the region that my vn should fall under so that there is an edge actually connecting vn and v j. Therefore, essentially the probability that n has neighborhood exactly as That n has neighborhood exactly as can be translated into a very geometric thing where we can say it's equivalent to the probability that n falls in the intersection of the sephere caps corresponding to vertices in S. Typet of mouthful, is that clear? Should I clarify that further? Maybe saying that it's like a CSP is the thing that will get this audience like online. Oh, I see. That's interesting. That's interesting because that's a part I didn't need to. But yeah, like what Celila is saying, like actually, yeah, essentially the conditional distribution, okay, the conditional distribution of the labels V1 through Vn minus 1 is actually a uniform distribution over a solution to a specific CSP. And this CSP is just saying, you know, like all the edges appearing in my graph actually have to have their labels being close together. Have to have their labels being close together, and all the edges don't appear in my graph would have their labels being far away from each other. So labels come from the sphere. Yeah, labels coming on the sphere. Right. But okay. But also, sorry, so I mean, here, I would think the intuition would be like if you give me the graph gn minus 1, that the conditional distribution on the v1 through vn minus 1 has very little structure to it, right? Like they, okay. Yeah, that's actually. Okay. Yeah, that's actually what's actually going to happen. Yeah. So, okay, I was saying a lot of words, but really, what I really just want to say is that to understand this distribution of neighbor of n, it suffices to understand for a typical set of s, where a typical set of s is just like, you know, we know that the neighborhood, the number of neighbors of n concentrates pretty well. So we can sample like a random, let's say it concentrates around some number n. Let's say it concentrates around some number n. Then we can sample random n points in the gm minus 1 graph and say, okay, what's the probability that this specific set will be the neighbor of n? Okay, if we consider such an event, then it suffices for us to understand the marginal distribution of the labels on this set of b's inside this conditional distribution, conditional geometric distribution. So actually, as we're So, actually, as what Ankar was saying, that what happens is if these distributions, if the labels of these typical sets of neighbors actually turns out to be roughly close to like uniformly identically distributed on the sphere, then what we would get is that this law of this distribution of neighbors will be very close to that of the Erdős-Renikraft distribution. So, yeah, so this is actually. So yeah, so this is actually what we're going to show. We're going to show that just a random set of labels will actually be quite uniform and independent from each other. Okay. So that basically works because for the end node, the likelihood of connecting to different previous nodes becomes independent because those are effectively drawn independently of Yeah, yeah, exactly. Yeah, this independency and the uniform, yeah, essentially gives a binomial distribution for the neighbor. Like you're doing a coupling with an Eric Strani graph, pretend you're doing a coupling with an Eric String graph. So you want, like, in the Eric Strange graph for any subset S, like, the probability that those are your neighbors is like PS, like 1 minus P9 minus F. Now we want to show that's approximately true even. Yeah, that's what you want to show, and that conditional. Yeah, so pick an arbitrary S, just you consider the arbitrary. Like now, the correlation there is that correlation because I don't know. Yeah, but but then here this uh definitely depends on the S, right? So if it's also an S this doesn't hold, but then yeah, so there are well separate. Ah, I see, okay. So when I write GM minus one, I see, okay. So when I write gm minus one here, this is already a specific graph. I guess something I was trying to hide is that this is a typical graph, like a typical nice graph that doesn't have an extremely high degree vertex, but just a nice graph from the random geometric model. So is the statement false force? Yeah, like I guess that the, okay, it's think about doing a coupling between an error-renewing graph and like a geometric graph, and we're going to build it one for text at a time, right? So start, okay, so one for text, like very. Right, so start. Okay, so one vertex, like very easy, like very boring graph. Now, like, we're in the induction step. Like, we already have an air-de-training graph, g n minus one. We showed that it was, like, reasonably likely to show up under the random geometric graph model. We're trying to add one more for its hex. So, given that we have this air to training graph g n minus one, we look at a geometric realization with it, right? We sample vectors conditioned on getting a scrap. So that's like sample vectors. Yeah, yeah, yeah. And now we want to know what's the, like, you know, catch. What's the like, you know, can we add an nth vertex without making equal to different things? Yeah, okay, yeah, that's a that's a better way of say describing how this GM minus one comes from. It comes from some sort of induction. Yeah. How much different is like conditioning on on typical maps as opposed to conditioning on like on typical like sets of That typical sets of minus one points on the sphere. Oh, okay. So actually, this saving, like I showed like two results before, and the first one, the kind of a worse one, comes from exactly where you're saying you're conditioning on actually the labels of the rest of the graph. But this one, actually, the gain here is exactly coming from not doing that and conditions that on the graph structure. Okay, so next then we're going to try to show that this comes up as a result of correlation decay. So essentially what ends up happening is that, okay, because we're in a very sparse regime, what happens is that g n minus 1 is locally tree-like, and that is very nice. And also, if we consider a typical small set of s that could be neighbors of the in-silver text, they're actually pairwise very far from each other. They're actually pairwise very far from each other. We can assume that maybe in the local neighborhood of like log n over log log n or something, it's just tree-like. Yeah, so they're very far from each other. So just from these two conditions, actually, we'll be able to derive this independent, almost independently uniform result. So let's zoom into what happens in one of the tree, and then we would see why that's the case. So, if we zoom into one tree, so let's say this is a vertex from S, and V, it's its label, which is its location label. And these are all its leaves in the tree of depths L. And the correlation decay here specifically refers to the vanishing of this quantity. The quantity of like conditional on me knowing all the labels of the leaves. Of the leaves, what is the information I know about the labels at the root? So it turns out that this distribution, this conditional distribution, converges to the uniform distribution very fast as L increases. I guess the specific number here, I shouldn't matter. Okay, it does matter for the proof, but it shouldn't matter too much because I want really explain it. But then the point is that because this is a very good thing, Because we know that essentially condition on all the leaves, the root vertex is kind of independent and kind of uniform. Then, if we consider like in the previous graph, we consider like a neighborhood that are far away from each other, that all of them get their conditional independency for free because they're not connected with each other in the 2L-step neighborhood. And also, like, because each of them have marginal distribution being uniform, so essentially. Distribution being uniform, so essentially we get this conclusion here approximately. And from that, we'll be able to derive that actually this neighborhood distribution of the last vertex is very similar in these two random graph models. Okay, so what breaks breaks down when D is not that not drawing logarithmic G? Not drawing lower entity? Uh oh, when D is much smaller? Okay, so what we break down is that this decay wouldn't be fast enough, is it? Like, wait, one second. You're saying when D is much smaller than there's no shortage of logs, right? So, like O tilde factor. This O tilde factor has factors that are polylog or weak in N, not in D. So, like, you know, this. So, like, you know, this is a number that's supposed to be smaller than one, and if you raise it into a large outray, it gets smaller and smaller. But once it becomes bigger than one, like d is like a large enough polylog and it doesn't go zero. Reflecting like the truth of the matter, which is that actually you don't have correlation functions. Yeah. I mean, other things break down too. Like, it's like a, yeah, it's like a perfect. Yeah, it's like a perfect storm of everything just going terribly wrong, but for a good reason, I guess. Yeah. Okay, so just a spoiler, actually this graph will come up again later in this talk, which I think is something more interesting. Like the connection between that and this. But okay, let me move on for now. So basically, yeah, this is to just recap. This is the result improvement we get. Improvement we get, and basically, for the distinguishing problem, this is currently what we know in different regimes. Like, we have the hardness and an algorithm collapse, well, okay, does not have a gap in the higher dimensional regime. And, okay, in sparse regime, sorry, not higher dimension, but rather dense regime. In the sparse regime, we have a little bit of a gap, but I think this could be potentially improved just by improving the analysis of the correlation decay. Decay, but I'm not exactly sure, but maybe I feel like this could be the case. I think, more interestingly, there's this gap in the relatively sparse regime. This one, yeah, I don't know how to close it. So, this will be some question if someone's interested in or have any idea about how to proceed here. Is there anything conceptual that you can say about what's going on? I mean, it sounds like the techniques are very different in the different regimes. Genes. But is there anything structural about correlation decay, about type of counting argument going into it? I see. So there's a couple of simplifications that I kind of ignore, which wouldn't exactly really hold in the relatively sparse regime. For example, I did say something like, oh, the neighborhood, if you want to know the neighborhood distribution, you only care about those labels. Those labels in your neighbor. But actually, that would break down when we're in this regime because typically we have so many neighbors and they have short passes among them, so they have very large correlation. So I think what I'm trying to say there is just that, yeah, it's unclear for me how to bound the correlation. I mean, another really different thing in the dense regime is that the restriction that you don't be named. That the restriction that you don't be neighbors with someone starts to really matter. So it's like the s this the typical thing that happens when you're trying to do like correlation decay arguments in the in the like graph, right? Like you want an approximate graphical graphical model structure and exactly and when you're in the spice regime you have it but then even once you become a little bit denser the requirement to not have edges to everyone who's not supposed to be your neighbor starts to like insert the sports on which can't make it work. So that's like like as I was saying like a lot of things start to go wrong but As I was saying, like a lot of things start to go wrong, but that's uh that's uh okay, kind of short on time, but but it's okay. Uh we can talk about weak recovery. So this is another question I'm I'm really interested in, uh, which is like uh the weak recovery problem, which is when we know that for sure this graph is sampled from a random geometric graph, and it is actually in the regime where it's not the same as. Where it's not the same as our Literary Anni graph. Then, in this case, can we recover the, can we recover? Maybe we couldn't recover the exact label because everything is rotationary and completely invariant on this here, but can we at least recover maybe, say, the inner product matrix? So there has been some work on this exact problem, but in slight different models than the one we were focusing on. Focusing on. So, I want to first talk about this more general model, which is called soft random geometric graph. So, in the soft random geometry graph, once you know the distance between two vertex, it's not the whether there's an edge in between them is not deterministic. It's not like if you're far, there's definitely no, and when you're close, there's definitely one edge. Instead, they actually have a connection function that maps this distance or the inner product to some probability. To some probability. And then what will happen is one will connect the two edges with the probability of rho applying to this inner product. So essentially, you add another layer of randomness and independency when you're sampling the edges. So the specific random geometric model we were looking at is basically rho being a source or function at some source of tau. And it only takes value 0, 1, and nothing in between. One and nothing in between. So you can, of course, do this integral to figure out what is the marginal edge probability of A vertex. But that isn't too important for the presentation. So what I want to say is that in this specific model, the weak recovery problem is formalized as when would there exist an efficient algorithm that recovers this inner product matrix. Recovers this inner product matrix up to some small error in Foubenius 1. So, okay, so it's epsilon square, so that's kind of saying like typically you expect a one entry of the matrix to be off by epsilon, square root of epsilon. Okay, so there's this result by Aria de Castrel, Aldan, Marculiser, and Peters, where they actually show that weak recovery. Show that weak recovery is efficient for this soft random geometric model. However, in low degree, which means they're considering the case where D is not a function of n, but rather d is some bounded thing that doesn't depend on n, and n could go to infinity without influencing d. So in this regime, they impose some condition on this connection function rho. I will not spell out what this exact condition is because it could be a bit mysterious. Because it could be a bit mysterious, but actually, what's really nice is that their algorithm is really intuitive. It's a spectral algorithm, as follows. What they do is they simply take the adjacency matrix that we see and then doing some processing, like remove some bad vertex and like very high-degree vertex, things like that. But in general, you can think about just this being in spirit, just the adjacency matrix. And the first remote. Jacques matrix, and first remove the top eigenvalue space of the JCNC matrix. After that, project the matrix on its current top D eigen space. And then this eigen space will end up retaining information about the inner product matrix. Yeah, we can just rescale it and essentially it will be approximation for the inner product matrix. So the intuition of that comes from the following. That comes from the following. So, if we do this specific projection, where okay, we first need to remove the trivial eigenvector and then do this projection onto the top D eigenspaces. What happens is that this is roughly like projecting this row V V transpose matrix. So what is this matrix? This is a matrix where we already fixed all the labels of the x and y sorry, all the labels of the vertices. Of the vertices, and then we apply this row function to it. But we haven't yet decided, like, but this is the matrix where entries could be like valued from 0 to 1 instead of just the 0, 1. Okay, so they're able to show that these two projections are roughly the same. So the top D eigenvector, top D eigenspace of A prime preserves that of like rho V V transpose. But okay, what Transpose. But okay, what is so good about rho V V transpose? It turns out that, roughly speaking, this dimension D eigenspace of Rho V V transpose is exactly a rescaled version of the inner product matrix V V transpose. And it's for reasons about, well, it's because of reasons related to the fact that we're picking this row. Row, okay. In this softer random geometric model, we're sampling our vertices from the uniform distribution. So there's some rotational invariance, and because of that, actually, the eigenfunctions of this operator rho will all be spherical harmonics. And we know like a degree one spherical harmonics actually tells you information about inner product between vectors. So this I will not really explain. Yeah, explain things. But the algorithm is quite intuitive. It's a very nice spectral algorithm. And very interestingly, it doesn't even require knowledge about row. You just do like a prediction. And that's it. Very, okay. And one comment I want to make that is that this algorithm works because, kind of because all the other eigenspaces of rho are Are relatively not significant in comparison to this like a d eigens space, dimensional d eigenspace. And that kind of requirement is precisely what's required in the eigengap condition, the regularity condition. That is to say, the contribution from all the other eigenfunctions are very little. But unfortunately, if we consider the row, which is this threshold function, This threshold function in the random geometric model, in the hard random geometric model, it doesn't satisfy this eigengal condition. So, this analysis at least doesn't hold for the random geometric model. Okay, I think I'm almost out of time. So, let me just quickly say that this recovery problem is also like studied and understood in the Gaussian mixture block model, which is a general. Block model, which is a generalization of the random geometric model into a different metric space. Well, okay, the space is just Euclidean space, but with a different distribution, which is a mixture of Gaussian. And this is shown by Chong Kingly and Swiel. Maybe really the last thing I want to mention here is that in this Alda McKulenser and Peters paper, they actually compute the KS threshold for this. The KS threshold for this sparse random geometric model. And as I promised, this graph appears again because without saying too much, to compute the KS threshold is equivalent to compute some specific quantity defined as this one exactly in this model. And I would just want to say that their KIS threshold actually is exactly this quantity, which doesn't make This quantity, which doesn't make much more sense, probably, because I haven't really told you what lambda one rho is. But I just want to say if we plug in the sparse random geometric model on the sphere, the chaos threshold will end up being like, give you d equal to order of login threshold. So, what that is saying is if we believe that chaos threshold correctly predicts the weak recovery threshold, then it's kind. Recovery threshold, then it's kind of saying, like, we can only hope to do recovery below D is smaller than others. Wait, sorry, but this is like this depends on the first eigenvalue of rho. Yeah, and you're just computing the first eigenvalue of rho for the threshold function. Yeah. Yeah. So yeah, actually just plug in this. This one is actually quite simple to calculate. For this result, they don't need any gaps in the spectrum or anything like that. No, no, this is like a hard. Yeah, this is just a KS partial. This is just a KS version. So, this is, yeah, it's independent of their algorithmic result. Yeah, okay. So, that's something to think about, which is like there are many things mystery about weak recovery, and we can try to think about whether there's actually this information computation gap, and also come up with actual algorithm that works for the hard-randed geometric model. And maybe think about maybe are there other notions of complete recovery that would make sense other than the spooky? Okay. Okay, yeah. You don't know the dimension for you to do this? Ah, yeah, that's a very good question. Actually, if you don't know the dimension, I think there's some work actually exactly on trying to learn the dimension. So these models are all under the assumption that you actually know the dimension, because then you need to do the projection on the correct dimension. Yeah. There's a gap in the spectrum. So you probably could detect. I think in that case, you probably could detect. Yeah. Yeah. Yeah. Yeah. Okay. I think uh these are just some of the citations. I don't have time to have one more question while we change over the projector. So what do you think about the uh this spectral This spectral algorithm of embedding by the eigenvectors would work for the hard random geometry? Yeah. It does. It does. Yeah. What do you like? So like as in my favor which one thing the like one we prove it to this uh like Gaussian motion lock model, but actually it doesn't like we do it by reducing the case we have roughly like two separated spheres. Roughly like two separated spheres, and the two separate spheres are like only more complicated than the one sphere. And so, in the one sphere, we can show that basically, I mean, there's some like in some parameter regime you can't do it, and then in the other parameter regimes, like so, so the like roughly, I mean, it's not like shut up to constants, but up to constants or maybe polylogs. Exactly when the recovery problem is well defined, you can do it with a step. So, what's the open problem? Is it not an open problem? Yeah, I was wondering whether you can improve or what about like sparse regime. Would it also work in sparse regime? In sparse regime, we didn't do the analysis carefully, but I think that if you do the analysis more carefully, then it will work. But it's like the usual issues with the trace method when you're in the sparse regime, like you have to probably delete the isolated vertices and that kind of thing. Like that kind of thing. Okay, so yeah, that I don't know, but okay, that seems like code. That's yeah, it's not like it's not a theorem in every situation, but like there's like an I strongly believe that it should question, but would there be any chance of collecting like much weeks and budget dependencies? Yeah, that's a great question. That's a great question. Because if you know the thing, maybe it's easier to know where they are. That's a super great question. I don't know how to answer it yet. Okay, shall we give Patelli the floor? Ready? Do you want to be recording? Can you press the recording button? Actually, if you just press stop recording.