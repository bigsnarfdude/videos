Okay, so welcome back library line. Our first talk after the break is going to be given by Luji Balog, who is a professor at University of Illinois, Obama. And he's going to be telling us about sunflower sunset systems with BC dimension, small BC dimensions. Thanks for the invitation that I can be here. Can be here, particularly playing inside or is it outside? So, the focus of this talk is somewhat different from the focus of other lectures. So, don't get scared of the amount of the slides. It's based on a two-hour lecture. So, I can stop whenever the bell on rings and if somebody wants to know details and you have to add other things. Cuppy Plays and others, it's a useful way of spending time, and it was an AIM project. Co-authors are Anton Barstein, Michelle Dark, who is a Zoom participant, Osaf Herbert, and Budua Shan. So, let me define first what the sunflower is. So, we have a set. So we have a set system on a ground set and R sets form an R sunflowers. For any two pairs of different pairs, it just means that these two sets cannot be the same as that two sets. So if for any two pairs of different pairs of Pairs of cells, their intersection is the same, and that can be phrased a bit differently. That we have an intersection, a so-called kernel, and the pathas and the pathas of the sets are all parallel between sets. It's an important to understand these definitions of asset pressure now. So, it's probably clear. So sunflowers were defined by Erdős and Roddo about 65 years ago. So assume that H is an L uniform hypergraph. So the L uniform means that all the sets in the hypergraph have size exactly L. Assume that it doesn't contain an arson flower, then the number of hyper edges is up to. number of hyper edges is upper bounded by R minus 1 to the L times L factorial and note that here there is no dependency on the number of vertices of the hypergraph one can put up some restrictions but then it becomes a different problem that I did not plan to talk about and in order to have a proof in the lecture I Proof in the lecture: I will put this statement and the proof is by induction on L. If L is equal to 1, then all the sets have size 1. And if the sets are different, because we don't have multi-edges, so we cannot keep edges, then it means that as soon as we have R of the singletons or R. Singletons or R sets of size one, then we have R sunflower because actually this intersection could be an empty set, and then we get R minus one upper bound. So now assume that L is at least two and we do induction on L. So we take a maximum collection of pairwise H disjoint hyperages. The number of them is at most The number of them is at most R minus 1. Otherwise, if we have R of them, then they form a sunflower. They cover exactly, or at most R minus 1 times L vertices, with each of them as size L. And the following is used, that if a vertex is in more than R minus 1 to the L minus 1 times L minus 1 factorial hyperages. Some factorial hyperages, then we can apply induction, but we can do that from each hybrid which contains this vertex. We remove that vertex, then the uniformity drops to L minus one. We apply induction on that system and System and then we are doing fine. And now, using that each vertex is an atmosphere, many hyperages, then we cover all the hyperages by r minus 1 times vertices, then we get this upper bound and the numbers just that's a simple inductive So that's a simple inductive uh proof. Uh uh in many of the graduate basic combinatorics courses these are taught or assigned as models. Any question? So then I uh start to talk about lower bound. Maybe that's a nice lower bound. uh nice lower bound uh that i construction not mine uh that i talk about it and it's not the best possible but it's kind of uh instructive to understand so we start with a complete rooted r minus one every tree so it means that this vertex is a root and every vertex has r minus one children and we go down depth And we go down depth L, where the depth L means that the number of edges from the root to the leaf in a part is L. So in particular, the number of leaves is R minus 1 to the L. And we define our hypergraph the way that we take the root and go to an arbitrary leaf and the edge set of the path. Set of the path is a hyperage. So we define a L uniform hypergraph with R minus 1 to the L hyperadj. And observe that it does not contain an R sunflower because if you want to build your R sunflower then there is a point where not all the edges of the Edges of the potential sunflower are the same, and if they split, then they cannot be all different because we have R hyperages, but only R minus 1 or 6. So at that point, we violate some values proof. And so here the construction is R minus 1 to the L, the upper bound R minus 1 to the L times its L factor. L time. It's L factorial and for this problem Arbisch and Rados set up for the parameters. The other cases are also interesting, but they were in particular interested to know when R is a constant, fixed number, and L is going to infinity. So in this case, if we compare the two functions, then this is exponential because R minus 1 is constant. R minus 1 is a constant and it's factorial, and it's R minus 1 to the L is a lower order term, and they conjecture that the lower bound is closer to the root. But again, with not exactly with this exponent, I think when r is equal to 3, then that exponent is 2, but maybe square root of 2 Maybe the square root of 10 is known. And now I phrase a general strategy that how to upper bound. It's a very natural strategy if you like probabilistic methods. So step one is that we take a p-random subset of the vertex set where p. Vertex set where P is 1 over R. So the P random means that for each vertex of the hypergraph, we flip a coin and we put the vertex to A with probability exactly 3. And step 2 is that we prove that the probability of that this vertex setting A contains a hyper edge is large. contains a hyperage is larger than 1 minus 1 over r. And assume that we do these two things, then now we can say that using some coupling arguments, we not only choose one p random subset, but we partition the vertex set into R p random subsets. It can be done by It can be done by uh coupling. So in uh the probability for each of the parts that it contains a hyper energy is larger than one minus one over r and using first moment technique it implies that there exists a partition of the vertex set, that each class contains a hyper edge that forms a sunflower, it forms even a matching with a Even matching with empty gamma. So that's a reasonable strategy. If I was more expert how to write slides, then I wouldn't have this red line before. So even without it, you would suspect that that strategy has some clause. For example, if there is a vertex which is in every hyperage, then it's clear that we don't have a matching. We don't have a matching, so that step is fast, and because we can do this easily, so it means that this statement is fast. So it means that that strategy doesn't work, but if you recall the inductive proof from Herbert Chandrado, if we have a vertex which is in many hyperage, then we can just remove that vertex and Remove that vertex and we gain something, and we play on the hypergraph that we obtain that we keep only edged which contains x but remove x and we drop L by 1. And still it looks kind of fishy, but this idea almost it works as So it almost it it is in the paper of Arvai, Slovaku and Zhang. It appeared in announce in 2021 and they practically made this argument that I presented not completely, they haven't resolved the entire conjecture, so they proved that if the hyper So they prove that if the hypergraph is L uniform, there is no R sunflower, then the number of hyperages, it's comfortable to scale with one over L is R cubed, so it's a constant times log L times log log L. So if we compare with the earlier product bound, it was L factorial over 1 over L. So they improved L to log L. And then people Look at that. And then people later clean the proofs, but like the gap between the complexity of the contribution, so that was an analysis paper that says I am discreet or the cleaning work, but still it's uh nice to clean and uh compared to the conjecture this login is still uh there. Uh yeah I can use it. Uh so usually if we can uh people cannot solve a problem then they put some uh more restrictions or try to solve some uh special uh case uh as Fox Park and Fox Park and Sukko did very recently, so they added an extra restriction that the V C dimension of the hypergraph. Okay, it's this page. So this restriction was added by Fox Pack and SOOP, but I should actually define it. So the restriction will be that for some Will be that for some D we say that the hypergraph has V C dimension at most D. So, what's the definition? It is the maximum D such that there exists X subset of the vertex set of size D that if we intersect the subset with the hyper edges, then we get all the two to the The possible intersections, and this was motivated, this definition was motivated by this great geometry. And here is an example which actually kind of suggests that why it is called as diversion. So the example is that let's say that we have a vertex set, which is a point set in. Vortex set, which is a point set in the plane. It can be finite, non-finite, but we assume that there are many points, not all of them on one line. And the hyper edges are collection of points which are maximal polynomial point sets. Then the VC dimension of this hypergraph is two or at most two depending if which will sufficient many points. So why not three? If we had three points then we need to intersect the three points as well so the three points must be on a line but uh I cannot choose like uh intersect exactly two points because there is no line which intersects exactly this uh two points. Oh, but the other example is this familiar uh picture or familiar hypergraph uh that I showed a couple of uh minutes ago. The VC dimension of this hypergraph is also one because if you want you choose uh two edges which are two vertices of the uh hypergraph, then uh we cannot get all the four possible intersections because if the Intersection because if the two edges are not on the same part, then there is no part which contains both of them. And if two edges are on the same part, then there is no hyper edge or no part which contains only the lower one and not containing the upper. So this hypergraph is also an example for a For a hypographic DC dimension one, and it's not a big restriction, it's not necessary dropping the number of hypodies under the restriction that there is no R sunflower. So Fox, Park, and Suka assumed that the V C dimension of hyper The V C dimension of hypergraph is one, a uniform R sunflower, then they group the Edel Shadow conjunction under this restriction. The bound is constant, it's not the smallest constant. So, one of our contributions, but it's really easy, just like modification of the Nardo Schrado proof that prove that the example that I showed is exactly best possible. So even if we put plus one, then we get the RSM flower. Main result is when we have general the VC dimension is D, then the Volksbach sucker almost proved the Erdős Corrado uh or Erdős Rado. Erdogan conjecture that we call it the log star is almost constant, is the number of times one has to take logarithm of L to get something under so 2 to the something to the log star l is the normal scale of 12. Constant, and what we proved is kind of a nicer formulation of this. So the log star is down, the log D is down, and we multiply with C R. So if we try to understand the result just uh uh a bit, then uh Then if you have an L uniform hypergraph, then the V C dimension is at most L because there is no set where we can intersect. There is no set of size L plus 1 which can be intersected. So if we put here, if D is L, then it's coincided with the best known bound. But we are not reproving the best known bound because our proposal goes by induction on Composable by induction on L. So if L is at most D, then we use that result as base case. And the main merit of this result is that it's using similar techniques as the OLWISE and the other results. So hopefully what. Hopefully, or there is a slight hope that that method can be pushed further to eliminate this bias. The main key key result which can be found in the literature is either concoli conjecture. The Khan Kalai conjecture or Frankston Khan Narayan Park or Park Farm or in that version result is the following. So assume that we have a hypergraph where the set sizes, edge sizes are at most L, we have some small Q, then one of the two Two conditions hold. So, what is the first one? So, the first one is that instead of having a complicated hypergraph H that we start, we might work with a simpler hypergraph F whose upset is containing H and Or h and if we have a pure random subset of the vertices, then the expected number of hyper edges from f is bounded by one half. It's kind of saying that in the probabilistic world the weight of f is small and then one can put up some And then one can put up some inequalities that if the weight of f is small, then the weight of h is also small. The other case is that if we have a slightly bigger p and the slightly bigger is just log factor larger, which makes our trouble that the conjecture is not yet resolved, but it's known that for such a general result, For such a general result, this logal factor is first uh is needed. So assume that we take a take a p random subset of the vertices, then the probability that a random vertex, so this notation means that the random set W contains a hyper energy is larger than one minus 1. So that's among the lines that I have my I have my easy approach. So, practically, it says that the proof method that I suggested in the very beginning either works or we can find some F well disinequerity hots. And our contribution is that if we add the condition that we see the measure at most D, At most D, then the first statement is the same, one half or two, so doesn't really matter. And the second condition is this log L is changed to particularly log D plus log star. And in the remaining minutes, I try to explain that how to make explain that uh how to make the proof work to the uh slun sunflower uh result. Here actually it doesn't really matter if we are using the first TORM and then prove the weaker result or or use the second TORM at the VC condition then we get the better result because the proof method is about uh uh same. Same. So and say that if it we have induction on L. We choose P to be 1 over 2r. Epsilon is one of the biggest that you see. It's one half. And we have to look at what's happening if we have either of the cases of the two taurams. And the second case is the same as I Is the same as I promised, it's just phrased slightly differently that we partition the vertex set into two R sets by and these are all random sets, so the probability for each to contain a hyperage is bigger than one half. So in expectation we have more than two R over two, so we have R over 2, so we have at least R sets which contains an edge, and we found matching. So that's the same as before. And the second case is when we might have a vertex or a subset of vertices which contains too many hyperages and this handled the way that I Handle the weight that R. So, if we are not in the first case, then we have a family F which whose upset contains the hyperage, but its weight is small. So, assume that we have too many hyper-edges, then we can partition the hypergraph into sub-hypergraphs based on that if I have a F set of the set system from Set of the set system from F, then HF is the hyper-edged containing this F. And then for each of them, we know that the number of hyperages is equals Q to the 1 minus L, because otherwise we can just remove it and apply induction. And if we know this, then it's just a simple Just a simple computation. So, first line assume that we have many hyper-edges. Then, in the second line, we know that this HF partition H because F are covering H. And then after I just here I write in the condition that Condition that I get and then from this area get that one over Q to the L is upper bounded by two thirds of the same function. Maybe I was a bit fast on this, but it's really just double counting the hyperage on F. How much time do I have for my remaining ten slides? So believe me that that's completely elementary assuming it remote definitions or understand the definition. And here I just try to have a feeling at how to prove this part one from a statement. from uh statement so that i just put here the the toram so it's the same as before and the uh idea is that uh we are using kind of a nibbling if you are familiar with the nibble method that we start with some random sets that's a q here just it and strange way so we start with a random subset and then we hope Then we hope that this random subset contains a hyper edge. If it doesn't contain a hyper edge, then for each hyper edge we write down the part which is not covered and then observe that from that system it's enough to worry about minimal sets because if I have a bigger set and a subset then uh it's enough just to cover It's enough just to cover the subset. I don't have to cover the bigger set. And then when I have this set system, then I partition them into two parts. If a set has size L half or bigger, then I put this set into F and then because it's big, it means that it's way. Because it's big, it means that its weight is small, so it won't contribute a lot, and it will cover many of the hypologies. And if it is small, then from these small sets, we keep working and we try to cover them. So we iterate the process choosing another random subset of vertices, but now L drop to L half, so the number of times. Half, so the number of times we have to iterate is at most log L. So that explains this log L factor. And in the end gain, either we manage to put every set into F, if every set is in F, then it means, like this guy, then it means that every hyper energy is covered by F, so we are in that case. So we are in that case, and because we always put in large sets, some calculations show that we are in that case. Or if the other case is that we finally manage to cover a set, then it means that we are in that case that we have a random set which covers something. And there is no other case because this requires actually. Because this requires actually more time than I have to think over that if I keep doing this, then there is an end. You cannot do anything if n is zero. There is no other thing. And what we did that for a fixed VC dimension, instead of reducing the size to L half, we reduce it to log L. So it means that the number of iterations is log star, so that's is log star so that's why we get log star there and if somehow one could make this to be even smaller so if you one could make the number of iterations to be constant and some of our injections would be solved and uh I think that's all that I can uh set you in do I have to press something? Yeah. Thank you. Thank you.