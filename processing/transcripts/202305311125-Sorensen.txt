Hi, thank you very much. I'll mention this in joint work with Sean Roadhouse and Kira Satimo. Very happy to be here celebrating Timo's birthday. Especially from a couple weeks ago, it's been a really honor to work with Timo during my PhD years. I walked into his office about a little over four years ago and asked me to work with him, and he's given me a great opportunity to get into this field, and it has been an incredible adventure to collaborate. I really appreciate being here with you. Collaboration workshop being very few in its own temperature. So, I'm going to give a very quick synopsis because I think these things have been defined a lot. And Chris's talk was a very good introduction to this talk. So, I'm working with the KPZ equation. The only difference from the previous talk, I'm going to use this parameter beta. This inverse temperature parameter. If you know what happens for beta equals one, you can just scale things. But later on the talk, I'm going to be taking beta either to zero to infinity, so it's going to be useful to keep beta in there. Useful to keep data in there. And then we have the Kohlhoff solution using the stochastic heat equation. And we're going to work this four-parameter field of narrow-edge solutions. So here we're starting at time s with the delta mass at x and running it to time t at location y. Okay. Oh, what's wrong? Okay. So this Huzman process has been constructed. Just to give a concrete idea of what a Buzzman function is, so Chris Chris mentioned some properties. This. So, Chris Chris mentioned some properties. A quick definition for a fixed direction, I'll be taking a limit of these ratios of these things. You can interpret this four-parameter field as a ratio of partition functions for the CDRP. So Z over R goes to negative lambda and R is going to negative. You're increasing the initial time down to negative infinity at a certain rate. This is one way you can define the Boozman function. Now, this works for a fixed lambda, and you can do this for a Kennelbent set of lambda. For a cannibal density of languages, for example, and fill in some holes. But potentially, there's some discontinuities of the process in the lambda parameter. So when you find this for a process plan for all directions simultaneously, in general, there's right and left continuous versions of this process. That's the lambda plus plan. Okay, so I have this set, big lambda, is a set of all directions where the Boozman functions disagree for some character points. So it's a random set that depends on this omega. So it's a random set that depends on this omega. For example, if you fix a lambda, Chris mentioned, then this is not a discontinuity with probability of 1. But there potentially could be some random discontinuities of this process. Oh, wrong way again, sorry. All right. So, and as Chris mentioned, I'll summarize this again, that the Boiseman functions are global solutions. So, this is a key point that we use here: that you can evolve the Boisemont functions from one level to the next using this formula here. So, you solve the KPZ equation. You solve the KPZ equation from some initial data. So, just a picture of what's going on here. Here, time I'm putting in the vertical direction. I'm looking at the Boozman function from Sx to Ty. So, this is a ratio of these, this limit of these ratios of these four parameter fields. And you can interpret this as the free energy of a polymer. So, you can be these paths going up to the negative infinity, or to infinity in a certain direction. And a path from Ty has to cross through time r at some point. A time r at some point. And I average over those possible points, and this is where this kind of idea of where this formula comes from. But the main point is that these are what we call global solutions or internal solutions. And they're stationary in both space and time. Okay. And a simplified version of what Chris had just mentioned is this one force, one solution principle. So, in fact, here I have a function, but you can actually do this for a measure. But you can actually do this for a measure. You start some function with appropriate growth conditions. At least get a little technical, right? But then you look at the solution of the KGZ equation from time r, and the time is going off to negative infinity. And you can use ratios. This is this mod plus c condition. We take ratios. And this converges to the Boozman function. Okay, whoops. And so one way, there's a lot of ways this technical condition, but one. A lot of ways, this is a technical condition, but one way to think about this is you could just set: if this condition doesn't depend on the time r, just some function, that function has some growth condition, then that will be enough to ensure that this condition. But it's certainly not necessary. It's a sufficient condition. You can think about these as conserved quantities because I would call it the one-force one-solution principle. But this principle fails exactly when you're in this set of discontinuities, because now you have two global solutions. You have the lambda minus and the lambda plus solution. So in that case, there's exactly two. So, in that case, there's exactly two things. Oh, darn, I keep doing it wrong. Okay. So, there's been a lot of research on the one-force one sclusion principle in the past. This was originally done by Sinai for the Stochastic Berger's equation in a compact setting. And then, more recently, in work of Bachmann and Kaktor and Kostakanen and other papers of Bachman and Bachmann Lee, there's been done in a non-compact setting for Fergus equation with Lucy-ranning force. Berger's equation with some reinforcing. So there's been a lot of work on this, and there's also a lot of work on that FEP and LPP models where there's Boozman functions, we say the regularity and then the direction parameter. There's not quite an exhaustive list, but it has been studied quite a bit in other models. So there's an open problem from this paper about whether this discontinuities is empty or not. Empty or not. So potentially it exists, but maybe it doesn't. So what they were able to show is that either it's empty or if it's not empty, it's countably infinite and dense. So zero-wise kind of an arrogatistic argument. So we want to figure out which one is the case. If this is the case, then really we don't want to talk about discontinuities at all. It's really a boring conversation. But we want to kind of rule out which one is correct. So now this has been done in some zero temperature models. So all the exactly solvable zero temperature models, we've done this. Exactly solvable zero-time models we've done this have been on for to show that there are discontinuities. But this might seem to be a little different because it's a positive term overall. You can think this is a smooth version of the zero temperature case. So perhaps the discontinuities go away. One thing that is known before our work is that the structure of the discontinuities is very different if they exist. So it's not the exact same problem. So maybe you could guess from these cases that there are discontinuities. Maybe you could guess from these cases that there are discontinuities, but perhaps because we're looking at a positive temperature model, the structures, perhaps they go away. But in fact, they do exist. So this is a subject of my talk, is that there are discontinuities. That set of discontinuities is countable infinite dense. But because of this, we just need to show that there's a positive probability that this is non-native. Okay. So one simplifying thing that we can do. One simplifying thing that we can do is we can know that it's enough to describe the Buzzman function, the Boumazon process, along a fixed time level. So my points are going to lie along the same horizontal line. And because of the additivity of the Boozmann function that Krista described, I can let one of these, these spatial points, just be zero. And then let x vary. So I'm letting lambda vary and let x vary. So I went from five parameters down to two. Say the regulator, this process, it's. Say the regulator, this process, it's much easier to work with. And the nice thing about this, too, is if you put your two points along the same line, you get monotonicity in the Lambda parameter. So much nicer to work with. Now, if you understand this process, well, we have this evolution rule for the Boozman functions, and so we can really understand the process, at least qualitatively, once we understand this process here. So, the purpose of this project was to really understand what is the distribution of this process as a coupling of Brownian motions with the drift. Of Brownian motions with the curve. So they've been constructed, they have separate properties, but we want to have some kind of public interpretation of what their distribution is. And each one is a Brownian motion with a diffusivity parameter and a drift. Here's a picture of what they look like. They're coupled Brownian motions. They're certainly not independent. They each have different drifts, but they're monotone. So different colors correspond to different values of lambda, and higher values of lambda are higher than the lower ones. So this red is higher than the blue. Red is higher than the blue. And not only that, the increments of the red are always bigger than the increments of the blue, and so on. The increments of the blue are bigger than the increments of the purple. So here's a simulation, and I'll describe in a minute what a distribution is, but at least qualitatively, it was already known before that this is the picture. This is a monotone family of grounding those two. Okay. So what we want to do is we want to describe this process, give you some kind of description of this in this distribution. So, to do this, we're going to expect this process. I'm going to call it f now. Before we had V is for the Boozmont function, so I'm going to parameterize it slightly differently, so I'm going to call it F. So, beta is a fixed parameter, and lambda is what's varying. Lambda is a drift. I want to describe a coupling of Brownie motions with drift. Now, I want to do this for all lambda simultaneously. Now, there's potentially going to be some discontinuities, and also there are discontinuities, so I'm just going to choose this to be the right continuous version. So, this is the lambda plus. I was just make a choice. Lambda plus, so this would make a choice. It lives on a nice topological space when we do that, so there's some nice things that are involved there. So I want to describe its finite intelligence distributions, and once I do that, I show they're consistent, and I get this process, for example, on a countable dense set of lambda, then I can fill in the holes appropriately. So what I do, these are certainly not independent. You see, they're monotone. But I'm going to start with independent brownie motions with drift. And I choose the drifts that I want. And I'm just going to. And I'm just going to map them through these mappings called D. So the first component is y1, so that kind of matches marginally. The second component is a mapping of y2 and y1. And it's going to give me another Brownie motion with the appropriate drift. But now it's going to be larger than y1, at least for positive time, and its increments are going to be larger than y1. Here's what the map looks like. You take the two Brownian motions, so here's D2. And as a function of Y, you spit out Y1. function of y you spit out y1 is difference of logs so one thing you might notice just from the structure and I'll talk a little more about some of the motivation for it for this particular formulation this is related to the O'Connell York polymer but for positive y this integral is bigger than this integral and for negative y it's the other way around right so we can see the monotony kind of coming just from the structure of this map From the structure of this map. Okay, so once you understand d2, you can understand dk despite induction. So we get dk by understanding dk minus 1, and then we apply it this way. So the really important blue line is when we understand the map d2. So this right here defines a family of coupled Brownian motions. Now the fact that this is a Brownian motion comes from a paper of a neoconnell and Mark York called Brownian analogs of Berkeley. Called Browning analogs, a Burke's theorem. So it's certainly not obvious that this should give you another Brownian motion, but in fact, it does. And it gives you the direct dynam. Once you know that, you can understand that dk gives you a Brownian motion just deductively. Okay. Then we've shown that this family is consistent, then you can construct a process defined for all directions. Okay. So Ofra talked about that. Stationary horizon on Monday. We call this the KPZ horizon because it is an analogy. It's a very, the construction. There's an analogy. It's a very, the construction is similar. The stationary horizon, for example, you could just change this log of an integral into a centrifuge one. But we call this the KPC horizon. And so here's another picture to remind of what this looks like. So once we have this description in terms of integrals and geometric running motions, we can simulate it. And we get these nice pictures. Okay, so here's the lemma. The K to Z horizon that I just described describes a Boozman process. As I just described, it describes a Boozmann process as a process of a couple of Brownian motions with drift with its equality distribution. Now, here you might see why I use a slightly different parameterization. So, I have to multiply by beta to be equal to the Boozeman function here. Then I have to play with the drifts a little bit to make them match up. And this will be convenient later on, because later on I'm going to send beta to infinity or to zero. I want this to stabilize. And so, f, each boundary function here has diffusivity one, whereas here the diffusivity is beta. So I can multiply by 1. Is beta, so I can multiply by. So here's the lemma, and then we're going to study, once we have this lemma, we're going to study, given that structure of the maps that define f, we can understand its regularity. Okay. So to prove this, we look at the O'Connell-Yore polymer. So here's the O'Connell-Yor polymer, what it is. It's a polymer model. We look at the upright paths from XM to Ym. And whenever I have such an upright path, I take increments of the Path, I take increments of the Brownian motions on each level. So I have an independent Brownian motion on one assigned to each level. Then I add them up. That's the energy of a certain path once I take this exponential multiplied by beta. And then the average such sum of paths is what we call the partition function, this is z beta. And so this right here is also, we see SD for something discrete. This is an analog of this four-parameter field that the Stukasa-Keet equation. All right, now I can also start, or go ahead to questions. Alright, now I can also start, or go ahead to a question. Okay. I can start the Okanagar polymer from initial condition, F. This is going to have to have some kind of different conditions because it's going off to infinity in this direction. But what I do is I have some initial condition on level negative 1, and I take the value of f at some point, x, and then from x0 to y, and I just take the O'Connor partition function. So I integrate f of x times this right here as an average version. As an average version of this of a last message system. Questions about the definition? I'm going to use this bar notation. This goes back to this plussee that Chris mentioned. I'm just going to force this function to take the value 0 at, or take the value 1 at y equal to 0. I think it's log, I get the value log 0 at y equals 0. It's just a way of normalizing so I have the inference instead of just the function. So you can think of this. Instead of just the function. So you can think of this as an evolution of functions in discrete time n. For each time I have a function y, and I want to understand this stable distributions. Okay, so here's a lemma that allows us, is going to allow us to prove this, this description of the Bluesman process, is that the KPZ horizon is invariant for the O'Connell-Yor Polymer. What I mean by that is I can think about differential conditions on level negative one, but I can't. Conditions on level negative one, but I use the same bulk Brownian motions to evolve these dysfunctions. So the KPZ horizon has a joint distribution structure of coupled Brownian motions. I run up to this model here. For each time n, once I re-center, it has the same distribution as the initial condition as it comes. So the proof of this is somewhat technical, but the most important, once we understand the following, Once we understand the following, there's a way that these evolve. This is a Markov process in M, a couple of initial functions. And the way it evolves is the following. So we have this partition function here, and then we look at the previous line, and then we look at the next Brownian motion. And I apply the same mapping D that I described before. That allows us to go from one level to the next. So this fact comes from this paper of O'Connell and Yor, and it's not. And it's fairly straightforward. But once we have this, proving this fact here, it comes from what we call an intertwining argument. And it originates in the particle systems literature from Pablo Ferrari and James Martin, and it's been adapted in LPT in a couple segments. And so that argument is the more technical part of the proof. But the idea is that once we know the evolution of these things, we can look at the structure of these mappings need to show this intertwining. Show this inner planning. They show that this right here, this distribution is invariant for a colonel. Okay, so here's, that was ingredient number one, is understanding this invariance for Karno-Yor. We want to somehow tie this to invariance for the stochastic heat equation. Here's the next ingredient we need, is that there's some scaling relations for the KPC horizon. This looks a little wacky. What I'm doing is taking my inverse temperature to zero at a certain rate. Temperature to zero at a certain rate, and then I get one-fourth beta. This corresponds to an intermediate disorder scaling. And what I'm doing is, I'm doing diffusive scaling on these Brownian motions with drift. So for fixed lambda, each one of these is a Brownian motion with drift. And there's kind of this messy drift term here, and this is a messy term that's affecting off. But it's just a quick exercise to show that for fixed lambda, just rounding scaling relations, that sort of deserve equal distribution. The non-trivial part is just showing that as a cup. The non-trivial part is just showing that as a coupling in lambda, this scaling also preserves the joint distribution. And it comes from this description in terms of we have mappings of Brownian motions. We show how they respect these scalings. So that's an important piece. And this is actually exactly equal for each n. But as we can think about sending n to infinity, and this thing turns into what we call initial data for the Stochastic Heat equation, or the KPZ equation. This is the exact scaling you want, the intermediate scaling that turns initial conditions. Scaling that turns initial conditions for a polymer into initial conditions for a stochastic equation. Okay, so once we have that, we know the polymorph conversions of a stochastic heat equation. And we needed to add some additional details to show some tightness estimates. But once we know that, we know that the KPC horizon is invariant under the Stochastic Heat equation. So we have a couple of initial conditions. We run it through the Stochastic Heat equation, look at its increments. Q equation, we look at its increments as a coupling of initial functions. And this has the same distribution for every t. So I found some an invariant distribution. And before I talk about how there's a one-force, one-solution principle, where at least for a fixed lambda, or for capable of lambda, there's only one possible thing that could be invariant in distribution, the Sugasti-Keat equation. And so this has the following consequence that I think I mentioned before as a corollary of this last. I think I mentioned before as a corollary of this last fact that the Boosemon process is described by the Kingdom of the Rise. So we show it for finitely linear, and we can just fill in the holes from there. Okay, so once we know what the Buzzman process, we have some probabilistic description of the Buzzmann process now. We understand its regularity. So we have this description of its fundamental distribution, since we want to somehow use that to our advantage. Now this description To our advantage. Now, this description is a description in terms of mappings of Brownian motions. Coming up with quantitative information about it can be a little bit tricky. There are formulas we can derive, and they can be somewhat hard to analyze. So the tricky part about this project was just finding what's the right condition we wanted to check to figure out. We want to show that this right here has discontinuities. It's fairly easy to show that this process is strictly increasing and it has stationary increments. Increasing and it has stationary increments in the lambda parameter. This follows fairly nicely just from the structure of the mats I described earlier. So here's a lemma we prove. It's a very simple lemma. Just arbitrary stochastic process. You have an inconstationary, non-decreasing, almost straight continuous process. So and then I have this expectation condition. So four conditions. If that's the case, then the following is true. Then the following is true. This limit of n times the probability of a small increment being bigger than epsilon converges to zero. So actually, this has a very, very short proof. I can just kind of walk through why this is the case. And heuristically, we can think about this as: you have a very small increment. What's the probability of the stochastic process jumps larger than epsilon? Well, you think it should be kind of small. Squad. Well, you think it should be kind of small when we're multiplying by n, so some kind of competition here going on. We're saying if it's continuous, then this has to converge to zero. So here's the proof. It's very short. I'm just going to look at the process on 0, 1. It's an incompetent stationary process, so I'll just restrict our attention to a compact interval. And I'm going to set Jn epsilon to be the sum. To be the sum of the indicators that xj over n minus xj minus 1 over n were larger than epsilon. So as I'm subdividing this interval into n pieces, I'm just asking for each one of those little small pieces, how many of them were bigger than epsilon? Okay. So by continuity, J and epsilon converges. Jn epsilon converges to zero, almost sure. Okay, but what's the expectation of Jn epsilon? It's just n times the probability that x1 over n minus x0. Why is that? Because each of these has the same probability that synchronous stationary, right? Okay, so now I want to use domain convergence. Well, it's true that epsilon times j and epsilon is Is smaller than or equal to x1 minus x0. So this is an increasing process, or it's a non-decreasing process. I jumped larger than epsilon this many times, so this is less than. That's why I used dominant convergence theorem and stuff. So it's a very, very simple proof. The tricky part was just finding the right condition to be verified. So what we can do from this is just from the description of the finite distributions of this process here. Of this process here. I'm just going to replace n with lambda inverse. We should, the limb nf here is bigger than zero. It stays away from zero. So we know we are having the problem. We know it's increment stationary. We know it's not decreasing. We know these reach Brownian motion, so they have finite expectation. But this condition fails, so the thing that must have failed is confidently. So here's like this condition here. So, in the last part of this talk, I'll just kind of mention. Last part of this talk, I'll just kind of mention what happens when we take limits in the beta parameter. So, this picture I showed before: this is the KPC horizon for beta equals 1. I had five different trajectories of grounding motions. This is when beta equals 0.1. This is when beta equals 20. So, there's different values of beta. So, they look very different. Now, for each any finite beta, these are all self-similar. You can just take one by scaling the others. Scaling the others. As you said, beta to infinity, there's a very large beta, something interesting happens. They tend to kind of stick together. So these are all set strictly separated. I think these are strictly separated as well. But when beta gets very, very large, they get very, very close. This actually converts to what we call the stationary horizon that Over mentioned a couple days ago. We're actually in the stationary horizon, the brownie motion like to stick together for some time and they split apart. And that's the subject of this next theorem here, that as beta goes to infinity, this thing converges and distributes. This thing converges in distribution to the stationary horizon. And see, it was important that I normalize so that each of these had diffusivity one, otherwise this thing would just blow up. Now, when beta goes to zero, something a little more boring, take a Brownian motion, just add linear shifts to it. And that's what's happening here. You can see it kind of looks like this right here is the pink, the purple one is just a Brownian motion with zero drift. You're just adding lines to it. Okay. So I was mentioning here you think about this process here, just kind of choosing beta to be equal to t to the one-third, and setting t to infinity. We know by what I just said that this right here converges to the stationary horizon. Now, because of the scalar relations of the KPC horizon, this is at the same distribution as this thing right here. Now, this looks kind of messy, but this actually looks like now it's scaled initial data that converges to A. It's a scaled initial data that converges to the initial data for the KPZ fixed point. And so, because of that scaling action, we can interpret this in terms of the following, combined with recent work that Xuan Wu just showed yesterday, that the 1, 2, 3 scaling of the couples and KPZ equation. So from different initial conditions, the stationary initial conditions, this converges to the coupled stationary KPZ fixed point that Ofer described a couple days ago. Thank you very much. Thank you very much. Thank you, Evan. Any questions? So, Newburgh, was there instability in the titles? Do you that say something about this? Yeah, it's just related to the fact that there's a failure of this one force, one solution principle. And there was random, there's random exceptional directions, where now you have two solutions. And let's see if I can go back to this. And let's see if I can go back to this. So, this one force one solution thing here fails. You could have this initial condition with the appropriate drift conditions, and you can't connect it here. It's going to converge to anything here. It could fluctuate. We actually don't know. It could fluctuate. We know, at least if you take things along the same horizontal line, there'll be some monotonicity where it will lie between the lambda minus and the lambda plus. But the instability is just because you have. But the instability is just because you have these kind of these gaps in the lambda frame where now you start from some initial definition with these appropriate direction assumptions and then it doesn't converge to a unique solution because there's more than one global solution. You have two eternal solutions. The lambda plus and lambda minus are two eternal solutions, but they have the same growth species. So is this somehow connected to the Is this connected to this branching or this this branching of uh integer physics somehow these the solution is is decided? Yes, yeah. So uh the the picture is a little bit different here. Uh is that so instead of GSS we have we have infinite volume, infinite path measures, right? So yeah, if you start Yeah, if you start from some point here, you measure on paths that are supported on the paths that have a certain direction, right? And whenever you're in this set of discontinuities, or whenever you're not in a set of discontinuities, there's exactly one infinite path match that's consistent. Now, when you are in this set, you have two. At least, so they're not, but you could couple them together, but they're two infinite path measures that are consistent with respect. Path measures that are consistent with respect to the point-to-point measures. That's correct, they are monotone, yes. So if you have this picture with the moving round motions, and you draw them all lambda, then you would not fill out the whole R2, I guess. It would be like the whole holes are connected to the jumps of the lambda parts and lambda minus. Yeah, so what you would see, let's see. Yeah, this picture here, yeah, you would have Have so it's somewhat hard to draw because these jumps are dense. But yeah, what you will see is you will see you will see a lambda plus directory here and a lambda minus directory here. Nothing any cleaner. One thing that is known from the work of Chris Spiros and Timo is that actually these directories they split apart immediately. They don't share any time together. Did that answer the question? Well, little bits, yeah. Well, a little bit, yeah. That also explains somehow why it has to be dense, right? Because you expect these holes to be more or less everywhere. Sure, right, right, right. So once you know there's like a positive problem, right. Does it also prove that there are only two? No, we don't know there's only two. That's not the problem. Here's certain and we don't know how to make it. But if there were three, they would still have to be promoted in some way. So yeah, anything else would have to lie between the planet. Anything else would have to lie between the lambda plus and the lambda minus? Because you can show that you just take some sequence of directions. This is right continuously, so the function for some other directional we converging here and another one from the left. So if there was some other infinite volume path measure, or what it moves one function, it would have to log between the two. Of this random set, by the capital lambda, at the level of probabilistic, like zero local time. Yeah, this is somewhat tricky. So in the stationary horizon, we do have some description of it because, so in both cases, they're dense. But actually, the structure here is a little bit different. If you were to look at a Boozman function, If you were to look at a Boozman function for a fixed x, for example, let's look at b lambda s. I'll just call s times s 0, 0, 0, 0. Let's just fix x and vary the lambda. As a process limit, it's a non-decreasing process, but the jumps are dense. So I don't know how to draw something like that, but anyway, this is a bad thing. The jumps here are all. Jumps here are all so in the stationary horizon, it looks much nicer. It is a jump process where it does this. Now, actually, even there, it's hard to describe where the directions. We can describe more things like the spatial locations where this thing increases. But in this case, it's not clear to us what the topology is for the set of directions. It's a dead set, describing its distribution is somewhat Describing the distribution is somewhat uh clear how we do that. It's it's it's an interesting question. There are no more questions, that's time for the next question. Yeah, so much going on. 