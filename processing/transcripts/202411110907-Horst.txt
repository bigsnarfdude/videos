No. Yeah, there you go. Ah, good. Okay, well, uh, thanks, Antonis, for the kind introduction. It wasn't clear to me that I'm part of the inspiration of the script show. So I'm pleased to take some credit for it. Thank you very much, everybody, for coming. I organized several BERS events in the past, the last one 10 years ago, and back then I think it was a lot easier to get into BERS. I think it was a lot easier to get into brewers. You understood that they received funding from three governments and not just from one. You were basically already in. So I think this time it's a lot more competitive. And thanks to Antonis, Chris, Wuenbeng and Emma for putting this wonderful workshop together. It's always a pleasure being back in Baltimore. So what I'm presenting today is joint work with Ying Chen and Hai Tran on optimal trade execution. Optimal trade execution under endogenous order flow. Jing is a professor at the NUS in Singapore. Hai was her part-time PhD student. And during his PhD, he continued working for the Singapore-based hedge fund. And he came up with this idea of incorporating expectations, feedback, and order flow into optimal liquidation models. And so, whatever we do is to some extent at least backed up by. Extent at least backed up by practitioners here. And so I brought a few extra slides in case I'm too fast on mean field games, extensions of this model, and I would be joint work with Kwanji Fu and Xiao Xia. Okay, so let's get started. I start with a small motivation and then spend most of the time on the single-player liquidation model. It's a very simple model, especially everything is deterministic, and what this allows us is to. And what his analysis is to obtain closed-form solutions for what we are doing. And once you have closed-form solutions, you can take your model to the data. Now, that's not my part. I asked Antonius which paper I should present and said I have one that has some data in it. And he said, yeah, go for that one, which is rare for me to have this empirical part. It was not done by me. Don't ask me anything about the empirical, please. I won't be able to give you any decent answer. So I show you some empirical results, and then, again, time-permitting. And again, time-permitting, we commented a little bit on game theoretic extensions of this model. So, what's the motivation? This is my usual first slide on basically any liquidation talk I'm giving. It says that market impact modeling from an economic perspective is a very old hard. It goes back a long time to Duka, O'Hara, but economists ask very different questions when it comes to trading under market impact. Trading under market impact. They basically ask the question: what is it that you know that I don't know? And then they try to derive, oh, thank you, endogenous market impact functions from economical bonds. Now, this is not exactly what the mathematicians do. So, this idea of block trading, of market impact modeling, gained renewed interest in the mass friends, which are about 20 years ago, and the focus there is very different. The focus is not on informational asymmetries, but the Is not on information and asymmetries, but the focus is on structural models that we can solve. So basically, the story is: if you're a broker and the customer comes to you and says, please unwind this one million shares of Silver Canada, let's say Tim Hortons, the broker is not even allowed to ask why. The broker has to just do it, not ask the why question. And so this is basically the focus in the mass finance literature when it comes to this problem. When it comes to this problem. And then we are mathematicians, so I mean, this is a nice new playground. So, this liquidation constraint, you have to bring your position down to a certain level, introduces all sorts of new mathematical challenges. You end up with PDEs, with singular terminal values, with forward-backward systems where you have two conditions on the forward, but no condition on the backward component. So, you can prove lots of nice mathematical results. So, this paper, from a mathematical perspective, is relatively straightforward, but it introduces. Relatively straightforward, but it introduces a new, let's say, twist to this liquidation literature. What is the twist? So, in the modern market impact, there are different ways of, there are different ways of introducing market impact. Instantaneous impact, which basically means I place an order and my order eats through the limit order book, and then the limit order book completely recovers. And then the limit order book completely recovers from this within a split second. Then there's permanent and transient impact, which means my trading activities, if I'm selling, drive the price down. And if it's transient, then this effect is discounted as time passes. You can put it all together. Again, it's a nice playground for mathematicians. And the additional twist that we would like to add here is the following. So if you place large order. Place large orders or many orders in the market, the market will recognize this. And so there might be an additional feedback effect of your trading on what's going to happen in the future. So typical examples are if you submit many orders, then at some point you don't have counterparties anymore that want to trade with you, or there's some kind of hurting effect that people tend to move into a particular direction because you trade in a particular direction. A particular direction because you trade in a particular direction, or there's a big issue of predatory trading, which has been considered in the economist literature for quite a while. So, then what we do is we'd like to mathematically model this feedback effect. So, our key assumption is this one. We assume that market orders, buy or sell, arrive according to a Hawkes process. And the Hawkes process is basically something like a Poisson process whose arrival intensity depends on how many events have. How many events have occurred in the past. And these Hawk's processes are super popular in the literature of stochastic volatility modeling, especially when it comes to rough volatility. So you want a microstructure model of rough volatility. This idea of having an order creating child orders is very powerful to generate all sorts of stochastic volatility models from a microeconomic perspective. Microeconomic perspective. These horse processes have been used in the liquidation literature before, all byte, usually exogenous. So it's part of a model they are there and then you trade against those. What we do is that we want to endogenize this. So we assume that we have market order flow that reacts to orders that have been placed, including yours. Including yours. If you're the large trader, you place an order the market reacts to it, and other orders may be placed in reaction to what you are doing. And this is basically the whole idea of this rough volatility market microstructure literature, that this is something that happened in real markets. And people have tested this hypothesis. Do orders really arrive according to Hawk's processes? We have done the same. And again, I'm not an empirical person, but my understanding is the answer to this question seems. Questions. So that's what we do in our model. And so we have a Hawks process that describes market order arrival, which is endogenously controlled by the large trade. So we have a stochastic control problem where we control Hawkes processes. That already means we have to put ourselves into a relatively simple setting. And then, as I said, closed-form solutions, and we're going to take those to be data. So, this is the model. The first slide is pretty standard. The idea is we have. Is pretty standard. The idea is we have a trader that needs to unwind a large block of shares within a relatively short period of time. So, as a rule of thumb, I don't know if this is still accurate, but that's what people told me 15 years ago, you trade more than 3% of average daily volume, you have adverse market impact. Good. So we assume that our large trader trades out of his position at some rate xi, so the simplest possible trading strategies that we can admit here. Trading strategies that we can admit here. And the key condition is you have to bring down your position to zero. Now, people always ask why. Why can't you just bring it down to epsilon? My answer to this is it's the law. My understanding is in the US, the law says you have to do the best for your customer. Now, what exactly does that mean? I guess it doesn't mean that you burn some of the shares. And plus, it makes the whole thing mathematically a lot more interesting if you insist on this big mutation concept. You insist on this big mutation constraint. So then, if you have market impact, the market price is not really what you want to get. So, the transaction price is the following. It starts somewhere, then it fluctuates at 42 Browni-Martin gain, which is basically completely irrelevant. It drops out of the consideration at some point. This is the instantaneous impact. So, this eta, that's basically something like inverse order book height, the more liquidity standing at the top of the book. Standing at the top of the book, the smaller this either parameter is, the less liquidity stands there, the more your order, if you submit it to the book, eats into the book. And then there is an additional process on which our focus will be. So this will be an endogenous market impact process that depends on the entire history of trading. And this is generated by our Hawks process. So here's the assumption of market order arrivals. So we assume it follows a Hawks process. So, we assume it follows a Hawk's process with an exponential current. This is super important. People tend to say, well, can't you go beyond exponential? And I will repeatedly comment on this assumption, but one comment up front, you want closed-form solutions. You can't really go much beyond this exponential coordinate. You can do sums of exponential coordinates, but then sums of exponential coordinates approximate power law coordinates very well. So, in that sense, exponential coordinates are perhaps a bit more. Financial kernels are perhaps a bit more general than you might first think. So there is an exogenous arrival rate of orders, which is the same for market buy and sell orders. This is us plus means buy minus means sell. This is the rate at which our large trader acts. This is the process for market buy and market sell orders. And this part here basically measures the past, the impact of past orders. The impact of past orders on the current market order flow. This i here is the difference between the arrival intensity for buy and sell orders. So if we don't have the large trader, this guy here drops out of the equation and we assume for buy and sell we have the same parameter. So there's no plus minus here, there's no plus minus there. So in this sense, the market is in equilibrium. So there's no systematic bias towards upward or downward price drift in the absence of the Price drift in the absence of the trailer. Internal trailer, we can compute the expected number of what we call net sell orders. So this is my sell orders minus my buy orders. And this expression can be explicitly computed, and it has this integral term here, which is the standard integral term for the simplest possible market income models. And there's an additional process here, this quantity here, which is the child order process generated. Order process generated by the trading of the large trade. So the large trader does something, then there is this Hawks feedback effect, and this feedback effect generates this child order process that we now have to take into account when we compute our optimal trading strategies. So, what we do then, there's this assumption that exponential discount rate minus average number of child orders is positive. Is positive, otherwise, this order flow process is not stationary, it's not isymptotically stationary, and we'll verify that in empirically that this is actually always true. We have here a cost parameter lambda, and we suggest this kind of endogenous market impact process, which again is the sum of a term that we've seen before in the literature and a new term here, which results from the feedback effect. Results from the feedback effect. So, if we now look at the revenues from trading, the revenues from trading are mark-to-market, which is this one here, minus instantaneous impact, minus an additional term, which is also standard. So everything in black here, that's the standard component. Now, what our model adds is the blue and the red part. And the blue part is very simple. It increases the fixed transaction cost. And the red part is something like a rebate. Red part is something like a rebate. So the model basically says you pay all your impact up front and then you get a rebate according to this red process. The problem with this is this function here as a function of your trading rate is no longer convex. There's this convex cost term here, and then there is this other convex term here, this rebate term, but they have different sides. So we have convex minus convex, and it's by no means clear that this cost function. means clear that this cost function is convex. So we're going to compute candidate optimal strategies and then we need a non-standard verification argument that tells us that these candidate optimal strategies are indeed optimal strategies. So again, this idea of paying all the impact upfront and then get a rebate as time passes makes our problem a non-convex. So we compute candidates. So, we compute candidate optimal trading strategies by a perturbation argument, and we find that if a strategic xi is optimal, it satisfies this integral equation here. And so it's an integral, here's little t is an integral over the whole trading interval, and there's another little t there. So, that's a Wienerhof integral equation of the second kind. These equations are kind of standard in the liquidation literature. There's a big book, 400 pages, that I 400 pages that does nothing but collect solutions to this kind of equations. And this equation stands somewhere on page 327, I think. So you know where to look, you find the solution. And this PhD stud is extremely good at this kind of things. So we look at the solutions. And then it turns out the solution very strongly depends on one critical parameter. And this one critical parameter is this parameter theta here. So omega was the quantity coming. The quantity coming from the Hawkes process, eta was the instantaneous price impact, and the scammer had as defined over here. And now, depending on the sign of this critical parameter, we either can show that the candidate solution is indeed optimal, or we cannot. And if it's negative, then it basically means that the instantaneous cost dominates. So, if this is negative, it means that basically this. Negative, it means that basically this term here is sufficiently small, so this eta is large enough. Instantaneous impact dominates. If it's positive, basically means this term here is too large, meaning this alpha here is too large, that's a Hawk's parameter. So that means the feedback effect from the Hawk's arrival dynamics is the dominating factor. And depending on which of these cost factors is the dominating one, we get the following result. So if So, if the feedback effect is not too strong, the candidate strategy is of hyperbolic form. So, there are four constants, and you can compute the candidate strategy. Okay, so it's a combination of two hyperbolic functions. If the impact, the feedback impact, is too strong, the candidate strategy is of trigonometric form, like this. And the critical case, which is somewhat pathological, can also be given in close. Can also be given new closed form. So, if you look at the difference here, this is the strong feedback effect, you see it's a sum of sine and cosine. So, what you expect is that at some point the thing starts to fluctuate. And if your optimal trading strategy oscillates, in my opinion, there's something wrong with your model. Either your model is wrong, because it just shouldn't happen, or your parameters are wrong. And in this case, where it's hyperbolic, the strategy is, let's say, very well behaved. I can show you a picture. Say very well behaved. I can show you a picture. So, this is the hyperbolic stuff. And we see the candidate strategy starts low, then increases, you create at an increasing rate, and then decreases again. And this makes sense. You have to account for the impact of future, of what you currently do on future arrivals. This is why at some point you decrease your trading rate, which is clearly shown here. So, we get this hand-shaped thing. We also get hand-shaped trading strategies. Trading strategies for certain parameter values of the strong feedback effect, but then if we increase the feedback effect too much, things start oscillating. And that already suggests that, at least to me, the strong feedback effect is wrong. Because this is not really what we see in the market. And so, in fact, what we can prove is that our cost function is convex, in other words, first-order condition. In other words, first-order conditions are sufficient if and only if no statistical arbitrar, by which we mean a beneficial object, exists. Beneficial reject means you start with zero, you end up with zero, and in between, you make money. Now, if you reverse, you can scale that and generate an arbitrary amount of money in expectations. So, that's what we call statistical arbitrage. And if the cost function is convex, or if this beneficial roundup doesn't exist, then the cost function is convex. If there's no statistical arbitrage, If this no-statistical arbitrage condition holds, then the candidate strategy, of course, is optimal. And the no-statistical arbitrage exists if the feedback effect is not too strong. So in particular, in this case, these oscillating trading strategies cannot occur. And we verify empirically with about 100 stocks that this data is always negative. So the strong feedback effect empirically just doesn't happen according to our analysis. But with Buddhism, your trade-in always remains positive, so you're selling all the time, right? Well, that's a result. A priori, no. A priori, no. But it comes out of the analysis. Yes. So a priori, you have to account for the fact that you may buy or sell, but then the optimal candidate strategy is such that it doesn't change time. So with the oscillation, the worst that can happen is that you just trade slowly and faster, slower and faster. You think that that's still non-pricing? Yeah. Yeah. Yeah. So if the market, the feedback effect is too strong, you can generate arbitrage. Here's a very simple example. You accumulate at a certain rate until halfway through the interval and then liquidate at the same rate. And I don't think you can see it, but this part up here for certain parameter values leads to a positive P and L. So all this here down is negative P and L. So everything is fine, but for certain parameter values, you For certain parameter values, you make money. So, again, then this suggested this data bigger than zero maybe is not what you should observe, and it's indeed not what we do observe in the data we have. So, when did I start on TOLIST? Ten first, maybe? So, I yeah, the verification result, I mean, to get to the verification. I mean, to get to the verification result, you have to have this result that no statistical arbitrage means round, no round flips means in this case convexity and so on and so forth. And to identify these parameters for which it holds, the key is this exponential decay function that comes from our Hawk's kernel can be written in this particular form. At some point, I came up with this formula, but that's very useful because Very useful because blah blah blah, eventually you can express your cost function like this. This is the Laplace transform of our trading strategies, so something positive here, and then this term. And if you can make this term positive, then everything is fine. So then if you choose eta, omega, and the other parameters in the correct way, we really see that this round trip, cost of round trip here, is indeed positive for any round trip. For any round trip, if we choose our parameter values accordingly, I'm saying this because, again, this representation of the exponential kernel is very important for the analysis. It doesn't work if you work with power law currents. This is just a sad fact of life. Okay, so then the implemented is using publicly available data, meaning we didn't use any proprietary data. So we used log. Proprietary data. So we use lobster data. You can buy this from my former colleague Nicholas Haus from Vienna. And we considered roughly 110 stocks, I think, from NASDAQ. And what we found is that all these Hawk's process describing market order were sub-critical. So our assumption that the discount factor is bigger than the child order number is verified empirically. And this impact number, this data, which tells me. I wrote this data, which tells me which form my candidate solution really has, was for all 100 cases always negative. And I would say, except for maybe five stocks, substantially negative. So it never really got close to zero. How did you estimate the parameters of the AUPS process? So that's precisely the question I want to hear because I'm a theory guy. So, they tell me there are standard procedures to do this. What else can I say? Well, the thing is, you cannot observe the intensity very easily. So, you have to filter them. I have to pass on that question. There there is a relatively long description of the statistical method in the paper, which was not written by me, so I Possibly written by me. So maybe you have one more question here. What type of data are you using, like a daily data or more hypocrisy data? And what type of data are you using here? Lobster data. Lobster data from... I mean, the paper's been around for a while, so the data is probably six years old. But it's just been more rigid. We know this lobster data stuff. I know, it's a paper data at all. But when you do that for a single day, it's not a time series of many focus infra data that you Focus intra-data and limit or book for a tiny period of time. So, I'm in no position to discuss estimation issues or data selection problems. It's just not. I don't know anything about this, and I'm ready to admit this. So, I definitely draw the blank on that kind of question. But given that there was a practitioner on board, I I trust that they were they knew what they were doing. They knew what they were doing. So we benchmarked our trading cost against the simple T-Wop schedule. So, T-WOP, what you would do in your resonant, you have prescription one parameter in your model, you trade out the position at a constant rate. And what would be the cost if you do this in our setting where orders arrive according to Bog's processes? And where depending on the type of stock, Stock, we got substantial cost reduction on average of up to 24%. So, here are a couple of numbers. So, we grouped stocks by tick size in basis points. This is one of the parameters from the Hawkes process. Don't know where they got it from, but that's what they found. And this is the really key quantity. So, this impact parameter. So, this impact parameter that we needed to be negative, and we see that they were always negative, and it's still substantially so nowhere close to zero. So that's good for our theory. Transaction costs and cost savings. Cost saving most substantial for stocks with small tick sizes. That may be due to some statistical inaccuracies, maybe, but anyway. But anyway, no matter which group we choose, it was ten percent or more. So the practitioners they tell me consider this substantial cost saving. So that's a count of cost saving. Maybe I quit this. And now two words on model extensions. So again, we heavily rely on having this exponential structure for order arrivals, but if we use If we use sums of exponential kernels, we can still get closed-form solutions. In sums of exponential kernels, Hawk's process with sums of exponential kernels approximates quite well, even if you only have two or three of these exponential kernels, a Hawk's process with power and Hawker. It doesn't mean that the optimal solution is the same, but the market dynamics can be reasonably well approximated with sums of exponentials. And if we have sums of exponentials, we still get close. And if we have some to the exponential, we still get close forms. Number one. Number two, we can introduce risk aversion. Problem is: what's your risk aversion parameter if you take it to the data? But theoretically, it still works. We don't have closed-form solutions, but we get solutions that are involving inverse Laplace transforms, but numerically this is very easy. So to some extent, we can do power law kernels, but we again focus for the empirical implementation. Focused for the empirical implementation on exponential collision. So maybe I skip the liquidation game and come straight to my conclusion. So what's all this about? It was about a portfolio liquidation model with some kind of self-exciting order flow for which we got explicit solution for which we identified a no-arbitrage condition in terms of. And no arbitrage condition in terms of the model parameters. And not surprisingly, of course, if you are in a world where this feedback effect really happens, you use our strategy compared to T Bo, ours is better naturally. And by anywhere between maybe 10 to 15%. A couple of questions are, of course, still open. Can we really start with a general kernel? My answer, I don't think so, because even rewriting this Wiener Hoff integral equation. This Wiener-Hof integral equation, which you can basically obtain for any curve. To rewrite this into something fractable only works for the exponential, for some of it. It's simply, you got stuck at that point. If we approximate the Hawk's process by sums of exponential currents, do the optimal trading strategies still converge? I think this is more a mathematical question. I tried a while and couldn't come up with anything satisfactory. So, if anyone has a good idea on a smart channel, Has a good idea or a smart student that wants to look into this, let me know. Okay, so that concludes my presentation. Thank you very much. And since we have four organizers, I can't. Thank you very much. Yeah, thank you very much, Sudan. That's very interesting talk. So we still have time for like one, two, quick questions. No, nothing empirical principle. Empirical questions. Yeah, thank you, uh Orish, for your talk. Uh it's very nice. I just had a question um regarding the aux process you mentioned because I fully agree that a general kernel would be super hard. I'm not even sure that it's control theory with general kernel for aux process. My question is more like is it I mean what is relevant in terms of kernel in this kind of problem? So the exponential kernel is the most relevant one, so is it necessary to do general kernel? One, so is it necessary to do general kernel? Is there any study about that? So, what's the advantage of study? I mean, you have a general kernel. I mean, is it something? I mean, I know it's a kind of empirical question, but is there some study which says that order flow is a NOx process, but not with an exponential kernel, something more general, or is it true theory? I think that really depends on who you ask. There is a hardcore exponential power law kernel community that said that we have. Community that said that it has to be power law. I disagree. There's an interesting, very recent paper by Sam Cohen that challenges very strongly empirical findings leading to power local components because he says if you account for time varying parameters in your Hawk's process, then basically everything can be described perfectly by exponential quantum. In particular, you are really far away from Really far away from criticality. So, this power law of you need a critical horse process to get this. And he says, no, if you set up your model slightly, slightly different with time dependent parameters. I mean, what people have really shown is how many of these exponential kernels do I need to get superior fit over a single exponential kernel. And if I remember this correctly, the answer is something like two or three. Correctly, the answer is simply two or three. You don't need that maybe, according to this paper. About the final open question about approximating the kernels, does it need to approximate an optimal strategy? Well, that's the open question. One of the open questions on the last slide, I tried to find an easy answer. I couldn't find an easy answer. I don't think it's easy. Because we have no candidates. But we have no candidate for an open solution under power. But maybe in other sites you could be seeing a problem that we started. Yeah. So just to know, as about the last comments about if Roxanne means by sums, or I guess combinations of experiential kernels, do people consider completely monotone kernels? So it's like So it's a completely monotone function, it's a very nice context function. Oh, okay, no, maybe I'll ask you after. Okay, because it's a very nice approximation for some sort of exponential functions versus a class function. I'll ask you after. That's one question after I would suggest that we then rule it again and we move all other questions to the break. So thank you very much. And the second speaker of this morning's session is Stevo Mastroglia, who is at the Department of Industrial Engineering at Berkeley, if I want to correct this. Yeah, we probably not choose it, but 