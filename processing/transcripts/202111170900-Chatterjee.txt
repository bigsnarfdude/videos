So, go ahead. Thank you so much, Marco. Good morning, everyone. Good evening for those who are in a different time zone. My name is Deep. I'm a postdoc at Illinois right now. And I'll be talking about some applications of machine learning for electromagnetic counterpart inference from gravitational wave data. From gravitational wave data. I would once again thank the organizers, especially Marco, Shaon, and others for inviting me to give this talk. I wish I could be there at Oaxaca, but then not this time, maybe for the next time. All right, hope you can hear me loud and clear, and my pointer is also visible. So I know that for the last couple of days, we've been hearing about an overview about observations, but I'll stay. LIGOVER observations, but I'll still give a quick overview. So, as we all know, that as of today, the LIGOVergo collaboration has completed their three observing runs. It all started out in 2015 with the discovery of the first binary black hole system, GW150914. And in the same observing run, 01, there were also two more binary black holes detected. So, in the next observing run, we went Observing run, we went a sharp increase from three detections to 10 binary black hole detections and the one binary neutron star that we all know very well, GW15170817. And then going from 02 to 03, there was another big spike. And as of last week, the third gravitational wave catalog has also been released. And there are 90 events. So that's again a very sharp, sharp, sharp jump. Very sharp, sharp, sharp jump. And we actually knew of several of these discoveries beforehand because 03 was the very first time that this discovery information was relayed publicly to astronomers to enable gravitation wave, to enable follow-up of these gravitational wave candidates. So a total of 56 public alerts had been distributed over the entire course of O3. And just for comparison, the first EM follow-up campaign was set up. Follow-up campaign was set up during O2, and only six alerts were distributed back then. So, once again, it's a big jump and just tells you that one can extrapolate in the future that it would probably be another big jump going ahead in 04 and beyond. So, I would like to spend this one slide on public alerts in 03 because that was something new and also an effort that I was involved with. That I was involved with. So, what this graphic shows over here is a timeline after the discovery of the alert through to dissemination of the alert to observing partners. So, it starts off with the original detection, and this is when real-time search pipelines process the data in real-time, and they find out candidates. And that is usually done within about 30 seconds to about. You know, 30 seconds to about a minute, sometimes a little higher than that. Then there is some automated vetting, some automated data quality checks, which at least decide that, okay, this is not entirely something wrong with the detector, a glitch. There are some initial classification rapid sky localizations that are computed. And then all of that information is packaged together in a machine read-readable format and sent out over the gamma-ray. Out over the Gamma-Ray Coordinate Network GCN as a first preliminary alert. And because there are multiple search pipelines which are parallelly processing the data and they upload several candidates, there is a need for clustering of these events, reannotating, re-evaluating what the preferred data products might be, and all of that is sent out again automatically as a second preliminary alert. And all of this happens between one to about say under five minutes. Say under five for five minutes, and it is around at that point that there are humans in the loop. So, there is human vetting that is done, people experts from detector characterization, data quality, low latency searches, the alert infrastructure, they all get together and decide whether this is indeed a true event. And if it is, then an initial alert is sent confirming its discovery or a retraction. Its discovery, or a retraction is sent, retracting the alert, retracting the event back, or there is no further updates that are sent. But if it is an initial alert, then an update is sent as and when Bayesian parameter estimation finishes and there's an updated data products, classification, sky localizations that are sent out in order days to weeks. So, like this, there were a total of 80 public alerts that were sent in total, only 56. In total, uh, only 56 were non-retracted, so that is not glitches, and uh, and you would find most of them in O3, but not all of them, because the thresholds that were used to send out public alerts versus what was used as a threshold for a catalog, they are different. And so, so, so most of them are out there, but a few marginal events have been left out, but uh, but uh, most automated But most automated alerts were delivered within five to six minutes of the discovery. And so, some more highlights. So, what all of this meant was that at least in O3, there was detection on an average every one to two weeks. And this was mostly because the detectors were much more sensitive. LIGO, for example, the two detectors roughly operated between 120 and 140 megaparsec. 120 and 140 megaparsec. This is the binary neutron star in spiral range, which is the angle-averaged distance out to which the detectors would see a standard 1.4, 1.4 solar mass neutron star. The Virgo detectors operated at about 50 to 60 megaparsecs, and Kagara joined in towards the later half of 03. All of those are supposed to go up further during 04. Go up further during 04. And very recently, there was an announcement saying that 04 is now pushed back to December 2022. But these are the projected detector sensitivities. And all of this would mean that, of course, the detection rate is further supposed to go up potentially every one to two days. And so that is very exciting. But at the same time, there has to be infrastructure put in. know infrastructure put in to handle that kind of that that kind of an alert stream. So a few highlights from GWTC3. Now binaries have been detected to much larger distances than they were before. There are much heavier masses binary black holes like 190521, which has been which I think has been published several months, potentially a year back now. But this is an event which is in the Which is in the intermediate mass, binary, intermediate mass black hole regime. There are heavier binary neutron stars. In fact, the only binary neutron star from GWTC3, which are total mass, which has a total mass of 3.4 solar masses, and that's also much heavier than what we have detected before and is not consistent with the standard galactic population of neutron stars. There are systems with asymmetric mass ratios, and these are important because. And these are important because I think for the first time there was contribution from the higher order modes that were significant for systems like these. Then there were neutron star black hole systems. We heard about them yesterday. And so the bottom line of all of this is that gravitational wave discoveries are now routine and the field has progressed from a discovery phase to an astronomy phase. And this is only going to go up looking into the future observing. Looking into the future observing runs. Unfortunately, the story is not as good on the EMGW front because even after four years of discovering the very first EMGW candidate, GW1717, still remains the only multi-messenger astronomy using gravity, the sole EMGW event. So once again, this was the strain from GW170877. This was the strain from GW170817, which two seconds after the merger, there was a burst of gamma rays that was detected independently by Fermi and Swift. And then after about half a day of scanning the sky, the transient was located in this galaxy called NGC 4993, which was later exhaustively observed and categorized to be this kilonova that was hypothesized decades back as a counterpart to merging neutron stars. So, this was a very nice picture. So this was a very nice picture perfect example of EMGW astronomy, but unfortunately it was the only event still. And therefore, so in this talk, I'll be talking about some work, which is on the real-time inference of counterparts from the gravitational wave data and on the EM side on photometric classification in the alert broker era and the application of ML in both. The application of ML in both cases because all of this is focused towards detecting and finding counter counterparts. Now, of course, I must mention that this is work that I have been involved with, and there are several other efforts from different angles that target the same problem. And so, of course, since we are talking about EM counterparts, the very first question that comes to mind is: okay, which mergers do we follow up? And there was a And there was an excellent intro to this topic and an overview to this topic yesterday. So I'll just do a quick cartoon overview of this. And the first cut answer is that those that at least have a neutron star, because you need the matter dynamics, the relativistic dynamics of any remnant matter post the merger to produce a counterpart. And so for binary neutron star systems, it is expected. Star systems, it is expected that you have two neutron stars colliding at a few percent the speed of light that is expected to leave some remnant matter. But for neutron star black hole systems, as we heard yesterday, it's a little bit more complex. It's a play between the black hole trying to swallow the neutron star and the neutron star getting tidally disrupted by the tidal forces that the black hole exerts on the neutron star. And this is so if you have a very So, if you have a very large black hole, and so if you have a large black hole, then the innermost stable circular orbit is pretty wide. And as the neutron star inspirals, the tidal forces may not be enough to rip the neutron star apart by the time it reaches this innermost stable circular orbit. And therefore, the neutron star may be swallowed whole into the black hole. And there is no remnant matter and therefore no counterparts. However, if the black hole is not as massive and has some spin, which is Massive and has some spin, which is shown by this squiggly line over here, then the ISCO shrinks, and at the same time, the tidal forces on the neutron star, they go up. So therefore, there is chances that the neutron star will be torn apart by the black hole and there will be some remnant matter. And this is sort of exaggerated in this cartoon over here, where if you have a very high-spinning black hole, your chances of having a Black hole, your chances of having a counterpart increases because the chances of ripping the neutron star apart definitely goes up. And so, maybe a better answer to this question is that, well, at least those binaries that leave some remnant matter outside the final black hole, that's a better answer to which mergers to follow up. And that's because the matter dynamics is required. And for neutron star black holes, And for neutron star black holes, this depends on the mass and spin of the black hole. And also, something I did not show in the last slide is the equation of state of the neutron star. And that is because, depending on how stiff or soft the EOS is, stiffer USs would imply larger radii of neutron stars, which are prone to be disrupted more easily. And therefore, the equation of state is an important factor in this inference. In this inference. And of course, the accurate answer to any of this, as we heard yesterday, is given by numerical relativity results. But we cannot do these simulations in real time. In fact, they are very expensive and only, to my knowledge, I think there are about order hundreds of simulations that have been done till date. So they are very computationally expensive. But empirical fits to numerical reactivity results are a good. Results are a good use case for a first-cut inference like this. So, here, for example, I'm showing qualitatively the elements that go into constructing such a fit. So, like I mentioned, that there are essentially two effects. One is the tidal radius, which favors disruption. So, the farther away the tidal radius is, the more prone you are to disrupt a neutron star. To disrupt a neutron star. But then there is this other effect of the length scale of the ISCO, which, if that is too large, then you have a fact, you know, that effect swallows the neutron star. So, you know, those are the two components that can go into such a fit. Of course, this is not the exact fit, but just a qualitative representation of what goes into it. So now, the data products that have been used in O2 and O3, we send out as part of from the collaboration, there are these data products called HAS-NS and HASRENT, which are together called the source properties that are sent out. HAS-NS is a model-independent fundamental inference of having a neutron star. So the very first answer, the very basic. Very first answer, the very basic first cut answer of having a neutron star. And this three solar mass upper limit has been traditionally considered as an upper limit for the neutron star mass in astronomy. And this is not based on any model, but just fundamental constraints based on the speed of light, the speed of sound on the neutron star being lower than the speed of light. The speed of light and constraints like that put an upper limit on the possible neutron star mass and that is three solar mass. Although astrophysically, we haven't seen masses that high, but that's a model-independent fundamental upper limit. And then there is this other data product, which is called Has Remnant, and that you Remnant and that uses additional information of spins and the style of disruption physics, and predicts whether there is any remnant matter that is left outside post-merger. I do want to mention that this is not very model independent because you need this EOS dependence. And so, therefore, you know, you could either have to fix a choice of EOS or marginalize that out. Or modulize that out. So, depending on that, there could be some model dependence that comes into this inference. But so, we need all of this in real time because the kilonova fades very quickly, right? And Bayesian parameter estimation, at least today, and I'm sure that we'll probably hear more on this in the talk by Greg Ashton later. But it still takes of the order hours through potentially days, although there are. Potentially days, although there may be plans of getting faster parameter estimation results out, but at least that is the state today. And so what we have at hand is this data from online searches. So I'll give a quick summary about what they are. So the online searches are basically gravitational wave template-based analysis, which means that you have a bank of templates. That you have a bank of templates and you match the data with this template bank, and there is a maximum likelihood template that comes out. But the sole purpose of the detection pipelines is to maximize detection efficiency at a fixed false alarm. So basically, they are supposed to tell you whether there's a signal in the data. And accurate parameter recovery is less. Parameter recovery is left to Bayesian parameter inference. So, basically, there are certain design choices that are made by search pipelines that enable quick discovery. And while some parameters may be very well estimated, like over here, what I'm showing in this plot is a set of binary neutron star injections. And what I mean by that is each of these blobs corresponds. Blobs correspond to a waveform that was injected in the noise. The search was run, and the search gave back some recovered parameters. And so if we plot the injected chirp mass against the recovered chirp mass, the chirp mass is this quantity which is best estimated from the gravitational wave phase, then we see that there is very good agreement. So the chirp mass is well estimated, but unfortunately, that is not. Well, estimated, but unfortunately, that is not the case with some of the other parameters. So, for example, like this is the same figure as in the last plot, but what I'm plotting here is instead the recovered component masses. And what we find, once again, these are not posterior samples, but these are individual injections. So each point over here corresponds to a point over here. And what we find is that they don't look the same. And in fact, I would like to draw your attention to. I would like to draw your attention to some features, like these dark points over here, which correspond to much larger spin than what they were injected with. And that is because of, again, this design choice that I was talking about in the last slide. So if you have templates that are placed, that are constrained to low spins below two solar mass, but are high spinning above two solar mass, there's a Above two solar mass, there's a potential that you may end up getting these kinds of effects. So, the point is, if one naively trusts this maximum likelihood estimate to do an inference like has NS or has remnant, that'll probably not be the best approach. And so, over there, in addition, there might also be certain biases that exist. Certain biases that exist. So, for example, here I'm plotting the recovered masses versus the injected masses. And one sees that there is this bias towards recovering larger masses for the primary mass and lower than the true value for the secondary mass. And so these kinds of biases may exist and do exist. May exist and do exist and have to be accounted for this kind of a low latency inference. So, what was done back in 02, and this is the work that was mostly led by Shawn Ghosh, and I also was a part of it, but was mostly led by Shawn, which is to use an effective Fisher formalism back in 02, where we would We would create these ambiguity ellipsoids in a three-dimensional subspace, and then consider that as a proxy for our likelihood. So the fraction of this ambiguity ellipsoid, the fraction of this that was on one side of, say, M2 less than three solar mass would constitute towards has an S and the fraction of this And the fraction of this that would satisfy that expression of having a remnant would contribute to that quantity correspondingly. Now, the issue there is that a Fischer approximation does not take into account any such biases. Of course, you basically assume that your likelihood essentially is a Gaussian around this triggered point, which always may not be the best approximation. Also, this computation. Also, this computation did take some time. And so, in 03, we looked at the problem in a different way, and that is via supervised binary classification. And here, so the idea is that because we have these sort of injection campaigns where we know what the true parameters are, we can label them as, say, in this. Label them as say in this case, let's take the has ns case. So we can label binaries as one color or the other, depending on which side of this line they are. And then as we run the search, we know that their recovered parameters may be off from what the true value is, but they will carry the labels. And so if you do have an observation in this space, you could do, and this is what we do: we use the nearest neighbor guard. Do we use the nearest neighbor guard at least in the current implementation to infer what is the chances of it being in one class versus the other? And so this is the idea. So we use fake signals. We label our binaries based on these fake signals and train on the recovered parameters. So this space. And we found that this particular parameter space of the masses, pins, and one detection statistic, these signals. And one detection statistic, the signal signal signal to noise was good enough. And so that now, when you have a new trigger with these observed parameters, you're able to predict which class it belongs to. And that is the basic principle of this binary classification. Okay, so in terms of prediction, so here, let's focus on the left panel over here. So what I'm showing over here is a parameter sweep. So we have put So, we have put in fiducial values into this classifier after being trained, and we are just fixing other parameters like the spins and the signal to noise and evaluating what it looks like, what the predictions look like. And so in case of an ideal search, of course, this color, color bar being bright means it has A has N S, and dark means it does not. So, in the ideal performance of a search, we would expect. Performance of a search, we would expect that there's this line and everything below that should be bright and everything above that would be dark. But of course, in reality, we expect a fuzz and we expect that fuzz to go down as we go up in signal to noise. So here, for example, from five through seven, through nine through 11, we see that that fuzz goes down. And that is what the expected behavior that our classifier has learned. Classifier has learned. Okay. And likewise, in the has remnant case, you have. So, in this case, like I mentioned a few slides back, having a very high spinning black hole would imply that this EM bright boundary increases because there is more chances of tidal disruption. And what we find is that that is something that is picked up by the classifier. So, this is what was used. This is what was used during 03, and here are some of the results that I'm showing. So, here on the top panel, what I have is the super event ID, so the ID of the candidates. I've only picked up those which did have some non-zero has NS quantity. And what I'm plotting, the color bar over here is their corresponding has remnant values. And then on the top panel is what was reported. panel is what was reported by the by the classifier and then in the bottom panel is you can do this in inference once your posterior once your parameter estimation is done and you have posterior samples you could re-evaluate this quantity and what we find is that there is broad consistency between the preliminary estimate and the the updated estimate but there are certain cases cases where Cases where there may be a difference between the initial versus the preliminary, the initial versus the updated results. So for example, in this event, this event was triggered off from a very asymmetric mass ratio template. And so the classifier was able to predict that it would have some chance of having a remnant. But then later, after doing Then, later, after doing a parameter estimation, and this is the results from the current production around the data that's available, one can find that the has remnant has changed. And so I want to mention that our current implementation also uses a specific choice of an equation of state, which is relatively stiff. Relatively stiff, and that for that was a choice made to accommodate, to be conservative towards not missing counterparts. And so work is underway right now to, in fact, relax that assumption and do a weighting based on equations of states. But at least this result is based on the 2H equation state. H equation state. Okay, all right. I also want to mention that there are also new techniques. So we did, so the current implementation uses this nearest neighbor algorithm, but there is work underway by Markov actually and students, Sushant at Missouri ST to implement a different genetic algorithm to learn this EM bright. To learn these EM-bright boundaries in this recovered space. And so, genetic algorithm essentially picks up these mathematical expressions. So, we'll have like a mathematical curve, which is work in progress. And then, as I mentioned, that because now we have model selection results, this was a work that was led by Sean Ghosh. We now have model evidences for the different equations of litter. For the different equations of states in literature against GW17 over 17 data. So, one immediate improvement that one can do is relax that assumption about fixing an equation of state, but rather weight your results based on these different evidences that are present. All right, and because this is a conference about multi-messenger astronomy, I thought that I would. Multi-messenger astronomy, I thought that I would do a quick couple of slides on pre-merger alerts, which are relatively new. There is not any official word from the collaboration that this would be deployed on O4, but this was demonstrated to work in a period after 04. So, the basic idea is that because binary neutron star waveforms are Because binary neutron star waveforms are long, order a couple of minutes, they accumulate signal-to-noise ratio during its evolution. And so thereby, if we do use templates that are cut off at different frequencies, like that mentioned over here, of course, this is not the true picture because the data analysis is done in frequency domain. But if we do use templates with cutoff frequencies, then and if the Then, and if the signal has accumulated enough signal-to-noise, then one could potentially send a discovery notice at this point when the binary is still in spiraling with about a minute or maybe tens of seconds to spare so that one can potentially catch the prompt afterglow coming out of a merging neutron star. And so the idea, while that time is not maybe not too long for Too long for complete scheduling of robotic telescopes, but at least for instruments that can start to buffer the data or high energy instruments like SWIFT, that could be a trigger to start observations or to buffer their data for post-processing. And so this was commissioned on Mount Data post-03. And here are some of the results. Some of the results. So, what I'm showing over here in this plot is a latency budget across different observing runs. And what we see is that there's a steady decrease in the latency. So these blue bars are time to detection from merger time. And then these orange bars are time to send the alert notice after discovery. And so, what we find is that at least, you know, at least the sincerely. And at least the second half of O3B, we consistently send these notices in a time scale of about five minutes. And especially for this particular work, it was demonstrated that one can send the notices before the binary has merged. So there were these five mock events that were sent out, and most of most of them, the notice was dispatched. Dispatched before the binary had merged. Okay, so at this point, I think I want to switch gears a little bit. I want to talk about on the EM side of things. So, in particular, the role of alert brokers. So, what brokers are an interface between surveys and researchers? And the idea is that, say, current And the idea is that, say, current surveys like ZTF or LIGO or future telescopes like LSST would do their initial processing of the data, but would send their bulk data stream down to these entities, which are the brokers, for further data enrichment, filtering, and the different science cases. So, and of course, I took this figure from Gautam Narayan here at Illinois, who is part. Here at Illinois, who is part of the Antares broker, but it can be any broker. And in fact, I think now there has been a selection of certain brokers, I think five or six from the Rubin project that would serve as official LSST brokers and Antares is one of them. But could apply, but the same ideas apply to any other broker system. The idea is that they provide automatic data. The idea is that they provide automatic data enrichment, so annotation, characterization, ranking, these kind of services would be available for either facilities downstream. So something like a robotic telescope or to the individual researcher to schedule further triggers. And they could also have other facilities like maybe an interface to query some data or to run some analysis in a Jupiter type. Analysis in a Jupiter-type environment. But that's a central idea. And all of this is needed, and we heard a lot about this in Michael's talk, is that we are already in the big data era in science overall and going ahead, especially in astronomy. The Rubin metrics are really high and unprecedented. And potentially, there needs to be new. There needs to be new technologies that are needed to process and disseminate data at that scale. And so I thought I would bring spend one more slide on talking about follow-up. So we already know that follow-up did get hard in 03. In O3 compared to O2. And maybe it's safe to say that it will only get harder as more and more as we get a bigger data stream. So I thought I would pick out certain phrases from this nice paper written by Michael last year. So essentially, before going to that, let me spend a few minutes. That let me spend a few minutes on this figure, which shows the luminosity distance against the sky localization of the events that were seen in O3 compared to this 170817. And what immediately stands out is that there is this gap where there was no events detected, which basically says that 170817 was definitely an outlier. And these blobs are inversely proportional to sky localization areas, which also Proportional to sky localization areas, which also says that these events were most of them were farther away and also had pretty large sky localization areas. And so to tackle with that, there needs to be coordinated searches between different facilities. And then half the battle is won in classifying objects found by these surveys. Because whenever you scan a big area of the sky map, you're bound to find objects that will potentially be unrelated, and filtering those out would be a challenge. Those out would be a challenge, and also because you know there needs to be judicious use of these resources because large-time allocations will not be awarded in the future because you cannot spend the same expensive resources like spectroscopy time on every single event that you find. And so, therefore, like a visual of what a scenario might look like in the future. Of a scenario might look like in the future could be something like this: that you have a sky localization, and once your telescope has pointed in the sky, you see several new objects. And I think this is also the case today, but will definitely be all the more in the future. That potentially there is a true kilonova in that part shown by this X symbol, which potentially has a light curve. Symbol, which potentially has a light curve like this, very short time scale. But then there are also several other objects, things that look like a type 1ax supernova, a core collapse supernova, which you have to make a decision using the very initial first couple of epochs of photometry that they are not related or worthy following up. And so that is the And so that is the idea of this photometric classification. I'm just going to play this GIF real quick. And so now what we need, or rather, what would be useful to have is this kind of a real-time photometric classification algorithm. So this is, I'm showing a GIF from a work by one of my colleagues, Daniel Mutakrishna. Daniel Mutakrishna, who scored RAPID, is basically a supernova classic classifier. It's a real-time classifier of different types of supernova. So the idea is that on top, you have a normal type 1e light curve. And on the bottom, you have class probabilities of different transient classes. So before the supernova has exploded, it's in the pre-explosion class. But then as it starts to rise, as the light curve starts to rise, you have all the classes that take off, but eventually the right class. Take off, but eventually the right class takes over and then is able to predict that this is a normal type 1 supernormal. Just gonna get back to presentation mode. And basically, this kind of a classification algorithm would be very useful to have because you will potentially detect several of these objects associated with the same sky localization. Associated with the same sky localization. And so before moving ahead, I want to mention about this effort called PLASTIC, which was a photometric challenge that was hosted on Kaggle in 2018. And the idea was that it was a public challenge to classify different light curves across extragalactic and galactic transients. And it's a big data set that's available. And it's a big data set that's available out there, and a subset of this was used to train RAPID. And now I do want to mention about one specific aspect of using not just the photometric information, but also some contextual information that is available sometimes during discovery. For example, like the redshift. So, if you think about it, different objects have different prior probabilities of. Different prior probabilities of having a redshift. For example, Kilonova, it's like very likely that if you detect a low redshift object, it may be a kilonova, as opposed to something like that is at redshift of, say, greater than 0.1, it's unlikely that it would be a kilonova, right? So, therefore, one can use contextual information like the redshift to guide these photometric algorithms. So, here Algorithms. So, here, for example, what I'm showing on top is the stock rapid algorithm, but fed this 1708.17 type light curve. And what we find is that, well, initially there are 10 classes. So they all start out at about one tenth the probability. And then, well, in this case, you know, it does not do a very good job of classifying the object. But if you did provide, if you did train the classifier using, If you did train the TASPAR using this contextual information like Redshift, then you find that this contextual information provides like a prior to your algorithm and it guides the classifier towards an initial classification score. So this is of interest. Unfortunately, we don't have the redshift in real time. So that's something that's you know that's that's something that that we don't have available of course in in this this case we we knew of the redshift uh of 17.78 only after you know not during this discovery time but but much later down but but we do have several gravitational wave data products that are available like the sky localization and and several uh properties of of the sky localization several properties of the gravitational wave observables in correlation with this kilo In correlation with this kilonovae, may be used as contextual information. So, for example, so this is the result of a simulation campaign of simulating kilonovi and other objects all in the sky and along with sky maps and then analyzing what are the correlation of, say, you know, things that are associated with the gravitational sky map versus not. And one sees that, for example, the angular offset from the mode of the sky map, you generally expect that, you know. Generally, we expect that the Kilonovi will be correlated with the SkyMap, for example. So, this kind of information sets the Kilonovi apart from the other types of objects. And then same thing is true, for example, for the line of sight probability. So, you generally expect that the location would be consistent with the high probability region of the sky map. So, if you take the pixel probability associated with the sky, Pixel probability associated, then that could be a distinguishing feature. And that's what we see over here. So, in addition to the photo-photometric information, if we supply this contextual information, then that could help the classifier in distinguishing the Kilonova from the rest. So, for that, we need to simulate these scenarios like this, and which is. Scenarios like this, and which is what we do. So, what we do for this project is we simulate observed light curves of Kilonovi using this software called SNANA, which is the central engine for generating light curves in this work. So, we use the publicly available SET models for Kilonova. And here are a couple of examples by Matthia Bulla and Dan Kaysen. And then our And then our recovery is based on the cadence and the observing strategy of ZTF DR3. So in this case, we have to make a choice of an observing scenario. Of course, it is true that the public ZTF-DR3, that cadence is not the same as what ZTF does, for example, during a target of opportunity setup. But I will show a few results that this is mostly robust again. Mostly robust against an aggressive cadence. But what this does is that what we end up getting at the end of this simulation campaign is that so we have simulated binaries, we have obtained their sky localizations. Now we take the light curve proper, or rather, we take the intrinsic properties of the gravitational wave, like the masses and spins, we convert them into ejector properties. And this is again something that Michael was talking about yesterday. Something that Michael was talking about yesterday. And then we are able to generate light curves based on the model grid given by Bulla or Kaysen. And then once we run it through SNANA, we have taken into account all detection uncertainties and selection effects, like the distance inclination selection effect, the distance inclination correlation in case of gravitational wave data, or say the Milky Way extinction in case of. In case of the light curves themselves. So it is an end-to-end data set that we are able to build. Now, of course, it is not enough to simulate just the kilonova, but we need to simulate the rest of the sky. So that is also what we do. So, for example, these different supernova subtypes, tidal disruption events, all of it generated with the same observing strategy. The same observing strategy. So, at the end, what we have is this end-to-end data set. So, we have the gravitational wave binaries, we generate sky maps, we use the ejector properties, map them to a model grid, generate a light curve using SNANA, plus the GZTF observing strategy. We also generate all other different objects. In this work, we only focused on extragalactic objects. And then we have a full data set. And then we have a full data set containing the sky localization, the Kilonova light curve, plus the other light curve. So, you know, we are able to do the scenario that I talked about a couple of slides back. And so, what do we do with all this? So, now, of course, the time-varying data is in the flux in different passpans, but then there are also these sky map features which we can use as contextual information that would guide a classic classifier to do. That would guide a classic classifier to make a distinction between kilonova versus other objects. So, so we use that to train a temporal convolutional network. And let me quickly show some of the highlights of what our TCTCN is. Now, this mostly looks like a regular convolutional network, except the convolutions go one way, like so, such that the output at any instance depends only on the input at that time and before. Input at that time and before. Now, one can control how much of a big of a correlation one wants by essentially increasing these dilation layers, which make you go further and further back in time. And so at the end, we train on, so we use a cross-entropy loss function, 60-40 split, and what we find is that because there's a temporal network, we expect that as more and more information comes in, the classification accuracy will change. The classification accuracy will change. Now, the important thing I want you to notice is that at least for the kilonovi, what we find that the accuracy is not changing a whole lot. And what this means is because of the usual sparse photometric data, the contextual information itself is oftentimes a good classifier. But for the other objects, for example, type one is For example, a Type 1A supernova that may land bang on in the center of your sky map. While there may be initially a misclassification because of this prior information that it is correlated with the sky and sky map, that it is a kilonova, but subsequent photometric information would correct that and you would get a correct classification as more and more data flows in. So, of course, the You know, the proof is in the pudding. So, here, what we do is we don't use 170870 data for our training data, but we test our predictions on it. And what we find is that, you know, despite different instruments having taken data at very different cadences and very And very, you know, this data set is very heterogeneous. But despite that, we find good classifications. So, this red curve is the Kilonova class. For 17017, what I haven't shown is that, of course, there is a sky map and there is a location in the sky. So, we have all the contextual information. But that, in addition with the photometric information, we are able to reach to the correct class fairly quickly. And this is irrespective of. And this is irrespective of what instrument we use. And then here are some more results of LCO, Magellan, and HST. HST does not quite have GRI, but we use proxies of filters that are close to that. And the results are still robust. I do want to mention about this feature that we often find, and this is because most of our training set does not contain very long light curves. Very long light curves. So it turns out that if light curves are really long, then sometimes the classifier towards the later stages is guided towards thinking this is not a kilonova, but that can be removed by data augmentation. And then so there is, so that was 17087, which was the only kilonova that we have seen, but there's also 80-20-19 NPV. So 1908-14 was one of the best localized events, and at least. And at least there was this one specific candidate, 1819 NPV, which was very much consistent with the gravitation wave sky localization, but was later ruled out as a type 1 BC supernova. And what we find is that our classifier is also able to rule that out. Although initially, based on the very first epoch of data and the consistency with the sky map, you get a high score, but then that is ruled out as more photometric information comes in. out as more photometric information comes in. All right, so about some future work. So of course we had plastic, which was a photometric challenge. Right now there is an effort towards doing a plastic version too, essentially, which is called Elastic. But what that would be is essentially a mock data challenge for these broker teams. broker broker teams and uh our grad student here working with with with gotham alice galiano uh has a sample of of of these host galaxies and and correlate correlation of hosts with different transients which which we are using to put in host information into into these mock data challenge alerts and and and send send them out as uh as um you know mock data for for broker teams to test uh test on so that's that's a that's an effort on on That's an effort ongoing. And because there was some discussion about anomaly detection in one of, I think it was day before yesterday during the discussion session, I thought I would just bring up a couple of efforts that are ongoing. There is this work called VR In by Ashley Villar, which is an anomaly detection algorithm based on an autoencoder network. Based on an autoencoder network. But the essential idea is to look at a low-dimensional encoding of your light curve and then put some kind of threshold on some distance in this low-dimensional encoding, this low-dimensional subspace. And that is also a work that is being explored by grad students in our group, Patrick Alya and Konstantin Manvaranchev, for development of an anomaly filter for the centralist broker. Filter for the Centauri's broker. And so, okay, I think I'm mostly out of time. So I'll just leave the conclusion slide over here and take questions. Thank you so much. Thank you. So, we have a couple of questions in the chat. So, Michael, do you want to Want to ask a question? Okay, I can't see the chat. Let me just go ahead and okay, so I'll just read out. Sorry, go ahead, Michael. Yep, so sorry. Sorry, go ahead, Michael. Yeah, so sorry, I have to switch to here when I want to talk. All right, so my chat questions were two. Basically, do you include kilonova imposters, right? I mean, we're running an online search with ZTF for kilonova transients, and it's not the supernovae, it's not the TDEs and such that it's causing us trouble, right? It's the CVs, it's shock breakout supernovae, it's Uber afterglows. I mean, we have a, you know, an entire search for this kind of thing. And so, are you including the kind And so, are you including the kind of transients that are actually problematic for fast transient searches? I mean, I'm not really seeing them here. So, we are right now working on one particular transient type, which is the M2R flares. We are working on that. So, currently, our postdoc, so Constantine here has a M Dwarf player model. We have incorporated it in SNANA. Incorporated it in SNANA. And so that is one fast transient type that we are working on to include in this data set. Some of the others, I am not entirely sure, but if they are included in the entire plastic data set, then it would be very trivial to incorporate in this analysis. But basically, if we do have models. So basically, if we do have models for them, we can easily incorporate it into this analysis. So does that help, Micah? Yeah, I mean, it seems like there's a bunch of work that's required to put in the relevant models for fast transient searches, right? I mean, if that's what's missing, right? I mean, I don't see how, like, how is it, how can you claim detection efficiencies at that level when you don't have? Detection efficiencies at that level when you don't have the fast transient models that are actually limiting the current searches, right? So at least say, well, for example, let's say this transient that had caused a lot of excitement. So we find that the results are pretty good for transients like this. And then at least for the other... I mean, if we just look at that image you just showed, right? Like, you know, once you fit it to a kilonova model, of course, after day three, you know, once you have, once it's not fading faster than a half mag per day or whatever, it's ruled out, right? So I, I, like, I don't know how it's that, you know, like just looking at it, you already, you already know the answer, right? Well, but but it's, it is also true that, and of course, you were. And of course, you were probably more involved with the follow-up of this one object than me, but I think that there was considerable follow-up done for this one object, for example, beyond the third 30. And what we are seeing is that so for sure, there is a need to include more models, some of which is Models, some of which is already underway. But at least the first set of results, what we find is that this is pretty encouraging. And also, you know, given that a network like this is sort of robust to the different heterogeneous cadences that might occur in a TO type scenario. So I would say that, yes, So I would say that, yes, that there is work needed to be done, but then this is also pretty promising, at least in my opinion. I mean, but just looking at this, right, I mean, the vast majority of the objects will be at the edge of the localization volumes for most of the telescopes that you're looking at here. It's obviously not true for HST and Magellan, but for the survey-style telescopes, right? And so that beautiful light curve that you have that, you know, beautifully fits that kiloneva light curve, right? I mean, Beautifully fits that Kilenova-like curve, right? I mean, that's not the scenario that you're trying to solve, right? I mean, that's just not the reality that we live in, right? Well, we don't so we won't be fitting light curves really. So as the data flows in, I don't know what do you mean by that? So we just train on objects as they are detected. Yeah, I understand. Yeah, I understand. But I'm saying that that image that you showed, which had detections out to like seven or eight days, you know, like that's, you know, at the nominal TOO cadence, that's, I mean, at the edge of the localization volume, we'll be lucky if we have two or three of those points with meter class telescopes, right? And so this is, this is, I mean, I understand for 1717, this works fine, right? But that's, you showed the plot at the, you showed the comment plot, right? Like that's, that's just not what's going to be happening looking forward, right? What's going to be happening looking forward, right? Sorry, which one? You showed the plot from 03 where all of our objects are at the edge of our localization volume, right? I mean, you showed a 17.17-like-like curve, but the vast majority of our events just aren't out there. That's the point you made. So shouldn't you be, you know, shouldn't we be much more worried about things that do not have that beautiful sampling that you were just showing there? So, I mean, that's what. So, I mean, that's what I'm saying. So, I think that, let's see. So, a lot of the so that is also one of the points that I'm trying to make that of incorporating these contextual information in our searches. So, even in the absence of photometric information. So, as you see, that the predictions sort of start off with all of them. They start off at, say, non-zero 0.5. 00.5, and that's based on simple correlation with the sky map that we have fed into the neural net. And so, yes, even in the absence of photometric information, I mean, in the absence of photometric data, I don't think even light curve fits would be a very good, would be very informative, right? So, you know, we are, so that's a basic fundamental limit. Fundamental limitation that we have that is in absence of data. But then what sorry to interrupt, but I think we have to continue so we can take this discussion for the discussion session. So any other quick questions here in the audience? If not, we thank again Deep. So and we can go for the we can advance to the next speaker. To the next speaker. So