Told us there is really no such thing as an average person. This is well known, right? So, at the risk of being overly dramatic and irritating people, if you run a clinical trial and you find the therapeutic regimen that works best for the average person, you're really finding the therapeutic regimen that works best for no one, right? If you even imagine a normal distribution of responses to some therapeutic regimen, there's very few people that are really sitting right at the average. There's a lot of people in those wings for which this therapeutic regimen is not the best option for them. Is not the best option for them. There's nothing surprising about this, right? Of course, we can't just throw statistical inference out the window. It's enormously powerful, but it does, like we just finished saying, rely on properties of large populations that can obscure what's going on in the individual. So these high consequence decisions, like those in oncology, got to be based on more than just data analytics. The only way to optimize intervention for an individual is to reframe their treatment as their own personalized trial. So that's kind of what's going to be guiding. So that's kind of what's going to be guiding us here for the next 20 minutes or so. The question, of course, becomes, how do you do this? And this is how I see the next little bit going. So quick, very quick summary from a couple of different kinds of magnetic resonance imaging measurement types, really just like two or three slides on that. Then talk to you a little bit about how we use those images to link to physics and biology-based modeling. And then slowly this notion of a digital twin will pop out, hopefully. And there will be audience participation at the end. So please. Participation at the end, so please pay attention, if nothing else, so that you can chat at the end. Um, all right, is everybody out there um suitably um excited and at the edge of your seat? Yeah, I love it. I'm clearly missing out on good times there. I hope all right, so this is kind of old news right now. This has been around for a little more than a decade. I'm using what's called dynamic contrast-enhanced MRI or diffusion-weight MRI to separate responders from non-responders. So these I'm From non-responders. So these on my images are over a decade old, right? This is pre-therapy. This is a sagittal cross-section of a woman with an invasive ductal carcinoma, actually, a very large eight-centimeter tumor. And the reds indicate red dots, red and yellow pixels indicate areas of high perfusion, high blood flow, high vessel permeability. And then after one cycle of therapy, which for this patient was about one week later, you can see there's been a reduction in the number of red and yellow pixels, and although the tumor size has not changed, as opposed to a non-reduction, Not changed, as opposed to a non-responder here, where there's actually been an increase in the number of red and yellow pixels, an increase, if anything, an increase in perfusion. So, this is a person who's clearly not responding. And indeed, at the end of the day, they had a residual disease, whereas this patient on top actually had a complete pathological response. I should note that the image quality nowadays is much better than this, but these images hold a special place in my heart because I actually acquired them. So this was back in the day when I was actually running studies and there were big levers on the MR. And there were big levers on the MRI machine, and then the water would flow and things would spin. And okay, not really, that's not really what happened. But anyway, so if you do the ROC analysis on this, the area of the curve for this measurement, this DC MRI, gets you at about 0.77 for being able to separate after one cycle of therapy patients who had residual disease and those who had what's called a pathological complete response or a PASCR. For diffusion-weighted MRI, it's a similar setup, but now we're looking at cell density, right? So a diffusion-weighted MRI has been developed. A diffusion-based MRI has been developed over several decades now to look at areas of cell density, right? It tracks where water is flowing, and the idea is that, sorry, where water is diffusing. And the idea is that if you have a lot of cell density, a lot of cell structures, that will cause water to not diffuse as far per unit time. So in cellular dense tissues, you don't see a lot of red, but in response to positive therapy, you'll see a lot more red because now water can diffuse further per unit time in those. Use further per unit time, and those red pixels indicate higher water diffusion, as opposed to the non-responder here, where there's, you know, there's little if any change. And when you do the area in the curve analysis for your ROC stuff here, you get an area under the curve of 0.81. Of course, both these measurements can be put together at the same time. And so if you do that, you get an area of the curve of 0.88 for predicting who's going to respond and who's not after the first cycle of therapy. And this is kind of well known now, and it's used if you're familiar with the iSPY clinical trials that have been going on for. ISPY clinical trials that have been going on for actually right at a decade now. There's ISPY 1, ISPY 2, and now they're just starting up 2.2. So these types of measurements are now used as part of that clinical trial. But it does give us motivation for using these parameters as inputs into a mechanism-based model because on their own, at least statistically, at least retrospectively, they can separate responders from non-responders. So there's some signal there, right? So the whole metaphor, if you get nothing else out of this presentation, which of course is entirely possible. Which, of course, is entirely possible given the speaker. I would encourage you to look up this perspective we wrote whatever this is going on 10 years ago. It talks about using quantitative imaging as the satellites for tuna forecasting. So here's a global map of humidity. It goes into some version of the Navier-Stokes equations to make a weather forecast. We can do the same thing with these maps of perfusion and cell density, go into some kind of tumor model to make. Density go into some kind of tumor model to make a tumor forecast. And so that's kind of our paradigm for doing this. So now we want to tie this to a model. And of course, a common continuum model to start with is this reaction diffusion equation, right? If you let n be the number of tumor cells, the rate of change of n with respect to time is equal to how the tumor cells are diffusing plus how they're proliferating. And we have logistic growth up here. And there's a whole, you know, you could have this model be just exponential growth, which is good for cells and dishes or cells on the hind limb of a mouse. Dishes or cells on the hind limb of a mouse, even at early time points, but they grow up to a carrying capacity. So you put this carrying capacity term in there, and that's fine. And that's good for cells, even in the dish or even on the hind limb of a mouse, but there's no spatial component. So you put in a diffusion term that describes how they're expandingly symmetric or expanding in a radially symmetric manner. But when we first started doing this, we first started taking this model and calibrating it to sequential data and then predicting forward, the tumors we were predicting were always much larger than what we were. Always much larger than what we were seeing. And so we realized, you know, we've got to couple this to the surrounding tissue properties, right? These tumors don't grow out into space. And in hindsight, it's kind of obvious, but it was not obvious, at least to me, when we first started this. So now this D, which is describing how those tumor cells are diffusing out into space, is now exponentially damped by this thing called the von Miesa stress. And so you start with your mechanical equilibrium, which says that the divergence of the Cauchy stress tensor is proportional to the gradient of the. Is proportional to the gradient of the cell number. You use that diffusion MRI data to give us our estimate of cell density. So this is a measurable lambda is gotten by Hooker by Crook, but then it's an ordinary differential equation that you can solve to get sigma. And then you turn it into a scalar through the Cauchy stress tensor. And that's going to exponentially damp the diffusion, the movement of the tumor cells so that they're going to go in the direction of the least stiff tissue. So the other thing we add, of course, is a treatment term. So you got over here. Of course, is a treatment term. So, you got over here the movement of the tumor cells, how they're proliferating, and then here's the treatment term. And this concentration of the drug we're estimating from that contrast-enhanced MRI stuff, the DCE-MRI. So the diffusion weight MRI is going to give us the values for wherever there's an N in these models. The contrast-enhanced MRI data is going to give us an estimate of the initial drug concentration. And then we've got to either fit for or assign in one way or another the diffusion of the tumor cells, how they're. Diffusion of the tumor cells, how they're proliferating, perhaps as a function of space, and then the efficacy of the drug, and then the rate at which the drug's efficacy is decaying. So, those are the parameters that we got to get our hands on by calibrating to sequential data. And so, here's sort of the schematic for it. So, in this case, there's three measurements, and we're going to take the first two. It says a breast tumor, nipple would be here, chest wall would be down here, and the red indicates high cell density, and the yellows and greens lower values. And so, we'll take these two. Values. And so we'll take these two measurements and spatially calibrate it to this model up here. So this is a single patient calibration. And then we predict how the tumor cells are going to change in space and time along here. And then there's going to be a little another little image right here that's going in time. It's showing us how the drug's being delivered here based on that contrast enhanced stuff. So here's the thing. Every time there's a little burst right here, that's when the drug is delivered. It has a corresponding effect on the tumor cellularity. On the tumor cellularity, and let's just take another peek at that again. There's the burst of the drug, it has a corresponding effect on the cell density over time, spatially resolved over time. And the idea is to compare the prediction with what's actually measured. And so when you do that, this is kind of what you get. And it matches up spatially, you know, kind of kind of nicely, right? The blue is the predicted, the red is the observed MR. And if you do the towards correlation coefficient between what you're measuring and change in cell counts from that diffusion MRI versus From that diffusion MRI versus what you're predicting for that model, you get a really good close to the line of unity for this for 139 patients. You do the ROC analysis, this is on 50. I need to update this to the full patient set, but it's there in the curve is 0.89. Okay, so if you've been paying attention, and I am certain that you have, this is just a riveting thing here. You could say, Tom, big guy. At least that's how I imagine you speak to me in your head, Tom, Tom, big guy. To me, in your head, Tom, comma, big guy, comma. So, if there's if it's 0.89 and the other stuff was 0.88, that was a lot of work to just get, you know, a statistically insignificant improvement area of the curve. But there's a really important point here. As before, the other thing was a retrospective statistical separation of groups. And this is a predicting into the future whether or not a patient will respond or not respond on a patient-specific basis. Furthermore, there's a mechanism-based model under the hood that characterizes how the tumor cells are moving. Tumor cells are moving, about the surrounding stresses, how they're proliferating, and how the drug is being delivered. So, there's some key mechanisms in there that's accounting for these changes. So, we think we're getting pretty good at predicting how these tumors are changing, at least in space and time, at least in the neo-adjuvant setting here. And the question is, does this nonsense generalize? And so we try the same formalism to the ISPY data. So, this is data that's acquired at 10 different centers. You know, it had nothing to do with this collection of data. The data that I just finished showing you is from MD Anderson. I just finished showing you was from MD Anderson and their triple negative breast cancer moonshot program. So it was at one center, one subtype of disease, and it was, you know, with 20 PhDs hovering over the scanner and the data every step of the way. This is a, this now, this iSPY is 10 sites, you know, 10 different, potentially 10 different ways of doing QAT on all the different aspects of data collection. The framework for calibration is the same as before. You're going to take the visit one and visit two data, calibrate your model to predict to visit three, and then run. Two vins at three, and then run it out to the end of the end of the intervention. And when you make these plots again of changes in, what is this? This is changes in total tumor cellular, TCC, total tumor cellularity. So changes in cellularity that was observed from those diffusion data versus what is predicted. You still get these really nice CCCs lining up on the line of unity. Similarly, for the change in tumor volume, the TV. And so they're still solid with patients from... And so they're still solid with patients from 10 different centers across many different disease subtypes. These subtypes are here, HER2 positive, hormone receptor positive, HER2 negative, and triple negative. And so at this point, when we started to get this, you know, I was starting to believe that this was really something that was useful because it's one thing to do it at a single site that is, you know, a world-leading cancer center that is a research-oriented cancer center versus something that's acquired from 10 different centers. So we're trying now to. So we were trying now to do this from pre-therapy. Everything I've shown you up to this point requires two data sets, one at baseline and then one after the therapy has started. And so if you want to do it from baseline before therapy starts, there's maybe only a handful of ways to do that. And the most obvious one is to link it to a machine learning technique. And my guess is this slide is probably the one you guys wanted me to talk about most given the topic of the conference. And so I apologize, but there's And so I apologize, but there's some other things I really want to talk about. So we'll see how this goes. All right, so now we're trying to link this mechanism-based model with the neural network to assign model parameters globally. So not a spatially resolved parameter set, just global ones, based on a patient's initial conditions. So the patient comes in, they have their initial DCE and diffusion maps, and there's a couple of other measurements that we make from the imaging. Those are their initial conditions, and we're trying to use those to assign the model parameters. Those to assign the model parameters that were in the model at the top of the slide a few slides ago. So, what we do, of course, is sort of the natural thing. There's a, I don't know, I think there's 100 and some odd patients in this patient set. So, we take them, we take 80% of them, calibrate our model, the one that was on the top of the slide a few slides ago, to all three time points. So, every patient has their set of initial conditions and their calibrated parameters for the training set, the 80% of the data. Then you take Ms. Jane Doe out of the test data set, you have her initial conditions, pass it through the Her initial conditions, pass it through the neural network to get the estimate of the parameters that would work for her, put that into her model, and run it forward in time before any therapy has started. So, this is the idea of trying to do a prediction at baseline. So, here's the schematic. Just like I was saying, you have your mechanism-based model that you're applying to your training data set. They have three data points, so you calibrate the model to that. So, those patients have their initial conditions plus their calibrated models. Then, you go over here to Ms. Jane Doe, you have her initial conditions. Jane Doe, you have her initial conditions. You pass those through the neural network to get an estimate of what the model parameters are. Those go into that mechanism-based model, and then you make a prediction for how her tumor is going to change in space and time. And if you do that, here's the outcome for this. So the upper limit for this particular data set is going to be an area of the curve of 0.89. So that's what was able to separate responders from Path CR, pathological complete response from non-responders using the measured data. So that's, you know, that's, that's. You know, that's the gold standard because that was what measured. If you use the modeling framework that we talked about previously up to this, then you get an area on the curve of 0.86. If you use that predicted from the baseline, that combines the neural network with the mechanism-based model, you're down at 0.72. It's perhaps kind of remarkable that you can do that well from baseline given just the initial conditions. So, one might imagine that you could initialize your digital twin with a population-based. Twin with a population-based set of data to pass through your neural network to make a prediction for Miss Jane Doe from day one to her next imaging time point, which you then update by calibrating just her data, and then you can go patient-specific from there on. That's one approach. We could talk more about this if you'd like to. But I'd like to get to this little bit, and we're coming up on this audience participation time. So I'm hopeful that you're still there. We started at 3.33. Started at 3:33. We're at 3:49, 16 minutes. We only got, you know, maybe 10 or 11. It's practically over, really. So, we're almost there. All right. So, this is a schematic that doctors Wu, Lorenzo, and Hormo Guerimo and David are there in the audience. They're the two especially handsome guys down there. All right, so a patient presents with some sort of physical state, call it S. It's indexed by I, right, because we're going to iterate, right, the whole digital twin ideas that you have to pass back and forth between the. Back and forth between the model and the person ongoing, right? I should probably start by saying, you know, I define or one can define digital twins in three components. One, you want to use a mathematical model to virtually represent a physical object. You want to use that model to predict the behavior of the object. And then you want to enable decision-making to optimize the future behavior of that object in like a bi-directional flow of information from the object to the model and back again. So there's this iterative patient. So there's this iterative patient presents. The blue indicates the things that exist in the real world. The reds will appear, and those are the digital counterparts to the real thing. You have your observational data. We do a lot of imaging. We came up through imaging. I'm told there are other kinds of measurements. I'm not really sure what they are, but we use a lot of imaging. And then the things you can do to the system. So there's the imaging studies, biopsies, you can adjust the treatment. These are all things that exist in the real world indicated by the blue there. Then there's the digital studies. Blue there. Then there's the digital state, your definite element mesh, your boundary conditions, how whatever your parameters are, your treatment regimen. And I want to call your attention to that here. So, right now, what this schematic is showing us here is that the patient gets a bolus of the drug, and then there's this refractory period before they get the next bolus. This is actually supposed to be imitating triple negative breast cancer, where the patient will get adriamycin cytoxin, and then 20 days go by, and they get another hit of it. But everybody knows, like we said at the very beginning. But everybody knows, like we said at the very beginning, that this is not the best way to give the therapy to everybody, right? Perhaps it is once one-third every seven days, or perhaps it should be closer to what we do in radiation treatment, where the whole dose is divided up into N days, where N is the number of days they're going to go into the radiotherapy suite. This actually isn't logistically possible for systemic therapy, but you could imagine something perhaps once a week. And what's the best one for an individual patient? Well, without a mathematical model, you're left with Without a mathematical model, you're left with trial and error. And there's not enough patience and there's not enough money to sort out all the different ways you could potentially give adromycincytoxin to Ms. Jane Doe here. So you have to build a mathematical model. Then there's the quantities of interest, the things that tell you about the distribution of therapies, cell density, et cetera. And here's an example of one dosing regimen and another one. And there's a pretty big difference based on whether or not you give the dose all at once, which corresponds to this guy over here, or a third. Guy over here, or a third of a dose every seven days, which corresponds to that guy over there. And of course, you don't have to just look at those two. You could look at a change in dosing and a cage in schedule and have outcome on the vertical axis here. And this surface you can generate for each individual patient. And then you can find the optimal regimen for the patient based on what their surface looks like. All right. So I want to zoom in on this for a little bit before I say that. So the patient comes in at the physical state, you make your measurement. At the physical state, you make your measurements, you build your digital model, you track the quantities of interest, you try to maximize your rewards, your outcome, your overall survival, your progression-free survival, whatever. And then the whole thing repeats. You're going to do some new set of measurements, perhaps adjust the treatment, and it all cycles back around again. All right, so I want to zoom in on here and look at this plot. All right, people, this is one of those plots that took like 10 years to make. And I'm not being, I'm not being over, I mean, it didn't take 10 years to render this plot. Take 10 years to render this plot. That happened very rapidly. First, we had to build the mathematical model, figure out how to calibrate it to data. What kind of data did we need? What were the predictions, good, bad, or different? What did we need to add to the model? Is it invertible? Is it identifiable? All that nonsense. But what we can do at the end of the day here is this is a surface indicating final tumor burden for one patient. And on this horizontal axis here, it's how long is this patient receiving Taxol? And on this axis, it's how long do they receive adriamyc. It's how long do they receive adriamycin and cytoxin. And then on the vertical axis is the total tumor burden. So this black dot indicates what the patient actually received. This is a real patient, and this was her final tumor burden at the end of the day, as estimated by the diffusion weight MRI. This surface here indicates each vertex here is one combination of taxol duration and AC duration. You can see that it goes down as you move in this direction. And there's this gray plane. I hope you can see. I hope it projects. I hope you can see that there's a gray vertical plane here. Gray vertical plane here because everything that's above that plane, we would predict the patient would have residual disease, and everything below it, we would predict the patient would have a high likelihood of a pathological complete response. So I hope you can appreciate that there's three dots, which are standard of care options, which the patient could have received that lie below that horizontal gray plane there. So, what this is saying is our model predicts that any one of these three standard care measurements would have. Three standard care measurements would have had the patient perform better than what she actually received. So, if I've explained this properly, hopefully, you had some semblance of a shiver go up your spine. So, without changing the total dose, just shortening or lengthening the duration of either one of these drugs, you can increase the treatment efficacy. In this small set of patients, there were 18 who had residual disease, non-PCR. And just by changing the duration of the therapeutic regimen, we can get eight of them to have Path CR, at least. Have PAT CR, at least in principle, using a standard of care dosing regimen that's already on the books for a reasonable. It's a 22% improvement in PAT CR in this cohort. I want to stress at the risk of insulting or irritating, not really insulting, irritating people that you cannot do this with an AI and big data only approach. You can't do that because you already have to have all of these experiments done for different textile duration, a different AC duration for your population so that you can go and look. So, that you can go and look at this. And there's not enough money or patients to run every one of these grid points through all these different types of therapeutic regimens. In fact, it's even worse than that because these grid points are made for this individual patient. They're not made for a population. Everybody's surface looks a little bit different based on what their particular characteristics are. You got to have a mechanism-based model under the hood to do this kind of thing. All right. So, we've applied this to look at retrospective validation and model. To look at retrospective validation and model prediction. So, we try to use our digital twin and our patient data to simulate different therapeutic regimens that patients received in clinical trials that were used to nail those standards of care that were the teal dots on the previous slide. So like, where did these things come from? They come from these clinical trials we're about to talk about. This table is busy. I'm going to encourage you just to look at the bold titles here. This is the name of a particular trial. It was a conventional resume, four cycles of Adrian. Conventional regimen, four cycles of adribycin and cytoxin, then to four cycles of taxol. That's what those things mean. And then there's a dose-dense regimen here, and it's every two weeks instead of three weeks. So this is one clinical two of these grid points, right? It's four cycles of A and C every three weeks versus four cycles of A and C every two weeks. Can you still hear me? Yes. Okay, I just got a warning that's. Okay, I just got a warning that said bad connection. But this is so this is what the digital twin, our digital twin predicted using these therapeutic regimens on our patients that we've worked with, and predicted that this one outperformed this one. And that is aligned with what the trial actually observed, a significantly better outcome for this regimen versus that one. And then there's these two other ones. This had three different arms. These are the ones that we predicted. This is what the trial said weekly and bi-weekly. Taxol provided similar outcomes that were superior to try. To try weekly taxol. And that's what we found as well. I think this one is statistically superior to this one. But that's the same trend that we found and the same order that was found historically. And then for this Southwest Oncology Group study, it had these four different arms and all regiments provided similar outcomes in breast cancer subtypes. There's a non-significant difference, but our trend matched what their trend was in terms of what these numbers went from. Terms of what these numbers went from lower to high, although there's not a significant difference to them. So if you can imagine trying to run all the different clinical trials for this one subtype of breast cancer, it'll take forever. We have to build a mechanism-based model that can describe these things. All right. So using a patient-specific model, you can identify therapeutic regimens that are in theory. Remember, these are predictions superior to what the patient received. I would temper that in theory thing by saying this is the same model that gave really. Saying this is the same model that gave really good predictions when you know the answer. So, predicting response versus not response. What about other disease sites? And we're still inching closer to that true digital twin, and the audience participation bit is rapidly approaching us. All right. So, here's the similar idea for high-grade glioba. These patients, this is work that's led by our graduate student, Hugo, and David Hormuth is intimately involved with Caroline Chung at MD Anderson. So, the patients come in for treatment and imaging. This is a two-phenotype model that David developed for an enhancing reusing a non-enhancing region. And different parts of the tumor have different characteristics, and so you get different model parameters based on where you are spatially within the tumor. So we do this analysis from baseline to week three. We look at cellularity from the MRI at baseline into week three. And you can look at the areas either with your model or just looking at a difference in cell density to get a map of the areas that are high. Density to get a map of the areas that are highly proliferative, and it's these areas right here. So, this thing is the actual standard of care radiation dose map the patient will receive. It's this thing right here. And you can see that this darker region right here corresponds with this main tumor mass right there. So, the idea here is to say, well, we know these particular areas, these green guys right here, are highly proliferative. So, we need to target those. So, let's put an additional dose on top of the standard of care dose map. Standard of care dose map and boost the dose to those regions that we know are highly proliferative. And we're told that treating radiation oncologists will do this, but they need good guidance on where to do the extra dosing. So this is the best dose map we could come up with. And then we put it through their system to find out what the actual dose map is that can be delivered. You know, it's not perfect, but it's quite good. It's about a, they have about a bit control over where the beams are going down to about a millimeter. And so the idea now is. And so the idea now is: how does your standard of care dosing scheme match to your optimized one, which is this guy right here? So, how do the tumors respond when receiving this or when receiving this? And so, this is all in Coleco. This is the response to the standard of care radiation plan over this two-month time period. And this is it for the boost to the highly proliferative area. So, definitely, this region vanished here, which is not surprising, right? Because it got this hit right there. So, that region vanished. There, so that region vanished. Um, some of the other regions got smaller as well. You can see that we let this guy grow, so that guy definitely needed to have a greater dose right there. Um, that corresponds to this region right here. We needed to increase the dose there in hindsight to 2020. And then if you look at it over all slices, this is the total tumor burden for the standard of care radiation plan versus the radiation plan boosted to the highly proliferative areas. And so it's, I don't know. Proliferative areas. And so it's, I don't know what this is, is 8,200 down to 5,000. It's eight-fifths, whatever that fraction is, reduction in tumor burden. Again, in theory, but these are predictions that could be tested. And of course, you can do data simulation to update your model predictions. That was that I index on the little breast cancer digital twin schematic I had a few slides ago. You can use the first two visits to calibrate, make a prediction, have that guide the dosing scheme you're going to do. Patient comes back for a visit to you update, you make a new prediction. Back for visit two, you update, you make a new prediction, and so forth. And that's a straightforward thing to do. So, here's the measurement, and here's the prediction as you go in time. You can see if you're updating the model prediction as you go from visit one, two, three up to visit six, the predictions are matching pretty well. So if you're doing an update every week and you're using that update to guide your radiation plan, you know, there's real hope that you could actually have a real difference in an outcome. So, here's the point to all this. Actually, before we get to that, Actually, before we get to that, this is a trick a friend of mine taught me a long time ago. When you want to tell a story, but you don't want to give the impression that's the only thing the lab is working about. There's a host of other projects that are going on here. We have ongoing projects in prostate that being led by Guillermo Lorenzo, who is there in the audience as well. And Headneck is being led by David Hormuth. We're saying this to new studies in sarcoma and cervical cancer, the cervical cancer is led by a great project. The cervical cancer is led by a great project, a great student, Reshme Patel. And then we have preclinical projects trying to incorporate hypoxia into that radiation treatment model. One of the areas we're really behind in both imaging and modeling, of course, is an immunotherapy. Medical oncology has gotten ahead of us in imaging for immunotherapy. And then, of course, testing these predictions of digital twins in the in vivo, in the preclinical in vivo setting. And then we do a bunch of theoretical work that people tend not to care about. How do you, given a family of models, How do you, given a family of models, how do you select the best one? Are the parameters identifiable? How far into the future can you predict? You know, what is the best way to, when is when do you need to collect data to update your model, those kinds of things going forward. All right, so here's the point. If you want to design something that is useful for an individual patient, we're saying you got to rely on an individual patient's unique characteristics. You know, I no longer think digital twins are really mathematically or scientifically limited. Or scientifically limited now. I think they're ready for prime time. It's now an issue of how do you logistically get them into the clinical setting. They're practical now. You can do the predictions. You can recursively update them. You can recursively make new intervention plans. You can do it on a patient-specific basis. But how do you deploy it in a practical manner? So this is the audience participation time. I'm going to put three bullet points up there, and then I'm just going to say, you know, what do you think? And I'm hopeful that people will say something. That people will say some things. This has to be a collaborative effort. How do you make these practical? How do you get physician and patient buy-in? You know, we've talked in our team about what a digital twin report should look like in the same way that a radiologist generates a report or a pathologist generates a report. What would you put into that summary that's going to communicate it to a physician? How can you explain to a patient that their treatment plans come up by a mathematical model? What do you mean mathematics came up with the way that I'm supposed to be treated? That sounds very, it could sound. To be treated, that sounds very, it could sound scary or distant. How do you logistically make these things happen? How do you interface a digital twin report with an electronic medical record? How do you generate them in a 24-hour time period? We have ideas about that. And then what's the first use cases? We have ideas about that too, but I would love to know what you guys are thinking. This is basically the end of the presentation. So I'm hopeful we can talk about it. Actually, I'll just stop there. Talk about it. Actually, I'll just stop there. I think the last thing is, well, no, I should thank everybody, and then we'll go back. Yes, so the work that I showed today was led by Ernesto and David, Guillermo, and Changua, who's now an assistant professor at MD Anderson. Reshme and KC are both phenomenal graduate students working with our team who will be on the job market in the next six to 12 months. They are phenomenal and it will be sad to see them go, but they will be fleeing the nest shortly. All right, people, what do you think about these things? Other questions? Yes? I'm Alberto Corluke from the University of Oslo. I think it's great work, that's what I think. And I have a couple of questions about the baseline predictions that you make. So you use a sort of two steps procedures. So first you estimate your parameters. I guess you use their Your parameters, I guess you use their point estimates, and then you train the neural network, right? So, but and my question is: so, doing it that way you don't sort of consider the uncertainty that you have in the point estimates of your parameters, which I guess it can be quite large in some cases. So, have you considered any ways of taking into account that uncertainty or propagating that certainty between the two steps? That's the first question. That's the first question. The second is, so it's related. So it's you use only image data to predict your parameters with the deep neural net. But I wonder, I mean, I'm sure you have a lot more data, like 150 subtitles, maybe some molecular data, some clinical data. Have you tried using more data than just your images? I'll try to handle. I'll handle this. I'll try to handle the second question first because, well, it's fresher in my mind than the first one. So, yes, so these imaging data sets were acquired also with, there was genomic data available as well, but there wasn't a lot of it. It is hard to find high-quality imaging data that is available at multiple time points, that has the quantitative contrast-enhanced stuff and the quantitative diffusion, and has in parallel genomic data that is available for each. That is available for each patient, say, at baseline. In principle, it would be straightforward to add it to the neural network, right? It's just another set of rows in your training and your feature vector. So it could definitely be done. After we did the neural network work with the imaging data, it was hard. That was a really hard project. And the woman who worked on it, Casey Stowers, is honestly absolutely brilliant. And when she came to the lab, we had people say, whatever your hardest problem is, you need. People say, whatever your hardest problem is, you need to give it to her. So that's what we did. And it took two, two and a half years to make this work. It was really challenging. And so afterwards, when we said, how do we feel about extending this to include the genetic data? She was like, how about a different graduate student? I want to work on a different problem. It was really, it was just really challenging. In principle, it should be straightforward to put in the new genomic data into your feature vector to train. Into your feature vector to train and practice, it is hard to get your hands on those data sets. Um, period, uh, but you could do it. Uh, the first question was about uncertainty. This, thank you for asking this. This is actually something we care a great deal about. So, our lab has one foot in the cancer imaging world, another foot in sort of the math modeling work. And so, we care an awful lot about the uncertainty and the measurements. We've written a host of papers that have been appreciated by tens of people on the test retest studies for. The test three test studies for different PET measures and different MR measurements. You make a measurement, you take the animal out, you make it again, or you do it on a person. You take the measurement, they get out, they walk around, they come back into the scanner, and what's the error in the measurement? So, we have a really good estimate of the error in the measurement that we're using to calibrate the models with. So, we can definitely make a hurricane plot when we calibrate the model, but we can also have error in the in the, that accounts for both error in the model and error in the data. Error in the model and error in the data sets themselves. So we care about that a great deal. When you were talking about point estimates, so when we're calibrating the model in the training set, we have 80% of our patients. They have three time points. We calibrate the model on them. There is, of course, error in those model parameter calibrations. So when you go to Ms. Jane Doe to take her initial conditions, there's error in her initial conditions. And so you could cycle. In her initial conditions. And so you could cycle through your neural network with various combinations of her initial conditions, given the error you know that you have in that measurement, to come up with a series of predictions from which you could take means and standard deviations on. I feel like that was a long put into the answer. I hope it answered the question. I'm sorry. Hey, Tom, it's Heiko Endeline from the Anderson Cancer Center. I've never heard of you, Heiko Endeline. Sounds really. I've got eight questions, but I only asked two. So the first one is again when you did your beautiful model to forecast from baseline and took the neural network model, you got about an AUC of 0.7, which was amazing. Yeah, 0.72, yeah. Yeah, I'm sorry, I didn't, yeah. 0.72. Did you have a chance to look at Did you have a chance to look at which patients you didn't get? Can you narrow it down to the specific group of patients where this is actually higher and then use it for them inside of making it better for others? I'm going to repeat your question because I'm not sure if I got it. You said, did you try it in different, I think you said, did you try it in different patient subgroups to see which one was better or worse? Is that what you said? Right. Your patient group is quite heterogeneous. Did you say indigenous model has higher accuracy than others? Well, in this model, the patient subgroup is actually. The patient subgroup is actually pretty uniform in the sense that it's all triple-negative breast cancer. There are different flavors, you know, there's currently, I think, six known subtypes of triple-negative breast cancer. So we didn't partition it into those different subtypes of triple-negative breast cancer. If we did, we wouldn't have enough patients to train the neural network on, which allows me to say this is one of the fundamental limitations I'm always screaming about in AI-based methods. You got to have an enormous training data set, and that is super complicated. I said, and that is super complicated, super difficult in oncology, right? Because it's not Miss Jane Doe just doesn't get cancer or breast cancer or even triple negative breast cancer. As you just point out, there's subtypes of triple negative breast cancer. So if I want to use a neural network or any kind of AI methodology, I got to have a population of patients that are that subtype of cancer that matches Ms. Jane Doe's characteristics and then also includes all the possible therapeutic regimens that she could receive. And that data set doesn't exist. It is unlikely, extraordinarily unlikely. It is unlikely, extraordinarily unlikely to ever exist because the therapies are getting more and more specific and the diagnosis are getting more and more specific. So, I mean, I've written and screamed at this at anybody who will listen. I really think the neural network way of attacking how you're going to treat a patient is just fundamentally limited. More data is not going to solve the problem. I didn't quite mean to retrain the network for a bit more, just to, for example, to look at demographics like age or smoking history, what have you, if there's so many groups, right? There are certain models, right? You don't really train the model, just the model that you've trained probably has some underrepresented demographics in it. Those are probably the ones you miss. But if you know what you're leading in your training set, do you have higher confidence in predicting those? That's a reasonable thing. Yeah, there's, I mean, it depends on how many patients are going to be after we look at the different. If you pick age, you know, if you go age 16 above versus age 60 and below, how many patients are going to be in? And below, how many patients are going to be in those things? I don't know the answer to that, but we could definitely take a peek at it. It should be easy to do. Thanks, Tom, for a great talk. My name is Pat Bram from the University of Melbourne. I'll actually try to answer one of your questions that you posed to the audience. And then I'll ask a related question. So, for physician, I would bypass patient BIM. I would try and get physicians involved, and I would. Try and get physicians involved, and I would, in fact, attempt to target young physicians, who many of whom will have research requirements or at least be strongly encouraged to do research in order to get consultant positions. And usually, oncologists have pretty broad latitude on half the techniques that they use. So, I would imagine that a physician, at least in Australia, can do what they like. And in terms of interpreting data, at least that's the current, that seems to be the current standard. The thing that I'm actually curious about is, in terms of like, you know, you'd like. Is in terms of like, you know, you'd love to do this in like a clinical trial or something, but it's really unclear to me ethically how that would work. Because if the idea is that this is more predictive of, it can lead to better treatment outcomes, you know, it's maybe not ethically acceptable to withhold treatment if you have a way of assigning better treatment. So I wonder if this is something that you've thought about and have any. You thought about and have any perspective on? Yeah, it's to your first point, perhaps we should be working in Australia. The physicians that I've been talking to will say, wait, they will say things that are very similar to what I'm about.