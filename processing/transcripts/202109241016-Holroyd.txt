Great pleasure to be joining you. Although I have to say, I'm sure I'm not alone in this. I am insanely jealous of the people who are actually in ban. So I hope you're having a good time. And I'm not sure if deliberately, but your camera's not on. Oh, that's not so good. How about now? Is it okay? Yep. Right. So. Right. So, yeah, so it's late in the week, so I'm going to sort of give a talk where I don't really give any proofs is my plan. And instead, I will sort of try to focus on the big picture of this topic. I mean, this is slightly tricky because actually on this subject, I would say we don't really know what the big picture is. There's this thing and it There's this thing, and we know that it works, and we have proofs, but we don't really understand why it's working. Um, it's a big mystery, but it has something to do with random permutations. So that's a suitable topic. So, right, so suppose we have a stochastic process, as usual, so by which I just mean a family of random variables indexed by something, and normally the Something and normally the index set has some structure, so maybe it's a graph or a metric space or something. And actually, for what I want to talk about, the index set might as well be the integer line, Z. The topic I'm going to talk about is absolutely very, very interesting just on the integer line. And in a way, that's kind of maybe the most interesting case in the sense that once you understand the integer line, you understand. You understand a lot of other cases as well, although not everything. So, what I want to talk about is what you might describe as the simplest and strongest possible mixing condition, and that's something called finite dependence. So what it means is, roughly speaking, you have a fixed distance, and any two variables that are greater than that distance apart are. Than that distance apart are independent from each other, completely independent. Although it's a bit stronger than that, I want it not just for two individual variables but only two sets. So here's the actual definition: if I have two sets of locations and the distance between them, I mean, think of it on Z if you want, two intervals of integers, and the distance between them is greater than K, then the family of random variables on A is independent of the family of random variables on B. Of random variables on b okay, it's a very very strong mixing condition when you get to distance k, you're just independent, and it's called k-dependent. Really, probably it should be called k-independent, but it's not, it's called k-dependent. And a stochastic process is finitely dependent if it is k-dependent for some k. Everyone okay so far? If anything's not just ask. So here's a simple example. Here's a simple example. This is a Markov chain, which happens to be one dependent. It has four states, and they represent the states of two coins. So the process is I have a coin in each hand. And then the step of the process is I exchange the coins between my two hands and then I toss the coin in my left hand. Okay, so maybe you start with heads, heads, and one head moves to the other side. One head moves to the other side, the other coin gets retossed, maybe it turns into a tail. Now I have tail head, etc. So it's just a very simple knockoff chain on four states, nothing complicated. So, however, this is a one-dependent process because if you, as soon as you've done two steps of it, you have retossed both of the coins. So you've completely forgotten where you came from. Forgotten where you came from two steps ago. So that means it's one dependent because it means that the process restricted to the negative integers, for example, is independent of the process restricted to the positive integers. And that's one dependent because one is how many points you miss out in between. And notice. Notice, I mean, it's a very simple condition, but it's subtle and it's easy to put yourself up. So, what does it mean for a process to be Markov, a Markov chain? Well, it means that the past and the future are conditionally independent given the present, given X0. So that's a different condition. Markov says they're conditionally independent given X0, and one dependent says they are just independent. So they're different conditions. So they're different conditions. Neither one implies the other, but this process happens to be both. So if I'm interested in finitely dependent processes, there is a very obvious way to construct some of them, lots of them, and that is as block factors. So what does that mean? It means you start with an IID process. Let's say this process of u variables, and all the randomness comes from that. And the way it comes is that the value of the process X that I'm interested in at a point is some fixed function of the U's within some finite distance of it, a function of a block. So some fixed function of the use on a fixed radius ball. And if you want to just stick to Z, which is fine. Which is fine. We have some IADUs indexed by Z, and the process I'm interested in is given by this. The value at location I is some fixed function of, say, consecutive, an interval of R consecutive use at the same place. In that case, we call it an R block factor. Okay, so here's a simple example. So here's a simple example. Take uniformly random variables on z, uniform on the interval 0, 1, and look at which way the inequalities go between consecutive pairs of them. So let xi be the indicator that ui is bigger than ui plus one. So the indicator that you have a descent location i. Okay, so that gives you kind of an interesting process, actually. A kind of an interesting process, actually. This, this, this process of these Xi's, not that complicated, but kind of an interesting process. And it is a two-block factor of the IID U random variables. Because the so xi depends on two consecutive ui's. And since it's a two-block factor, And since it's a two-block factor, that implies that it's one dependent. The red process is one dependent, and you can see why, right? So if I move, so for instance, this zero is independent of this one here because they depend on disjoint sets of blue variables and more generally, you know, everything to the left of the origin is independent of everything to the right of the origin. Everything to the right of the function. So, sort of obviously, any block factor, so when I say block factor, I always mean a block factor of IID, right? A block factor of an IID process, just like this. So, any block factor is finitely dependent and stationary because we're using the same function everywhere. function everywhere. And if you restrict to z, then you can say an r block factor is going to be r minus one dependent. And stationary the r minus one is just because of some nonsense about how we choose to index things. So you might ask about the converse duplication. So is it true that a finitely dependent A finitely dependent stationary process can always be expressed as a block factor, or are there other examples, other finitely dependent processes that don't arise in this way? And this, are there any stationary finitely dependent processes besides block factors? So, well, okay, I showed you the process with the use, but I showed you before a different one-dependent process, this coin process. Coin process, then that didn't seem like a block factor, it seemed like a Markov chain, but actually, you can express it as a two-block factor because you just have this sequence of independent coin tosses, and it's basically you just look at pairs of consecutive coins. That's the same thing. So, it is a two-plot vector. So, that's not an example. So, this question has kind of an interesting history. It goes back at least to To 1960s, a book by Ibra Gimoff and Linik. And for a while, it was just an open question, and several people commented on the fact and introduced the block factor assumption as an extra assumption when they wanted to study finitely dependent processes because it actually makes things easier and that they didn't know whether actually. You know, they didn't know whether actually is an extra assumption because we don't know about the equivalent. But okay, it turns out the answer is they're not equivalent. So first proved, well, at least the second thing, the second statement here was first proved to be not equivalent by this team in 1989. They gave a counter example, and then there's sort of a long history of people. Of people trying to find slightly better counterexamples. And this paper in this paper in 93, Burton Goulemeese proved the stronger thing that the top line on my slide here is not equivalent. So they gave a one-dependent process that's not a block factor for any block length. And there's this sort of whole series of papers where people tried to find counter examples, but somehow all. Somehow, all the counterexamples people found had the feeling of counterexamples specially constructed for the purpose, and they're not so easy to construct. So, there's this sort of general feeling that maybe there are no natural counter examples, whatever that means. So, however, recently we've learned some more on that question. So, here's a Um so here's a here's a different question that you might ask yourself seemingly unrelated but actually actually key so suppose we demand that our that our stochastic process satisfies some local constraints, hard local constraints. So, what do I mean by that? Suppose we're thinking about a stochastic process X, and let's imagine This x, and let's imagine that the individual variables take values in a finite set. And if it's a finite set, it might as well be just the integers one up to q. So assume that. And let's suppose I forbid some finite collection of finite length words. So formally, I have some finite set of finite length words like the word. Finite length words like the word one, two, three, one, or something. And they look at the set of all sequences that don't contain any of those words anywhere. Okay, so it's a hard local constraint. You have a sequence of numbers, but there are just certain local patterns that you're not allowed to see. So you could ask if you impose Such a constraint? Well, how does it interact with these probabilistic properties that we were looking at? So it's called a shift of finite type. If you take the sequence, the set of sequences that satisfy local constraints, like this, a set of local constraints, it's a shift of finite type. So fix a shift of finite type. Does there exist a block factor process that? Block factor process that almost surely satisfies the constraints like the nest, or does there exist a stationary finitely dependent process? Interesting questions, and they have applications or at least motivations in distributed computing, which I should mention. So, and I want to discard two trivial cases. So, if my shift of finite type is such that Is such that there is some constant sequence, like all ones or something, that is allowed that is not forbidden by the constraints, then it's not very interesting because it's just too easy to satisfy those constraints. And another thing that I want to rule out sort of in the other direction, if, for example, my system of constraints is this, I have ones and twos, but I'm not allowed to ever see one, one or two, two. Ever see 1, 1 or 2, 2, then there are only two possible sequences, right? It's alternating 1, 2, 1, 2, 1, 2, or the other way around, 2, 1, 2, 1, 2, 1. And so that's a kind of periodicity, and it's just too strong a constraint. It's too rigid. So in particular, the only stationary process that lies in this set is the one that's this with probability half, or this with probability half. And for example, Probability a half, and for example, that's not finitely dependent at all. Because if you know the symbol at the origin, then the symbol at location a thousand is the same. So it has very, very long range correlations. So somehow that's not interesting as well. And there's a general condition that rules out things like that. So I'm not interested in those cases. So actually, there's just a canonical choice. If I rule out cases like that, there is a There is a canonical choice of constraint system or shift of finite type, and that is proper colouring. And it's canonical in two senses. One, it's sort of the most natural and parsimonious example once you rule out these trivialities. And two, it turns out, in a sense, it's enough. In a sense, it's enough to study this. Once you know the answers for this, you can reduce the answers for the others. I will come back to that. So, what does it mean? You have a number of colors, at least three, and the constraint is just adjacent locations on Z have to have distinct colours. You have a proper colouring of the integers with Q colors. Q colours, and we call those Q colours. Here's an example of the three colours. So, yeah, so now I can ask: does there exist a block factor colouring, or does there exist a stationary finitely dependent colouring? And when I say that, I mean almost surely, right? So I mean, does there exist a block factor stochastic process so that almost. So, that almost surely it is a proper colouring here if the integers, the sequence of venom variables, is a proper colouring. So, you might try, for example, we want a random colouring of the integers, stationary random colouring. So, one natural choice is what you might call the uniform q colouring of the integers, which is just the mark. It's just the stationary Markov chain, where whatever colour you are here, the next colour is a uniformly random choice of the other Q minus one colours that are not that one. So just the Markov train with this transition matrix, that's a perfectly good stationary cue colouring of the integers. But it's not finitely dependent because it's a It's a standard Markov chain. The correlations decay exponentially, but exponentially is not good enough, right? So the colour at the origin is correlated with the colour at n. Very weakly, it decays exponentially with n, but it is correlated. So it's not finitely dependent. X0 is not independent of Xn for any n. So it's not finite dependent, and that means it's also not a block factor. So that's so good. So right, so does there exist a block factor colouring? So actually the answer to this is no. So there's no block factor colouring of the integers. This is not so obvious at all. And it's one of these results. Um, and it's one of these results where it's a little bit difficult to know whom to attribute it to, actually. Um, the earliest paper I know that really states this is one from the computer science literature by Noah in 91, but you could argue it goes back to Ramsey's theorem because it's sort of a variation of it. So, anyway, I mean, this is the statement. So, so this top thing follows immediately from this. Very nice and A very nice and non-trivial statement about independent random variables. Suppose you have IID random variables on UI, on any space, any distribution, and you have a function of R arguments, which takes only a finite number Q of values, right? Just a function from R numbers that only takes Q possible values. And let's look at F of the And let's look at f of the random variables u1 up to ur. And then let's shift the interval along by one and look at f of the same function of u2 up to ur plus one. And let's ask whether those two are equal to each other. And the answer is with positive probability, they are equal, no matter what function you pick. It's an interesting and non-trivial fact. And just to give you a You an idea of it really is something non-trivial here. Not only is this probability positive, there's a bound on it. And the bound is a tower function of height r, which is the number of arguments in the function, the length of the block. There's a q at the top, but that's less important. So it's one over a tower function. And this is essentially tight. So there are Functions which achieve this up to the constants, whatever that means. Not that hard to prove if you do it the right way. It's kind of a Ramsey theory type proof. You apply the induction to exactly the right choice of thing. But yeah, I promised I wasn't going to do proof, so I won't prove it. So you can't do colouring with block factors. So. So the other question then is: Can you do colouring with a finitely dependent process? And we're sort of at this point, we're sort of conditioned to suspect that the answer might be no, because we kind of think that block factors and finitely dependent processes are sort of the same thing, except for these very weird cases. Weird counterexamples that people work hard to construct, and people didn't even know whether they existed for a while. So yeah, so a very abbreviated history of this question, at least as I experienced it. So a bunch of us, including Odo Schramm, became interested in it a while ago and he proved the answers no. He proved the answer is no for a Markov process or even a hidden Markov process, which sort of covers many of the things you might try to construct a stationary process, I guess. And also the answer is no for what turns out to be the first non-trivial case. So there's no stationary one-dependent three-colouring. And so Oded somehow converted it into some fancy Hilbert space question, which we asked around a bunch of famous analysts and none of them knew the answer to it. So here's the question. So, well, it turns out the answer is yes. There is a finitely dependent state. Finitely dependent, stationary, finitely dependent colour. And so this is a result by myself and Tom Liggard from a few years ago. So there exists actually, well, the two very next cases beyond the one that's impossible, there exists a one-dependent four-coloring and also a two-dependent three-coloring of the integers. And so it's actually very nice. So it's actually very nice. So, so colouring provides an answer to this old question. So, colouring distinguishes between finitely dependent processes and block factors. You can do a colouring with one and not with the other. And also, turns out, once you know that, you can deduce the same holds for every non-trivial shift of. Every non-trivial shift of finite type, but so these are these constraint systems, and where non-trivial means I throw away those two stupid cases, one and two, that I don't care. So really, there's a big difference between finitely dependent processes and block factors. If you're interested in any hard constraint at all, then you can do it with one and not with the other. And also, when you know this, you can deduce. You know this, you can deduce some such facts more generally as well, like on ZD. Although I won't go into the details, but you can say some such things on ZD. But it all still all comes down to this that, yeah, so Tom Ligget, I'm sure most people know, very sadly passed away last year. Really, very, very sad. He was very active as a mathematician until very recently, as you can see. I recommend this memorial article that a few of us put together, published by the AMS. And in particular, the bit that I wrote in this sort of tries to tell the story behind this film, which I'm not going to. Thibhen, which I'm not going to tell now because I want to talk about something else, but it was a very, very interesting journey. One of the most extraordinary mathematical journeys, I think the most extraordinary mathematical journey that I've experienced. So I recommend that. And so anyway, very sorry that Tom is gone. He was really one of the finest people I ever met. So. um so um nonetheless i'll tell you the i'll tell you the construction behind this theorem so so i'll tell you how to get this process and there's really only one construction so for any of the last page of theorems that i showed you that there's there's only one way we know to get um finitely dependent processes Finitely dependent processes satisfying a constraint such as coloring. So, more precisely, there's one construction plus embellishments and variations. And I mean two different things by that. By embellishments, I mean once you have one of these magical processes, you can kind of massage it. You can construct other processes from it by doing straightforward things to the actual process, treating it as a black box. And there are a few variations, and I And there are a few variations, and I want to talk about those. I want to focus on talking about one of them, actually, ultimately, because that's where the random permutations particularly come in. So I hope I get there in time. But first, let me tell you the basic construction. So it's extraordinarily simple. I can tell you how to do it. So let's take four colors, start with a uniformly random colour. Start with a uniformly random colour. One colour of one, two, three, four. Maybe it's green. I don't know what colour. I don't know whether green is one, two, three, or four, but which one it is. So pick it at random. And now I'm going to insert another colour next to it. Maybe I insert red to the left of it. And now I'm going to insert another colour. And there are three places I can insert now. I could insert in the middle between the two, or I could insert on the left or on the right. The two, or I can insert on the left or on the right, and I'm going to choose randomly. I'll tell you exactly how in a second. And maybe I choose to insert in the middle, and maybe I choose to insert blue. Now I have a colouring of length three. So now maybe the next step, I choose to insert a new colour here, and maybe I choose to insert red. Okay, so what's the rule? The rule is at any step of this process, I have all these places where I have all these places where I could insert, and I will always have a proper colouring so far. So I don't have two adjacent colours the same in what I have currently. And there are all these places I could insert either at one end or any of these places in the middle. And how many possibilities are there? Well, actually, there are always three possible choices at the end because this is red. So this could be anything other than red that I, if I. Anything other than red that I, if I insert to its left. But here, for instance, between the red and the blue, two of the colours are taken. So if I want to still have a proper colouring, there are exactly two things that I could put there, green or purple. So it's always going to be like that. There'll be three possible choices at the ends and always two choices in all the middle slots. And what I do is I simply choose uniformly from those possibilities. I choose uniformly. I choose uniformly a slot together with a colour. So it means I'm slightly more likely to insert at the end than in any one of the middle places. And then I insert that colour. Is that clear? So, okay, but I'm not quite done. So if I do this for n steps, that certainly gives me. Certainly gives me a law of a random word of length n. And it's a proper colouring, a proper faux colouring of length n. That's obviously true. What's less true, well less obviously true, but is true, is that these laws, the law of x1 up to xn, they are consistent over different n. And they give you a stationary process on z. Process on Z by Kolmogorov consistency. It's not obvious that they're consistent from this description I gave you, but it's true. So why on earth would this be a finitely dependent process? That's not obvious at all. But so what is true is actually for any number of colours, that was four colours, but you could do this for any number of colours, it gives you a stationary. Colors it gives you a stationary Q coloring of Z by Kohlog of consistency, but then what turns out to be true? I mean, this is so, so strange. If you do it with four colours, the resulting colouring is one dependent. Also, if you do it with three colours, it's two dependent. And just when you think there might be a pattern here, if you do it with any other Q. If you do it with any other Q, it's not finitely dependent. So it just magically works for three and four. Very, very, very, very strange. And the story of how we came up with this is several years of trying much, much more complicated things that eventually got boiled down to this, by the way. But once you have this, actually, it's not too hard to prove. It's just a complete. Too hard to prove. It's just a computation, but somewhere there's just like a magical cancellation that happens. Again, I won't show you the proof, but and it's somehow based on you end up with a recursion for the cylinder probability, the probability of getting a particular word of length n. And it's basically proportional to the sum of the probabilities that you get by deleting any one element. Only one element from the word. And that's true if what you have is a colouring, otherwise zero. And then so you can sort of see where this might come from, right, with this insertion process that I described. Not to imagine that this formula might hold. And then you just use induction, but some mistake step, there's this magical thing that happens. Not that hard proof, but very mysterious. So request. So, questions: why does this work? We have a proof, but what's really going on? And we don't really know the answer to that. We've accumulated a few more things that we know, and I will tell you some of them, which maybe help to shed a bit of light on it. But we still don't really understand. And another question you might ask is. might ask is, well, is there a direct construction on the whole of Z? Because this construction I gave you was a bit unsatisfactory because I told you, you know, here is the law of colouring of length n, and then you have to check that these laws are consistent and use omega of consistency. So is there some way that I can just hands-on construct the process on the whole of Z? It's a very natural question. Maybe if I have such a thing, it would help with the first question. Okay, so, well, what does it mean to have a construction on Z? So of course, it's not a very precise thing. So here is a precise definition, which maybe is one way to formalize it. So what about a finitary factor? So this is a G. This is a generalization of the notion of a block factor. The definition is: again, suppose I have an IID sequence of U's. I don't know why I said Z D here. It could be just Z. You have an IID sequence of U variables, and all the randomness comes from them again. So the process, my process is a function of the U's, and the function is translation equivariant, which means if you were to shift all the U's one to the left, then U is one to the left, then my resulting x would get shifted one to the left. And moreover, x0, say the value that I spit out at the origin is determined by the use within some random distance that itself depends on the use. So, this function is such that if you want to know x0, then you can find Then you can find out what it is by first of all you examine u0, and then depending on what you see, maybe you examine u1 and u minus 1. Then, depending on what you see, you examine the next two. But at some point, some random point determined by what you've seen, you will know what to write at zero. Okay, that's what it means to be a finitary factor of an IID process. So, in particular, a block factor, which we talked about before, is simply a finitary factor. Simply a finite wave factor with a bounded coding radius. The coding radius is this random distance that you have to look to. And it's kind of like a stopping time, right? I may carry. Is that the same thing as continuous, Andrew? Yes, except you have to be careful where you put the almost surely. So yeah, I should, in fact, I didn't say it. I should have said the coding radius is an almost surely finite random variable. Surely finite random variable. Oh, I see. Okay. Yeah, sorry. Glad you brush it up. So where it says R less than infinity, it should be R less than infinity almost surely. So then it's the same thing as almost surely continuous if you define it properly. Whereas if it's literally continuous, then actually that's the same as it being a block factor, I think. But yeah, it is a continuity condition. But it means, you know, intuitively, it means. But it means, you know, intuitively, it means you get the process from just IID noise, you construct it from IID noise, and there is some way of actually finding out what is the thing that you write at the origin. You don't have to examine everything to find out what you put at the origin. There's some procedure where you learn what it is. Okay, so that's a formal version of what it means to be a construction on Z, on model of Z. Z, right? On model of Z. So turns out the word-dependent for coloring can be expressed as a finite factor of IID. And we can ask about the coding radius, right? As well, we can ask the quantitative question. And unfortunately, it's not a very good bound. The coding radius, the way I constructed it, at least has a power law tail, and it's a bad power law. So in particular, it doesn't have finite mean or anything like that. You know, anything like that. And actually, there's a general result. Again, a complicated providence claimed by Smolodinsky a while ago, but he never wrote down the proof. And now there's a proof by Enon Spinker. Actually, any finitely dependent process is a finite refactor of IOD, but it's sort of an abstract proof. It doesn't really tell you how to construct it and it doesn't give you a good boundary. A good bound on R either. Maybe it's a parallel bound. So, okay. So, okay, so how does all this work? So, it sort of comes down to random permutations. So, if you remember this insertion process that I constructed, I mean, I sort of wanted to do that on the whole of Z. So, there's a random permutation, which is the insertion order. Which is the insertion order. It's the order I inserted all these colours in. Forget about what's written here for the second. Just listen. There was this random order that I inserted the colours, and it's almost a uniformly random permutation, but it's not because I had this biasing to be more likely to add at the end points. So, and I somehow want to take a limit of that picture. It turns out when you take the limit, It turns out when you take the limit, you can take a uniformly random permutation on Z. And what does that mean? It means every integer has an IID random arrival time, say uniform on 0, 1, and that's the order that they arrive. And the reason you can do that is because these endpoint insertions are rather rare. If it was a uniform insertion, then the probability of inserting at the end would be one. Probability of inserting at the end would be one over n. So you'd expect logarithmically many of them for large n, and you have a few more than that because you have the bias to insert at the end points, but they're still rare enough that in the limit you can take it as a uniformly random prototype. Okay, so that's what you do. You take IID random arrival times, and then you construct a graph. When an integer arrives, you connect it with two edges to its current neighbors. To its current neighbors, the closest integer to the left and to the right that arrived earlier than it. And then you get an infinite graph of which this is meant to be a finite piece, looks a bit like this. And then you want to uniformly Q-color that infinite graph. So maybe not totally obvious what that means, but you can make sense of it for Q equals 4. And that gives you the construction on Z as a finite. Um, Z as a finitary factor. Um, for q equals three, it's too rigid because this graph is kind of built up out of triangles. So it turns out there are only three factorial proper colourings of it for q equals three. So the only way to choose a colouring of it is to do it globally. So you don't get a factor of time to affect IED if we do that. Anyway, that's. Anyway, that's what the process looks like. But I want to show you this one final thing, which in a way is the most interesting, especially if you care about random permutations. So more recently with Tom Hutchcroft and R. V. Levy, who was a very good student, who's graduated now. So for Three dependent three colouring or two dependent four colouring or one dependent five colouring and all larger cases, we can do it as a financial factor of IOD with a very nice exponential tack. And it's by an extension of this process that based on Mallow's distributed permutations, loosely speaking. Mutations, loosely speaking. The Malows distribution on permutations of finite length is the probability of a permutation is proportional to a parameter t to the power of the inversion number of the permutation. So you have this table of the number of colors and the k of the k dependence. So the one in the corner you cannot do at all. The two ticks are check marks are these two original cases where we can do it, but We can do it, but we don't have a very satisfactory construction on the whole of Z. And then, all these other cases, we can do it with construction on Z with a very, very nice exponential term. Why? So really, really quickly. So the idea is you replace, I mean, this is this brilliant idea of my former student, Arvi, who had the idea of The idea of looking at the q analog in the sense of Kroman choics, although we know q is already checked, so it's the t analog. Uh, you replace the recursion with this, where you have powers of t. That's the key idea. And it turns out with a particular magic choice of t, depending on your k and q, it works. You have the magic cancellation again. And it turns out the insertion order we want. So, again, you have these endpoint insertions, which we're in. These end point insertions which ruin your measure, so it would be the Mallow's measure, but you have end point insertions which have a different weight attached to them. And the point is before when we had a uniform permutation, there were not many of those endpoint insertions in the limit. They're very rare. But if you start with a Mallows permutation, then they're not so rare because what does the Mallow? Not so rare because what does the Mellows permutation look like for a fixed parameter t but for say large length? It's kind of a permutation of the identity permutation, right? So it's like you almost just insert the colours in order from left to right, except there's some randomness, there's some jiggling around, but it has a strong directionality to it. So that means actually a positive proportion of the time you're going to insert a new element right on the right-hand end. So the End. So the endpoint insertions matter. So it turns out there's another, what we call the bubble-weighted Mallows permutation measure, which is actually close to some things we heard about earlier in the week. You have a parameter to the inversion number times another parameter to the number of endpoint insertions, which is the same thing as the number of records of the inverse permutation. And that's the one we want. And then that, it turns out, you can extend to Z. And here's, okay, so. Z and here's okay. So, here's the final construction: what it ends up looking like. You take k and q k is the dependence that you want, and q is the number of colors, like one of these three, and you let t satisfy this magic equation here. Strange polynomial equation. So, for instance, for these three cases, the values of t are these things at the bottom, and then you let s And then you let s be something else. S is some parameter that's to do with the endpoint assertions. And then you do the following: you take the integers, take a Bernoulli process with parameter s, which is this thing. So colour some of them black, but that probably. Then those integers you colour using the stationary Markov colouring on that set. So you have Q colors, you take the stationary Markov chain that I. Stationary Markov chain that I talked about before, but you just do it on this subset on this sub-sequence. That's what you do, and then I'm going to fill in each of these white intervals that are left. And the way you do it is like if I take this first interval from the red to the green, so it was like that, I choose a geometric random variable. I choose a place that's a geometrically distributed distance from the left-hand. Distributed distance from the left-hand endpoint, geometric with parameter t. If it goes over the end, you just ignore it. And then at that point, you insert a new colour, and it has to be different from the two colours it sees on the two sides. So it has to be different from the red and the green. Otherwise, it's uniform. And then you recurse. You do the same on this little interval between the red and the blue, and this little interval between the red and the green. Going until everything's filled in. And that's. And that's what these are some actual simulations of these things for these three cases: one-dependent five-colouring, two-dependent four-colouring, three-dependent three-colouring. And the graph that I've drawn is somehow the equivalent of this graph that I drew before for the uniform case. It's the colouring comes with a random graph that's associated with it, and a pair of points are joined by. A pair of points are joined by an edge, if they were required to be a different colour when you constructed it. So you can also think of it as you construct the graph and then you uniformly random colouring of the graph. And because they sort of have this nice, fairly local structure, you can do everything with exponential terms. So, I guess I'm done. There are lots of open questions. We believe that maybe not on that much evidence, but on some evidence, we believe there is a unique one-dependent for colouring of Z. There is only one to prove that. Coming down here. Here we don't know for the one-dependent four-coloring or the two-dependent three colouring, so the two original ones. Can you express those as a finite refactor of IID with, let's say, finite mean for the coding radius? Okay, here's a very nice question. This would be another way to construct these things, but I have no idea whether this exists. Is there a Markov chain on On a compact metric space. So a discrete time Markov chain, but think about a continuous space. Think about the space is maybe a sphere or space of rotation as well, something like that, some geometric space. So is there a Markov chain on a compact metric space that with two properties? Number one, Two properties. Number one, it exactly mixes in K steps. So, what I mean by that is: you start somewhere and then you take K steps of the Markov chain. After that, it's completely mixed. It's in its stationary distribution already after K steps. Exactly in its stationary distribution. So, like the coin Markov chain did that in two steps, for example. And number two, at every step, it determines. Deterministically moves by at least epsilon. So it can't, after one step, it can't land within epsilon of where it just was. Is there a market training like that? If there was, it would give you another way of constructing a finitely dependent colour. And maybe you can just find out why these things work by finding the right one. Or maybe that can't exist. Or maybe that can't exist. I simply don't know. And finally, Terry, this is a very interesting question, which someone asked me, and it turns out it just really highlights some of the things that we don't know at all about this subject. So it's a continuum version of some of these questions. So I won't tell you how it's related. I'll just tell you the question. So is there a point process on R which is stationary? Which is stationary and which is finitely dependent. What does that mean? It means there's some distance so that everything that happens over here to the left is independent of everything that happens over here to the right, provided there's a distance C between my two hands. And the gaps between consecutive points of the process, the points on the line, each gap is bounded both above and below by two fixed numbers. Try to fix numbers. So the distance between two consecutive points is between one and two. I would love to know the answer to that. Okay, that's enough. Thank you. Questions one question? One question. So with the Q larger than four, so the one-dependent things to construct, are they also invariant to reflection on that? Yes, they are. They are. And it's not obvious from the way we described it. And it requires a separate proof, but yes, they are. Yeah, we actually, we don't know about. yeah it's actually we don't know a particularly simple proof of that actually it's it's what it was one of the harder things to prove can't you just take a tiling of for the last question on this slide let's take a tiling of the line with the intervals of length two and one sort of randomly random tiling what do you mean by sort of randomly just random perfect perfect random tiling with intervals of length one and two and have the Of length one and two, and have the and the points be the end points of the intervals. Yeah, well, that's that's equivalent. But how do you choose? So, so if it's, I mean, I mean, one obvious choice is sort of the Markov chain, right? The stationary Markov chain where. No, I take a uniform one. I take a uniform one on a large cycle and then take a limit. Okay, but I don't think that's going to be finitely dependent. I think it'll have exponential decay correlations, won't it? Oh, exactly one and two. Yeah. Oh, exactly one. Yeah. Exactly one or two. No, but then the problem is, so you want the distances, the gaps to be exactly one or two. So then the problem is you can't make it, the only way you can make it stationary on R, on the whole of R, is like you sort of give a random little shift, right? It lives on the integers, so you have to give a random shift, but then it's not finitely dependent at all. Because if you see your one-third best integer here, then you. You're one-third best energy here than you are over there as well. Yeah, no, it's a good suggestion, but doesn't work. Yeah, and I've tried lots of things like this as well. Not to say there isn't something else simple. You could choose the, okay, never mind. You could choose the points at a random location in the tile. Yeah, but that you'll still have the same. Okay, then you then you fail the other the other the other property. Okay, never mind. Other property, okay. Never mind, I'll think more about it. Yeah, yeah, please do. I um, actually, I haven't thought about this for a while. I put it aside, but I think it's a very, very, very interesting question. I'm not sure it's necessarily helpful, but like, if you say, what if like the tiles are like uniform between like one and two? Like, instead of just like weight one, like weight one half and like tiles of length. like weight one half on like tiles of length one and like and tiles of length two i know i'm not sure i'm not sure i completely understand but just like uh is is this why is this not working let's say if you have again if you have tiles of lengths exactly one and two is that what you're talking about no just like uniform so you so instead of like choosing like uniformly between tiles of like length one and two you choose like a tile of uniform lengths between one and two i don't know okay but then okay but then how Okay, but then okay, but then how do you choose it? I mean, I mean, for example, if you if you just I mean, the obvious measure is something like a Markov Shane or renewal process, I think, right? Where is it possible to and then that's not going to be finitely dependent, it's going to have exponential decay of correlations, but okay, but it's okay, yeah. I mean, you know, you have to really do something to make it finitely dependent. It's not the sort of thing that just comes. Okay. So that's fine, thanks. Okay, we should, we should, or we could perhaps pick up this discussion again at the end of the session, but we should perhaps move on. So should we thank Ander very much again for a lovely talk? Let's well, we'll start in a couple of minutes. I don't know. Lauren, we can are you going to be sharing slides or? Oh, yes, I will.