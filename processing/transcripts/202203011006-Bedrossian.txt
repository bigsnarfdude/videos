Okay, all right, I get okay, all right, yeah, thanks for the invitation. Um, sorry I'm not there in person, but um, yeah, it's still nice to be part of this conference. Um, so okay, right, so I'm going to talk about some joint work with Alex Bubenthal and Sam Punchen-Smith. Alex is there, but I think Sam is not. But okay, so I guess I don't have to introduce the Navier-Stokes equations. This is just, I guess, this is W. This is just, I guess, this is W. So I'll make not that much reference to the Navier Stocks equations in this talk, but these are the equations I'm talking about. And I'll actually be mostly more or less discussing the two-dimensional case. So as we know, when we send this parameter to zero, we generally see turbulence. Certainly, if you add stochastic forcing, you will see turbulence. It's different in 2D and 3D, but regardless, you will see some version of turbulence. So for example, So, for example, it's marked by chaos both in the evolution of the velocity field and in the motion of the Lagrangian particle trajectories. And really, you observe that these, it gets more chaotic in an unbounded way as you send Reynolds number to infinity. So you get some kind of spontaneous stochasticity. You also get anomalous dissipation of various types depending into 2D or 3D, and some kind of, we think, some kind of small-scale universality. I will only discuss in this talk. Discuss in this talk how to, we don't know how to prove Eulerian chaos yet, but some progress towards proving Eulerian chaos, so proving that the velocity field itself is chaotic. So, in particular, I will talk about how to do it for finite-dimensional approximations. It's really important to understand finite-dimensional approximations can't be turbulent. Turbulence is really intrinsically an infinite-dimensional thing, of course. And so, this is not really a talk on turbulence. It's a talk on random dynamical systems, as you will see. Okay, so how do we talk about chaos? So I'm going to use a measure called the Lyapunov exponent of the top Lyapunov exponent. So if I have a dynamical system on some kind of configuration space, so phi T is the map that takes the solution, takes the initial data to the solution, then I want to understand if the gradient with respect to the initial condition grows exponentially asymptotically or not. So this is what I'm going to call lambda one, the top Lyapunov exponent. And if this is positive, sort of for almost every initial condition in a suitable sense. Every initial condition in a suitable sense, then I'm going to call the system chaotic. It's not the only notion of chaos that's used in literature. This is the notion of chaos that we use for this talk. So, Alex will talk a little bit more in the afternoon, but essentially, it's almost impossible to verify that this thing is positive for any realistic system if you have deterministic forcing. But actually, as it turns out, even if you have random forcing, it was still pretty open of how to prove the positivity of these Lyapunov exponents for systems like the Navier-Stokes equation. So, even if you. Systems like the Neviver-Stokes equations. So, even if you truncate the Never Stokes equations to finite dimensions, it's still not clear. And you use stochastic forcing, it still wasn't clear how to prove chaos for the Navier-Stokes equations. So, that is the result that I'm going to present. So the first, this chunk of the slide is just a really ugly way of writing DeLirk and Navier-Stokes. So, what is it really? So, take Navier-Stokes in a periodic box in vorticity form, take the Fourier transform, and you then have, you know, for every Fourier mode, so every point. You know, for every Fourier mode, so every point in the lattice, you have a complex-valued unknown. That's your now your dynamical system, so to speak. Now I'm going to truncate it into a big square in Fourier space. And then I get a big set of stochastic differential equations. That's what this is. Just what happens if you write down that SDE. So the Laplacian, the dissipation becomes this. The nonlinearity, sort of the part coming from the Euler equations, becomes this, where CJK is. Becomes this, where CJK is some rational function, and it's actually important for the talk that it's a rational function. I've rescaled things to put the square root mu in front of the forcing. This is convenient because in the limit, the energy will sort of remain bound at order one. Remember, there cannot be anomalous dissipation. So, if I had this kind of scaling, I would definitely diverge. The energy would be like one over square root of nu as time and. As time went to infinity and then new went to sound zero. So I've rescaled it so that it's kind of like the nice limit in which you could get something which is non-vanishing and not exploding. So what we prove is that if you make the truncation large enough, there's nothing magical about 396, it's just a numerical number that falls out of a proof that we did not bother to optimize. If the dimension, so essentially we're saying if the dimension is high enough and the forcing is hypoelliptic. And the forcing is hypoelliptic, so that just means you need you need a couple of modes non-zero. You need something with four or six or something. I think it's just four for 2D number stokes. Then you can prove the existence of a layout of exponent. As we'll see, that's actually not new, essentially. But in particular, you can prove that it's strictly positive for all new sufficiently small. And you can get this quantitative estimate, which is not expected to be sharp at all, but at least it's better than the nothing that existed before. And so Before. And so this is something that, of course, this top-you have exponent is negative if nu is large, because then if nu is large, then the dissipation is basically dominating the dynamics. And so this you need to take new small. So you need to be in a regime where the nonlinearity is somehow dominant in order to see chaos. Okay. So just so I'm just going to be referring to Uh, so I'm just going to be referring to this stochastic differential equation. I will take additive noise. Some of the stuff in our talk works for multiplicative, in this talk most works for multiplicative, but I won't deal with it. So, these x k's are just fixed constant vector fields that do not depend on w. And the entire entirety of the deterministic system is encoded in this vector field, which depends a little bit on new because of the viscosity. So, in the setting that I'm studying, finite dimensions and everything like that, there exists a nice stochastic flow of diffeomorphisms that tells you. Flow of diffeomorphisms that tells you how the solution evolves. So it's a dynamical system that's parametrized also by the noise path omega. So it's not a time-homogeneous dynamical system. If you want to shift time, you have to shift the noise path as well. So it's a random dynamical system. Everything is well behaved, so you can define a Markov kernel, which basically just tells you the probability of finding the solution in a particular set. Solution in a particular set, and we get Markov semi-groups. So, this is the one on observables. So, phi is a scalar-valued observable. So, it tells you the expected value at a future time given the initial condition. And this is the evolution of the law. So, it tells you that if the initial value is distributed like mu, then the solution is distributed like pt star mu. Because I'm in finite dimensions, both of these guys solve partial differential equations. guys solve partial differential equations. So they solve these potentially sub-elliptic partial differential equations. So L is called the generator and its adjoint L star is this. And using vector fields as derivatives. So I'm just going to use vector fields that derivatives interchangeably. Okay, so there exists a stationary measure that is actually quite easy to prove, even in infinite dimensions. And one can prove also in infinite dimensions that Can prove also in infinite dimensions that under the conditions that we're stating, this stationary measure is unique. Now, in finite dimensions, where you really solve a PDE, the stationary measure can be realized as a solution of this particular PDE. And it's also, you can prove that it's smooth and strictly positive. In fact, the strict positivity and boundedness can be proved uniformly in new. Okay, so now you have some nice, you have a nice probability measure, which gives you your stationary statistics. Measure which gives you your stationary statistics, and it has very nice tail decay. So it decays basically as Gaussian tails. I mean, actually, you can prove that u is a function of w, sorry, is pointwise bounded by something like this uniformly in mu. This is some work with a graduate former graduate student. So under this tail, under the fact that you have such good control in the tail, you can actually apply what's called the multiplicative regatta theorem to give you. Called the multiplicative regotic theorem to give you the existence of the optimal exponents. So, the full power of this beautiful theorem I'm not putting on the slide, um, one I'm giving is just kind of the two most basic uh predictions, which is that one, you have a top Lyapunov exponent, and it's very important. This number is deterministic and independent of the both the initial condition and the noise path. So, this limit holds for almost every choice of initial condition and noise path, and it's the same. And you also have a sum or total Lyapunov exponent lambda sigma, which is the asymptotic. To sigma, which is the asymptotic expansion and compression of Lebesgue measure, which we actually know in this case, I'll explain the next slide. So, for the case of stochastic Nader-Stokes, in the setting we have, this is actually minus nu times the trace of A, where this A is the descretization of the Laplacian. So, what is the MIT? So, what is cool about the MET? The MIT is a beautiful theorem. And how you should think about it is, or how I think about it, we have added a About it. We have added a forcing. So the gradient of the flow map, derivative of phi, is a matrix J, and it solves this ODE. It's just, so it's really just asking, do solutions of this ODE blow up as time goes to infinity or not, blow up exponentially as time goes to infinity or not. And the MAT, you know, if Wt were actually stationary, so if Wt were just equal to W naught, then you would use basic spectral theory or the joint. Basic spectral theory or the Jordan normal form to say what are the possible exponential growth and decay rates that can be realized from solutions of this ODE. So, what the multiplicative ergodic theorem tells you is that if Wt is an ergodic, statistically stationary process, you can still do that using the multiplicative regotic theorem. It doesn't need to be stationary. So, that's the analog. Okay, so now about estimating the operator of exponents, why is it hard? So, you cannot. Why is it hard? So you cannot use Jordan normal form, of course. And it's hard because the problem with cancellations. And Alex will discuss this more in his talk, essentially you have something called code twisting. And I'll explain it just briefly. Alex will discuss more. Let's think about a simple toy model. So let's take random determinant one matrices, so IIDs, take random matrices from some instribution, just some distribution, multiply them together, and ask whether or not this random matrix multiplication grows exponentially. Grows exponentially. You can think, actually, if you think about the chain rule, this is a pretty good toy model for solutions to a random ODE like this. So you can think of some really obvious examples, some obvious and some less obvious examples, when lambda will be zero, when the output of exponent will be zero. So one, of course, is if there's none of them, of all the matrices are just random rotations, there's no growth at all. If you have this example, then there is growth, but it's very self-exponential. Uh, there is growth, but it's very self-exponential, so you're just kind of shearing the vectors, um, and maybe you get a growth like square root of n or something like this. The most interesting example is: let's, and you should think of p as something like 0.9999 to make it interesting, is you expand the x direction exponentially for a long time, and then you finally draw one of these, one of the other matrices, and then you flip the x and the y axis, and then you compress the y axis exponentially. And asymptotically, Exponential. And asymptotically, no matter what p is, as long as it's not zero or one, this will always produce a cancellation. If you stare at it long enough, you won't see any growth. It'll grow like e to the square root of n kind of formally. You can imagine if you actually have a real dynamical system underlying your problem, it's really hard to keep track of this because no matter how long, no matter how long of a finite time window you look at it, you can never tell anything about the asymptotic, the atomic sponge. So the point is that randomness is going to help us deal with this twist. Going to help us deal with this twisting of the cones. And I'll just talk more about a beautiful theorem at Furstenberg, which tells you that in this particular toy problem, these are essentially the only counter, the only examples where lambda is equal to zero, up to a deterministic change of basis in a certain sense. So having randomness in there gives you some really strong rigidity. And we also have to take care of this as well. Alex will talk more about this. So a fundamental thing that you can see here, and actually. Thing that you can see here, and actually, Mark talked about this projective processes already, is that we really have to keep track of direction. And so then that's why you study this kind of normalized Jacobian process. And in this, when we're in a finite dimensional setting, so the sphere is compact, so it's easy to prove the existence of stationary measures for this process in our setting. And in fact, the solution for this SDE problem, the projective process, if you look at the cover. Projective process: if you look at the coupled WV process, this guy actually solves an SDE on the sphere bundle. And so you can lift these vector fields in this ugly lifting operation, and you get a new SDE, which you can prove has all the same nice properties, has Kalman Grove semi-groups, has generator L-Twiddle, and so forth. Sam and Alex, sorry, this follow-up paper by Sam and I, sorry, I quoted two papers. One paper sort of proved the conditional theorem about a whole class of problems. The conditional theorem about a whole class of problems and put forward a general method for estimating Lyapunov exponents. The second paper that Sam and I wrote was more or less basically to prove one of the assumptions that we need, which is that this projective process has a unique stationary measure. So the point in that second paper was to prove the uniqueness. And so let's just say there is one. So we did prove it for another source, as long as n is bigger than 396. And okay, so if we work into that assumption that we have a stationary measure, what's called mu Twiddle, it also solves a PDE, which is crucial. So it's important. So as you can guess from the construction, this stationary process, or sorry, this stationary measure encodes statistics about asymptotically as time goes to infinity in a sense of which directions are undergoing compression and expansion in the format. So where is the format expanding? So, where is the format expanding and where is it compressing? And so, actually, you can relate it. You can relate mu Twiddle to the Lyapunov exponents. There's a couple of formulas for it. This formula is the simplest. This is supposed to be mu, but of course, it's, well, we have no idea how to use it quantitatively for a system as complicated as Nelford-Soaks. So, we found a new formula, and maybe Alex will say more about the context of this formula. I don't know, actually. But essentially, it relates Fisher information, Fisher information like quantity. So, here. A fission information like quantity. So, here mu twiddle. Maybe I'm being a bit pedantic by using different notation for the density with the spectral beggar measure and the measure itself, mu twiddle. But f is the density of the spectral begg measure of mu, which we assume is where, well, we prove that mu twiddle is smooth in our example. And so, this is the formula that we're really going to use. So, as it turns out, you can relate the top Lyapunov exponent and the totally opponent of exponent to the forcing direct. Exponent to the forcing directions. It's Fisher information in the forcing directions. And you have the small parameter showing up here. For the specifically, so that the Fisher information formula holds for pretty much any SDE that satisfies a few basic assumptions. So specialized to Navier-Stokes with additive forcing that we're talking about here, we get this simple formula. And so in particular, because we know And so in particular, because we know that the compression expansion of the Beck measure is just minus two times trace of A, you just get this constant here. So this is a nice formula because it's just on the stationary measure. It's time infinitesimal. And the left-hand side is positive definite. And so that means you can cut up in configuration space on the sphere bundle and estimate each little piece separately, things like that. So that's really useful. You can't have weird cancellations on the left-hand side. Have weird cancellations on the left-hand side. Now, your first instinct, if you're an analyst like me, is to somehow relate this Fisher information to some kind of Sabo-ev regularity, something like this, to something that you can really get your hands on about F, which I'm indicating that depends on new, F depends on Nu. So that's what the superscript is. But it only contains, it only works, depends on derivatives in the direction of the forcing acts. So that's where the concept of hyperliticity comes in crucially. And so the idea. Crucially. And so the idea is: how do you relate regularity in a small subset of directions to regularity in all directions? And this was actually codified by Hormander in 1967. So this is the version that I'm going to be discussing in this talk called the parabolic Hormander condition. So if you set a vector fields on a manifold, because remember, we are on a manifold. So even if you don't like doing PDEs on manifolds, it's too bad. So we are on a manifold. And we form, so what you do is you form these set of vector fields. So, what you do is you deform these set of vector fields. So, the first one, whoops. So, the first one is just the set of the forcing vector fields. And then you, in each successive layer, you take commutators with the previous layer and all the possible vector fields that you have. So this is kind of the deterministic vector field, and these are the forcing vector fields. And if you eventually take enough commutators such that you span the tangent space everywhere, then you're then this is. Then this is, then you're going to say it satisfies Hermann's condition. And Hermann's theorem, or some version of Hermann's theorem, proves that actually, if your vector fields satisfy Hermanner's condition in this sense, then the Markov semigroups are instantaneously regularizing. So P twiddle will map L infinity to continuous functions, for example. So ellipticity or proving, so if the generators were elliptic, if this guy, then that's the same. If this guy, then that's saying that this guy spans the tangent space. That obviously is a special case of this much more general hyperloticity condition. And so it's worth mentioning that no matter what you choose these forcing vector fields to be, they cannot, especially with additive, with additive forcing, I mean, they cannot possibly span the tangent space of the sphere bundle because these forcing vector fields, if they don't depend on the solution, will just be of this form. So, they obviously cannot span the tangent space of the sphere bundle. So, you have to use hyperluticity whether you like it or not. And so, one of the main technical tools in our work is the following actually PDE estimate, which says I have a solution to this projective Kolmogorov equation on the sphere bundle, whatever. That's my stationary measure. And if I have some kind of uniform projective hyperliticity as a parameter, as the parameter nu is varied, then I can prove the following uniform. Then I can prove the following uniform lower bound on the Fisher information. Where I can say that for every bounded set or whatever, for every nice bounded set U, and there exists an S star, which actually doesn't depend on the set U. Sorry, I quantified in the wrong order. There exists an S star such that for every bounded set U, you get this lower bound. And the point is that this depends on new. So F nu depends on new because this generator depends on new. This generator depends on new. This generator looks like new times some second-order terms plus a first-order term that also depends on new. Looks like this. So these are the adjoints. But the point is that the constant C and the regularity S star does not depend on B. So once you have this uniform. So, once you have this uniform sort of quantitative uniform hyperlipticity, hyperlipticity estimate, you can put that together with a Fisher information identity and get a lower bound like this, where this does not depend on new. Okay, so you really have this U fault bound. So the proof of this lemma, and actually it doesn't really have anything to do with dynamical systems directly. It's really an L1 version of kind of hormonal. Kind of Hormander's theorem or a step of Hermander's, the most important step of Hermander's theorem. Let's put it this way. It's an L1 version of the most important step of Hermann's theorem. And it didn't seem to exist in the literature simply because people didn't really need it for the applications that they were looking for. And so we kind of went back to Hermanner's paper, which was written in L2, and derived the L1 version of this. So one of the Hermanner's main insights is to find the right S. Is to find the right estimate. And to find, I mean, especially if you want to be quantitative in epsilon or new or whatever, you know, if you want to be quantitative in a small parameter, the point is you need to find what are the few norms that are maybe a priori bounded. And so here we're sort of actually in the real, in reality, no norm is going to be a priori bounded essentially instead of is except for L1. But we're going to assume that the Fisher information is for some weird reason bounded, which implies some L1 estimates on the gradient. So if you assume that that's bounded, you can then So, if you assume that that's bounded, you can then derive essentially one more estimate, which is this weird dual space estimate of this type. So, this is, you're going to go back to the PDE. This is actually the only place where you use the PDE. We use the fact that you have the solution to this problem. And you multiply by a test function and integrate. And so, you get this kind of weird dual regularity space on the deterministic part of the solution. Deterministic part of the solution: you have this weird dual regularity space that just says you basically are w minus one, infinity in the forcing directions. Sort of weird kind of way. And then those are the only, that's the only, these are the only two things you can assume control on. So if that doesn't give you regularity, you're basically dead because nothing else is going to be uniform and new. And so you have to prove this estimate for some version of this estimate. Estimate. Well, I'm being a bit the only way we know how to do it is through this technique, which is how Hermanner does it. So you basically prove this functional inequality. And here, there's sort of two steps. And a lot of people only remember the first step, which is actually the easy step. I don't want to get too far into it, but essentially, Hermitter, the first step, one step is to prove that if you had positive regularity in every direction, then you would have this bound. So if this Would have this bound. So, if this was somehow a positive regularity space, like some kind of fractional regularity in the x-0 direction, then you could prove this estimate. That is actually the easy step of Hermanner's proof. And that is also where Hermann's condition comes from, the proof of this easy step. The hard step is actually how to get that fractional regularity in the first phase, first place. And that's the step that actually needs to be changed to go from L2 to L1. Go from L2 to L1. Basically, the L2 framework uses the fact that L2 is self-dual. And it's a bit more convenient. So we have to do some technical slog work to adapt that, the second half of Hermann's proof. But it does work, you know, one. And so what we did in our first paper with Sam and Alex is we proved that for a whole class of systems that look Proved that for a whole class of systems that look like Golika-Never-Stokes, so that includes 2D and 3D, Goika-Never-Stokes, and Lorenz 96, and a couple of other models like Sabra, finite-dimensional truncations of Sabra and GOI and things like that. Essentially, what we proved is that it's sufficient now to prove this projective hyperlipticity condition. So it's a conditional theorem that says if you have this projective hyperlipticity, then all then for any of these models, then these models are chaotic in finite dimensions. So you use this to prove, for example, Lorentz 96 in dimensions bigger than or equal to seven, because Lorentz 96 is. Because Land 96 is so simple that we could actually verify this projective hyperledicity by hand. It's not obvious how to do that, but it could be done kind of by playing around with some matrices. And okay, the point is that the previous quantitative estimates were for very specific systems that they got really good estimates for these simple systems, but that's because you had a really good understanding of you. So the method we designed is for systems. The method we designed is for systems which you can't really understand the projective stationary measures. And you probably never will really precisely understand them for something like Never Stokes or even Lorentz 96. Alex might talk more, but essentially you can kind of guess the approach we're going to use from the a priori estimate, which is that the Fisher information, the hyperollipticity gives us this a priori upper bound. That means that if you proceed by contradiction, you say, well, suppose that it's lambda one divided by nu is bounded. Lambda one divided by nu is bounded. Then you have some kind of uniform regularity, and you want to prove that that's somehow wrong. And so the point is that if you have this uniform regularity, then you can pass the limit and construct an absolutely continuous density, invariant density, for this deterministic projective process. So basically deterministic Euler. And then you actually have to use some deterministic dynamical systems. So you can't avoid this completely. So you need to somehow use that for deterministic systems. Use that for deterministic systems, this can't happen if the gradient of the flow map grows at all, more or less, undoubtedly. That's because growth in projective space implies compression. Sorry, growth in the flow map implies compression in projective space. And you don't have randomization, just regularize things. So if something's growing, it's always kind of growing, roughly speaking. And then it turns out that because the non-linear And it turns out that because the nonlinearity is a homogeneous polynomial, we get a really beautiful identity that tells us that we get unbounded growth no matter what. It's essentially using the fact that because you have a nonlinearity and it's scale-invariant, trajectories are kind of shearing along energy shells. And I actually think of it as a pro, not a con, that our method worked on such a with such little information. I mean, it was designed to do precisely that, work on very little. To do precisely that, work on very little information. So, that's, but I'm hoping that we can get, as if, if we ever get more information, which maybe we never will about Naundry Stokes, that we can improve these estimates that we get. I think I'm out of time. What is the... Well, you're kind of out of time, but do you need more? It depends. I mean, basically, to prove. I mean, basically, to prove, I can just say a few words about proving projective hyperlipticity. Essentially, it's you we have no idea how to do it directly. Um, so it's essentially two steps. So one step is to reduce it to a problem of matrix Lie algebras, which is actually not that hard to do, but not obvious that it's possible. Essentially, you can reduce it to this question about Lie algebras that's the second derivative of the nonlinearity. These matrices are kind of, yeah, essentially the second derivative of the non-linearity. Yeah, essentially the second derivative of the nominarity, but it doesn't depend at all on the force forcing of the noise anymore, the structure of the noise or anything like that, just depends on the nominarity. It's just a question of if you commute, you take enough linear combinations or commute these matrices enough, do you get all trace-free matrices? That's a hard problem when you use some computational algebraic geometry, and I think that I don't have time to discuss. But it's computer-assisted proof, even though it works for all dimensions sufficiently high. For all dimensions sufficiently high and all aspect ratios of the torus. So it's not a simple direct computer assistant proof. You need to do a lot of deductions in order to use algebraic geometry on the problem. But the fact that the nonlinearity is a rational function of integers and stuff like that is crucial. But anyway, I don't have time to discuss, so that's about it. Anyway, I hope that. Anyway, I hope that we're kind of hoping that there should be a lot of directions to go from here. But there's a lot of challenges to improving even what we have. And as you can guess, going to infinite dimensions is difficult. We don't know how to do it yet. So with that, I guess I'll just end. Thank you very much. From the point of view of equity, we also postpone. The view of equity, we also postpone questions until the discussion in 25 minutes.