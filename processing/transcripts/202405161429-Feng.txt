So, we deal with all of these things basically using these two subfields, C server, catcher. Okay, so after all of these things, I want to mention that for brain image and learning data, one of the critical properties of this image data, different from the traditional like a catalog, this kind of standard image, is that the brain images is more or less consistent across different patients. And they are more standardized, registered, and that's a key property. And also, the sample size is really much more. And also, the sample size is really much more limited compared to the standard immediately. And so, this is significantly different from the cat or dog registration or like cat dog recognition or one, two, three, like handwritten digits recognition problems. Okay, so this is some background on the brain imaging data and some existing approaches for image data problems. So, the most simple approach is basically we are doing hiding. approach is basically we're doing high-dimensional regression. Basically, we vectorize all the image voxels and apply some like penalizations like LASO or sort of variation in limit. We see a lot of penalization approaches using this approach. So simply you can see that ideally this is not this clearly it's not an ideal approach. Basically you break down all the speech information and it's computational matistic, right? So we don't want to vectorize all the emails. It's no idea. And the second type of And the second type of approach is more popular over the past days, and we see that a lot of tensor approaches, right? So, yeah, tensor decompositions. We see a lot of decompositions, CP decomposition, target decomposition. And the new decomposition I just learned over the process is that Job Ladies is about tubercombition, right? So, a lot of new compositions. However, they are all designed for general tensors, right? They're not specific for images. So, what's specific for images? Images. So, what's specific for images? Smoothness, clearly. There are a lot of unique properties for image data. I know I want to focus specifically on image data. And for general images, we know that a convolutional neural network is one of the most dominating approach. Not only in statistics, but also in machine learning, computer science, all of this. But they are dominant. However, the control networks are usually designed for large samples and large samples, and particularly for high resolution images, clearly blood samples. Images clearly in both samples. And another drawback of seeing that is the lack of desire, the multi-includability. Well, for the magic imaging, clearly, we need the multi-implacability, and we want to know why you have such a result and why it does. So this is kind of the major differences of the medical versus general being used conventional networks. Okay, so I'm gonna give our proposal and we call it Proposal, and we call it Deep Chronicle Network, and it's designed for image analysis and provide a unified treatment for both matrix and transparent image data. So we can deal with like MRIs or even FMRIs. You have a four-vary image data. And adapt to low sample sense limitation and computationally scalable to high-resolution images. And enjoy good interoperability and not a black box model. And achieve desired prediction power, hopefully, as good as it. Power, hopefully, as good as the end. Okay, so this starts our targets a little bit ambitious, but yeah, we want to do that. And this is the outline of the talk, and the meaning of this talk, I'm going to introduce the deep hard network, and I'm going to write deep hard network in a convolutional form and relate it to CNNs. And then I'm going to build convolutional connections between deep hard network with a CPD convention. And then give some serial guarantees, some studies, and give you a very Studies and give you the reapplication to brain imaging study and with the MNI data. So Alzheimer's disease analysis. Okay, so yeah, deep chronicle network. Here is a very basic introduction about, since I'm going to talk about deep chronicle network, and I first need to introduce the chronicle product. Basically, I believe most of us are kind of familiar with that. I'll just put it here. So, suppose the TensorC is written. Suppose the tensor C is written at A correct with B, both A and B C are tensors. So you kind of like this form, right? So each of the small components for A multiplies with the tensor, small tensors B, right? You got a big tensor C. So this is rather standard simple generalization for the matrix version of Cornell curve per night. Okay, and then I'll discuss our work in a very simple way. Our work in a very simple generalized linear model session basically, I want to mention that, so we can address both linear models and non-linear models. You can address both continuous outcome and the binary outcomes. So, here our x, the y is our output, either continuous or binary. x is the input image. Here I let it equal to a 0 or tensor, d1, d2, d3, but of course if you have a four-der tensor like frm equals 5, and we can generalize this to high-order tensors, and the coefficients are very. Sensors and the co-efficiency is our target, targets we want to recover. And at this moment, let's omit other design variables such as age and sex, they can be added back to the model even. So we want to focus that at this moment. So based on the textbook knowledge, we know that actually for the general designer model, we can run a link function and the lean function of expectation written into this linear forms, right? So very much standard. So the major thing that we want to model the coefficients. Want to model the coefficiency using a Ranker Kronecker product transition because the T V D with L vectors. So we write it into a continuous vector, so like V L R, Kronecker V L minus Y R, Kronecker until 1, V1R, and do all of them together. So for short, we write this into form. And here all the BLR, they are unknown tensors of unknown dimensions. Okay, so all of these they are unknown and even with the gram it's unknown and we call it actually the L. Later we can see we call it the depths. L is simply the depth of this is also unknown. So everything is unknown. And here is a simple illustration of the decomposition with L equal to 3, right equal to 2 and for you to read this coefficients. This coefficients, your coefficients kind of like a circle in the middle, and we want to decompose this into a chronicle force. So you can see that it's a rank two, right? So basically, kind of division is two parts, left and right, and actually you can write three kind of things chronically together with two components together. Okay? So, very simple illustration, and we named this framework Deep Chronicle Network. Deep chronicle network actually because it resembles a very simple convolutional network with purely convolution layers. Okay, so this is the very first idea. So this is Ergen, the illustration for this decomposition. And here I want to mention that this decomposition implicitly imposes the blockwise Mosen structure. You can see here that clearly this is the image. Clearly, this is the image, right? You have the smooth properties, and clearly, you've got this coefficient that's smooth. And this is a property particularly suitable for image analysis. And also, you can achieve tremendous dimension reduction. For example, in this case, we are considering actually here is a very simple example. We are considering the the the image of parameters sixteen squared, sixteen by sixteen, a total of uh two hundred fifty six parameters. And we do reduce this to here is two by two, two by two, here it's four by four. Here is 4x4. So you've got two of them combined together with 48, right? So from 256 to 48. And you can do this more graphic. For example, if you have an image of 256 by 256, and you are taking all of these filters like 2 by 2, so you can do 206 2 to the 8th power. So you can do 8 of these multiplied together. So you can get basically each of the filters. Each of the filters is 2x2, so you got 4 of them. So the total of 8 of them, 2x2x8 is 32. So the rank here is 3, 4, whatever. So you can see that you have the total number of parameters you have, it's much less compared to here. It's like more than a few thousands. So you achieve a huge dimension. And given this chronicle product decomposition, and we can do the Composition, and we can write this MRE, we can write this log record functions, and like this is rather standard with the TLM setting. And clearly, when your outcome is Gaussian is continuous, I'm not reduced to only least squares. And this problem could be solved easily by alternating mutation algorithms, by iteratively updating these blocks, like iteratively. So I can show you that later, that intentionally, this is very much really. That essentially is very much related to CPT compilation and all of the CP techniques can be applied here. Okay, so yeah, this is rather standard. And then I'll show you that actually the deep chronic network can be written in a convolutional form. And here I mentioned that our convolution is slightly different with the traditional convolution in the sense that I want to non-overlap convolutions. Okay, so here is the definition of non-overlap convolution, and here is Convolution and here is an illustration of overlap convolution versus non-related convolution. So for overlap convolution, basically we are moving this like one pixel by one pixel. Here we are doing this jump. So basically the try size in networks, the stry size basically equal to the filter size. Okay, so we are doing this instead of the sliding the window, we jump in the windows. But we can move all, we can do all of the big pictures without overlaps. And without overlaps. So this is the the non-overlapping the non-overlapping convolution definition. So if we have this definition, then we can write the deep chronic network in a convolutional form. Essentially, if your original form is like the link function with someone working on the expected value of y, this form, xi working on xi inner product with the coroning products. Then this is equivalent to say that. Then this is equivalent to say that your outcome is equal to the input x doing convolutions with 1, 2, 3, all of them together. And essentially, this tells us that the response Y indicates definite model by summation of consecutive convolutions between image and your vectors. Okay, so we're doing all of these convolutions. So we are doing number convolutions for all the times. Originally, no time is from L to 1. That is from L to 1, here is 1 to L. Okay, so you are basically, so to BYL is your first convolution, B2, second, and you are doing L. And therefore, the decay could be viewed as a network consisting only of convolutional layers. Here is more, it's a picture illustration. Your input is your big tensor image, and you are doing a convolution with the small tensors, V11, and you got a smaller tensor, and they're doing again, and they got an even smaller tensor. And doing again, and you got an even smaller sensor until the end, you got a scalar. And the second time, you are doing another filter, B1, 2, and B22 until then, you got a scalar. And you're doing these R times, and the end you got all of these scalars combined together as your final outcome. So from this picture, we can see that when we regard L as the depth of a decay, this is the depth, and here this is a R is a width, okay? So from this false network. Okay, it's from the spots network. And the chronicle factors can be viewed as convolution filters. And the convolution in TKN has no overlap with each other, indicating that stress signs equal to the filter size. And such a non-overlapping design makes TKN to achieve maximized damage reduction. From this perspective, the polling layer usually imposed in converter networks that is now required. And notice that the activation function DK is taken as an identity function. And if you introduce a nonlinear function, DK could also introduce to generalize it to a nonlinear version by putting solve these nonlinear functions. Okay, here H is a non-linear activation function, but then we can take it as the value of whatever non-linear activation functions you prefer. And the non-linearity can be solved by standard deep learning frameworks such as PyTorch. Okay? So there are a lot of parameters. Okay? So there are a lot of parameters involved here. I dimension that basically all of these field sizes, all of the depths, with our unknown. So how do we do that? And the question is the depth versus widths. And how do you choose that? Let's go back to a convertible neural network. So the chronic curve approach can also deal with nonlinear functions? Yes, yes, you can do that. You actually, you can, of course, you can just put nonlinear functions here. You can still solve that. But the thing we Solve that. But the thing we need to study is: if for such a convolutional neural network or fort-corruption product, if you put non-linear functions here, do you really get benefits like in the sense because you are doing non-relation sign, you are doing basically here is kind of under-reparation, under-parametrization rather than over-parametration compared in the general what's the benefit here? And this is some question I'm still thinking about, and I haven't Still think about, and I haven't a clear answer. But yes, of course, you can do that. How do you do that? You can just put the it here and you can still solve this, and we can still like. Oh, back to Zen. No, you cannot write to the Zen. Yeah, no, you cannot write into the chronicle before. Thank you for the question. And uh, yeah, uh some questions that we need to answer is uh Uh some questions that we need to answer is uh depth versus width and uh even converting networks or gander deep neural networks. We have the question of um how do you choose how do you choose the depth and how to choose the network structures. Basically, the structure you really need to be carefully tuned in order to achieve the optimal prediction power, right? We all know that. And how the depth and width of neural network would affect its prediction power has been intensively studied over the past decades in the community of neuroscience literature. I just listed a few of them. So, and in our framework, we also got this problem. The depth R, the width R, depth L, with R, and the field size, DL, PR, QR need to be determined. And however, in your analysis, only the width R is tuned. All of the others, the depths and the filter size are actually fixed. Okay, here's why. Here are some reasons. First, for any given L and the field size DLPLQ, there is a corresponding R such that the L. Here is a corresponding R such that any tens of this size can be approximated. So, as long as your R is large enough, no matter how you choose your error, how you choose your filter sizes, you can always approximate such kind of coefficients. And so we don't want to choose all of them. And also, in our case, we really prefer deeper decay in the sense that we always want, for example, in the case of 2 to the 8th power. To the eighth power, 26 for the image size, and we usually want the filters to be 2 by 2. So, deeper decay is usually preferred. Why? Because it achieves greater dimension reduction and adapt to a limited sample size by dimension. We want to work on the magic image setting. So, magic image, we always have much more limited samples. So, we want to have a dimension reduction. So, we want to do deeper models and with the dimension reduction. Create reduction reduction. I just uh that once you've given the the depth and the field size are tuned by the standard based information criteria. So this is um this is some connections between the our DKN versus the commercial networks. Now I'll show you some connections between the DKN versus the CP decomposition. Okay, so the CKP decomposition, we've seen that many times. So here we are considering the CP decomposition. Now we are considering the CPT computation in a regression setting. So the coefficients which have like the outer products of this BLR together. So I'll mention that the DK is connected to the CPT computer in the following way. Okay, so basically if you got a decay with a low chronicle rank, and then it's equivalent to a low CP rank by a certain reshipping, and I'll show you what is the reshipping here. And then basically, here that you got the extracted y with the link function equal to this inner product of input with the chronic form equal to that your transformed image inner product with the auto product. Okay, so what is this reshaping? So the definition of reshaping here and uh here is the illustration basically the the picture is much easier to understand. So Easier to understand. So essentially, here we are considering the case of L equal to 3 case. We can use L equal to whatever. It's a general definition. And here's the illustration here is for L equal to 3. So essentially, for L equal to 3, you got a 3-order tensor. So if your L equal to 4, you got a 4-order tensor. The order of the tensor you get is basically equal to the number of components you are doing in the chronic product. The cornic product. Here in the L equals 3 case, and essentially your matrixization of these factors, here you got a small matrix here. And the second part, you have a matrix here, and the third part here. And for all the right components, you got a big matrix and doing all of this again and put all them together. So you are doing a lot of mutualization for a lot of local vectors. Factors here. Okay, so after this kind of reshaping, and we can see that if your input have this chronic product form, chronicle three things chronicle together, you got this big thing. And equivalent to see that you reshift the image. You shift coefficients equal to this outer product form. Okay. So you got this equivalence between the Equivalence between the cornicer product versus the CP population. So once you get that, and then we can write again for the Um so you are going to your your D one the for each component the the phone number dimensions are different, right? Of your dimensions are different, but you are still can be different, yes. Then how do you reach you you are reshaping the Y? No, reshaping the B, which means the B. You are reshaping each B. Each B, yeah. So each reshaping. So essentially reshaping all the input image X and the corresponding coefficients will be. Okay, that's where. So you have a T of X I, right? A T of X I, yes. Right. A T of X. Yes, you're reshaping the images. The input images. But each component has different configurations, so your reshaping operation will be different. No, for each of the images, we are doing the same reshaping. Because for all of the coefficients, we assume that they share the same coefficients. They share the same coefficient C, and does coefficient C share the same configuration? So R equals one to figure R, they all have the same configuration. That's something I can show them in a few slides later, actually. Right now we are assuming that all of them have the same diagram. Yes, yes. Thank you so much. Yeah, for the question. Yes, here for all of the different R's, we assume that they have the same configuration. But in the fourth level, by the way, we can generalize this to more. Generalize this to a different configuration. So, yes, here we are doing all of the same configurations. Yeah, and we are doing the doing the doing the reshaping here, and we can write this into a CPT computation form. And yeah, we have this equivalence, and the quadratic product somehow equivalent to the transformed CP product. Okay, and so here the remark. So, essentially, this tells us that the CP. So essentially, this tells us that the CPT composition and chronic composition, they are somewhat equivalent after transformation, after reshipping. Therefore, we can, the deep chronic number can be solved by two-step later. Basically, the first is that you shift the original images into the transformer, the T of X. And then you implement the CP decomposition, CP regression, such as alternating minimizations using the CP techniques and reshaping images. Then you can solve all of this. And you've got all the This and you got all the beasts clearly for specific composition of all of the vectors, the b's, and you mitronization for each of the b's, you got all of the chronic curve, the chronicle matrices in the chronicle matrices or tensors in the original input, the original coefficients. Okay? Yeah, and I'll mention that the DK actually includes the CPA at the special case. For example, if you consider the input images of D1D23. The input images of d1, d2, d3, and you consider l equal to 3 and the filters, all of the filters taken as vectors. So they go d1 by 1 by 1, 1 by d2 by 1, 1 by 1 by d3, and then the transformations equivalent itself. Clearly, in such a case, DK includes CP as a special case. However, DK is more adaptive, more general in the sense that it provides more flexible and data framework by allowing different size of vectors, different configuration of vectors. Different configuration of factors. And also, I mentioned that the size of the factors BLR and the numbers per layers allow to be different across the R. And the different reshifting operation TR can be applied. Then basically after you're doing that, so for each of the for each of the for each of the ranks, basically you are allowing to, we are doing like different appreciations. And we can run this into this form. We can write this into this form. And then, no longer, this is no longer a CPT composition because the input now is different, right? So, so, so, clearly, this is not a CPT composition, it's much more general. And but it still can be solved by alternating minimization with all this PR updated each. So, again, here we want to say that the DKAM provides a much more flexible framework in the sense that not only we can allow this, even if we allow all of the over the ranks beyond the same size. Over the ranks, they are the same size in filters. And this still includes CPS become as a special case. If you consider different configurations over different ranks, clearly it becomes much more general. Okay? Some serial guarantees. Actually, serial guarantees, I don't want to talk too much. Basically, all of this CP decomposition, CP properties can be applied here. So I only want to Can be applied here. So, I only want to show you a little bit of things we have done. Basically, we want to bound the distance between the estimated coefficients, C hat, and the C coefficients when the network structure is correctly specified. Otherwise, we cannot do that. And we should know that actually, for the chronic product optimization is clearly not identifiable if you don't specify the L, specify the R, clearly, you have a lot of different identifiability issues. You want to clearly specify the network structures, clearly. The network structure is clearly identified. And here, in this instance, where normal distance refers to the tensor angles, and we want to balance the distance. And again, here we will much more restrict the instance and focus on the rank one DK. And otherwise, this becomes more complicated. And even for rank one case, we know that it's highly non-convex. And all of this we are developed under the case of automating e-squares. So yeah, very simplified scenario for the other part. For the other part. And we are doing this and the RP condition. The RP condition here is slightly different in the sense that we are considering the RP condition for chronic product structures. Because of course we are doing chronic product decomposition and the RP condition generalizes to a chronic product case. And so we know that I don't want to read this. Basically, the RP condition originally developed for sparse vectors and later generalized to Later generalized to low-ranked matrices. Here is kind of a low-rank matrices, but more general is kind of a low-chronical tensor case. So under such a low-rank chronicle product, RT condition, and we show that the ordinanization could converge to the truth geometrically. I know that for the RT condition, there are some previous literature showing that it can be satisfied by many random matrices such as Gaussian matrices. Gaussian matrix with ISO. This is some previous GP triangle. Yeah, and we are suppose the model is under a linear model and with a coefficient C again that we are considering restricted case in the sense that the ramp is equal to one and assume the argument condition and we also assume a small enough constant such that actually the noise could not be too large to be bound with the noise actually for some constant. Actually, for some constant, and then the DK can solve using automaticization algorithm with a correct descriptive number structure and a special spectral analyzation. Then we can get this basically a geometric convergence. The distance between your C hat T versus the truth is bounded by this. The kappa is some constant, it's something um between zero to one, don't want to specify it here. And uh so kappa to the t power and the mu is some unifyation, uh errors. I have a here this here is errors. This here is the set of errors. So, computational errors, variables. And you can see that after many times, T here and those point goes to here. And the Tau actually the Tau is of this order. I don't want to discuss too much here. Basically, all of this is essentially developed by the CP framework. So after we shipping everything, it's almost done. So yeah, this is some statical results here. Results here and some simulation studies. So, simulation study, we consider both regression and classification tests. And for regression, we consider linear model, and for the classification, we consider a standard only model. And we consider a logate model, and pi contracts, but the normal image size, 128 by 128. Of course, we can do a tensor case later, I'm going to show you the tensor case. Here, we consider matrix case, but actually, there's no big difference. The image XID drawn from normal, and we consider two different samples, 500 and 1000. We consider two signal shapes, Y circle and three circles. We considered two cases, why it's sparse, why it's causing sparse. Sparse basically means that the coefficient is sparse. Only the middle signal is non-zero, and the causes sparse basically that is the background is still non-zero, but with some random noise. Okay, some computing methods with test regression with CPC. ET methods with sensor regression with CPD composition and CPT composition with some LASO localization. And we also did some nuclear non-localized matrix regression here because clearly we are doing essentially this is a matrix case. So here it's a nuclear normalized regression. Okay, and again, as I mentioned, the DK is implemented under the deepest model, essentially, for all of the filters we consider 2 by 2. So we consider the same of the multipletic effect. And here is an illustration for the SMD images. Actually, here, this column is your true image. Here is a sparse case, so it's only got a true circle here, and here is the Causes Parsec. Background is just some Gaussian noise. Here is two circles, and background is arrow. Here is two circles. Background is a Two circles background is a background is some noise here. Okay, and so this column is recovered by our deep clinical network. This one is a low-rank matrix regression, and here is a tensor regression basically. Of course, this is tensor regression. We applied this info matrix case. And here is this regression with some organizations. Okay? So you can see here that for the two of this. For the two of this sparse case, and the two corner product recovers very clear images, right? So, of course, you can see that this is not exactly circles, somehow like the blocks. Just because actually we are doing two matrices, and essentially, all of this you are doing this blockwise forms. You cannot get this exact s the the the circles, but so the locations can be much more clear. And um More clear. And I'll mention that actually here that the locations of these circles are rather random. They cannot be written as this quantity common form exactly. So basically we are implemented DP under misspeciform in the sense that actually this is not equal to an exact quantity planner form. So we are not doing this fair hour setting. So, and the more quantitative variance, we are doing the estimation prediction, accuracy, and both sparse setting and cost far setting. Here we are consider the regression next slide for classification. Basically, across almost all the settings, we demonstrate the best performance for estimation prediction, one circle, two circle, in all the cases for regression. And then here, in this case, it's for Here, in this case, it's for classification. Okay, yeah, I don't want to read this. Yeah, essentially, you can see that in terms of estimation, we got one of the smallest errors. And then, some analysis on the admin data. And yeah, here we are considering more tense, we are considering real tension. Okay, so we are considering. Okay, so we are considering as Hamburg's disease neuroimaging initiative and it's kind of designed to detect and track AD with clinical genetic imaging data. And we use T1 wave NRMS guy represented by a three-order sensor to analyze two types of images. And here the images go through all of this process and graphic also goes. And again, we consider two analyses: one is a binary or co-op setting, and this is the classification setting, suggesting if a participant Setting suggesting if the participants have a certain behavior or not. So the patient setting and the continuous is a regression setting that we consider in. We use continuous outcomes just in the mini-mental state, the nature IMMC score, a common used reference for type of disease. And the image of size, like a tensor of 64 by 64 by 64. And we use two phases of admin. Admin Y admin Go as training and the third phase. I need go as training, and the third is at least three as testing. And the training and testing set contains like 400, 200 subjects, respectively. And we report the prediction, including the Lumi square error for classification and the classification accuracy for this case, for this test data. Okay. And notice that here we can also implement concrete neural network. So basically, we include the test regression including the That the tensor version includes the CPT computation, and the tensor version, the CPT computation with massive validation, and the container networks. And yeah, we have the best prediction because even compare with SAM. So again, as I mentioned, before saying and the subject case, the major issue is that the limited sample size. The sample size is like 400 in the training, and much more difficult to train. And especially if you consider the deeper models, and you have a lot of parameters, that would be troublesome. That would be troublesome. And also, we actually plot the estimate coefficients for the decay and we highlight the coefficients that it's bigger than 3 times of standard error over the mean. And here you can see that he can basically identify a clear region. And later, I can show that this one is hypotenuse, which actually has been proved related to the Is related to the time of Case. Well, if you do the CPD computations or CPD computer with lots of validations, so you are basically identifying all of the signals more like sporadic and rather random. And yeah, under these regions, in classification regression, oh, I have information that they are here, it's like this is for regression and this is for classification. They are consistent. We are totally totally different uh outcomes and there's identical regions, yeah. Counts and the identity regions are consistent again, suggesting that the regions, yeah, they are meaningful. And so, the classification and regression are consistent both around the region of hippocampus and the medical literature have suggested that the hippocampus improved to be associated with hemis and findings are consistent with the existing literature. So, yeah, probably I'm a little bit early. Yeah, I don't have a concluding slide. Yeah, I don't have a conclusion slides. Yeah, basically, and we did this deep chronic network and highly related to the conversion network and related to the CPD composition and it's specifically designed for medical imaging data. And we find it is quite good in terms of imaging, random image data analysis. Yeah, thank you. Can you say more about selecting the configuration of all the configurations again? All of this is basically using 2x2. And all of this is processed into 2 to the some toss power of the image. And they just used all of this 2x2. I didn't choose anything. So I just want to avoid that, seeing that we are robust across different configurations. Yeah, that's my purpose. That's my purpose. For example, you can capture circles, you do a 2x2 and a 3x2. Yeah, yeah. I totally agree that I cannot clearly identify the shapes, but again, we are considering center of medical methods. More critical things, it's under such a limited sample sense scenario. And I want to locate the regions rather than exact shapes. Medical meeting must exact circle sort of way of that's much more difficult, I feel, like Much more difficult, I feel like under such a limited semi-science setting, it's somehow not a true case, so we kind of give up doing calculations. Your experience is deeper is better? Not deeper is better. Deeper vers means that you don't need a lot of parameters and when you have a much limited sample size, that is why. Yeah, yeah, yeah, yeah. Maybe that's that's a very good point. Actually, yeah, I got some a lot better on the structure of singular and how they work. Well, and uh yeah, here I actually mentioned that we are still under the the case of linear case value. And that's a case of linear case right now. So actually uh we're gonna release a paper uh uh non-linear s a non-linear deep coordinate network and we allow the the functions to be non-linear and still get some zero guarantees. So they're gonna be released more paper. So yeah. And that will be more general. No, we all at this moment only consider one layer of commuter and all the others we can keep. Of commodion and all the others, we don't care about this structure, it's whatever structure and the SLI. The first layer is convolution, and you have a general function T. For example, I don't care what function T and the SLI is this function, and whatever structure it is, I identify the filter. But I have only recovered the one linear structure at this moment. On the anomaly case. So when you compare with CNN and what is there? So when you compare with CNN what is their architecture of here is only two Liar CNN and uh because of all of this is two years in and uh two layer two size. I remember correctly it's like twenty twenty. Yeah. I I I I actually ten ten and thirty thirty doesn't make too much difference. I I don't want to tune that a lot because you know so so so so our case I don't want to tune the filter size for the for the thing, I don't want to tune the tune the t the the the the taste. Yeah, I can do to do to do too. Yeah, but I mean so so here the major issue here is that if you are doing this non-overlapping non-overlapping case if you are doing this with overlaps, that's more appropriate for the kernel setting, right? Here, here is a much more under-represented parameterization, right? It's called the beat, like 2522. So, so, so for sale. So, for neural networks, it's like over connection. So, what are the benefits of hormonal connectivity versus underformation? Something I'm not exactly familiar with, but something we need to think about. What is the conceptual, I mean your initial series for any L capital and R, but then you switch to L capital equals 1, right? L capital equals 1. L equals 1. L equals 1. Oh, he is still in my. So, my question is: what is the conceptual difficulty in having, say, L, 2, 3, 4, or any finite number? The most difficulty is that when you're doing these alternative animalizations, you have a lot of identifiability issues. And yeah, that's actually I try to To do that, I mean, it's quite hard in terms of analytability and in terms of actually there's a phone number right here, and I just can't do R input. Okay, so the problem starts with the identity file like the language. Yeah, yeah, it's also part of that. Yeah, and also all of this is basically we can borrow the borough the idea about the literature from recommendations. So I don't want to focus too much on that, so I didn't.  Hello? Have me for the foot image? It works.    Yes, but yeah, it's got a lot of money. So you have a second one. Yes.   Okay, well  I'll second the handle. I think I think it's a little bit of a.  So it says that all the number of teams.     I don't see the key of your house. I don't give a sound.  How etc.