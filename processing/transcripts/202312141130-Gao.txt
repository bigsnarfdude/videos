So, today I'm going to share our recent work on robust knockoff inference. And this is a joint work with my postdoc advisors, Yin Fan, also one of the organizers, and Jin Chiu Liu at the University of Scalden, California. So, Model X knockoffs framework is kind of a general approach to variable selection with some error rate control. Some error rate control. And so consider we have a response, we have a response Y and the picovirus X1 up to XP. And of course, we can collect n data point and write it in the data matrix and the response vector. And our goal is to identify relevant covariance. So what is the relevant covariance? The XJ is called relevant if it is not independent of the response. Not independent of the response given all the remaining variables. Of course, so we can collect all the relevant variables in the stat H1 and we define H now to be the complement of the H1, which contains all the irrelevant variables. And so we want to select as many variables as possible with controlled errors. Here we consider false discovery rate, which is defined as expectation of this false discovery. Of this both discovery proportion. And okay, so there has been a lot of literature focused on FDR control. And for example, the BH procedure, which is very well known and popularly used. But most of the work relies on valid p-values. But they can theoretically control FGR on their specific dependence of p-values, such as independence. Of p-values, such as independence or positive dependent p-values. And more importantly, an issue with those methods, which depend on p-value, is that in real application, we might not able to obtain valid p-values. So a desired property for a valid p-value is that it should be uniformly distributed under the null hypothesis. However, we usually, maybe not usually, but sometimes we find that. But sometimes we find that the p-values can suffer from non-uniformity, especially in high dimensions. And sometimes, maybe in high dimension, we cannot even obtain the p construct the p-values. So Lakoff's inference framework is the distinctive feature of this framework is that it can bypass the use of p-values to achieve certain error rate control, like FDR. Rate control like FDR or the K family-wise discovery rate, and it is model-free. It does not require any assumption on how the response Y depend on the variables. And it allows for any dimensionality for the covariates. And it also can achieve exact finite sample FGR control. But the price to pay is that we need to know the covariate distribution, exactly. Covariate distribution, exact knowledge of the covariate distribution. And the basic idea of knockoffs is to create fake copies of the original variables. And those fake copies should mimic the dependent structure of the original variables, but they are independent of the response Y. So those fake copies can serve as a control group when we assess the importance of the original variables. Original variables. Okay, so the formally the knockoff model X knockoffs for the feature vector X should be a new family of feature, which is also in the same dimension and should satisfy the following two properties. The first one is for any subset S, if we swap X J and X J tilde for any J in the For any j in the subset S, then the joint distribution should remain the same as the original ones. So, this can this actually indicates that the XJ and XJ tilde should be kind of exchangeable in the joint distribution. And the second property is that the knockoff variables should be independent of y, given the original variables. So, this property is very easy to satisfy. So this property is very easy to satisfy because we can just construct knockoffs without looking at the observations of y. Here's an example for generating knockoff valuables. If x follows a normal distribution with covariance matrix sigma, then we can construct this joint vector of x and the knockoff copies x tilde. Knockoff copies at Stilda to follow this extended normal distribution with augmented covariance matrix defined in this form. So from this construction of the joint distribution of the X and X2, that we see that because of the symmetry structure of stigma or symmetry means if we swap the J's column with the J plus P's column, the stigma orc remains. The sigma arc remains the same. So, in this case, by this construction, we can easily see that this joint vector satisfy this swapping property. And okay, so if the joint vector follows a normal distribution, then of course we can construct knockoffs from this conditional distribution, even original variables. Variables. And because to sample from this conditional distribution, we can directly construct based on this construction. So here, because the mean is a function of x, and then we can construct the z, which is independent of the data and consists of id normal entries. And we multiply by the square root of this covariance matrix, then we can start. Covariance matrix, then we can directly generate the knockoff copies. So, this is the construction of the knockoffs under Gaussian distribution. So, an important observation from here is that because Z, you can generate in different realms, the Z can be different, realizations can be different. So, the knockoff variables is not, they're not unique in realization. Sometimes it's not even unique in just. Not even unique in distribution. Okay. Okay. So the knockout procedure consists of three steps. The first step we already introduced, we need to construct the knockoff variables from this known distribution, given the original variables. And the second step is to compute the knockoff statistics. The knockoff statistics kind of measure the comparative importance of the X importance of the xj and sj tilde. So which means a large positive wj tilde means that xj is more important than the control variable xj tilde. Therefore the xj could be relevant. And a very important property for this knockoff statistic is that based on that swapping property you can show that if wj is an anti-symmetric function with respect to xj and xj etilda. Respect to XJ and XJoda, then this knockoff statistic will be symmetric around zero in distribution. So, this is a key property for ensuring FDR control in the knockoff procedure. And a typical example for the knockoff statistic is the regression coefficient difference. We can directly regress y on the joint vector of x and x tilde. Here, beta j hat is the coefficient. Is the coefficient regression coefficient of xj and the beta hat j plus b is the coefficient for the corresponding knockoff copy xj tilde. So this difference actually measures because this part, this second quantity should close to zero. And so if wj is large, which means the beta j hat is large. And the third step is to select relevant variables based on the knockoff statistics. The knockoff statistics. And a large positive Wj means Xj could be relevant. So we can select variables with Wj tilde greater than or equal to some data-driven threshold. And it has been shown in this very well-known paper that if the threshold defined in this way, then the knockoff procedure can control, exactly control FDR at the targeted level. Control FDR at the targeted level, Q. And the intuition here is that it's as follows. So FDR, this is the definition of FDR. And in the numerator, because for now, J, WJ is symmetric, then we can see that this numerator is approximately equivalent to this quantity, which is now such that wj less than or equal to negative tau. And because we don't know which j is. And because we don't know which j is now, but we can always control this numerator by the total number of wj satisfy this condition. That's why this, because this ratio is defined to be controlled by Q. So that's the idea of why FGR is controlled by the Q, the target level Q. And the minimum, the definition, minimum is trying to select as many variables as possible. So this is the ideal knock. Um so this is the ideal knockout procedure which can ensure exact FDR control if we have knowledge of the ideal distribution. But in practice it such information is always unavailable. So usually we will have approximate distribution and proceed with the knockoff procedures. And the approximate distribution can be either misspecified or estimated. So the question is where our work is trying to achieve Our work is trying to address this question: that what if the approximate feature distribution is applied to generate knockoffs? And will FDR can be still controlled? So here is simulating example. Here we generate Y from a linear model and X from multivariate T distribution with new degrees of freedom and the scale matrix sigma. Scale matrix sigma. Because sometimes in practice, we don't know nu and sigma. And it actually this distribution looks very much to a normal distribution. It's common practice to use the approximate normal distribution to generate knockoffs with estimator sigma hat. Actually, we can directly estimate sigma by the quarter matrix of sigma of x. So with this misclassified distribution, we can proceed with the knockoff procedure. Proceed with the knockoff procedure, generate knockoffs variables and compute knockoff statistic and select variables. This is the performance of this approximate knockoff procedure. Here we can see that the target level Q is 0.2. We see that the approximate knockoff procedure actually provides very well, very good FDR control, even with a misspecified distribution. So more evidence can be found in The more evidence can be found in the literature. So, we're trying to provide some theoretical guarantee for this robustness, the phenomenon of robustness of the approximate knockout procedure. Okay, there has been some work in the literature that provides partial answer to this question, for example, these two paper, but their results are based on some specific settings like the linear model setting or with. Linear model setting or with a latent factor model, or directly under the Gaussian covariance. And also, some of the work assumed that FGR function is Lipschitz continuous with respect to the parameters. And this is not very easy to verify in practice. So, our work is will avoid this condition. So, the most related work is So the most related work is this paper by Barbara Candice and Samworth, where they bound the FGR inflation when approximate distribution is used. And the inflation is bounded by the distance between the approximate distribution and the ideal distribution. But they only require the closeness of this distribution, but does not require that these realizations are close in these two procedures. In these two procedures, because we mentioned that the knockoff variables are not unique, right? Okay, so specifically they prove that this inflation of FDR is bounded by the empirical KL divergence between the approximate and the true conditional distribution of XJ and XJ minus negative J. So negative J means all the remaining variables. Variables. So, this is a very general result without any model assumptions. It holds for any X covariance distribution and for any model, for any relationship between Y and the covariates, the very general result. But there are two key assumptions for ensuring FDR. The first one is that they require this estimated distribution be learned independently. Distribution be learned independently from the training data. And secondly, for FGR control, a symptomic FGR control, they require this quantity goes to zero in probability. But in practice, these two assumptions might not be satisfied. For example, we don't have the luxury of the independent data set, and the second property in And the second property in actually in the t-distribution example, numerical example we shown previously, they require this degree of freedom mu should grow at least at this rate. And we will show that if we with our new technique of coupling, we can relax this assumption under some specific settings. So now we're ready to present our theory. In our analysis, we analysis we so their analysis trying to use the this this this the distribution the distance between the distribution to measure the inflation of FGR but in our work we will require that this we compare we we couple the approximate knockoff procedure with an ideal procedure and where this knockoff variables and the knockoff statistics in this And the knockoff statistics in these two procedures should be closed in realization. In realization means their values should be close, not only in the distribution. Okay. I will present our result in two steps. First, I will show a specific example to illustrate the copying idea. And secondly, I will present our general theory briefly. In this illustrating example, we consider again, we consider apps. Consider again, we consider x follows a multivariate t distribution with new degrees of freedom and the scale matrix omega inverse. And here, the nu and omega are both unknown. So, suppose there is an effective estimator for this precision matrix. Then, as we mentioned, it's common practice to approximate the t-distribution with a normal distribution and generate the And generate the knockoff data matrix from this approximate distribution, which matches the first two moments. The matched moments means the first two moments are matched for the approximate and the true distribution. Not the true distribution. We directly incorporate the estimation of the first two moments. Okay, so as we mentioned, as we presented. Mentioned as we presented previously, this is how we generate knockoff procedure, knockoff variables from a normal distribution. Here, Z is independent of the data, and the R is actually we should choose R as large as possible, but we need to ensure this part should be positive definite. So, here we use a misspecified feature distribution. And the question is, how do we construct the coupled ideal knockoff? The coupled ideal knockoff variable such that they are close in realization. So, here, actually, we use this key representation for the t distribution. That the multivariate t can be represented as the ratio of the eta, which is a normal random variable, divided by the square root of the normalized q normalized q. So q is the chi-square distribution with new degree chi-square distribution with new degrees of freedom. With new degrees of freedom, and the numerator and denominator are independent. So, based on this representation, we can construct the coupled ideal knockoff data matrix in this form. So, actually, this data matrix looks very similar to the approximate one. The only difference is that we replace the estimated parameter with the true ones, and this additional part is because of the nature. Because of the nature of the t-distribution. So we can show that using this key representation, we can show that by this construction, this coupled ideal knockoff data matrix, we call it ideal, which means this x2 does satisfy the swapping property. Remember, we can swap any, which means in simplicity, it means xj tilde is exchangeable with xj in distribution, okay. J in distribution. So here we have construct this ideal knockoff data matrix, which looks very similar to this one. And by comparison, we see if the sigma hat, the parameters are well estimated and the degree of freedom goes to infinity, we can see these two data matrix should be close to each other in realization. And I need to mention that we don't, this data matrix is by construction, we don't need to generate it in reality. Generated in reality. We only need the existence of this matrix. So, and we show that if the parameter, the class, the precision matrix is well estimated, so this rate is a very common rate for the precision matrix estimation. Then we show that this scaled distance between the columns of the two matrix should be bounded by these two terms. The first term, These two terms. The first term matches the error, approximation error of the precision matrix estimation. And the second term actually reflects the effect of the misspecified distribution type. So this is how we construct ideal coupled coupled ideal knockout variables and analyze the Krinos. The clinosis in realization. So, in the second step, we will show how to construct ideal, the coupled ideal knockoff statistics such that these two are close in realization. Because the Wj, the knockoff statistic, depend on the variables differently based on different construction, we only consider this regression coefficient difference for simplicity for an example. In our paper, we also have other examples. We also have other examples. So, this is the we consider a linear model in this step, and we denote the x hat arg as the augmented approximate data matrix, and the x tilde arc as the coupled ideal data matrix. I will not go into too much details for how to calculate the device LASO. So, in simplest So, in simplicity, we just calculate the regress y on the augmented data matrix and obtain the coefficient for the approximate data matrix and also the coupled ideal data matrix. So, if we use all the so if we use all the we use we if we use same regularization parameters in when calculating the beta hat and the beta tilde, then we can see that the only difference for We can see that the only difference for obtaining beta hat and the beta tilde is this difference in the design matrix. So, if the design matrix are closed, then this estimation should be closed. And the RCD knockoff statistic defines this difference in absolute difference of these coefficients. So, beta j hat is the coefficient for xj, and the beta j plus p hat is the coefficient for xj tilde, or xj hat. Efficient for xj tilde or xj hat. So we show that this wj tilde, the distance between this approximate knockoff statistic and the ideal knockoff statistic is bounded by this term. So the first part is the coupling error for the knockoff matrix, and the second part is the L1 estimation error of the coefficient. We see that the more accurate the knockoff, the knockoff variable, Knockoff, the knockoff variables, the more accurate the knockoff statistics. Okay, so finally, we show because the selection is directly based on the knockoff statistics, so we can show that if we have the approximate knockoff statistic is close in realization to the perfect ones, then it should have asymptotic FDR control. And here, in this t distribution example, we require the new growth at this rate. Here, S is the Rate here, S is the sposity, the number of non-zero coefficients in the model. So, to compare with the elegant result by Barbara, Candice, and the Sammers, they require that this new under this, in the same example, they require the new should grow at this rate. And their new should actually, imagine if P is greater than N, then it means the new should be greater than or. The new should be greater than or should grow faster than order of n. But our result actually is free of the sample size n and only required this condition. So if s the spaws level with small order of n, our condition is less less restrictive. But I need to acknowledge that their result is quite general without any model assumption, but our result is obtained under Result is obtained under some specific settings. Because our analysis of the realization depends on specific construction of the Knockoff statistics. So maybe I don't have too many, because of the short of time, I'll briefly go through our general theory. So in our paper, we provide a general theory on the robustness of the FDR control for the knockout. For the approximate knockoff procedure. We call these three steps of the knockoff procedure, and our theory will have three layers respectively to these three steps reversely. So our first layer will pose a general condition on how close this approximate Knockoff statistic should be to the coupled ideal Knockoff statistic in realizations. So this is the general, this is the So this is the general coupling appearance condition on the Knockoff statistics. So in this layer, we have no specific model assumptions. And under this very general condition, of course, here Bn should satisfy this condition. This condition, here G is the tail probability, averaged tail probability of the Knockoff statistics, which means order of perturbation, the perturbation of order Bn. The perturbation of order Bn will not make much difference in the distribution of Wj. So actually this equation will provide some order, some rate of convergence for Bn. And under this general condition, we show that the approximate knockoff procedure can achieve a symptomic FDR control. Okay. And in our Okay, and in our second layer, we show that what condition should be on this should be posed on this knockoff variables to ensure that this general condition is satisfied. And in this layer, we need some specific models, actually. And we post this coupling accuracy on the approximate knockoff variables. And here the And here the recall that in the previous illustrating example we showed this this copying error for this knockoff variable matrix. And in this layer, we showed that for this regression coefficient difference with knockoff statistics, the new coupling condition on the X can actually imply this general condition. imply this general condition on the knockoff statistics. And we also show some the single result for the marginal correlation difference knockoff statistics. In the third layer we go to the root of the knockoff inference because the error is actually originally induced by the approximate distribution. So the intuition is that if this approximate distribution Is that if this approximate distribution approaches this true distribution, then this knockoff variables should be close to certain coupled ideal knockoff variable matrix and similar for the knockoff statistics. And we show that under specific condition for the f hat, this coupling condition should satisfy. So and therefore we show that the coupling condition in the first two layers are satisfied to ensure the FDR control. To ensure the FDR control. To wrap up, we established a general theory on the robustness of the knockoff framework. And we showed that this approximate knockoff procedure achieved a symptomic FGR control. And our analysis relies on the new idea of coupling to analyze the distance of the realizations of the variables in the two processes. In the two procedures. In our paper, we also contain some results for the K family-wise error rate control. So this concludes my talk. Thank you.