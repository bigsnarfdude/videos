This wonderful setting. I really enjoyed all of the talks and a lot of the interactions. My talk is gonna be based on two works with two of my graduate students. One is actually gonna be a little bit more optimal transportation than diffusions, but you will see. And the other one is with Wan Tian Shu. And part of it is actually also together with Anna Korba. So, okay. I'll talk about here in the first part. I'll talk about slice lossing distance and geometry of slice lossing distance. I'll motivate it soon as to why we want to study it, but let me somehow first introduce it. So what is the slice loss? Let me first introduce the Radon transform. So what the Radon transform does, given a function f, right, you integrate the function f over the slices perpendicular to a direction theta, right? To a direction theta, right? So if this is the origin, and here is the direction, well, right, so here is the direction theta. In that direction theta, you go up to the distance p. So this is the origin, p will be the distance, and then you integrate over the orthogonal complement of that point. So we integrate over hyperplanes, right? We average the function over hyperplanes, and that is then going to be Radon transform, right? Has a certain symmetry, right? So it's if you plug in minus theta and minus p, you get the same as with theta p. And in this talk, I'm going to be denoting this by f hat. So usually f hat is used for Fourier transfer, but there is a famous book on redon transfer by Helgerson, and he uses f hat. So for that, you know, to be consistent with that community, we used f hat also in our paper. So then what is slice-based metrics? So we would like to compute the destination. So, we would like to compute the distance between two measures. And the way this is done is: given a direction theta, we project those two measures to this one-dimensional set. So, once the measures are projected, we compute their waste and square distance between the projected measures, and then we average this over all possible directions, right? So, this is kind of can be effectively computed, right? Because one-dimensional vasterstan distances are easy to compute. So, after the projection. Distances are easy to compute. So, after the projection, this is easy to compute. Okay, averaging over all directions is really impossible, but you can sample sufficiently many directions so that you can approximate this rather easily, even in high dimensions. So, again, this is nothing else but the average of the Basersten distances between the projections. What is interesting a little bit about the topology of the distance? So, it was shown first in the thesis by Bonotte on bounded domains and then recently by Raktar and Guo that. By Raktar and Guo, that the metric that a topology that Wasserstein metric and slice-wasserstein metric introduces on the space of probability measures with bounded second moments is the same, right? And actually, if you're in a bounded set, then you can bound it. Obviously, slice-wasserstein distance is less than the Wasserstein distance, but it can also be shown that slice-wass distance is bigger than a certain power of the Wasserstein distance, times a constant, which depends on the radius of the domain that you consider it in. So this was, let's say. Domain that you consider it in. So, this was, let's say, shown by Bonovsky. So, it has the same kind of topological properties. And, you know, what is kind of part of the somehow part of the motivation? So, part of motivation comes from statistics. So, if you like machine learning, right? So, you would like to deal with objects that you can approximate well in high dimensions, right? So, let's, you know, and how do you approximate these objects? You want to have kind of some robust discretization that you can execute in high dimensions. And, you know, as you know from High dimensions. And as you know, from Monte Carlo integration, taking random points is allowing you to approximate integrals very well. You would like to approximate, it is convenient to approximate measures by empirical measures, or namely by point, by discrete measures. Okay, here I'm using the same mass, but that can also be relaxed. This result would still be the same. So, what is our goal? Our goal is to use such a metric, right? Such a metric, right, in which we can approximate the given metric of measure of interest in high dimension. So, D is also, here I use it for a dimension, and is the number of points that we will be using to approximate the target measure. Again, you're using this particle measure. And if you're looking at the Vasich dimension, so these are classical results. Okay, let's say, you know, for rigor, okay, let's say I'm looking at the bounded domain, the density bounded from below, even that. Density boundary from below, even then, what is important for us is that to approximate the measure in Washington distance, your error is going to be like n number of particles raised to the power negative one over d, where d is the dimension. So in high dimension, you're not able to approximate the measures accurately, right? For a given error, right, even just error one half, you need exponential number of points in dimension to be able to achieve this error. On the other hand, for slide. This error. On the other hand, for slice classage time, because you see, you know, for in one or two D, you can do this one over square root of n, this is called a parametric rate, it also arises in Monte Carlo integration. Since your, you know, projected measures can be approximated well, and even if you somehow have to approximate all of them at the same time, right, this can still be done reliably. You could also do that if you projected to two-dimensional sets. You would still kind of have this parametric approximation result. So you're using a somewhat Results. So you're using a somewhat weaker metric, right? But it metrizes the same topology, but it can be approximated well in Heidegger. This makes it, you know, statistically attractive. And there are now, you know, there are such results actually, you know, these are recent results in actually statistical literature. Okay, so, you know, to be able to study, so what I'm interested in is what is the geometry of the slice laser distance. So namely, we feel, you know, the wonderful thing about the Wasserstein distance is that we About the Hausserstein distance is that we understand the geometry, right? There is this Riemannian geometry, we know the geodesic, the autocalculus, and so on. Slice Hausserstein doesn't seem to have geometry on the surface because the projection and these various projections destroy the geometry. But let's see, let's kind of take a more careful look. To take a more careful look, I need a little bit of background on Radon transform, right? So, Radon transform is introduced here. The dual of the Radon transform is this operator which I denote by check or by. This operator, which I denote by check or by R star, right? So this is now in the dual domain. So in this theta p domain, it is also obtained by averaging. If you look at geometrically, kind of, if you think of this as radial coordinates, this would be averaging over a sphere, right? So the primal transform is averaging over hyperplanes, the dual transform is averaging over spheres, right? There is this property of the adjoint. And so. And so, to understand, you know, kind of what is the, you know, when we are doing Radon transform, we are, of course, integrating and thus regularizing. So, what is the regularization obtained in Radon transform? So, it's surprisingly actually difficult. You know, if you give me a space and to characterize what is the range of the Radon transform, this is surprisingly difficult, right? And these results, let's say, this result from Shafarudino. This is, I think, 2020 or 2019 paper I saw, what he introduces are these attenuated. Introduces are these attenuated, right? So he divides these attenuated solveless spaces, right, which, you know, kind of the primary exponent would be S, right? So you should think of T as being less or equal than S, right? So this highest power of this psi to power 2s, right? And here is the, you know, this is kind of the standard Fourier domain representation of the Fourier of the sub-olar spaces, you know, in addition to right, really, not, let's say, if I didn't have this, well, if I just have psi to 2s, then. Well, if I just have psi to 2s, that would be the homogeneous sobola space. If I had 1 plus xi squared to power s, that would be the standard sobola space. But this is a, you know, somewhere in between, if you like, right? So two kind of frequencies are penalized, right? So what he shows is that Radon transform is an isometry between Sovole spaces shifted by this power d minus one over two. So this is exactly how many derivatives are rebuilt by integral. Are rebuying by integrating over d minus one dimensional hyperspace. And note that when s is equal to t, this is nothing else but the homogeneous this is nothing else but the homogeneous sovel space. So right, this is an isometry between homogeneous sobless spaces and one when doing Radon transform one is gaining this d minus one derivative. So yeah, right. Okay, so if one wants to define the inverse, right, so basically you need to So basically, you need to take those d minus one or two derivatives. So, what this is done in the odd dimension, you're just taking the one dimensional derivatives in the p variable along each of the directions. And if the dimension is even, right, that involves actually also somehow a zeroth-order operator, namely Hilbert transform again along the Transform again along the p variable. And then the inversion formula is really just this: you know, r star lambda is the inverse of the, so the dual composed with lambda is the inverse of the Radon transform. Okay, so now, okay, let's look at the, what would be, okay, so to understand the geometry, let me see, first see what is the speed of the curves, right? What is the metric derivative of curves in the slice positional space? In the slice-Wasserstein space, right? So we look at the definition, right? If I can interchange the derivatives, right, then here I really just have to take the metric derivative of the Wasserstein, of the Wassette, this is but for these projected measures, right? So basically, okay, you know, these linear operators and thus, you know, the derivative commutes. So here we have the, you know, the projected path, and we take the time derivative. So we, you know, this is the metric derivative, one-dimensional Vasishton metric derivative, and we add. And we average that over all of the directions. So, in particular, if you have a curve rate which is driven by the velocity v, then all of the projections move by the velocity, which is the projected average of these velocities. So, this now involves a weight. It is not just the Radon transform of the velocity V, but you need to take the mass into account. So, right, you know, here it is kind of explicitly taken into account. So, in that case, the slice waste. So, in that case, the slice-Wasserstein metric derivative is nothing else but this should be the average over theta. I'm sorry, right? The average is missing here, right? Here it is, but here it's disappeared of the L2 distances of the projected one. So, again, to kind of study and to create a Bennel Brainy functional, it's useful to go to the flux variable. Useful to go to the flux variables, right? So we introduce this quadratic form in the flux variables, right, which is given in this form. And right, so now this is jointly integrated. This is what should have been here as well. This is now jointly integrated, you know, over mu hat and the basically projective space. Okay, so you know, this is more or less somehow analogous to what one would see actually in the Wasserstein distance. So what we have is Distance. So, what we have is that for any curve, right, if I look at the continuity equation in the flux form, then the metric derivative is less than, so this is a metric derivative in the slice-washer sense, is less than what this quadratic form is giving us. And on the flip side, right, if there exists, so given an absolutely continuous absolute continuities with respect to slice Wasserstein metric, there exists a unique element. So, basically, in the Wasserstein case, you're looking at In the Wassersten case, you're looking at gradient vector fields and their closure, right? So, what here you're looking at is something similar to that, but the closure is somehow more complicated because it, right, you know, it is given by this formula over here. So you take the function phi, you take its gradient, its Radon transform, then you take the one-dimensional lambda operator, right? You multiply by the Radon transform. By the Radon transform, the density, you take the dual, you hit it with another lambda, and okay, this is what it is, right? So we can, you know, these elements, right, they are not, you know, the difficulty that somehow arises here, these elements are not even measures, right? So this J, right, they are elements of some very weak, you know, the dual of a high-dimensional subolar space, right? So in principle, it is difficult. So, in principle, it is difficult somehow to work with these objects. This regularity kind of is not optimal, right? We are missing actually one half of the power here. But, okay, this is what we were able to obtain. What is more important is that, okay, with these two inequality, with these two inequalities, you get the equality, namely, there exists a tangential vector field in this flux sense. Now, so this is somehow just about the velocity of the curves, but let's somehow understand the geometry, right? But let's somehow understand the geometry, right? So we are given the slice-faster stand metric, right? But it's not clear immediately, right, if this is a length space. So, namely, do there exist geodesics, right? Or even does there exist a sequence of paths which minimizes, you know, so which in the limit minimizes the slice fossil distance and achieves basically the slice fusion distance. Now, I'll try to convince you here that this is not the case, right? So you see, if I would like to have the you know, because You know, because the slice-wasserstein metric is just the average of the Wasserstein distances. So, if I want to interpolate, then in every dimension, I would need to interpolate in the Wasserstein sense. So, I know what should be the Radon transform. So, basically, the Radon transform of my interpolation should be really nothing else but the Radon transform of the, you know, of the, sorry, it should be the Wasserstein interpolation of Radon transforms in every direction. Radon transforms in every direction. So, in this way, you do get some one-dimensional, you get an element of this space of functions in the theta p variables, but is its inverse going to be a probability measure? And somehow the answer is no, right? So, there are many functions in the User-Radon transform is always going to be a probability measure, but which are not actually probability measure. But which are not actually probability measures. So, let me try to convince you of that somehow very quickly. So, let's look at the characteristic function of a ball in Rd. So, I'm really drawing just one-dimensional profile, right? So, if you do a Radon transform here, right, you know, let's say 2D, you'll get really the upper half sigma. So, this is the primal domain, this is mu, and you know, this would be half because you know, when you're projecting, right, remember, this is a circle, right? This is a Circle, right? This is a disk, right? And you're projecting here, you get more mass than towards the side. Okay, but now let me actually drill a tiny hole here so that, and okay, let me add a little bit so that it still has total mass being equal to one, but it's no longer a probability measure. So if I do or I don't transform here, what I will obtain is something like this, right? So you get a positive measure. This is radioisometric, so in every direction, you get the same, right? The same, right? So basically, there are many measures, which total mass one, which are not probability measures, which are such that for every theta, the Radon transform is a probability measure. And that is actually what happens here. So we have examples in the paper. It's not difficult somehow really to think of it, right? So there are, you know, basically there are no geodesics in the Vashvin space. If you allow for sign measures, then yeah, sign. Or sign measures, then yeah, sign measures with respect to slice-lasserstein would be, you know, in some sense a DZ length space, but this is not really somehow so interesting. So then, okay, so then what can we say about really, you know, what are the, you know, somehow shortest paths in the Wasserstein sense? So one kind of warning, right, and this is right, so, well, okay, so if you have in general, if you have a metric space, right, and you know, you have, you know, continuous curves of it, you can talk about. You know, continuous curves of it, you can talk about the length space generated by that metric space. So now you can talk about what is the minimal distance achieved by paths by connecting two points by IC curves in the slice-blastersten case. And well, okay, so we can define this LSW and we'll study this LSW. But before that, another warning, right, is if you look at just point measures, right? And if I, this is something that was noted actually by Filippo in his book, right? By Filippo in his book, right? So, if I look at just point measures and I move them by some velocity, and I'm wondering what is the slice Wasserstein velocity, so note that then by looking at various projections, I can actually recover on the average the velocity of each of the points. And thus, for those curves by, you know, obtained by moving point masses, you get that slice-lasserstein metric. The derivative is nothing else but one over the dimension time. Is nothing else but one over the dimension times the Wasserstein, somehow metric derivative, right? So for point masses, you basically behave geometrically. Again, if I look at slice Wasserstein distance restricted to the space of point masses, this is the same as Wasserstein distance up to this constant. But we are not interested in that sub-manifold. So in particular, right, so on the flip side, what happens when we look at very smooth measures? So let me look at measures, right, which are supported on some nice domain omega. Some nice domain omega, right, and are bounded from above and below by the Lebesgue measure. I think ν also needed to be bounded by, you know, I have both bound same bounds, I think, for both measures. In that case, both slice Wasserstein and Wasserstein, okay, so in general, I have this inequality. And then, if we furthermore assume, actually, you know, in this second case, is that the measures are actually supported away, so let's say. Away, so let's say here I have uh uh here I have a ball and I have two measures which differ only in the interior of the ball, right? So they differ somehow only away from the boundary, right? Then both the Wasserstein, both the slice-Wasserstein and the length space corresponding to the slice-Wasserstein are actually metrically equivalent up to a constant to the homogeneous negative sublime. Uh, uh, sub-lock space. So, this is a kind of mirrors what happens with the Wasserstein metric, right? So, if you look at very smooth measures, then Wasserstein is like h minus one, right? There is this very nice paper by René Peyret, right? Then what we have for very smooth measures, slice Wasserstein behaves like negative Sobole norm, right, with much higher, you know, more negative. And for discrete measures, it behaves more or less like Vossish time, right? So. Right, so yeah, and okay, what we also have is that if one is looking at this, looking at this length space, right, so right, one still has this parametric somehow approximation property, right? So even if you, you know, basically you can build an AC curve between any two measures in the slice Flasherstein space where this distance would be approximated somehow. Where, so sorry, you know, where you can approximate. Where, so sorry, you know, where you can approximate any measure with the error. Well, we have to pay a logarithm, but it's more or less somehow uh parametric error. Okay, so uh so right, what about somehow gradient flows in this uh metric? So if one wants to write, you know, take you take an energy E, the formal slice Wasserstein gradient flow is the following equation. So this is actually, you know, this is a pseudo-differential operator. So this would be the Wasserstein gradient, but then you take the But then you take the Radon transform, then you hit it with these objects that pinch with this previously, and the continuity, you know, this is the flux that you obtain, and the continuity equation that arises is going to be the slice-fascist ingredient flow. So formally looking, these are non-local equations, right? Because they involve all these non-local operators, right? Namely Radon transform and its dual. The order of the operator is actually D. So this is exactly what we would expect, right? What we would expect, right? If you look, you know, if you look at the negative Sovolev gradient flow, what you would obtain is a higher-order partial differential equation, and something similar is happening here. We did not actually study this object, right? Again, we wanted to, we were interested in doing things in high dimensions, and I want to have an effective way to approximate this somehow in high dimensions. What I just want to kind of, you know, I want to actually make the following point, which, okay, let's go through this slide, right? Let's go through this slide, right? So, first of all, if you have two metrics such that the metric derivative with both of the measures is the same, right, then if you have a gradient flow with the kind of, because there is a, you know, SW is less than LSW, right? So if you have a gradient flow with LS with SW, it is immediately gradient flow with LSW. On the flip side, if the energy would admit an evolution variation. admit an evolution variation inequality with respect to LSW, then this would be a curve of maximal slope with SW. So morally, right, these gradient flows should be equivalent. But the problem with either of them is actually the following, right? So let's consider a very simple energy, namely a potential energy with a, let's say, a smooth potential U, right? So what I'm claiming, right? It's okay. So a simple thing to, you know, this is a trivial observation. So in general, the Vassage time grading flow. In general, the Wasserstein gradient flow and LSW gradient flow do not coincide, right? So, let's say you take a very smooth measure, we know what Wasserstein measure is. Wasserstein gradient flow is just here, you have the gradient of E done, whereas this operator is giving you some different flux when you're dealing with continuum measures. On the other hand, if you're dealing with point masses, then both the metric derivative and the metric derivative and the metric slope actually coincide. So for the Wasserstein and LSW and also for SW, which means that for point particles, the Wasserstein gradient flow and the LSW gradient flow actually coincide up to a constant factor, right? So basically near the points, this is really behaving like Wasserstein and the gradient flows coincide. I mean, behind it is actually this inequality. actually this inequality here. So actually what we can show is that when you are near, so when in when in infinity Wasserstein sense you are near the set of point masses, then the slice Wasserstein and the Wasserstein divided by one over D are actually very close to each other. Note that this is, they're not just close to each other, they're close to each other in a relative sense, right? Because here I could divide by SW2. So again, as I said, but here it's written the Uh, said, but here it's written the metrics laws coincide and the uh the gradient flows are the same. So, for so then it makes clear that actually slice suspend flow gradient flow doesn't really make sense and it's horribly unstable, right? Or at least point-wise point mass approximation do not make sense because since for point masses they coincide, then if I take a limit in the weak topology, I would in the limit of slice Wassersten gradient flows, I would like recover the Wassersten gradient flow and not the continuum. Gradient flow and not the continuum slice Fassersting gradient flow. So, these basically the point here is not that this equation doesn't make sense, it is that this geometric viewpoint via slice Fasserstein is not the right way to look at it that one would need to study in kind of using PD tools somehow directly. So, okay, so thus far, I studied the geometry of slice flustering and what I concluded is the slice faster than gradient flows are not somehow the way to go. So, then what are the gradient flows? Way to go. So, then, what are the grading flows that one could do in high dimensions, right, and still be able to approximate the answers accurately in high dimensions? So, okay, let me somehow introduce some of the proposals that could be done for that. I need the reproducing kernel-Hilbert spaces. So, what are reproducing kernel-Hilbert spaces? They're just Hilbert spaces where the pointwise evaluation is a continuum operator. In particular, sufficiently regular Sovoli spaces are RKHS. When you have RKHS, then RKHS, then by Reese representation theorem, right, there exists an element of the space such that the inner product with that element gives you the pointwise representation. The inner product of these, these are called feature maps, so these feature maps gives you the kernel. This kernel is a positive definite, right? And for a function f, given, you know, with this convolution of k with a measure, we have that its norm in the RKHS is going to be really nothing else but the convolution with k. But the convolution with k, and in a spectral way, it can be uh written like this. So, typically, right, you know, you know, if k is a Gaussian, right, this would be uh, you know, this is really just a weighted, you know, solvolar space, right, in the, you know, kind of in this, you know, the weighted L2 space in the spectral representation. And, you know, typically these are rather somehow smooth functions. The dual norm of the RKHS, of an RKHS in statistics, or you know, it's called the maximum mean discrepancy. You know, it's called the maximum mean discrepancy, right? So, basically, here you're really just looking at what is the difference of the integration, the you know, the maximum difference of the integrals in over all functions that belong to the unit ball in a reproducing kernel Hilbert space. What is really nice that this maximum mean discrepancy can be written as this double interaction integral between difference of the measures, right? So, note that this, okay, so let me not get into this. This, okay, so let me not get into this. It has connections, a number of connections with the non-local interaction equations. But here, okay, I would just notice that in terms of m and d, one can approximate measures again somehow parametrically, right? And okay, so what are the gradient flows that we've been considered? So here is very briefly, you know, for the relative entropy, right? The gradient flow of the relative entropy with the Bushstein metric, of course, is the Fokker-Planck equation. But now, what if instead of the Instead of the L2 norm, the weighted L2 norm, we replace the geometry by unreproducing kernel Hilbert space, right? Then the Rayleigh corresponding Rayleigh function, again, instead of the weighted L2 norm, would have the RKHS norm. And if you look at the gradient flow in that sense, one obtains the following equation. So it has a very similar form. So this is what is called the stochastic variational gradient descent. So I feel that, okay, I kind of have been Kind of, I have been talking to, you know, spending some time talking to people from machine learning and so on, and I found kind of some or you know, encountered some equations that I think are interesting for our community. So I'm, you know, a little bit showing some of the interesting objects, which I think are deserving of a more careful look. Anyways, this, you know, you could see that this is really nothing but, in some sense, a smoothed Fokker-Planck equation. But unlike the Fokker-Planck equation, right? like the Fokker-Planck equation, right? What is interesting is that this equation would make the right-hand side would make sense if I would plug in rho to be a point particle measure, right? So this is kind of a it kind of serves as a dual to you know the approach that Katie, Francesco, and Jose Antonio had for this blob method for diffusions, right? Where I have a deterministic particle method which approximates Which approximates Fokker-Planck equation. In this case, on the continuum level, so there, on the continuum level, you change the equation, you introduce a bias or an error in what your minimizer would be, but you preserve the gradient flow structure on the discrete level here. On the continuum level, you have no bias, but this is no longer a gradient flow. Of course, for discrete measures, the relative entropy doesn't make sense, but there are approximation properties. So, namely, right, this is a very nice paper by Lu Lu. This is a very nice paper by Lulu and Nolan, who showed that, you know, for one thing, right, is that in time, right, this converges perhaps very slowly towards the desired target measure or the minimizer of the relative entropy. And right, so linearization indicates, because this is a compact, the linearized operator is a compact operator, there is no, of course, gap in the spectrum. But what also one also has is that on finite time intervals, On finite time intervals, one actually has the convergence of particle approximations, so namely solutions of this OD towards the SVGD. Now, one problem with the SVGD is that it still does not, you know, it makes sense for particles, right? But it does not behave well in high dimensions. The reason for that is that doing this well, you know, you want this difference to be, you know, small if you want to approximate well. But the problem is that if you But the problem is that if you want to evaluate convolutions, right, this suffers from the cursor dimensionality. So, if you look at you know those kernel density estimators, right, you need again a number of points that grows exponentially with respect to dimension to be able to allow this thing here. So basically, even though you smoothed out the velocity, you did them in kind of in this radial way. And as soon as you kind of start doing any convolutions, then this suffers some crossover-dimensionality. And this is actually something very visible, you know. And this is actually something very visible. You know, there are these papers in statistics where they exhibit what they call a variance collapse. That kind of you're underestimating the variance of the target distribution horribly if you use SVGD. But in moderate dimensions, it somehow still behaves very well. Okay, so here I just want to make a remark is that I view this in statistics, it is very often important. Is very often important, you know. The desire is very often to obtain samples out of a given measure, right? So, let's say if you have a measure mu, which you only know through a potential, right? So, you know that your mu is, you know, Gibbs measure of somehow form, mu is like E minus U, where U is known to you, but you don't know the prefactor in order to generate samples out of this. This is actually an important problem on an industrial scale. You know, kind of standard approaches are, you know, we have some Markov chain, Monte Carlo, Hamiltonian, Monte Carlo methods, and similar. So each particle is running independently, right? But this can be seen actually as another approach of sampling, right, which is closer to us, which is we are not moving one particle at a time, but we have an interacting system of particles which tries to approximate the target measure. And I could, you know, discuss later outside of this talk as to why I think. Outside of this talk, as to why I think this is advantageous. So, if we want to see how well this method approach is working, right? So, the black line are the, so these are low dimensions, so dimension two, three, and four, and the target measure is as simple as can be, so namely Gaussian. So, the black line is the error of the IID. So, the perfect answer that kind of those other sampling algorithms could get of the IID sample. But if you look at kind of the error measured in this MMD sense, so again, Measured in this MMD sense. So, again, minimum of the integration error over some class of functions, which is again something relevant for downstream tasks, we see that the error is actually much, much lower than for the IID samples, right? So as an ensemble, they approximate the target measure better. Can one prove this? This is certainly a challenge. So it's an open problem to prove this. One can also do that in the slice washerstein, right? Wasserstein, right? So, and again, the approximation error is improved. But I just said that, you know, SUGD doesn't work in high dimensions. So, what is our proposal for working in high dimensions? Well, the proposal is actually even further restricting the class of velocities that you allow. So, the name of the game is, okay, you want a gradient flow that you can approximate in high dimensions. You want to have a set of velocity that can be approximated effectively in high dimensions. So, let me restrict the velocities to the following. Velocities to the following, right? So, first, let me introduce a family of semi-metrics on the tangent space. So, as the following. So, this is, you know, I'm looking at the kinetic energy like in Wasserstein, right? So, I'm actually looking at the Wasserstein tangent velocity, but I'm restricting the velocities to only those vector fields which take the direction theta and are actually have the same value across the perpendicular. Value across the perpendicular somehow line. So basically, velocities of all the particles that project the same point for this direction theta needs to be the same. So this is obviously a very restrictive class of velocities, but that's then not exactly what we look at. What we look at is now I take the inf convolution basically of all of these velocities. So basically I look at what would be, so given, you know, I'm looking at those velocities. So, given I'm looking at those velocities v that can be decomposed as an integral of such one-dimensional velocities, and then we are looking at what is the total somehow length of all those vector fields which are used in this decomposition. So, if one obtains this, such a metric, right, is actually comparable to the Wasserstein metric, but it's not a geodesic metric. There are very often there are no geodesics. But what one can do is write on a gradient flow and one obtains actually. Right on a gradient flow, and one obtains actually a very you know interesting object. So, one obtains a non-local somehow equation where the velocity is given as following. So, you project your density, then you take the logarithm of the density, and right then, you know, basically this is basically the diffusion theorem in 1D that you would get out of the projected density, and then you also project all of the. You also project all of the gradients that you pick up along this. You basically take the average over that slice of all of the gradients of your velocity again, project it in this direction, theta. Now, what is somehow nice about this? Okay, so one obtains, right, so this is a non-local again equation, right? We actually did not yet show the existence, we focused on other issues, but I do do not think that the existence itself is actually very difficult to prove. Difficult to prove. What is interesting about it is that this can now be fully evaluated just based on the projections. So, again, you know, this is one thing that is in general true, right? If you look at the gradient flow with respect to inf convolution, you're getting the average of the gradient flows. So, this is actually the average of the gradient flows with all of those semi-metrics. But what is kind of a beautiful thing is that all of these objects that I need to evaluate the velocity are just one-dimensional, sorry, here, are just one-dimensional. Sorry, here are just one-dimensional object available from me from each of the projections. So then, what one needs to do this, okay, when I need to do this on a level of the approximation, once I project, then I can approximate the projection density like we did at the beginning for the slice faster time, right? So in 1D, right, I could use, now here I do have a convolution, but this is a convolution on a one-dimensional direction after the projection, right? And, you know, this OD can be written in what is, you know, can. Can be written, and what is kind of wonderful about it is that usually in these particle methods, you need to compute the interactions, and that in itself is a step of order n squared, just computing all the pairwise distances. Even if you have a very local kernel, just to realize who are your neighbors. Here, after the projection, I can sort these points with basically n log n computations. I can look at a relatively small window, and the computation is more or less of order of n. Less of order of n. So each step is extremely cheap to do. I sample one direction at a time, you know, like in the stochastic gradient descent or in the random batch method. And one obtains that this actually both works very fast. You know, it is substantially faster than the, so what is this? Is this speed or accuracy? Yeah. So in terms of, right, so the accuracy as a function of time, it works much faster than the SVGD, but the kind of samples that it The kind of samples that it obtains actually have an error better than the IID samples. So, again, black line is IID samples. The samples which we obtain somehow are below. And there are other, anyways, there are other metrics and other gradient flows, which also can be approximated in high dimensions. What is kind of, what are the, you know, somehow open questions, right? So the open questions are really, what are the convergence? Really, what are the convergence properties of these gradient holes? They will not converge exponentially, but can one show that, you know, along most of the directions or kind of on a large scale, that they will converge to being relatively close. We are really interested in the early phase of convergence rather than so much at the late phase. And somehow, in general, right, you know, finding appropriate gradient flows, which are both approximably high dimensions and have good conversion properties, I think pretty open with. I think a pretty open mathematical question. So, with this, I think it's a good time to start. For the numerical experiments, did we use Gaussians for the targets? For the targets. For this, yes, but we have later some, you know, we have, I have used a variety of very unique. Used a variety of very anisotropic distributions and it did work well, right? I'm happy to. I did not, yeah, I did not look at things which are supported really near a very low-dimensional manifold. That's kind of a thing we see that we can, I can show you. Okay, thank you. So, thanks. It's very interesting what we pointed out in the beginning, the first part of the talk about the fact that the German clause do not coincide with LSW and W, except in the discrete case. Except in the discrete case. So, what do you mean when you look at the guardian flow for SW? Do you mean a curve that satisfies the energy distribution condition? So, right, okay, this one here is written formally, right? So, which is basically, I'm looking at the formally remaining, so basically I on the level of autocalculus, right? So, I have my inner product, right? We identified what this is, right? So, you know, this is what this is what. So, you know, this is what you obtain, right? Is there a hope that maybe the JKO scheme for the SW converges anyway to the guardian flow W? Oh, you mean even for these measures? I don't think that that would be the case. I would believe that when you start with, because if I started with a very smooth measure, then it really behaves like a negative sub-alloid. And I believe that if you did the gradient flow in negative. And I believe that if you did the gradient flow in negative subl using JPO, you would get the right thing. So just a contribution to this question, in fact. But then what would you expect in terms of continuity with respect to initiatives? I believe it should behave well. Aha, right. Well, certainly don't look at particles. I mean, you would get a high-order PD, right? Which may be, you know. You know, so it should, you know, if you start with a delta mass, you should smooth it out very quickly, right? You know, kind of, yeah. More comments? Just curiosity. Formally, you've got to draw the PDE that you get is that one, right? Yeah. Can you get, for example, the integrations or some classical example and check that they are the same? And check if they are the same or different, what happens? So, can I, uh, right? Yeah, can I, you know, right, you know, like for any energy, can I write what this object is, right? You mean, you let's say all of these operators are explicit, right? So, kind of if you could write this out, do I have any insights? I have none, right? I mean, the kind of I really are not, you know, on my kind of for our purpose, like looking at this piece. Of for our purpose, like looking at this PD in kind of high-dimensional hopeless, because then it seems that I need to resolve the space. So, kind of, I this was for us telling us this is not what we should be looking at. For me, this was kind of the end of the road. It's a kind of an interesting observation, right, that you know, this you know, this slice was extreme doesn't give you anything lambda convex, right? But, you know, I did not look further into this. Yes, yes. So, I was talking with June about this distance, and he was saying that. This and he was saying that so this non-geodesic, uh, you could embed it to a larger space in which you consider uh sort of measures on R cross the sphere. And then on that space, you can consider this metric and it's geodesic there. And so it's like having like the sphere in Rn with the ambient component, like the ambient metric. And so then the geodesics are going to the that's what he was saying about this. That's what he was saying about this. And he was also writing some more general comparisons of it. So I'm not, this sounds very interesting. I'm not aware of it. Again, if you looked at sign measures, then you would get a recovery. But it's not interesting, right? Because I don't, I have negative mass. But this sounds more interesting, right? Yeah, you preserve positivity is just no longer the Launch House Bonopo measure, the LP measure that's. Yeah, go stop. Any other comments, question? I didn't understand the speaker in.