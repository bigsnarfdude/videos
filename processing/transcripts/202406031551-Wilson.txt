This question of, like, you know, how do we understand what humans do? And in particular, I'm going to assume that in many decision pipelines, a human is going to retain control. I think this is just realistic. So we, you know, my premise is that we know humans are often biased, statistical processors. This is kind of what Ben set up, that often they do worse than the actual aerial method. But we also often see that when we have to have a human in control and we provide them with the model, they can do worse than the model alone. Worse than the model alone. And this happens even if we know they have, you know, useful, relevant information. And, you know, of course, we can't always get rid of them for legal reasons, accountability reasons. I'll talk about other reasons later, perhaps, in the talk. But kind of my premise is that humans are often here to stay. And that's frustrating. But it also explains why in the last five years or so there's been this sort of renewed interest in studying model assisted human decisions. Model-assisted human decisions. So there's even a call for sort of a new science of model-assisted decision-making. This has been studied since the 70s or so. But I think there is reason to care about how are humans using model predictions in order to make decisions. And we need to understand that in order to intervene to design, for instance, better interfaces for them. I'm going to assume for the most part that we are not going to consider that we could redesign. To consider that we could redesign the entire decision pipeline. Sometimes, in medical settings, for instance, there are proprietary models that would be very hard to just change completely. So, yeah, so we need studies to understand how are humans using model predictions, how can we make them better. But having looked at a lot of this kind of research, I want to focus on a few ways in which I think we hold ourselves back when we're trying to understand what are people doing with predictions. And so, the first kind of problem I see, I'm going to call Kind of problem I see, I'm going to call it the people are stuffies approach. And I see this a lot more in the kind of HCI, human-computer interaction sort of approach to studying people using models. But I also see this when AI and ML researchers want to kind of take a more human-centered approach. And I think the problem is that there's often this assumption that because we know people are biased and irrational, we don't really need to provide a well-defined problem when we study them. We can sort of study humans, you know, interpreting whatever problem they're solving as. Interpreting whatever problem they're solving as they will in some study, or perhaps we can gather evaluation data from the wild. But we basically can sort of assume that their intuitions and their biases are worth study without actually sort of making sure that we understand the problem they're solving or that we're giving them a well-defined problem. And the reason I don't think we can ever really get signal when we're sort of assuming that we don't have to really define the problem rigorously is because humans might be solving many. Because humans might be solving many different problems. They're given a model, they're making some decisions, maybe in our study. And if we don't tell them exactly how they're being scored, what's the value of different types of errors, if we don't tell them things about the model and its expected performance a priori, then they might be filling in the gaps in all sorts of ways. And so I think it's sort of, and of course, another problem is that as researchers, when we go to analyze how well are people making decisions, we're often Well, are people making decisions? We're often assuming that we have some problem definition that we understand. And so, there's often, I think, a potential mismatch. And so, when we have this, you know, we can't really conclude that people are even biased if we're trying to understand, you know, are people making worse decisions than the model would alone? And we can't identify ways to improve their behavior, which is sort of the goal in a lot of this type of research. And so, as a very first step, I think when we're studying humans using model predictions, we have to make sure that we've got a well-defined. To make sure that we've got a well-defined problem and we've communicated that to people. And if we're trying to study, you know, actual data from the wild, we need to make sure we really understand the problem as they understand it. So statistical decision theory, I think, is a natural sort of framework for thinking about, you know, the problem of a human with access to some statistical information, making some decision. And so a problem can be defined first by an action space. So what are the actions the human is choosing between? The human generally The human generally, the quality of their decision is going to be defined both by the action they choose, but also by the realization of some payoff-relevant state that's usually uncertain at the time of the decision. People have access to some information, some signals, which might include, for instance, you know, if it's a doctor making diagnosis decisions, you know, the medical images that they're using, the patient profile, but also optionally things that we might care about intervening on, like how do we explain model predictions? So, different types of explanations. Predictions, so different types of explanations. But when we have a signal, you know, we can think of that as inducing kind of this structure that the decision maker is trying to learn from. And so, you can think of like the signaling policy as inducing an information structure, which is basically a joint distribution over the signals and the payoff relevant state. And this is what the decision maker is sort of learning from. And once we define a problem like this, one of the values of doing this is that first it allows us to sort of set up kind of a scale for what's attainable. Scale for what's attainable for this decision problem. Like, how well could a human, given this statistical information, possibly do? And we can do that. The kind of method that I've been using in my own work that I think can take up as part of the way is to identify or to think about a rational agent. So what would the rational decision maker facing this problem do? With this signal, how would they optimally learn and make the utility optimal decision? And we can think about whatever our scoring rule, like however we want to weigh false positives and false negatives. Way false positives and false negatives, you know, what's the expected score of that decision maker relative to what we actually see from humans using the same statistical information on the same problem? And so, to make this a little more concrete, I'm going to just think about specifically this research we see on like designing for complementarity. So, we have a human using some AI, and we want the team, the two of them, to do better than either would alone. And so, the typical sort of workflow we assume is that. And so the typical sort of workflow we assume is that a human has their own notion of what the prediction is based on their own internal model of the data generating process. They're going to have access to the AI prediction and then make the final decision. And if we look at this literature on how we study model-assisted decisions to achieve complementarity, this conventional definition people use of appropriate reliance is sort of naive. So it's generally assumed that the human goes with the AI prediction when it's correct. Goes with the AI prediction when it's correct and rejects it when it's wrong, then that's appropriate reliance. And probably in this crowd, I don't need to spend a lot of time talking about why that's pretty problematic. So, for one, it's potentially not attainable. So, because often there's limited information that the human has when they make these predictions, it's going to treat all errors the same because we're often just looking at accuracy, which is often not realistic in specific domains. And, you know, it's equally going to penalize a bad decision when the human and the AI are, you know. Decision when the human and the AI are almost very similar in terms of their probability of making the right decision versus they're very different. And so, what we actually, I think, want to know when we study humans assisted by AIs and we care about getting the best possible performance is first, how well does the model-assisted human do, of course, but also how well could they do? We need some sort of scale to make sense of what we're seeing, and why do they mess up? And so, if we think about sort of expected performance. Sort of expected performance of this human plus an AI relative to the human alone and the AI alone, it really matters, for instance, is the human almost attaining as much of the possible sort of performance in this setting as they could, or is there still a long way to go? And so I don't think I have all my tweets on. Oh, no, you're fine. Is this your last one? No. Oh. A couple of minutes long. Yeah, a couple minutes. Okay. So I'm just going to skip. A couple of minutes. Okay. So I'm just going to skip over this. Again, we have a paper where we show, you know, you can take this particular appropriate reliance decision and frame it as like a decision maker has access to a signal, which consists of the features of the instance, the human breach and the eye prediction, optionally some explanation, and you can actually kind of create this scale or unit that defines kind of the information value that the human adds to having the better of the two alone. And I will leave that, and you can decompose the loss that you see. That and you can decompose the loss that you see. So, anyway, my higher-level point, though, is that the first step in trying to understand how people use model predictions is to give people a well-defined problem and make sure we're analyzing the same problem. And that allows us to figure out what's achievable, how close are we getting, what are sources of loss, et cetera. But that's really, I think, only the first step because we're always making assumptions when we try to study these human or AI-assisted human decisions. Human decisions. And in particular, I think we often, in these evaluations studies we run, try to kind of artificialize the problem in a few ways that are common. So we often assume one, or we talk about just the human, so we assume all humans are interchangeable, but that's, you know, anyone who's studied behavioral research knows that that's often not the case. And I think it's very true in some of these domains we're talking about. I think we really don't spend enough time thinking about the utility function or the scoring rule that we're using. Or the scoring rule that we're using. And we should always allow for the fact that might be different. And a lot of our work about showing accuracy improvements ultimately really hinges on how we're setting up the loss function. And finally, I think the kind of interesting part, a lot of times with having a human in the loop is not, like, they're not only going to mess things up potentially. I think there is potentially a scenario where they can still help. And this is when we doubt the validity of some of our modeling assumptions. So we have some AI model and we think. So, we have some AI model, and we think maybe something's going to go wrong. It's going to encounter some instance that wasn't well represented in the training instance. And so, a human maybe actually could help here. But I think the final point I want to end on is that it can still be really hard in some cases to formalize what the human is doing, especially in these cases where we think things like the distribution might shift. And so, an example I'll give to end with is conformal prediction, which is something I've been thinking about recently. Which is something I've been thinking about recently. This is simply the idea that we want to basically create valid coverage prediction sets. So, you know, rather than just returning a single prediction from our model, we're going to return a set of labels with some user-specified coverage guarantee. So, you know, like a 95% chance on average of containing each label. And what's interesting to me is that while there's a lot of applications of conformal creation that don't involve humans, one of the motivations that you see a lot in this literature is like, this is going to be better for human decision makers. Like, simply, because we have these. Makers like because we have these guarantees, somehow this might must be better than giving humans, you know, the Tom Cape predictions, for instance. But I think what's really interesting is, you know, one, that it's hard, because it's sort of an abstention from making a prediction, ultimately, it's hard to even think about like what's the right thing to do with this prediction set. You know, a rational decision maker, it's hard to even formalize what they would do with this. It's kind of underdefined by definition. And in particular, you know. And in particular, you know, if humans are often valuable on these out-of-distribution instances, like what does it mean to say that a valid coverage, uncertainty quantification, or uncertainty set is going to be useful when the cases where we actually need the humans, that coverage guarantee might not hold. And I think it gets really interesting because I think, like, you know, many Texas algorithms are adaptive. So even if the coverage guarantee kind of breaks out of distribution, the set size might still convey information. Might still convey information. But it's really hard to sort of formalize that and make predictions when the whole point of having the human there is that we think something's going to go unexpected. And ultimately, I think it comes down to how does a human use a prediction set like is very hard to say. People reason associatively, and it's very hard to kind of formalize that. And so overall, I think my takeaways are, you know, we have to start by trying to rigorously define problems and make sure the human is solving the problem we think they are. But then we also have to ultimately remember that, you know, what we're actually Ultimately, remember that what we're actually trying to study is often right outside of what we want to normalize when it comes to people. So, anyway, the process that I found works is sort of try to understand the context of the problem, define it or rigorously define it, communicate it to the people, and then use the gap to drive what you do next. So, anyway, that's all I have. Thank you.