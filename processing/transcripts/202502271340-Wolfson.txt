Everybody said that, it failed. So we'll see whether I'm able to do that successfully. But I'm going to be talking about some of the work that I've done related to understanding sort of everyday life. And I'm going to mix in some of the work that we're doing sort of methodologically and also a little bit about the data themselves. I have to have a disclosure slide because, as many of you know, I'm involved in a company where we basically offer data collection services to researchers to collect these kind of data. To collect these kinds of data. So the data collection piece is handled by the company, and then I'm involved on the academic side analyzing the data. And the data that we're working with is really about answering the question, what did you do today? Now, I think a lot of us are collecting data that try to answer this question, but what we're focusing on is data that's like actually matches what you conventionally answer what somebody asks, like, what did you do today? Right? Like, well, I started the day at home, and then I drove to work, and then I was at work, and then I went out for lunch, right? So that's what I talk about. For lunch, right? So that's when I talk about daily activity patterns or daily habits, that's the kind of data that I'm talking about. So that's sort of like just what are you doing with your life, essentially? That's the kind of data that we're interested in collecting. Now, there's a lot of people who have thought about how to structure these kind of data and how to talk about these kind of data. Obviously, you've got folks who are very interested in activities of daily living, so having to do often with home-associated activities that have to do with, you know, are you able to feed your To do with, you know, are you able to feed yourself? Are you able to clothe yourself? Various things like that. How much time do you spend at home, etc. There's other folks who care a lot about this idea of community participation. So how much are you able to get out of your house and connect with other people in your community? To what extent do you go to social events? Do you go shopping for yourself, et cetera, et cetera? So there's this notion of community participation. And then folks who are often sort of the social science side think a lot about life-space mobility. So how do you move through the world, right? How do we move through the world, right? Putting all those concepts together. And so, those kind of data, again, I'm going to make a pitch which I hopefully won't be too difficult to make, is that data like this could be very potentially useful for health research for a number of reasons. The kind of patterns that we see about your daily activities could predict adverse health outcomes earlier. They could provide more sensitive outcomes for assessing intervention effects because we haven't traditionally measured them. They could be important effect modifiers for a lot of interesting interventions. They could predict treatment header. Interventions they could predict treatment heterogeneity and they could provide potentially vital context for other sensor-based measures, right? We have streams of data from accelerometers, CGMs, which could potentially really benefit from this sort of what were you doing when this measurement had information. So the work that I'm going to talk about very briefly today to sort of frame up the type of usefulness you can get out of this data is an ongoing project around the idea of measuring time version in cancer care. Of measuring time burden in cancer care, which has been sort of coined this notion of time toxicity, right? So we know a lot about the physical toxicity of cancer care. People have been working on the financial toxicity, but of course, just undergoing cancer care is just really time-intensive and very disruptive to your everyday life, right? And so part of the research, we got this R01 to study the time demands of cancer care. And of course, the big challenge in trying to get that is that these are folks that are undergoing cancer treatment, and so you cannot ask them to fill out. And so, you cannot ask them to fill out a 30-minute instrument every day that asks them to recount everything they did during the day, right? So, we need to reduce the burden of data collection for folks so that we can get some information about what their everyday activities look like and hence how those things might be disrupted by undergoing cancer care. So, just to give you an example, this is actually data that's really kind of hot off the press. I'm only showing you information from 20 of the participants we have in the study. We had about 80 people total participate in the study. Total participate in the study. But just as an example of the kind of data that we get and the insights it can provide, what we collected on these individuals is information about their visits to medical clinics. So we sort of automatically detected when these folks were visiting medical clinics. And so we were able to record the time traveling to those clinics. From self-report, we were able to get information about how much time they spent waiting and how much time they spent receiving care. So one of the interesting things you can see here, for example, at every row of this plot is a visit. So some of Visit. So, most people had multiple visits over the course of the study. So, there's, you know, this is maybe about 50 rows of data from 20 people. And what you see here is that if you look somewhere around the middle, you see that maybe about half the visits, or at least down here, half the visits, more than half of the time spent dedicated to this quote-unquote medical visit was not spent receiving care. It was spent traveling to the clinic, waiting for service, traveling back from the clinic. Traveling back from the market, right? So, in terms of understanding burden, it's very helpful to have this kind of information. So, this is just a small piece of the kind of data that we can get, but again, a lot of this data was automatically sensed from the smartphone. The only thing that we had to gather from people was a single response about the waiting time that they had and a single response about the, and that was really it. All we had was the waiting time. We can infer the travel time, we can infer the care time as the non-waiting time. So, we don't, it's a really Non-waiting time. So, we don't, it's a relatively low-burden way to get something that's very interesting, which could potentially be actionable in terms of delivering to health systems in the future. So, that's ongoing work. We just finished data collection, and we're going to keep working on this data. So, as I mentioned, the big challenge with capturing human activity is the burden. This is a screenshot from the data collection instrument for the American Time Use Survey. This is where somebody from the Bureau of Labor Statistics, whoever administers the Time Use Survey, would call somebody up and go through this data collection. Call somebody up and go through this data collection process where they ask you, Okay, so you woke up in the morning. What's the first thing you did? Okay, well, you got out of bed, you went to brush your teeth, etc. Right? So, you walk through this very detailed time use inventory. It takes about 30 minutes to an hour for this to be done for one single day. So, as a result, the American Time Use Survey only collects 24 hours of data per person. But when we want to understand people's daily activities, taking one day out of their entire year is really not a very good slide. So, we need much more information than that. A very good slide. So, we need much more information than that in order to be able to make useful inferences about patterns. So, we really need to do something to reduce the burden. So, this is the joke slide, right? You know, the infomercial, there must be a better way to do that somehow. And so, of course, we all walk around with our smartphones, right? Increasingly, even in populations that have not traditionally had access to smartphones, we use them. We're seeing increased penetration in those groups. And so, of course, as we go through the world with our smartphones, we get information about GPS, we get information. Information about GPS, we get information about your speed, we get information about linear acceleration, and all of that can kind of be put together to create essentially a picture of what you were doing during their day. So that yellow arrow represents about 10 years of work on our side to convert all this data into this app that collects this kind of calendar view of how people are spending their time. I'll just mention that one of the nice things about this, so there's sort of two nice things about this, but one nice thing about being able to put the data into this comprehensive The data into this comprehensive view is it allows us to solicit information in a much more efficient way. Right? It's a lot easier to report about what happened at a medical visit if we say, we saw that you had it, it looked like you had a medical visit between 2.15 and 3.30 today. Please give us some information about that medical visit and it's structured in that way. Rather than if you ask somebody at the end of the day, did you have a medical visit? What time was it? How long did it last? Right? That adds a lot of burden. So if you can prompt, if you can pre-populate a lot of that, that helps. Pre-populate a lot of that, that helps a lot in terms of data collection. So we can attach a lot of rich information to data with the structure. The other thing that's really nice about this is that having thought about how to present a unified structure, as a statistician, I'm always thinking about how I'm actually going to analyze the data. So we're thinking as we put this together in this interface, what the data that come out of it are going to look like. And so we go from this rather raw accelerometer, TPS, et cetera. Accelerometer, TPS, et cetera, data to what is a very structured data set that comes immediately out of the app. So, this is an example of the data that actually comes straight out of the app to the researcher. So, it's this episodic view where it says, you know, from 10.03 p.m. to 11.49 the next morning, you were at home, then you were in the car for this long, et cetera, in this sequential way. And there's a lot of other information that's attached to that that you're not seeing here, right? You've got actual trajectories, you've got locations, you've got any survey information that was attached to that episode. Information that was attached to that episode, right? That's all there in additional columns. But it being structured in this way allows you to move to analysis very quickly. So the plot that you saw a couple of slides ago, it was literally like an hour to two hours from the data that came off the app to making that plot. So the highly processed nature is very useful for researchers. I'm going to talk a little bit about some statistical perspectives on these data. One of the things that got me interested in working on this was that they really have some very interesting stuff. Was that they really have some very interesting statistical aspects, right? From a sort of statistical perspective, we have these episodic things where you have these both continuous and categorical features, right? The durations of the episodes are, of course, continuous. An episode can last 17 minutes, it can last seven hours, right? But the actual state that you're in, of course, is discrete. So you've got this discrete and continuous time. People's days are highly structured, right? Most people's days kind of have. Most people's days kind of have the same rhythms, like you're at home and then you go to work and you come back, right? That's pretty common, but there is also, of course, a lot of substantial within and between person heterogeneity weekdays. Weekends are very different from each other, right? Night shift, day shift, very different from each other. So you've got this interesting structure, but yet a lot of heterogeneity. There's a lot of complex temporal correlation, not only on the short term, but also on the long term. If you leave for work an hour later, you might get back in the morning, you might get back an hour later. In the morning, you might get back an hour later in the evening, right? So that's a very long-term correlation that you might need to worry about. And of course, there's this issue of missing data. So your device might not have collected the information you need to make these kind of inferences. So lots of things to deal with. So we've really just been starting to dip our toes into how to work with these data. So the first thing we started thinking about is kind of representation. How do we represent a day in a way that we can actually work with statistically? And the first idea that we came up with. And the first idea that we came up with is kind of motivated by this picture, right? So you can imagine taking somebody's day and color-coding what they did during the day in this way, right? The first time I did this plot, I thought, you know, I've seen this kind of picture before, looking a lot like genetic sequences, right? It looks a lot like a genetic sequence. And so the obvious thing to do is to potentially just encode this as a sequence, right? So you can imagine chopping up the day into, say, five-minute intervals. Into say five-minute intervals, and then you just represent the status that somebody is at in each of those five-minute intervals, and now you've got a sequence. So, the great thing about this representation is we know how to do a lot of stuff with genetic sequences, right? We can do alignment, we can compute distances, we can do clustering and all that kind of stuff. So, our first work was just really just taking the standard toolbox from genetic sequences and applying it to dates. So, you can calculate alignment distances, standard optimal matching, leverage findings. Standard optimal matching leverage fine distance, which looks at the number of insertions, deletions, and substitutions between sequences. And for these, for this two sequences, you have a distance of three because there's three differences. You could accommodate sequences of different lengths if you wanted to with substitutions. It's a little hard to think about what that would mean for a day. But nevertheless, you can convert those distances into a distance matrix. Once you've got a distance matrix, you can feel hierarchical clustering. And so you can, in the sort of very standard unsupervised learning. Of very standard unsupervised learning setups start to group days according to their characteristics. So, focus probably on the first column here. On the x-axis, we have the part of the day. The y-axis is representing essentially the proportion of the time, the proportion of people who are doing that particular activity. So in the top left here, you have folks who are mainly at home during the day. This red represents home. And then group two is folks who are basically going to work, right? They're mostly at home. By the middle of the day, they're mostly at work. By the middle of the day, they're mostly at work. By the end of the day, they're home again. So that's a completely sort of unsupervised clustering. No a priori information put into this other than the similarities of the days according to that sequential distance metric. The problem with these sequence-based alignment methods is that the alignment approaches can be fairly slow. So for example, even with fairly optimized algorithms, we took a matrix with 100 daily sequences of five-minute resolution, so each of the sequences was about 288 characters. About 288 characters, and it took several minutes to do that 100 by 100 alignment. So you can imagine 100 daily sequences is not even a lot. That would be like 10 people studied for 10 days each. That's not a lot. So if you move up to 1,000 by 1,000, that's a million POI sequence alignments that you need to do in order to get your distance patient. So the problem rapidly blows up in terms of computational complexity. What we wanted to do was come up with something faster. So this is where Martha comes in. Where Martha comes in. And Martha, one of the first things she did when she came onto campus is she started working on this problem. And she came up with a really ingenious idea of thinking about these data, these sequence data, not as a sort of genetic sequence, but more like a text. And so one of the things you can do with texts is you can use a text and derive what's called an adjacency matrix. So the adjacency matrix counts the number of times two words appear adjacent to each other. So you can imagine deriving an adjacency matrix for these data. Driving an adjacency matrix for these data, which count the number of times any two states appear next to each other. So you can represent this sequence as this adjacency matrix. I'll just note for later, one of the cool things you can do with this is that the translation from sequence to adjacency matrix could easily be non-uniformly weighted by sequence position, transition types. There's a lot of flexibility in how you go from a sequence to this adjacency matrix. Lots of ways that you could imagine modifying how you make that translation, how you make that representation. You make that translation, how you make that representation. So you can take this adjacency matrix and you can derive this adjacency matrix for a whole bunch of different sequences, and then imagine stacking all those things into one big matrix by just concatenating the columns with each other. So now we have this matrix M. What we can then do is apply fairly standard S V D approaches to do a dimension reduction. So basically what we want is we want a reduced representation of this. Representation of this adjacency matrix. And we can do that. We can basically do the dimension reduction by choosing the first H columns of this V from the singular value decomposition. Once we've done that, we've got this compressed representation of this adjacency matrix, and we can just do standard multi-dimensional clustering on that reduced matrix. So we can do k-means, you can choose your favorite multi-dimensional clustering algorithm of choice. And it turns out that this adjacency matrix representation also yields useful cluster evaluations. Also, yields useful cluster evaluation metrics that you can compute relatively quickly. And of course, because you can weight that adjacency matrix translation from the sequence to the adjacency matrix, you can modify your clustering results to emphasize certain times of day, certain kinds of transitions. That can all be very easily built in by just changing the way you convert your sequence into an adjacency matrix. A fairly flexible technique. Feature of this approach: the clusters, generally. The clusters generally are fairly good, fairly interpretable. Here's an example. This is from hierarchical clustering, so you can see here we sort of mix together things that look quite different from each other, whereas this adjacency matrix clustering seems to do a very good job of sort of identifying folks who are mostly at home, you know, out in the later day, engaged in more sort of transportation. Maybe these are people on road trips, et cetera, and then folks who are in that traditional kind of work motif. So nice things. It's also Nice things. It's also way, way, way faster, so a factor of 10 to a factor of 150 times faster than hierarchical clustering. So we can now actually handle large-scale data with this approach because computationally it has huge advantages. As I said, we're just dipping our toes in the water when it comes to these data. Some of the ongoing work we have, we're very interested in doing synthesis and predictions. So some of the talks that have been given at this workshop, I think, are super. Been given at this workshop, I think, are super motivational for our work. We want to be able to synthesize realistic human activity sequence data, but in a way that we actually have some control over how they look. So, we want to be able to kind of simulate in a way to say, like, what if, could we simulate the days to look more like weekends or more like weekdays, and how do we do that while maintaining some realism? We have a paper on that that's going to be published soon. And of course, once you can do some kind of synthesis, you automatically wonder whether you can do forward-looking predictions. So, based on what you've done. Looking for predictions. So, based on what you've done up until this point of the day, can we predict what the rest of your day is going to look like? What the patterns are likely to be, potentially very useful for driving interventions. Also starting to think about X on sequence and sequence on X regression, where X might be a scalar. How can we use these human activity sequences as predictors and outcomes and regression models? How can we identify the most important parts of the data that explain differences in outcomes? So, all the standard stuff that you would ask about. Standard stuff that you would ask about sort of functional data, we want to ask about this sequential data as well. And that is that. So I managed to maybe hopefully get us kind of back on schedule, but we'd be happy to take some questions. So you talked really fast, and I've just still got to go. So what do you do with missing data, though, in this? What do you do with missing data though in this um uh in in this situation? Formula? Yeah, so you know, we because we are using fairly large scale displacements as our method for detecting activity, as long as the phone is on and has a GPS signal, we can more or less do okay. So, we it's actually pretty common for us to have. Pretty common for us to have a high percentage of participants in the study have at least like 90 to 95 percent complete dates. Right, furthermore, it's also a lot easier to imply simple kind of last observation carried forward. Like if you're home at 11 p.m. and then your phone's off until 7 a.m. the next day and you're home when it wakes up again, right? It's probably home, right? So some of the structural elements of the day allow us to make more sort of educated guesses. Sort of educated guesses about that kind of interpretation in terms of that type of invitation. I'm wondering two things. First is the New York City ride the training of this. Yep. Yep. The second thing is like with this adjacent matrix, you can include missing data patterns and then Martha's going to answer that question, right? You could include missing data as a state and include that in the adjacency matrix. There's also Adjacency matrix. There's also, like, if you just want to remove the missing data and then have a shorter sequence and then, like, reweight the adjacency matrix to be the length of a normal sequence, you can't. Then you're not taking into account the pattern of the missing data. But if you want to include the missing data as an informative pattern, you can just include that as a C. Unfortunately, you need to catch up. I find myself saying that all the time. Myself seeing that all the time. That part of using our identities enables. Yes, so there's a couple of reasons for that. First of all, we're often not actually interested in specifically where the person lives. Because often, you know, like maybe generally you could be interested in what neighborhood they're in, but I don't care what your street. In, but I don't care what your street address is, right? For the purposes of understanding. So, yes, that issue of identifiability is very important because we might be able to talk to the IRB about releasing this kind of level data where we just say you're at home, you're at work. We've argued that that's not really identifiable, just your sequence of activities during the day. As soon as you attach any kind of detailed location information, obviously that becomes highly identifiable. And if it's done in the context of a health study, now it's Done in the context of a health study, now it's PHI, and you have to be very careful about how you deal with it. But we've been able to successfully argue that just that sequence of states over time is not identifiable information. You don't say that because I know how long it took to drive to the household, I guess you have some information about the region. I mean, you could, you probably, there are probably, if you really worked hard, you might be able to do it. I mean, it comes down to how easy or difficult it would be. But certainly, you know, if you knew. But certainly, if you knew where, roughly where it was collected, you could maybe start to assemble this information. But we have been able to sort of successfully argue that if you remove that detailed information, that detailed information obviously is in the data set because it's all the underlying raw GPS data is in there. So if somebody really does want to dig into that and look into more geographical stuff, it's there. But for our purposes of sort of sequence analysis, we're not using that data. Yeah. Yeah, I mean the phone accelerometer isn't that useful, right? Because who knows where the person's wearing it, where they're carrying it. We're really interested in those larger scale patterns, those larger scale displacements. Great work, very impressive. But I had a question about, so you're looking at you're basically clustering people. And you're the eventual infrared that you want to work on. So what so you're you're looking at daily activities, but it also can change over time. So any thoughts on how you and this was for the 74 people. Yeah, so we've we've also done there's some analyses in the paper that also look at week patterns instead of just days. So you can actually take seven days consecutively and use that as the sequence. It's a longer sequence. It becomes much more of a problem for alignment. If the alignment-based Alignment, if the alignment-based methods, but for the adjacency matrix, it's not a problem. Scale-wise, it's not an issue. So, you could take longer scales if you wanted to look at longer-scale patterns. If you're asking about whether or not somebody belongs to the same cluster over time, we've actually been clustering days, which sort of, so you could imagine some of a person's days could belong to cluster one, and some of their days could belong to cluster two. In terms of character, Belong to cluster two. In terms of characterizing individuals, you could obviously use like the proportion of days that belong to cluster one, the proportion of days that belong to cluster two. That itself could then be subject to individual level clustering to determine sort of individual profiles. Okay, so the very interesting data. And for the clusters, have you look at how it looks like for the weekend? The cluster on data, right? So, but just we just do an automated system. Right, right. I mean, so if you look at some of the, if you look at some of the clusters that we derived, you could imagine that, like, you know, in here, this is probably a mixture of weekend and people who work at home. Right? They have to have some. Right, right. And so, you know, in reality, of course, you would have information about day of the week. So you could, of course, stratify and add some organic. Stratify and add some more detail. This doesn't include any covariates or any of that kind of stuff. So if you're really doing this in practice, you'd want to incorporate that information as well. Great work. And I think my question actually discusses, I don't remember anyone else's here at the end. So this data happens over twenty four hours, right? So this is compositional data. And um doing S V D with until spreading is human kind of computer probably. Or transforming data to some mechanic. Yeah, doing independent transformation. Like, are there different textiles if you try it or float products? Martha thought about it a lot. I did look into it. So, yes, the data is compositional, but it's really difficult to do standard compositional transformations because there's a lot of true zeros in this data, and there's not really many currently. Not really many currently compositional data method transformations that allow for true zeros. So I did a little bit looking at it, like a little, like adding instead of a true zero, adding like a really small value and trying some of those adjustments and then doing SVD. And the results were not as good, I thought. So it, and that was very kind of ad hoc. So nothing super principled, but yeah, I think it's just a challenging thing because there's often lots of true zeros in the adjacency matrix. There isn't a just. There is an adjustment you can do for semi-compositional data that has zeros, say that's like a modified symbol. Not sure how cool we're going to. We'll keep the zeros and zeros and just adjust them. Got it. What transformation is you see? It's called modified central log ratio transformation. So it's like central log ratio, but it keeps zeros. The only thing is that it's not preserving the actual data. Not preserving the actual data, it preserves the relative rats. So letting the data and also just to follow up on this, if I was a clinician, I always wanted to be, but if I was a clinician, if I look at this data, the first thing I would do is something very simple, right? Like, how much time do I spend in this category, this category, in this category, and when you start looking at the second-order properties, transitioning, etc. Approaches, transition in, etc. But did you try to understand how much your approach adds to something simple? Yeah, I mean, there's always this question of, you know, we compare to a couple of different other approaches in the paper. But yeah, I mean, there's, I think, lots of different ways that you could think about it. I mean, obviously, this is completely data-driven without necessarily needing to a priori determine the features of the days on which you would cluster, right? So, of course, you could view, you could think about your. Course, you could view, you could think about your probability of being at home, your probability of being at work, and all that kind of stuff. But again, you don't necessarily, it's not always clear exactly what those things should be. This is relatively straightforward because we've actually reduced the data here down to four categories, but actually the original data have about 12 or 13 categories. So, like, should I group together education and social to say that those are going to be like similar? You know, how do I aggregate all that information? I aggregate all that information. So, I think it just comes down to the standard argument of how much do you want to pre-specify in terms of your features that you're going to aggregate on, versus how much do you want to be more data-driven and just sort of see what the data are telling you. That's what you should ask. So, about the number of clusters, what do you try? Yeah, so we've got a cluster evaluation metric that sort of relies on the same idea of doing a decomposition from sort of a distance to like an average adjacency matrix for everything in the cluster. So you can essentially measure, it's almost like a centroid-based idea, but using this adjacency matrix decomposition. So you can get some decent idea of scoring. That was silly questions. Bartha, sorry. Are you guys working on hypothesis SEMAX? Or what are you working on next? You said in the last slides, but then I got interested in all the questions. Yeah. In progress, still deciding. But something inferential at this potentially causal where you're thinking. Causal, where you're thinking of kind of a categorical function, or like, because you can think of this in a functional way too, where the functions are the probability in each state. So it's thinking about that kind of as your treatment in a causal form, or just thinking about sort of like scalar on function type stuff, but with a sequence, because we fundamentally believe it's also about the pattern in the sequence, not just like the probability of each day, but kind of how those all function together. Together. So it's kind of just thinking about that. We haven't done anything interesting. Sorry. Why are you doing that? All right, I think we should probably move on to our next speaker. Thanks very much. 