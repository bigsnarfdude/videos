Okay, so Frederick was not able to attend, as it turns out, and so we did just a little shuffling of the schedule. And Jim Phil has agreed to be promoted from a 30-minute to a 60-minute talk at the last minute. And the organizers are really thankful for Jim. And he said he had more than he could say in 30 minutes anyway, so it's a win-win situation. It's a win-win situation. So, we're thankful to have Jim Phil here from Johns Hopkins University. He's going to talk about density functions for Kui Kwan. Thanks for chatting. Thanks. First of all, I really express my sympathy to Frederic for her travel and woes in trying to get here and being unable to do so. I'm glad to have the opportunity to speak to you today. If you don't know what quick quant is, don't worry. Is, don't worry, I'm going to be discussing that soon. If you don't know what density functions are, I can't help you here. Okay, so let me, if I had planned for a longer talk, I would have written some slides about this, but just let me say some things out loud. So quick sort, very, very famous sorting algorithm, and find, also known as quick select, were both invented in the 1960s by 1960s by Tony Hoare. And so it was, so I'll review what quick select is. I won't be discussing what quick sort is, but as almost everybody in the room knows, a traditional way of measuring the cost of these algorithms is by the number of key comparisons required. And it was first established by Uwe Russler using what's come to be known as the control. Using what's come to be known as the contraction method, and independently by Niray-Renier, using martingale methods, that the center and scale normalized number of comparisons required by Quicksort has a limiting distribution. And that this limiting distribution can be characterized as the unique distribution on the real line with mean zero satisfying a certain distributional identity. But it's worthwhile keeping in mind. It's worthwhile keeping in mind whether I'm talking about quick sort or quick select that there are no formulas for anything. There are no formulas known for densities, distribution functions, characteristic functions, moment generating functions, anything. So all the work has to be done implicitly from recurrence relations or distributional identities or that sort of thing. Okay, and now I want to say, I'm going to talk about some work that I did with Swante Jansson, but I do. I did with Swante Janson, but I do want to point out that many people in this room, and many people outside this room as well, have contributed to our knowledge about Quicksort, and I don't mean to slight any of you in that regard. I'll review some of the history of QuickSelect in my talk, but not QuickSort, other than I want to make these comments. So it was, I already mentioned that the discovery of the limiting distribution. The limiting distribution. It was Tan and Hydra Costas who first proved that this limiting distribution has a density. And then Svanta and I studied the smoothness of the density, turns out to be infinitely differentiable, boundedness of all the derivatives, decay properties of the density, and its derivatives in the tails. We show that the tails are at least. At least exponentially thin. And then just a few years ago, I think it's about four years ago now, Svanta wrote a paper in which he got much more precise results about what the tails of the limiting distribution are like. For quicksort, the left tail is doubly exponentially thin, and the right tail is roughly Poisson thin. That is, it's like exponential of minus x log x. Okay, and then so. Okay, and then so now what's the story with quick select? Well, I'm going to tell you about that. First of all, as you'll see, there's a whole family of limiting distributions, not just one. Depends on what rank you're looking for, what order statistic you're aiming for. But the same sorts of things come up, namely there is a limiting distribution, there's no formulas for anything, and we want to know about the behavior of these distributions. So, a few years ago, So, a few years ago, I started reading about what is known about quick select. And in particular, it was known that there's a limiting distribution for any choice of quantile, as I'll discuss. But almost nothing was known about these distributions, like do they have densities, for example? And so what my now former PhD student, he thinks. PhD student, he finished about a year and a half ago. Wayne Hung and I did is to establish that all these limiting distributions for quick quant, quick select, in the way that I'll formulate, all of them have densities. And once we figured that out, we started to ask questions. What are these densities like? And that's what I'm going to tell you about today. Okay? So if you want to leave now, you can leave because you've certainly heard what the talk is. So, heard what the talk is about, and the rest is the results. Okay, so here we go. What I'll show you is: all right, so for quick sort, you have, to get a limiting distribution, you have to center and scale. For quick select, you only have to scale. You don't have to center. Okay, that's well known to a lot of people in the room. Furthermore, so for those who don't need my introduction, of course, Who don't need my introduction. Of course, at the initial stage, you do n minus 1 comparisons, right? So if you're scaling by n, there's already a term of 1 in the limit, right? So it'll be convenient for a lot of what I say to subtract off 1 from that random variable, to put a random variable that's basically on 0 to infinity. So what I'll prove is that, regardless of what quantile we're looking for, t, the limiting distribution of the scale-normalized number of key comparisons used by Number of key comparisons used by quick quant to find the teeth quanta with one subtracted in a randomly ordered list has a Lipschitz continuous density function that's bounded above by 10. That the density is positive everywhere you would think that it's positive, that it enjoys, like Quicksort does, superexponential decay in the right tail. Then we can push as Wayne and I did. As Wayne and I did, so I mentioned that Svanta found more detailed information about the tails, and then Wayne and I pushed that much further for quicksort in the right-hand tail. We get more refined asymptotics, we get this sort of refined asymptotics in the right tail. And then we use the right-tail asymptotics to bound large deviations for the number of key comparisons, not in the limit, but for finite values of n, for large, but Finite values of n, for large but finite values of n. And as was pointed out by a referee of an original, a reviewer of an original, sorry, an original version of our paper, we can actually do perfect simulation from the limiting distribution, as I'll discuss. Okay. So QuickQuant, as I've mentioned, is a cousin of Quicksword, and there's Cousin of quicksort, and there's a wide gap between the average case and the worst case for the cost. Because it takes, as I'll review, order n comparisons on average to find a fixed sample quantile among n keys, while the number of comparisons can be as large as n squared. So that's motivation for studying the distribution of the number of comparisons. You want to know how unlikely it is that you're going to have to really spend a lot of comparisons. And the goal of our work and the goal of this talk. Of our work and the goal of this talk is to show that the limiting distribution has a density and to study the smoothness and decay properties of this density. All right, and I already mentioned the other stuff there. Okay, so now comes the part where if you didn't understand anything I was saying so far, because you don't know what find or quick select or quick quant is, I'm going to tell you about that now, okay? So let's start with quick select. It's also known as find. I think find was the original. Find, I think Find was the original name that Tony Hoare used, and I don't know where QuickSelect came in as an alternate name. But at any rate, QuickSelect, n, with two parameters, N and M. It's an algorithm designed to find a number of rank M in an unsorted list of size N, and I'm going to assume all these numbers are distinct, and so they may as well be the numbers 1 through n. It works by recursively applying the same partitioning step as quicksort, but As quick sort, but only to the sublist that contains the item of rank M until the pivot that we pick has the desired rank. So here's an example of quick select 9, 3. I'm looking for the third smallest item in a list of, an ordered list of 9 numbers. And as you'll see, the number of comparisons used is 15. Okay, so let's suppose that we always choose the first key as the pivot. Now, The pivot. Now, in practice, we want to choose a random key as the pivot, but you can, let's suppose that the list is already permuted into random order, okay, and it turns out to be these nine numbers. You might wonder, why didn't I use the numbers one through nine? And the answer is because I'm going to return to these numbers for another reason later on. Okay, so suppose we always, and let's also suppose that in conceptually, but not Conceptually, but not algorithmically, building this binary tree that when we sort into numbers that are smaller and numbers that are larger, we maintain the same relative order of the smaller numbers after the partitioning. And same true for the larger numbers. Except that we're not doing quick sort, we're doing quick select. So here's how it goes. So you say, okay, take the first item, the first key, 88, and compare everything to it. Compare everything to it. And then what do you find out? You find out that there are five numbers smaller and three larger. That says we, in trying to find the third smallest, we should look in the left subtree. We don't have to be concerned with the right subtree anymore. So the first comparison with a pivot cost us eight comparisons. Then we know we have to go to something smaller. So we take the first key that's smaller than 88. That's 46. 88, that's 46. And now we compare all the items that are smaller than 88 with 46, right? And we find out that we should take the right subtree here and so on, right, until we find the third item. So the first comparison cost us 8. The second comparison cost us 4. This one cost us 2. This one cost us 1. And so the total cost of quick select was 15. Everybody clear with the? Everybody clear with the operation of the algorithm? Okay. So let's let CNM be the random variable that denotes the number of comparisons needed by quick select. Knuth, way back in 1972, found an exact formula for this. For each n, this is symmetric and unimodal in m. So the minimum values are at the extremes, m equals 1 or n, and then you get. equals 1 or n, and then you get 2n is the average. And when m is in the middle, for example, if n is odd and you take the unique middle value of mn plus 1 over 2, then the expected number of comparisons is asymptotic to a bigger constant, 2 times 1 plus log 2 times n. So from just the computation of the expectation, you get the idea that if you were to take the number of key comparisons and divide by n, And divide by n, there would be a, might be a limiting distribution, and that turns out to be true. All right, now, yes. Hn it was nth harmonic number. Pardon me? Hn was a. Yeah, I'm sorry, I should have mentioned that, right? Hn here is the nth harmonic number. So now I've used the term quick quant. What is quick quant? I'll call it an algorithm, but it's really a sequence of algorithms, or really maybe a family of sequences of algorithms, right? Sequences of algorithms, right? Because there's one for, I'm going to pick any number t, any quantile in the range from, oops, sorry, in the range from 0 to 1. And I'm going to, by quick quant n, t, I'm going to mean, think of n becoming large, and we're trying to find the mnth order statistic, but we want mn to be about fraction t of n, okay? Fraction t of n. So we want mn over n to converge to t. And if you do that, then you see that the expected value, when div, expected number of comparisons, when divided by n does indeed have a limit, right? And the limit is this entropy function. Now, right, so what I want to say next is how things turn out for the limiting distribution. For the limiting distribution. All right, so it was first established that there is a limiting distribution for quick select, that is for quick quant, by Rudolf Grube and Uwe Rüssler. And in fact, they did it on a process basis, right? That is, instead of looking at one value of t at a time, they looked at all the values of t at the same time. So they took a specific They took a specific choice of M sub n such that M sub n of T divided by N converges to T. Namely, they took M sub N of T to be floor of N T plus 1 if T is less than 1. If T equals 1, that would work out badly. That would be a number of comparisons to find N plus first order statistic out of N. No, then they just use N there, right? And what they proved. And what they proved, really a very, very nice paper, is that this sequence of processes, xn, viewed as elements of d0, 1, the space of functions that are continuous from the right and have limits from the left, endowed with the scorah topology, has a limit in weak convergence as n tends to infinity. And furthermore, we can describe what that limit is. And it goes Limit is. And it goes like this. I wrote it in symbols down here, which you would have no chance of deciphering in any sort of real time. So instead, I'll show you an example. Okay? So now let's make our observations be on the interval from 0 to 1. And let's suppose in this example that we're looking for the, you know, we're in the limiting regime now. We're looking for the population median 1 half. Now we know where it is. It's at 1 half, right? But we're trying to find. Right, but we're trying to find it by the same sort of algorithm in the limit that we would use pre-limit to find an order statistic. Okay, and so now I'm reusing the same numbers as before, only I'm making them decimals, right? So suppose here's our stream of nine uniform random variables rounded to two places so I can show them on the screen here. And let's see how fast. And let's see how things work. So, what is this limiting random variable for number of comparisons divided by n? It goes like this. So, initially, we're looking, the range of, how can I put this? The range of values through which we're looking are from 0 to 1. And the first pivot turns out to be 0.88. 0.88. Okay, so we know that's too big, so we go to the left, and so it's the same story as before. We go to the left, then right, then left, and so on, right? Okay, but what is the cost in this limiting regime? It's the length of this interval plus the length of this interval plus the length of this interval, and so on, right? So let me just go one step. Once I've realized I need to go down this left subtree, I realize I'm looking for something smaller than 0.88. Now the range is from 0. Now the range is from 0 to 0.88, so the next contribution to the limiting random variable is 0.88 minus 0, and the one after that is 0.88 minus 0.46, and so on. So the limiting process can be expressed as this sum of lengths of intervals, right? And of course, always, regardless of what t is, the first one, the first interval always has length one. That's why I find it convenient to subtract off that one before we start talking, right? Before we start talking, right? So later on in the talk, I'll have a random variable named j of t. J is just z of t minus 1. Okay. Now, what would happen if you don't make the same choice of m sub n of t that Grugel and Risler used? Then the story becomes more difficult. In fact, I don't think it's known what the answer is for general such sequences, but at least univariately. But at least univariately we can say what the answer is, namely it doesn't matter if you change, right? That is, if you take any sequence m sub n of t that has the property that when you divide by n, you get a limit of t, then univariately you get the limit z of t. And so I should have mentioned maybe in the introduction, but I'll say it now. We are only studying this limiting process univariately. It was a challenge enough to find that there are densities and get the properties and so on. It would be very interesting to extend this. Be very interesting to extend this to multivariate distributions. Or, yeah, okay. All right, one of the key tools in showing that the limiting distributions do have densities and getting properties of them is the following observation. If we consider a sequence of independent random variables, each uniformly distributed on the interval from one-half to one, and let v be this perpetuity. Perpetuity, then the random variables z of t are all stochastically dominated by v. This was shown in the Grubel and Russler paper, and we find it very useful. This V, as shown in their paper, enjoys superexponential decay in the right tail, so that hints that we'll be able to get superexponential decay. I mean, it actually shows, right, that we'll get superexonential decay in the right tail as well. Also, it's true, and I'll use Also, it's true, and I'll use this later on, that every Z of T stochastically dominates Z of zero. Yeah. What is the relation between the V's? The V K's are IID uniform on one half one, and then I form a single random variable V this way from the V K's. And the claim is that all of the limiting random variables Z of T, they're all stochastically dominated by this one. Stochastically dominated by this one random variable V. Okay? I missed where the C something. Yeah, again, the VKs are these uniforms. You make this V, it's got some distribution, right? And then the claim is that all the limits from quick quant, these Z of T's, they're all stochastically dominated by this V. Robin. Yeah, Robin. I'm confused about a remark from a couple of minutes ago about doing this in a multi-variant case. Yeah, so what, you know, as I've already hinted, I'm going to show you that Z of T, or I'll discuss that we showed, that Z of T has a density with respect to Lebesgue measure, right? And so now suppose you ask, does Z of S comma Z of T have a density with respect to two-dimensional? Density with respect to two-dimensional measure. And what sort of density does, you know, what are the properties of it? I thought that the individual items that were being compared were just being effective. Yeah, so sometimes this is referred to as multiple quick select, right? You know, where instead of just trying to find one item, you're trying to find, you know, say several orders. Okay, let me mention a little bit about the literature. So Some of the earliest papers involved these ideas. Quick rand is a name that Wayne and I gave to what Mahmoud, Madaris, and Smyth did, which is to say, suppose that you add some additional randomness by saying the quantile that we're looking for is also uniformly distributed on the unit interval. And then they are able to find explicitly what the limiting distribution is. Distribution is, if you do it in terms of J of t, z of t minus 1, it's actually the convolution square of the Dickman distribution, which comes up in quick min. So quick min has been very well studied. Huang and Tsai have a very nice paper about it. So quick min refers to the case where t equals zero, right? We're looking for the, or m equals one or m equals m. We're looking for an extreme order statistic. You get a limiting Dickman distribution. Limiting Dickman distribution. If you don't know what that is, sorry, I'm not going to write it down now. Mark and Huber and I showed how to do perfect simulation from that distribution, and independently, Luke and Fozzie showed how to do that. There's a very nice paper that, and I put this in red because Wayne and I, in our work, made heavy use of this paper, a paper by Kodai and Mori about quant. And in addition to the Grugel Russell. In addition to the Grugel-Russler paper that I mentioned already, Grugel has a paper about quick quantum. Everything I'm talking about today measures the cost of the algorithm by key comparisons, something that in the last 10 years or so has become very big industry is to do things instead in terms of symbol comparisons. And a former PhD student named Take Nakama and I talked about quick quantity. And I talked about quick quant symbol comparisons. And then there's the case of worst-case find where an adversary is choosing which T it is you're looking for. And Jason Matterer, another former PhD student, and I discussed that both for key comparisons and for simple comparisons. All right, so I hope everybody understands now what Z of T is, what random variable that is, or fixed T, right? Fixed t, right? And we're going to address the following fundamental questions concerning the univariate distribution of j of t, z of t minus 1. What's the support of the distribution? Does this distribution have a density? If so, what are its properties? And can we simulate perfectly from the distribution of J of T? Answer: yes, we can. To our knowledge, these questions, we were surprised. So, Svant and I did our work on. So, Svanta and I did our work on quick sort back in the early 2000s, right? Kind of basic questions about the limiting distribution. Does it have a density? What is this density like? It hadn't been studied, apparently, for quick select, quick quant, except in two cases, as I've already mentioned, quick min and quick random. The Dickman distribution does have a density. The density is flat over the unit interval. For over the unit interval, and then it decays. And it has interesting properties like it is continuous everywhere, it's differentiable everywhere except at the value 1. It's twice differentiable everywhere except at the values 1 and 2, and so on. So it's kind of an interesting distribution. All right. So Quickman is, what exactly is it? Quickmin, you're looking for extreme orders, which is small. Looking for extreme order, so this is smallest or largest, right? T equals zero or t equals one. Now I'm confused why that's random. Because the choice of pivots is always random, right? You know, in the worst case, so just to fix those ideas, it could cost you order n squared to find the minimum, right? Because maybe the random pivot you choose is the smallest, largest one. And so now you need to look through all the So now you need to look through all the other n minus one items and you pick the largest one again. Because you decided beforehand you're doing it in this structured way. This is not the quickest way to find it. Quick quantile. This algorithm applied to that or statistic. Okay. So how do we prove it has a density? Well, there's a very, there's only Well, there's a very, there's only sort of one, I should have said this also at this chart, there's one idea in this talk, one idea, and the rest is grubby work. And the one idea is that the convolution of distributions when one has a density has a density. Convolution is a smoothing operation. And so we're going to apply that conditionally. So I'm going to show you that. All right, so I'm going to show you that conditionally given something, j has a density, and then you mix those densities and you get a density. All right, so what conditioning are we going to do? All right, so let's remember the notation that delta k of t is rk of t minus l k of t, right? And that j of t is the sum of these delta k of t's. Then what we can do is show that the conditional distribution of of delta 1 of t plus delta 2 of t, given L3 of t and R3 of t has a density. So that means that J of t has a conditional density given L3 of T and R3 of T, and therefore J of T has a density. And now, and so why does, why does it have a density of density So why does the condition? Yeah, sorry, I left out an important step of that. This thing has a conditional density given L3 of t and R3 of t, but we also have to deal with the terms delta 4 of t plus delta 5 of t and so on. But the point is that the sequence of intervals here is a Markov chain. So the random vector Random vector that goes into making delta 1 of t plus delta 2 of t, and the rest, delta 4 of t plus delta 5 of t, and so on. These are conditionally independent random variables given L3 of t and R3 of t. So since one factor in the convolution has a density, the convolution has a density, and then you mix and you get a density, and so you get a density. Now you may have wondered why conditions. Why condition on L3 of T and R3 of T? Why not condition on L2 of T? Obviously, you can't condition on L1 of T plus R1 of T with the same idea because you get zero for what appears before that, right? But why not on L2 of T plus R2 of T, on L2 of T and R2 of T? And that's because you don't get a density in that case, right? That is, if you look at the, oops, sorry, if you look at the conditional distribution. Conditional distribution of delta one given L2 and R2. L2 and R2, what happens? You started off with L and R equal to 0 and 1, right? At the first step, only one of L or R moved, right? So once you know what L2 of T and R2 of T are, you know what happened at the first step, right? So it doesn't work to condition on L2 of T and R2 of T. That's unfortunate because it makes our computations. It because it makes our computations more involved. We have to add two lengths of intervals. Okay, I'm going to, there's slides that have an insane amount of detail I'm going to skip over. At any rate, the point is that there is a density. Okay, and we, having found that there is a density, we wanted to know how these densities behave. Well, the density. How these densities behave. Well, the densities are uniformly bounded by 10. You might wonder, what's why 10? The answer is: we don't, we think the right answer is e to the minus Euler's constant. And then your next question is, why e to the minus Euler's constant? The Dickman density has maximum value e to the minus Euler's constant. And from computations we've done, we believe that that's the worst case. That's the worst case. t equals zero is the worst case, that they're bounded by e to the minus gamma, which is a number between zero and one, so it's not ten, right? But we can get ten by our estimates. That's actually very similar to when Svanta and I got bounds on the bound on the density for quicksort. I think we got an answer like 64 or something, and we think the truth was two-thirds or whatever. So at least. So, at least we can show that the densities are bounded. They're all Lipschitz continuous. The support is what you would think it would be. Now, let me explain what I mean by that. So, we've already subtracted off the one for the initial interval, right? Yeah, probably. But if this Lipschitz continuous, there are a similar constant that's two big for the Lipschitz constant. The Lipschitz constant. Yes, the Lipschitz constant, we have an absolutely horrible. Absolutely horrible bound. That'll come up later on. But yeah, we don't know a good estimate of the Lipschitz constant in that case. What I mean by that the density is positive where you would think it would be is that it's, you know, there was this length, interval of length one to start off with, right? Suppose I'm looking at t less than a half. t less than a half, so the minimum of this minimum here is t, right, then it's going to cost you at least t for the next compare, the next length of the interval as well, right? So it's easy to see that to get positive density, you have to have x greater than or equal to the t in that case. And in fact it's we can show that the density is positive precisely here. This is analogous to how Svanta and I showed that the Svanta and I showed that the limiting quick sort density is strictly positive, the continuous limiting quick sort density is strictly positive everywhere. We can in fact get joint continuity in T and X. There is, in a manner of speaking, no left tail for this distribution, right? Because the distribution starts at the value t, but you can talk about what's the behavior of the density near t. Of the density near T, right? And there we can show that the density is infinitely differentiable, strictly increasing, strictly concave, and strictly log-concave. And we can say even more than those properties. And in the right tail, the behavior up to these first two logarithmic terms and remainder is the same as what it is for quicksort, which is sort of interesting. Furthermore, as I hope I'll have some time to explain, As I hope I'll have some time to explain, the explicit bounds on the densities, the Lipschitz constants, and the Kolmogorov-Smirnov distance between the distribution at finite n and the distribution in the limit, z of t, enables us to do perfect simulation from the limiting distribution. Just as Luke and Ralph Neininger and I showed that you could do perfect simulation from the quicksort. Simulation from the quicksort limiting distribution. Okay, I won't go through the details on the slides. I'll leave them up for just a bit so you can read what you want to read. But I think it's not a good use of time to talk about the details here. The densities are all uniformly bounded by 10. The density function is for each fixed 10. Is for each fixed t, the density function is uniformly continuous, and Lipschitz continuous. Uses, starts off by using right continuity of one of the ingredients to this density and then a dominated convergence theorem argument. Positivity, I've already mentioned. Super exponential decay bound, right? So we can prove that for We can prove that for any t and any theta, that's what makes a superexponential decay, we have this sort of sub superexponential decay. Left-tail behavior. I'll just put this slide up for a minute. We can actually show that as you approach the left-hand endpoint. Left-hand endpoint of the support of the distribution of F, we get a uniformly absolutely convergent power series expansion. We can, in principle, say what the coefficients are, but not in fact because it involves these random variables j of w, right? So it's sort of self-referential. But we know some properties about these coefficients, and we can bound them above and below. Below. And we can do things for t equals half as well. Lipschitz continuity, that took the most work, but we can prove Lipschitz continuity. Let me spend a few minutes talking about the right-tail asymptotics of the, first of the distribution function and then of the density. Here's how we can proceed. As mentioned earlier, Z of T is stochastically dominated by Stochastically dominated by v, this perpetuity random variable that I mentioned before. So the moment generating function, for positive arguments, of z of t is dominated by the moment generating function of v. And then we establish an integral equation for that random variable's moment generating function. And then use similar ideas to what Wayne and I had done for the right tails of quickspring. The right tails of quicksort to bound M, and then you get the right tail estimate, the right tail bound by just using a turnoff inequality. How do we get the lower bound? Well, we get that by using the fact that I had in very small print earlier that Z of T stochastically dominates the Dickman distributed Z of zero, and we know about the tails. zero, and we know about the tails of the Dickman distribution. That worked out nicely. Not only does the distribution function have this sort of asymptotics, but so does the density, which you would guess from the result about the distribution function. In fact, from the density result, by integration, you can get the distribution function result. Unfortunately, it doesn't go the other way around, but the way we get the result for the density function is by The density function is by establishing and then using an integral equation for the densities and then the right-tail asymptotics of the distribution functions. We know that the, so for the, let me go back a page, for the survival function, for the right tail of the distribution functions, we know that this estimate on the right is uniform in t. Uniform in T, both upper and lower bounds, right? For the densities, we know that the upper bound is uniform in T, the lower bound we don't know. It's true for each fixed T, but we don't know for uniformity. Okay, let me say a few words about large deviations. All right, so we know that if I take the number of comparisons required by quick quant and then divide by And then divide by n, that has a limit, z of t. And so the probability that cn of t, number of comparisons, divided by n is larger than any fixed number, converges to the probability that z of t is bigger than that large. That's that same number. That's just convergence in distribution. But what if we want to ask about the probability that c n of t over n is bigger than some increasing function? Bigger than some increasing function of n, like log n or log log n, something like that, right? Or n to the one-half, right? Can we say anything about large deviations? And the answer is yes. And what we need as an ingredient to establish that is a bound on Kolmogorov-Smirnov distance between the two distributions. All right, so sorry, I have CNT already has the staling unit. So, this is a lot to read, but let's just look up here. Here's the definition of delta nt. Think of delta as being like 1 over n. So, 1 over that is like n, right? Okay, and that'll be helpful in interpreting the later slides as well. So, how did we get this result? Well, Kodai and Mori did the bulk of the work for us. Bulk of the work for us, they found the convergence rate in the Wasserstein D1 metric, and then we can easily extend that to Kolmogorov-Smirnoff distance. And then this, because this lemma is then a consequence of a lemma by Svanta and myself, which bounds Kolmogorov-Smirnoff distance in terms of Wascherstein distance when one of the two distributions has a bounded density function. And we already know about bounded density function for C of T, so we're in business. Okay, so here is the large deviations result. So again, if you took x to be a fixed number, you would know that the probability that c sub n of t is bigger than x converges to the probability that z of t is bigger than x as n tends to infinity simply by the limiting distribution result of Gruville and Rusli. We can get the same asymptotic equivalent. Asymptotic equivalence for larger values of x. How large? About up to log n over log log n. So we can't go up to like n to the 1 half or something like that, but we can go a little bit further than the range. So this is like with in the central limit theorem, for example, right? You can go out to a certain distance beyond constant, like n to the 1/3 in the proper scan, and still get n to the 1/6, and still get Sixth, and still get central limit theorem type behavior, right? Beyond that, you have to use corrections. And here we say that you get the same result out to log n over log log n. So there is the large deviation result in this case. In fact, you can, if you're only concerned with upper bounds, you can go out all the way. That is, Kodai and Mori showed that, in fact, not only is it true that Cn of C, not only is it true that Cn of t converges to Z of T in distribution, but Cn of T is stochastically dominated by Z of T. So if you want an upper bound on this, X depends on N, large deviation probably, you can get it in terms of this, right? And we know what an upper bound for that is, it's this one. So there's no restriction at all on how large X can be in that case. Now, this might not get you the This might not get you the right, you know, a sharp bound, but it does get you a bound. Last topic is perfect simulation from the distribution. Okay, so I'm going to show you that it's possible to do perfect simulation from the distribution of z of t or j of t equals z of t minus 1. And please, if in In terms of Bob's talk yesterday morning, this perfect simulation algorithm would not deserve to be called an algorithm because there's not a very good, there's hardly a trace of an analysis of this algorithm. In fact, I think it's probably like the perfect simulation algorithm for quicksort, it's probably pretty slow, you know, maybe quite prohibitively slow. But it's a proof of concept. It's a proof of concept. We can sample perfectly from the distribution. Okay, so it goes like this. There are four ingredients that we're going to need in order to be able to do perfect simulation. We're going to need a bound on the fourth, so I'm fixing t here, right? So you won't see t appear anymore. Fix the value of t, right? We need a bound on the fourth moment, a bound on the density, a bound on the Lipschitz constant, and what I'll The Lipschitz constant, and what I'll call a semi-local limit theorem, which is to say that not only is it true for intervals of fixed length, that you get that the distribution of this random variable has approximately what you get by integrating the density over that interval. But it works even for short but not too short intervals, right? So, in other words, we established that there are sequences. We established that there are sequences delta n, that's the width of the interval, and epsilon n, that's the error in the approximation, so that you can get approximations to f in terms of the exact distributions for finite n. So why do these four ingredients mean that you can do perfect simulation? In my original slides, I wrote, I have no time today to describe. I have no time today to describe in detail how this works. Okay, but I have a little time, so I'm going to describe it. So the algorithm is based on classical von Neumann rejection sampling, and the ideas come largely from Luc DeVroy's fabulous random number generation book. And exactly the same ideas were used by Luke and myself and Ralph to do perfect simulation from Quicksword. From Quicksort. Okay, so how does it go? It goes like this. So there's an elementary theorem in Luke's book that says that if you have a Lipschitz continuous density, you can bound the density by this. Lambda here is the Lipschitz constant. And the Lipschitz, yeah, okay, so that, which you can bound this way. All right? So now to further bound the density. To get to further bound the density, let's bound the tail of the distribution. I talked about very fancy bounds on the, you know, very sharp bounds on the density before, but for this, let's just use the fact that there's a finite fourth moment. Capital F is a C D F is a fourth. It's the C D F of the distribution we're aiming for, right? The distribution of J, right? Okay, and so if you have a bound on Okay, and so if you have a bound on the fourth moment, you can get a bound on this, this way. So to help you out, I put reminders of things over here. So the fourth moment, we're bounding by k1, right? The Lipschitz constant, we're bounding by k3. And so, okay, so you can bound things this way. So that means, and k2 is a bound on the density, which we was k2 we could take to be 10, right? So that Right? So that means that if you define a function g this way to be the minimum of the bound on the density and this bound on the density function, that this is an upper bound to this f. Capital lambda, I thought that was taken to capital lambda is a bound on what? Sorry, say it again? There's a capital lambda? Lambda is the Lipschitz constant, and I'm saying we don't know the Lipschitz constant. And I'm saying we don't know the Lipschitz concept exactly, but we can bound it by K3. Okay? And similarly, we don't know sharp results for any of these things, but we know bound. So this G dominates the function f. Which means you could do acceptance rejection by saying simulate from the density g and then accept with. And then accept with probability f of x over f of w over g of w if you get the value w, right? Now the question is: how do you simulate from this thing? Well, first of all, that doesn't necessarily integrate to 1. In fact, it doesn't integrate to 1. It integrates to this quantity inside the brackets. But how do you simulate from the density proportional to this? Well, Luke's book is great. You can actually sample from that distribution. Can actually sample from that distribution in an elementary fashion as follows. You just generate two independent uniforms and let W be this. And that miraculously has the density that you wanted, the G tilde density. Can you go back for a sec to show what G is? Oh, sorry, wrong way. This thing. Minimum of a constant and another constant times x to the minus 2. times x to the minus 2. You can actually sample from that exactly. Okay, great, okay. By doing this. Okay? Now there's one last step. So you say, all right, so I have to accept, if I generate w, then I have to accept w as an observation from f with probability f of w divided by g of w. Problem is, in quick quant, as in quick circuit, we don't have any formula for f of w, right? So how do you do that? So, how do you do that? Well, we don't know f of w, but we know arbitrarily good approximations to f of w. That's what the semi-local limit theorem is for, right? Epsilon ends in the semi-local limit theorem 10 to 0. So we can generate the uniform. We want to know whether or not the uniform is less than or equal to f of w over g of w. Just generate a high enough, you know, an accurate enough approximation to f of w to figure out. Approximation to f of w to figure out whether that's true or not. And that gets you the exception. That particular sample uniform. I mean, whether that particular sample uniform is less or greater. Yeah. Okay. And that's how you do it. Now, I will say maybe a tiny bit about how bad an algorithm is. Just as one indication of how bad it is, if you say, like, all right, how long does it take to get an approximation? To get an approximation to F of W, the answer is, yeah, so if you're required to use the nth approximation, how long does that take you? And the answer is it takes you like order n to the, remember delta n is like 1 over n, so it takes you about time of order n to the fourth to do that. So, and there are worse aspects of this, but at least it shows in principle you can do perfect symbols. It shows in principle you can do perfect simulation from the limiting distribution. All right, so there's a summary, and that is all for today. Thanks a lot. Do you know if the limit distribution is unimotal, the density is unimotal? No, that would be something interesting to prove. We think that it is. That it is, and we think that it actually. I remember, we've simulated this, right? So we kind of can make some conjectures. We think it goes like this. So remember that the Dickman density starts off flat and then decays. We think that for small values of t, the quantile we're looking for, it starts at, we know it starts at zero, the density. We think it goes up really sharply, really quickly. In fact, we studied that. Quickly. In fact, we studied that left hand tail, so we know it does go up very quickly. And then we think it sort of levels off like the approximately levels off like the Dickman distribution density does, and then decays. So yeah, we do think it is probably unimodal, but no idea how to prove it. Probably. I'm curious about quick quant as an algorithm. Is there a regime like if t equals 2.5, are they trying to find the media? 3.5, trying to find the median. Is it a good way of finding the median compared to, you know, in terms of Conrado has studied this way? There are ways to improve, too, right, you know, by using medians of samples and so on. How much do you believe carry to the case where you are using some samples to select the piewards? I'm in particular, I think that if if you are using samples that are Are using samples that are, let's say, of five elements that grab the pivot, then you would have very interesting phenomena. For instance, that the density is common on bolt. Yes. So using average of files instead of detect yeah when one or one uh the usual thing to do is to pick instead of picking the people at random, you pick five elements or You pick five elements, or nine, or whatever, then you pick the median of those, this small sample, and you use that as a view at each recursive stage. Here's a great thing about this audience. I can take a question and turn it and ask it to somebody. How widely is quick select used? Well, it's the method of choice for primary median, I think. So it's a method of choice for the median. It's not a method of choice, obviously, for the main or the max. Is there some kind of between, like, as long as you're between? Cut off or between, like, as long as you're between a third and two-thirds, it's a pretty good thing, right? I think you just need to be a little bit above constant. So, if you're looking not for the fifth smallest, but something that depends on it. But even like the root nth smallest. So you know that you're saying as long as t stays away from 0 or 1. I'm not sure if there's a. The other answer is: I only know of one programming library that ships a selection method, and that's the C. And that's the C library. And that uses quick select some weird variation. Yeah, by the way, many people in the audience know also too of Yaroslavsky's dual pivot refinement or whatever extension of QuickSort. I don't know whether anybody has considered that sort of set of ideas for QuickSelect. Oh, you have? Oh, you have to. What would you call it quick random? Yeah. And then the expected case, but no distributions. Yes. I don't quite understand the equation slide number CT2. Ah, thanks. Very well prepared with that question. Yeah, this one. I see that the right-hand side coincides with the right-hand side. Coincides with the right device. So suppose you take something that's safely in that range, like log log n. Okay, that's certainly smaller than log n over log log n. So if you want to know the probability that the normalized, remember this is the normalized number. Remember, this is the normalized number of comparisons, is bigger than log n. Sorry, log log n. That would have the asymptotics exponential minus log log n times log log log n minus, you know, and so on, right, like this, which is something that tends to zero. So it gives you an estimate of how unlikely it is that C N of T has this unusually large value, bigger than log. Oh so the n is already incorporated into this CN is already divided by that. Yeah, I'm thinking like yeah yeah yeah that's the C important I messed up when I was talking about it. The C n of T already has the division by N. Okay, so in other words you'd expect this to be of constant size. If you take an Xn that becomes large like log log n, this is an unusually large, really unusually large Large, really unusually large value of Cm of T. And so this probability is small. Sorry, I might be missing something here. I'm just thinking, like, normally for large deviation rate, it's probability that normalizing greater than like exponential of minus n times the rate. Right. There's a very well-developed theory of large deviations for. Developed theory of large deviations for things like sums of IID random variables. And this has nothing to do with things like large deviation principles or that sort of thing. I'm just talking about upper and lower bounding, at least asymptotically, the probability of getting unusually large deviations from the mean. Okay, I got it. Thanks. The entropy that you showed is has like a two times. Showed it has like a two times of the microphone. Yeah, let me go back to that. That's near the start, right? Trying to go off these and maybe I'll get on my computer really faster. Here, yeah. That reminds me a lot of the qualescence theory, like the first time that you have the two populations and one dies. Populations and one type extinction probably, extinction time, expectation of extinction time for two kinds. And also, it's like you have a diffusion, a Bramian diffusion, and whenever it hits zero or one. Have you ever thought about some region? Because I think it's exactly that. That's interesting, but I wanted to test my understanding in Test my understanding in the capital V that you define. Is that intuitively the cost of this quick worst, or what the name was, where essentially an adversary decides which side you have to recurse on. That's a good question and I forget whether that's true. Let's see, where would be a lovely intuitive explanation for this dominance? Explanation for this dominance. Yeah, I mean, it is something like you always choose the larger side, right? No, but yeah, now I remember now. So it corresponds to saying when you do the split by the pivot, always choose the larger side. Now that's not the same as worst case find because worst case find might require you at the top to go to a smaller subtree and what's worst case find maybe. Well, what's worst case find maybe? Well, so worst case find refers to, let me say, worst case quick quant, for example, right? Like I'm trying to find the T, right, in the interval from 0 to 1. And so I build, you know, let's say I'm doing quick sort, I build the tree, and now the adversary gets to say, which T am I looking for? After the fact. Yeah, after the fact, right, yeah. Okay, that's the other difference. That's the worst case. And that would be the worst. In other words, you say, for the given data, what's the word? For the given data, what's the worst it could cost you for t. This is not quite that bad, right? This v. It's saying always choose the worst branch of the two. And so, you know, again, that's sort of a greedy version of worst case, but not the worst, worst case. Last questions for Professor Phil? So we'll have 30-minute coffee break, and we'll resume at 10:30.