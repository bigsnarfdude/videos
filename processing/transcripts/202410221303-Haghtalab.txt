Objective learning. And I didn't take Shai's advice in trimming the talk down to 20 minutes. So, but but but um you know I'm more than happy to skip slides mostly because if I did trim the talk then it would be all the technical stuff and I think some of you might prefer to not ask the questions that are completely exploratory in order to see the proofs and others might prefer to ask all the exploratory questions. So, this is really your game if you can decide how you want to go about this. This. But probably the optimal place to start asking questions is after I introduce the framework in a few slides. I guess who's, I mean, I'm going until 1.35? Sure. Okay. Okay, so let's get started. Okay, so kind of like from science fiction to industrial research to academic research, in some sense, the holy grail of thinking about modern ML and AI has About modern MLN AI has been in search of general-purpose algorithms, general-purpose models that can do a lot of things all at once. And with recent advances in AI, we are starting to see some progress towards moving beyond models that are trained for one specific task or one specific population according to one specific notion of success, and starting to think and move beyond that to models that simultaneously excel at many tasks, many populations, many different measures of success. Many different measures of success. And even for a simple, you know, example of trying to think about a predictor, risk predictor for the chances of developing diabetes in the next five years of a person, we actually do have these guarantees we want that go beyond the classical average accuracy, but maybe we want so many other things. Like, perhaps we want that predictor to be accurate based on no matter what your age is, no matter what your sex or gender. Is, no matter what your sex or gender is, no matter what your race is. But also, we want it to be calibrated, perhaps, to not have, you know, high false positives or false negatives. Or we wanted to, like, when we make decisions based on those predictions, for those decisions to actually be good. So it doesn't, you don't have to go all the way to neural networks to think about multi-objective general purpose learning, actually, even in classical sense, and even with small models. What you're asking. Models, what you're asking them to do is more and more every day because we want them to be more general purpose, we want them to be more robust. And in some sense, this kind of multi-objective learning is supposed to give us guarantees, supposed to be one way to achieve these general purpose guarantees, like having multiple objectives being satisfied. And multiple objectives are a good way to both think about model capabilities, like in the sense of good performance. Like, in the sense of good performance on many downstream tasks, even if you didn't really account for it. Also, for robustness and fairness guarantees and reliability more generally. For example, you want good performance for all recognizable subpopulations or good performance even if there was some adversarial or unpredictable environment that you developed your algorithms on. So, in some sense, the modern MLN AI is saying that we should scale data, we should scale models, and we will solve all of these things hopefully someday. And we will solve all of these things, hopefully, somehow by the larger scale of data and models. And maybe you have started to see some progress that is truly based on scaling of data and model sizes in terms of having more capable models. But persistently, you're going to have these reliability issues, where the larger data set size and larger models didn't really give a way to deal with these problems. Like, we still have unequal performance across different rates. Performance across different race and gender. We have persistent ways of jailbreaking models, even when there are safety trains. So, scaling data is not the sort of panacea that people outside of this room believe it to be, perhaps. I know that people inside this room are definitely much more thoughtful about thinking about scale of data, and that's why they're going to enjoy this song. It also really depends on like just like IIT samples are fundamentally different. IID samples are fundamentally different than like carefully collected samples. So, one way to think about this is that the places that we have done, well, there are places where you don't really have anything adversarily happening or anything that is happening around recognizable axes, but they're sort of random things that have happened and you want to provide guarantees. Shai, you should wait at least for one more slide. Shai and I go way back. I was his honor. Go way back, I was his undergrad, so we know how to talk. So, before Shai, you get your chance of asking questions, you were late. I was just telling people that optimal place to ask a question is actually after I've already provided a framework. But let me first ask my questions and then we'll ask your question. Okay, so how should we, so with this in mind, kind of like what I'm thinking about these days is that I'm thinking about these days is that how should we actually design our learning systems that have these provable multi-objective guarantees from ground up if we wanted those guarantees to exist and not accidentally come up? What's the right mathematical framework for this? And what are the design principles, algorithmically speaking, sort of data curation, collection speaking, and on many different angles that we typically think about machine learning? With that in mind, this is the plan for the talk. I will give you one unifying framework. It's going to look Framework. It's going to look toy-ish. It's fine. We said less than 25 minutes we have left of this talk, so that's what we have to do. Then I'll give you some toolkit for designing algorithms within that framework, and we'll talk more about algorithm design and then how actually we also need to be thinking about data collection simultaneously. And since I don't really have time to give you a perspective of all the other work that's happening in this space, I'll just end with like a quick mention of a few things that I did mind. Like a quick mention of a few things that I can't explain. With that in mind, are there questions? No, no, so I mean, I think we have discussed it with you, but I want to put the point more clear. I mean, if we cannot, you were just saying we have failures in solving a single task. Now you want to solve several tasks together, it's not going to get any easier. What you're really doing is you're shifting the baseline. Instead of saying, I want to succeed so well. Saying, I want to succeed so well on each task. You say, I want to succeed as best as I can on the worst or the most difficult of those tasks. Yeah, so you have a kind of framework here. You're just drawing your target around where you can hit. Good. So I think we disagree on what it means to fail on a single task because what I'm going to do here is to say that we all love agnostic learning and hack learning and we think that it actually does something reasonable. Think that it actually does something reasonable when you have IID access to data and you're going to be tested on the same data. So, that we assume is success, not failure. So, with that in mind, then going for more tasks, sure, it's more complicated than agnostic learning, but you're not starting from a source of failure. With that in mind, let me tell you the framework because I feel like you know a little bit more than the rest of the people about what we're talking about. So, I guess. I guess when I think about multi-objective learning, especially for this talk, I want you to think about you want learning guarantees that are holding over different application domains, perhaps, for different types of stakeholders who have their own limitations and rules. And if I were to think about the diabetes predictor example, what you might think these stakeholders are, they might be hospitals who represent patient populations, or they might be patients themselves. And if you're thinking about hospitals versus or patients, Versus or patients, you definitely have hospitals that are, you know, children's hospitals or working with a very different set of types of patients than a hospital that is a research center on renal problems or hospitals that are primarily in rural areas versus urban centers. So these are situations in which when you're thinking about your learning algorithm, it has to sort of both interact with different stakeholders and try to provide guarantees for all of these stakeholders. Guarantees for all of these stakeholders. So, in that sense, when I think about multiple objective learning, I'm thinking about how we can enable learning processes that can satisfy multiple objectives, possibly for many different stakeholders, and hopefully from collectively fewer resources that would have needed if we just did things without bringing them together. So, what is the framework we're going to be working on? We're going to go with thinking of these hospitals as distributions, B1 to PK. B1 to DK. And we're going to assume that we have sample oracle access to these distributions. So at will, we can actually collect more samples. We can go to a population and ask for more of the patient data of that. We're also going to assume that we have multiple loss functions. So rather than just thinking about the single loss function that's like binary prediction loss, you could now think about wanting to have many different ways of measuring success. So other examples of loss functions that are particularly Other examples of loss functions that are particularly nice are, for example, thinking about whether you have false negative or false positive sort of error, and that would be shown here. Or perhaps you want to think about calibration style of losses, where you further more condition on the actual level set. So, this is a little bit again more, even more fine-grained than just all spots in the post-sign level. So, these are examples of losses, and of course, we're going to think about expected loss. We're going to think about expected loss of as measured on these different distributions with a capital L. So now let's go back to what the agnostic learning perhaps or PAC learning in particular would have wanted. In PAC learning, we're saying, can I learn a function that is as measured by one loss function, loss j, on one distribution, di, from which I took samples, I have low error in expectation. And that is In expectation. And that is L of, you know, F over DI as measured with loss Lj. So what you are asking is now to say that I want this to be true, not just for a single distribution, single loss, but for all the distribution and all the losses that I planned. Okay? So this is one form of multiple objective learning. And if you can do this, in some sense, you can think about it as universally uncovering something, a good model, as the universe being defined by the set of distributions on losses like that. Of distributions on losses like we're talking about. This is not always possible, just as path learning is not always possible. So, when this is not possible, we're going to try to do as best as possible. And as best as possible can be defined by really saying, this is what I'm trying to do, as best as possible, as competing with a class that somebody gave me. I want to compete with that. And if I can do that, it's like as if I did as best as possible given that I was competing with a specific project class. Effect function cost. Typical questions you might have in mind right now here is that does F belong to H? Doesn't have to, but there are some people who like it to belong to H, so there are definitely a group of people who think about that restriction. Other questions that might come up is why a single F? And yep, there are some applications where you want to personalize, but if you're especially thinking about sort of you want a model and you want to put it in the world for it to be general purpose, you want that model itself to be a single model. Model and it might not have access to these like knowing the underlying features or losses. In short, while I'm giving you a model that admittedly looks toyish or maybe a little bit pessimistic in terms of what benchmark you're dealing with, I want to say that the actual tools that I'm giving you, in a provable sense, go way beyond what I'm presenting to you today. In particular, some of the other examples, like that's personalized solutions. Like, that's a personalized solution. This is a slightly different version of thinking about the problem where you want to maximize the regret where kind of meeting a perdistribution benchmark rather than a main max benchmark. And sort of many others fall in this category of the types of problems where you're trying to somehow have a model or set of models that have good performance as measured by many different losses, many different distributions, and they're all at the end boiling down to multi-objective line. Boiling down to multi-objective learning. Okay? So, my argument here is that let's solve the multi-objective learning, let's develop fundamental tools, sample complexity-wise, algorithm-wise, for that, because that way we can actually fundamentally understand many other application areas and domains without necessarily having to build algorithms and insights from scratch. For consistency, and actually for conciseness, really, for the rest. Actually, for conciseness, really. For the rest of the talk, I'm going to assume that we have a single loss function, take it to be binary loss, and uh but multiple distributions. Everything I say extends very naturally to natural classes of losses like calibration losses or many others, but I'm not going to go into details of that. So from consensus, multiple distribution, single loss, quantum loss. The other reason I think we should be studying this in this generality, in this framework, is that, in fact, in Framework is that, in fact, in the past number of years, there have been a lot of work from slightly different communities occasionally that essentially are trying to solve the version of this problem for a specific motivation that they have. So, when I first started thinking about this type of problem, I was just finishing my PhD and I was thinking about what it means to collaborate between data set holders. So, in my mind, my motivation for studying something like this was: I have distributions d1 to dk. I have distributions D1 to DK. These are agents with their own sort of data sets, possibly, and I want them to collaborate to learn something collaboratively. So this is definitely a version of thinking about collaborative learning, collaborative federated learning, when you want per-agent type of accuracy guarantee. But many other works in this space have looked at the version of this for other reasons. So agnostic fair-federated learning. Agnostic fair-federated learning, we're thinking those people were thinking about the eyes representing sort of client distributions and they had furnace. I pressed the record. It is recording. Yeah. This work, in fact, was built on an older line of work called multi-source domain adaptation as well. I would say, again, there are similarities with the min-max perspective there. Then there was definitely this group distribution. There was definitely this group distribution robust optimization coming more from core ML, sort of applied ML, where again, the idea was to think about fairness and robustness goals when you're thinking about these VIs as possibly different worlds in which or possibly different distribution shifts. And you know, I can actually go on and on, and for the interest of time, I'm not going to, but especially in the fairness literature, there's a lot of reasons you want to think about a min-max way. You want to think about a min-max way of optimization. So, for example, the talk we also saw by Jamie was about sort of trying to do a kind of a min-max where max was taken over subgroups. So, in particular, this multi-calibration, multi-group fairness, there are different formalizations that are essentially different instantiation of multi-objective learning for different motivations. So, this is kind of like just giving you a little bit of a history and why I think we should be studying it at this level of granularity. Should be studying it at this level of granularity rather than specific applications. And what I want to do is to say that at this level of granularity, we are really thinking about the generalization of a classic model of agnostic learning from one distribution to multiple distributions. And it helps, like this is literally what we know what to do for like decades, this is what we're trying to do. And it helps to come back for comparisons, whatever results we get here to compare to the single distribution setting. To the single distribution setting. In particular, for just a quick reminder, there are many ways to think about this statistical learning. One way is to think about you have a finite VC dimension class, and it's kind of your sample complexity is Vc over epsilon squared, and sort of your algorithmic paradigm is just DRM. That's kind of like a very clean, nice way of thinking about statistical learning. We are asking: what is the algorithmic paradigm and what's the sample complexity here, especially as it compares to? Especially as it compares to the dependence on VC dimension and perhaps how does it relate to empirical risk minimization? Is it empirical risk minimization or is it something else? Okay, so that's the framework. If there are any questions about the actual framework, this is a good time to ask because the next thing I'm going to do is to tell you about sort of naive ways of or simple ways of trying to design algorithms, the pitfalls, and then perhaps the right way of thinking about designing algorithms. Questions? Okay, so let's look at sort of again, people in this room are way too smart to think that scaling data is the only thing or the main thing to be doing. But for people outside of this room, one would argue that the reason scaling data is not the best way to go about this problem is that sort of the average accuracy is not a good proxy for the worst case accuracy. For the worst case accuracy. And so that's even in natural data sets is not the case. So you really don't want to just scale your data and take uniformly the same amount of samples across all tasks. If you do that and you just said, I just take more data, run the optimization a little bit longer, what happens is that because the average loss is not a good proxy, even if there was, especially if there is a label noise, even without label noise, what happens is that some distributions are just naturally easier to learn than others. Are just naturally easier to learn than others. But the unfortunate part is that you don't know which ones are easy, which ones are hard until you learn something about them. Sure. Like the average loss, you average the loss. Over the distribution. So you take all the losses, you average that. And then you do it. And you use that as a proxy to solve this problem. Right, you're saying it's a bad proxy. That's a few things. So data is generated by a mixture of those decisions. Exactly. Exactly. That's why this is a bad idea. Exactly. That's why this is a bad idea. Can I make another comment? Sorry? Can I make another comment? I think there are also people outside this room that are smart. That is true. I guess the part that I will say is that everybody in this room is definitely smart. But I'm not making the same comment about people outside of this room. But thank you for the correction. Okay. So you're being recorded. Okay. People who were invited. People who were invited and said no are also smart. Okay, so this is actually more than just like what I call the standard approach. This is a pitfall of, in fact, not thinking about where to collect data after having already some information about your distributions and tasks. Essentially, what you actually can show is that if you collect an aquary fixed size data set from B1 to BK, as long as it's a query fixed before you actually see anything from these data sets, Fixed before you actually see anything from these data sets, you're essentially ignoring the varying distribution difficulty and relevance. And you're going to have a lower bound that essentially says that, so kind of like if you're collecting it in one shot, you're going to have to collect an amount of data that linearly increases on the number of tasks that you have to do, or on number of distributions. In fact, it's k times the sample complexity of this single task. So, what this is really saying is problematic in the sense that what you're saying is. Problematic in the sense that what you're saying is without an adaptive approach or an algorithmic data collection approach, multi-distribution learning doesn't save data any more than just learning any specialized purpose models without them. So what do you need to do, and this is definitely problematic for collaboration, robustness, for many other ways. So what you need to do is to then to effectively learn with multi-objective, multi-distribution learning guarantees, the learner has to actively curate and shape the data. Curate and shape the data sets that you're working with and not just scale those data sets. This is what I've been referring to as on-demand sampling. It's kind of like active learning, but the difference that you're actually sampling X's, not just Y given X's. So in words, on-demand sampling is when you generate your training data from wherever you need it, whenever you need it. So you kind of actively decide where to take XY samples from the distributions you have access to. Now with Access to. Now, with on-demand sampling, what you can imagine is that you are going to collect samples adaptively. For example, you'll take some number of samples from the eye, then you see how the eye is doing compared to other distributions, and then you decide if you're going to collect more samples from the eye or you're going to collect samples from other distributions. And if you do that, essentially the blow-up in the sample complexity is going to be logarithmic rather than mere k. In fact, I'm going to. In fact, I want to be a little bit more precise because it's actually nicer than logarithmic. What we are saying is that if you do this in the right way, you actually only get an additive. So you pay the single task complexity plus something that is only k, k long k over some squared. Is it login? So this is the mention. Good question. Give you one second. If you don't, you're multiplying these lucky. Okay. Multiplying these look. Okay, so Chai is asking: if this was internet, what would it look like? Then it is D plus K times what we know, the best, it's actually not my result, the best is D plus K long K over epsilon squared. What I'm going to give you as a warm-up, if you have time, I'm not sure if you will, is something like log K times log H over epsilon to the fourth, but we don't have to pay the epsilon to the fourth. Okay? Okay. Structural risk minimization. Because in structural risk minimization, you do pay exactly this log, the number of classes that you have to search until you get your age, on top of the number of classes. Okay, let's take out offline. I don't think at a technical level I see the similarity to SRM. We have different epsilons for different tests. Right. I see what you mean, but at a technical level, I don't see how SRM could have helped. Let's talk about it offline. Okay, so now what I'm going to do is to tell you the algorithmic framework. So far, I just gave you the sample complexity and how it compares to the single distribution. I told you that you can just scale data and do your ERM longer. So if not ERM, then what? What I like to think about multi-objective learning and solving it is to look at its own. Solving it is to look at its own definition, which to me is a zero-sum game. What does it mean? I have a game between two hypothetical agents. These are not real agents. The minimizing agent is trying to find an H star within an action set of H. The maximizing is trying to find a distribution within the distributions of D1 to DK. And the loss function, which one gets the loss, one gets the opposite, is the loss of that function on that distribution. So effectively, what we're asking for is. Effectively, what we're asking for is: can I find an approximate min-max equilibria? And I think a very beautiful line of work from 90s, early 2000s is about how finding min-max equilibria actually relates to neural grade learning. This is works by Rob Shapiri and Joel Freund that really say that actually, algorithmically, somewhat the most general way we know how to get at these min-max problems. How to get at these min-max problems is to get one of these hypothetical agents to play a no-regret algorithm, and the other one can be no regret or best response. So, that's what we also want to do in terms of algorithm design. And you can think about many different algorithm design, sort of, this is one paradigm, like how to solve minimax, but many different dynamics are useful. There are a couple of things to think about, though. We are asking for sample complexity, and where is that coming from? It's coming from in two places. One is that this game, this loss function, is never This game, this loss function, is never written down. Usually, in game theory, we think that the loss is like some matrix we have access to. But to even know what the loss is, I have to estimate. So I need to have estimators which take some samples. Also, there's the rate at which an algorithm actually converges, because then how many steps do I need to do? And both of them come in play when you're thinking about optimal algorithm design. And the right trade-off is to think about, you know, both of the accuracy of the estimator. Of the accuracy of the estimator you need as well as the rate of the convergence. So, both of them actually, one of the interesting things here is that both are coming together rather than just thinking about convergence rates. So, how does on-demand sampling help? I like to think about a very particular approach when we are thinking about the Noruga algorithm as the maximizer and the minimizer as the best response. What this means is that the minimizer, the hypothetical minimizer, is given a weighted Given a weighted distribution, like weight, like a mixture of the D1 to DK as weighted by the other agent, and takes samples according to these weights and then runs the ERM on those samples. So you could kind of think about it as I have a distribution PT, its weights are decided by the second agent, and I'm just doing ERM. Take enough samples and ERM. And then the maximizing player is a no-regret person. And what they're doing is that they're maintaining essentially these alpha. They're maintaining essentially these alpha ITs, and these are proxies for how well a distribution was already being treated by the previous hypothesis that were learned. Where are the samplings coming in and where is the on-demand part coming in? Well, you have to take samples to run ERM, and you have to also take samples to know how well a distribution has been doing so far. So, for these proxies, you also need some samples. Yes? Can you clarify the no regrets or no? Can you clarify the no regrets there in agents? What is the regret here in Tokyo? The regret is so they want to be maximizing the loss. And the regret is the gap between the choices of the DIs they made and the losses that they got, added together, compared to if I fix the actions of the min player, looking at the worst case distribution. Okay, so that's where the sampling is coming in. And note that it's on demand because even On demand because each player is actually taking samples after the other player has moved. So, for example, minimizing guy looks at these alpha weights when it decides how to split its samples according to these alpha i weights, essentially proportional to those weights. And this guy looks at h1 to h t minus one to figure out how well that distribution has been treated. So another kind of wild it reminds me very much of Booster. It reminds me very much of boosting. Because when we do boosting, we look at the arrow, we change the distribution to where the arrow is, then we pick the next stage. Absolutely. So in the same beautiful line of work where min-max and Norregad are connected, same paper in fact, you get this other equivalence boost thing. This is not a sort of it's by design, I think the fact that multiple weight is like the solution to many of our problems feels Many of our problems feels like something almost celestial. So, yes, boosting, very, very much related. Okay. We can call it Steve. We can call it Steve. Celestial. So there isn't, yeah, it's an inside joke for some of the people in the room. If you go out at midnight, apparently you can get northern lights, and the northern lights are called Steve. Okay, so let's open up. Let's see. I have less than five minutes, which is which. Than five minutes, which is which we come to the point that I told you I was going to skip some proofs. These are the proofs I'm going to skip, which tell you sort of hypothetically why the approach I gave you kind of works, but it's not optimal. These are also the proofs that I am skipping, which tell you how to think about the optimal approach, but I'm going to skip it anyway. And after that, I definitely am going to skip the empirical performance, but I do want to actually say one quick thing here, which is Say one quick thing here, which is that while I told you that you know, to get the right guarantees, you should definitely think about active on-demand sampling. You might imagine that there are situations that the data set is collected for you. And you still want to know if this line of work says anything. And not only it says something at the theoretical level, actually empirically, it's led to much better algorithms. And sort of the key idea here is that usually people think about just reveiging data sets. I think that was also. I think that was also came up a lot in what Jamie presented as what practitioners do about different weighing functions. And this is definitely something we see. So, what this line of work says that don't just re-weight, actually resample it, sub-sample according to those weights. And this additional randomness that your sub-sampling is giving you actually, in a sense, is helping you do much better on, again, standard benchmarks. So, I like that you don't need to. That you don't need to really believe that your application is actually amenable to on-demand sampling. Even without on-demand sampling, there's something here to be useful, broadly. We are over time. Let me just end by saying that there are a whole bunch of really interesting other questions in this domain that I would love to see being addressed by others. Sort of one aspect is if the control of these distributions, like for collaborative learning, were done by actual agents that had. By actual agents that had other incentives. Like, they wanted to get a good outcome, but they also didn't want to do unnecessary work. They wanted the burden sort of to be the blame on the cake being fairly sort of distributed with them. The others are, where are these lost functions coming in from? If you just assume that they were given to you, but I think it's very natural, especially in today's kind of alignment of AI world, to think about loss functions not being explicitly defined. And lastly, And lastly, I would just love to really understand multi-objective learning seems to be working, but also a little bit dancing around this idea of when you can use data from some other source for others. And I think this is a very interesting problem for me, just generally, like what are the sound and optimal approaches for augmenting data? Because, like, in some sense, this adaptivity thing that I was telling you says that you can't just self-play and hope that things work. You can't just generate arbitrary. You can't just generate arbitrarily. If you're generating arbitrarily, you need this other filter to come in and tells you what part of your data is good, what part of it is garbage. I think it also says something about fine-tuning. Instead of training one model and never fine-tune, again, this says that when is it okay to go get more data at fine-tuning stage? So I think there are a lot of very natural questions that are coming up, and we're trying to think about them, but not perhaps directly addressing them within this framework yet. I would love to see more of them. Word yet. I would love to see more of that. Okay, with that, sorry, I'm going to. I went over my 30 seconds. Sorry, do you have any more? No, I have to teach. Okay. Okay, save. Okay, cool. I'm saved. Come on. Oh, I should I should say this stuff. I mean, if I