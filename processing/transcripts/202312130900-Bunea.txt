So today I would like to share some new results regarding the inference for the Wasserstein distance in topic models. And before I get started, let me acknowledge my amazing collaborators, my former student, Mike Wink, who's now in Toronto, and John Naus at NYU. The talk outline is in front of you, but let me summarize it. Front of you, but let me summarize it for you. What I want to transmit today is really three key points. One, that if you want to compare mixing distributions, the Wasserstein distance between their mixing measures is indeed a canonical distance to use, and I will give an axiomatic justification for this. And the second point is that whenever you want to Whenever you want to perform inference for the Wasserstein distance between mixing measures that are sparse in a way that I'll explain in a minute, then there are a lot of delicate things to take into account and I'll explain how to deal with them in a particular context, namely for mixtures arising in a topic model context that I will define shortly. And three, in an era in which we have challenges. In which we have Chat GPT next to us, I want to make the point that simple models like topic models coupled with good distances can still be useful and sometimes powerful tools for things like outlier or fraud detection. And I'm hoping to have time to give a very simple and I would say cute example to that effect. So, with that in mind, let me see if hopefully the If hopefully the slides are showing, let me get started. For those who are not fully familiar with the universe of mixtures, let me give a brief recap of what's going on and why I am asking the questions I've just posed. You see in front of you two plots side to side. The left one has two colors, orange and green. Orange and green, and they are of two densities. There are plots of densities that are both mixtures of Gaussians, and they are almost indistinguishable. So, if you wanted to compare these two distributions, any distance you'd use, including the Wasserstein distance, would return something pretty close to zero because the Wasserstein distance or any other distance is oblivious to how these mixtures have been generated. And in the right plot, we reveal how they've been generated. We reveal how they've been generated. Namely, the green line is a mixture of three dotted Gaussians that you see there, and the orange just two completely different from one another, and green is different from yellow. And for this reason, if you want to really compare two mixture distributions, what you compare is something that's called their mixing measure that I will define shortly. And in this case, we And in this case, we did a calculation of Aster Stein distance between the mixing measures associated with these distributions, is no longer zero. So, the question that I want to ask, and this is what folks who work in mixtures do without blinking, because it's the natural thing to do. The vast restaurant distance between mixing measures is a natural thing to use when you compare measures supported on a different number of points. Of points and the points here are the mixture distributions themselves. So, the first, the next few minutes will be devoted to explaining or more specifically proving or showing the result that we proved that says why the vaserstein distance, the not other distance, is a canonical choice. So, to set the stage for the rest of the talk, I will have some notation to introduce, not a lot. Notation to introduce not a lot. As I have said, we are talking about mixture models here. So, I'll call the what in a mixture model, what you need is a family of things to mix. So, I'll call it A, A1 up to AK are going to be the discrete or continuous mixture components. And once you give yourself some weights, which you can put in a vector in delta K, delta K will be the probability of. In delta k, delta k will be the probability simplex, so dimension k. You can form a mixture distribution, and here are two of them. The letters R and S will typically denote the mixture distribution. Sorry about that. This is something that I don't want to do. I don't know what's happening here. Sorry, I apologize. I don't know how to go back. I'm sorry about this. I don't know. I truly don't know what went wrong. I truly don't know what went wrong. This is very strange. Okay, somehow it worked. All right, and the mixing measures associated, so the mixture distribution will compare a pair of distributions. They are typically denoted by the letters R and S. So they are convex combinations of the AKs with coefficients alpha k and a mixing. And a mixing measure will be denoted by bold alpha. And the mixing measure is nothing but a convex combination of Dirac measures supported on the mixture components. The points on which these mixing measures are supported are distributions themselves. And this is what we are going to be comparing shortly: alpha and beta, and explain why that gives much more is much more sense and compare. Much makes much more sense than comparing RNS. So, the Wasserstein distance between mixing measures, just to have all the objects defined, for those who are not fully familiar with what the Wasserstein distance does, the simplest way to explain, if you have two random variable X and Y with respective measures alpha and beta, and you have a distance between the random variables X and Y, then the Wasserstein distance is the smallest expected. Expected value of the distance between x and y, and it's smallest over all possible joint distributions that have marginals alpha and beta. And for discrete, if alpha and beta are these particular discrete measures that I will be calling, as I said, mixing measures, the Wasserstein distance takes this expression where D is a distance that we have in mind. Distance that we have in mind that we can use on this to compare two mixture components. So, the question that we pose is the following. If now I know how to compare two mixture components via this distance D, say, can I lift this particular distance? Can I generalize it to It to a distance between mixture distributions. And specifically, I want to construct a new metric, let's call it SWD, that's tailored to mixture distributions in the sense that it is being the mixture. I want this new metric that I'm about to define to be jointly convex. To be jointly convex, as any good distances are, and to reduce to when applied to mixtures that are trivial, in that k equals one, this new distance that I'm about to introduce should coincide with the distance d that I had before. So, thus, we arrive at this new object, which we call the sketched Wasserstein distance for a reason that I'll. For a reason that I'll explain in a second, between mixture distributions, R and S, is the most discriminative, the largest biconvex function that respects the geometry of the A. And the most important result that I want to convey is that in the now you see the That in the now you see the side-to-side slides, you observe that in the slide I've just presented, there is no mentioning of the Wasserstein distance, except for the definition that I introduce here because I'm about to explain why I have the word Wasserstein in there, but there is no Wasserstein whatsoever. There is the abstract definition, and the theorem that we prove is that this object here is indeed a metric on the space of mixture. Indeed, a metric on the space of mixture distributions. And the cute thing is that this particular metric on the space of mixture distributions is exactly the Wassellstein distance between their respective mixing measures, alpha and beta, which is, to the best of our knowledge, the first axiomatic justification of why, if you want to compare two mixture distributions, R and S, that are in. S that are in the set S that look like this, it's a good idea to compare their or the best way in the sense of this definition is to compare them by calculating in Wasserstein distance. The Wasserstein distance acts now on mixing measures. This is a very large and rich field, and some earlier references are here, and unfortunately, Are here, and unfortunately, because of the time, I cannot give credit to those who really deserve it for having used the Wasserstein distance between mixing measures all along. Here is yet another reason why it's a good idea to do it. So, now we are gonna switch gears a little bit, having convinced ourselves that we can compare mixture distributions by simply By simply calculating the Wasserstein distance between their mixing measures, we are going to be interested in using this tool and making inference for it in the context of topic models. In the context of topic models. For the first slide here is a quick recap of what a topic model is. And in one line, it's a In one line, it's a collection of discrete mixtures with common mixture components and different mixture weights. And how does it arise? Well, it has been proposed a number of years ago in text analysis and it's still being analyzed and understood today. In fact, only in the last, I should say, five years have we understood it from a statistical perspective. Statistical perspective. And so let me give you the background using the text analysis jargon, although topic models are now used way beyond text analysis. So in the text analysis language, what we have is a corpus of documents, a collection of documents, little n of them. Each document is written using P words. So we have a dictionary of words and we view every document. We use We used the simplest possible representation of a document, namely, the one that's called the bang of words representation. That is, for us, a document gets stripped of meaning. We forget about what we know now from ChatGPT with the contextual embedding and other cool stuff in that direction. We just simply have a document viewed as the observed frequencies of the words from the dictionary. The dictionary. So, some words of the dictionary appear more frequently in a particular document. And we keep tabs of this. And so, what we basically have with each document is a draw from a multinomial distribution with P categories. N is the length of the document, the number of words. They can differ from document to document. The document is indicated by the superscript I, and R I is the. I and Ri is the vector of cell probabilities, which is the true probability of a word appearing in a particular document. And with this notation and denoting by AK the conditional distribution of words given topics, which is an element in the p-dimensional probability simplex, the topic model assumption says that each document, that is each document probability generating generating Generating probability, Ri is a mixture of the topics whereby AK, what we call topic, a mixture component, since we are AK records the unobserved conditional probabilities of words given a topic. So AK is, of course, not observed and nor is. not observed and nor is the proportion in which this topic is covered here in a document. So alpha k i are the proportions in which a particular topic k is covered by a document i. So what we have is, as I said, very simply a mixture distribution and we will have n of them, i is from one to n for each document. To each, sorry about this, I'm going completely in a different direction. I don't know what's happening here. I apologize. So I have a mixing measure that I associate with the document. And the task that I'm going to set myself is to compare pair of documents. And for a corpus event documents, my ultimate goal is to create a matrix of pairwise comparisons and take decisions relatively. And take decisions relative to that matrix. So, for the rest of the talk, since I'm looking at comparing two documents, and by document now I understand a mixture distribution, and I've learned that to compare to mixture distribution, I should compare their mixing measures. I will denote them by alpha and beta so that I don't carry a superscript with me. And the vasage time distance that I will be Be using from now on, also admits a dual formulation, which is more a minimal for inference as experience and existing literature has shown. So, that's just a dual formulation of the Wasserstein distance that I defined before. So, the task now is to estimate this object in a way that allows me to construct fully. Construct fully data-driven confidence intervals. Not an easy task, I should say, although it appears pretty simple to begin with. So let's see. There are two stages to this. One, I'd like to construct a consistent estimator. And so our first contribution is on the minimax optimal rates of estimating this distance. And then I'll move to asymptotic analysis and construct a confidence bound. Confidence bound for it. So, what's the strategy here? First of all, as I said, I'll repeat it: the whole talk subsumes that I have identifiable mixtures. After all, I compare this mixture, this object alpha, the mixture weights, and the mixture components are a feature very prominently. So, this is part of the ongoing assumption for the talk. And in the topic model, there is the growing literature on identifiable. Is the growing literature on identifiability? I think it's safe to say that it's a well-developed field by now. So I will assume that I have an identifiable topic model. And now there are many methods of estimating the mixture components. And I will just use one of them. Any one of them would work for what I'm about to say. And that, you know, the construction of this estimator would warrant a talk in itself. So just so that I make you aware. Just so that I make you aware, that's a different direction that I'm not discussing now. Once I have estimated the mixture components, one can estimate sample by sample. So what I have is I have each document, you see it here, is a mixture of this A case. And what I want is to estimate now the mixture weights. I can simply, if I observe, the only thing that is observed. Observed the only thing that is observed in this context is the frequency of the words in a particular document. So I have observed frequencies here and I can just do an MLE to estimate the mixture weights. And why not? Let's consider this estimator, the MLE, and estimate the distance via a plug-in estimator where we plug in the MLE for alpha and beta, and for A. And for A in this set over which I take the soup. And if we do that, one obtains immediately an upper bound on the distance between this estimator and the Wasserstein distance between mixing measures that reflects to think there are two terms here. The first one tells you that since we are estimating Since we are estimating an object that's essentially in dimension k, so the Wasserstein distance between mixing measures, the mixing measures are in front of you, it's alpha and beta. And if Ak were known, then this is just an object with K parameters. So you expect the square root K over N rate of convergence from samples of size N for each document. But of course, But of course, we do not know the mixture components. And the second term is the error incurred by estimating this matrix of mixture components. They are all vectors in delta P and it is known by now that this is the minimax optimal rate for estimating this. And very recently, we proved that if you want to estimate That if you want to estimate the Wasterstein distance between mixing measures in this context, this upper bound up to logarithmic terms is tight because there is also a matching lower bound. So, so far, so good. The problem would say, okay, fine. So, now show that this estimator is asymptotically normal, not so simple. In fact, we will need to use a slightly different estimator if we want to do inference. And here is how one does inference for this distance. Distance for you know, one for based on an estimator of the distance. So, there are two important steps that one needs to establish. One first needs to study the asymptotic normality or the asymptotic distribution rather of the mixture weights. And then one can apply a functional data method. So, W, I'll call it W tilde. If you have estimators of the mixture weights, call them alpha tilde and beta tilde. Beta tilde. And if you can estimate this f hat well, there are details in each step here. It is known that you can apply a functional delta method to obtain what is known relatively now for in the field of inference for Wasserstein distance, a non-standard limit, but standard in this literature, and the observed is a square root rate of convergence here. And if you want, if you insist on having a normal limit here, and we do insist that for interpretability, then what we converge to is just a maximum of Gaussians, because F transpose X is a linear combination of Gaussians and well, which is a Gaussian, so it's a maximum of Gaussians over a particular set. Particular set. So the issue here, if we go back to the application, which is that to topic models, if you think of a document as being explained, are you okay? Yeah, we have to. Yeah, we have to okay the emergency here is so sorry that we need to stop for a few minutes. Sure. Well, okay, I hope everything is fine.  Hi, Florentina. Uh, you can you can continue. Oh, is everything fine now? Yes. Now everything is okay. You can continue, please. Okay, I'm really sorry. Okay. So let me take it to where I left it. I think the only point I want to make is that if you are in a topic model framework, what happens is that if the meeting What happens is that if the mixtures come from a topic model framework, there is a meaning to them. That is, for instance, you have documents that are complex combinations of topics. So say that the corpus covers a lot of topics, politics, sport, cooking, and so on. But any given document will not cover all the topics covered in the entirety of the corpus, meaning that what we are dealing here naturally is sparse mixture. Is sparse mixtures. So the question is to construct asymptotically normal estimators of vectors that are on the boundary of the probability simplex. And very simply, if you are on the boundary of the simplex and your estimator also lies on the simplex, like for instance a maximum likelihood estimator, then it cannot possibly be asymptotically normal because it cannot fluctuate. Normal because it cannot fluctuate left and right if around the point on the boundary. So that is the where we leave the usual classical regime under which this type of problems have been studied. And what we do is in some sense classical in principle, we do a one-step update of the MLE, and that's called alpha tilde here. And there is a very And there is a very principled way in the paper of how we construct the update. Remember, we are estimating a vector in delta P by adding something to alpha hat that's the MLE and it is in delta K. We will be leaving the simplex, so we cannot leave the simplex too much. So there is a delicate choice of this update, but it turns out that it is indeed asymptotically normal, and then we can proceed to what. Proceed to what we want to do. Here is a picture, a sanity check that simply shows you that if you have zeros, if you want to estimate a zero in this context, that is a boundary point. The MLU will not be asymptotically normal and it's not a matter of sample size. It's simply a matter of what I've explained. You cannot have asymptotic normality for a point on the boundary via something inside the Something inside the space that we are bounding by a certain slice. So, putting it all together, we have obtained an estimator of this distance that we cared about. And most importantly, we know how to estimate the quantum of the limiting distribution. So, we have a fully operational battery for inference for the varsity. For inference for this Wasserstein distance between mixing measures. This is a slide rather that simply shows, as you would expect, that our method works. But this is the average coverage length of a 95% confidence interval for a particular problem. And the fact that what we are proposing works is not the point I want. Works is not the point I want to make, but rather that things that are typically proposed not for the vast distance between discrete distributions but not discrete mixtures does not work. Namely, whenever you have a complicated limit, it's common sense to try a bootstrap. And it is known that in this particular problem, you need to do a amount of You need to do a M out of N bootstrap, and M out of N bootstrap really doesn't work, and the coverage is very poor. But that is not to say that you cannot do other bootstrap schemes, which we have shown here to work. So, this is the, I'm getting to the end of my talk. This is the cute example that I promised. So what we did is we looked at the international movie database, which is a corpus of movie reviews. In this example, after we pre-processed everything, we ended up with 500. Ended up with 500 distinct words, and we have about 20,000 documents. We fit a topic model using our favorite method, and we estimate about K hat equals six topics. These are the movie themes, horror, love, romance, romantic comedy, and so on. And the impressive to us, at least, thing was that in this That in this humongous number of documents, in this humongous corpus, by calculating pairwise distances, we can find outliers and in general, this can be used for fraud detection. In particular, here is document four, which is a video game review. And here is the excerpt of the video game review. So if you go back to this, these documents have identifiers, you can always go back and read what the document is really about. Is really about, and you can do a sanity check of your mathematics. And what's happening here is that there are a number of documents, and in red, you see the document four is at a larger distance than the rest. And the confidence intervals, the not presented here, that we have for the red numbers, don't overlap with the ones for the other numbers. So, what this is saying is that This is saying that if you have, you know, topic more, depending on that, you don't have to go all the way and use contextual embedding for those familiar with the term and complicated deep learning analysis of a particular corpus for some simple tasks. And this is a simple task that we are setting ourselves here to figure out something that stands out, outliers. And it looks Outliers. And it looks like the Wasserstein distance estimated by our method and combined with a simple topic model assumption can do the job. So it's a good point to start. This talk is based on a number of works, some that have appeared, some that are under review. And I'd like to thank you very much for your attention.