As some of you know, I'm Bill Cannon. I work with Jay Dunlap and Jen Hurley on modeling the coupling of the circadian regulatory system of neurospicrassa with metabolism so that we can understand the metabolic feedback into the circadian clock. I'm going to talk today about how we build mass action kinetic models given the challenges that both John and Michael have. Both John and Michael have brought up regarding parameters and such. And I'm going to describe how we use these models to learn regulation from the ground up. So what do I mean by the ground up? Typically in biology, we think of function as arising from gene expression and leading to RNA expression. And then on to protein expression, metabolic activities, and there's the phenotypic dynamic. And that is the phenotypic dynamics or regulations encoded by the genome. And this is certainly more or less the way it works within an individual. But we've already heard of scenarios in which we remove gene program control and the cell still functions, just doesn't grow as well. So the alternative way to think about cell function is from an energetic perspective. In this regard, metabolism is what fuels the cell, it provides energetics, allows the cells to be able to. Energetics allows cells to be alive. The reactions which are required, in turn, dictate which enzymes need to be present and active, which in turn tells us something about RNA expression. But we need to think about not only energetic opportunities in this perspective, for example, harvesting energy from the environment, but also about energetic costs, for instance, producing, expressing an enzyme, cost energy. So, this alternative is operation. So, this alternative is operational really over a longer time period than an individual lifetime. It's operational on an evolutionary time period, one in which natural selection is active. And so to understand this perspective fully, we have to use a bit of an abstract understanding of what life is. In statistical thermodynamics, we like to talk about dissipative systems, such as Bernard-Rayleigh convection cells, shown here in the middle. Shown here in the middle. In this setup, one has a hot plate on the bottom and a cold plate on top, and fluid in the middle, say water. As the temperature of the hot and cool plates diverge, the water moves to the top of the cell in a more or less random manner. But once the temperature difference becomes great enough, the water becomes organized and starts moving in circular patterns. So we call these circular patterns dissipative structures. The reason these patterns form is that these patterns are the most efficient. These patterns are the most efficient way to dissipate energy from the hot plate to the cool plate. So, these dissipative structures are the most probable state of the system under highly non-equilibrium driving forces. And dissipative structures actually occur all over nature from hurricanes and tornadoes, which we're more familiar with, to nutrient cycles in the environment, the cell cycle, and the TCA cycle. So, when we say that these So, when we say that these structures compose the most probable state of the system from a thermodynamic standpoint, what we're saying is that they have the highest entropy production rates in the dynamical sense. That is, they dissipate the energy in the environment the fastest and most efficient way possible. So, these principles, when applied to biology, lead to a principle of natural selection based on maximizing entry production rates, in which part of the energy Rates in which part of the energy dissipated used to build more dissipative structures, or in this case, cells. Obviously, a generalist microbe with a relatively larger genome will require more energy for growth and maintenance than a specialist with a highly reduced genome tunes to its specific environmental niche. So given the right stringent environment, the specialist will out-compete the generalist in biomass production. So this isn't to say that This isn't to say that cells simply follow thermodynamics, and all we have to do to make predictions is understand the thermodynamics. Because once a cell is born, its regulatory program is more or less determined. But on the evolutionary time scales, if the cell's metabolism isn't matched to the environment such that it can make optimal use of the environmental resources, then it's going to be out-competed by cells or microbes that can better align their metabolic processes with the environment. With the environment. So it's from this perspective of the time scale of natural selection that metabolism and energetics drives phenotype. And so this leads us to our philosophy of building models from metabolism up. We start with the environment and a genome and characterize which metabolic pathways are the most active thermodynamically. Once we know which metabolic pathways are required, this dictates the method of the method. Pathways are required. This dictates which enzymes must also be expressed and active. But in determining whether a protein is present or absent, we also have to consider the cost involved. Synthesizing amino acids and proteins requires a lot of ATP, so it sometimes makes more sense to just deactivate an enzyme that isn't operating rather than degrade it. On the other hand, if the cellular resources are limited, the cheaper proteins can be degraded to provide raw materials for other. You provide raw materials for other proteins to be synthesized. So, both the thermodynamic costs and benefits have to be considered. So, ultimately, on the Neurospira project, we're interested in coupling this, our metabolism up model with a model of the circadian clock dynamics developed by folks at the University of Cincinnati. But what we're using our metabolism-up approach. Using our metabolism-up approach in several other projects as well, including synthetic biology projects, in which you can imagine that the thermodynamic cost to produce a desired end product has to be balanced with the energy that the synthesis of that product will take away from cell growth and viability. So here I'll only talk about Neurospira project and how we develop the metabolic model. So this gives rise to the question of just how. To the question of just how these models come about, especially with regard to the problem that John and Michael have pointed out that performing the measurements that you would need to obtain all the rate parameters for the ODEs for a reaction network of hundreds to thousands of reactions would be extremely hard, hugely labor-intensive, and entirely monotonous. So I'll go briefly into the physics of math in addressing this problem. In addressing this problem, but if you want to skip the math, I'll just tell you we basically just use the statistically most likely set of parameters. So let me explain what I mean by this. So here's a typical mass action ODE at the top where the k's are rate parameters. The n sub i's are the concentrations or accounts of metabolic species I. And the gammas are the stoichiometric coefficients for the metabolite I. Coefficients for the metabolite I in reaction J, then the challenge is that we need to know what the rate parameters are. A first approach to this that started kind of chipping away at the problem was to use ensemble modeling to do essentially a brute force search over parameter space to find the most likely model that fits the data. And that was work pioneered about 12 years ago by Jim Lau's lab at UCLA. Second approach. A second approach developed more recently by several different labs, especially George Carney-Dakis, at Brown, is to infer the correct parameters using neural networks with back propagation and iterative experimentation. However, from the statistical thermodynamics perspective, we can simply calculate the most likely parameters based on an equal a priori assumption. Basically, any rate parameters consistent with the thermodynamic equilibrium constants are possible. The key idea here is really that, regardless of the kinetic parameters, the system has to obey the overall thermodynamics of the network. Once the overall thermodynamics are correctly incorporated, tweaking the rate parameters within these bounds won't change the overall dynamics greatly. It'll slow things down, it'll speed things up somewhat, but the biological function will remain the same. So, here is how we determine the rate constants in detail. We do it using a maximum entropy method. If the metabolites were independent, then each metabolite would simply be distributed according to its Boltzmann distribution. But what we're dealing with is not technically maximum entropy, but maximum path entropy, where the path is the reaction given by the mass action ODEs shown again here. Action ODEs shown again here, which simply says the rate of the chemical reactions proportional to the concentration of the reactants, right? So we do some trivial algebra to reformulate the law of mass action into an equation with both kinetic and thermodynamic terms. And then we assume that the dynamics will all occur on the same time scale. So here are our dynamics here. Here's the thermodynamic terms in blue. We assume that they're all in the same time scale by giving. That they're all in the same time scale by giving them the same constant value. This is an equation that was derived back around 1950 after Arrhenius developed his equation, but before transition state theory was developed. In fact, this equation, the Marcelin equation, was a key breakthrough leading to transition state theory. Unfortunately, the French postdoc who developed this in 1915 was killed two years later in World War I. Later in World War I in the trenches in France. The Marlson equation is clearly wrong, but it's very convenient. And later, we'll put the correct dynamics back into this equation. The assumption is convenient because not only is the system now driven only by the thermodynamic forces, which we can easily calculate, but the energy surface is now convex. That is, it's a smooth path to a single global minimum. Global minimum. We can easily optimize the reaction rates so that the metabolites are at steady state. Once we have the steady state solution, we know the concentrations and reaction fluxes. And so we can easily back calculate the rate parameters. And then these rate parameters can be put back into the original mass action ODE, such that we no longer assume that the forward and reverse reaction rates occur on the same time scale. Occur on the same time scale. So that gives us a thermodynamically optimal set of rate parameters. But then if we really want, we can relieve this assumption of thermodynamic optimality by simply sampling the parameter space around our optimal solution. So the optimal solution is just one of many possible steady state solutions that can be found. So let's talk about these other steady state solutions a little bit. Here I have a simple Bit. Here I have a simple reaction system consisting of two reactions, three metabolites, A, B, and C. A and C are fixed, but B is free to vary. The concentration of B depends on the set of rate parameters used in the ODE, of course. We can use the set of rate parameters that gives us the thermodynamically optimal solution, or we can use some random sub-optimal set of rate parameters. Regardless, we can calculate steady-state solutions that have different steady-state concentrations. Solutions that have different steady-state concentrations of the intermediate D. All that's required is that the net flux of the first and second reaction are the same. So, here in this plot in the middle, I've plotted all possible steady-state solutions. Each one differs by the steady-state rates shown along the X and the Y axes. The X axes has the steady state solutions for reaction one, the Y axes for reaction two, and then the steady state concentrations are. Steady state concentrations are shown on B. So, as you can see by the fact that all solutions fall on the diagonal of the X and Y axes, the net flux of reaction one and two are the same. So what also differs among the solutions besides the concentration B is the work of free energy required to reach that steady state, which is color-coded in blue being very favorable and red being unfavorable. Unfavorable. And what I have plotted in the dashed line is the maximum entropy solution. So we can take a look at what happens when we move away from this system a little bit later. But what you can see is when you move away from the maximum entropy solution, your free energy starts to drop. And so these solutions are less favorable. The other thing to notice is that the maximum The other thing to notice is that the maximum entropy solution isn't the fastest steady state rate. Fastest steady state rate occurs down here. But what you can also see is that this region starts to narrow down here. So there's not as many solutions at that rate as there are up here along the border. So we can see what happens when we take this simple system and then move it up to a more complex system that has 20 reactors. That has 20 reactions, and we're going to look at central metabolism, specifically glycolysis, and the TCA cycle shown here. Bill, can I ask a question? You bet. Back to that slide. Yep. Previous slide. Is there some place in this space where you would say, say, if there's a rapid equilibrium between A and B, but the same equilibrium constant? What would that, is there something where you would see that in this space? Would see that in this space? Yeah, so it would be rapid equilibrium. I'm pretty sure it's one of these two extremes. I can't remember which one, but we definitely looked at that. So you essentially have equilibrium between A and B, and then production of C is very slow. Okay. And likewise, you have the opposite case too. B and C are in rapid equilibrium, but you very slowly go from A to B. Got it. Got it. So, here, this we're basically going to look at the same type of situation, but now we've got 20 reactions. And we're looking at central metabolism. The diagram on the left has the steady state rates plotted against the free energy dissipation rate, or this is also equivalent to the entropy production rate. And we've marked the maximum entropy. And we've marked the maximum entropy solution here and the kinetic model with the highest flux up here. And so you can see that the maximum entropy solution does not have the highest flux and it doesn't have the highest entropy production rate. But if you plot the rate of change of the probability of the system, shown over here in the right diagram. The right diagram, which is directly related to the free energy of the system, what you find is that these high net flux states are orders of magnitude less probable than the maximum entropy solution. That is, it takes less work to reach the maximum entropy production state than the fast but thermodynamically suboptimal states. States. So when we use these parameters in a mass action kinetic model without any regulation being applied, what we see is that this maximum entropy production state, we see that there are many metabolites whose concentrations are predicted to be much too high above physiological levels. So this one millimolar is approximately around the maximum physiological level that we would expect to find. Expect to find, but instead, we find eight concentrations that are predicted to be above that level, in particular concentrations for acetyl-CoA and fructose 1,6-bisphosphate are expected to be in the tens of molar range, which is well past the point where cytoplasm would become glassy and diffusion would grind to a halt. So the cell would fail to operate. So this would appear to be a failure, but it's Be a failure, but it's actually an opportunity, as I'll explain. And so, this is why I was asking Michael about Daniel Atkinson. We uncovered some of his scientific artifacts and documents way before there was web-recorded history from a time when low throughput elaborate experiments ruled the day. So, based on a set of these experiments, Daniel Atkinson, he was an enzymologist in physical. He was an enzymologist and physical chemist at UCLA, proposed that the primary role of metabolic regulation is actually to control metabolite concentrations. The reasoning is that without that control, the concentrations of metabolites, like these phosphorylated sugars and acetyl-CoA, will reach levels that make the cytoplasm highly viscous. In the extreme case, again, diffusion would grind to a halt and the cell would stop operating. So if this is true, we can kind of So if this is true, we can compare our predictions to experimental data and think about inferring regulation. So fortunately, J and Gen are very good at generating data, and several of the data sets that we have are metabolomic data sets on absolute and relative quantitation. We're working on getting metabolic flux data as well. And we have an even larger set of Larger set of metabolomics data on use from the Rabbinowitz lab. So we use this data to characterize and constrain our model solutions. Since the data on the metabolite concentrations are averaged over populations of cells, subcompartments of thin cells, and both metabolites that are free and bound to enzymes. We constrain our data to methods that using an inequality. Using an inequality constraint. And the reason is that our predictions are only for the levels of metabolites that are free in solution. But the experimental measurements measure all metabolites, both free and bound. So we use a loss function to measure the disagreement between the model and the data. And that just measures the ratio of the prediction to the observation. This allows us to determine, to quantify how high the metabolite. How high the metabolites are outside the range that we expect. If the predicted metabolite level is greater than the experimentally observed level, loss function is that of the log ratio, indicating that the metabolite level must be reduced. Otherwise, it's set to zero. And then so we can characterize, you know, which ones are out of caliber or out of disagreement. The hard question, however, The hard question, however, is how to reduce these metabolite levels. In the first approach that we developed, we relied on metabolic control analysis and specifically on concentration control coefficients, which are really just measures of the sensitivity of changes in metabolite concentrations to changes in the activities of enzymes. If you will, this just gives us a point estimate of the percentage. Gives us a point estimate of the percentage change of metabolite concentration to expect with a change in the enzyme activity. These activity values themselves are just scalar values that range from one full activity to zero. And these activity coefficients are used to scale the reaction fluxes through the ODE shown on the bottom. So to make this a little bit more concrete, I've got an example here. An example here. At the top again is the concentration control coefficient, and below in the table are four metabolites that were initially predicted to be at very high levels. The values in the table are the coefficient values for each of the metabolites across the rows with respect to four different reactions, which are the columns. And so, as you can see by surveying the table, the hexokinase reaction over here on the left has all positive. Here on the left has all positive concentration control coefficients for each of the four metabolites, meaning that if we reduce the enzyme activity, we likewise reduce the metabolite concentration. So out of the four sets of reactions, we choose to regulate the hexakinase reaction. And so we decrease the reaction, the activity value from one down a little bit. We apply the regulation, then we resolve. Apply the regulation, then we resolve for a new steady state. Most likely, those same four metabolites are still too high, and so we recalculate concentration control coefficients again, and we keep doing this in an iterative manner until all concentrations are brought down to physiological levels. So this is a deterministic approach that works pretty well on this specific problem, but we don't expect this to always be working with a convex optimism. Always be working with a convex optimization problem. So, the second approach that we developed uses a stochastic approach. We developed this because I'm pretty sure it won't be convex in the future. We use a reinforcement learning for this approach, which if you remember, reinforcement learning is the method that Google used to teach an AI system how to beat humans at the game Go. So, it would seem, you know, that this is. So, it would seem that this is a pretty sophisticated and probably really difficult to understand machine learning tool. But in reality, reinforcement learning is really just solving a Markov model using a Monte Carlo approach to search state space. The key innovation in reinforcement learning is that instead of taking just one step and then accepting or rejecting a move like you would in the Metropolis-Hastings approach, you take many steps and you record. Many steps, and you record how well you did. And these recordings of whether the use move was useful or not, when done over millions of simulations and millions of different paths, provides a way to find the most optimal region of state space for your system. What we do here is instead of always choosing the most optimal regulation as we did before, we choose it stochastically. And so we're not so. And so we're not so reliant on point estimates of the influence of an enzyme activity on a metabolite concentration. Like the control theory approach, the reinforcement learning approach is done iteratively. So we repeat this process millions of times. So we get millions of solutions instead of just one. And then from those, we choose the best solution that maximizes the entropy production rate. So So, when we look at the performance here, we show the results of both approaches when applied to central metabolism. You can focus on the green X's and the boxes on the three plots, which represent the metabolic control analysis solution for the green X's and the reinforcement learning for the boxes. What you can't see is because the scale on the left is not a log scale, is that the two Not a log scales that the two approaches actually find pretty much the same answer. There's a third approach we tried, which we call the local metabolic control approach, which didn't perform as well. So we don't really talk about that one too much. But also what's apparent in these diagrams, especially this middle one, when you compare the enzyme activities to the free energies, what we see is that the reactions What we see is that the reactions that are controlled, the ones that have regulation applied to them, are further away from equilibrium than the ones that are not controlled. And this is a well-known phenomenon that's in biochemistry textbooks. However, the reason for this that's given in biochemistry textbooks is that the reactions are far from equilibrium are the ones that need to be controlled because they control the flux from the pathway. However, what you can see from our analysis is that these reactions are first. Analysis is that these reactions are further from equilibrium than the others because they are controlled. They're just like a reastat. You turn up the resistance and the potential drop across the resistor increases. So here you can see that after we apply this regulation, all of our concentrations are now controlled. They're under the one millimolar limit that we set in this case. The only exception is that we have a very important This case. The only exception is succinate, which is barely above the one millimolar level, and that is a highly soluble metabolite. So it's not too surprising that that's a little bit high. When we compare our predictions to what's available in the literature, we find that all the enzymes that we predicted to be regulated are known to be regulated in a range of different experiments. So this doesn't prove that the method. So, this doesn't prove that the method's right, but it does show that we're on the right track. To really show that this method is correct, we need an elaborate experimental setup really in which we test for regulation of each of the enzymes that are predicted to be regulated, and even those that are not predicted to make sure we didn't miss any. But setting up such an ex vivo system would be pretty expensive and labor-intensive, but possibly in the future, it might be a lot. But possibly in the future, it might be able to directly test the hypothesis of that part of that primary reason for regulations to control metabolite concentrations. But we can at least be comfortable in that the theories seem sound and the experimental data don't contradict the principle. So currently we're working on the next logical step for control, and that is with respect to natural selection. With respect to natural selection. That is, it's not enough to control metabolite levels alone. In order to be selected for the next round, the micro really has to reproduce faster than its competitors. So in this regard, we're looking at optimizing growth by additionally turning down some metabolic pathways to channel material and energy into the reproductive pathways. In the previous model, all pathways were allowed to be active and regulated. Regulation is only applied to control metabolism. Is only applied to control metabolite levels. Now we actually want to maximize growth as well. This problem is definitely not a convex optimization problem, but nevertheless, we did use some nonlinear optimization methods to aid in the development of our objective function. And we're now implementing this in the reinforcement learning framework. But so far, the results look pretty encouraging. I'll go into that in just a second here. So, this is what our current So, this is what our current model that we're applying this to looks like. This is the model of Neurospa Crasa. It's got 200 reactions, they're all modeled by mass action kinetics, 225 metabolites. The set of reactions includes all essential metabolism, secondary metabolism, nucleotide triphosphate synthesis, amino acid synthesis, cofactor synthesis, cell wall synthesis. These are all de-novosynthesis. Synthesis, these are all de novo syntheses, and then DNA and RNA production. The model right now has minimal degradation or lipid pathways in it. And in the model, what we hold constant are the external glucose concentration, the ATP, ADP, AMP, and then the NAD and ADP DPH ratios, inorganic phosphate, CO2, and you can. Phosphate, CO2, and you can see the others. Then, ideally, we use this to run our simulations. And so, on our initial results, what it looks like is this. What we found for optimizing growth is that regardless of boundary conditions that we use, that is those substrates that were concentrations that we hold constant, specifically the NAD. Specifically, the NAD and NAD pH ratio, the method separates out the synthesis of RNA and DNA because they compete for the same resources, the nucleotide triphosphates. That is, whenever we try to maximize the growth, we either end up maximizing RNA production or DNA production, but never both at the same time. Whether DNA is synthesized or RNA is synthesized depends on how we set the relative level of NAD. At the relative level of NADPH and NADP. When NADPH levels are high, DNA is produced, otherwise, RNA is produced. Likewise, when NADPH is low relative to NADP, pathway runs and glycolysis is mostly shut down by regulation. So we seem to be mimicking some behavior of what we know happens in the cell cycle, but we need to be able to complete them all by adding in. To be able to complete them all by adding in fatty acids, sterol synthesis, and also the degradation pathways. And then, ideally, if we want ATP and ADP levels and those cofactors to be free variables, we also need to add in oxidative phosphorylation. Then when we have a more complete model, I think we'll be able to really analyze this along the dynamical systems approach. And that would be really interesting to see if something resembles. interesting to see if something resembling the cell cycle emerges from this without having to program it into it. And so we're not only using this for looking at metabolism, but we're actually coupling metabolism to growth of a mycelial mat on a different project with UC Riverside. This is looking at soil microbes where the metabolic model drives the cell. Drives the cell wall synthesis and the hyphal growth. And then eventually we'll be adding in bacteria, pseudomose, in this effect. And we really want to look at the benefits, the synergy between growth of the mycelial network and the bacteria when metabolic exchange occurs. And then also on the synthetic biology projects, we use this to characterize those synthetic pathways. Those synthetic pathways as far as the free energy of producing our target metabolites, the power and resistance across those pathways. And if we have the right chemostat experiments, we can even characterize the growth of a microbe itself. We did that for E. coli, and it looks like the free energy of an E. coli molecule is about zero, and that's when you compare. Zero, and that's when you compare it. We can actually calculate a pseudochemical potential for a microbe if we have the right chemo set. That's not to say that a microbe has a standard chemical potential. It just says that we can define a reference chemical potential. And so with that, I think I'll end it and thank our contributors on this, a number of people at PNL, Mark Alber and his group at UC Riverside. Group at UC Riverside, and then Jay Dunlap and Tina Kelleher, and Jen Hurley, and Megan Jankowski, and Hannah DeLos, who developed some of the metabolomics analysis. Thank you.