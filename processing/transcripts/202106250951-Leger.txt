And Flavia is going to talk about TIDLER expansions for entropic optimal transport. Go ahead, it's your floor. All right. Thanks, Vlad, Sumik, for the introduction. I'm especially happy to be able to present this in front of this audience. So I created my slide on this website called slides.com. So I will be sharing my screen and you can follow along. But if somehow the If somehow the internet connection on your hand is not that good, what I can do is I can also in the chat paste a URL. And so if you want, you can also follow kind of directly locally on your computer, especially if there's some internet issue. And you will be able to just follow with me along as I change the size. But otherwise, you can also just look at the screen. Okay, so this is by the way. So, this is by the way, a joint work with Pierre Roussillon, François Xavier, and Gabriel Pelec. Okay, so let me start quickly with a little bit of background. So, here I wrote the primal and the dual form of entropic transport. So, I assume it's going to be very familiar to all of you. So, I won't spend too much time, but just let me highlight. Too much time, but just let me highlight the entropy H here. This is the product measure, we have the cost C and we have the dual potential phi Ms. And the value we recorded as optimal transport epsilon. Okay, and mu and u are the two measures. And our setting is that of continuous measures, mu and mu and And new and general but smooth costs C. Okay, so different from the semi-discrete setting, for instance, or the discrete to discrete setting. And what I will present you today is a first order Taylor expansion of the optimal potential that I will from now denote phi epsilon as epsilon goes to zero, as well as a As well as a second-order Taylor expansion of the transport value OTF0. And so this turns out to be a difficult task, like much more difficult than we thought when we first started this project. And mainly due to the fact that at strictly positive epsilon, all the equations, all the quantities are smooth. But at epsilon equals zero, the optimal transfer problem, the equations become singular. The equations become singular, we're dealing with a singular plan concentrated right when you have an optimal map on the singular set. Sorry, the maximizer phi is a measure here? So here, phi is the maximizer of the dual problem. So it's the kind of Kantorowitz potential or like the entropic potential. Thanks. This is the Schrodinger potential, which is probably the Schrödinger potential, exactly. Let's call it the Schrödinger potential. The shooting of potential. Yeah, and I mean, maybe one simple way to just see why this might already be difficult is that you will have to probably rely on regularity of optimal transport because to hope to have a terror expansion of the form, you at the very least need unicity of the Katorovich potential phi zero. Okay, so the motivation. Okay, so the motivations to do these sort of things are there are several. So I can list maybe some theoretical motivations to understand the convergence as epsilon goes to zero of the entropy problem to the optimal transport problem. There's also a connection. If you know how the potentials converge, then there's a link between that and like a large deviation type principles. For instance, the one presented. Principles, for instance, the one presented by Promet Kosar a few days ago. On the more computational side, since we know that in many cases, the entropic problem is easier to solve than the non-regular optimal transfer problem, then by having this kind of relation, for instance, let's say you're interested in phi zero, the control potential, or its gradient, the Moj map for quadratic transport. Moj map for quadratic transport. Then you can approximate it well by taking the difference between the left-hand side and then the first-order term here. Then you could do the same kind of things for the optimal transport value. All right, so what was known on the potential in general is that the potentials are known to converge to the control-based potential in various settings, some very general ones with general costs. ones with channel costs with third-general spaces or also on the dynamical models so on the more like a schrodinger bridge type of problem again on very very general kind of spaces and a few days ago also uh jonathan showed us that in the semic discrete case he proved a first order tayle expansion and he showed that the first order term was zero okay but the semicrete case is very different But the semi-discrete case is very different from the continuous to continuous case. And in fact, what we obtain is very different from this. A little bit more is known on the value for quadratic costs. So quadratic costs are known to be equivalent to dynamical problems called Schrodinger problems. And in that case, using the specific form of these dynamical problems, it has been shown by confirming. It has been shown by Comfort Yetanini that the following second-order expansion holds. And in particular, the second order terms involve degrading in time, a kind of feature information term on displacement measures. And much closer to our setting, there's this result of Schumi Pat, which showed a first-order expansion in the static case and The static case and for general smooth costs. And so, what we will do today is basically add the next term here in the expansion. And now I want to explain a little bit why, at least in the quadratic case, you can easily guess formally what this first order term of the potential will be. Okay, so this is how it goes. Okay, so this is how it goes. We know that you can obtain, in general, potentials by taking derivatives of this kind of cost or action with respect to, in this case, the second variable. So because phi is on new space, we take the derivative with respect to new and new phi. On the other hand, like I showed in the previous slide, we have available asynthetics of the Asymptotics of the entropy cost, and in particular, the epsilon term takes the following form. And so, by combining these two things, we can get the first order epsilon term, which is here the left-hand side, right, taking epsilon equals zero, by taking two derivatives, two partial derivatives, one with nu, one with respect to epsilon. So, what you get is a derivative with respect to nu of the entropy, right? And so, formally, what you expect is the first term to be this kind. This guy. And this is exactly what we show. So, in the first part of the talk, I will only state result for the quadratic costs, just because it's simpler to state. And then in the second part, I will explain how to generalize it for more general costs. So for now, we look at the quadratic cost. And so our result is to show that we have this. That we have this asymptotic expansion. And I will give a more precise theorem later. And this asymptotics for φ allows us to derive a new asymptotics for the cost. So this is in particular for quadratic costs. And so I said a few slides ago that it was known that the epsilon squared terms was, in a dynamical case, which is equivalent here, a time integral of official information. And so this is a new. Of the fisher information, and so this is a new formula that is equivalent, right? They are equal. We don't know exactly why, but this is just expressed with static quantities. Sorry, but what is the S? Yeah, so here there's a lot of things that I didn't define. So here, S is the map, optimal map that goes from Y space to X space. From Y space to X space. And new is already new, we know what it is. And this G are also this matrix here. So you use the Jacobian of the map or the Hessian of the potential to define a metric G. Exactly. Exactly. And this is what I will talk about in the second part of the talk, how to understand. Second part of the talk: how to understand these things more geometrically. So, for now, like let's let me be a little bit vague about this just to show that this is kind of the expression that you get. It's not very complicated, but it's also not very simple. And there's some strange terms that appear. So, an important idea in this work is to use strong concavity of the dual functional. So, on the first line here, I recall. So, on the first line here, I recall the kind of classical dual formulation, which is a maximization over two potentials by m sign. And if you want to have as much strong contravity, or let's say strong convexity as you want, it's a good idea to already eliminate one of the two variables. So taking a maximum website, and in the end, just having an optimization problem over only one variable. And so this is called the summary problem. And so it evolves the submitual functional that I wrote here. That I wrote here. In particular, it looks very much like the semi-dual optimal transport functional that I wrote below. And it involves this term, which some people call a soft C-transform. And also the reason that I wrote the non-regular functional is because for that case, it's very clear why a semi-dual approach gives you much more in terms of strong convex. gives you much more in terms of strong convexity than dealing with both potentials, right? So this functional, as we will see, as we will show, is strongly, we can show under some condition that it's strongly concave. If you were dealing with two potentials, then we know that for the optimal transfer case, the objective function is linear actually. And then you have some you have some constraint. Okay, so it would be much harder maybe to talk about strong convexity in that case. Talk about strong convexity in that case. Okay, so eliminating one of the two variables that's a good idea for strong convexity. And one of our tasks will be to show that this functional j epsilon is uniformly convex, sorry, it's convex uniformly in epsilon. And so in particular, you get it at epsilon goes to zero. So here I wrote the formulas for the Hessians. The formulas for the Hessians that's what we need, right, to study convexity or concavity. And that's actually maybe one of our contributions: is that to show that it's a good idea to write Hessians in the following form, right? As like last test, the way you would write it when you work on Hilbert spaces, for instance. So Hessians tested against test functions. And maybe the exact form doesn't exactly matter right now, but you see that you get this singular one over epsilon terms. Epsilon term and it also you can see that these entropic plants appear in the expression right so that's the entropic plant for the entropic problem and you compare the expression for the Hessian of the non-radical rise functional so we somehow need to show that these guys are strongly concave and that it holds at the limit when epsilon goes to zero And maybe just since, yeah. So maybe slightly beyond the purpose of this talk, and also I think since today is like kind of maybe a more numeric session, I wanted to also highlight personally why understanding this session has been important. So I showed that you can understand synchron as a kind of gradient descent. So it's in a position something called a So, it's in a position something called the mirror descent on the dual variable, so on the file. And so, understand this kind of the convergence rate of this kind of method, or if you want to accelerate this kind of method, you want to design pre-conditioner for this kind of method, then it's always a good idea to understand the hashing. And even for the non-regularized problem, for the optimal transport problem, with a colleague, we derived an algorithm. An algorithm that scales in the sense that it works in infinite dimension. So it doesn't, the conversion doesn't depend on the problem of the discretization of the problem. And so for this, it was important to also study kind of concavity properties of the semi-dual function. Okay, so now I'm ready to go over the strategy of the proof and And what I did is on the left hand side, I wrote the actual strategy that we used. And to make it a little bit easier, on the sorry, on the right-hand side, I wrote like a kind of simplified strategy if we were working in finite dimension, like if everything was working as in finite dimension, and in particular, if all the norms were equivalent, then life would be simpler. So the first step is to show this kind of strong concavity of the dual functional. So you can see that. Dual functional. So you can see that because of this term, if you're familiar with this notation, so these are subola spaces, you can see that this is actually not exactly, it's not really a strong concavity, but it's in the same spirit. And in particular, if you were in finite dimension, you would be simply showing a strong concavity of your functional. And you know that when you have strong concavity, then you kind of control the distance to between two points, right? So the distance between two points, like here, is controlled by the distance between the gradients. The distance between the gradients. Here it's a little bit more complicated because we have to deal with different norms. For the second step, if you define the competitor as being the first two terms of what you expect your Taylor expansion to be, right? So in this case, we expect it to be this guy. Then the exercise, the goal is to show that the derivative at this competitor is At this competitor is a big over epsilon squared. And furthermore, like even if you didn't know what this term was, this equation can actually determine, like help you determine what it is, right? So this gives you uniquely, up to higher terms, what the first two terms of the expansion should be. So you choose your phi hat such that you have this kind of bounce. And then the last step is. And then the last step is simply to combine these two. So you see that phi epsilon is the maximizer of your problem. So the first term vanishes. And so then you control the distance between your guess, phi hat, and the true maximizer. And you can show that it's epsilon square term. And so this is what will happen in finite dimension, like if everything, if all the numbers were equivalent. For us here, it's a little bit more complicated. Bit more complicated, and so at the end of the day, we don't exactly get a big o of epsilon squared, we get a quantitative bound which is a little bit weaker, okay? But it's still enough to work, okay. And so for the main steps that I just showed, we need to use a kind of custom version of a Laplace expansion to see why. To see why, to convince yourself why, maybe we can just look at this entropic plan. And when this entropic plan is integrated against various measures, this is typically the setting of Laplace expansion. So this is what we show. This is what we obtain. U is a non-negative function which is zero at a unique minimizer. The left-hand side here has a form. Has the form of a Laplace expansion. And what we show is the first two terms of the expansion for multi-dimensional quantities. And maybe I should say a few words here. So we use sigma as the Hessian of U to be kind of like a metric. So you like to define on your on R D a metric by the Hessian of U. Then this quantity here This quantity here in geometry is the natural volume form associated to geometry. And so you see that it shows up here. And we also show that this quantity here, which is a little bit complicated, is nothing than the curvature of your space with this metric. So the scalar curvature. And here, if you're not used to the geometric notation, when you have repeated indices like this. When you have repeated indices like this, it means you just sum over them. And so our contribution for this formula is on two fronts. So, first, we interpret these terms in a geometric light, try to explain this is a curvature, this is a volume form. And then, maybe more importantly, for our purpose, we need to bound the error for left-hand side minus right-hand side in a quantitative way. In a quantitative way in some sovereign spaces. And so, to the best of knowledge, this was new. What's the R here? R is the function that you're integrated against. It's just maybe the H from the previous slide. Sorry, which quantity? It's the H from the previous slide. It's just a test function? It's just a test function. We will. Function, we will sometimes apply to the h from the previous site, sometimes apply to other quantities like h squared. So we use, so this is kind of a standalone proposition. We use it two or three times in the work for different quantities. And yeah, maybe I can say a word like, you see, we want R, R is integrated, so we want to see R as a kind of a density, right? So N form, and so that's. And so that's why we only want to make a bit like this radonicodium kind of derivative between R and the natural Riemannian taste. But anyway. So Flavia, this is so if I if I want to understand this previous expression in terms of the language that I have written in the paper, so I take this object called divergence, which is the slack variable, and you extract the Riemannian metric out of it, and then you and you consider the curvature out of it. And you consider the curvature out of this, right? Is this the correct way of thinking about it? Yeah, that's exactly the correct way to think. I see. So, okay. I mean, Leonard is probably here. I mean, this is very information geometric. So maybe he. Yeah, yeah. So I will talk a little bit even more about this later. Yeah, but exactly. Exactly. Okay. And so now we can state the main theorem a little bit more precisely. I state also here in the quadratic case. I state also here in the quadratic case. So, we need to make a few assumptions. Some are more important than others. Let me just quickly go over them. Maybe that's a pretty big assumption to make. We need to assume that the solutions pi epsilon stay bounded in some big subolet space uniformly in epsilon. And so, we need to assume this because we're not aware of any kind of conditions on the measures only, mu and nu that would guarantee. Only new and new that would guarantee this uniformly in epsilon. Maybe a second big assumption is that we for now only work on the whole space RD. We haven't quite figured out how to adapt this kind of Laplace expansion ideas to, let's say, bounded domains. And maybe a few words also on this one. We somehow need to have conditions on the densities so that they don't vanish, for instance. And so here I just wrote in terms of being bounded above and below by. Terms of being bounded above and below by Gaussians, but there could be other kinds of settings which could be, which could also work. And so then we showed that the difference between your maximizer and your competitor is more than epsilon, epsilon one plus delta for a strictly positive delta in some H1 kind of sublime space. And the constants key here would depend on all the previous quantities. And oh, and there's something, oh, yeah, I for. And oh, and there's something. Oh, yeah, I forgot to say also, something actually quite important. We also need to assume that there's a zero missing here, that the Cantor Rich potential, phi zero, is six, has six derivative in L infinity. So we kind of assume a lot of this regularity, but it's needed. Yeah, and then once you show like properly this syntax on your potential, then you get also. Your potential, then you get also for free in some sense asymptotics on your entropy curve. And as you can see, that's the formula I showed earlier. This formula is not so nice, right? I mean, especially the quadratic term. And so it's because it's not so nice, it leads to consider more general cost and the geometry to kind of try to understand maybe these terms a little bit better. These terms a little bit better. Okay, so now I want to explain quickly in one slide for people who don't know this kind of seminal ideas from about 10 years ago of Kim and McCann. And I will simplify a little bit the exposition. Okay, so now we're just kind of taking a step back for one slide and we're just considering, we're back to optimal transport in the general. To optimal transport in general spaces and with general costs, most likely so from now on, we want to look at general costs, not the quadratic cost anymore. Okay, so assume that you have these two spaces, x and y. So here I just represent that line, but of course they could be like much more general. And you have four points. So you have two on x and two on y, psi, sorry, x, x plus psi, and then y and y plus eta. Okay. Ela. Okay, now moreover, you assume that you have gamma. So, gamma is just you assume it's a support of an optimal plan, and so in particular, it's a C cyclical monotone subset of X times Y. And of course, you can see gamma as a matching between X's and Y's. And so here we assume that X is matched to Y and at that point is matched to that point. And then you look at this quantity here. And this quantity And this quantity tells you the error in cost that you would make if you matched the points to the wrong ones. So if you were matched this point to that one and that point to this one. Because you see, this is the cost of x matched to y and then that point match to the point. And then you subtract to that one. Now, as you take psi and eta smaller and smaller, right? Infinity means small, what you obtain is this quantity. So you can Quantity, so you can maybe quickly check in your mind that if you do a Taylor expansion with respect to ψ and eta, all the first-order terms will vanish, all the zeroth order term will vanish, all the first order terms will vanish, and then the second order term will be this one, and then it will be high order terms. And so the amazing idea was to consider this as a metric on the support of like on this. Of uh, like on this subset gamma, okay, and so then it means that, oh, yeah, and maybe I uh there's something that I forgot to say is that because of C cyclical monotonicity, this quantity is always non-negative. Okay, because this is basically a consequence of C cyclical modensity that you know, if you do the wrong match to the correct match, you get, of course, a non-negative quantity. And so, then what you get at the end is a rematch. So, then what you get at the end is a Riemannian metric on gamma. So, this is a kind of a different view where you represent gamma as a curve in an x-y space. And maybe the precise form of the metric G is not so important right now. But I think the very important thing to remember is that without any structure on your spaces, X and Y, just with the structure of your cost, you get a natural Riemannian metric on gamma. Riemannian metric on gamma. Okay. And so this was studied maybe in the length of information geometry by Schumik Paul, Leonard Wong, and Young. And also here, I have to say that I simplified the exposition a lot because what him and Macan do is not restricted only to gamma, but they consider in general a metric over the whole product space. So it's more general and More general, and it's a semi-metric, not a Riemanni metric. But I wanted to keep it a little bit simple. Okay, and so now that we have defined this metric, we can go back to our asymptotics questions. Now that we have a natural metric on our space, when you have a metric, you can already define this first quantity. This first quantity M, which is the square of the terminal. This is in Riemannian geometry the natural density, the natural end form associated with geometry. So what it means is like the equivalent of the Lebesgue measure, but on Riemannian manifold. The next thing is to define, and this is a little bit more complicated, to define a differential form which is related to this object called uniform. Object called in information geometry the cubic tensor. This is a kind of a classical object in information geometry. Okay, so what you get in the end is a differential form on this thing. All right. Now what you do is you solve an elliptic PD that I wrote here. And what this elliptic PD means in some sense is to find a projection of this one form onto exact forms. Exact forms. Okay, so this would be just the derivative differential of a scalar function xi. And this elliptic PD we see only has natural geometric quantities associated with your problem, the measure nu, the Kim-Mackan metric G, and then this one form which comes also from the metric. Sorry, can I ask you about the BI? This different method. Yeah, the BI is mysterious. Yeah, the i is uh mysterious. Well, I mean, so just the only thing if I mean, somehow I don't quite understand the notation, but if the derivative was the covariant derivative and if g was the Levi Civita derivative is not the covariance associated to Levi-Civita, okay, it's the projection of the Kimakan covariant derivative onto gamma. I see, okay, and so this is different from the And so, this is different from the Levici Vita connection. In information geometry, people study different connections than the Levici-Vita. They study pairs of connections. They call it dual connections. They have two connections, whether you project to X space or to Y space. And the Levitic Vita actually is the sum or the half sum of these two connections. And so then it means that for this connection, the metric is not like this covalent. The metric is not like this covariant derivative is not zero. And that's why, actually, in the quadratic case, things simplify a lot. And then BI can be computed more explicitly in terms of M and Xi as well. And so with these two quantities, M, M is rather simple, Xi is a little bit more complicated, you know, defined implicitly by Pd, then we've shown what the First auditory expansion of the potential for general cost looks like. And so it looks like this. And so here I wrote it in a kind of hand-wavy manner, but what we show is like for the previous, for the quadratic cost, so we show a quantitative version in a sober space, and then with a very clear dependence on the constant and on epsilon. And yeah, and also in the polarity case, you see that this M and this psi kind of cancels out. This m and this psi kind of cancel out each other, uh, and here you see that this expression is uh very geometric, right? Because you know, you take the log of this. Um, so it always bothered me that for quadratic, you just take the log of nu, right? So why should nu be the density of the measure with respect to the Lebesgue measure? Why should the Lebesgue measure play any role? So maybe here we understand a little bit more, but it's actually you can write it as the density of nu with respect to the natural volume form as. Volume form associated with your problem. Okay, and so this is the form of the first order expansion. Oh, yeah, and maybe now a remark on this equation that I just wrote. So, you know, you could be a little bit bothered by the fact that for the quadratic cost, everything is super explicit. And here we need to introduce some kind of non-explicit quantities. So I'll just recall them here quickly. This, so I just recall them here quickly. But the point is that just psi defined by LPD. And so, if we do the same, the trick of the same ideas as before, so how to formally, how we could formally guess the form of this Taylor expansion from the known shown by Schumikpal, the known form of the expansion of the OTFM. So, in particular, it was known that this term It was known that this term here is an entropy and with respect to m. So, here we show that m is actually the natural measure in your problem. So, if you take a derivative here, then what you would get is that you would guess that the first order term would be a derivative of this entropy. And at this point, your stock or you know that it's going to be a little bit complicated because the way that this measure M depends on. Measure M depends on you is pretty complicated, it's pretty normal. Like already, the way that the measure depends on the potential is hard, and then you would need to relate how the potential depends on you. So, I think the point of this slide is just to say that it's not surprising to have maybe a little bit complicated expansion of your potential. And I don't think there's any hope in general to simplify this. Any questions? Maybe I should take a little break. Do I have more? You still have six minutes. Yeah. Okay. Yeah. So now that we have, so it's like for the quadratic case, now that we have kind of a clear idea of what the asymptotics of the potentials are, we can derive asymptotics for the cost. Okay. And so this is And so this is what the second order tele expansion looks like for the cost with for the entropy cost with general cost C. I wrote it down just so that it's here. It has a few quantities which are geometric in nature and then are explicit here, but not yet completely clear to us. And so we are almost there, but our goal is to really understand this term here. This term here purely in terms of geometric quantities. In particular, you can see that this term here looks like a feature information and this term here looks like a term that we also had before and that I vaguely alluded to. And I said it was a curvature. And so this is my final slide. Is my final slide. We haven't, so we haven't quite, in the general case, been able to do it. I think we're going to be able to do it soon. But at least for the quadratic class, we've understood that the epsilon squared order terms are made of Fischer information types of terms and curvature kind of terms. And so you see here, that's like the little picture. So kappa is really. Kappa is really the curvature of your subset gamma kind of immersed in the product space. So it's an extrinsic kind of curvature, but it could also contain, of course, intrinsic curvature, right? But I think it's important that this term has an extrinsic coverage. Has an extrinsic curvature component. So it's a second fundamental form? Right. So it's related. So you could say it's the scalar curvature of the normal curvature. So the second fundamental form is basically a curvature of the normal bundle. And so then you take the Ricci curvature of that and then you take the scalar curvature. Yeah. So it's very much, yeah, exactly. Yeah, so it's very much, yeah, exactly. It's related to the second fundamental form. Uh, okay, sorry, thank you. Okay, thanks. Yeah. All right, okay, so I'll go back for his great talk. So let's say once more, it's the second fundamental form of gamma in X cross Y, where you, but you've here, you're using the, my, uh, my. Here, you're using my geometry with Kim on X cross Y? Yes, so on X cross Y, I use the Kimakamo, so your pseudo-Riemannian metric. On gamma, you use the no, so then, right, so then you have the, so the second fundamental form uses the ambient kind of metric, uh, right, exactly that you project onto gamma and then uh. And so I think it's interesting because it's an object that hasn't, I think, been much studied in this optimal transport thing before. And you can see why you would have to get a kind of entrinsic curvature. If you're in the quadratic case, then your metric is just constant, right? It's just 0, 1, 1, 0. If you're in 1D, so if X and Y were each. So, if X and Y were each one-dimensional, then the curve would be one-dimensional. And so, a one-dimensional curve has no intrinsic curvature. And so, in that case, we know that these kind of complicated terms that I came before, that I showed before, they were not fission formation terms. And so they have to be extrinsic curvature terms. And indeed, that's the case. So, thanks for this very interesting talk. And so, I want to understand. So, I want to understand the meaning of this curvature. So, for the quadratic cost, we know that the so-called C-divergence is a Brackman divergence, and the information geometric curvature is actually zero in the duly flat sense. Exactly. So, any notion of intrusive curvature here wouldn't really make sense because they will all be zero. So, we would like something like an embedding curvature of information geometric manifold in McKinsey's cumulative. Manifold in McKinsey Kimakin's geometry somehow. Yeah, yeah. Or maybe you can say like that, I think. Or maybe just you don't even need to say the embedding because since you have to consider gamma as being inside x dot y, you can only consider the Kimakan metric. That's how it's done for the second for dimensional form. But you can also consider, right? So if you have just a metric on X. metric on x like the metric on the on the whole space on x dot y then it gives you one metric on gamma and one metric on the normal bundle of gamma but to get the tensor on the gamma you need to do that horizontal or vertical projection so yeah exactly exactly any other questions uh hi uh can i ask a question hi can i see the previous great talk can i see the previous slide uh 20 21 page 21 oh i see okay i just i don't know why i i missed anything so in in in the next slide so did you uh consider any uh connection to the mattering of one curvature with this formulation i tried to but it seemed to be uh different objects okay But I don't want to say anything too definitive, but it seemed to be because I was wondering why you had to separate the official information and discover term. They seem to play different roles, but so you mean that you think they could be merged into one entity? I was wondering, maybe because this OT quantity is more like a global, while this college is local, so maybe that's why you need this feature information to compensate that. Yeah, yeah. I mean, for me, it kind of makes sense that the curvature, I mean, at least in this 2D picture, would play a role, right? So you, OT epsilon is like blurring your gamma. So if you have curves with different curves. Curves with different curvature, it makes sense that it would affect the expansion here. Sorry, the Fisher information, I mean, does it involve just tangential derivatives along gamma? Right, right. So somehow you're separating it into the derivatives along gamma and then the terms that come from the embedding of gamma in the ambient space. Yes, exactly. But I don't know in the so for the quadratic cost the curvature of x dot y is zero. I don't know for general cost if this term I think would probably contain both, but there's this ghost Codadzi formula that relates all this kind of curvature. So in most general case, there might be three curvatures maybe: the curvature of the ambient space, the curvature of gamma as a manifold, and then the extrinsic curvature. Maybe out of these three, two are. Maybe out of these three, two or like a stool degree of freedom. But I expect to have at least one more kind of right. Well, you don't have the in this case, gamma has intrinsic curvature, but it doesn't play much. Its role is hidden. It doesn't play a role, it seems. What do you mean, sorry? In this case on the screen? In the case on the screen, you only have the extrinsic curvature of gamma, not oh, yeah, sorry, extra. Exactly. Yeah, exactly. Yeah. So I don't know why having a curved ambient space would suddenly. Why having a curved ambient space would suddenly make the intrinsic curvature of gamma play a role, but yeah, that's true. Yeah. Giovanni, you raise your hand. Go ahead. Yeah. Thanks, Flavian, for the... Hi, Giovanni. Nice talk. Hi, Flavian. I had two questions. So the first one was, are there some easy to check assumptions on the measures so that the regularity assumptions The regularity assumptions of your main results are known to hold. So, if I go back to the very beginning, so the answer is not that I know of. For instance, I think this is maybe the strongest. Right, so I think it leads to a kind of an open question of could. question of could could there be a kind of a unified theory of um entropic problems and an epsilon equals zero problem so this is maybe what's missing to obtain like we don't know any any inform any assumptions on the measures that would make this uh true that would uh guarantee that phi epsilon is bounded uniformly in epsilon in some large surplus so we need So we need to kind of assume it extra so it's not right, it's not distributed ugly. It's a it's you really need global bounds, it's not enough to have uniformly on compact sets or something like that, like on every on every compact set that this holds uniformly in epsilon that I don't know that that that I think you might get from existing theory, okay. I was also wondering if your method for expanding the potential does also give at least at the heuristic level ideas on the next terms. Yeah, yeah, yeah, it does. So I think, so you see at this level, you could replace the epsilon square. You could replace epsilon squared by an epsilon cube. So you need to find a comparator which would have three terms, which kind of make this quantity vanish epsilon to the zero, epsilon to the one, and epsilon to the two. And if you do that, the same proof should work the same. The problem is then to obtain this version of Laplace expansion. So the same kind of version. So the same kind of version with, so that I didn't write here because I didn't want people to go mad, but so it's a version that has precise bound in sobola spaces. Already I know the, I think the next order terms, you can derive them, but they are very, very, you know, long and complicated. So I don't know, it will be maybe that's the main the main point. But yeah, this method of proof I think should be maybe uh And maybe, uh, maybe a third question now that I remember something I wanted to ask along the way. So, all your arguments based on you know concavity or semi-concavity, they are a bit reminiscent of the properties of the value function in control. And since there is always some Hamilton-Jacobi hidden in duality, I was wondering if you could have an interpretation in terms of the convergence of asterisk. The convergence of a stochastic problem towards a deterministic control problem of your results. This is a little bit beyond my competence, I think. So this I don't know. Okay, so one quick question. So, does your cost, the example of cost, cover the WP cost for P larger than two? My cost needs to be My cost needs to be six times differentiable. Yeah, I think it's yeah, yeah, I think so. But then again, I need the same bounds that you have to make in your paper. I need to know that the C divergence is strongly convex and then also has convex boundaries. So this is even maybe more complicated. But actually, I was wondering if you could if that could be. Could be if that could be a consequence of MTW. Yeah, somebody really needs to sit down and calculate this bounce on C divergences. I mean, some of these bounds should be followed from existing geometries. I just don't know how to do it. Somebody really should sit down and derive these bounds. This is just a fundamental question. How is the transportation cost deviating as you move away from the optimal curve? Optimal curve. I mean, this is just a fundamental question. Yeah, and you see that vergence. I mean, even this week, we've seen that they popped up many times, right? Promethe called them rate functions for his large deviation principle. Jonathan called them the delta ij. You guys call them C divergence. Yes. They are just. They're just all the same. Yeah, they're just all the same. It's all this. I think Robert and Yanghun say this. This is the cross. Yang Hoon says this is the cross cross. What do you say? This is a cross something in your cross difference. This is just a symmetrized diversion. Right. So they're all the same. Yeah, it just pops up everywhere. Right. When you're cost to the logarithm, then the cross difference becomes the logarithm of the, what you call the cross ratio on convex analysis. Right, right. That's also true. Yeah. So this just this, yeah, exactly. That's why we called it the cross difference, or at least why I called it the cross difference. Yes. Called the cross difference. Yes, and yeah, it has so many ways of looking at so many natural interpretations. So, as I said, really, somebody needs to prove these bounds of how this thing expands. Yeah. Okay. All right. Okay. So, if there are no other questions, let's thank Flavia again for his very nice talk. Thank you. By the way, could you post your paper, the arc?