For inviting me to this workshop and giving me this very exciting opportunity to speak here. But it's an absolute pleasure. So, today I'll be talking about some work related to these relaxations local Hamiltonian boxes. Part of this talk will be based on older work from a few years back with some results. And the second half will be about ongoing work and discussions with several people, including Angela, who's in the audience here. And so the main idea is like taking some. Idea is like taking some Hamiltonian problems we're interested in, in physics, in computer science, in quantum complexity, and trying to find some easier versions of these that make it tractable. So first, let's begin with this idea of a local Hamiltonian. So a k-local Hamiltonian, or in physics final, a k-body Hamiltonian, consists of terms coupling at most k qubits. Something like this. Here h is the Hamiltonian, this is a local term. And this k is typically constant, independent of number of qubits. Constant independent number of qubits. So, this is an abstract formulation of models you might encounter in many-body physics. And typically, you have some underlying interaction structure or graph associated with it, maybe a hypergraph. Here, this is a two-local Hamiltonian, this is Heisenberg model, and so you have corresponding interaction graph. Now, given such a model of a physical system, we'll be interested in various questions corresponding to, let's say, ground energy or general equilibrium problems. The ground energy is the minimum idea. The ground is the minimum eigenvalue of h, and this is something that's highly relevant. We're also interested in quantities called the partition function, which is this object, trace of e power minus beta h. This is relevant for computing thermodynamic quantities. And we'll also interest in this log, which is the free energy, which is minus 1. These are relevant to physics and also come up in various things such as statistical inference, all these kinds of stuff. So, and in general, these are very And in general, these are very hard problems. Finding ground energies, this is a QMA hard problem. And approximating party functions is even harder. There are these so-called counting complexity classes called sharp P, and the parting function is a hard problem for this Sharp P class. Okay, so these are very hard problems, and of course, we cannot solve general hard problems in polynomial time. But certainly, we can have special instances where these can admit efficient classical algorithms using. Classical algorithms using various techniques, and we know of certain results. And even in the cases where we know that the problems are hard, such as this Heisenberg model that I showed you, one might ask, well, what is the best approximation one I can get? And there are many techniques for doing these, Tensor Networks, Markov Chain Monte Carlo, etc. In this talk, I'll focus on this so-called idea of using convex relaxations to these local Hamiltonian problems and seeing what kind of results we can get using these functions. We can get using these functions. So, this is a framework to design classical approximation algorithms for these kinds of hard local Hamiltonian problems, such as computing the ground energy and the free energy. So, I'll mainly talk about the first two points here. The first is a published result with Sergei Bravi, David Causett, and Paul Vogsdam. And here we gave one of the results was an approximation of algorithm for the free energy of so called almost all dual connected Hamiltonians. Almost all dual-connected Hamiltonians. And then I'll talk about some extensions we're thinking about for geometrically local Hamiltonians, particularly 1D systems. And more recently, with some other co-authors, we wrote a paper on designing a new SDP hierarchy for the so-called quantum max cut problem, which is a version of the Heisenberg model. But I probably won't be getting into this in too much detail. Right. So the starting point for this kind of framework is this variational. kind of framework is this radiational characterization of the ground state energy and the free energy. So the ground state energy, this E of G s, this you can write as minimum over all density matrices rho of this trace of rho of energy. Equivalently, this free energy F, you can write as this minimum of the free energy function, which includes the energy plus an entropy term. And again, I might sign off because I plus sign there. So the point is, we can write these kind of radio. We can write these kind of variational formulations, but these give rise to exponentially large optimization problems, even though these are kind of in some sense convex optimization problems. So these will take exponential time, and we cannot, of course, do this. They are just intractable on the face of it. But we can, in a sense, relax these to get more tractable optimization models. And in the recent years, there has been a lot of interest in using this to get various algorithms. Here's a non-exhaustive list of papers that use these ideas. Papers that use these ideas to arrive at various results. So, the main idea is that we have this optimization which is very difficult, and now we're going to relax it to an easier optimization. And instead of keeping track of the full density matrix, which is exponential size, we will keep just stored reduced density matrices. So, we'll just keep track of marginals on subsystems of qubits of size. Of qubits of size at most k, where k will be much smaller than n, typically a constant. Now, if I were to ask you, well, give me tens of local marginals that are compatible with the full quantum state, that is very hard to do. That's checking local consistency of marginals, that's a QMA hard problem. So, that's again something we cannot solve. So, instead, what we're going to demand is that these marginals are just consistent locally. So, here I have these two marginals almost. I have these two marginals or these subsets of qubits, and they overlap here. And this local consistency or local compatibility just means that they give rise to the same two-body marginal over here. The thing that makes this more tractable or relaxation is that this margin may not be consistent with a full density matrix or even anything that's, or they may not even be consistent with the state, or anything larger than k plus 1 q. Okay? So the So the relaxation, one step of the relaxation, is that you're relaxing this notion of global consistency, which is looking at local compatibility. And this makes your data structure, of course, smaller, because if I just have all k local marginals, I can roughly store them in a matrix of size that goes like this. So for constant k, you can think of this as being polynomial size. Any questions about this? Yep, excellent. So now the second, the overall strategy. So now the second overall strategy for this kind of relaxation is this so-called relax and round strategy. So we start with a difficult minimization over this density matrix rho and some functional f. And we find an easier version where we change rho to this pseudo-state rho tilde, in our case this collection of local marginals, and then we replace the object with some f tilde. And the way we design this, this set of all these serial Set of all these zero states in some sense contains the set of all states as well as other in-physical options. In our case, this means that the set of locally compatible marginals, they consist of marginals that did not come from a valid quantum state, but a set of marginals that come from valid quantum states that's contained in those. And if we design this f tilde of rho to be convex, and we ensure this condition, And we ensure this condition, we are guaranteed that if we do this easier minimization, we get to something that's below the true minimum. Okay? Now, at this point, this is just relaxation, so we don't have a handle on how far these two are. And that's where this so-called rounding argument comes in, which is we take this whatever rotill domain, this is the pseudo state that minimizes the modified optimization, and we take this and And we take this and kind of do something to it so that we produce a valid state at the end. So, rho 0 is a valid state. And what we ensure in this kind of rounding argument is that this thing and this thing, they are in some sense close together. So, we bound the error in how much these two can differ. Now, the true minimum will always be sandwiched in between these two, and therefore we have a bound on how much this differs from this. Does that make sense? This differs from this. Does that make sense? This is sort of like a sandwiching argument. So we solve the relaxation, which gives you something like a lower bound. Then we do a rounding, which takes us to A state and therefore A feasible value. And we know how far these two are. And thus we have sandwiched the real value in between. And this idea has been used in many results in the classical CS literature. And in this work, we were building on this result by Wystewski for these so-called dense IZ models. Dense icy models. So, a few comments. So, these relaxations thus give outer approximations to these local Hamiltonian problems. You're approximating the problem from outside the set of valid quantum states. So, this is morally different from, say, the usual variational approaches you'll encounter, which is you choose a particular unsats and minimize over that family of states specified by the unsats. Specified by the amount size. And so here I talked about this locally consistent margins, which is sort of like a linear program in relaxation. There is a more structured way of doing this, the so-called quantum Lussel hierarchy of sums of squares relaxations. And these give you, for the local Hamiltonian problem, semi-definite programming relaxations. But I won't go into that because I get nothing. All right. So given the relaxation that I formulated, just this locally compatible marginals. Comparable marginals. Dealing with the energy term is easy. If our relaxation just contains all the expectation values corresponding to terms in the Hamiltonian, then we have some value for the objective corresponding to the energy term. So trace of rho h, I can just compute as this. And if this rho contains like the rho ij's, all two body marginals, I can write down some number for this object. Typically, we'll consider, we'll keep track of the k-local margin. We'll keep track of the k local marginals for k greater than t. So, for the energy, we don't need to modify the objective function as such. But for the entropy, we also need to consider modifying the objective function. And this is because the entropy kind of, as written down, it depends on the full state. But you can do this by just instead considering some kind of sum of subsystem entropies and using properties of the entropy such as subadditivity. In this particular setup, where we had an almost all to Where we had an almost all-to-all coupling, we used this so-called pseudo-entropy, which is like a sum of an entropy of some kind, some subset, plus a bunch of subsets, conditional entropies. And this was, this choice, as you'll see later, is a consequence of the rounding algorithm that we choose. But the key point is that if we have a superpoly chosen sum of subsystem entropies and this C is small enough, this is something that I Of this is something that I can compute with access to just local marginals. I can write down some number for it, and therefore, this is an objective function that I can compute based on my data structure. And we can improve nice things about it, which makes the optimization problem, the relaxation, make sense. Alright, so that's about the relaxation. So, this gives you a lower bound, and we don't yet know how far that is. And now we need a rounding argument, and in this Argument. And in this earlier work, we used this notion of so-called correlation rounding. And this correlation rounding is a classical algorithm or a classical scheme. And we came up with a quantum version of that, again building on some older work by Branto and Harrow. So what is this correlation numbering? Well, firstly, let's consider looking at a simple case where instead of just quantum states or quantum marginals, we have just classical probability distribution. Just classical probability distributions and marginals of classical probability distributions. So these are, for now, think of these as being bits or spins. These regions indicate marginals you're keeping track of and they are locally compatible wherever they overlap. So we have a collection of classical marginals which may not be compatible with a global distribution and allow us to the quantum case. And the rounding thing we want to do is we want to take these, modify them a little bit, and come up with a full distribution such that the local Such that the local marginals don't change much. But in particular, the two-body marginals do not change much. Because that will preserve the energy part of the object. That's the rough idea. And the correlation among this scheme kind of goes like the following. I choose a subset C randomly from this set of spins of size at most k minus 1, so 1 less than the maximum size of the marginal I'm tracking. And then I sample an I sample a configuration of the spins in this, or the variables in this, according to the corresponding local margin. So this p tilde of x. Once I've done that sampling, then I sample the other spins that are not in this set from the corresponding conditional distribution. So condition on this xc, I sample x1. And I keep on doing this for all j not equal, not in c. And then I will end up with a distribution that looks something like this. And this is now going to. Something like this, and this is now a quanta fi distribution, which I've constructed by just looking at my local margins. And now we can bound how much the marginals of this distribution are different from the marginals coming from this collection of locally compatible marginals. So, these were marginals from, say, an invalid distribution. This is a valid distribution. And the marginals have changed a little bit. And you want to bound how much this has changed. And the intuition behind is that. And the intuition behind is that if we condition on a small subset of variables, this makes the other ones almost independent. And therefore, this procedure does not change the marginals by a lot. And the more concrete way of doing this is by doing the following. You compute something called the conditional mutual information, the mutual information between i and j, conditioned on C, which is like a quantifier of how correlated I and J are, conditioned on C. And you can show that if you know average. And you can show that if you now average this over all pairs i, j, over all subsets c, then this is small. This falls off as 1 over k. So this is what makes this rounding argument work in the classical risk case. So you can show this visual information bound, and then from that you can go to some kind of measure on how far your marginals have changed. That's the best. Just sorry, that might be a stupid question, but this is still go to. This is still going to be a variation problem giving a lower bound, right? This will be. So the relaxation gives you a lower bound. Once you do this, you've gone to A distribution and you know how much the objective has changed. And therefore, you know you have a certification. Okay. Okay, this one C. Thanks for the question. Okay. So and from this basically by using standard inequalities, things inequalities Inequalities, basical inequality, that kind of stuff, you can show that the rounding error goes off as one of a square root of t in the energy part. So, this is in the classical case. In the quantum case, this is a little tricky. So, this kind of CMI bound, this still goes through. But now you have to deal with this conditioning part, and that is tricky because there's no immediate notion of quantum conditioning. So, we take our guidance from this paper by Brandon Harrow from 2013 on product state approximations. On product state approximations. And there, the idea is that you take your marginals, your quantum marginals, and use an information-complete POVM to convert them into distributions. And then effectively do the same correlation rounding scheme there. And this works out. You have to deal with some error terms. But basically, you get a similar approximation error. So, this deals with the energy. The entropy part is trickier. We basically have to show that this kind of rounding map preserves the energy. This kind of rounding map preserves the entry. And this corresponds to showing that, in some sense, strong sub addictivity is saturated under the map. Ask me later about this. So once we do all of this, using this correlation rounding, for this dense Hamiltonian, so delta is the denseness parameter, and this essentially quantifies how strong each coupling is compared to the average of all the coupling terms. So this just says all the Terms. So this just says all the coupling terms must be of roughly uniform weight. They can't be fightingly fluctuating. So under this denseness assumption, we get an approximation algorithm which approximates the free energy within this kind of error and has running time this. So for constant epsilon, this is polynomial time. Now this is not a great approximation. This is in some sense like a constant relative approximation to the free energy. And typically you would want like an additive approximation, but this L goes. Approximation, but this algorithm does not achieve that, and we suspect this is not probably not doable for this case of this tense hammock products. But more than that, I want to just focus on some takeaways more than the result, is that this algorithm itself, which finds this value of the free energy, this is just running convex optimization at a formal level. This rounding algorithm that I showed you, which was pretty complex, we have to use this information complete, POVM, all of that. This information complete POVM, all of that. That is more a part of the proof. That's just a certificate that what you obtained is a correct value, but it's not something you need to run. And yeah, so if you know the entropy part, this is also an algorithm for approximating the ground state energy. So this leads to an algorithm for computing ground state energy of these dense Hamiltonians. This was done in this paper by Brandon and Haruka mentioned. But there, the optimization was optimization over. The optimization was the optimization over product states, which was fairly complicated. And at least to me, the protocol that we had was somewhat simpler. But the general structure, you tell me whether that's annoying, but how does this compare to the Anderson bound? Which is like a brutally simple bound that also gives rise to a certified lower bound. It's basically just the upside-down triangle inequality for the operator norm. They come totally to. Than all. They come together to put into pieces and then you chop it off, and then you stare at it and you see that by looking at the open bound recognition patch of an open Hamiltonian, you get a certified lower bound of a true many body Hamiltonian. But is this called like any Hamiltonian, or are you thinking of just this kind of all-to-all-connected Hamiltonian? So, any of this. Any of this. I'm not sure, but I'd be happy to look at this. Because, I mean, that's like, I don't know, it's a one-line proof. It's like super simple to implement. Super simple to implement and keep pretty good bounds. It's better than many of the SDP relaxations. Okay, that's an idea. I became to know more about this. It's a cool paper. It's like at the time when physical review was still like two columns and one article would just start at some point. It's a five centimeter article. Okay. You through the introduction. Okay. Okay. I'll chat with you later about this. Thank you so much. Alright. Great. So the general idea that I want to emphasize here is that. I want to emphasize here is that you can kind of forget about this global consistency of the states at the outset when you do this kind of when you're solving these local Hamiltonian problems. And in a crude sense, you can enforce this later to arrive at your approximation. And now some immediate questions I had following this is that know something like this. So in this case, we use the CMI bound, but that was really like kind of in some sense structure agnostic. It was just using the fact that this Hammond. It was just using the fact that this Hamiltonian had auto-all-to-all connections, and that if you average over all possible pairs, all possible choices of this random subsystem, you would have some kind of decaying CMI. You're not using any structural information about the terms in your hand, you're putting it or anything like that. The one question you might ask is, when we have this kind of relaxations, is there a way to systematically leverage structural information? And I've used this particular rounding map based on this decaying CMI, but some of you might. CMI. But some of you might know there are these notions of recovery maps. If a tripod at quantum state satisfies this small conditional mutual information, you can recover the full state from its marginals by using the so-called rotated text recovery app. And you might wonder, well, can we use this for rounding states? And this kind of leads to the next part of my talk, which is based on ongoing discussions with Angela and others. And there are some cool ideas here. We are here to arrive at a result, but maybe some of you will have an idea. Result, but maybe some of you will have an idea as to how we can take that idea further. So now, can I ask first quick question? Like the scaling of the algorithm that you presented, I mean this exponential in like this delta parameter and the position. Do you think this is like an artifact of like improved strategy or do you think this is kind of necessary? My suspicion is that this might be necessary. Because in the classical case, people have analyzed what happens with these relaxations and corresponds. What happens with these relaxations and corresponding with like mean field approximations? And you kind of get similar answers. Maybe there's just no way to improve this further for these kind of almost all connected points. So give me now a few minutes to a different device. Anybody can ask other questions for the technical difficulties. So now we want to think about, well, how do we deal with, let's say, how can we think of dealing with geometric locality in this kind of relaxation schemes? So here I have this. This 1D Hamiltonian consists of nearest neighborhood terms. Think of these dots as qubits. And again, we're going to think about this ground energy problem, but more generally, the free energy problem. But I want to minimize this object. And I'm going to assume beta equals 1 because I do want a complexity because about units for that. So now I want to formulate an optimization which will involve just locally compatible marginals. Now, in the previous case, I was keeping track of locally compatible marginals. Of locally compatible marginals over all possible subsets. Here, because you have this notion of geometric locality, what you can do is instead of consider relaxation where you just keep track of nearest neighbor marginals. So choose some chunk, keep a margin over that, take the next chunk, and again enforce local compatibility conditions. And so this is something we already saw: that this is going to give you some kind of lower bound. To give you some kind of lower bound. Now we want to arrive at a similar relaxation for the free energy. So this is called the energy part. Now we want to deal with the free energy. So come up with something for the entropy part. And then we want to answer, well, how good are this outward approximation for water? So this part is pretty easy. This, we have been trying and we don't know what to do. We have some ideas, but we haven't arrived at the answer. And so these are the two questions that I would like to know for this kind of this whole thing. And what I'm going to show you is Show you is a simple first relaxation and then a simple example where you get some kind of idea as to how one might build a proof for this kind of 1D relaxations. So firstly, what does one do for the entropy? So here, you have these four subsystems, A, B, C, D. Think of this as being qubits, maybe qubits, doesn't matter at this point. And then my data structure is this pair of locally compatible modules. Pair of locally compatible functions, ProA, B C, Pro B, C D. And the local compatibility says that they match up on B C. So the energy again you simply replace with things that come from your marginals. That's fine. For the entropy, well, we would like to compute this link, but we don't have access to the state on the ABCD. So we use the chain rule, first write it as this, then use strong subadditivity, and then arrive at an upper bound to be entru. And then if we feed that in, we And then, if we feed that in, we get a lower bound to the free energy functional, and therefore we have arrived at a relaxation. Okay, so we replaced the entropy with this thing. So, it's like a sum of the entropy of ABC and this conditional entropy, B given B C. Any questions about? So, this is beta relaxation. And I guess you also have an estimate of what is how the CMI decays? Sorry. If you know how the conditional meter information decays, presumably you get an estimate of the errors here. So, this is what we'd like to know. So this is what we'd like to know. Okay. Let's get that open question. Right. Yeah. So this is just for this four subsystems. But you can convince yourself that this easily extends to chain of any length. And this relaxation that I showed, this was kind of used in this paper by Pullan and Hastings, where I talked about this Markov entropy decomposition. Where I talked about this Markov entropy decomposition, it's the same thing basically. And we want to know how, can we say how close this is to the optimum, that is the true free energy. And as you might guess, they should depend on, say, how much these two terms don't commute. If they commute, you think that they should be exact, at least in this 1B case, and the size of the marginals we are tracking. And the toy example that I'll show you, which is perhaps fairly obvious, is this case of 1B commuting systems. Now, again, this is. Now, again, this is obvious because a 1D completed Hamiltonian, you can compute the free energy exactly analytically. But the main thing is kind of like the way we do it, which is kind of reframing the analysis in a different way, which might give you some ideas as to how one can extend this. Right. So, what do we do? Well, firstly, we're going to revisit some things you might be very familiar with, which is this idea. Very familiar with, which is this idea of the relative entropy. So, the relative entropy is this function, the trace of rho log rho minus rho log sigma, and it has two important properties. The first is non-negative, and it's equal only if rho equals sigma, and that it is non-increasing for a CPT heat map. Now, using these two properties, you can arrive at the following implication: is that the minimum of the actual free energy, this is the gate state, and this. And this gives you the actual log party function. It's a pretty simple proof, and you can convince yourself very easily. Here it's kind of laid out. You just, what you do is, for sigma, you just plug in the actual Gibbs state, and then you arrive at this implication, the minus log z, less than equal to. And from equality, it follows that the minimum is given by the artwork in, it's the kickstate. Any questions? This is just a basic thing. This is this you can find in John Presker's lecture notes, for instance. John Presker's lecture notes, for instance, that if you minimize the free energy, you get the log partition function and the gas state. Now, for this commuting relaxation, where we don't have a state, we just have locally compatible marginals, you can do something similar. So we can arrive at this claim at the top. And to use this, we will need this small variant of the data processing inequality, which is this thing that the relevant troop is non-zero. The relative intro is non-zero. But this is going to hold for collections of locally compatible marginals, not just states. So, here the claim is that I now have two pairs of locally compatible marginals. So, rho ABC, rho BC, rho B C D, they match up on B C. Likewise, sigma ABC, sigma B C D, they match up on MC. These are just locally compatible marginals. They are not necessarily compatible with the full state on ABC. Even for these, these two pairs. Given for means these two pairs are locally compatible marginals, the following inequality holds. That if I write down this sum, this combination of relative entropies, this is greater than or equal to zero. And the equality holds if and only if these two are equal and these two are equal. And the way to see that this holds is that, firstly, we know that this must be greater than or equal to zero, just from the usual property of relative entropy. Second part, this thing must be. This thing must be less than or equal to this because this is arrived at by taking a partial trace, which is a CPTV map. So the sum of these two must also be greater than or equal to zero. And then you can arrive at the equality condition. Any questions about that? So this is just a claim we're going to use to prove the Tombwater. So then, what we can do is, so we write down. Is. So we write down, we take this combination of relative entropies, write that down in this fashion. Then for the sigmas, analogous to how we showed that the gate state minimizes the actual free energy, for the sigmas, we plug in the marginals of the gate state over here. So we plug in these marginals. And then we just compute these things. And what happens is, if you do the computation, you go through all the steps, and take advantage of the fact that these terms of the Hamiltonian commute, things nicely cancel out. Things nicely cancel out, and you arrive at the following thing. You get that this functional plus law of z greater than equal to 0, and the equality is achieved only if this is equal to the marginal of the killed state. So this is in the commuting case. This is what happens, and this is what we expect. Now, this is again perhaps obvious, but the interesting thing is that I started with this inequality that holds for just locally compatible marginals. I plugged in some kind. Was marginalist, I plugged in some kind of guess for what the argument should be and then arrived at this property. So this is what it says here. The main idea is that plugging the dip state marginals gives me this thing. And of course, the open question is, what can one do if the terms in H do not commute? And Angela and I tried a bunch of things, which was, we tried plugging, or Angela, plugging in the marginals of the one B gives state using various properties, but somehow what happens is that. But somehow what happens is that the bounds we get are not that great. And they seem to be worse than what you would get if we just split the chain into multiple pieces and so and then compute the free energy of each piece and sum them up. It seemed to be worse than that. Very difficult question. Yeah. Shouldn't there not be like a tool in front of the minus P D C process V C? Not in this case, no. I mean you could, you could. I mean that will still be greater than or equal to zero. But to arrive at the claim with P, this sub is. To arrive at the claim with B, this surface is. So, the other way one could think about this is like looking, taking this, the analysis we saw of the commuting case and trying to interpret it even in a slightly different way. So, the general idea is: I want to guess some kind of answers for these sigmas. And I want to arrive at some claim like this: that this object must be greater than or equal to some z tilde. Some z tilde. And then maybe some property should hold for the argument, the sigmas that minimize this, which is might have decaying CMI. And why would that help? Well, in the commuting case, what basically happens is that the things we're feeding in the ANSAs there, it's a product of commuting PSD operators, which means that they have zero CMI. And effectively, what's going on is you can view this as, well, you've got these pairs of, you got a sigma. These pairs of you got a sigma ABC and you have a corresponding sigma B C D, and both of them have zero CMI, and therefore you can reconstruct sigma ABC, how much marginal sigma B C. So what, so the, and similarly for sigma B C V. So then your rounding scheme is given these two pairs of, these two marginals, is that you apply the recovery map that reconstructs sigma B C V which is sigma ABC. And this is non-rounding scheme. So this is in some sense an exact Rounding scheme which holds. sense an exact rounding scheme which holds because these things have zero CMI. In a non-commuting case what one would like to show is that something similar holds. So instead of this exact rounding scheme there ought to be an approximate rounding scheme based on this approximate recovery condition. So we have for instance this following kind of result from these works by Fauze-Renner and then subsequent work which is that if I can bound the If I can bound this error occurring this kind of approximate recovery operation in terms of the conditional mutual information. And then, so the idea, the aspiration is that we formulate a similar relaxation and we try to prove something about the argument in terms of either the mutual information or the commutator of the things that minimize this. And if one could do something like this, then one could use the approximate recovery map as a rounding scheme and then arrive at some kind. Surrounding scheme and then arrive at some kind of approximation. So, this is one of the ideas that one could try to perhaps pursue. And then, given a stupid question, I'm not familiar with these recovery graphs. As far as I understand, they are just guaranteed to exist, but for explicit wronging schemes, you would have to. So, that's the point I was trying to emphasize. The wronging scheme, it need not be explicit. It's like a certification, right? You just need to know that. Certification, right? You just need to know that it exists, and you need to know how much the error is. That is the cool part. So that's why this would be great. Oh, that's cool, okay? Yeah. So if it works for this, it would be just, you know, really cool. So try to prove either the foreknown for one to use. So that's for known for the marginals of the actual gift state. I think there are some results in that kind of thing. This is, I'm trying to argue this. This is, I'm trying to argue this for the marginals that I get from this relaxed optimization. So that's different. That's why that's where this becomes slightly different. Any other questions? Yeah, and then if this can be worked out, it'd be interesting to compare it with other known algorithms. These works like Goer et al. And I think Alvaro has some results as well. And see how much, what we get from this relaxations approach. Because it's nice to have different unusual algorithms. Nice to have different amines reports. Sometimes one performs better than the other in previous regimes. But yeah, so yeah, I'm kind of interested in seeing how much one can push this kind of relaxations ideas. Because I like this kind of structure where you're not worrying about the intricacies of the full quantum state, which, in my view, is not something you can reasonably access in polynomial time. So these kind of relaxations, they seem to be, in some sense, more Sense more tractable if you're in a setting where you're restricted to some kind of polynomial time or mutation. So that's kind of my general idea, and it would be nice to see how much we can push this, because I think there are various results from information theory and I guess this kind of analysis of give states and that kind of stuff. It would be interesting to port these over in this kind of algorithm setting and seeing what results we get. And with that, I think I'll end my talk. Thank you all for listening and hopefully. Thank you all for listening, and hopefully, this rush presentation made some sense. There's this recent paper by Bernard Farzi, who also looks at SDP relaxations for Gibbs states, but from above and below, which is kind of interesting. He kind of sandwiches the problem through relaxations of local margins. What you say compared to that approach, do you know? I don't know very well. I don't know very well. I've looked at the paper, but I think I have not been able to digest it fully. But I'd be interested to check. And if Angela has more. Yeah. Yeah. But it's very much in line with these. Most of the introduction that you said would also apply to that case. The introduction would fully apply, but the technical details, which is where the main work is, that's quite different. They use this kind of ideas from Kinesmata physics and this kind of theory, this kind of, I forget what the conditions are, there's an acronym. And that seemed to be kind of key in proving having this kind of certificate. Key improving having this kind of certification. And those I don't quite follow. But I would like to know more about those. If you're able to finish the algorithm for one dimension, more or less how it will compare to. It should be compared. Maybe maybe a little Maybe, maybe a little, I can imagine it being better, perhaps. Because, you know, like in this case, I don't need an explicit, I don't need a polynomial size description of the state, necessarily. So this seems to be a little more, there's some little more freedom. But there's one way of computing the free energy, which is, I think, maybe neither of those, which is this one, well a paper like Hara Noer, he does compute like kind of free energies in small bits and then sort of multiplies them in a right, I mean for you, I don't know if if you know. Right, I mean quickly I don't know if if you know that, you know, you compute this kind of smaller figure, you know, maybe if you have if you have simply have correlations then this works, you know, uh and this seems to be quite difficult. Yeah, so I would say I believe that this should be, if this, if we can get it to work, it should be at least comparable. But I would suspect that in some regions this is this could be something that other approaches don't because you know in this you don't need to have like uh some kind of description of it. Some kind of description of it. I don't need a classical description. I just need to have some kind of description of the margins and that A state exists approximately consistent with those. And that seems to be more, there seems to be more real room there. Maybe. I don't know. This is just a guess. Maybe there are temperature scaling, maybe? I think there should be a temperature scaling. Like, let's see. I would expect something like this, perhaps. Temperature scaling. Maybe no minus 9 there. There. So, yeah, Hamban band with minus signs clearly. They're also applying these techniques for learning the screen? I haven't, but there's this paper by Yuan Tang and others. Yeah, but there, I mean, I mean, if you just say take the the what results also by Wahara and Hunter and so on, where they solve max entropy just to get them. And there's some sort of relaxation would give you some Would give you some parameters that are promised not to be completely off or something like this. I thought about it. I'd be keen to know more about it. I mean, it's if it's just a map, that should work, right? If it as long as there's a map that fits your density matrix, that should work. This is one comp but yeah, I I you just you know, like the most abstract formulation is there is just a map, can you whatever, it doesn't matter. Sure, but you mean to know that the output is density matrix. They know the actual matrix. Okay, the map has to figure to a actual state. Just to go back to this, I mean, how well does it work in practice? I mean, I know that your emphasis on proving the theorems that you've shown, but I mean, at the end of the day, it's the method, right? So how well does it work? How well does it approximate the green? So in practice, these SDP relaxations are pretty bad. Because generally people think of these kind of relaxations where you're keeping track of marginals over all subsets, and that blows up combinatorially. And that's just horrible. And that's just horrible. But that's for an all-to-all Hamiltonian. That's all to all Hamiltonian. But then, say, for a local Hamiltonian, actually you mentioned a local Hamiltonian. Then you should be able to do something like this. But I don't think this has been implemented in practice. At least not to my knowledge. That would be cool to see. I think that would be very cool to see. I think there are papers from long back. People have looked at these reduced marginal methods, reduced density matrix methods in quantum chemistry long back. And there people might have done similar things. But I'm not. Similar things. But I'm not pretty familiar with the results to know what goes on there. But people did think about these a long time back. More recently, I don't think people have done numerics with these kind of STP relaxations. But I agree that it would be very cool to see. I know the quantum chemistry, that was like late 90s. I mean, that's a big part. I think. Yep, okay, I think so I remember you said that often the speaker curves parts are good, but the the algorithms. The algorithms that they scale well? In the dense case, yeah, the approximation was not great. But I mean, we have not analyzed, I mean, you know, like this, this is like broken progress, and we don't have a result yet. I was just wondering whether one can get from some hardness results on probably some lower bounds on like or some some lower results more probably. Quite possible. Yeah, I would imagine that's possible, yes. Yeah. I mean, generally, this will be like, you know, this should be a hard problem. Cannot work for everyone. Yeah, cannot work for everything. But the question is, like, does this give you something new? Something different? Yeah. Okay, there are no more questions that uh yeah, maybe we can attack Anifa. Yeah, maybe you can take my next one. So yeah, there's a break from now at the same time. Thanks.  I think I can find out the problems in the first one. Okay, that's it's coming up. Okay, it's kind of like a message. That the state code also keeps the control damage. Yes, so I just like want to come there from these margins jump out of the actual thing. Yeah, so I was wondering, like in the like number, you think you wanna like um also minimize the energy, but you know the marginals. So. Yes. So that is exactly what this tanned paper does. Yes. Yeah, but I mean in the tank paper they're just doing one of those. So that one the more recent one. Yeah. Like you want to match the margin of the edge state but then it gives you an expedition which is not so many features and expeditions. Have you found that that's a a problem? Subject problem that's massive. I would like to call public. Of that family. Yeah, but that you have not great fonts, but these fonts exist. So, but yeah, the tank result is not. I mean, I think it just costs so much.