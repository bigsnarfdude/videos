Okay, so our sorry mistake here in this afternoon session is we have cigarettes from Life, and we're going to speak back. Yes, I'm right. I'm going to speak about the RTK construction of the KPZ Fitzpatrick. Okay, thanks, Neil. Thanks to everybody for being here and for team of the people in the Kairi's community. So, yeah, I have been working partly on integrable probability since the time this was not called integrable probability. But yes, I think something that inspires me. Something that inspired me to start looking at the stochastic models that had integrable structures was this paper of Martin and Timo, where they looked at ASEP stationary, and then it was the first time they was proved uh big to the one third. And uh yeah, when I when I read this paper I said, Okay, this is really beautiful. Uh yeah, I want to do something in this direction. In this direction, and yes, so yeah, then yes, I put all my efforts to do something in this direction. And then I was lucky that Timo came to visit Wardek, and then when Nil was also there and told the Wardic ISK community, then Ivan joined a bit soon after. So, yeah, so this has been. So, yeah, so this has been really a starting point for me, even though there is no LSK here. It's the master of coupling. I never managed to become a master of coupling, so yeah, I found LSK and Pulse a bit easier. So when I was putting this, I was deciding to put this slide, yes, I was a bit uncomfortable, but analyst of mathematics is so big and Malton and Timo is. And Malton Timo is so small. So I said maybe that's not appropriate. But then, yes, I thought that some things are big because they stand on the shoulders of giants, giant giants. Also, what balances. Okay, so now to the topic of the talk. So the starting point is another landmark work here by Jeremy and also the And Konstantin Matetski and Daniel Remenik. So yeah, the KPZ fixed point. So what a KPZ fixed point does? So it looks at the TASEF. So you have TASEF particles at locations Y1, Y2, YK, and then you want to evolve them in time. In time. And then you want to see what is the probability that starting from initial conditions y1, y2, and so on, they end up after time t at positions y1 prime, y2 prime, and so on. And TASEP has been around for a very long time, but always it was restricted to uh special initial conditions, which actually doesn't allow to build this kind of universal Markovian process. Because well, if you if you start, let's say, from the wedge initial condition, Start, let's say, from the wedge in this subcondition, then you evolve it, you get something else, and then you want to start from this something else to go to the next step. So integrated operability was always doing, was very successful in, let's say, few initial conditions, like wedge initial conditions or flag initial conditions, or like Marton's paper in the stationary setting. So So um okay so what was the the remarkable thing uh achieved by Matthes, Sterling and Reminiqu was uh formulas were made available for general initial conditions. And I will try to describe the setting very briefly here. So integral probability tells us that you know the probability that you end up at certain locations can be given in terms of Can be given in terms of a theoretical determinant of formula. That's a general fact. The difficulty in this case is actually to come up with a manageable formula for the correlation kernel k, which will allow you to take some scaling details. And Kostel Mateski and Remening, they managed to do this in the general setting in this formula. So there is some operator. Some operator Q, which I'm going to describe in the next slide. And there is another operator S. This operator is again an SXY. And then this you can think of it as a kind of inverse Laplace transform of generating functions of certain random blocks. And there is another kernel here, S epigraph, which the remarkable thing is that it which the remarkable thing is that it can be given in terms of uh random walk uh uh functional. So you have the expected value uh with respect to uh geometric random walk which is associated to this kind of q which I'm going to describe in the next slide and then you run the random walk until some heating time and then you evaluate this s bar functional at the location of the stopping time of the random walk and this s The total time of the random walk. And this S-bar again is a contradiction, which I'm not going to write down. So, what is the random walk involved here, and what is the stopping time? So, the random walk is a geometric random walk. So, in the case of the original KPC fixed point K per, this TASEP has homogeneous rates. Homogeneous rates and the random walks basically where homogeneous random walks. But in the setting actually that I'm going to cover in this talk, the random walk is a geometric random walk and the jump probability from i to i plus 1 is given by this kernel qi and it's a geometry with parameters little qi. This will be related to the rates of the tassel and the work And the walk jumps strictly to the left, probability qi to the y minus x. And you run the random walk until you fall on the right of the points y1, y2, y3, which are actually the initial conditions of transit. So for example, here, the work starts here, makes a strict jump to the left, it's on the right, on the left of the Y1, then makes another strict jump to the left. The streak jumps to the left, it goes on. This is the first time that actually is on the right of the boundary condition. So, the formula that we had here runs this random walk until you fall on the right of the initial conditions. And then you evaluate the expectation of this functional, which is a contributor, and then you plug it in here. And this q is a transition. And this cube is a transition probability, the n minus m step transition probability of this geometric. Okay, so now that actually I hope I gave you an idea of what is the coil formula of the K-pony, which then allows you to take the one, two, three scaling limit. Let's go a bit more into details of the process, with the TASE process. The tassel process. So the tassel that I'm considering in this work has the following features. So the particle K at time t decides to jump to the right if there is no particle occupying it, the neighboring side, by this very specific probability. This is a time-dependent rate and also a particle-dependent rate. Dependent rate. And this reduction is manifested basically by Radio scale. And I'm going to come how this is done later. You have some restrictions on the parameters. Yes, these are that's they are not so big descriptions, but you need to have them and. And okay, so the updated rule of the Task is sequential, meaning that when I want to decide the particle to jump, then I look at the rightmost particle, and then this particle jumps with a certain rate. Then I go to the next one, then the particle will jump with a certain rate or not, depending on whether the neighborhood decides to apply. So, this generality of time and space in homogeneous rates can allow Rays can allow for various scaling limits. And so, inhomogeneous rates have been studied in the past, again starting in hydrodynamics from the work of Timo and Krug, and then followed by the school of Timo. And in direct operability, the first one with RSK techniques was done by Johansson. And then there are lots of works with. With their aids that have such kind of stress. But the objective of this work is to put this in homogeneous framework within the KPZ structure. Okay, so how this is done? So the tool that we are going to use is Rebosa Snec Nut correspondence. And very briefly, let me say what is the Let me say what is the RSK correspondence. Actually, there are four types of variations of RSK. So there is a more standard one, which is the row insertion. Then also you have the dual radar insertion, the column insertion, then the dual radar insertion, and the dual column insertion. And for this talk and for TASEP, the relevant one is going to be the dual column insertion, which works as follows. So LSK actually is a bijection between a matrix and a pair of young tables. When you work with the dual column insertion, you can only have a matrix where the entries are 0 and 1. What is the meaning actually of these 0s and 1 in terms of particles? So if you see, so this is the updated rule of the particle of The particle configuration. So if you see one, this means that the first particle will like to jump to the right. If you see zero next one, then this means that the second particle does not need to jump, does not want to jump. The third particle does not want to jump, the fourth will want to jump, and so on. So the number of entries, the number of columns you have correspond to the number of particles that you have. And the rows correspond to how many times. How many times you update, how much time you run the task process. If you do the combinatorial mapping, then you get two Young diagrams. So the Young diagrams consist of Young tableaus, and these tableaus are filled in with numbers. They are going to filter with numbers one up to K. In the standard LSK, you get that mostly people. That mostly people have seen, you get two tableaus with the same shape. When you do the dual column at the scale, you get two transpose tableaus. So this tableau here comes from this one, the shape of the tableau comes from this one by flipping it. Very easy. Okay, so another point of view that is going to be useful for what I want to talk about. For what I want to talk about is this mapping between young diagrams and girlfriend children patterns. So I don't want to go into very much into the details how it's come here, but yes, a Gerfan-Schellen pattern is an array of particles such that this particle is in between these two. The general idea is that the the then idea is that you associate numbers to the locations of these particles and then this number is going to correspond actually how many ones the first row is going to have the value of the second particle is going to be how many ones the twos the first row of the young diagram is going to have and so on and then the same thing you do with the second row so the second the second row here second row will actually encode the second row in the younger code how many twos you will have then how many Many twos you will have, then how many twos and threes you will have, and so on. So, when you start doing LSK dynamics with this input metrics, the evolution of this juncture block actually is going to couple two things. It's going to see the taxet in this diagonal. So, this diagonal here is going to be the particles of the taxet process. Typically, the bottom low is also It's also basically closely related to eigenvalue distributions, Dyson processes. And there is an intimate relation between the green and red line. Okay, so let's put so the previous slide was about combinatorics, and now we want to put some probabilities and then And then we put the distribution on the matrix that comes as an input to the Ramosist correspondence as follows. So the probability that wij is equal to 1 is given exactly by this. So this is the rate that the particles jump. That's why I had chosen the uh particle rate at this particular with this particular green, because this is the way this is the probability distribution that makes the model solvable. That makes the model solvable, and this means what? That you can compute the push-forward measure very explicitly. So, if you have a matrix with the newly entered in this fashion, then you can compute the probability that the shape, the shape is the bottom-up of the P-tab law is equal to lambda, and the Q-tuple, the transpose, will also have shape lambda, it can be given explicitly in terms of resonance functions. Explicitly in terms of shoot functions. So you have a product of shoot functions. So shoot functions are the generating functions of Young tableaus, but now actually I want to give to discuss a bit more path representation of shield functions. So a shield function can be encoded as the weight of a path ensemble, non-intersecting path ensemble. Non-intersecting path ensemble at this pass. So you start pass from low zero at n to the minus one, minus two, minus n and then the paths move up and to the right until they end up at the locations lambda one minus one, lambda two minus two, and so on, where lambda one is so it's the same. So lambda one will be the value of this part. Value of this particle, lambda 2 will be the value of this particle, lambda 3 will be the value of this particle. Or equivalently, lambda 1 is the length of the first row, lambda 2 is the length of the second row, and so on. And then the paths get a weight. So every horizontal step that is done on level 1 gets a weight Q1. So if this path made X steps to the right, then this segment will be. The right, then this segment will get the weight q1 to the x, and so on. Now, also, what is important for the setting I want to give later on is to apply the Gesell-Geno formula here. So it is known that if you have an assemble of uh non-intersecting paths and you want to compute the weight of it, then this can be given in terms of a determinant path. Given in terms of a determinant. And with the determinant of a path starting, let's say, at location minus i and getting to lambda j minus j. So if you get the determinants of the weight of starting from one point and ending up at one of the target, then you get the whole weight of this non-intersecting path ensemble, which is actually the shield function. Here I put a q1 hat in order. Q1 hat in order to distinguish with the Q1 operator that I had before, because the Q1 operator was the work was moving to the left, strictly to the left. And here, the picture, the particles move weakly to the right. So, for example, here it can go up and up, or here it can go up and down. It doesn't need to have a horizontal state. Okay, so also here I can encode in the same way. Can encode in the same way skew-school functions. So skew-school functions, you have the young tableau of shape lambda, and then you want to take away some file of shape mu. So the the pseudo function corresponding as a generating function of this skew-young diagram. This skew-young diagram is a skew-shield function. And this can also have a path, non-second path representations where you don't start everything from zero at this minus one path to this condition, but you can start from location mu1 at level one, and then the next path will start from location mu2, and so on. So mu1 will be the length here, mu2 will be the length here, and so on. Okay. Okay, one more element that I'm going to use about shoe functions, which is going to be very useful later on, is a different representation of shoe functions in terms of strict weak paths. And this probably brings the picture of the strict weak polymer that some people here have worked on. So I can also represent the su function. The Schu function has a total weight of non-intersecting paths that can go only up, and if they go up, they get weight one, or they can go strictly one step to the left, and then they get weight pi if the jump happens at level p i. And yes, okay, so Okay, so right, so this representations of pseudo functions is going to be quite important in because I'm going to speech these pictures later on. Okay, so now let's see, I mean the goal here is to actually get a formula for the transition probability of the tassel, and then I want to represent this formula in terms of a way. We represent this formula in terms of weights of paths, like in the previous pictures. So, how is this done? So, we use the dual column RSK and we start evolving the Gerfan-Cherling patterns. So, when we start evolving the Gerfancherian patterns on the left diagonal here, it is going to evolve as a tassel. And then there is an intimated relation between the evolution of the TASF and the bottom of the robe. Of the tassel at the bottom of the rope, which is a shape, and this relation comes to some intertwining. So, in particular, with this intertwining, you can also show that the bottom level evolves as a mark of process on its own filtration. So, here P is the evolution of the transitional probability of the tassel, R is the transitional probability of the bottom level. Probability of the bottom below, and the two transition probabilities are intertwined by some operator lambda. And actually, lambda turns out to be the kernel that you have if you write the formula in terms of summations of the shoe functions. I'm not going to write the formula, but uh yeah basically remember that uh the pixels I had in terms of paths is going to play an important role in the representation here of the law. The representation here, not alone. So, this formula was first established, I think, by Deakin and Warden, or at least this is the paper that we used to step and warden. Having this intertwining formula, then you can write the formula for the transition probability of the data set by basically taking the number on the other side by inverting it. And in this case, And in this case, it turns out that you can invert lambda and has this very interesting formula in terms of a determinant of elementary symmetric functions. So, in the paper of Dicken and Wolen, this seemed like more like a guess and check business. But here I can rewrite these elementary symmetric functions as inverses of this operator's q and q. Of this operator is Q, and Q is the resistance of the ability of the walks moving strictly to the left. And actually, if you view it in this way, then this formula becomes quite transparent. And here, for the purpose of show you, so the inverse actually can be represented as a strict weak path where I have actually a negative. So this is not strictly a transition to probability because it is a minus here. Because it is a minus here. Okay, so here we have the transition probability of P in terms of these operators. And each one of these operators will have a path representation. So if I put all the pictures, all the paths together, I get the following picture. So y1, y2, and so on are the initial conditions of the task. And you want to Conditions of the task. And you want to compute the probability to go from these initial conditions to the y1 drive, y2 drive, y3 drive, and so on. And what you see here is basically the probability to go from y to y prime as a convolution of these three operators. So lambda was intertwining. R is this. R is this strictly path going on, and lambda minus one is the obvious. I guess there should be many questions why we have this does one of them have negative transition properties? Yes, this one has negative uh weights, which is the the inverse. But uh yeah, the cold the whole maybe maybe the whole point is that um The whole point is that, yes, so this lambda we know it explicitly and it's related to the Schuylkill. And the Schuylkill, what you see here, is a pass representation of the Schuylkill. So the lambda inverse, it can be computed as a determinant of this Q inverse, the capital Q inverse operator, which also we can write explicitly Which also we can write explicitly, and by yes or no, we can have a path representation in this strict week with negative weights. And uh the R operator is also something that can be computed explicitly and is related to the shoot functions but with a strict weak with a strict weak uh representation. Okay, so we get uh we get this path path uh picture for the transition to probability of the TASE. And uh almost here you can start seeing some relations, if you are guided also by the formulas you you had, uh you can start seeing some of the some uh connections with the uh the heating probabilities that uh Jeremy and uh Ganyev and Constantine. And Granjeb and Possibly derived, but not exactly. The path going in this way somehow doesn't make things fit. So there is a technical transformation that one has to do and basically say that, okay, essentially the transitional probabilities you get from the previous picture turned out to be the same if you reflect the path instead of going from left to right. Instead of going from left to right, they go from right to left, and basically they make strict, they move strictly to the left. This is not so use, and actually, in fact, actually, this is not the correct place to do this transformation, but eventually that's what happens. Anyway, so the message that we want to keep here is that the transitional probability of the TASE can be read from weights of non-intersection. From uh weights of non-intersecting paths starting from the initial conditions of the tassel and ending up actually in this case, they don't end up at uh the terminal conditions of the tassel, but the terminal conditions of the tassel at the boundary of the Reiner sample at the bottom. And now, once you have uh uh non intersecting paths, you can start playing all the business about uh determinantal point processes. Determinantal point processes that we know since the works of Boydin and Johansson, and so on. In particular, so what I'm going to do, I'm going to district attention to the bottom of the screen. So these dots here are going to form a determinantal point process. And then basically you want to see to say, okay, what's the probability that the determinantal point process or the Leibniz sample the line of sample here is below the boundary formed by y1 prime, y2 prime, and so on. So I'm going to compress all the way from here to here to just one function. So if I go to the next slide, I restrict attention to the particle process at the bottom and then you have a pass. And then you have a bus moving along, going through these dots. And then the weight of the upper dot is this epsi, which basically is everything that I compress, all the weight that I compress from that point on. So the initial condition is here. So the initial condition on the passer are hidden in the child functions on the top. Now, this is a determinantal point process, and basically, you want to ask, okay, what's the probability that the passer is below these points here? It's the same as asking what's the probability that this determinantal process is below the boundary followed by y1 prime, y2 prime, and so on. The determinantal process is going to be this. So, q is a transition to the mobility going from one level to the other of the random walks moving street. Of the random walks moving strictly to the left with geometric steps. And then the determinant of psi is the weight that you have on the top. Now, once you have such a determinantal point closest, then asking what's the probability that the top line is below a certain Kelt is a standard thing and is given by a fairly. Is given by a Ferrarm determinant. And the Ferrarm detector has this particular structure. So people who have done detector programmability recognize this immediately. Originally, this was done, the formulation I followed here is from the work of Johansson. So let's forget just the mystify I did this. Let's forget about this candle and let's look at this. This candle, and let's look at this part. So, this part has at some point you have to invent an inverse, you have to invent a metrics M. So, the metrics M is the total weight of the path starting, let's say, somewhere far away and going with strictly weak, sorry, strictly strictly left jumps to Uh left jumps to the point J. So in the work of Genemy, so this part of the code candle was hidden under function phi, which is the bi-orthogonal ensemble. So that's actually what was also the formulation of both. The formulation of a positive samoto and spawn. That was a starting point, I think, for you for you. But if you unravel what is this by orthogonal function phi, you see that it's this weight, and then you have to invent this matrix. And the difficult thing about in integrative plurability, the main problem that one encounters in integrated probability is actually to invert the matrix. Matrix. Now, the nice thing that happens here is that actually this matrix, if you send this starting point, so when I say this starting point, you know, here, these points, I can pull them all the way to infinity. If you pull all these points to infinity, then actually this matrix becomes up to triangular. And inverting an upper triangular matrix is an easy task. Easy task. So the key point about getting the KPC fixed point formulas was actually to observe that this matrix that you have in this correlation kernel is actually invariable. In the work of Jeremy and Daniel and Posta did, so they had the input psi here. So they had the input psi here from the Schutz formulas, which is given in terms of determinants. And then actually they managed to find functions phi, which are bi-orthogonal to this input functions of psi. So here the perspective is a bit different. So we look at, you know, we go through LSK, we select these paths, and then we get a determinantal point process with the standard machinery. And then the key idea is to observe that, okay, actually, you. idea is to observe that okay actually you can you can invert this matrix because it becomes arbitrary angular and the reason that it becomes albit triangular is actually quite quite interesting and yeah I think this is something that maybe has not been used or observed or observed much so maybe let me just go very very quickly through this little computation so mij is a weight so you have a path So you have a path starting somewhere at infinity at level i, and then it moves with this geometric animal qi. It jumps somewhere, and then the next level it jumps to the left again, continues jumping until it goes at the top level. Okay, so maybe maybe a pixel here will help. So I have this path moving here and the next path was moving here. And every time you go from level i to level i plus one, then you have the weight qi until you go at the level n where the weight that you put here is this psi j which depends on the Which depends on the initial condition of the tassel. But now, if we went back to the path representations that I had before, the psi can be given as a composition of this kind of local operators. So R was a transition the path pixel in the middle, and Q inverse was the transition pixel of the top. And an observation that And uh an observation that uh uh already appeared in the work of uh Jeremy, but um not in this context was that uh these operators uh actually commute. So if you start moving all these q's to the left of the R, then you will have a Qi, Qi plus one, Qn, then Qn inverse, Qn minus one inverse, and so on. So then you start seeing cancellations. So if I is bigger than J, then Then you will cancel all the Q's and then you will just stay with the Q inverses. But the point is that the Q inverses are operators that can move at most one to the left. They can move one step up or one step to the left. So if you want to go from infinity to a point, Want to go from infinity to a point, let's say, yj, which is the existential condition, then you start from infinity. The operator also moves either one up or one to the left. So you can go just, you know, in finite time, it can go finite steps to the left. And then the next operator can go finite steps to to the right. So if you put these two pixels together, So, if you put these two pixels together, then basically you cannot start from infinity and go to some finite point with just steps that go at most one to the left at every point. So, and this happens if i is bigger than j, which means that the matrix m i j is absolutely unlike. Once you have this observation, then you can put it into Then you can put it into to get some to get the bounded value problem for the correlation canon, which was the main lemma in your work, which say, you know, from, you know, since I saw your work, that was what I was attracted to it, and then I want to understand it a bit better. So, yeah, so once you have this, you have this determinantal point process, you can write a correlation. You can write a correlation candle. You have the observation that the matrix M is invertible because it's arbitrary accurate and you can invert easily. And then there is a relevant bounded value problem that you can have. And then basically this gives the bi-orthogonal functions. And then eventually this leads to the hitting probabilities because we know that we know how to solve boundary. We know how to solve boundary valuable probability. Actually, this is quite a bit more subtle in this case, but I think that's at the point. So, yes, and I think maybe at the point I will not analyze this boundary body problem, and maybe I'll just stop here. And thank you for your attention. Because you get you get uh then some new formulas and so the formulas uh we get uh uh have the same structure but what we can do is that we can extend them so that you have a particle and time inhomogeneities. So the original work was about homogeneous tassel, but now with this setting we can. But now, with this setting, we can do it with the general homogeneous arrays. And then, yeah, this actually calls now to get some interesting asymptotics from these formulas that we have. I should also say that Daniel and Konstantin, they also extended to more general loss they can allow in homogeneities, but the starting point again is these huge formulas and then solving the biophogenalization problems. Analyzation of the work. So, in terms of the asymptotics, have you looked into the asymptotic analysis for these formulas to get to something like the fixed point, and in particular this transition region where you go from strictly convex to linear? Although there's lots of work when you start doing asymptotics. Uh no, I don't think it matters. 