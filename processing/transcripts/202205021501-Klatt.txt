First of all, sorry for the misunderstanding. Second disclaimer, this is less of a talk and more a list of questions. Since I don't normally work on interpretability in ML, just standard ML. So I'm very much enjoying this workshop. So, I'm very much enjoying this workshop to learn more about it and how it relates to my own research. My own research usually deals with machine learning and intensive care. And so what is the intensive care setting? It's characterized, imagine you're a patient on intensive care, then what happens is that you're monitored by a multitude of systems monitoring your heart rate, respiratory rates, temperature, all kinds of labels. Are all kinds of lab volumes just recording this high-dimensional time series of you as a patient? And then, what you have as a clinician, usually on top of this, is a layer of rule-based alarm systems. Let's say, for example, a patient's blood pressure drops below a certain threshold, then there might be an alarm that goes off for cardiovascular failure. And now, the problem, which also Thomas will talk more about this afternoon. Talk more about this afternoon: is that those alarms or those rules they're not personalized? So it's one threshold or like a combination of features for all the patients. And so in order to not miss any fatal events, of course, these rules are quite like tuned, quite sensitive, which also then leads to many, many false alarms, which in turn leads to something which is called alarm fatigue. And what does this mean? And what does this mean? It means that in the worst case, doctors or nurses, they might just turn off or silence an alarm which has gone off without even looking at the patient. Because they just, I mean, they just can't. It's too many alarms, so they cannot for every alarm check what's going on. Sometimes they'll just silence them. And this is where machine learning comes in, because then, with the help of machine learning, we can learn personalized models of a patient's health state and have personal models. Health state and have personalized early warning systems and alarms with a much lower false alarm rate. But yeah, the problem is that while this is nice on paper and on your own computer and a historic data set, this of course ultimately has to be translated into the workflow. And this is where trust and then interpretability come into play. So interpretability is needed for, as I just said. Is needed for, as I just said, the personnel to trust those personalized alarms, but also to tailor their reaction to the alarm. So if the clinician or the nurse has a means of knowing what exactly the model reacted to in the patient's state, then they can also tailor the intervention with this knowledge in mind. And so, yeah, when I was thinking now about interpretability and all the time series we have now in intensive care setting, there was a lot of. Now intensive care setting, there were many questions which popped up, which I guess some of them are solved, some of them are not yet solved. And I tried to group them, or I group those, name those challenges that I discovered in my own thinking. The first of all is this temporal nature of the time series, like the non-locality, which also just talked about. So it's not just that a model might react to a certain value of a feature, but sort of Feature, but sort of more development. So, for example, if you take blood pressure again, it might first go up and then drop if you're in a really critical state, or temperature might first go up and then lower if you're really going to a bad state. So I don't... Yeah, first question would be like, how are there means in the portability which can capture a model's attention on this, like, focusing on it? On this, like focusing on a trajectory rather than a single feature or a single value. The next one would be irregular sampling. So these time series, they're sampled at very different points in time. So your heart rate might be sampled fairly regular, but the lat values come in very irregular. And at any point in time, you don't always, or generally, you don't have all the features present. So then how do you gauge a model's attention on a Engage a model's attention on a certain feature if not everything is present all the time. And the last one is non-random missingness, which, for those of you which are dealing maybe with medical time series, it's a very common theme that variables are not missing at random, and the absence of a certain measurement usually does carry information. And then again, how can you capture maybe that a model is reacting not to a certain value of a feature, but actually to its Value of a feature, but actual to its absence. So, yeah, I took a quick look around. I mean, one of the more straightforward things you could do is calculate a risk score at every point in time, take the gradient of the risk score with respect to input features, and say this is my feature importance now. But this actually tackles none of those two challenges. So, neither the attention or trajectory is normal when I'm missing this. And also, you would assume that your time series. Also, you would assume that your time series is regularly sampled. Like, work of our own lab, for example, has focused on representing time series as a set. Then you sort of certain when the irregular sampling you can deal with it and say, okay, intention was put, like if you put an attention layer into the set learning, you can say, okay, attention was put on this feature at this point and another feature at this point, and this all together led to a decision. I was very thankful about the talk from Nicola Don. About the talk from Nikolai Deutschmann because he said, well, he showed how like which problems there are with attention and self-learning, and also there you cannot really capture trajectories that are important. Then another work by Thomas, for example, was focusing on representing time series as shapelets, like a collection of shapelets, and looking for statistically associated shapelets with a certain outcome. So there, finally, you have something which can pay attention. Have something which can pay attention to a certain development of a feature, but then again, there it's very sensitive to different time scales and also can regular something. So I want to close with many open questions still and I'm very looking forward to this week and also like yeah hearing your expertise maybe on interpretability and time series and discussions during the coffee break. Thank you. 