 So I think we can start again. As our next speaker, it is a pleasure to introduce Petro Pinto. To introduce Peter Pinto at Darmstadt University, Technische University Darmstadt in Germany. And he will speak about quantitative results of variants of the proximal point algorithm. So please. Thank you. I also would like to thank the organizers for the invitation and for the opportunity to give this talk. This talk. So, my idea for this talk is to discuss several strongly convergent versions of the proximal point algorithm and we will see some recent proof mining results. I will begin by recalling what is the proximal point algorithm and some basic. And some basic facts. Then we'll look at the Alpern-type proximal point algorithm and the multi-parameters proximal point algorithm. And for the second part of the talk, we'll see some splitting methods and the quantitative results that were obtained about them. So my idea for the talk is not to look To look, to focus on the technicalities and the precise expression of the information, the quantitative information that was obtained, but instead to discuss these algorithms showing some connection between them and to talk about the roadblocks that one encountered when doing the quantitative analysis. When doing the quantitative analysis. So, throughout this presentation, X will be a Hilbert space, and we say that A is a multivalue operator, is monotone if it satisfies this property. And it is maximally monotone if this property, if you cannot extend this operator while keeping this monotonicity proper. Keeping this monotonicity property. Now, one important problem in optimization, convex optimization, is to find zeros of A. So to find X such that zero belongs to A of X. We denote by zero zero A as the set of zeros of the operator A. And since we are interested in this problem, In interested in this problem, we are assuming that the solution set of this problem is non-empty, so we assume that this set is non-empty. So for gamma positive real number, the resolvent function, which is denoted in this way, is defined as the inverse of the identity plus gamma A. A and this has very nice properties. Namely, this function is a single-value firmly non-expensive map or just one-half average. And the set of fixed points of this function coincides with the zero points of the operator A. Now, what is the approximate point algorithm? It was initially introduced in In 1970, by Martinet. And in 1976, it was studied by Rockefeller. And it's defined as this in this way. And this is a well-known method for approximating zeros of maximum monotone operators. Now, this M term here is an error term and this term and this makes it easy to actually compute this iteration and all the algorithms that I'm going to discuss today were actually studied also with error terms but to make things easier we'll just drop this this error term in the course of the presentation okay so what is known about PPA rocket About PPA. Rockefeller showed that PPA is weakly convergent to a zero of the operator. Later, Guler proved that in general, PPA does not converge strongly. This brings a natural question, which is how can one modify this algorithm in order to ensure strong convergence? So we will look at some of these modifications of PPA. Modifications of PPA and then discuss quantitative results about them. In the course of this workshop, there were already many talks where one talked about quantitative results, but nevertheless, let's recap some things. So, proof mining, which is the research program that looks to analyze non-effective mathematical proofs using proof-theoretical techniques, trying to obtain Techniques trying to obtain new information. That information can be in the form of algorithms, rates, or effective bounds, but we may also weaken certain hypotheses and we can reveal through the course of the analysis uniformities of the balance. But even though these good theoretical methods, namely a functional temperature, Theoretical methods, namely functional interpretations, guide the analysis that one is doing and the extraction of the information, they are themselves absent from the end result. So in the end, we get a result that can be published in journals of the in the area that it makes sense. So it it's it doesn't include logic in the end, the end product. Logic in the end, the end product. I just want to recall something. So, a rate of convergence is a function for the convergence property. A Cauchy rate is a function. Similarly, it's a function that satisfies this, but now this time for the Cauchy property. Now, in many instances, this is not available. So, instead, Is not available, so instead, one turns to rates of metastability, which is a function that satisfies this property. So we only have regions of stability for the property that we are considering. The metastability property, now that we forget about the bounds, the metastability property is actually equivalent to the Cauchy property. Cushy property. This equivalence is non-constructive. So even though we, in many cases, depending if we can formalize the proof in a certain system, we are guaranteed to have a function phi like this. We can't then transform into a Cauchy rate. Still, it's important if we look at the convergence result that The convergence result that says that a certain sequence converges to a certain point, and then we switch to the Cauchy property because it's an easier statement and we get the rate of metastability. Then if we stuck there, then that would not be a complete treatment of the original theorem. So it's actually important to know more information about this. To know more information about the sequence. And this is usually done, at least in the examples that I'm going to show here, by looking at synthetic regularity results. So I call a rate of a synthetic regularity a rate of convergence for this limit. So this is a rate of convergence for the fact that the points in the sequence are approximate fixed points. Approximate fixed points. Sometimes I also mention quasi-rate of asymptotic regularity, which is just a metastable version of this statement here. Okay, now that we recapped everything, let's look into the open type PPA. So the open iteration is defined. So the upper iteration is defined in this way as a convex combination of an anchor point and the image by T, where T is a non-expensive map. And we start with any initial point and we have an anchor point. And there are several results about this saying that under certain conditions, under parameters, this iteration is strongly converted. Iteration is strongly convergent to a fixed point of t. Why does this matter? So this is a method in fixed point theory. And since the fixed points of the resolvent functions coincide with the zeros of the operator, it makes sense to look at these methods and try to apply them for the inclusion problem. Problem. So by combining the approximate point algorithm with this iteration, we obtain the so-called open type PPA, HPPA. So this is the same thing, but in place of T, we put the resolvent functions. Since here I have the operator A fixed, I'll just drop the notation with the upper A and just write it like this. Okay, now I'll discuss two case studies. So, this is the HPPA, just to recall. In 2011, Boycanyu and Morusanu proved that it is strongly convergent to zero of A under these following conditions. So, these two conditions are very natural and are necessary. When one is talking about the upper iteration, it says that. It says that alpha n goes to zero in a slow way. And we have this condition that the gamma n converge to some gamma, some positive gamma. And we also have this condition that was sometimes also discussed in terms of the upper iteration, as was also mentioned by in the first talk by Xu. First talk by Shu. So, for example, we can take alpha n to be this sequence and gamma n to be this one. Notice that due to this condition, we cannot have one over n or n plus one here. Nevertheless, together with Lucien, we decided to attack this problem and try and understand to get quantitative information. Quantitative information. So, throughout the presentation, I'll discuss the original proof and try to point out the relevant details of the proof. So, this original proof by Boykenyu and Morusen actually relates the HPPA with the browser type sequence and then uses a well-known technical lemma by XU. So, in a paper with Lilstian, we obtained the following quantitative information. So, first, Zn is the browser type sequence. So, Zn's are such that they satisfy this expression. So, this is for alpha n strictly between zero and one. This is a strict contraction. And so, it has a fixed point. Contraction and so it has a fixed point and which is called zn. So the gamma here in this resolvent function is the gamma of the limit of the gamma n. The first thing that we did was to obtain a rate of convergence for this limit. And this is actually the hard part of the proof. This requires to analyze Xu's lemma. Luckily, several quantitative treatments of it. Several quantitative treatments of it were already available. We just tailor it a bit to our situation, but we got the rate of convergence for this limit. And then it was possible to obtain rate of asymptotic regularity relative to this function, which is defined in a simple way, just a maximum of the rate. Of the rate of convergence and also the rate of convergence for the fact that alpha n goes to zero. We also need to have some bounding information on the parameters of the problem. So on the starting point of the iteration and the anchor points. And this point P here connects to the fact that we are assuming the zero set to be non-empty. It's possible to work this in a It's possible to work this in a different way, but we just did it this way. We also obtain a rate of metastability, and this is a very general result. So if I know that this limit holds and I have a rate of convergence for it, since we knew that by previous results by Columbia, we knew about the Back. We knew about the rate of metastability for Zn. So this outputs the rate of metastability for the Xn using this definition with the maximum. So the proof is very has this lemma. So we don't even, I didn't even wrote here what the lambda is, but all this new information is just lemmas. Lemmons that don't actually require to know what it is, the growth of the quantitative analysis is then to actually extract this rate of convergence. Now, on to the case study two. This is a this now. I'm looking at a result by Xu in 2002. This is this, if I'm not mistaken, the same paper in which Same paper in which he introduced HPPA independently of Kamimura and Takahashi. So here note that I'm taking the anchor point to be the same as the initial point of the sequence. This is just for simplicity. It works with a different set of conditions. So again, we have that these conditions are on the alpha n's. Now we assume that the gamma n goes to That the gamma n goes to goes to infinite. Now we can take in this case this natural choice for the sequence alpha n, and for example, we can take gamma n to be the sequence n. Again, the proof relies on the lemma by Xu, but now it also relies on a metric projection argument and on a sequential weak compactness argument. Compactness arguments. So we have a problem here because the proof is now using arithmetical comprehension and we wish to avoid that. So what are the actual inputs of this problem? So we have these conditions. So naturally, we look at quantitative statements for it. So sigma zero is a rate of Sigma zero is a rate of convergence for this, sigma one is the rate of divergence for this, which just gives us a point after which this sum is greater than what we asked. Tau zero is the rate of divergence for the gamma n. And we also have this measurizing information again bound on these parameters of the problem. And here we need we work with this bounding information on the gamma n and the resulting quantitative information will be uniform on the gamma n below tau 1. And what did we obtain? We obtained rates of asymptotic regularity and rates of metastability under these conditions. And it was actually possible to bypass the use of Actually, it's possible to bypass the use of the projection argument and of the sequential weak compactness argument. And this was viewed as a particular case of technique developed in a paper together with Freiera and Wilstian in 2019. This is a second. This is a secondary goal of this talk to discuss when this macro was used in removing sequential weak compactness. So I'll be pointing this out. Okay, so I want to also stress some recent quantitative studies that were done in different settings. Studies that were done in different settings. Namely, Kolenbach studied HVPA in the setting of Banach spaces and under very weak conditions. And Andrei Sipus working with the same weak conditions, but now in the setting of Katyro spaces. I have put the references to these papers at the end. And together with the comeback by analyzing a result. By analyzing a result by Suzuki, we recently obtained an algorithm which translates rates of metastability for the original outperteration into rates of metastability for the viscosity generalization. The viscosity generalization is one where we replace the anchor point with the image of a strict contraction. And so this can also be applied here and you can then take the rates of metastability from the previous slides. From the previous slides, and by using this transformation, get rates of metastability for this viscosity HPPA. Now, going on to the multi-parameters PPA. In 2008, Yao and Muhr studied the multi-parameter PPA, which I define right here. So now we have an additional term which incorporates. Term which incorporates the Xn. So this can be viewed as a hybrid version of HPPA and Kronozelski iteration. We ask that these parameters add up to one. And notice that if one day end if we vanish this term, then by this condition we recovered HPPI. As far as I could understand, the motivation behind I could understand the motivation behind introducing this version of PPA is actually to go a bit further to try and get more weaker conditions on the conversions of HPPA, but the authors were not clear, at least for me. Okay, so let's first consider these three conditions. These are actually the conditions that I mentioned in That I mentioned in this slide, these very weak conditions. So the limit of the Dauphin is zero, the sum of tauph n is infinite, and the limit of the gamma n is equal to positive. Janu showed the strong conversion of this MPPA under the previous conditions, together with this condition here. Together with this condition here. Sadly, this condition does not allow us to take lambda n to be equivalently to be always zero. So we don't recover from these results HVPA. We also work with this condition. The proof now also requires a technical lemma by Suzuki in order to establish the conditions of Shu's lemma. Shuzlam. And the problem here is that Suzuki Islam relies on the existence of a certain limb soup. Okay, so we replaced the limb soup with rational approximations, and then it was possible to obtain rates of asymptotic regularity or quasi-rates, so a metastable version of rates of asymptotic regularity and rates of metastability. Stability. When we started doing this analysis, me and Bruno, the idea was that this would be an intermediate step into the analysis of what I'm going to show in case two, where one can actually recover HPPI with very nice conditions. So we look at the proof of this by Yao and Moon, and everything looked pretty straightforward. It was very easy. Pretty straightforward. It was very easy to analyze. It was similar to previous proof mining. There was only a lemma that appeared there that we had to look into, but we are confident that we could do the analysis of that as a narrative way to talk about the second analysis, which was actually our goal. But in the end, the technical lemma by Suzuti actually proved more difficult. Actually, it proved more difficult to analyze due to this LIM soup, and we end up this turned out to be a lot more interesting than we initially thought. For the second case, the study about the MPPA, we looked so in 2012, one can prove some conversions of MPPA, but now with Now, with this condition only. So, the lim inf of the delta ends is zero. So, in particular, if I take this to be always zero, then since this goes to zero, then this becomes one minus alpha, then this delta one just goes to one. So, in particular, this is satisfied. So, we actually recover the HPPN. How does the proof work? The proof works? So, one considers this auxiliary sequence, which is the norm of xn minus x tilde, where x tilde is the projection point of u onto the zero set of a. And then the proof goes by is a discussion by cases. Either this sequence is eventually decreasing, and then we can follow the proof in the usual way using Shu's lemma. Using Shu's lemma or not, and then in that case, we have to rely on a lemma due to Munj. This appears to be a feature of these kinds of proofs where one relaxes the assumptions, the conditions that we have, and end up to have to use this proof by a distinction by cases and relying on this lemma by magic. The fact that The fact that it's a distinction by cases in itself is not a problematic point. The difficulty here, at least for us, was how to work out with proxies of the projection points while doing the distinction by cases. So we ended up having to, one considers the epsilon f instance of metastability, and then we argue that we find a good enough. We find a good enough proxy to the projection point that we don't want to have access to, and do a distinction by cases for that. And that's how this result is proven. Okay, so this was done with Dinesh recently. Okay, so this finishes the first. The first part of my talk. Okay, no, now I have to say. Okay, so we obtained rates of asymptotic regularity and rates of metastability. Also, it was possible to bypass the use of projection and sequential compactness. It's just a small remark. So here we obtain quasi-rates of asymptotic regularity, and the reason for this quasi- the the reason for this quasi comes from the the the complexity involved in this Suzuki's lemma here the the the the quasi-rates of asymptotic regularity come from this discussion by case distinction by cases where we we did it this way okay so now if we take blended Okay, so now if we take blend to be always zero, then we recovered HVPA, and the remaining conditions are actually those studied by working back in the setting of Banach spaces. So we have a less general setting, but a more general iteration. So that's how this result compares. In terms of the complexity of the bounds, I think. Of the bounds, I think since the proofs are very similar, they are of similar complexity also. Still in convex analysis, it doesn't look at the sequence with this projection point, but with the browser-type sequence, if I'm not mistaken. You can correct me in the end. Okay, so for the second part. So for the second part for the second part of my talk, we will look into splitting methods. Trying to compute, to evaluate these resolvent functions is sometimes as hard as the original inclusion problem that we are trying to solve. One way that One way that was considered to try and avoid this is by splitting the original operators into two operators. So we get a restatement of the original problem, which is to find an x, which is a zero of the sum of two operators A and B, one of which we ask to be maximal monod. The forward-backward and the double. The forward-backward and the Douglas-Ratchford algorithms have been proposed to solve this problem. But in general, these algorithms are only weakly convergent to a zero of the sum unless we impose stronger conditions on these operators, which sometimes we can do that, but then sometimes we also reduce the applicability of the methods. Okay, so this slide is about Tikhonov. This slide is about Tikhonov regularization. In the proximal Tikhonov algorithm, which I write as PROX-T, it is based on PPA, but first one changes the original operator A into this operator AN defined in this way. So this is the definition. The interesting thing that I want to remark here is that while That I want to remark here is that while in general PPA is weakly convergent, we get strong convergence for this algorithm. As such, this term mu n times the identity must be crucial in guaranteeing the strong convergence. And this motivated Bot and his co-authors in 2019 to introduce a modified version of the To introduce a modified version of the Kronozelski-Mann iteration, which I call here TKM, it's defined in this way. So we start with any initial point, and then the next point is defined as this convex combination, but we have this beta n here next to the x. And if we remove this beta n, so if we take it to be. And so, if we take it to be one, then what we get is actually the usual Karnose-Lski-Maniteration, which we note that in general is only weakly convergent. So, the conditions that we are going to be working with must prevent that if you are looking to have strong convergence. And Bottom and his co-authors show the strong convergence of this method by Of this method by asking that the batten goes to one, but in a slow way, so in particular, it cannot be always one. And we also have this sum must converge. The lambda ends have inf positive, and this sum is satisfied. So they looked at some conditions that in order to establish the strong convergence of this algorithm. The strong convergence of this algorithm. So, this is what I want to say about this. Now, what does this have to do with these inclusion problems, with these splitting methods? So they use this modified version of the Cronus LT man iteration to introduce a modified version of the forward-backward algorithm and also of the Algorithm and also of the Douglas-Radford algorithm. So I present here the definition. And A is a maximum monotone operator, and B is a single-valued co-coercive operator. Gamma is just a positive real number. And so this just corresponds to having the TKM iteration with T being the original forward back. Being the original forward-backward algorithm, which is defined in this way. So, this is the resolvent function. So, if you recall, it's identity plus gamma times A, the inverse of that composed with this one, which I think it's what gives it its name to the algorithm. So, if we put this expression in the place of t in the here, we get this algorithm that I'm This algorithm that I'm showing here. And so then we can naturally obtain conditions that show the strong convergence of this Tikhonov forward-backward algorithm to a fixed point of T, which coincide with the zeros of A plus B. Similarly, they introduced a modified version of the dark. A modified version of the Douglas-Radford algorithm. So, again, this definition just corresponds to taking in TKM T to be the original Douglas-Radford algorithm, which is defined in this way, where these R functions are the reflected resolvent functions defined in this way. And naturally, they also obtain strong conditions for the strong convergence of this method. Convergence of this method. So, together with Denise, we did a quantitative analysis on this subject and we looked at the original convergence proof of TKM and it followed similar arguments to the ones before. So, it was pretty straightforward. In that analysis, we obtained rates of asymptotic regularity and rates of metastability. It was also possible to remove the projection and the sequential compactness arguments. And as a consequence, we get also quantitative information on the asymptotic behavior on these splitting, modified splitting methods. Lastly, I want to discuss a generalization of the open PPA, where we have this alternating fashion, this method which alternates between eight PPAs between odds and even terms. So we have two operators. We have two operators again, and well, this is the definition. The only important remark that I want to make is that we have to take this anchor point in both of them to be the same. In 2012, Boyken-Morson proved that this method converges towards a common zero of these operators A and B, so naturally also zero of the sum. Also, zero of the sum under these conditions. So, both of this alpha n 1 must go to zero, one of them must do so in a slow way. The beta n and the mu n must be bounded away from zero, and they ask that these limits be one later, which was actually published before, but in a follow-up paper, they dropped this. They drop this condition. So, we are already expecting that if they are replacing this condition on the parameters of the resolvent functions, that most likely it will again involve a reasoning by discussion by cases and the use of Ranger's lemma. The proof with the condition for is arguments as we saw in the We saw in the previous proofs, and it uses the lemma by Suzuki, the one that we analyzed in connection with the multi-parameters PPA. The proof without the condition for is again a combination of the projection argument and the discussion by cases. So one looks at this auxiliary sequence where x tilde is the projection of the anchor point onto the the the the the set uh the intersection of the zero sets and if it's if the sequence is eventually decreasing then we can follow the usual combinatorial arguments and apply shoes lemma with a slight generalization to a compass to to encompass to accommodate for the the the fact that we have these two parameters the alpha n and the lambda n and if it's not eventually increasing then we have to use the Increasing, then we have to use the Menger's limb. Thus, all the techniques that we needed to analyze these proofs were done before. So it was just a question of looking into the actual proof and get our hands dirty and applying similar arguments before, tailoring them to the case at hand. There was also some technical. There were also some technical lemmas that very easy ones that allow us to look into the odd terms and the even terms and get the general results. And in 2021, we gave a quantitative treatment on the conversions of this alternating HPPA. In both cases, we obtain quasi-rates of asymptotic regularity, rates of regular Symptomatic regularity, rates of metastability, and we managed to eliminate the projection and the sequential weak compactness arguments. So, as I said, both this Suzuki's lemma and this discussion by cases forces to only get quasi-rates of asymptotic regularity. Some final remarks to recap what I talked about today. So, we did some. So, we did some overview of recent quantitative results on strong convergent variants of PPA. We talked about the Alperen PPA and the multi-parameters approximate point algorithm. We also looked at splitting methods, the forward-backward in the Douglas-Rushwood with this Tikhonov terms. And we looked at the alternating Alpern PPA. Very briefly, I was mentioning. Very briefly, I was mentioning all the roadblocks that one encountered in the way of doing this analysis. And the quantitative information that we obtained was rates, in some instances, quasi-rates opposympatic regularity, rates of metastability. It was possible to eliminate arithmetical comprehension, namely when I talked about the projection sequential with compactness or the LIM soup. And this, I think, also that the treatment, this is for people that know about this function interpretation, the treatment of the projection argument or the weak version, the epsilon version of the projection argument using the bounded function interpretation. In my opinion, but it's very subjective here. I think it was useful in allowing. Useful in allowing us to look at all these results. There is a possibility of going to the monotone function interpretation or the boundary function interpretation. And I think that for me at least, it's more easier to look at the treatment with the bound function interpretation and then not be scared of applying it to new results and get quantitative results. In terms of reference. In terms of references, I want to mention this paper where we talked about this technique for justifying the elimination of sequential weak compactness. This paper by Kolenbeck is the one where he also discusses the Alpern approximate point algorithm in the more general setting of Banach spaces with very weak conditions. There is also this preprint of Andrei Sipos. Andre Sipos that looks in the setting of Katziro spaces. This is the paper with Kolenbeck that talks about translation into the viscosity generalization. And recently there is also a paper with Lustian and a student of his that discusses this Tikhonov main. Tikhonov main iteration in the in a geodesic setting. These are the references for the papers with the original proofs. And with that, thank you. And that's it. Thank the speaker. So, question, please. So then I have a question for layman. Do you think this quantitative, more elaborated proof Elaborated proofs become easier by it. Well, in terms of the logical principles that they require, they naturally become easier. That's one of the goals that one pursues. Now, as all these finite results, they naturally become more complicated to read. But when one is doing this quantitative analysis, these studies, one actually has to look at what depends on what and where this argument is used with what goal. And I think that that brings new light to how the proof actually works than if one just read the original proof. But yeah, it depends on taste. There are a lot of people that like these more technical things. Things that I mean maybe this is in the aim of the famous lost problem of Hilbert to present the proof in the simplest possible manner and maybe the simplest manner is when the quantitative aspects are made explicit. Yeah, I think there is that connection. Yes, we managed to avoid, in this case, I talked about this elimination of arithmetical comprehension. So we avoid the need of having this projection argument, of working with the sequential weak compactness and having to work with the limbs. These are just the examples that I show here today. But so in the end, you get very, you only need very weak principles. You just replace the projection with some inductive argument, very simple argument. In the case of the LIMPSUP, you are working with just rational approximations to that real number. Actual number, so it's a lot easier in terms of the strength that you need to work with that. Yeah. So in that sense, it has to do something with the elimination of ideal elements. Yeah, exactly. I think it's a nice illustration of Hilbert's problem at work. So it shows on the one hand how ideal elements conceptualize proofs. So one can, you know, compress. So, one can compress a proof in a very short abstract argument. On the other hand, the analysis shows that it's in principle not necessary. So, one can give a proof where basically only triangle inequalities and just basic calculations remaining. And so, there's a trade-off between this conceptualizing power of ideal elements. On the other hand, if one wants more information, one has to remove these elements at least partially and to get extra. And to get extra information. And also, once they are eliminated, often one can immediately see that things generalize, for example, to geodetic settings, where the original argument might not even be applicable. And so I think it's Hilbert's programme at work. But Harvey Friedman, I think, is a question. Yes? Harry, I'm Mold. Yeah, yeah, yeah. Hi. Hi. I saw a brief account of the fact that Terry Tao uses a lot of non-standard ideas and methods to do concrete things. Do you agree with that assessment? And what kind of proof mining connected with Tao's work has arisen? Okay, so I don't know that much about the work on non-standard stuff, but the metastability rates, the name, this was known to logicians, but the metastability name actually comes from Terence Tao, and I think it was him who brought this to light in terms of the general public. But even though this was not Even though this was known many years before. But I don't know much more than that in connection with Terrenstar's work. Well, I can ask the same question of Ulrich. Yeah, I mean, yeah, there have been some minings. In particular, there's some nice work by Henry Tausner, I think, in relation with some. In relation with some work of tau. I mean, some of these things, like the multi-dimensional or what's called semi-ready theory, they of course have horrible bounds. So I think one knows some kind of Ackermann-type bounds, which result by proof mining. But this is more like in your area, it's more like one of these almost Goolean phenomena things. So I don't think that, I mean, I don't think that, I mean, this has been, of course, a spectacular bounce on in some situations by Gawas, who got Kalma elementary bounds for things were previously only primitive recursive bounds and before even Ackermann bounds were known. But I think that would be very interesting whether these Goers ideas can be understood. Can be understood in logical terms. I mean, this is completely different arguments using harmonic analysis instead of the previously used topological dynamics and so on. I think there is some paper by Henry Tausner which sheds light on this from a logical point of view, but I'm not familiar with the details. But he would be the person to know most about this. This at Carnegie Mellon, isn't he? No. He's in Philadelphia University. Philadelphia, that's right. University of Pennsylvania. Thank you. By the way, I mean, you ask at some stage about where to see concrete functionals and functional adaptations. So, in these case distinctions here, which are hidden in this use of this lemma by Maim J, one has a big law of excluded middle, you know, either something. Middle, you know, either something tends to infinity or does not. And one has to smoothen this law of excluded middle. And this is done exactly like the functional interpretation, the tunfield interpretation of the law of excluded middle proceeds. So here it's really, you know, a seemingly completely non-constructive case distinction becomes sort of quantitatively smoothened in a way which makes it possible to actually perform this. And that's in both in this. And that's in both in this one paper by Bruno Dienisch and Petro Pinto, was mentioned, and also in my paper, which was mentioned. One can trace this, the treatment of the Law is grouped in the middle by functional interpretation very neatly in these papers. Although it's not mentioned explicitly, but it's clearly from an expert that that's what's going on. Other questions? Other questions, remarks? Okay, so then we have a short break till six o'clock, where we have the last speaker of ours.