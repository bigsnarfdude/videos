Distribution generalization and deep predictive models. So to get started, imagine you have this problem where you'd like to take a bunch of images and figure out whether there are penguins in it or if there are cows in it. And the way that you'd go about doing this with machine learning is you'd be like, let me get a very, very large data set, say like 10,000 images that are labeled as cow or penguin, and I'm going to train a neural network to do this. Train a neural network to do this. The neural network you might use is a convolutional neural net. And then, how do you know you've done a good job? Well, you'll have a held-out set of data that you did not train on, say a thousand of them, and you'll evaluate your performance. You'll say, like, how accurate am I? Or you might have some other metrics that you'll consider. And if you were to do this on this problem, you might get a number like, hey, I'm 95% accurate. And that accuracy is simply. And that accuracy, assume that the labels are balanced, so the baseline is 50, so 95 you should be a little bit proud of. Then you get an image like this, and you feed it to your predictive model, and then you get the label of cow out. And then you're really like, it really makes you question what did you actually learn? Or like, what did the model actually learn? And in this case, because penguins Because penguins don't appear on grass that much, mostly appear on snow, cows appear on grass much. If the model can classify grass versus snow, it'll be relatively accurate at penguins versus cows. Just really a question like what was actually learned. You might think that this is unrelated to health. So I have like a health example here. These are two-dimensional, like t-SNE, that we heard about. Like T-SNE, we heard about projections of activations in a convolutional neural net. On the left side, we have one that's randomly initialized. You just draw the parameters from a distribution, set it out. On the right side, it's pre-trained. It means that you take a related data set, optimize towards that, so you're starting from somewhere a little bit better. If we look at the bottom, we can see the label. So the label here is whether there's a fracture or not in the hip X-ray. And we can see that the labels. And we can see that the labels occur more down here on this plot. If we map this to the top plot, we see they're mostly in this green region. This green region is a particular type of scanner, the X5000 scanner. And so if you were taking this data set, trying to classify a fib fracture versus not, one way to do it, and this paper actually shows this is likely the way that it's being done, is to Way that it's being done is to classify the scanner type, and then if the scanner is X5000, say that there's fracture, otherwise, say that there's not. Another example of this similar kind of phenomenon was for COVID-19, where people took chest x-rays that were publicly released that were COVID-19 positive. They merged that with existing data, and because the existing data was Data and because the existing data was older and public, like the NIH chest X-ray data set, they were COVID-negative before COVID was there. And then they trained a classifier, and they're like, look, you can get really good performance for detecting COVID from chest x-rays. Then somebody comes along and was like, let me try to do some of this explainability and see what's important. They're like, hey, it's funny, I'm getting these regions that are like above the shoulders. Already leads to a question. And so then they did a really good job. This paper here did a really great job where they're like, okay, let me shift the person to mechanically take the images and shift the position of their shoulders so it moves from lower to higher and then see how the probability of COVID-19 changes. And it turns out, yes, it does. And that's because one of the data sets where the negative examples were, I think, were towards, there were more topics. Towards, they were more top aligned, and the positive examples tended to be a little bit lower. And this is a common issue. Like, you know, like how sick you are in some sense will influence not only what is collected, but how it's collected. So, you know, if you're really sick, you might not go to an x-ray machine. You might have a portable x-ray taken to you because you can't move as well. You might also have wires inside of you because those wires are like. Of you because those wires are like keeping you alive in some way. But they're also, those things are related to maybe what you're trying to detect or predict. And we saw this example in two places where we saw it for images for classifying animals. Let's talk about the chest X in one. But this problem occurs in other places. I'll talk about a language example a little bit later. There's one for electrocardiograms. Electrocardiograms. But before I go further, from a computer's perspective, without more knowledge, this problem is even hard to tell. It's hard to say that it's happened. Because constructing that penguin image on grass, there's some knowledge required, or you have to find the subset of your data where there are penguins on grass, evaluate the performance there. Or you could have data from a different distribution, like for the COVID-19 example, if somebody gave you positive and negative cases from a single place. In negative cases from a single place, you might be like, hey, my performance is not very good. What's common in a lot of these problems is that there are nuisance variables, like variables that can predict the label somewhat well. And if you assume that the relationship between this nuisance variable, like the background and the animal, is fixed, then your performance will be good. Because if I don't get images where I see wings. Where I see like way more cows on snow, my accuracy still will be 95%. I'm just saying that again, in this example, this is kind of the label, but if you can classify from the background, it is a nuisance, but still predictive. And so the setup of the problem is the following. Here you get to see data from a distribution. Say the blue dot is where you get to see data. Say the blue dot is where you get to see data from. It has a particular relationship between the label Y and the nuisance Z. That's what this index is, and that's why there's a D here. And this is a big family of distributions. The only thing that can change between the different points is the nuisance-label relationship. Everything else in this full joint is fixed. And your goal here is to build a predictor that has low error regardless of this relationship. I mean, that if I got some, if I got a data. mean that if I got some if I got a data set where there are a lot more penguins on grass and cows on snow, you will be able to still perform well. And so one idea, and it's a good idea, is like, hey, if I tell you the nuisance-label relationship is a problem, let's break it. Let's make a data set where the label and the nuisance are independent, denoted by this P little independent sign. And we can say, let's use this as a predictor. Can say, let's use this as a predictor. p independent of y given x. When I say model, it's going to be a conditional distribution. When I say predictor, I'm going to be referring to conditional distributions. This idea doesn't work by itself from like a textbook example of explaining away. What explaining away is, is that if I have two independent variables and I condition on something that depends on both of them, the two independent variables The two independent variables can become dependent. Simply put, like if x is equal to y plus z, then if you know y, you know exactly what z is. They're dependent. And this is a problem here because if the appearance of z in your predictor is uncontrolled by what x is, you can get predictors that are worse than chance. Where chance just means like forget about x, predict from the marginal. But there is a good predictor here. So if we take x1 plus x2, the z's cancel out. You get something that's just 2 times y with some noise. You scale that appropriately, you get a good predictor. And so the idea too is to use representations. So representations is a machine learning speak for a function of your input. This function of the input is going to be learned. This function of the input is going to be learned. You can think about it as like the last layer in a neural network would be a type of representation. And we want to find representations that don't induce the dependence that sort of led to the bad performance. We call those uncorrelating representations. Specifically, when you condition on this representation, the label and the nuisance are independent. And under this, we'll be able to say a few things, but first I just wanted to say that. We'll be able to say a few things, but first I just want to define what the performance of a model is. So, a model Q is a conditional distribution. The performance of this model is the negative expected KL divergence. So we want KL divergences to be low. So it's negative, so high performance the better. It's also the expected log probability of the model. And you can say some things, so we went a little bit further. So now if you take R. So now, if you take R and this uncorrelating set, write down this predictor in this randomized distribution, we can say that the performance of the predictor is better than the performance of random guessing because it's equal to the performance plus some non-negative term. So it's like I made a little bit of progress. But like better than chance is maybe like a baby progress. We want to say what is the best representation here. To get an idea of that, To get an idea of that, we want to know when in representation R2 is better than R1. If we assume this property, which I'm going to call blocking, where given R2 and the nuisance, R1 has no extra information about the label, then we can write down the conditional distribution and we see something surprising. We see that the predictor is proportional to the right-hand side, but on the right hand side, To the right-hand side, but on the right-hand side where R1 appears, there's no y. So, meaning when you re-normalize this, this constant will come out. But in words, knowing R1 doesn't change the prediction. So you can push this further. You can define the maximally blocking representation. You can first show that it is optimal for all distributions in the Newscent-varying family. What works as representations in this uncorrelating set. That's helpful, but it doesn't tell you how to find it. You can also show that it's information maximal. It gives you a hint on how to find it. But if we find something else with the same information, is its performance as good? The answer is yes. So any representation that has the same information as the maximally blocking one will have the same performance. The point of all this is. The point of all this is going to be to get to something computational: that the best uncorrelating representations have the highest information with the label. This will push us to an algorithm. This algorithm called NERD has two steps. The first one is nuisance randomization, where your goal is to produce an estimate of this distribution where the label and nuisance are independent. One way to do it is with re-weighting. The one way to do it is with re-weighting. If you're familiar with propensity scores, it's a very similar idea. You can also put your serious machine learning hat on and try to train a generative model that produces images conditioned on the label of the nuisance and then simulate a data set. It's actually quite hard, our generative models aren't good enough to be able to train from generations and then pass in a real image and hope that it works well. Casting a real image and hope that it works well. The second step is to learn the representation, which is find a representation with high information subject to regularization so that it lives at the uncorrelating set. To estimate this information, we use likelihood ratios. We've also used this in variational inference, where you train a classifier to estimate the likelihood ratio. Here, we've penalized the joint information, which removes some. Which removes some local optima. I have a picture for that in a little bit. Going to some results. This is the same problem I set up before, except it's a classification problem where y is binary. If you just train a model directly, it's really great on the training set on held out data. But if you change the value of A, which changes the nuisance-label relationship, its performance is quite bad. This is worse than chance, because the label is 50-50 here. Because the label is 50-50 here. Though if you do generative nerd or re-weighting nerd, they are better. The performance is sort of closer to stable than closer to what the best linear predictor would be for the scale. But you can see here using the nuisance can actually give you better performance in the data set that the nuisance label relationship is fixed. A slightly more realistic example where you're trying to classify. Realistic example where you're trying to classify water birds or detect water birds versus landbirds. Waterbirds appear on water more, landbirds appear on land more. Again, if you just train a model just on the natural data set, the performance will be pretty good. If you move to a data set where you observe a higher rate of land birds own water and vice versa, your performance drops quite a bit. If you do re-weighting performance, Two re-weighting nerd, the performance is relatively stable. Still slightly worse than if you could use the nuisance, but way better than 66%. You might tell me, okay, what was the nuisance in this problem? The nuisance in this problem that we use is just the outer border of the image. If you try to train a model, like to predict just from the inner patch, basically is the same as using the full image. So like this small bit of information goes a lot more than this problem. That was a lot more in this problem. You can do the same thing in chest x-rays. The results look relatively similar. Generative nerd is not very good, again, because it's hard to have generations. It's hard to have good generations that you can classify from. Rewaiting nerd is more stable. Performance still is degrading. And the performance overall is not very good. It's better than 37%. The setup here is. 37%. The setup here is you have like more examples of a label in one hospital than another, and then you try to query the reverse. It's not good enough. We'll get back to this point in a second. There's a lot of related work that's been happening in this area, partly because the problem setup is really simple. Detecting cows versus penguins is something that we'd say it's a vision task, machine learning should be good at. Vision tasks, machine learning should be good at it if you have a large data set. But people have been like, hey, it's not going the right way. But if you have more questions about this, I can answer later. But what I present as the only thing that supports high-dimensional nuisances and not requiring a nuisance at test time. So let me get into this local optima bit for a second. So, what did I mean by local optima? Here's the nerve. The nerd objective, where instead of penalizing joint information, we're going to penalize the conditional information, which is what the math said we should do. And this is a two-dimensional plot of the objective, where the representations are just linear combinations of x1 and x2. There's a solution where you combine x1 and x2 and just get a function of z. Red line here where it's x1 minus x2. When you have that function of z and you condition on it, there's going to be no dependence between y and z. So that term will be very small. But you're not going to be able to predict the label very well because z in the setup is independent. The nuisance is independent of the label. There's this other solution which has higher global objective but and is actually And is actually the one that's just the label plus plus. So, anytime I switch something like that, you should ask me, did we lose anything? So, nerd conditional, like the one that the math says that you should do, gives you a type of model invariance, which means that if you start from samples from two different points in that two different distributions in that family, the model you'll end up with at the end, if you could estimate it well, would be the same. Would be the same. If you impose joint independence, which is a stronger constraint, you'll get something a little bit further. You'll get that the risk will be the same across members of this family. And that's basically because the joint distribution of the label and the representation is fixed. Still haven't got to did we lose anything. There's a lot of notational setup here, but effectively what this slide is saying is that. Slide is saying is that you can set up the representations that are jointly independent, you can set up those that are conditionally independent, and there's a family for which the performance for conditionally independent is going to be strictly better for at least one distribution. It's technical, but the idea is that the uncorrelating set is larger, because conditional independence is implied by joint independence. The optimal representation in these families will be a sufficient statistic. Will be a sufficient statistic, and the minimal sufficient statistic will satisfy the conditional independence, but not joint independence. So the nuisance is an extra variable. I'll miss you well. Like three slides. The nuisance is an extra variable. Somebody has to give it to you. You can ask yourself, what if we use some knowledge of structure in the problem that we have instead? So this is a very typical machine. Typical machine learning data set of hand-drawn digits, but they're colored. One they're green, the zeros are red. And there are two ways to classify it. One way to classify it would be to just read off the color. Another way to classify it would be to actually use the shape. And so an idea is like when you have data that's like this, you can use the label destroyed image where you're just permuting patches around, where the permutation size of the patches is getting smaller as you move to the right, and use that as a nuisance variable. That's a nuisance variable. If we do this on water birds versus land birds, relatively, you're not even querying much from it, you get a reasonable improvement over just training it directly. You can also do the same thing in natural language inference, where the idea is to see if a hypothesis is entailed by a premise, contradicted by the premise, or just neutral. By the premise, or just neutral with respect to it. And these data sets, they're very simple heuristics that work well. Just count the number of matching words. So lexical overlap will be relatively accurate. Here you can see something very similar where instead of doing it on top of a model that's trained, we built it into a procedure that trains a model and then upweights the hard ones. And we permuted, instead of permuting like pixels, we're permuting words here. Here. And the thing that I mentioned, I should mention this. So here we're reporting worse accuracy. There's some discussion about fairness earlier, where you can ask yourself, like, if the relationship is not changing, do you care about this? And in some sense, you can say, which examples are you getting wrong? So in the cow-penguin example, you're getting wrong. Cows that are on snow, penguins that are on grass. And so if you have this problem where you can shift. If you have this problem where if you can shift this distance-variable relationship and the performance will change, there's going to be implications for performances on subgroups. If those subgroups are what you care about with respect to fairness measures, you have to pay closer attention. And I think this is my last slide. So there's still some questions here. One, you kind of should pick on me a little bit, because in all these examples that I'm showing, the labels are almost deterministically in the features. Features. Like if you're classifying a cow versus a penguin, boy, that's in there. And so the maximally informative thing in the distribution that has any usage-label relationship should be the same. The story is a little bit more complicated than what I described. There's like learning dynamics for neural networks that play a role here. Like if you classify many examples correctly using the nuisance variable, your effective data size looks a lot smaller, so it's easier to overfit and do something different on them. To overfit and do something different on them. There's questions about generalizing out of support. Like, if you get a new scanner, that might not have an X that looks like anything you've seen before. How do you generalize that way? What assumptions do you need? Where does the information come from? And then a practical one is like, how do you close the gap in a known achievable performance? Some people might say collect a very, very large data set. And then that will help if your data set is diverse enough. But people build very, very, very large languages. But people build very, very, very large language models and still see some of these biases inside of it. So data probably is not going to be the only answer to this. And this is a reference for the paper that I'm talking about. Excellent. Question And I had one. I had one. Oh, this is just, I might not exactly understand what you were doing in one of your evaluations, but you were showing that the model was relying on the use of variable. Yes. It was predicting something. And then when you change