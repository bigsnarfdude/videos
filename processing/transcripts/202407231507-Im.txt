Since you know Neil right here proposed to you and since he dreamed it up and made it happen up until 2024, April 2024, we saw that US canalcaps has hundreds of thousands of loci associated with multiple traits. Traits and actually, thousands of GWAS have been performed according to the GWAS catalog so far. So, but as the catalog says, many loci have been discovered, few have been explained. So, that's kind of the goal of many of us here. And one of the reasons we proposed Predict Scan now almost 10 years ago was to try to explain those roses. So, at least find the gene, what's the culprit? So, you all know what a GWAS is. We've heard a lot about TWAS today, but let me just go briefly over this so that we can have a common set of notations. And I'm hoping that when you write your papers, you will follow these notations so it's easier for me to read your papers and more likely that I will accept the review of your papers. That is hard for a statistician. Okay, so what is a G was a simple linear regression of a phenotype vector with n individuals, which is the total size on your genotype matrix. This is the matrix. Rows are people, columns are SNPs. I mean, it could be other variations. I call them SNPs. And then you just run one regression at a time, ignoring everything. Regression at a time, ignoring everything else, and that gives you this table of SNP-level results. And that's few words, that's good. So, regarding notation, so there are many effects on many things, right? So, I would like to use delta for the SNP effect on the phenotype. So, you know, delta disease, complex disease. So, I can remember. Because even within the lab, I cannot figure out what people were talking about. So, delta for the SNP effect on the disease. The SNP effect on the disease, so that's going to be perfect. Okay, so then because all these SNPs, we don't know what they mean, would like to know at least what's the causal gene. So it would be nice, you know, a lot of people have tried to use actual observed transcriptome data to try to, instead of associating with a SNP, with the observed expression language. So you could run this observed transcriptome-wide association study, and this study is happening. And these studies have been done in the past. And, you know, many issues with this. I mean, there are many advantages, but some of the issues is that RNA sequencing is a lot more expensive compared to genotyping data. And please ask questions anytime. Also, many tissues such as brain and pantheons are not accessible. And also importantly, differentially expressed genes can be a consequence rather than the cause of the disease. Cause of the disease, right? So, this reverse causality is less of a problem when you do this other thing that we propose, which is essentially the TWAS. I don't think I need to explain what a TWAS is, but essentially you take the genotype data, you predict expression levels of genes using these linear combinations of different sets. We have this database PredictDB where we save all these weights that are needed to. Weights that are needed to calculate the expression level. So you end up with this from the genotype, you generate this matrix of predicted expression level, and then you run one y of your phenotype, you regress against one column at a time, and that gives you your DOAs or the code it initially predicts them, and that gives you gene-level results. And here I like to call it beta, so the effect of a gene on the phenotype. A gene on the phenotype, we call it data. Okay, so you know, in order to do this, we use what we call reference transcriptome data, where you have genotype data, gene expression data, an example of which being GTEx. I know I don't need to spend a lot of time on this. I mean, for people who haven't looked at this, this gives you a sense of how well we can predict gene expression levels. Those are four genes that are cherry-picked for being the most. For being the most well predicted. So, x-axis is predicted expression, y-axis is observed expression. You can see pretty high correlation. And then, if you look at all the genes, these are like 13,000 genes expressed in whole blood, and you can see, and these have been ordered by their estimated heritability, and red dots are prediction performance. So, genes that are more heritable, the red dots are higher, so we predict that. So, that makes So that makes all sense. Many advantages, disadvantages. I'll skip this and go to the, okay, the idea behind TWAS can be extended to many other molecular traits. Many people have done that in this same room. And so, whatever we are going to talk about, TWAS, it extends immediately to all these other type of methods. Methods. Okay, so this is all good, and we heard a lot of talks about extensions, how to improve some of the limitations, etc. But the idea we had was that the test we were running was valid. And then there was this paper that came out by Elu Pedal, who said that TWAS used inflated type 1 error. So, and it was published last year. And we know kind of there are known sources of inflation, right? Like in GWAS, you don't find just the causal SNP, you find all the SNPs that are around because of LD and so on. So for TWAS, in addition to LD, there's this coregulation thing where if a SNP is altering expression of two different genes, but only one is causal, you're going to the T was going to identify both, it's not going to identify one. Not going to identify one. And that's inflated typewriter error because you're going to say that this gene is significantly associated with the disease, whereas it's not. So those kinds of inflation we know they exist, but we are willing to accept it and we find the method useful despite these limitations. So what they are referring to, rule cry, is not that kind of inflation. So the model that, you know, TY The model that, you know, T was modeled would be Y is your phenotype, beta T, that's an error, T is your predicted expression or your genetic component of gene expression, and it's just a linear combination of mistakes. So X and K s are the states, right? So what you et al are saying is that because we use the noisy version of the predictive expression instead of the true predictive expression, that's the cost. Equation, that's the cause of the equation. But you know, if you know about error in variable literature, and if you assume that the error is not associated with the trait, which seems quite reasonable to me, then even if you use the noisy version, that's not going to inflate your type 1. We know this. And just reduces your power, yeah. Exactly. Reduces your power, yes. Because if it's all noise, you're not going to detect anything, but it's not going to create new associations. And you know, we have reviewers saying, oh, what if this error is associated with the phenotype? That's a pretty kind of pathological case, it seems to me. Anyways, okay. So they continue. They continue on to, they proceed to demonstrate mathematically that that's the case. And then they say the null hypothesis in T was is that the beta, this beta is zero. But then they say because there's an error in the prediction, their null hypothesis is this other one, where it's not very clear because it's not very rigorously written. But essentially, what they are saying is that the null hypothesis that TWAS is testing is that this testing. Is testing is that this estimator is zero. So an estimator is a random variable, continuous random variable. So the hypothesis of a continuous random variable equal to zero. So the event has probability zero, right? That's not how you do hypothesis testing. So anyways, but you know, they have some simulations we were trying to understand, you know, kind of following their derivation is very hard because the definition, even you. Hard because the definition, even when you make some clear order by using the true variants or the Sanskrit variants, they didn't follow the annotation. So, anyway, so we worked with Yan Yu, a former student in my lab, and Cestus, future student in my lab, to try to see what was behind this, right? Was behind this, right? Because we saw some of their simulations, there seems to be some inflation. So, you know, I learned by doing, so we simulate. So we have the same model I said, y is beta t plus epsilon. t is a linear population of snakes, and this epsilon is independent of t. That's the usual linear regression assumption. And then, okay, so to simulate, you say, we say beta equals zero, right? We say beta equals zero, right? Because this is the null hypothesis. This is also important. One of the reviewers got confused about that. So they don't know that when you calculate type 1 error, you have to have, you're calculating under the null hypothesis. So that's beta for example. So m is the sample size, we just put 1,000. M is the number of SNPs. So we are going to consider for this experiment independent SNPs. I like to use different numbers. I like to use different numbers, so I don't get those things. Methods you develop to avoid bugs or detect them early on. So manual radio frequency 0.4, so this epsilon T was just normally distributed, gamma. So we kind of generate it as normally distributed random variables. X is the matrix with a binomial distribution. With the binomial distribution, they are independent. We have this gives us a matrix of n, 1000 by 999 steps. They are all independent of each other with four minor frequency expressions. It's just x times gamma. So this is a matrix multiplication. It just works out. Y is beta times t, but beta is equal to 0, so it's actually the path of gamma. So if we fit this, what do you think will happen? We get type of an error. Pype one error controlled or not? Let's guess. My title, I need to change my title. Actually, that's very good. Okay, so under the usual TWAS assumptions, we don't see any inflation, and the way we look at it is we take, so we run this regression that gives you estimated. Regression that gives you estimated effect size, the beta hat, we divide by standard error, that gives you the z-score. You square it, that gives you the chi-square. So here, these points are the chi-squares of having depleted this thing a thousand times. So observed chi-square on the y-axis, expected chi-square under, so this thing under, you know, it should be chi-square, standard chi-square with one degree of, you know, so that's expected here. Expected here, this is observed roughly as expected. And this is the, if you average the 1000 chi-squareds, that gives you this value here, blue, which is within the distribution that we expect for the sample mean of 1000 chi-squared with one degree. That's very good. So, this, at least, we don't have a value. So, now we have a polygenic component, right? So, we are saying, Right, so we are saying we add the polygenic component because you know we know that most complex traits are highly polygenic. Seems like every SNP has some effect. So the epsilon where we were just assuming to be independent, we continued to assume epsilon t was independent of t, but we sort of recognized the fact that y is a linear combination of SNPs, with where each SNP has a very small effect. Each SNIP has a very small effect, but it's not different. So essentially, I'm doing exactly the same thing. The only difference is that y has this delta, this is effect, by x. So it has, but delta is independent, right? These are all independent sampling. So delta is independent of t. So this epsilon, this Of this, it is still true that T is independent of X1. T was the usual regression assumption applies, right? Yes? So your explanatory variable T is independent of the noise term of your power. Okay, so what do you think will happen? You don't have to be right, you just have to answer. Final suggestion. We didn't expect this actually. So yes, you observe chi-square is, you know, it's away from the one-to-one line, so it's much larger than expected, and the mean, the average chi-square is way above the expected values. Okay, so next what we did. So next, what we did is those were kind of simulated genotype and so on. So next we applied the same thing with actual UK biography genotype data. And then with the UK biography genotype data, we calculated predicted expression levels using the existing waste that are available in public databases. And then we associated with the null phenotype Y. Since there is no null phenotype. Since there is no null phenotype in UK biovample anywhere, we just simulate. So that's, yeah. So the null phenotype will be something like this plus epsilon and beta students. So the difference here is X is actually biographical genotette data and this was fifteen or twenty thousand and this is we use what is it? What is it? I'll have my SNPs. So we did that, and again, we see the inflation. So it's not, even this would be a real T was association with the null phenotype and we are seeing this inflation. So what are the properties of this inflation? So we did this simulation over a thousand times, many times for different combinations of the heritability of the Of the heritability of the polygenic components of Y for different values of the sample size, and then the product of sample size and heritability. And you see this kind of nice linear behavior, and it's at one, right? So if heritability is zero, so that's polygenicity is zero, then it's one. So when the average chi-square or expected chi-square is one, that means that we don't have prefection. That means that we don't have inflation. If it's greater than one, we have inflation, and that's fine. Can I ask you a little bit about this? So heritability and polygenicity are slightly different. Holy gymicity? Yeah. But so the way we're defining polygenicity is infinitesimal model. So every SNP has an effect. And then this, the variance. And then this, the variance that this component explains of the phenotype. That's what we are calling qualitative. So under this model, yeah, this is the variance of delta divided by the variance of y. So it's not the same. But this is, I should call it, the polygenic heritable. Okay, so then we hypothesized that expected chi-square will have this form where this phi thing here is the slope of this plot. And that thing will vary from gene to gene, from mediator to mediator. So we did the math with some reasonable assumptions, and we get and we get this formula where you can see psi over m is the phi and then psi has this form that is a function gamma is remember gamma the snip effect on the gene on gene expression or the mediator and sigma is the L D matrix so so so now we know So now we know there is inflation. Can we correct it? So can we estimate the inflation factor first? And the inflation factor affects... So one way is to when we are calculating is we simulate many combinations of sample size and heritability and we estimate the slope. So once we estimate the slope, we can calculate the mean of the chi-squared. mean of the chi-square distribution. We did that for genes, the tablets and brain features, and you can see these are the values of that slope. Brain features are predicted with a rich regression, so it's fully polygenic, so they tend to be more consistent. Whereas gene expression Slow, whereas gene expression is very sparse. So, sometimes some have few, others have seven tens or hundreds, a few have just one, and so on. But metabolites are somewhere in between, so more concentrated lesser than gene expression. Can we correct inflation? So, how would we do that? So I finished. I finished. So instead of using the standard chi-square, we use the non-central chi-square to calculate the p-values. Instead of pretending the p-value is this one here, we say it's the red one, right? So if we can calculate phi, we can calculate the non-centrality parameter. You apply this equation in R if you are using, and that's that. So, okay, so this will affect T was, P was, ISO T was, X, anything you. US, X, anything you, any of those univariate models. I've also seen many people apply PRS as a covariate, right? Those will also be affected. So those need to be included. So what about error prediction? Doesn't really matter. Okay, so take on messages. If you run standard T was or any X was, you'll get inflated P values. These P values P-values, these p-values inflation will grow with your sample size and the habitability of your trade. You can estimate this phi function, and once you have that, you can fix the problem. That's it. Thank you very much. Okay, so we have time for one good question. This is very interesting and makes a lot of sense. Uh so it seems that this is uh quite analogous to the horizontal geometropia. To the horizontal pleotropy MR analysis. So, would it make sense to simply exclude those pleotropic variants or genes from T1 so that the rest of the analysis won't be available? Yeah, so we are assuming if you could do that, that would be great. But you know, with LD and all that, it's not possible. Yeah, probably you cannot do that in general. Probably you cannot do that in general, that's my guess. Yeah, so we are assuming that the trait is fully polygenic, right? So in that case, there's no way you can exclude it. And if you look at the formula, this may remind you of the LDS four regression forms. It's actually the same thing. It's the kind of small effects of each SNP that's actually continuous. I see your cover a little shelf talk in the way. Yes. Yeah, it's true. So there are methods. So if you include all the SNPs in the region, you will fix this. The thing is, doing that in some statistics is not trivial. It's kind of, you know, the CT was method that does something similar. But again, so L D mismatch is the main issue there. This much is the main issue there. So I think there is value in continuing to do univariate TWAS, but also this other vintage and CT was kind of interesting.