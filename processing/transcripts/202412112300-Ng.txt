And thank you to the organizers for giving me the time to speak at this workshop. Avishek earlier said that his talk was a little bit unusual compared to the rest of the talk so far. I think mine is going to be even more unusual in that I'm going to be giving more of a review kind of talk to go over really four pieces of work that I've done over the past few years. Of work that I've done over the past few years. And this has something to do with fault tolerance, as you will see later on. But really, the topic is on the resource cost of quantum technologies, on how you can use fault tolerance concepts to really think about how costly is the particular quantum device that you're building and how to really optimise and reduce the actual practical cost of implementing some of these algorithms, some of these computations that you want to do. Some of these computations that you want to do, yeah. So, um, I've been doing this actually for a few years and really with uh together with my collaborators. Let me see if I can have a laser pointer. Sorry. Okay, yeah, yeah. So, together with my collaborators, Alexia Orfe and Rob Whitney, they are both from France. Alexia is now in Singapore, heading up the Matu Lab, which is a joint Singapore and French conversation. And French quantum lab. So let me get started. Yes, you will see I'm going to be very high-level in many ways, okay, really, just to give you a sense of what this subject is really about. And towards the end, I will tell you about really what is the applications of some of these kinds of resource cost considerations. What is it that you can learn from that? And I will only really skim over the results. And I'm hoping that people. And I'm hoping that people will come up to me. Although it would work better if I was actually there in person, but I'm hoping that you can write to me if you're interested in learning any more of these particular topics. I should say also to the chair that earlier in Avishak's talk, I actually couldn't hear the questions from the audience. So if there's a question from the audience, please use the microphone or something so that I'm able to stop and answer your question, okay? And answer your question, okay? Okay, let me get started with this. Um, I'm gonna start very generally. Probably, this slide for this audience is not so important, but of course, the point is to try to build reliable quantum computers so that you can do all these wonderful things that we have been trying to invent as users for the quantum computers. And I actually personally like to think of having quantum computers as just simply. Of having quantum computers as just simply a different kind of toolbox that adds to our classical computational abilities. And then you may be able to deal better with quantum features that is very natural in some of the systems we are studying today. So the goal, of course, is to build accurate and useful and large scale, so that is actually useful for problems that you cannot solve on a classical computer. So you want accurate, useful and large scale quantum computers. Large-scale quantum computers. And the obstacle, of course, is that you have noise and imperfections in the physical implementation. And of course, you know very well what are the typical things that you want to do in order to be able to reduce that noise and get this accurate and useful quantum computer. I go actually from left to right to think about the different methods that people are using. And as you go from left to right, you have, of course, increasing power. You have, of course, increasing power in removing the errors, but it doesn't come at no cost. I'm talking about resource cost, right? So, as you go from left to right, you really have increasing complexity. Here, I don't mean the complexity in the computational complexity sense of the word, but really implementation, complexity, how complicated is the physical system that you have to build. So, when you go from left to right, you get better ability to remove errors, but also increasing cost, increasing complexity. Increasing cost, increasing complexity in the device that you have to build. So, if I go into a little bit more detail, we start always, of course, when you want to build a quantum device that is accurate, that is scalable, you always want to think about first the hardware design, where you really want to choose a physical qubit and have some sort of a shielding from noise sources so that you already start at the physical level with a well enough, with a quiet enough qubit. Enough qubit with good enough gate manipulation. Okay. Then you go to what I will consider as more low-level error suppression, largely the things that we're doing right now when we are still in the early phase of doing error creation, all these sort of NIST kind of discussion. You do low-level error suppression, and this can come from what I've already said, this NIST type things where you do error mitigation, you do post-processing, the classical post-processing. Post-processing, the classical post-processing, to try to remove some of the effects of errors. But you can also think about more traditional methods, which has to do with perhaps looking for some decoherence-free subspaces, where these are noise-free subspaces, special spaces within your Hilbert space, that allows you to operate with lower noise than otherwise possible. And then you can also think about applying coherent pulses, dynamical decoupling pulses. Pulses, dynamical decoupling pulses to your system in order to try to average away some of this noise. These things are low level because they come at a pretty low cost. Even dynamical decoupling, you're usually talking about single qubit, few qubit kind of pulse of gates that you have to apply. Then of course we come to the thing that many people here will know very well, which is of course error correction, where in this case you're talking about general purpose. General purpose, ability to remove any kinds of errors. And here you want to be able to introduce redundancy and encode your information so that you can detect and correct errors. And then, of course, the error correction operations themselves can be noisy. And this is why we talk very often about things being fault tolerant. And then you can scale up, of course, your error correcting codes to correct more errors and hence to have more accurate quantum computation. Quantum computation. Okay. So, this is of course a setup that many people are already familiar with. Sorry, I have a question. So, can you go to the previous slide? Yeah, so in what sense are the techniques in the middle more low level than the techniques on the right? Low level in the sense of lower cost. Let me put it this way. Because the ones actually in the middle, you don't, well, very often, like if I think of decorants free subspace, Like if I think of decoration subspaces or dynamical decoupling, you are talking about already having already the physical system in place. You're not going to have to add additional qubits, for example. You don't need to add redundancy. The diameter decoupling is really a per-physical qubit kind of operation. The error mitigation, very often what you're doing is that you are repeating. You're not actually adding in terms of the number of qubits. Sometimes you will modify some of the gate or You will modify some of the gate operations, but they're generally low-level in terms of resource cost, right? Compared to error correction, where you're going to add a lot of redundancy, you have to do encoded operations and so on. Okay. Okay. Now I come to, in fact, I jump to really thinking about fault-tolerant quantum computing because I like to think of this in the context of the quantum computer. In the context of resource costs, as the fault-tolerant quantum computing really offering you some link from the user who is actually using the quantum computer to the actual hardware details that the person who is using the device actually has to deal with. So, where does this come from? I start really with a very typical relationship that you find in talking about fault-tolerant quantum computing schemes. Where, okay, let me. Where, um, okay, let me this is sort of the formula. I'm going to explain each of the pieces inside this formula. So, you start all the way on the left side with the computational error rate. That's what this P is. So, it's the probability of a computational error. Okay. And this computational error rate is usually some sort of a target accuracy that is set by the user, okay, depending on what is the algorithm that you have in mind, what is the problem size that you have. What is the problem size that you have in mind? You have some particular target computational error rate. And this target computational error rate, you should think of it as the error rate per computational operation, per computational gate that you want to do. So this is usually something that is determined by the user in the sense that I have some accuracy that I need to reach so that my overall algorithm has a certain accuracy. And then I have this case. And then I have this k. The k you should think of as the physical scale of the computer. In this sense of trying to think of fault-tolerant quantum computing with error correction, you should think of this K as determining really how much error correction I'm going to do. The amount of really redundancy that I'm going to introduce to be able to correct the errors. So you can think of this as like the number of physical qubits per computation. Of physical qubits per computational or logical qubit. And this cape itself is determined, you should think of it as both by the user and the fault tolerance scheme. The user in the sense that I need to be able to scale my error correction protection large enough such that I get to my target computational accuracy. Okay, but how quickly that scales as I increase K, how quickly the accuracy is. As I increase k, how quickly the accuracy scales is something that's determined by the FTQC scheme itself. This is why I sort of put both things here: both the user and then the fault tolerance scheme itself. So you should think of this K as something as related to the physical resource cost that I need to invest in order to be able to get the target accuracy that I want. Okay, so that's sort of the left-hand side of this equation. And then the right-hand side, you have eta. Okay, eta is the Eta is a physical error rate per component, per physical component in your quantum computing device. And this really characterizes the noise that I have. And this is determined really by the hardware properties of your device. And then I have B. B is you should think of as really for every increase in the scale of my error correction, increase in my K value, I have to increase my scale. I have to increase my circuit size by a certain amount in order to be able to enable all my error correction, all my fault tolerance computation. Okay, so that's what B really characterizes, increase in the circuit size for every increase in the scale K. Again, this is something determined by the fault tolerance scheme, and you should associate this with a physical resource cost. Then finally, we have this F of K, which is really just the exponent that determines. Just the exponent that determines how quickly the accuracy of your computer grows as you increase this error correction protection. And this is completely determined by your fault tolerance scheme itself. So these are the sort of all the elements and C is just an overall constant that can often appear and are very often also determined by the fault tolerance scheme itself. So this is sort of a typical formula that you can get. Of a typical formula that you can get, some of the fault tolerance schemes will be able to derive this: what are the B values, what is C, what is F, really from just the features of the fault tolerance scheme itself. Or this can be something that is observed numerically by simulating the particular fault tolerance scheme. And something that's very familiar, I'm sure, to everyone is if you look at this formula, you can, in fact, read off. You can, in fact, read off exactly what is this fault tolerance noise threshold that becomes the target for many experimental groups that try to build quantum computing devices. And if you look at it, what you really want is you want to have the property that as I increase k, okay, so this f of k is a monotonically increasing function of k. So I want to increase k in such a manner that this p of k decreases. Okay, and given Okay, and given the form of this, you can immediately see that that is happening only if this b eta itself is less than one. Yeah, okay, and then you see immediately that this requires then my physical error rate, this eta parameter, the physical error rate per component to be less than this threshold one over b. Okay, so and then remember that the b is really how big your circuit needs to be in order for you to be able to do error correction. Okay. Be able to do error correction. So, immediately from this type of a formula, you are also able to derive what is this fault tolerance accuracy threshold for your particular fault tolerance scheme. So, this is sort of the picture that everyone should be already very familiar with. But what I want to ask is really now what happens if you have this situation, which is actually something that you observe very typically. Something that you observe very typically in the current quantum computing devices. What happens if eta itself, here we treat eta as if it's a constant, but what happens if eta actually depends on the scale of your quantum computer, this physical scale of your quantum computer. Okay, and this is actually uh, so the typical thing that happens is that eta, which I remind you, is the physical error rate per component. Okay, it's not about the Component, okay, it's not about the total noise of the whole device, it's the physical error rate per component in your device. So eta usually increases. If you look at the typical experimental setup, eta increases as k increases. Okay? So usually this happens because you have some sort of a resource constraint. Perhaps you have a total energy constraint such that as you increase the size of your computer. Increase the size of your computer. You have less and less energy to be able to distribute to each of your physical components. And then when you have less and less energy in many of these schemes for implementing gates, you will find that the fidelity of that gate operation will decrease. So whatever is the reason, actually, you'll find that this is a typical situation for current quantum computing devices. Of course, this is this sort of eta, depending on K behavior. Of eta depending on k behavior, what I call scale-dependent noise is a characteristic of any kind of a device that is not really fully scalable at the moment. But right now, at least all the devices that we have today, very often they are subjected to this sort of situation where eta increases as k increases. Now, I want to ask what actually happens when I have the same formula, but now eta is actually. The same formula, but now eta is actually dependent on k, and in particular, eta increases as k increases. Okay, and you can, of course, see already very sort of intuitively, very pictorially, if you look at this, this is all my cartoon of what really happens. So, I now have the same formula as I had before, but the eta now depends on k. Okay. And you can think of really the standard fault tolerance situation as the following. The following. Ignore the pink line for a moment. Okay, we just look at the blue and the red lines. I think of increasing my computer size. So this is K. Okay, this is increasing K. As I increase my computer size, my error correction gives me the benefit of being able to correct more and more errors. Okay, so I have increasing benefit. But to be able to do more and more error correction, I need to invest more and more. Invest more and more physical components. Okay, that's sort of where the cost is. And this cost itself, actually, what is plotting is really the amount of noise in the overall device. So because I'm increasing the size of my physical device, the total amount of noise increases and is a constant gradient line only if eta itself doesn't depend on k. Eta is the k, right? Eta is the noise per component, and then the cost, which is the total noise, increases at a constant rate, and hence it is a straight line only if eta doesn't depend on k. Okay, so in a typical situation, if the cost increase actually slower than the benefit increase, so you're able to correct some errors, this gap here between the benefit and the cost is really a characterization of how accurate your computation is going to be. Okay. And then if your cost is in fact And then, if your cost is in fact growing at a faster rate than your benefit is growing, then you have the situation that you're sort of above the threshold. You cannot use error correction to improve the accuracy. And then the cartoon picture of what happens when eta itself is increasing as k increases is really this pink picture where I have this increasing cost. So the gradient is now no longer constant. Eta, which is the gradient, increases as k increases. And I have this pink line. And then what you see is that I have a limit. And then, what you see is that I have a limit, a maximum accuracy for my computation that I can actually attain under this sort of a situation. Okay, so this is sort of what this picture on the left is trying to depict. There is a maximal attainable computational accuracy when you have this situation of a scale-dependent noise. Okay, and right now, we're very much in the regime where we're very much in the situation where our experiment. Situation where our experimental devices are really like this. You go beyond a certain scale, you start running out of extra practical resources that you're going to be able to use to continue scaling your system without the noise per component increasing. So we are sort of on one of these curves where you have, after a certain point, there's scale dependence, and then you achieve only a maximal accuracy, even if you try to scale. Accuracy, even if you try to scale your system further, the accuracy actually starts dropping. So, the only way to get through to the next line is really to have a decrease in the scale dependence of your particular device. So, you will find that it's no longer just asking about, well, is your physical error rate below a certain threshold? You also need to ask about, yes, I can start below the threshold when my computer is small enough. When my computer is small enough, do you stay below the threshold as you try to increase the physical size of your quantum computer? Yeah, okay, so you'll find, of course, that there's a much more stringent requirement in order for you to be able to continue along this sort of path to more and more accurate computation. So that's sort of the maybe the bad news side of things. But you can, in fact, turn this around a little bit and really use this same equation, still having. Equation, still having this sort of scale-dependent noise, to think about resource optimization. Yeah, okay, that's what I want to spend maybe the last five. Actually, how much time do I have? Okay, maybe I should just continue until the chair stops me. I think you can go on for another 10 minutes. Okay, very good. Yeah. Okay, very good. Yeah. Thanks. Yeah, so let me spend the last 10 minutes actually telling you a little bit more about this resource optimization angle and also some of the conclusions that you can get from a study like this. So I now want to think about still using the same formula, but really as that I have some particular target accuracy that I want to achieve. Okay. And then I have the I have the description of two things, really: my fault-tolerant quantum computing scheme, okay, and then the other thing is a description of what my physical hardware actually looks like. And then the goal here is to think of it as I want to find the physical parameters that are able to achieve my particular target accuracy that I want to get for my algorithm. Algorithm while minimizing the resource cost. Okay, you define whatever is going to be expensive for you in terms of resource cost. And then you have this, whatever is your system parameters you're going to be able to optimize over. And you try to find the parameters such that the cost is the minimal for the given target accuracy. Okay. And I wanted to give you an example because the example that we have explored in this Example that we have explored in this particular work from last year, where we were really looking at a superconducting qubit system. So that's sort of what the hardware here is really trying to depict. You have a superconducting chip all the way at the bottom. This layer is really the classical layer. And you will see that there are in fact a lot of attenuation stages because these are wires that go down from your classical layer into... Down from your classical layer into your quantum layer. And it goes through a lot of attenuators to try to remove the errors. Remember what I said about isolation of your quantum qubits itself from the noise sources? Okay, that's what all these attenuations are for. That's what all the A's are. And then you can talk about also putting the qubit at a certain temperature. You can put the classical, the electronics, the The electronics, the classical computer that you use to control your system, really at some other classical temperature. Okay, so that's sort of a hardware description that you need to have about your physical device itself. And then on the other side, I have, in fact, a particular fault tolerance scheme that we have chosen to use. I actually chose a concatenated 7-qubit steam coat scheme, really just so that we have a very easy way to derive this. Have a very easy way to derive this particular formula. I know exactly what the coefficients and all of these. So, what did we do? So, in this particular example that we explored in the paper, what we did is we treated as the adjustable parameters in your system. First of all, of course, K, which is the size of the code that you're going to use in this particular concatenated code scheme. The K refers to the number of levels of concatenation that you're going to do. Going to do okay, and then we also have the physical hardware parameters, which is the attenuation that I go through for every stage because these are sort of cryogenic levels. For every stage of cryogenic cooling, I have a particular attenuation factor that I need to have to be able to remove the noise. And then there are two other parameters that are very often easily controllable in this sort of situation. One is the classical stage temperature. Okay, this is the temperature of your classical. This is the temperature of your classical electronics that you have, the control systems, and then, of course, the qubit temperature itself. And then the point is to optimize, in our particular case, we're optimizing for the power cost of the entire quantum computer to achieve a particular target accuracy that you have. And you can understand that this is because without having these sort of hardware parameters, Having these sort of hardware parameters, what is going to happen when you try to optimize and think about what is the K necessary to achieve your target accuracy? Essentially, what you're doing is you just need to find a large enough K value, which is a large enough error correcting code, in order for you to be able to get the P that you want. But what is interesting about this situation is that once you put in the hardware description as well, these A, the temperatures, and so on, you have one additional. So on, you have one additional source of control in this optimization, which is really the fact that we remember that we are talking about scale-dependent noise. So if I, for example, increase the qubit temperature, you already know immediately that even if k remains constant, the eta, the physical error rate, is probably going to go up. Yeah. Okay. And it's not necessarily a bad thing that the Necessarily a bad thing that the physical error rate goes up because what actually happens is that I just need to adjust these parameters such that my target P is attained. Even if you are able to attain a particular target P with a larger eta, okay, while saving on the resource cost that is required to cool your quantum qubit down, then you might. Down, okay, then you might win the game in the end. Yeah, okay. So, my point is really just that we are not just simply doing an optimization over k value, but also you require all the hardware parameters to also enter. So, with this sort of a setup, let me just give you a little flash through the next few slides very quickly to give you a sense of what are the things that you can compute from such a complicated setup. So, for example, you can think about what the So, for example, you can think about what is the power requirement if I want to solve the factoring problem. And this is not for a very large thing, it's in fact solvable in like one and a half hours for the particular quantum computer that we're talking about. And you can assume some sort of futuristic gauge and cubic quality, but near-term control electronics properties, and you see that. Properties. And you see that there's a particular power requirement. Of course, these numbers, the things that you should learn out of these slides is that the kind of applications we can for this, but not actually the absolute numbers, because I'm really assuming specific numbers that may or may not be relevant for the device that you're interested in. And then you see that for certain settings, we find that the power requirement to get a certain target accuracy that is necessary for solving this problem is it can require something like a nuclear power plant. Something like a nuclear power plant. Yeah. And then you can find yourself in a position that, well, if I can improve my gate and qubit quality by a factor of 10, actually that goes down. The power requirement goes down by quite a lot. You really just need a regular power station. And then you find another reason, if there wasn't already enough reasons for this, to use surface codes, because you find that using surface code, near-term gate quality, and near-term control electronics, you only need 40k. Electronics, you only need 40 kilowatts of power, which is like having an electric car that's driving on a highway. So, these are sort of the kinds of things that you can ask. If I want to optimize a resource, what is really the kind of power, put some numbers to solving, to the cost of solving these sort of problems. And then the whole point is really to, in specific situations, we can show really that what I said earlier is not just about optimizing. Earlier, it's not just about optimizing over the amount of error correction that you want to do, but really optimizing over the full stack, the hardware parameters as well. And then you can find yourself having a huge improvement in terms of the power cost when you go from 10 gigawatts against for specific situations. You can look up the paper, you can ask me later. You can go from 10 gigawatts to like 2 megawatts with the optimization. So it's actually very important to be able to optimize over the full stack. Optimize over the full stack, the hardware parameters as well, not just the error correction parameters. Then you can show for yourself this sort of interesting regime where you can have a quantum energy advantage, but not computational advantage. If you look at the graphs a little bit, so I'm again trying to solve this factoring problem. In this particular region here, okay, so what is what I have on top is really the What I have on top is really the time in seconds that you need to solve the problem. Okay, and this is for a relatively small factoring problem you can do also on a classical computer. So in this little region here, you will find that the quantum computer is not actually faster than the classical computer because the classical computer is this black line. Never mind what all the lines are, but the quantum computers are these are the colored lines. They actually take longer. Lines. They actually take longer than the classical computer in this region. But you can see that, in fact, in terms of energy efficiency, the quantum computer itself in this region is more efficient than the classical computer, which is the black line. So you can find yourself regimes where there is a quantum advantage, but not necessarily a computational advantage. Maybe last minute or so. You can also do the same kind of analysis. You can also do the same kind of analysis of resource costs and really count what is the cost of doing different elements of your universal quantum computation, including in this particular paper. I will give the references on the last slide. You can talk about whether or not magic states are actually really that costly because there are a lot of papers that talk about trying to optimize the magic state distillation, the magic state portion of the universal. Magic state portion of the universal computation. And actually, from a resource cost calculation, you find that it's actually not that important. So you can have different kinds of for different codes for different fault tolerance approaches. And you're able to compute the resource cost of this. And in fact, you'll find that the magic state component of this is just really not as important. There are other things that you should optimize before you get to this. Okay, I don't have time to go more into this. Okay, I don't have time to go more into this. And maybe let me skip the last part and only simply to tell you that we also have a little bit of discussion. The paper hopefully will be posted soon on really resource accounting when you're talking about quantum metrology problems and not just quantum computing problems. But maybe, let me save this for another time. Okay. So here are my collaborators, and then the three papers I talk about, the first three, and then this last one is about the quantum module. This last one is about the quantum module. Thank you.