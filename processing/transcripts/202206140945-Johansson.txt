Thanks, Seb, for the nice introduction to some of the forecasting work. I am going to touch on a lot of the same themes, though I think mostly say things that are complementary. So where does pandemic forecasting go from here? Now, I wanted to first say, Seb already covered a lot of this, but some of the things that where are we today? And so these are real-time forecasts. These came out last Wednesday, posted on the CDC website. There were forecasts. There were forecasts submitted by teams last Monday. There's another cycle of that going on today, so there'll be new forecasts out tomorrow. These are forecasts for COVID hospitalizations at the national level in the U.S. So that is a real applied important outcome. It's both important in terms of people who are being hospitalized with COVID, who are sick, but also in terms of the health system burdens. And so this is the kind of information that can be really helpful for people making decisions about public health. These are done on the Public health. These are done on the national scale and also on the state level. It would be even more helpful to get them down to sort of hospital level so that hospitals could also use these kinds of things for planning, but already getting some resolution there. You can see that there's a lot of models here. You need a lot of models if you're going to build ensembles. So, I've already talked about some of the advantages of ensembles. I'll talk about them again. We then create this ensemble that's on the right, and this is sort of the standardized version of forecasts that are in the app. Of forecasts that are put out there. I guess another thing to note here is just that there's so much variation in the individual models. So you can see that some are predicting exponential growth. Others are predicting very pretty, not super sharp declines, but declines here. And when you put it together, there's still a lot of uncertainty there, but at least we're capturing that uncertainty. We also, because we've had these set up and running for a while, we have evaluation. Up and running for a while. We have evaluation systems in place. So, both sort of ad hoc ones that people run on their computers, but also ones that are available online that anyone can go and look at. Looking at things like coverage with mean absolute error. I'll have a reference to one of those that's done by CMU a little bit later. So, we've made a ton of progress here, but I think there's still huge gaps in what we're doing. And that's really what I wanted to focus on. So, the things that we can improve and forecast. That we can improve in forecasting: one, we can improve the reliability of forecasts, we can also decrease the uncertainty in the forecast, the outcomes. We can extend the horizons. What I showed you was four weeks out. It would be helpful if we could do these six months out, of course. We could also increase the spatial resolution. Some examples of those. So, here's, I think we've already covered what interval prediction interval coverage is, so I won't really explain that, but Won't really explain that, but the top two plots here. So, these are all showing case forecasts across US states over time. So, the top two plots are the 95% prediction interval coverage. The top one is for one week ahead forecast, and the second one down is for four week ahead forecasts. And then the bottom is the actual reported cases on the national scale. So, what you can see here, the solid black line, I know that these are all really small. Solid black line at the top is 95%. Black line at the top is 95 percent. So, you want your forecast coverage to be really around that line, and so every point here is representing across the states and territories that there are forecasts for. You can see that there's some big issues at some time. So, there's this whole stretch there from sort of the spring of 2021 where coverage is quite high. It's like right around 95% or a little bit above. But you see this big dip in late 2020. In late 2020, which is sort of that winter wave that we had, and then also again in the Delta wave in the summer of 2021, as well as a small dip in February 2021. And so these are really associated, if you look down to what's happening in the epidemic of those times, it's times when there's rapid growth or a rapid decline, but mostly the rapid growth. This is one of those pages that is updated continuously on a weekly basis that you can all go look at, and you can look at other coverage intervals. Coverage intervals. So, you know, even if forecasts aren't particularly precise, we do want to make sure that we have those prediction intervals appropriately specified, and they're clearly way off at important times of the epidemics. Cases are the worst example of this, and so that's why I'm highlighting that. Another is that we want to reduce uncertainty, right? So, these are the hospitalization forecasts that I already showed from last week. This is the ensemble. You know, we're currently around 4,000. You know, we're currently around 4,000 new hospital admissions per day. Four weeks from now, July 1st, four weeks from the last data point that was used for these forecasts, are we going to be at 2,000 or are we going to be at 11,000? Those are pretty big differences when you think about what that actually means. So while this is helpful and I think gets us going in the right direction, you know, it's very different from looking at the curve here. So there's a median estimate there. So, there's a median estimate there in gray from one of the models that shoots up high. So, it's very different from that, but also it's not super precise. And really, we'd like to have more precise information to make decisions based on. Thinking about forecast horizons to get out further. So, there's a number of sort of different looks at this. This is one particular version. These are, so we, from the onset, because of work that we've done, particularly with influenza prior to COVID. Particularly with influenza prior to COVID, we just looked at four-week ahead forecasting for the main work in the ensembles. But some of the teams submitted longer forecasts. So these are two examples at the top, one from the COVID-19 SIM group and the other from IGME. And you can see three examples of long-term forecasts from them. You can see that they're, you know, they're sort of, if you kind of like pull back from it, you can see that they are sort of. Pull back from it, you can see that they are sort of following things that are happening with the data to some degree, but they're actually saying pretty different things. So, in the first one, you're expecting like the blue forecast is expecting it to go down and stay very low, while the orange forecast is expecting it to go flat and then have this really steep rise where what we saw was an increase in transmission and then a decrease over this time period. And the second, you can see that both are predicting that there's some sort of peak in the future, but different peak timings and very different peak timings. But different peak timings and very different peak magnitudes. There's a lot of uncertainty around them, so that's sort of a good thing. And then the third set, you can see that they're both predicting declines and then steady declines where actually what we saw was the delta equipment. And so like, these are not indictments of those models, but it is very hard to predict out of those scales because of variance, because of behaviors, because of policies. The second plot down is a sort of a more summarized version of this. More a summarized version of this. So each line in here represents a different prediction horizon from the subset of models that did them for longer horizons. And it's a the WIS is on the average WIS is on a log scale here. So every time you take a jump up, that's another forecast horizon in the set of 1, 4, 8, 12, 16, 20. You can see that there's like very quick, very substantial increases in uncertainty as you go up to those horizons. We also see satellite. Those horizons. We also see sacrifices in coverage intervals, right? So the prediction interval coverage is at the bottom there. None of these models did particularly well, even at the shorter-term horizons in their prediction interval coverage, some better than others, but all of those also declined at those longer horizons. Lastly, to be most effective, as we just had some of the discussion there, is to have forecasts at a more local level where people are actually taking action. There's some actions that can. Actually, taking action. There's some actions that can happen at a national or state level, resource allocation, that kind of thing. But actually, implementing things and people's behavior really depends on their local environment. So the more you can get to that, the better. This is an analysis that we're still working on. A number of different teams doing case forecasts. Case forecasts we did at the county level, the state level, and the national level. Here we're breaking them down. So each sort of vertical line is a team, each color is a different resolution. Each color is a different resolution. And so the majority of the models here are showing that the smaller the area that you're looking at, the higher the error is. So basically, we're doing better at the national forecast than the state, better at the state than in the big cities, better in the big cities than the small counties. So that's something we'd obviously like to be able to improve upon. As Seb mentioned, there's been. As Seb mentioned, there's been a number of collaborative forecasting efforts over the years. So, I'm going to talk about some other examples from some of those and things that we've learned from some of them and some of the things that I think are different with COVID. I will note that only a few of these are related to like emerging pathogens rather than ones that have extended historical data. And I think that's a big difference in some of the findings that we've seen about prediction accuracy and what makes a good forecasting model. Accuracy and what makes a good forecasting model. So, one is the Ebola challenge that Seb already mentioned, and that was really done synthetically post-hoc. Then there's also the Chigungunya challenge, which there haven't been sort of comprehensive results out of. So, really the COVID forecast hub and the various COVID-19 forecasting group efforts are really, I think, where we're getting the best glimpse of this, but it's one pathogen and potentially different from others. And potentially different from others. This is not from one of those challenges, and I'm just going to talk through this as kind of an example of one of the things we've looked at. Also, forgive me for analyzing mean absolute error rather than the proper score. I would do this differently if I was going to do it today. But I basically looked across Mexico. We had historical dengue data and looked at, I normally work on vector-borne diseases for those who don't know. So some of these examples will be around vector-borne diseases. Dengue is endemic in most of the Dengue's endemic in most of Mexico. It's been around a long time. There are big epidemics some years, and then other years there's more sort of low levels of cases. And it would be really helpful to know when those big epidemics are likely to happen or early on when they're happening so people can prepare for them rather than respond to them. Similar to situations with like hospitals overflowing with people happen all the time when these big dengue epidemics happen. So, what we did here is we were just trying to look at time series models. We are just trying to look at time series models and say, okay, what can we do about forecasts when we're looking at out-of-sample predictive capacity? Let's start with something really simple. We'll say, okay, today we're going to take all the average of all the previous months of data that we have and say that's what we expect in each of the next six months. So this is the mean absolute error when you predict that. That's just a simple mean model. We know that's not going to be a good one. We can then say, okay, we know that dengue is seasonal. So what if we say that? So, what if we say that we're now predicting for July? What if we say that our prediction for July is the average of all previous July's? Right? And we can put some statistics around that too. So it's not just an average for year, it's the mean absolute error. So it doesn't really matter. That's another approach. So then we get better predictions. So we say, okay, we're just acknowledging that there's seasonality and we're using historical data to account for that. Let's say we know that dengue is transmitted by vectors. We know that vectors are sensitive to climate and weather. To climate and weather, what if we use climate data? So, temperature is probably the biggest factor. We integrate temperature in these and we do just slightly worse. So, it's not the temperature doesn't matter. We know that temperature is important in a number of biological ways for dengue transmission. But when we're forecasting, it's not adding any information to just understanding the seasonal variation that's happened. Okay, we know it's an infectious disease, right? So, cases that next month should be related to the case we're seeing this month. Should be related to the case we're seeing this month, and so on and so forth. We can incorporate that, and we also get a gain in forecast accuracy, but only for one month out, right? So, it's a sort of a short-term autocorrelation that helps us there, but actually it brings things down quite a bit. We can then think about SIRIMA models. So, this is just a SIRIMA model, this one. So, we're thinking about both that autocorrelation and how seasonality comes into play, and how seasonality there's. There's also some sort of multi-year dynamics that come into there a little bit. This is just using a very simple out-of-the-box Serimo model. And actually, you do quite a bit better. How good is this model at predicting? It's still not great. So we're still like reducing the error is like half of what the long-term mean is. And two or three months out, you don't know whether you're in the midst of the biggest year you've ever had or the smallest year that you've ever had. Smallest year that you've ever had. I haven't shown those here, but those are the other pieces we need to think about. But I think this is the kind of study that actually we don't do enough of. Seb talked about another one where we can really sort of piece out what the contributions of different pieces are. These could be very different if we're then thinking about the integration of these factors in a mechanistic model, for example. These are the dengue forecasts that we did in the dengue forecasting challenge. These are all out-of-sample forecasts for three different outcomes. When's the peak going to happen? The peak is going to happen, how high is the peak going to be, and what's the total incidence that we're going to observe over a season? There are over four seasons on the x-axis, is the week of the season when the forecasts were made. And you can see that the teams are sort of all over the place here. The accuracy of the forecast increases as more data accumulates during the season. So, if you think about peak week, by the end of the season, you already know when the peak week is. It's really how confident are you that the peak already happened, which has some value on its own, especially when you're. Value on its own, especially when you're sort of in the midst of things still. Peak instance and then total incidence. And one of the things I wanted to highlight here is two of the sort of comparison models. So one is a null model, which is just saying like it could happen anytime in the year. The next is the baseline model. And here we use a Serima model based on the work that I showed on the previous slide. And you can see that it doesn't do terribly compared to most teams at most times. At most times. And in fact, for peak week early in the season, it is the best model for both locations that we looked at. So, even this simple model was performing better than these other models that were incorporating a lot of other data and a lot of other more complex model structures. The other one is the ensemble, of course. And this is just another example of the ensemble performing better than most of the teams at most time points and being one of the most reliable models overall and high scoring models overall. We're not showing the. Scoring models overall, but we're not showing the overall metrics here. But you can see it sort of consistently among those. West Nile virus forecasts. So here's looking something a little bit different. It's how many neuroinvasive cases are we going to see in counties in the United States. These forecasts are for 2020. We sort of had to abandon ship a bit there, but we got the forecast and ahead of time. We've recently been evaluating them. So these are some preliminary evaluations of that. We're doing it again this year in 2022. I wanted to. I wanted to mainly highlight here the so these are teams were able to resubmit updates to their submissions over four different months. They didn't have to do that. So all the teams had submissions in April. Most of the cases occur later in summer, but some happen earlier on. And there may be early climate indicators in the year that teams can use to help their predictions. If you look on the right is sort of a zeroing in or zooming in on the upper part of the plot on the left. Of the plot on the left. So, this is really where you can get a look at how the models are performing relative to each other. So, here that the ensemble is the solid black line. Again, it's performing among the top of all the models that we're looking at. There's this other model at the top, essentially, except for one of the team models that at one time point beat it, which is this dash model that is a negative binomial model. And this is just simply taking every county, fitting a negative binomial to the number of Binomial to the number of cases observed each year previously in the historical record for that county, and then using that as the prediction for what's going to happen. So, in this case, actually, the historical data is essentially the best predictor that we have of what happened in 2020. And we'll see what it looks like this year when the data finally come out. We can present that same historical model that's very easy to generate, but we're also presenting the ensemble because we know that the ensemble is the way. We know that the ensemble is the way that most teams have, in most of these challenges, have actually been able to make more reliable forecasts than simply relying on historical data. So, hopefully, we'll get there. Here's some examples from influenza forecasting. So, these are some of the model comparisons that we've done. And so, these are looking at different targets, seasonal targets versus weekly targets. Seasonal targets similar to the ones that dengue ones that I talked about, weekly targets being one, two, three, four weeks ahead. weeks ahead. Log scores from a bunch of different teams here for different different targets are in different colors within each of those. But I want to focus on the comparisons here. So in the first column, there are comparisons between teams who are new to influenza forecasting this year versus teams who had done it previously. And you can see that those lines are sort of the mean comparisons between them. And teams who had been doing it for longer tended to It for longer tended to have better forecasts. So, why is that? Maybe like they've been refining their models, or maybe they're just used to the forecasting system and producing forecasts on a weekly basis. Next is model type. So, looking at mechanistic versus statistical models, statistical models tended to perform better across a whole wide variety of approaches. This is the same thing that we found with the dengue forecast, and we've also found it with the West Nile forecast. With West Nile, I think it's particularly not surprising given what we saw with the Not surprising given what we saw with the historical data model being particularly good. The next is data sources. So the outcome being forecasted here was ILINET. And so the ones that only used ILI-Net data to make their forecasts, so they're building a model around the data that the data, historical data of what they were trying to predict, tended to do better than those who used additional data sources. So this gets at some of the model complexity and how model complexity. And how model complexity can sort of penalize you, especially when you're thinking about out-of-sample predictions. And then the third is another look at the value of ensembles. So not only do we see that an ensemble of all the models tends to outperform the majority of individual models, but we also see that teams that take ensemble approaches outperform teams that do not take ensemble approaches. So another reason we have sort of faith in using the ensembles. Ensembles. This is a summary table from the COVID-19 death forecasts. So there's these are looking at relative WISS. So we're comparing team WIST to baseline WISS. And so that's the same model that said what? Yeah. So is this compared to the national scale, or is it like the state scale or is it this? I think this is states. I think this is states only. It probably does. Yeah. But yeah, we don't. It probably does, but we haven't looked at that. I mean, the COVID-19 case forecast, we have a little bit of an opportunity to look at it, but haven't really had a chance to dive into that thoroughly yet. So these are the COVID-19 death forecasts for the United States. Get that exact evaluation time. I get the exact evaluation time period here, but it's sort of similar no matter when you look at it. In this case, at this particular time, COVID Hub Ensemble is the best on average over time. They're ranked here by relative whist. There's some other metrics here in case people want to look at them. The COVID hub baseline model and the CEID walk, random walk model are ones that don't use other data sources. All the ones above. All the ones above use at least COVID case data, right? So here we're looking at deaths, which are a lagging indicator relative to cases. And every model that we know is outperforming a simple baseline, which is predicting that flat projectory. It's the same model that Seb talked about, a flat projectory with expanding uncertainty in the future. All the models that are beating that are using like an earlier indicator. Now, if we think about case forecast, we don't have that earlier indicator, right? And case forecasts. Earlier indicator, right? And case forecasts have performed a lot worse and shown worse reliability. Some of those comparisons are difficult, but at least that reliability plot that I showed you earlier, death forecasts have done much better than that. Another component here that Seb also mentioned is model calibration. So we can look at different prediction intervals and what's the, so if you predict like a 90% chance of something happening, is that happening around 90%? Is that happening around 90% of the time when you're predicting that? And so we can sort of scale those here. And so the version on the left is what ensembles tend to look like more or less, where there is if you predict a 90 to 100% chance for a set of outcomes that those happen, those tend to occur 90 to 100% of the time. We see more often among team models overconfidence. This is an extreme example here that when they're predicting 99. Year that when they're predicting 99, 90 to 100% of the time, something to happen, it's actually happening about 30% of the time. Underconfidence, we also see sometimes. So they're saying that this is 60% likely, but it actually happens 100% of the time when they make that prediction. No confidence. So it's very easy to make those models and say we just don't know what's going to happen. So you can have good calibration to the fact that you're always getting the observations within your 95% prediction interval. Within your 95% prediction interval, but you also have no confidence about what's actually going to happen. You can also have all sorts of competence and like no ability to distinguish at all. So this is even worse than being overconfident. And it's like, you know, the frequency of observation has no relationship to what your prediction of that is. We see that all the models that perform better are better calibrated. How they get there is still a bit of an unknown in many cases, I think. In many cases, I think. So, what makes a good forecast? I will talk a little bit more about that and based on the things that I just discussed. But I think we also need to remember the things that go into a forecast. So it's really hard to figure out exactly what makes these forecasts good because even when we're talking about 100 different forecasts, there are many different combinations of the different components that go into a forecast. So there's the data inputs that you're using, how many, which ones they have. How many, which ones they are, how you're integrating them, what's your model structure? Are you using like a statistical model, a Bayesian model, a mechanistic model with some sort of simulations? How are you doing your parameter selection, your model calibration, your fitting process? And then how are you generating your forecast? So how are you generating your uncertainty intervals and processing those? Are you using ensemble approaches or not? All of these make it really difficult to sort of identify why individual models are outperforming others. Individual models are outperforming others. So we can just say some generalities. And you'll notice I have, I just sort of threw this together, but made sure to add some asterisks. This probably should be asterisks on everything here. So better forecasts happen when people integrate more models. That's been, I think, pretty clearly established on the whole. We've also seen that statistical models have generally outperformed mechanistic models. COVID has been a little bit different in that respect, and I think maybe that's partly related to the lack of. Partly related to the lack of historical data, at least at the beginning of the pandemic. And still, you know, we're not into sort of a seasonal thing per se yet, where the variants make it a lot more complex and also the behavioral components than what we've seen with most of these previous ones. Autoregression, I mean, there's clearly autoregression in infectious diseases. And I think everything we've looked at suggests you need to have some component of that in your model relating to your near-term predictions. Near-term predictions to the most recent data that you have available to use. Historical data clearly helps in most cases, but it's not always available, right? So that's, I think, the biggest caveat there. Calibration, you know, teams have started paying more attention to this, trying to get your uncertainty right, because it really helps. And it's really important when you're thinking about it on the decision maker side too. Single model, right? Opposite of multiple models, mechanistic. Uh, the opposite of multiple models mechanistic has tended to perform worse. Uh, not an indictment of mechanistic models. I think we just need to think better about how to integrate them into forecasting, and particularly useful when we're thinking about things where we know that there's components that are going to change, like vaccine introduction for when you didn't have a vaccine before, or changing transmissibility because of a new variant. Lots of things related to why they're important. Things that lots of things related to why they're important, but also we need to be careful about how we're using them for forecasting. More data sources. So, you know, for COVID death forecasts, using COVID case data and hospitalization data clearly helps. But for many other data sources, we don't really have that clear about how much they are contributing to the models, though there's plenty of reason to think that they should be able to contribute. Climate data, so when we've looked at, you know, the example. When we've looked at the example I gave for the dengue forecasting, and we saw that both with the team forecast in the challenge and in those individual model study that I showed, that we saw that climate data didn't actually help with the forecast, but it's not that it's not important. So is there a way to incorporate that that actually will improve forecast once you figured out some of the other pieces that are contributing to that? We've seen the same thing with vectors. So mosquitoes, people who use, not many teams actually use mosquitoes in the dengue forecast. Use mosquitoes in the dengue forecasting models, but there wasn't mosquito data available. If that data had been available, which there usually isn't, could you do better forecasting? Unclear. For the West Nile challenge, using vector data that is also limited availability, but there's sort of more of it on the geographical scale in the US did not make predictions better. Not to say that it couldn't, but it's sort of unclear what's how some of these other components that we know that are important for treatment. Components that we know that are important for transmission really can be used to help improve forecasting. I did want to talk, so I'm going to go through some questions and I'll try to be cognizant of time. So I'll have to go through these quickly, I guess. How should we assess forecasts, right? So I think we've seen, you know, we've already talked about the importance of proper scores using probabilistic forecasts rather than point forecasts. I think we're going to just hopefully. I think we're going to just hopefully see more and more of this, right? We're not still not seeing that all the time. But I think there's even within the proper scores, there's some things that we haven't entirely sorted out. So these are three different dummy forecasts that I made just to illustrate how these metrics are different. The first is like a very, you know, you could say that these are like for hospitalizations four weeks ahead in the state. So it's really important, like, do we know, are they going up, going down? What's kind of the, what's the magnitude of what we might be facing? Might be facing. The first one is a very naive forecast that says, like, pretty much anything is possible. We expect to be on the low side. So the red dot here is the actual observation. Again, I just made this up, right? The second is a forecast that's sort of more reasonable, has a good bit of uncertainty. And the third is one that has a lot of certainty relative to the others, but is a little bit off, right? And so the mean is actually less off than the second one. Off than the second one. So it has a lower absolute error than the second one and a lower WIS, but actually a terrible log score because it actually said that the observed outcome was not possible. So it assigns zero probability to that potential outcome. Now, that certainly, you know, when we actually analyze these, we set a lower bound because you can't recover from an average of a negative infinity. So we sort of fiddle with that. We sort of fiddle with that when we do analyses of log scores. But then when we look at the WIS, so the WIS really penalizes based on the proximity of the whole distribution to the observed value. And so in this case, the WIS does better. In the bottom forecast, the WIS does better. The bottom forecast does better by WIS than the middle forecast, but it also misses this important thing, right? This important thing, right? So, you could have what we're doing when we're doing these analyses is looking at thousands of different forecasts, analyzing them all together. And I think what we want to avoid is being careful about sort of the interpretation of WISC alone, because you can have a forecast that performs very well by WIS, but actually gets lots of things objectively wrong by saying that there's zero probability of a certain outcome happening. Certain outcome happened. And so, when we think about it from like the decision maker side, that uncertainty is really important that we sort of the middle forecast being maybe, maybe sort of a more useful one in that case. But also, the third one is actually giving us more information in a way. So, how do we balance those out and make sure that we're understanding this when we're doing these analyses and communicating them to the decision makers? Again, I mentioned I think we need more studies. I'm trying to figure out what the value of different model components are. The value of different model components are. Extending horizons. So, Seb already talked about the scenario modeling. And I think this is a really helpful way to get at some of those decisions that need to be made at longer horizons. So, trying to understand what's going to happen six months from now, a year from now. This is just an example from the US Scenario Modeling Hub, most recent example of looking at different levels of waning versus the possible impact of a new variant. Impact of a new variant that has some immune escape characteristics. You can see when we project these out, there's a whole lot of uncertainty there. And that uncertainty is sort of a useful thing on its own and helps us understand some of the things that could lead to those dynamics. And I think we'll see more and more of this. And this is a way of dealing with some of those longer term horizons without having to do explicit forecasting. It also presents new challenges for how you build ensembles and how you evaluate. And how you evaluate these models and understand sort of the relative contributions of them. How can we incorporate behavior? Right? So, most of the models on scenario hub models use specific assumptions about mitigation measures going forward. Most of the forecasting models either say we expect the same level of mitigation measures to be present over the next four weeks, or we expect a change in them. A change in them according to the recent trend. So, sort of similar to the baseline versus like RT component. And so, that's the way they're dealt with. But actually, we know that there's feedback loops here, right? So, how do we start integrating those models? It's also a problem in the scenario hub because when we think about these scenarios, right, how do you parameterize what those relationships are? So, if we see an increase in cases, how many increases before we see a change in behavior? How much of a change? Change in behavior. How much of a change in behavior do we see? What's the impact of that change in behavior on transmission? And if we're going to set up scenarios that a number of teams are going to be running, you have to make sort of some decisions about those so they're all in the same parameter space and you can actually have some sense of what that means. But then how does that reflect reality? Where do forecasts go wrong? So I think some of the particular places that we've seen, this is looking, these are examples of ensemble forecasts. And so, if I can maybe, yeah, okay, I can see here. So, this is one example right down here of one of the behaviors that we see commonly. We see an increase, and really the ensembles and many of the models just sort of point at sort of this flat trajectory. So, they're like projecting sort of a continuation, but actually we're seeing an increase. And it's hard for the ensemble to catch up to that. We see some other models that predict those increases really well, and they predict them right up until the case counts are already actually going down. Actually, going down, and so they'll really shoot off at the top. We end up seeing, sort of, at these critical time points, right, where there's a lot of divergence in the model. Some are predicting a peak, some are predicting those sort of shoots to the top that there's the uncertainty really gets huge. So, you know, that is sort of accurately reflecting in these cases at least the uncertainty or capturing in the prediction intervals, but little information to really concrete information to go on. When we get to the tail end of these peaks, When we get to the tail end of these peaks, we see the decrease is also slow to catch up. So they tend to predict it to keep going and kind of level off and not capture the speed of declines that we see. There's also work on building better ensembles. I think I just need to go ahead and unfortunately not describe this more. There's a lot of interesting questions about ensembles. How do you do your training? How do you do your weighting? Questions like that. I think we've seen. Like that. I think we've seen a lot of changes in the way that we do modeling, especially working collaboratively. And I think we'll continue to see changes there, but we also have to continue to work for those changes. And then so lastly, sort of conclusions. I'll put together some thoughts here. So the standards for data code and code sharing have really taken us a long way in terms of being able to understand models. In terms of being able to understand models and forecasts, right? And I think they can take us a lot further. They've enabled us to be able to make these predictions on a weekly basis, but they can also enable us to figure out how to do that better. Time and time again in all these challenges, we've seen that people are bringing like great ideas to the table in models and making bad predictions with them. So we know that these like there's good early indicators of things, but how do you integrate those into Of things, but how do you integrate those into really making a good out-of-sample prediction is hard, and we need to continue to work on that. Seb also mentioned this: that like forecasting can be the test of causality, right? So I think there's a lot that we can learn by doing good forecasts beyond just having good forecasts. Recent and long-term historical outcome data are consistently important ingredients. So I think this one is something that we've especially seen in some of the challenges where we've used data. And some of the challenges where we've used data where there's more historical data available, and how do we think about that differently when we have an emerging pathogen like COVID-19 or the next pandemic? I think we don't generally do a great job of sort of evaluating the contributions of particular modeling approaches or parameters or models or data. Sorry, it was the other thing I had in there. And so I think we could sort of do a better job of. And so, I think we could sort of do a better job of that, trying to really figure out where we're getting information gains from. One of the approaches is like comparing to baseline models, right? Another is sort of doing comparisons where you're swapping in one set of data for another or doing sort of ablation studies. And then lastly, emerging pathogens really require adaptable approaches for all these things. We have new data, we have changing data, changing case definitions, new data systems coming online. Systems coming online, limited historical data available. And we have these mitigation responses that are on a scale that we've never seen before, right? So, a lot of things we need to think about. Thankfully, there are a lot of people thinking about these things, including all of you. Ton of people contributed to this work in one way or another. So, I've named a bunch, Seb, and a lot of those overlap in sort of these groups. But really, the teams have contributed a ton of this work to this, too. And there's hundreds of people that I can't. and there's hundreds of people that I can't make here. So thanks.