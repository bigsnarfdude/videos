And let me start maybe with a big thank you to the organizers for organizing this great event. I'm really enjoying it. And thanks for all the speakers for sharing their fantastic knowledge. Today, I would like to talk about the line of work which I have pursued in the last two years, where we basically looked at graph convolutional networks, GCNs, which are among the very popular graph neural networks, and we were looking at the Networks, and we were looking at the limitations of those and came up with a very powerful framework to kind of make them more powerful with a hybrid approach. And with we, of course, I mean these collaborators and myself. So big thanks also to Ymeng, Matt, Michael, and Guy. It has really been a great experience working with Experience working with them, and it still is because we are still not finished, of course. So, before diving into the actual framework we are proposing, let me give you a short overview into the setting we are working with. I think we had many great introductions into graph learning, so I will go with a very scarce or coarse-grain introduction. But generally, we work on data sets. Generally, we work on data sets like social networks, citation networks, and molecules, all of which are very nicely modeled by graphs. Most popular, or if we think of graphs, we probably think of social networks like Facebook, where our nodes are users and the edges describe like the friendship between the users, for example. Citation networks are actually or were until recently the most common benchmarking data sets in graph neural networks. And here we can Graph neural networks, and here we can think of something like Wikipedia, where our nodes are Wikipedia articles and the edges of our graph describe how the hyperlinks are wired between those websites. Then other objects we have seen in many talks so far are molecules, where we can see our atoms as the nodes of the graph, and the edges describe how those are kind of connected to each other. Common tasks we do on those data sets are node classification, where we want to separate the nodes of our graph into different classes. In citation networks like Wikipedia, we can think of it like to every Wikipedia article, we want to kind of put it into a group like what area it comes from. Is it like more here? You can see biotechnology, medicine, technology. Medicine, technology. That's the kind of task we would pursue there. Then we have graph classification and regression tasks, often seen on molecules. For example, we want to maybe find out whether a molecule is an enzyme or not, or we want to just regress how much energy a certain molecule has. And then, lastly, of course, link prediction on the social network, of course. On the social network, of course, based on the edges or relationships we know of the social network. For example, in Facebook, they want to be able to propose you new kind of relations that make sense. So that's another very popular task. Today, I will mostly talk about node classification. And yeah, for that reason, let me briefly talk about what kind of input. Yeah, what kind of input we use for graph neural networks usually. So, we have already seen that we always have some kind of graph structure, which is usually encoded in an adjacency matrix. But we also have another important ingredient for most models, and that are so-called node features. And those give you sort of a local information at every node. And what we usually do is that we represent those. Usually, do is that we represent those node features in a so-called node feature matrix, where each row of our matrix, which you see kind of visualized here to the right, represents the node feature of one specific node. Think of maybe a citation network here for a Wikipedia article. This node feature would be an Euclidean word embedding of the text of the Wikipedia page, for example. Yeah, and so the goal becomes. Yeah, and so the goal becomes for whatever task we are actually working on to learn within several layers of a graph neural network more informative node features so that out of our neural network we get the new node feature matrix which has like the necessary information to do our prediction or classification. Let me quickly go about the graph convolutional network because that's About the graph convolutional network because that's really the basis of what we have been thinking about and what we wanted to expand on. So there's both a spectral and a spatial interpretation of all the things you will see in this presentation. And I will mostly stick to the spatial interpretation in the node domain today, because I think we have already seen some nice spectral theory here, for example, in Mike's talk on Monday. So the So, in the GCN filter, which was introduced by Kip Pinwelling, it nicely decomposes into three basic steps. First, the aggregation step, where we implement some sort of message passing between nodes. So what happens here at every node, we will do an aggregation where we will replace our node feature in our node feature matrix by some function of the feature of the node itself and of its neighbors. And we will do this at all node symmetry. We will do this at all nodes simultaneously. And so we come up with this diffused or aggregated node feature matrix as the output of our aggregation step. And yeah, notably now each node feature here in this matrix has information from the neighborhood of the corresponding color in the snapshot of the graph to the left. So this is followed then by the transformation and Followed then by the transformation and non-linearity, which kind of are applied in the same way to each of the nodes. So nice maybe parallel. If we just disregarded the aggregation step, our graph neural network would just become a multi-layer perceptron. So that would be very boring. So that's the aggregation step is really making a huge difference here. Yes, some quick parallel to Euclidean CNNs is basically that you have this nice property. If you iterate several layers of a GCN, your receptive field at every node increases. So after two layers of GCN, as we can see here, we have kind of aggregate information not from a one-step neighborhood, but from a two-step neighborhood instead. It's very similar to CNNs. To CNNs. And GCNs can really be seen as some sort of generalization of the sliding window idea in CNNs, only that we kind of lack regularity on a graph that would allow us to really learn the actual weights of our sliding window. That's why we kind of stick with this very generic aggregation functions. Okay, so and in the beginning of my PhD, I was basically reading into the literature what kind of limitations we face when working with graph neural networks and in particular GCN. And one very popular narrative is that of low-pass filtering, which means if we look at our information on our node features in the spectral domain, most GNNs and in particular GCN basically And in particular, GCN basically mostly are sensitive to the low frequency or low variation information over the graph. And that, of course, you lose some information if you iterate layers of that. And this problem has a kind of a corresponding interpretation in the spatial domain or in the node domain, where it's called over smoothing. So we've seen that we basically all do aggregations all over again. Aggregations all over again. And so, if we do that again and again, of course, our node features will become very similar. And in particular, in the case of node classification, that's not desirable because how can we differentiate two nodes if their features are very similar? And closely connected problem to that of oversmoothing is so-called under-reaching, where GNNs can only relate information from nodes that. Information from nodes that are within a distance equal to the number of layers. So, if we wanted to somehow learn a relationship between the two red nodes, we would need at least a five-layer GCN, for example, to kind of share information between them. But of course, as we have seen over smoothing, that's usually prohibited because of the oversmoothing problem. So, they are inherently connected, the oversmooting and the under-reaching problem. Problem and yeah, so then after reading into this, like we were kind of trying to figure out what we can do about this, and of course, in our case, the solution is graph scattering. So, and the and the motivation for this is that using scattering, we can define bandpass filters over the graph signals, in contrast to the usual low-pass filtering in other GNNs. In other genomes, and also they have wider receptive fields. So, in some sense, we both combat the over-smoothing and the underreaching problem at the same time with such features. And what we introduced in 2020 at Neurops was this scattering filter, which looks very similar to the GCN filter, except that the first, like the aggregation is a bit different. Now, instead of just aggregating from one step, Of just aggregating from a one-step neighborhood, we will do a more complex diffusion or aggregation operation, which I will define in more detail in a second. And traditionally, as we can see on the bottom, that's how we use those diffusion wavelets in traditional graph scattering. We will build kind of a cascade of diffusion operations, psi here, and non-linearities to filter our signals. Filter our signals and our network will kind of turn out to be something very similar, also, some kind of cascade, up to the change that we also introduce learned parameters in this model. So Michael Perlmutter has already given a very nice introduction to the spectral properties of scattering on Monday. So I allowed myself to kind of steal his. To kind of steal his nice slide here, which shows or which kind of shows the idea that we can cover kind of our space of signals with a nice decomposition into frequency bands if we use a certain filter bank of those diffusion wavelets. And yeah, but as I said earlier, today I will go more into the spatial interpretation of diffusion wavelets because of diffusion wavelets because that's something which is really i don't think it has been really explored so um let's look into that uh today so what actually happens if we apply a diffusion wavelet at a certain node is uh three uh basic steps so if we choose the diffusion wavelet psi j so j is the order of the diffusion wavelet the first step will be that we will diffuse or aggregate our node features or aggregate our node features 2 to the j minus 1 times. And for j equals to 1, that's what we can see here to the left. So at the black node, we will aggregate from a one-step neighborhood. And in the second step, what you can see to the right, we will aggregate twice as many times. So from a two-step neighborhood, and again, yeah, do this aggregation operation at our black node. And then the And then the very important step is the third one, where we will calculate the difference of the two aggregated node features by subtracting the two outcoming node feature matrices. So this is a very big difference to what the GCN is doing, because GCN is just aggregating or averaging, while here we can see this as a comparison operation at every single node. Operation at every single node where we compare kind of the one-step neighborhood to the two-step neighborhood at each individual node. So it's a comparison operation. And the same thing you can see here for J equals to 2, where we will look at the two-step neighborhoods, first and three, four-step neighborhood to the right, and again compare those two neighborhoods. And yeah, as you can already see, Yeah, as you can already see by increasing J, our receptive field of those filters grows very fast. And so our model gets a very big reach, while at the same time, provably establishing a bandpass filter, which arguably is less subject to the oversmoving problem. So that's the interpretation of our idea. Yeah. Yeah, so now coming to the actual framework we are using. So, of course, there's the question: like, how can we combine the information from the different filters? Because, of course, the GCN filter is very useful as well. It works great. So, we really want to combine the forces of GCN or low-pass filtering and band-pass filtering at the same time. So, what we will usually do, we will We will calculate several low-pass filters and several bandpass filters and just store those filter responses in a list, let's say. But now the question becomes how can we combine the knowledge from those different filter responses? And in the scattering GCN, like the first paper we published about this, we use just a simple concatenation operation followed. Concatenation operation followed by an MLP. So we would take those here in this case, it's six different filter responses, which is very close to what we empirically used in our models. We will just put them horizontally next to each other, concatenate them, and then apply an MLP or a linear layer. So, what this actually does, the linear layer, then it will learn a linear combination of the different filter responses. Of the different filter responses coming from different band pass filters or low-pass filters. And so the model can highlight certain frequency bands or also like leave them out. So the model gets access to wide a variety of kind of spectral information. And this has proven very powerful in the setting of semi-supervised node classification, where we tested. Where we tested the performance of the model in like related to the over-smoothing problem. So, what we did here, we compared our sketching GCN model to a variety of other models, which are arguably subject to the over-smoothing problem. And what we did is test them or experiment with them on four different data sets, which here are ordered from left to right according to increasing connectivity. According to increasing connectivity. And that's important because connectivity, according to Lee et al., is known to amplify the overs moving problem. So what we would expect now is that our scheduling GCN will have a relative improvement over the other models when we move from left to right. And that's exactly what we observe here. Like while on site here, we are slightly outperformed by rough at. Outperformed by rough attention. On Core and PubMed, we are already like the best performing model. And then on DBLP, which has the highest connectivity and hence the highest over smoothing problem, we outperform the other models by quite a large margin. And it's also telling that the next best model on UBLP is actually a graph scattering baseline from Luhrmann. Luhrmann. So it seems to be really beneficial to add this bandpass information to the model to overcome over smoothing. So yeah, that was kind of the first part of this line of work. But yeah, there was a bunch of things which we noticed about our model, which made us think to maybe elaborate on it. So we have seen that this concatenation plus We have seen that this concatenation plus MLP idea basically acts on the spectral domain. It selects certain frequency bands which we want to use. However, this is a very global approach to combining our different filter responses. And I mean, complex data might have, or the task might depend on very different regularity patterns depending on what graph region you're dealing with. So we would like to have. With. So we would like to have a more local selection of kind of how to choose the features from the different filters. And that brings us back to the idea of seeing the diffusion wavelet as a comparison operation at the node level. Because we actually, like that was what we did in the That was what we did in the geometric scattering attention network. Here we define a node-level attention mechanism where every node can kind of choose from here the six different aggregation or comparison operations which are implemented from the low-pass and bandpass filters. So, let me show you maybe what this gives us in the end. So, let's The end. So let's look at the node V, which is colored in red here. And what the low-pass filters will do, we will get here in red a feature in our filter response, which corresponds to a average over the one-step neighborhood of the node V. Then the second filter, what we usually did is like apply the GCN filter twice to our signal. Like A is the diffusion matrix of the GCN. The diffusion matrix of the GCN model, and that would aggregate over two-step neighborhoods. And yeah, in the same way, the third low-pass filter would do something similar. And now, in contrast, the bandpass filters on the right, here we will compare, like we will do this comparison operation. For example, here that would be psi zero. We would compare the node to its one-step neighborhood. For psi one, we would For psi one, we would compare the two the one-step neighborhood of the node to its two-step neighborhood, and um for psi two, as we have seen before, we compare the two-step neighborhood to the four-step neighborhood. So, and what now happens is that our model at every node has access to these three six operations, three aggregation operations and three comparison operations. And the importance of them is learned by an attention. Them is learned by an attention mechanism. So we will first concatenate our node feature of the node V before and before the diffusion with that after the diffusion. So the red one, which we have seen a second ago. And then we will multiply this with a learned attention vector, which is shared across the model. And that will give us an important score of the specific filter for the known. Specific filter for the node V. And of course, if we do this for all the filters, we get six important scores. And by soft maxing, we will approximate like the choice of one of these operations which we want to choose at node V. Yeah, and then we will kind of wait at the node V by the in the in the combination of the filter responses we The combination of the filter responses, we will weight the different filter responses by the score which comes out of the softmax. So, and this basically chooses one of those operations I've talked about. But by using a multi-head attention mechanism, we can also make it more flexible so that every node can kind of pay attention to different kinds of operations. We tested this then on This then on kind of more complex data, and one proxy for more complex data in graph learning is like less homophile data. And so we have here kind of some benchmark data sets ordered by increasing homophily. So homophily is basically like how likely it is that two neighboring nodes share the same label in the node classification task. So if we have Task. So if we have high homophily, our clusters of nodes tend to share the same label, while for low homophily, we have more like variation or more high frequency labeling of our graph. So we can envision it. Yeah, so, and what we now expect is that our geometric scattering attention network to the very right will kind of improve relatively over the other methods, including the scattering GCN model when we. The sketching GCN model when we decrease the homophili, so from bottom to top, and indeed, like for the first or the bottom four data sets, which we also tested for sketching GCN, our GZN performs relatively similar to the sketching GCN. But then when the homophilic decreases, the GZEN really outperforms the other methods. And especially on chameleon, which has the lowest homophily, we are even like The lowest homophily, we are even like 10% better than the original sketching GCN model, which we introduced before. So that's kind of our advocate for the claim of kind of generalizing better to more complex data sets. Another nice insight in this model was found when we used a similar model for kind of Model for kind of approximating solutions to the maximum click problem. So this is actually a quite new paper we put on archive recently. So actually the goal here is, so we have a graph, for example, from Twitter, like 150 nodes, and we want to find the largest clique in that graph. That's an NP-hard problem, but it turns out with our hybrid scattering networks, we can approximate solutions quite well. Approximate solutions quite well. So, and this left part of the model is kind of our hybrid sketching network, and it's trained in an unsupervised way and outputs kind of a probability over the nodes of our graph, which is supposed to tell us how likely the node is to be in the maximum click. And then we have a rule-based decoder on the right, which goes through the nodes according to decreasing probability and Uh, probability and subsequently checks whether we are still in the clique. And by this approach, we approximate like the maximum clique quite nicely, especially compared to other graph neural network methods, which are just GCN and Erdős. That's the only ones we were able to find. But even when we compare it to a solver, the GRUBY solver, when we limit the time. Solver, when we limit the time budget, we can still outperform that actually. But I don't want to highlight actually the results. The interesting thing I want to talk about was like the alleviating over smoothing aspect of this, because here we have two snapshots of the kind of probability distributions which come out of our hybrid model. So one of them is calculated from a GCN model, and one is coming from a hybrid scattering model. Hybrid scattering model. And as you probably guessed, this one is coming from the hybrid scattering model because it's much more diverse and much more, much less subject to the oversmooting problem. So this was the nice insight also for the geometric sketching attention network. Yeah, so that's it for the empirical results. Lastly, I want to talk a little bit about some work which we did. Some work, which we did theoretically on hybrid scattering networks, where we were interested in how well they can capture the geometry of local node neighborhoods. You can see this kind of as an analogon of isomorphism testing only transferred to the node level. Because like the Weisen-Ferr Lehmann test is really something you want to apply for whole graphs, and for node classification, this is really not all. For node classification, this is really not all that useful, actually. So, and what we now want to do with this theory is to discriminate two nodes based on the topology of their local neighborhoods or k balls in a bit similar way as Fernando has done it in his presentation. So, we define k-intrinsic node features as node features that agree for all pairs of nodes. For all pairs of nodes with isomorphic k-step neighborhoods. So, and k-intrinsic node features can be easily constructed. For example, the node degree would be a one-intrinsic node feature, because if our one-state neighborhood of two nodes is isomorphic, they will always have the same degree. Of course, that's a very simple example for an intrinsic node feature. But the intuition is basically that you. Intuition is basically that using intrinsic node features, every node gains a line of sight of radius k to differentiate itself from others using structural difference that break isometry in the local neighborhood. So let's look at the two black nodes, V and V prime. As you can see, the one-step neighborhoods and the one two-step neighborhoods of the two are isomorphic. So with one and two-step. So, with one and two-step, one and two intrinsic node features, they would have the same feature. However, if we expand our neighborhood to three, we see like the structural differences which are highlighted in red, then for example, we would be able to discriminate them. For example, with the three intrinsic node feature of average degree in a three-step neighborhood. That's a three-intrinsic node feature. Intrinsic node feature. Yeah, so that's kind of the definition of those node features. And we did some theory on those. First, we had this theorem here where we show that if you use k-intrinsic node features together with the L-layer GCN, we cannot discriminate nodes who have an isomorphic L plus K step neighborhood. So this is actually quite intuitive. So, so this is actually quite intuitive because it's you can see it as a more sophisticated or more specific formulation of the under-reaching problem. Because if your node feature completely depend on the graph structure and your neural network has only a certain reach, if your ball around your two nodes, which is isomorphic, is large enough, you can never discriminate them. So that's basically an That's basically an alternative formulation of under-reaching here. However, we have this very nice second result where we show that we can always come up with a sketching-based filter which can discriminate them but only if we want to discriminate them. So let me spell it maybe also. So we again use k-intrinsic node features. And then if we have actually a structural differences. Have actually a structural difference manifested in our L plus K-step neighborhood, then we can define such a scattering filter. So what does it mean that a structural difference is manifested in our L plus K-st neighborhood? That would be here these blue nodes. Imagine we are using K-intrinsic node features, one intrinsic node features. That means they take into consideration the one-step neighborhood around them. The one-step neighborhood around them to calculate or to establish their node feature. Then, those blue nodes, this blue node will be able to discriminate itself from this one, this one from this one, because they kind of see that there is a structural difference nearby them. And this is then the information which our model can propagate to the nodes v and v prime respectively and use to differentiate them. So, yeah, this is a very nice result. And what has to be said is that the arguments are very much based on reach. So actually, you could also define a GCN filter, of course, who has enough reach to have this property. But the thing is that our sketching filter is not subject to over smoothing. Is not subject to oversmooting, while the GCN is kind of prohibited to do to give it a reach which would be high enough. And also a nice insight is, of course, that we can absolutely say, yeah, we will be able to discriminate them. There's nothing can stop us there. So this is what this second theorem says. Yeah. So that brings me to the main takeaways of this presentation. Takeaways of this presentation. Hybrid sketching models alleviate oversmooting and under-reaching. And in a certain sense, I would say they are kind of optimizing the trade-off between oversmoothing and under-reaching. Because yeah, you add reach while oversmoothing to a certain extent, but not as much. And yeah, as we have seen empirically, it works very well on complex data here, low homophily data in this case. Low homophily data in this case. And yeah, theoretically, we were able to show that they are quite good in capturing the graph structure. That's my presentation. Thank you very much for your attention. And I'll take the questions.