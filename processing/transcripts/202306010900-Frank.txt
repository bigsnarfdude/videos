And we haven't been able to completely solve this. And if anybody has any ideas, I'm certainly interested in talking with you more about this. So, this is actually a project that many of us here in the room have worked on together. Jos√© Antonio Carrillo, Matthias Delgadinho, Jean-Dolbeau, Franca Hoffmann, and then for the second paper, we had helped from Mathieu Levine from Peristofin, who helped us. From Peristofine, who helped us there. And so I'll write the problem right away here at this beginning. So, this is a minimization problem. Okay, so you have a certain energy functional, and you try to make this as small as possible. More precisely, it's a free energy functional where you have a double integral where a particle at X interacts with a particle at Y, and they interact through this interaction kernel, which I choose to be a power function. choose to be a power function okay x minus y to the k and k here is a positive number okay it's a parameter of the game and then so this is an attractive force that tries to bring stuff together to aggregate stuff okay now this term is penalized by an entropic term which acts as a repulsion or tries to spread things out to diffuse things okay and so that is a Okay, and so that is rho to the m, and m is here between zero and one. So that's this fast diffusion machine, if you want. Okay, good. And well, this is defined on probability densities rho, and our goal is to minimize this. And the question that we're asking is: sort of, does this problem have a minimizer? This sounds like a very, I know, very boring question, like, I mean, something that has been solved 40 years ago. 40 years ago. But I try to convince you that it is not, or that there are some unexpected things. So, of course, there are questions before and after. So, before you ask whether there's a minimizer, better that the infimum is not minus infinity, because if it's minus infinity, then there's nothing to do. Okay. And now, so, but once you've shown that there's a minimizer, then you can ask, what do we know about these minimizers? How do they look like? And that's again, I mean, so, so, both the first question. I mean, so both the first question, the finiteness and the knowledge about the minimizer, this is again relatively standard. The surprising thing is this question about other minimizers to this question. Okay, and so I mean from if you step a little bit back and why is one interested in this, this you can think of somehow as a prototypical model of an interaction. I mean, with two way of a competition between two different forces, a repulsion. Forces, a repulsion and an attraction, and whether there's a minimize a means, can these two forces sort of reach a balance? And I already, I mean, this is clear in this workshop, that somehow what's in the back of our mind is a certain aggregation diffusion equation that's the gradient flow, Wasserstein gradient flow of this energy functional. And eventually, we would like to use what we learn about the minimization process. We learn about the minimization problem in our study of this equation. But I don't have anything to report about that equation, so that's something for you to think about. And so I want to tell you the conclusions of the talk right away so that you can decide whether you want to continue or not. So there is the whole parameter, remember the parameter region that depends on, so there's the dimension which is sort of So there's the dimension, which is sort of fixed, and then there is K, which is the power in the interaction potential, and then there is M, which is this parameter between zero and one. Okay, and so there are two lines here, which will not a good drawing, but so the upper line, the But so the upper line, the precise numbers are not so important. But the upper line is 2d over 2d plus k, and the lower line is d over d plus k. So they are asymptotic like 1 over k, but they're different. And so the story is down there, that's our region number one, there the infimum is minus infinity. So the whole problem down there doesn't make So the whole problem down there doesn't make any sense. Good. Number two, there, the problem again is very simple. We have a minimized and we're happy. The interesting part, the really interesting part, is what happens here in between, in this region number two, between those two lines. And actually, endpoints excluded. Okay, and so. Okay, and so there we're not able to prove the existence of minimizers, or more precisely, what we are able is to prove the existence of minimizers for a relaxed problem, but we do not know whether they are minimizers of the original problem. Okay, and so what happens, what is the relaxed problem? Sort of we allow deltas, okay? But it's not that the whole function, not the whole density will be a delta, but some part of the density decides. Some part of the density decides to become a delta. The rest doesn't. So it's a partial mass concentration, which at first sight is, I think, rather unexpected for such a problem. And so the question is, in there, this happens somewhere and somewhere else it doesn't happen. So we have proofs that in some part of this regime it does not happen. In other proof in other regimes, we can show that it does happen. And we would like to understand this better. I mean, what really decides? This better. I mean, what really decides whether delta wants to form or delta does not want to form? And I should perhaps say this right away when you look at the functional. So you see, I mean, this thing. So if you take a delta sequence, right, and you raise it to a power which is less than one, then it sort of disappears. It doesn't contribute. So the first part only sees that part of the probability. That part of the probability measure that's not a delta. The other term here, again, so this is a quadratic term, so you write out the square. So there's a delta talking to itself, but that is equal to zero, right? Because x minus y vanishes at this point, okay? But there's a cross term where the delta talks to the absolutely continuous part. And so that already tells you that you do not want to be a full delta, right? Because the full delta would have this. Delta would have this equal to zero and that equal to zero. So that explains why, if something happens, you want to have a partial, part of the mass inside the delta. But as I said, how much we don't know. So let me go back to put this problem a little bit into context, what people have done before. And this slide essentially tells you that problems of this type were really studied in detail. In detail 40 years ago, in the first half of the 80s, and rather well understood. So, what is different here in this problem is, so K now is negative. Okay, and when K is negative, I put a minus sign there. So, this is still just like before, this is an attractive interaction. Whereas here, the M now is greater than one, and I flip the sign there too, so that this. Flip the sign there too, so that this is again a repulsive thing that spreads mass apart. Okay, and so this I give you the reference. I mean, this has to do with lots of theory of symmetric decreasing rearrangement that was developed back then. You can use this to prove existence. Lyons, this is actually his first, I mean, the first example in his first concentration compactness paper, he proved that any minimizing sequence in That any minimizing sequence is relatively compact. Lieb found in one particular case, which actually happens to be that upper line, right? Found some hidden symmetry. There's a conformal symmetry that you would not expect when you first look at it, and used this to solve it in that special case. And so far, these are the general results. I mean, this is the general theory, and those you can apply, and that's it. And that's it. Order of magnitude deeper are uniqueness results, because there you really need to understand the very detailed properties of the problem at hand. And the first uniqueness result that I'm aware of is due to Lieb and Yao in the Colombic case. And then this whole problem, I guess, lay dormant and then it saw a renaissance in the last five years, say. And so there's And so there are some beautiful uniqueness results by Carlvis Cario-Hoffmann and Delgadino-Jan-Yao, which cover different cases of parameters m and k. Not everything is covered yet, but quite a large region. But anyway, these are results that cover the case k less than zero, and we're interested in k greater than zero. All right. And so the only All right, and so the only result that I know for k greater than zero is the result exactly on this line. This was done in 2015, where Du and Xu sort of saw that you have exactly the same symmetry that Lieb discovered. You have this conformal symmetry, and that allows you to write down a complete solution of everything. Okay, so that's this line. But the point that I want to get across these works here, which These works here, which shaped our intuition of what can happen in such problems, they do not show any mass concentration phenomena. All right. Now, the thing I want to do next is I want to talk about a different problem, a different but equivalent problem. See, when you add these two terms, so there's a row to the m and then there's a row. There is a rho, x, y, k, rho. These have different n things. The third term is the row, the integral of the row. These all have different physical dimensions, right, when you scale. And so what you can do is you just introduce a scaling parameter and then optimize over that scaling parameter. And in this way, instead of minimizing the free energy, you minimize this quotient. Okay, that's, I mean, nothing has changed really. Nothing has changed really. So now we minimize the double integral divided by, and there are the two terms in the problem, the L1 norm and the LF norm. Okay, good. Now, why did I do that? Oh, and I should say that there's an exponent theta appearing when you do that. And by scaling, I mean, this has to be a certain value. Now, why did I do that? I want to say I did do that so that you see now. See now the importance of this line, the upper line, okay, which we did not see at all in the free energy. Now, that line just comes about, just look at the formula for theta. You see theta can be both positive or negative. And it's exactly on this line where it changes sign. And that makes a big difference, right, whether the theta here is positive or negative, because either it plays in your favor. It plays in your favor, right? I mean, this is the inequality that we are talking about. If the theta is on that side, then you have to control it. If it is on the other side, then it helps you to control the other guy. Okay, so this is really a big difference. So we shouldn't be surprised that things really change qualitatively as we go from one region to the other. Okay, and so correspondingly, I mean, since the case that separates the line that separates the two cases. That separates the two cases is this case where we have the conformal symmetry. I sometimes talk about superconformal, subconformal cases, just to have a language for these two different regions. Okay. And now we go on. And now I state the results. This is the same results that I stated on the previous slide. I state them now for the multiplicative function. Okay, so first of all, First of all, the first theorem here says that down there, what I called one, if you're below, so that's a different line, that's the other line. If you're below that other line, then you're minus infinity. If you're above that, then you're finite. Good, that's first thing. Now, here, in the next theorem, here we see the conformal line. And that says that up there, the infimoral. There, the infimum is actually achieved. So, we have a minimize, everything is finite. So, the question is: what can we say about the second case? And now, here you see it starts to get messy. Okay, so there are cases and there are results, but they are obviously not final results. Okay, so what do we say? So, in dimensions one and two, we have a minimizer. Okay, remember, when I say minimizer, I mean always a minimizer without delta, right? Delta, right? I mean, an honest, we have a minimization problem with functions rho, and we won't ask ourselves whether there is a function rho that optimizes. Okay, so in dimensions one and two, there always is a minimizer. Good, fine, end of the story. Now let's talk about dimension three. There is a certain regime here, k less than something, where we can show there is a minimizer. Good. And then, so that cuts off. So that cuts off some. So up there, roughly speaking, up there, we have a minimizer. And then we can also, I mean, it's not really perturbative, but you should think of it perturbatively. There is sort of a line very close to but below the conformal line where we also know that above the line we do have a remedy. Okay, so the dangerous region is in the lower part of this region. In the lower part of this region, too. Right, so it's stated. So, if you are this is the conformal line, and if you are between the conformal line and a little bit shifted below, then you have a minimum. Okay, good. So, here's a picture. This is the same drawing that I have here on the board. You know, so the dark gray area, that's the region where we don't know. The white. The white thing down here, there is no question that infimo is minus infinity, and the light gray, everything is fine, but down here, the gray we don't know. This is a picture in dimension four. All dimensions look very similar, and you see this epsilon result that we get. I don't know whether you can even see it, it's a very tiny region there below that line where we can exclude it. But anyway, that's Anyway, that's this. Now, here comes the result part two. I mean, you see, we had this result and then we were trying for a long time. I mean, does it happen? Does it not happen? So we didn't know what to prove. I mean, we had these results that there it does not happen, there it does not happen. We try for a long time to prove that you never have a concentration of mass. So that this is just a spurious thing. But then in the second page, But then, in the second paper, which came out last year, we were finally able in one particular case. So, this is a rather special situation, but nevertheless, in that case, we were able to show that concentration can happen. Okay, so the case where this can happen is where this interaction potential is power four. x minus y to be power four. But the situation, even in that case, is not so clear, or I mean not so obvious. And you see, so in dimension, up to dimension five, you do have an optimizer, you don't have mass concentration. Only starting from dimension six, there is now, so we are in this picture. There is now, so we are in this picture. This is say k equal to 4. Okay, so I'm drawing this line, and now there is a cutoff point, which I'm calling MB. And below that cutoff point, you have the concentration phenomenon, and above it, you don't. That is what we can prove. The point where point where it where i mean where the transition happens between mass concentration and non-concentration is explicit this is this number i don't know whether it has particular meaning but um there you really see the dichotomy and of course k equal to 4 makes things easy because we can expand k equal to 4 right you write it as a polynomial and you can simplify a lot Can simplify a lot. I should say, but it does not trivialize the problem. So, for instance, you might think that, well, if I mean, if it would be really simple, then it should work for power six as well, or power eight or whatever. We cannot be power six. So, that is an open problem, right? So, we need things for power four that work for power four, but don't work for power six. Even though, of course, for six, you can also expand. And it's not for lack of trying. There's something conceptual that we don't understand there. Also, I mean, to tell you that it's not trivial, we are not able to write down the minimizer. Okay, it's not that you can solve this problem explicitly and then just check, but you, I mean, there's still some analysis going on in this thing. And now, I mean, that's not the end of the results. What I want to tell you also, I mean, this k equal to four, this is what we can do, but we also have numerical results. We did analyze this. We didn't analyze this. And so the simplest thing to do numerically is even integers. Okay, I'll do I have results also afterwards, not about integers. So this is k equal to 4. This is where we have the analytic result, k equal to 6, k equal to 8, k equal to 10. Okay, just, and what you see here, I'm not sure whether you can see the colors, but these different lines mean different dimensions. And so what you see is Is sort of in let's talk about k equal to 4 because that's what we've been talking about now. You see, for instance, in dimension 5, so okay, so what's plotted here is the mass versus a certain parameter alpha. And you should ignore the right of everything that's on the right that's over here, okay? On the right of this vertical. Of this vertical line, just forget it. The question is: when this curve comes down, see this curve comes, that's the dimension five curve. This comes down and it hits this line before it hits that line, that horizontal line. That means that there is a transition. In dimension four and dimension three, we're always sort of in the forbidden region, so therefore, over here. So, therefore, over here, we do not have mass concentration. So, mass concentration happens sort of when you're on the left. You see, for instance, here in dimension three, that if you look very carefully, the blue line cuts first here before it cuts there. So, in dimension three, you don't have a transition yet at k equal to eight, but in dimension. To eight, but in dimension three, you have a transition at k equal to ten. Okay, so as the you see, I mean, it looks that there's an obvious monotonicity in the dimension. Okay, something that's now, let's talk about continuous values. So, this was even integers, which is easier. Now, these are continuous values. So, the red dots, these are the even integers. This is what we did in the previous thing. There's a line, this line, right? Line, right, is the numerically computed line. And you see, so the you see, this is Q stands for M. I'm taking this from a different, with different notation. This is the critical M. And the critical M is a function, lambda is the same as K. So the critical M should be a continuous function of M. Okay, so there should be here a region. So there should be here a region. The story is sort of there should be a region here, an open region, where you do have mass concentration. And this is sort of clear in these numerics. And sort of what these numerics also indicate, it's perhaps a little bit technical, but you can think of this as sort of an analytic continuation, so that even when the problem doesn't make any sense anymore, the solution still makes sense. Anymore, the solution still makes sense. Here, you know, I mean, these are questions where the infimum is minus infinity, but where we still get a candidate for the optimizer. It's just that it's all the terms are sort of infinite, but it has a natural continuation in that region. Anyway, that was a side remark. So that was the pictures. So let's look a little bit at the math. Okay, so what I have here is this function. What I have here is this functional, the relaxed functional. This is the functional that you get when you plug in m delta plus rho. Okay? Because then you have the rho rho term. The delta delta we said was zero, and this is the rho delta term. Down there, the L1 norm, right? It's additive in the two things. Additive in the two things, and this one we said does not see the delta. Good. And now what I'm saying is that the value of this relaxed functional is equal to the value of the original. So I can always minimize allowing a delta, and I have the same infimum as for my original function. That's a simple computation. I mean, you just add to your row a sort of an approximate delta function and do the computation, and everything is nice. Do the computation, and everything is nice. So, this gives us a relaxed energy, a relaxed problem, which has the same minimal value of the energy. And so, therefore, the question now becomes for this relaxed minimum, does this have a minimizer? Number one, and number two, does the minimizer of this relaxed problem which now exists does show n component with which is positive? Component which is positive. So we've sort of decoupled the question. We've decoupled the question of existence of a minimizer into, well, showing that there is existence for the relaxed problem and secondly, studying the minimizers for the relaxed problem. Good. So no surprise, the relaxed problem has a minimizer. Good. That's not very hard because with measures you have lots of compactness. measures you have lots of compactness and stuff and as i said if rho star m star is a minimizer we have to ask ourselves is m star equal to zero or not good um i perhaps let me make a small um sidetrack what you see another good thing about the the relaxed functional is you see where this region one comes from remember i told you the region one is the uninteresting region because there the energy is minus infinity now Infinity. Now, minus infinity for the free energy means for the multiplicative functional, the constant is zero. Okay, but you see that immediately, right? You see the M upstairs and downstairs you have M to the theta. Think of M as super large. Just put all the mass into the M. Then you see it makes a difference whether theta is greater than one or less than one. Because otherwise, if theta is bigger than one, then you can. Bigger than one, then you can drive this to zero by taking n to infinity. And that tells you where this line comes from. This line is the line theta is equal to one. This is the line theta is equal to zero, right? Remember? That we saw immediately. Theta equal to one is where this line comes from. I just want to tell you where these strange numbers come from that we see there, which we did not see in the free energy functional at all. Anyway, so let's get back to what. So let's get back to where we were. We said there is an optimizer for the relaxed functional. Let's study the relaxed function. Good. So one thing I didn't tell you, this is, I mean, was hidden in this second slide about what we know from 40 years ago. We have rearrangement inequalities. So because of rearrangement inequalities, it's enough to study densities rho. To study densities rho that are symmetric decreasing. Okay, good. So any optimizer is necessarily symmetric decreasing. Good. Secondly, that's also probably not a surprise for people working with fast diffusion equations. The minimizer, if it exists, is positive everywhere. Okay, good. Now, also, we know that it satisfies a certain Euler Lagrange equation. I don't write it down right now, but it's useful for some of the things we're going to do. Now, the next question. Now, the next question you might ask is: Is the optimizer, namely the solution of this Euler-Lagrange equation, is this bounded? That sounds like the next step. And already there, we see that we don't know. And actually, the question whether the optimizer is bounded could very well be related to the existence of a delta. That's what I'm telling you later on here. Okay, and before I go on, I should say that we. Before I go on, I should say that we do have some uniqueness results. However, they are not particularly deep. They come from either standard optimal transport result or from a recent method by Newpace. So the uniqueness question to a large extent is open and is it again, it would be nice if somebody would take this up. I should also say when you talk about uniqueness in view of application to the time. In view of application to the time-dependent problems, you perhaps don't want to only study the uniqueness of these ground global minimizers, but of uniqueness of steady states, which is a smaller class. But that's again for the experts. Now, here's a lemma which has something to do with this boundedness question. Let's look at the second part. That's roughly speaking the more interesting or the more important part. It says that the solution row. That the solution rho of x has a behavior constant times exploding power function. That's the behavior as x goes to zero. Now, you should take this with a grain of salt because this constant in front might be zero. So either it's positive, then we have a singularity, or it's zero. Now, why this is interesting is if that constant turns out to be zero, then not only the singularity is gone, but Only the singularity is gone, but then the function is bounded. And even more importantly, there is no delta. Let's think what this means, the contrapositive. This means that mass concentration, M positive, necessarily has C positive. So if you have a delta, you also have a power singularity, which is sort of a little bit strange, right? I mean, the power singularity is also. I mean, the power singularity is already a singularity, and you have a delta on top of it. And think about the poor guy who has to implement this numerically, right? And who has to tell a delta apart from such a power singularity. Okay. And I mean, this is also think about the time-dependent problem when this is such a singularity forming or is a delta forming on top of it. So that's what I mean by there is something, the boundedness of the solution is related to the question where. Solution is related to the question whether there is concentration. Okay, now the, I mean, just a small technical thing. This, what I just said, is only true for k greater or equal than 2. But in general, you can sort of have an LP condition. The LP condition makes the constant equal to 0, and then you conclude the same thing that the mass is 0. So what I'm trying to say is forget the k equal to 0. to say is forget the k equal to zero k greater equal than two sorry k greater equal than two condition for all practical purposes the point is now we can look at this singularity and you know i mean the least what we require about our optimizer or the absolutely continuous part is that it's in l1 should be a probability density so then you ask yourself when is the power singularity 2x to the minus 2x to the minus 2 over 1 minus m. When is this compatible with being in L1? And that already excludes concentration in dimension 1 or 2 or in dimension 3 when you're up there. So that was the easy part that just comes from this analysis of the singularity. And it was so easy that for a long time this convinced us sort of it should always. It should always you should always be able to exclude concentration. Anyway, that's not the case. And so here is now the case k equal to 4, where we show existence of concentration. And this is the semi-explicit computation. So what I've written here is the Euler Luck. So I go back now for this part of the proof, it's more convenient to work. It's more convenient to work in this additive free energy formulation. So let's rho is now probability density. And if rho, I mean, what should I say? The whole thing should be a probability measure. So rho is allowed to have mass less or equal than one. And m is exactly the mass that rho is missing. Okay. And now, so you have an Euler-Lagrange equation, I mean, both for the rho part, and you have an Euler Lagrange equation for the M part. And you have an Euler Grange equation for the M part. And well, that's what you have. And the Euler Lagrange equation for the rho part is this thing. I don't know whether you recognize the terms. I mean, this is the derivative of the entropy. This is the interaction term. And this thing, this is, remember, there was an m times integral x to the fourth row. And if you differentiate that, right, so rho is replaced by an infinitesimal perturbation, and this gives you an mx. And this gives you an mx4 in the equation, and then you replace m by 1 minus integral of 1. And that should be equal to 0. And as I said, we are only interested in radial functions. And now we expand the x minus y to the 4. And there's a y to the 4. And then there are some other terms. I know, should I redo this? So, there are some terms, and some of these terms are of this form, for instance, there's a term like this. But then, of course, if you integrate this against a radial function, you can replace it by x squared, y squared times a constant. So, what you see is that this term here actually is integral x minus y to the 4. x minus y to the 4 of y a to the y, this is a polynomial ax to the 4 plus bx squared plus c. That's all it is. Okay, and we're gonna, and now you plug this into the row. So rho to the m minus 1 should be a degree 4 polynomial. Okay, and now you make an ansat. And here this is a degree 4 polynomial. A degree four polynomial, however, I drop a term, right? I insist that this coefficient or any zero-order contribution should be zero. So in particular, this is one of these guys that has the singularity. Remember, I told you this is the singularity guy. Okay, and now you stare at this for a moment and you see the coefficient of the x to the power 4. Of the x to the power 4, that's determined. And then you only have one parameter because you made this choice of the zero auditor, you only have one parameter to play with. And now you work, you sit down and you look at this and do some continuity arguments, and you see that there's a unique choice of B such that this satisfies this Olla-Lagoshi coefficient. Now, that's not clear, right? And this is what I cannot do for dimension six. Do for a dimension six, for instance. I mean, for k equal to six, because then I have to play with more parameters. Anyway, so this you can show. And by the way, the mu, the Lagrange multiplier, is uniquely determined. Okay, and then you can also compute the mass. Right, so given M, the fast diffusion exponent, there's a unique B, and then you for this unique B, you compute the mass, or rather the missing mass. Mass or rather the missing mass, and you find a nice continuous function of this fast diffusion component. Okay, and now you just look at this number, right? And so, I mean, in order for it to be an admissible candidate, the mass should be greater or equal than zero, because I mean, you cannot have a negative delta, okay? And that determines this critical, right? So, the mass equal to zero. So the mass equal to zero means m equal to md. md was this dimensional quotient that I had here. This is where the cutoff caps. Okay, and now you have to start working. I mean, this gives you the idea why it works. What you use next is sort of a hidden convexity. So what I have done now is I found a guy that solves the Euler-Lagrange equation that I hope should be my optimizer. But you know, Be my optimizer, but you know, it could be that this is just one guy, and the true minimizer is something else. But there is a certain hidden convexity in this problem that tells me, and this is again where I need power for, that tells me that if I have a candidate, then this candidate necessarily is the optimizer. Okay, so if the mass is greater or equal than zero, then this is a candidate that's allowed, and then using convexity, I can say. And then, using convexity, I can say that if I have a candidate, then this needs to be the minimizing. Okay, and now you have to play the other direction, which is a lot harder. And, or well, depends. And then you need to do the singularity analysis and say that, I mean, now you have to argue with a general minimizer and you have to think what you can say about those. And it turns out you want to show that. And it turns out you want to show that the general minimizer actually is of this form. But I think that's something you better look up in the original paper. I think I'll summarize at this point. We've looked at this minimization problem, which looks very innocent and then shows this mass concentration phenomenon, which we partially understand. So there are good regimes that we completely understand. And in here, in the bad region, some parts we understand and we hope. Parts we understand, and we have numerical evidence that there should be an open set where there should be mass concentration. And that's an open problem that I would be happy to discuss with you. And the other open problem is, of course, does this have anything to do with the long-time analysis, for instance, of the corresponding aggregation diffusion function? All right, that's what I wanted to say. Thank you. Any questions? Yes. Just a comment. You said that you have this delta and singularity at the same time. Then it's surprising. Well, actually, there are other situations, like, for example, in this composite-Einstein condensate model, where you have essentially all the mass concentration of the origin, then you have the singularity for forming, and then The singularity for forming, and then the mass gets sucked up by this delta continuously. This is what I would expect also in the dynamic model to happen. Okay. I'm not sure it's not so surprising. Yeah. Okay, it depends. I mean, it's so I think what you're referring to is a model with an external potential, I think, which is sort of different. I mean, here it's a system itself that decides to form a delta. Decides to form a delta. You don't put an external potential to make it do something for you, but you let it do itself. So I think, I mean, that's one comment. But on the other hand, expanding on that, right, I mean, in mathematical physics, there's this phenomenon, of course, the Einstein condensation. Many, many particle bosonic systems. And there you also have an effect of dimensions one and two being different from three and higher. And we don't know whether there's. And we don't know whether there is any analogy. And I mean, this phenomenon is not really, and there is also an interacting phenomenon. But there, the phenomenon is not understood. And it would be nice if there would be a connection. And so, yeah, it's a very natural idea. So, I still understand if in the case there is no oscillation, you can write down the optimizer. No. No. No. Well, yes, okay, yes. I mean, it is of that form, but I don't know what B is and what C is, and I don't know whether B and C are unique and so on. But I know, I mean, just from the equation, it is of that form. Yes. And so sort of the numerics we do for even integer power are essentially you make such a many terms, many coefficients to be 2j, and you numerically solve for all. And you numerically solve for all these agents. But here, the point where we really did something analytical was that you only have one exponent left, and then by continuity, this has to cut exactly at a certain point. So, my question is: this exponent affects how it interaction, how it behaves at zero and infinity. Does infinity actually play any role, or is it really k at zero? Any role, or is it really k at zero that determines the concentration? That's because if it's only k at zero, there is a comparison that one could make. Right, right. So our proofs rather look like it's the origin that matters. But I think we don't even can answer that question. But it has some algebra which you where you really Some algebra where you would really exactly that we use, but that should not, I mean, that should not have anything to do with the real problem. There should be a deeper reason for the, I mean, there should be a curve, I mean, the curve that we see numerically, and there should be an analytic expression for that curve that comes from some properties of the potential. And they should be insensitive to everything of the potential, but just depend on some very particular features. Exactly. Yes. Exactly. That that's the big problem behind the project, but you have a power law as the expansion also at infinity or only at the end. Yes, yes, we do. I mean, in the case where you have explicit formula, right? I mean, you can always write. So, what I wrote there, this is a formula for the, I think I showed you this one. This is for how this guy looks. This is for how this guy looks at the origin, but we have a similar expansion at infinity, yes. And again, so this is, for instance, something we use in the numerics in the non-integer case. There, we fit the asymptotics at infinity and at the origin and sort of do something in between, not touching infinity with the origins. Yes, but that's correct. I mean, if you take just the equation without the potential under those assumptions on the data, you mean the time dependent? No. No, no. This one? Or sorry? I mean, if you take an initial data, you test with a very simple resolution. With a very simple resolution, you don't produce a bounded solution also without the potential. And when m goes close to zero, you have a singularity formation, right? I mean, also called the standard mass diffusion equation, where the delta stays a delta. So whatever thing approximates the delta then stays a delta and then collapses, no, because they extinguish because the fusion creates on the whole space. So I think. So, I think here the fact of the potential is just that these solutions somehow leave for all times? Or is this connection with the fast diffusion? No, I think the connection with the fast diffusion equation should be that those are the states you converge to asymptotic in some sense. That would be the idea that you start with whatever initial. That you start with whatever initial data, nice initial datum you start with, and then as time goes to infinity, you approach-I mean, either infinite time or finite time-I don't know. You approach this guy, and therefore, as time goes on, you more and more form, I mean, either such a power law singularity and/or a delta on top of it. Yeah, that would be yes, yes, because also the pure positions are there. There are some parts of V La R C Yeah, and they say that this is called very singular solution and is an attractor in for the plug value. That is the variant that we definite masses. Yes, exactly. Yes, yes, yes, yes, yes. And that is an attractor of a large class of data which I've seen in a problem. That which I've seen in a problem. Yes. I don't know, but there are certain. So, my point is the potential is responsible for the formation of L M delta zero. Yes. G is the mass somehow. Yes. Yes. Okay. Yes. No, no, right. I mean, yeah, this guy just comes out from the algebra, but there is a delta zero on top of it. A zero on top of it, but I think because it's the information of Delta, I think it's quite normal already for them. Okay, yes, if you take non-sufficiently data which are not in a sufficiently high LP space, then that can also for smooth data. If they can one data can converge to that, but M is very close to zero. But then the other delta, I don't see it coming, and maybe. Come again, and maybe these two effects are exactly one the potential, one the first decision equation with the black initial data set. Okay, I think I don't know. It is a reasonable or always a question. If nothing time to stop, 