I needed you 45 minutes, but you can stop me at any point, please. Second disclaimer: I know expert neural networks, okay? So I know there's experts in the audience. I was a bit nervous about that. And I know there's people who probably have never seen a neural network before or have no reason to have seen one. And I'm somewhere in the middle. I'm a mathematician. I know the mathematical part. I know nothing about the implementation, about the computer science, about this stuff. So maybe if you ask the wrong question to me. Maybe, if you ask the wrong question to me, the wrong in the sense that a numerical question to me, I might not be able to answer that. Okay, so the fact that there's a wide variety of audience in this case means that I will start from the very beginning. It might be boring for some of you, but then I will try to get to the end, which means I will go quite fast, I would say. But I start from zero. I start from zero means I start defining what a neural network is. A neural network, you can probably get. You can probably get different ways to define it. To mean neural networks are going to be basically a family of functions indexed by a set of parameters that you can graphically represent them in a way that vaguely resembles, if you put enough imagination, a biological neural network. But basically, it's a family of functions with some parameters that I'm going to denote. So it's functions that go from Rd to R. They're going to have an input in Rd that I'm going to denote. They're going to have an input in Rd that I'm going to denote always X, they're going to have an output that I denote Y, and in this case, I start with something called two-layer neural networks, fully connected neural networks, or one hidden layer neural networks. And this means that these functions are constructed as follows. You take one input in Rb, and you multiply this input m times by m different vectors, which are your the first set of parameters that I denote u, u1, u2, um, and you're going to get m values. m is what I call the Values, m is what I call the width of the neural network, and these m values correspond to what are called m neurons. Then, each of these m values, I apply a nonlinearity to it. Typically, nonlinearities are like the sigma function or the positive value or the positive part. And then I'm going to get n new different values, each of which I multiply by another set of parameters that I don't agree, which is like scar multiplying by a vector v. And I get the output, right? So basically, my The output, right? So basically, my neural network is going to be some input x. I apply a linear function to it of size m times d. I apply a non-linearity and I apply another linear function and I get an output. Simple enough in this case. No? Why do I care about neural networks? So there's two reasons why people care about neural networks. First reason is that if n is large enough, they basically represent any function. But you can say that about many families of functions. You can say that about polynomials. Functions of functions. You can say that about polynomials, you can say that about Rigi bases. This is not an exclusive property. The second reason people care about them is because neural networks are very easy to evaluate and they are very computationally efficient to update the parameters based on according to gradient-based algorithms. So, what do I want to do with a neural network? I want to use a neural network to perform a supervised learning task in which what I'm going to try to do is a neural network. In which, what I'm going to try to do is, I'm going to try to get the best parameters to approximate a given function. In this case, it's not going to be a function, it's going to be a measure because, in real life, you're not given functions, you're typically given measures because you're given like data points. You're given for any x, you are given an output, so it's like a delta. So, you are given a measure, which is for me, it's going to be something I don't care about. It's something given by God, it's a measure. But if you want, instead of being you are given a measure, you can also think that you are given some function. For me, it's easier to think of one. Some function for me, it's easier to think of y as just some h of x that I'm trying to learn, and then rho is just a measure, it's the Labec measure in x only. And I'm trying to find the best set of parameters to approximate that measure or that function. What does it mean to best approximate in this case? I'm taking the quadratic loss. I'm taking the quadratic loss for simplicity. Most of what I say, you can put other loss functions here. I'm taking the alternum because it's the simplest one to explain. So, more precisely on the two-layer neural network, as I said, there's m set of parameters. You see that they can be thought in pairs. It's like UV, U in R D, V in R. And each of these pairs I typically take together. I call UI VI. There's M of them. I call them CI, which is an element of Rd plus one. And I can express my output, so my neural network, that sometimes I will call the predictor, as the sum of M. As the sum of m elements, m elements depending on the parameters. So each element is one of the XCIs. And in these parameters, I get that my neural network, which is dysfunctional on the parameters that I want to optimize, the parameters wouldn't be, is a heavily non-convex function of the parameters. Which means that a priori, even if I expect it to have a global minimizer, I don't have any reason to not have many local minimizers. And actually, I have them. I have many local minimizers. Have them. I have many local minimizers, which means that if my original task was to optimize the values of U and B, typically optimizing values of U and B, it's very easy to check for a set of values if they are a local minimizer. It's not so easy to check if they are a global minimizer, right? So the fact that the function is very non-complex, it's quite annoying. Still, people play with neural networks, right? And how do they play with it? Basically, they cross-sector. They cross their fingers, or up until now, they were crossing the fingers and saying, Let me hope that I'm not falling in the wrong place. How do you do that? Well, you basically have your parameters, you initialize them randomly, and you initialize enough of them randomly, and you just cross your fingers and perform a gradient descent. You perform a gradient descent, it means that your parameters are moving according to the opposite direction of the gradient of the function with respect to the parameters. You just move them, you decrease the energy, and you Them, you decrease the energy, and you hope that you don't get stuff. And the surprising thing is, it works. It works, and for some years, people were just playing with it and didn't really know why it works. But now there's a couple of mathematical observations that allow us to give reasons on why it works. It's two remarks, and then it's a deep mathematical theory behind that tell us: okay, let's see that this process, if we are, let's say, we don't need to be. Let's say we don't need to be so lucky, we just need to have enough neurons, which means enough parameters, and we need to start the entries. Like, we cannot be unlucky with the start of the entry. So, if we initialize them randomly, typically you're not going to be unlucky. No? So, what we can observe is what happens. Can we get a model when n goes to infinity that is well behaved? That is, can I let the number of neurons 2 go to infinity? The number of neurons to go to infinity, this is called the infinite width limit. And if so, can I say something interesting for that model that then I can translate to the previous model to say that we were not really that lucky and we were crossing fingers, but it was something natural that things were working. So, this is performing the infinite width limit. How do we take the limit m to infinity? So, you see, I don't know if I have oh yes, so you see we have the activation function, which We have the activation function, which sorry, we have the output, which is the sum of m elements. I want to take the limit when m goes to infinity. I cannot just take the limit. I'm initializing things. I'm putting things of size one. I cannot just take one limit. If I take just one limit of the output, I'm going to blow up. So I have to re-normalize things. If I want to see an object in the limit, I better have the right scaling. Otherwise, I'm not going to see anything. So what I do is I start by defining, instead of these weights, I change the scale and I say that, okay, the weights ui are going to have size. That's okay, the weights ui are going to have size one probably. The scale of the weights vi is going to be size one over m, which is this. This is the right scaling in such a way that I have a new pair of weights that I know u bar, v bar, so u v, the parameters is what I call weights that have size one. And now these are the size of these elements are independent of m, so it makes sense for me to try to let m to infinity. Yeah, there's an extra point. So you have your output, which is now going to be the same as before, just divided by m because I'm changing the sizes of the weights, which now have size one. And I want to let m to infinity. First observation is that at initial time, if I'm initializing things randomly, like with central Gaussian save, the first output I'm going to get is the function zero. But then it's going to move. And this is the, that was one remark. The second remark is this, which is a very interesting remark, which is: I want to be able to take the limit, I want a clever idea to take the limit. So, what I do is I represent the neural network with one measure. So, you said, if you remember, I said that H is a sum of m things. What I'm going to do is I'm going to represent the neural network as a measure in Rt plus one of M deltas centered at the weights, which are in position one plus. Which are in position one. Okay, so I take mu, I take the sum of m delta centered at the m weights, and I divide by m because I want something of size one, so I want a probability measure, I divide by m. And now this mu represents the previous neural network. Given a neural network, you have one mu. Okay? Let me just make a silly remark. You see, in a neural network, if you swap two neurons, if you swap two pairs of parameters, the output you get, the function you get, is the same. In a measure like that, if you swap two deltas, the measure is going to be. Measure like that, if you swap two deltas, the measure is going to be the same. So, in a way, measures preserve the structure of neural networks. So, it kind of makes sense. And then you can express the output in terms of the measure. It's just an integral. It was a semi-fortux, so now it's just an integral against smooth function given by the activation function and the variables, which means that you can express the previous functional, which was a functional on the parameters or the weights, as a functional on the measure, which is just an integral of a smooth function against the measure minus y quadratic part. Now you can. Minus y quadratic part. Now you can expand this and you actually get a functional that only depends on the measure. So given a neural network represented by a measure, you have a functional that depends only on the measure. That is like a quadratic functional in the measure plus a linear term. And this quadratic functional is for some kernel K that is symmetric, positive, definite, it's nice. You can assume it's smooth too. So we have reduced to functionals that we like, at least that our communities like. And the most important thing of all these analysis, well, there's just one observation is that. There's just one observation: is that the gradient descent I was telling you about before on the parameters can also be expressed in terms of a measure, it actually corresponds to the vast time gradient descent of the measure with respect to this function. So now everything I've reduced, all the studies of the neural network, which was a discrete study, I've reduced to studying a problem with this measure that so far I'm assuming that it's an atomic measure. But now an atomic measure can approximate any arbitrary measure if m is large enough. So I just, I can take the limit. I can take the limit m to infinity. I can take the limit m to infinity and get a well-posed problem that I can study, or for which I might be able to say some things. And this is what people do. So I'm saying everything very fast. This took some years and there's a very interesting idea. This is what people did. And then there's some natural questions. What can we say about the limit? First question: Is the limiting problem a gradient descent? When m to infinity, we want to understand when there's many neurons. Can we say that we are still minimizing an energy? Well, we just saw it. Still minimizing an energy. Well, we just saw it. Yes, it's a great descent. It's actually the vastest time gradient descent of the function. Now, if I let the time step go to zero, can I get a problem that we are even more comfortable studying, which is a continuous time problem? Well, yes, it's the mean field PD associated with the functional, which is this PD here, where df d mu is like the first variation of the functional. So it's just the integral of k against mu. Then there's a linear term that I'm not putting here, but basically there's an equation that we can play with. This equation An equation that we can play with. This equation is very similar to things that mathematicians play with. And does it converge in time? Like, that's probably the most interesting question, right? Because we did everything to be able to say that things were not just slack, things were based on a model, on a limited model. And the answer is yes, with quotation marks. It converges to global minimizers because we are initializing the neural network with enough information. We are initializing randomly, we are seeing enough information, so we are not. Are seeing enough information, so we are not, we are forcing luck to not play a role. And yes, in general, these measures converge to global minimizers of the functionals, which means that the previous model was not based on luck. But still, there are some interesting questions on this equation that are open, basically related to conversions rates. We still don't know global conversions rates or local conversions rates. As far as I know, the machine learning community had some. Community had some interesting results on conversion rates based on the structure of the data, but there's no general approach to that. And this is, it's more risky for me to say that, but I think until now, still it's not known noisy conversion rates, which means when you add an entropy to the functional or when you add a laplacian to the PD, which is more surprising because you would think that adding a laplacian should be trivial. Tell me. So, it was also really important. Now, you find a new that really matters here. What does it tell you? So, now if I need to actually implement my neural network and I need to choose my parameters, I have this mute. Even if it samples n parameters from it, it doesn't necessarily mean that that's possible for the underlying scripts. Yeah, there's a result that says that the minimizer to the under to the script setting is very close to the minimizer to this one. Setting is very close to the minimizer to this one. And when it was completed, you get closer and closer. So typically, you have results of that type. But again, this is an implementation question, so I might be wrong. But yes, there's results that allow you to pass the link. So as I said, maybe this is more risky for me to say, but adding a plasma, as far as I know, up until now, it's still not known conversions rates in that. And this is more surprising. So let me complicate the problem. So, let me complicate the problem a bit more. How do I complicate it? I go to deeply neural networks. What's a deep neural network? It's basically anything that has one more layer, at least one more layer. And the thing is the same. You add one more layer means that you have the same procedure as before. Just adding one more layer means, well, I add M more neurons. For me, layers are going to have the same size. So M more neurons. I'm going to have a set of M new things. I have the same U and V. New things. I have the same u and v from before, but now I have to relate the two layers, the two intermediate layers, the two hidden layers, and I relate again them by linear functions. I apply again a non-linearity here, but I'm going to get a new set of parameters that I call w, which is something of size m times n. So it's a very, very big matrix of size m times n, and my output function is basically the same as before. It's linear, non-linear, linear, non-linear, linear. Just one more time. And I can ask the same question: is there any Is there any reason for me to apply brilliant design of the parameters to converge to something? Or alternatively, is there any way for me to let m go to infinity and get some object in the limit? And this is basically one of the big questions in the year, you know? It's been studied a lot during these very recent years, and it's not clear. There's not a definite answer to this question, and I'm going to try to explain why. Basically, the previous approach on the vast steady metric doesn't correctly translate to more layers. Doesn't correctly translate for two reasons. One reason, maybe there are more, but two reasons. One reason is the fact that before measures were very well capturing the invariances of the neural network. Now, the invariance of the neural network are a bit more subtle. If you swap two neurons, you're going to swap like two sets of parameters first, but then you're going to swap like two rows or two columns of the matrix up value. Two columns of the matrix value, and you're not going to touch the last ones. But if you saw two neurons on the other layer, you're going to do the same, but forgetting about the first one. So the invariances from before are no longer there, and measures don't correctly capture these invariances. Maybe an object that would capture these invariants better would be something like a measure indexed by measures, but even that would meet additional restrictions to get an actual understanding of what's happening. The second reason is that the scaling is not obvious. It's not obvious. So, and it's not obvious. I'm being very generous here. It's not obvious. The community has tried different scalings, and different scalings work in different situations. And as far as I know, there's only one scaling that always works, which is this one. And the fact that the scaling is not obvious, what I mean is, you see, before we had M parameters, now we have like M parameters for the first layer, M parameters for the last layer, and M squared parameters for the middle layer. M-square parameters for the middle layer, which means that you don't know how to keep track of everything at the same time and get an object in the limit, which is our code. What object can we find in the limit? And basically, the scaling, I don't want to discuss other scalings, but typically, depending on the scaling you choose, you get different objects. But the typical problems that appear is that if you choose the wrong scaling, you're not going to get a non-degenerate object in the limit. Maybe how do you scale the what? How do you scale the? What size should the weights have in order to be able to take the limit m to infinity and get another generated object? Before I was telling you that vi had to be of size 1 over n and ui had to be of size 1 in order to let m to infinity and get some object in the limit that is not infinity or zero. Now the scaling is not obvious. There's different ones and the right scaling should be something that passes to the limit. You get an object in the limit. Such object in the limit still is The limit still is a three-layer neural network. You're not killing one of the layers by doing that. And such object in the limit is learning, which means I can do a dynamical evolution of the object and it's not stuck at initialization. Okay? And the only scaling data I know that preserves all these properties for any number of layers is this one. This is called the maximal parameterization. Here I'm citing Yang and Hu because they found the right learning rates, this parameterization setting. Learning rates that this parameterization satisfies all these properties. If you don't understand that, you can ignore it. But basically, the proper scaling is on u and v, which were the same as before, the scaling is the same. You need to take u. So now here, u and w v are the weights from before. u0, w0, and v0 are the initialization of the weights. And then there's how the weights move. What's the size of the increment? Before, the size of the initialization and the movement were the same. Initialization and the movement were the same. I was telling you, you initialize u and v are going to be objects of size one. So, here all the both objects, I don't know if it's clear, have size one. U and v are going to be like size one and size one over m right now. Another difference is the matrix value. The clever thing is that you initialize it with a size one over square root of m. Why do you put y one over square root of m? Here is because if you initialize with random variables one over square root of n, is the scaling of the central. Is the scaling of a central limit theorem, and it's the same that it's going to give you a random variable here. But you make sure that when you move, you're going to stop having central things, centered variables. You make sure that when you move, you move by things of size one over m. So things are moving slower than they are initialized, which means that when I take the limit m to infinity, the scale of the initialization kills the movement, but The movement, but still this passes to the limit. This passes to the limit, and you get an object in the limit that evolves. As you saw, like I told you, you need to apply the central limit theorem here, you're going to get some random matrix interaction. And these people have been studying for some years. It's not surprising that you get these interacting random matrices. And I should mention that everything I said, the fact that things pass to the limit, is part of a very interesting set of papers called Tensor Program Now. Set of papers called Tensor Program Now by Greg Young, in which he proves that there's an object in the limit. But that's where things stop. There's an object in the limit that we can more or less understand up to the point of its existence, but we cannot play with it. We know that there's a set of algorithms that exist in the limit. We cannot play with them, which means that we cannot take information from them. And the actual abstract algorithm that appears in the limit. Algorithm that appears in the limit is more complicated than before taking the limit, let's say. Okay, which means that the natural questions from before, which is, is the infinite with limit this object, is it the gradient descent of something? Is there a continuous time limit? And does it converge in time? We don't know the answer because we cannot play with it. We don't have an object. And I think the community is not very hopeful about the fact that we can have something with enough structure to play with it. And that's where we entered again. And that's where we entered the game. So we entered the game. Our goal was to provide some non-trivial examples of limiting objects that we can find and we can study and we can use in some setting. And what we did was to consider deep linear neural networks. Linear means basically that my activation function, which was a non-linearity before, now it's just going to be a linear function. I'm putting the identity here. This means that your output, your predictor, is just a linear function of x. Of x, and now you could tell me, okay, but now there's no secret anymore, right? Linear functions of x, and minimizing the altinorm. I can give you explicitly what's the minimizer of the altinorm of linear functions, and I can provide you linear dynamics that converge to it exponentially. And you could be right. So the problem is well understood, except that I'm doing that from a neural network perspective, which means that I'm trying to minimize this problem in a non-linear way, and I want to retain the structure. And I want to retain the structure. So I want to get an object, I want to get something that I can play with that retains the full structure of a neural network, which means that retains the non-linearity of the dynamics along the flow. So what do I do? I take the initialization from before, I get this predictor, and I move. And I move means I perform a stochastic rating descent of the atrix, and what I do is I train V, I train the increments of U, I consider W0 to be an initial atom. Value zero to be an initial atom, which is a random variable, and I train u, which are initialized randomly, and I hope for the best. I try to see if I get an object when n goes to infinity and if I can play with it. And that's our result. Our result is basically, yes, you do get something in this case. And you do get something non-trivial. And you do get something that you can touch, that you can touch and you can get results from. And what we prove is that when n goes to infinity, your weights, the evolution. Your weights, the evolution of the dynamics of your weights, which are random things, converge to the dynamics of some weights that we can explicitly express, which will still be random in the limit, but whose laws we can fully characterize. And we know the full characterization of these laws. And your predictor, which your predictor was VWU, which is going to be a function of t, a random function of t, is actually converging to a deterministic function of t. To a deterministic function of t, that whose evolution we also can explicitly characterize, and it's converging to what we expect to converge. So, since I don't have a lot of time, let me just say that the description of the model, now I would spend two or three slides trying to explain what's the explicit description. It's a non-trivial description, but it's an explicit description. And it's explicit enough for us to say something like, For us to say something like that lambda t, which is the predictor, which we proved that was a deterministic thing in the limit, now we can actually prove that it's convergent at an exponential rate to what we expect, to the mean L2 minimizer of the functional, which is something that we didn't even do with shallow neural networks. So we get something in the limit that is still non-linear. It's an evolution that's still non-linear, and we can prove a conversion straight for that. Not only that, we can also prove a selection principle, which means that lambda t never leaves the span of the data. Never leaves the span of the data, which is also surprising. Like, this is trivial if you just put a linear dynamic, but in a non-linear setting, this is completely not trivial and very particular of the explicit expression we get. So, the questions from before in this setting, we can answer them. I didn't really answer all of them, but we can answer them. Is the infinite with limit agree on this time? It is, I tell you, I didn't explain. Is there a well-post-continuous time limit? There is. It's an infinite system of ODs. It's explicit. And does it converge in time? It and does it conversion time? It does conversion time for global minimizer at an exponential rate. And let me just finish by saying that we did that, but we did that also with stochastic data in the same way that with deeper linear neural networks. It's just exponentially more difficult to present the problem when you add more layers, but everything works for more layers too. So I think this is everything. Thank you very much. So, okay. So, can you say something about the limiting object that you get? You said that you can characterize it as we show us, but yes, this is what I skipped. So, which one? Do you want? Do you want what we can characterize? So, the limiting object is going to be weights that evolve in time. We're going to have some weights that have some dynamics. And these weights are going to be, we express them in a basis of Gaussian variables, of IID Gaussian variables. Okay? So, you take any basis of IID Gaussian variables that are going to be a basis for UWB. And what we characterize are the coefficients. Okay? And then these provide the full law of the full evolution. These coefficients of the basis of Gash. These coefficients of the basis of Gaussian variables are going to be themselves the solution to an infinite with limit neural network with deterministic coefficients initialized with this. So you initialize the U matrix, which is now A, you initialize identity zero. B, you initialize, which is the middle matrix, you initialize with all zeros, but you at a lambda, which is this explicit matrix. I know I'm going very fast. And C, you initialize with one, zero, zero. With one zero zero. This is an infinite with limit neural network deterministic with explicit coefficients that gives rise to an infinite system of ODDs that now represent the coefficients of a basis of Gaussian IIT variables. And then you can express your infinite, so u infinity, infinity, w infinity, which is the weights in the limit, as these coefficients times the basis that I denoted j k. I know I went very far. I know I went very fast. I see why you skipped this. And then you can actually express the output. The output is just going to be AT lambda plus B C, which now is completely deterministic and explicit. And this is your lambda infinity t. But so you have A, B, and C, which is the solution to this infinite system of ODs. And now everything is deterministic, and your output is going to be this A, B. And your output is going to be this A B and T. That's it. You're asking for the outside random variables because that's how you prepare your initial data, right? That's a very good question. So, yes, I'm hiding that. Let's say, yes, you take a basis of Gaussian random variables. The first element in the basis can be the initial value. So, the first element, maybe it's not Gaussian, but it has to be sub-Gaussian. But the first element can be your initialization. Then, everything else are Gaussians. So, yes. Yes, I will probably ask for what you'll say. So in this first unit that you showed, uh the neutral limit was uh two-layer problem. Is it known what the values of the minimizers are, of the local minimizers? So just sort of the steady-state problem. Do you know what is the conversion rate of the local functional evaluated the local minimizers to the global minimizer? I'm sorry, see you again. So sort of the land the energy landscape of the of the steady state talking about. Of the steady state problem as a function of m. Is it known how the local minimizes? I don't know. Okay. And maybe Stepan moves. Depending on how many data points you have and how wide it is, you may not actually have strict local minima. And I think you can show that there are no high-wells local minima. But beyond that, I think it's still valuable. And the other question, on the last slide, you had a yes and quotation mark, why the quotation mark? No, this is because I copied it from before. In this case, it's yes. Yes, it converges the expansion. Thank you. So in the shower case, you've got a Bassesein gradient flow, which kind of corresponds to the continuous width, and here you've get an L2 of. And here you've got get an L2 of n radiant flow, which is kind of a countable width. Where does the difference come from? Yeah, we thought about that. We thought about that and I was very bothered by that sometime, but I cannot give you a proper answer because it's just curious. No, it is curious, but I. But I feel like it's more fake the continuous one than the countable one. So in the sort of scaling that we're doing for neural networks, so Cylian must be able to discuss some paper on this. Is this the same scaling that they can do or because I have to get a radio flow in the two like The up to like non-linear was it for deep neural networks? For like the two hidden layers? So for two kidney layers, you can actually get a scaling that you can put one over m here, instead of one over square root of m. This is called the integrable scaling. And that scaling gives something non-trivial in the limit that is closer to what happens in the passage time. So in the shallow neural network case. But that scaling. But that scaling is something that, in a way, I would say, forgive me, I would say that you are lucky that you're doing two hidden layers. If you do three hidden layers, then you get something that doesn't move anymore. But this game works in any number of layers. So maybe that's what we were doing. And the last time you mentioned that you have magic averages to the misers of the I mean, can you characterize minimizers? And what would we mean? No, so we have this function. This is lambda x. Right? This is, let me say, this is just lambda c dot x. So what I'm saying, you find the best lambda that minimizes this function. It's explicit. It's one. Yes, it's. And you converge to that one. It's a minal to minimizer. If rho is has enough layer. If rho has enough directions. Otherwise, you have one that is the one that is in the span of the data. What does it mean in terms of the neural networks? No, it just means that in linear neural networks, you know where they're converging to. Like what we're trying to do here is not find what's the limiting point. What we're trying to see is we have an object, a well-characterized object, where m goes to infinity that retains all the properties of the neural network. But we're not discovering. But we're not discovering the limiting value. The limiting value, we have many other ways to find it that are much better than this. What we try to do is we try to preserve the structure and get some object that we don't know what you put on a linear ID. I have a very simple question. When you begin your modeling, there is a modulation function simple, right? Yeah, the activation function. Yes, the activation function. Where you have to. Where you have to disappear. Yes, that's a deep linear neural network. So I said, let me study a model that is a bit simpler, which is in reality a lot simpler, which consists in sigma the identity, if you want. Yeah, yeah. But in the reality, sigma is, for instance, broken media. Yes, or sigma. Yes. Yeah. Is there any way of doing massality? Any way of doing mathematics with this linear methods with a broken? Yeah, that's the big question there, guys. Thank you very much. So, if you have more questions, let's postpone it to so we switch. We thank Shabi again. And we move to the next speaker.