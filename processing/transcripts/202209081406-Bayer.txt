With Nicolas Tapia and with Peter Fritz. And let me skip over this and let me start by mentioning kind of the opposite situation of when Josha ended basically with a paper with zero citation. I start with a reference to a paper which, according to Google Scholar, has 131,000 citations. I am a bit careful because I am a bit careful because I know that Uber Scholar is not always extremely accurate in these things, but still. And what is this paper about? It introduced a particular architecture of deep neural network, this residual neural networks, or ResNets. And I have here a slightly simplified version of ResNets, but you see the difference is, or the particularity really is here. Particularity really is here that your layers are composed from earlier layers by adding something to it. And basically, the motivation for introducing this architecture was that if you interpret this additional nonlinearity as something small, you basically have identity plus a small perturbation, and identity plus a small perturbation. An identity plus a small perturbation, if you take the derivative, you get something like one plus a small perturbation. And if you think about having large networks with many layers, that means you basically multiply lots of ones plus minus small numbers. And therefore, this should avoid the problem of either vanishing or exploding gradients. And indeed, it was incredibly successful, as you can see from the number of citations that this paper got. Citations that this paper calls. So, as a matter of fact, the formula that I've written down here is a simplification of what the standard resonate looks like in basically four ways. The first one is that there is no bias term. This is really a matter of notation, I would say, because if you add a row of ones to your layer, to your nodes, A layer, to your nodes, YI, then you can of course get the bias back and encode it into your images. A more important simplification is that we are skipping here over every layer. We have this addition here at each layer, whereas in a general MesNet you can kind of skip over. Can kind of skip over two, three, four layers. Meaning, you would add not the YI, but maybe every four steps you add the fourth previous layer. Yeah. That is really, I mean, you will see immediately why this is very convenient for us. Finally, we assume that I may. We assume that the dimension is constant, so the number of nodes in every layer is the same. And that is also pretty important to us. The third one that we use fully connected layers is not really important to us. And it's not really even an assumption that we really make. Now, you don't see any details here, but on the left now I have a picture of a Wesnet. A picture of a resonate, actually, a pretty shallow resonate with I think 32 layers. But how this is really look like, and maybe you see that in this case, just skipping over every second layer. And also, you probably get a hint of the colors, and colors here are different like architecture of the layers. You have fully connected layers, you have convolution layers, and so on, and different dimensions. So, that would be Dimensions. So that would be a more realistic scenario. Christian, what's the significance of the left and the right track there? The left track would be without the skip connections. So that is just, I mean, from, maybe even from that original paper illustration. And why is this an architecture that has been very attractive to me? Attractive to mathematicians trying to analyze deep learning because you can immediately see the structure of a differential equation hidden there. So more precisely, let's make this intuition more explicit and say I have here a small scalar parameter, let's call it delta t, in front of my nonlinearity. Now I basically interpret the Interpret the index of the layer as something like a running time. And I pass delta t to zero, which means the number of layers to infinity. And formally speaking, of course, if things are regular enough, I end up with a solution of a controlled ordinary differential equation like this. Okay? So, in other words, you can understand. So in other words, you can understand this as an Euler discretization of such a limiting object. And there we are basically in a very, very classical field of mathematics, namely controlled ordinary differential equations. I mean I understand in this room people would basically understand by controlled ordinary differential equation something slightly different and we will come to this in a second. In a second, namely something where the theater is rather outside, but still. And there have been quite a few papers on this point of view of understanding residual neural networks. In particular, there's works by Hart Butotto, who basically design new architectures for residual networks based on well-cosiness and stability properties of the underlying control. Properties of the underlying control ODE. And the main issue, however, is for this motivation to work, you need quite a lot of regularity of the control theta. And, you know, there is this kind of gap, I believe, when mathematicians usually try to analyze deep neural networks. Neural networks, they usually don't analyze the deep neural networks that are really used, they analyze convenient modifications, and that would be an example. Because when you look at how residual neural networks actually which features actual residual neural networks have, it turns out that regularity of the weights is not really something you observe. Really, something you observe. So, what we have here is we look at a residual neural network actually of this simplified form and we train it for some simple classification problem and we look at their weights. Actually, for reasons that will become clear later, I consider basically my weights as increments of some driving path. Driving path. And when I do this, and I just look at, in this case, I look at just one node out of this, I mean one particular coefficient in this weight matrix running over time. And you basically see this looks pretty rough. And the reason why this is rough, I guess, is because the way you usually start the The way you usually start in this deep learning is you initialize your neural network by somehow IID random numbers in independent random samples. Let's put it like this. The distribution may actually change. This is most usual. And then the training probability is only some kind of rather mild perturbation of this and doesn't necessarily have a regularizing effect. And if you start from constant, it's from zero. If you start from constant from zero weights, do you get a very good question, I've never tried this out. If you start from a white noise, you will get. Yes. This is what you see here. You basically start from a white noise and you know. But it's highly correlated then, after the trainer, right? It is correlated, yes, for sure. If it's I mean. For sure, if it's at least the roughness has not improved. Let's put it like this. That would be basically a part of why. Also, I mean. And so before, what I showed you here was a Desanet with 128 layers, and I think it becomes clearer when you look at a bit larger. When you look at a bit larger resonance, so here 512 layers. And if I told you that this orange line was a trajectory of a Brownian motion, I think you would believe me. Defining delta W to be here, delta W is uh you just train a a decidual neural network with five hundred twelve uh uh layers and this is the weights that you get. The weights that you get at the end. So there is no approximation? Is it trained by like full edge? Or is it more noisy when you do mini-batch or stochastic? It's mini-batch trained and I think there's some minor batch normalization and some of these tricks are involved, but nothing I mean, nothing unusual, nothing fancy. Nothing fancy. Sorry, you which weight did you pick here? Just this is the 01 weight. But in principle, they look similar, but some of them are... I mean, the roughness looks similar. The scale varies. So So the bottom line is if you want to analyze your residual neural network as an Euler approximation of a smooth controlled ordinary differential equation, that's probably not will not work so great when you see how this actually looks like. And to be fair, in this literature they very much are aware of this issue and therefore they Therefore, they penalize for irregular weights, so they take care of this issue in the training. And you cannot start with white noise, yeah? Well, I guess you can start with white noise. I mean, the thing is, I mean, you can start with white noise. It will just become quite regular. Just become quite regular very soon. I mean, if you to be honest, I don't remember what those guys suggest as initialization, but if you just open, I don't know, a standard book on deep learning, they roughly start with white notes. I mean, there are different different heuristics on how you initialize, but at the end of the day At the end of the day, it boils down to whiteness. And each layer has the same width in this setup? In this setup, yes. So is that your last bread material? What is the difference with this neural ODEs? Well, the difference of the neural ODEs, thanks for the question, is so in a neural ODE, you basically have an ODE where the vector fields is a neural network. Neural network. Here we understand the neural network itself as kind of an OE. So it's just something different. Even though on a if you took theta to be fixed, then that's more kind of like an Eural E. If you took theta to be independent of time, could we say that? I don't know. No, because here, I mean, the vector field here is just this, right? Yeah, but I mean, it's a neural network. Yeah, sure. Yes, you're right. Formally speaking, yes, you're right. So it's a very simple neural network if you take data to be fixed. Yes. And then that's a neural ID. Nice, you mean it's a normal normal? Okay, so we have a several layers within the vector network. But I think the at the end of the day the At the end of the day, the problem is just something different, and it tries to solve a different problem. Because here we just have a neural network and we try to understand it as the solution of some ODE in some way. Right, there is another motivation actually, there is a quite recent Actually, there is a quite recent work by Rama Kont and co-authors where they consider a very simple, simplified residual neural network. They make a bunch of assumptions and at the end of the day they show that the trained neural network, when you take the limit as the depth goes to infinity, it converges, the weights converge to The weights converge to a function which has finite two variation. I mean, the assumptions are clearly very specific. Still, I'm actually a bit surprised that you can do something. What are the restrictive assumptions? The second? So the third bullet point? The third bullet point, I think, is a very restrictive assumption. There's also There's also, I mean, you basically there's also assumptions on the learning rate, which are I mean, I think it's a pretty amazing result, to be honest. But at the end of the day, I I wouldn't say that the assumptions are I mean this the assumptions are chosen so in in order to make it work, right? In order to make it work, right? And not an actual form. And finite variation is a sense where brain motion has finite variations, right? As essentially finite two variations, yes. Okay, it's a little bit tricky because they claim that they show that they have one half held, but it's probably not true. But wait, why do you think it's not likely? Because brain motion doesn't have... Yeah, but we don't know if it's broad motion. But if you really? If you usually have some scale units faster than brain motion. For example, if you regularize just a tiny bit of use in the logarithmic. I should say that there is also another paper by Lamocomte and co-authors where they indeed obviously have a very good idea. Where they indeed obtain, well, to be honest, they obtain two scaling limits. They either obtain a bounded variation scaling limit or they obtain a Brownian motion scaling limit. But there I think the assumptions are even more specific. And in fact, it doesn't include the training. Anyway, what are we looking at actually? Well, we consider the structure that we have discussed before. That we have discussed before is residual neural network. And we basically consider this as this kind of, say, difference equation. So we have a driving path, if you like, W. We have these vector fields, which we are going to assume to be C2 essentially. And you have this difference equation. And what we are interested in, we are interested in the stability with respect to the input data zy. Okay, and for this purpose we basically go back to what Josha told us and we use kind of a part of the iterated sum signature in order to lift our driving path. Right, and as kind of hinted at before, we are going to use p-variation metrics for the path in order to study the stability. And at this point, I don't want to, I think I don't need to say too much, but one thing maybe to keep in mind is, of course, in the setting that we are with discrete time. With discrete time finite length sequences, every sequence has finite p-variation for every p. I mean, the problem is not that the p-variation is finite or infinite and solutions may or may not exist. The problem is merely how well can we estimate the stability with respect to p-variation nodes. Okay and uh this is basically the the main result of the paper. So coming from a rough path setting, this is probably not very surprising. So you get, we consider the, if you like, the Young setting and the setting of p-variation between two and three and you get in essence a discrete time version of the usual Of the usual rough path estimates for stability. And as a matter of fact, I would like to say that the point of this paper was not so much to, I mean, there were earlier papers about rough pathways chumps, where you could, in fact, obtain results like this. I mean, where you could. Where you could understand this equation as a rough differential equation ribbon by a rough part of chunks. And you would have this kind of results. But of course, this is a much more complicated setting. So here we have a direct approach which is self-contained and yeah, so I think there is some interest in this. Now Now in the short time that I have left um right I think I have uh said uh several things. Yeah, maybe one thing to to say, what is the proof is following basically the usual strategy, so we use properties of deep sum signature, we use discrete Soring lemma, we use an a priori bound based on the p-variation norm of the final sigmas, and we also Of the five and sigmas. And we also use a disclaimer-graph workable inequality of some kind, which in fact is responsible for having the exponential and which you could avoid using if you wanted to. I mean, there is really no reason why you would have an exponential here. At the end of the day, you have n integrations, and you could get away with an n power polynomial instead of the exponential. But of course, things would become even more messy. Even more messy. Okay. And again, from the point of view of why, might this be interesting from the point of view of deep learning? Well, what we have here is a Lipschitz continuity, is a bound for the Lipschitz constant of the neural network seen as a function of the input data. And this is actually a problem that has seen quite some interest in the machine learning literature. And yeah. Yeah, and if you I mean most of the papers are dealing with standard deep neural network not this residual neural network but in essence they boil down to I mean many of them boil down in using finite variation as the one variation estimate and basically the idea is if you now take again up this this Up these weights for this 512-layer residual neural network, and now I have plotted the p-variation norm, and you see the on-variation is huge. Of course, it's finite. How could it not? But it's huge, and if you go down p equals 2, you lose basically one order of magnitude, maybe a bit more. Then at p equals 2, you assume. Then at p equals to as usual there is this kind of jump when the second order guy starts being engaged. So you jump up. But actually if you look closely when you reach three the norm is actually smaller than a two. So there is some interest, some relevance here I believe the problem is the constants are not very good and And that is partly, yeah, partly certainly also has something to do that we are using a worst-case pathways analysis here. Whereas it would certainly also be interesting to have a more probabilistic average case version of this digits estimates. And I think with that, I want to thank you for your attention. Pay attention. Could you go back to the