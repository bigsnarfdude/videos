Yeah, so percolation, I think, was first considered sort of in lattice-like graphs in the context of statistical physics, sort of as a model for the flow of a liquid or gas through some sort of porous medium. But the mathematical model underlying it is very simple. We take some graph, for example, say the square grid and some probability p, and we look at a random subgraph of our graph G given by retaining each edge independently with this fixed probability. Fixed probability. And the sort of standard question we're interested in is about you know when there exists an infinite component and what sort of structure this component has. So a particularly simple model of percolation would be to look at the binomial random graph, which I hope everybody is at least vaguely familiar with, where we take our host graph here, the complete graph Kn, and again we take a random subgraph by including each edge independently. And here we've gotten rid of a lot of the geometry that existed in these other cases, these lattice graphs. And so we hope this case. Case of these lattice graphs, and so we hope this case will be a bit easier and perhaps simpler to analyze. And in this case, it doesn't obviously make sense to ask about an infinite component because for any n and p this graph is going to be finite. But still, if we look at this graph on the right scale, then we're still going to get the same sort of behavior where there's some critical period where the likely component structure of this random graph changes quite drastically, below which we're going to only have very small components and above which there's going to be some large dominating size components. Large dominating size component. And it's a very famous assort of Erdos and Reni that this behavior happens around p being equal to one over n. So this is a very informal statement of what Erdos and Rennie showed. But if we take p to be c over n c constant, then when c is strictly less than one, with high probability, our random graph is only going to have very small components of logarithmic size in n, and all of them are going to be very simple in structure. They're going to be trees or have at most one cycle. Conversely, when c is bigger than one, Conversely, when C is bigger than one, with high probability, there's going to be a unique giant component which is going to be linear in size, and its structure is going to be quite complex in some ways. We'll call this the supercritical regime. Now, as I said, this is a very coarse picture of the phase sign written here. Much, much more is known about the structure of this graph as c tends to one as a function of n, but that's not going to be really the topic of the talk today. What we're more interested in is the structure of this supercritical giant component. Now, the case. Now, in the case of random graphs, a lot is known about the structure of this component, which we'll call L1. And I'll just note down a few sort of things that are known that will be relevant later as motivation. So, for example, we know that the giant component is linear in size, and it was a very long-standing open question whether or not it actually contained linear length paths and cycles. And this was eventually answered in the positive by Itai, Komrosh and Semi-Aidi in the 80s. That the say the circumference of the giant component is linear. Giant component is linear. And a sort of similar result by Funtlackis Kunanostis more recently was that this giant component contains complete miners of size order root n. Both of these can be seen to be optimal in terms of their dependence on n. And you can maybe think of this second result as saying that the giant component is topologically quite complicated. In particular, it implies that the genus is linear in n. So if you want to embed it on a surface, you need at least linearly many handles in order to do so. So. So, sort of on the other end, Cheng and Liu showed that the likely diameter of the chi component is very small. So, whilst it has a lot of big structure in it, all the parts of it are very close together in some way. It's got a logarithmic diameter. And rather more recently, Fantakus and Reed, and also independently, Benjamini, Cosmo, and Warmold showed that, in fact, even more so, the mixing time of a random walk on this giant component is really quite quickly. It's polylogarithmic in N. And whilst all of these things were sort of proven were very different. Of these things were sort of proven with very different methods, and they'll often introduce quite interesting and new methods to the sort of field. A more modern exposition can allow you to deduce a lot of these or all of these results I've written here as sort of consequences of a single sort of theorem, which uses this very nice notion of expander graphs. So roughly, an expander graph is one in which every small set of vertices has quite a large external neighborhood, so neighbors outside of itself. And so you can think that if you look at the That if you look at the neighborhood balls growing around a vertex, they're going to grow quite quickly, exponentially. So, so formally, we say a graph is an alpha expander for some fixed constant alpha, well, for some number alpha. But for any subset of the vertices of size at most a half, we have that the size of its external neighborhood here is at least alpha times the size of w. And these expander graphs have turned out to be very sort of fundamental objects of study in a lot of different topics of comatoics and computer science. Of comatoics and computer science. And in particular, they have sort of initiated studies that's shown a lot of deep and surprising connections between many different areas of maths. So they sort of bound together geometric ideas of boundary and isopermetry with sort of linked into structural graph theory and also spectral graph theory. And from there to sort of the mixing times of random walks and randomized algorithms. And so we're interested in these things just as graph theoretical objects mainly. As in today in my talk, obviously. Um, as in today, in my talk, obviously, in general, we're not, but they have many nice properties, these things. So, let me list just a few of them. So, these expanders are always connected, but actually, a lot more than that. They're sort of very, very well connected in that all of their balance separators are linearly large. So, if I want to split my graph into two parts of roughly equal size, I need to remove linearly many vertices to achieve this. Now, it's not obvious, but with a bit of work, some quite deep work, you can show that this implies already that the you can show that this implies already that these graphs contain very large pattern cycles and very large minors, so linear length pattern cycles and square root size minors. On the other side, these graphs have logarithmic diameter. As I said, the neighborhood balls grow exponentially quickly. And using this link between the expansion and the spectral graph theory and the mixing time of random walks, you can show that the random walks on expander graphs mix very quickly. They have a logarithmic mixing time. So one can deduce all of these nice results. So, one can deduce all of these nice results about the giant components of G and P from this very nice theorem that's proved quite recently by Benjamini, Cosmo, and Hormold, and also Kuvalovich gave an independent proof that says that if we take a supercritical random graph, then there's some constants, alpha and delta, such that GMP is going to contain a linear size subgraph on delta and vertices, which is an alpha expander. And from this, you can very quickly deduce the existence of long paths and large minors. And in order to talk about the mixing time of the random walk or the diameter, you need to know a little bit more about. Or the diameter, you need to know a little bit more about how this expanding subgraph sits inside the giant component. But one of the nice features of Benjamini, Cosmo, and Warmord's proof is that they give a description of the giant component, which essentially says that it's built by taking this expanding subgraph and just sticking on quite small pieces. So these pieces are sort of logarithmic in size, and they call this a decorated expander. And from this, you can deduce all these nice things about the diameter and the mixing time. About the diameter and the mixing time, and so on. So, this is sort of what's known. Well, there's a very few things that are known out of the many, many things that are known about the random graph. And we're going to be interested in a slightly different percolation model, which is percolation on the hypercube. So I hope that everybody knows what the hypercube is already. I've written a little definition here, but it's sort of the obvious generalization of a square to a cube to higher dimensions. And so we're going to be looking at random subgraphs of the hypercube. Random subgraphs of the hypercube given in the same way by retaining each edge independently with probability p. And our hypercube here is in some sense a very high-dimensional graph. It very naturally occurs as the as a subgraph of high-dimensional grids. And in particular, as d tends to infinity, this dimension is also growing. And so there's a lot more geometry now dictating which vertices are adjacent in the graph, which is going to make our analysis of these random subgraphs a little bit more difficult. And so this model was considered very early on, sort of. Very early on, sort of soon after the Herush and Veni's sort of introduction of the random graph model, and a lot of things were known about it. And for a long time, there was a nice open question about whether or not it displayed a similar phase transition as a random graph. I think this is a question of Verdosh and Spetzer, and this was eventually answered again by Itai Komosch and Semerady in the 80s that say, yes, at around P is one over D, so one over the regularity of the hypercube, we get a similar sort of behavior. So explicitly when Sort of behavior. So explicitly, when c is less than one, with high probability, all the components will have size big O of D, and so logarithmic in the number of vertices of the graph. Whereas when C is bigger than one, with high probability, this random subgraph is going to have, again, a unique giant component whose size is linear in the total size of your graph. And all the other components will have logarithmic size. And so one thing we can now ask is, well, what is the sort of likely structure of this giant component? And this is the sort of topic of the talk. The sort of topic of the talk today. And so, obviously, we can think about analogues of the previous questions that have been considered. So, what's the length of the largest cycle in L1? How large of a complete mine does it contain? What's the diameter? And so on. I should say, in particular, this was sort of the starting point for the work was this question here. I think this natural analog of this question that Itai Komosh and Senready answered for the random graph is a very interesting question, and we haven't quite solved it. So, because the So, because the hypercube is a lot sparser than the complete graph, and whilst it's quite globally well connected, locally, it's not very connected at all, it looks a lot very tree-like, it's maybe not clear what sort of answers you should expect to happen here. And so we can give a few guesses, or at least say hopes, as analogues from the random graph case or from what the naive sort of bounds would give. So, let me just sort of describe what's the best we could hope for in these. What's the best we could hope for in these various cases? So, what's the longest cycle we could find? Well, it's definitely bounded above by the size of the entire graph. So, it can't be much, we can't do better than linear in the number of vertices here. So the best we could hope for is linear in 2 to the d. In terms of the largest complete minor, there it's maybe not as obvious, but it can be shown that the largest complete minor in the hypercube, in fact, already has roughly 2 to the d over 2. So around up to polar. Two. So around up to polynomial facts in d, the square root of the number of vertices. So again, we can't do much better in a random subgraph. And again, for the diameter and the mixing time, it's maybe not obvious because these things could decrease when taking subgraphs, but it's very unlikely that the diameter of the random subgraph is smaller than the diameter of the cube, which would be d. And similarly, it's unlikely that the random walk mixes much quicker. And so we probably need to take time at least d log d here. So these are all sorts of. D log d here. So these are all sort of perhaps best possible hopes. And the sort of answers that we get are going to be these are true at least up to logarithmic terms in terms of 2 to the d so polynomial terms in d so explicitly this sort of main theorem that we prove which is forthcoming work and still sort of being tweaked to give the best possible result is that if we look in the supercritical regime then with high probability the giant component of the percolated hypercube expands quite well. Hypercube expands quite well. So we don't get quite constant expansion, but we get expansion of a factor which doesn't lose much more than an inverse polynomial in D. Now it's not too hard to see that we can't hope for constant expansion. If we look at the hypercube itself, so hopefully this picture will make sense. So some of you think of the hypercube as being all the zeros at the bottom, all the ones at the top, and the rows here are fixed numbers of ones. Then the middle layer here, Then the middle layer here is a nice balanced separator of the hypercube, but it has size sublinear. It's 2 to the g over root d, about. And so there's a balanced separator of the hypercube, and it's really likely that the giant component is going to be split roughly in half by this separator. So we expect there to be sublinear separators in the giant component. So we probably can't do better than expansion of one over root D, and probably even a little worse than that, because we are taking a random subgraph here. Because we are taking a random subgraph here. So, the actual expansion, as I will mention later, we don't know what it should be, but it's an interesting question, but it can't be constant. And so, in particular, if we use these nice properties of expander graphs, we can get some almost optimal answers to these questions about the structure of the shining component that I was posing a second ago. So, in particular, we get a cycle whose length is almost linear in 2 to the d, a complete minor whose length is almost a square root of 2 to the d, and polynomial. To D and polynomial in D diameter and mixing times here. And in fact, with a little bit more work tweaking the sort of methods to get us the expansion, we can improve some of these polynomial constants here. So it's perhaps not too interesting, but I think d squared is getting very close to optimal, whereas d to the six sounds quite bad. And I should maybe mention that since we published the sort of first draft of this work, it's been shown by some other people by similar, but not very similar methods that Very similar methods that the strike component contains a long path of order 2 to the d over d to the 32. So here, this d squared is really quite an improvement on that, at least. So getting quite close to optimal. And in particular, these last two statements here answer some published questions of Bolobash, Wuchak, and Kayakawa, who asked about the likely diameter. Well, they asked if the diameter of a random process where you Of a random process where you add one edge to the hypercube one by one ever was super polynomial. So we get a partial answer, at least in the supercritical regime here, that it's never super polynomial. And similarly, Pete asked about the lazy random walk and whether the mixing time was polynomial in P. Okay, so that's sort of the conclusions of what we did. I'll talk a little bit about some of the methods, although we won't have time to say too much. If there's any questions, now would be a good time. Now would be a good time. And then I'll talk about some open questions, I guess. So, yeah, so our methods are all very hands-on and combinatorial. So, very much, oh, I maybe should have said, so in terms of the phase transition in the hypercube, I again gave a very coarse picture, this result of Aitai Komosh and Tamaradi. Much, much more is known there as well, and it's a much more active area of research. So, it's only very recently, I think, that the actual size and location of critical window was found there. And there's been some very Was found there, and there's been some very interesting applications and some tools that I think were developed first for percolation on sort of high-dimensional grids that have been applied to there. However, as I said, our methods are very hands-on accompanying, although we sort of work much more with the sort of ideas that I Tai Komos and Senmurady and later Bolobash, Rochak and Koa Kau were working with in order to find this expansion. So let me maybe just sketch at least why it might be reasonable to think that this expansion does happen. It won't be binding. Does happen. It won't be by any means a proof. So I'm going to use sort of two little results as black boxes, I guess. So there's a very well-known isopermetic inequality for the hypercube, which basically says that if we have a set of vertices which is not too large, so not more than half of the hypercube, then the number of edges which leave it is at least, well, at least the size of it. There's in fact a much stronger result known. So for very small sets, this is more like D times the size of the set. Like d times the size of the set, but for large sets, this is closer to the size of the set itself. And we'll also use the following sort of little lemma, which is not too hard to show. Once you know that there is a giant component, you can deduce this pretty quickly using that as a black box. But actually, the giant component is quite dense in the hypercube. So with high probability, every vertex is a distance at most two from one of the vertices in the giant component. So given this, let me sketch why the following sort of statement we should believe. Following sort of statement, we should believe it. So, suppose we have a fixed partition of the vertices of the giant component into two sets A and B, and they both have size at least t. And with high probability, there's going to be a large family, so sort of almost t, but maybe losing a polynomial and d factor, of vertex disjoint paths from A to B in our percolated hypercube. So we look at our partition into A and B of the triang component, and we can extend this to And we can extend this to a partition of the whole hypercube into two parts, A prime and B primed, where everybody in A primed is very close to A, it's within distance two, and everybody in B primed is very close to B, it's within distance two as well, because everybody is within distance two to one of the two parts of my partition of the hypercube. Now, since A and B were big, by the edge isopometric inequality, we can find a large set of paths between A and B, find at least, sorry, paths, edges between A. find at least sorry paths edges between a prime and b prime so at least t edges here they might um share n vertices and so on i haven't drawn that in the picture here and similarly for each of these edges we can extend it to a path of length five between a primed and b primed very easily but again these paths might share some vertices or edges but because the maximum degree of the cube is very small at least compared to its total size most d we can easily greedily thin this out to a dis vertex disjoint family To a vertex disjoint family of paths of length five between a and b, of size maybe t to the five, sort of asymptotic order. Then, since we're looking in our percolated hypercube, each edge is appearing with probability about one over d. And so we're going to expect that about t times d to the 10 of these survive our thinning process. Of course, there's some issues of independence here, but they can be dealt with. And so, since we expect there to be quite a few, some concentration results will tell us that with very high probability, there's going to be about this order a family of. Going to be about this order, a family of about this order of vertex adjoints paths. Now, this might seem to be great already, we've got this, but there's a problem with the order of quantifiers. And the problem is for any fixed partition, this is very likely to happen. But there's many, many, many partitions of the vertex set of the giant component. And so the way we get around this, which is a pretty, I think, common trick in random graphs, is to reduce the number of partitions that we have to consider by leveraging the fact that we already know that L1 is connected. And so the point is, once we have a spanning tree of L1, point is once we have a spanning tree of L1, there's not many partitions of that spanning tree that don't have many edges across the partition already. And so we can drastically reduce the number of partitions we have to consider for this union bound. So these ideas are basically enough to tell you that linear size sets expand quite well. And then there's a little bit of extra work with some quite nice ideas, but not too much effort to show that sets sort of this expansion happens at all scales. So that's a little sketch of some of the ideas at least. Of the ideas, at least. So, let me maybe then, if there are no questions about this, move on to some open problems that remain, maybe some avenues for future research. So, I guess maybe the obvious one is we've established that the giant component has some measure of expansion. And it'd be interesting to know what the optimal is. So, as I said, it can't be better than one over root D, but it's probably better than one over D to the six. And in fact, I think today one of our co-authors sent me an email that suggests. Today, one of our co-authors sent me an email that suggests we can get it down to d to the five, even, but that's um, you know, I don't think we'll get it down to one over root d. But perhaps this isn't the right question anyway, because as we saw in the random graph case, it wasn't really the case that the whole giant component was a very good expander, although it's quite a good expander. It was more that there was a large subgraph inside it that was a good expander, and the rest of the giant component sort of hung on it in small bits. So, perhaps a better question is, if we are only looking for a linear size expander, Are only looking for a linear size expander, how good expansion can we ask for there? And then it's not as clear what we can hope for. Again, I suspect that you would need this inverse polynomial factor of maybe one over root d, but you should be able to do much, much better than one over d to the six here. I should say, as I say, we're still sort of tweaking and optimizing our results, so please don't feel the need to solve these questions too, too quickly, but I think they are very interesting and please do solve them eventually. So similarly, our motivating sort of question. Our motivating sort of question, and one that I would still like to solve, is whether or not we have linear length in terms of 2 to the d paths and cycles in this giant component. So, sort of explicitly, is there some constant delta depending on c such that with high probability we contain a cycle of length delta 2 to the d and a related problem that is also very interesting is well whether or not as c tends to infinity this delta is going to tend to one. So, what does that mean? It means that as we get out of this sparse regime up to sort of omega one over n are we gonna have Of omega one over n, are we going to have an almost spanning cycle, so an almost a Hamilton cycle? This is the case in random graphs. And this question, the second question here, was asked explicitly by a group of researchers, Kuhn and Ostas and some of their students, who very recently determined the threshold for containing a Hamilton cycle in this percolated hypercube model. So that was a very long-standing open problem, and that happens at about P is a half. And they used the existence of these almost spanning cycles in a very key way, but they showed that they exist for constant probabilities. That they exist for constant probabilities, whereas you should expect them maybe to exist, as I say, just above this sparse regime. So, I think this second question here is very, very interesting. And then, finally, perhaps a more approachable problem is whether or not similar results hold in the site percolation model in the hypercube. So, it is known that a similar phase transition happens at P being about one over D in the site percolation model. So, I should say, so we're looking now at a random induced subgraph of the hypercube where we retain each. Hypercube, where we retain each vertex now rather than an edge independently with probability p. So people have shown, I think, maybe I don't know who was first, perhaps Bobash Wucek and Kayakawa, that above one over D, we are going to have some unique giant linear size component in this model. And the methods that they use to prove these things are very hands-on composites. So they're not using a lot of this sort of lace expansion stuff. Sort of lace expansion stuff or anything there, and so it's believable that our method should also be applicable in this case here. But there may be there may well be and probably are some surprises that pop up if you try and do things straightforwardly. So I think this is a very achievable sort of thing to look at. And if you're interested, you're welcome to ask me and I could give you some tips, whatever. Yeah, so thank you very much for listening. And if there are any questions, I'll be very happy to answer them. 