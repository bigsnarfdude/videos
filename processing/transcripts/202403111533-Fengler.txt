And today I'm going to talk about coding bounds for the noise and some application to unsource communication and if there's time also to group testing. This is joint work I've done with Alejandro and Yuri. So first I'm going to introduce the A channel and then the second part is going to be on random coding achievability balance and the third part I'm going to talk about methods for low complexity decoding. This is an extension of the work from 2022. 2022. So, what is the A channel? Like, it's a multiple access channel. Like, this is an example for three users and a block length of three. So, each of these columns corresponds to one channel use. And what happens here, like in each channel use, each user transmits a number between one and two. And what you get in the output is a set of the numbers that were transmitted. Here, 751, you get the set of 751. So, the difference between a set and an order. So, the difference between a set and an ordered sequence of numbers is that in a set you don't have the information about the order, and in addition, if there's multiple ones in one channel use, you use the information that there has been the three appeared. Those are the two things that you lose. But in general, you get numbers in, you get a set out. That's the key point of a mage. You can represent this in binary numbers if you want to. So, you can represent each number between 1 and q by a binary spring. Between 1 and q by a binary string of length q with the 1 in the position of the number. If you do that, the action of the a channel becomes a bitwise or of the input bits. This is exactly the same. So the set of transmitted numbers here were the first three, the fourth was not transmitted. So these descriptions are completely equivalent. If you like to see this as a formula, it's pretty much just this. So the output is the This, so the output is the union. This channel was introduced by Chang and Wolf in 1981, which was a long time ago, but it became quite relevant in the context of unsourced communication. So here we're going to look at the unsourced case, and for the information theoretic results, this pretty much means that we make all users have the same input distribution. And as a second point, And as a second point, we look at the case with noise, and we look only at insertion noise, which means in addition to the numbers that came from the users, there may be faulty numbers in the channel output, and each symbol between 1 and Q appears in the noise independently with a probability of delta. So let's quickly look at an application, and this is like the basic setup for the AWGM answer. For the AWGN unsourced map. So we have a common codebook for all users. This is like this big matrix A where each column is one code word. Now, in the unsourced communication system, each user picks one code word from this code book, and at the receiver, they try to recover a list of transmitted code words. So you might already see, like, this is like you put numbers in, M1, M2, M3, and you get a list of numbers out. So there is some relation. So there is some relation with the A channel, and it became like it becomes more apparent in the coded or coupled compressed sensing scheme. Where you pretty much, this is a divide and conquer approach where you use this basic scheme in each slot. So you transmit fragments of messages in each slot, and then you want to piece them back together. And this turns out to be exactly an HM problem because, like, here, in each slot, you put in numbers, you get a In each slot, you put in numbers, you get out a list of numbers, and in the end, you want to reconstruct the sequences of numbers. So, even if you have perfect, if you can recover this list perfectly, you have a noiseless A channel. But in general, there will be noise, so the decoders here might miss some of the messages or add some which were not there. This is precisely the motivation to look at the noisy A channel. So, let's say we have like So let's say we have like an inner decoder that gives us access to likelihood ratios, and we can use thresholding to get a list of numbers out. And by choosing this threshold, we can trade off the number of wrong messages we have in there or the number of missed messages. And in this work, we choose to choose this threshold low enough so that we don't have any missed messages, but we only have false alarms. Alarms. I'd like to mention that it's also possible to go the opposite way, like you choose the threshold high, so there are no false alarms but only misdetections. Like other people have done this route, we focus on insertions. So before I introduce the random coding results, I'd like to introduce some notations. So if I write x with the subscript a set, that means we take the union of all the x's that have index in the set. Index in the set and also talk about cardinalities. Cardinality pretty much means the number of distinct symbols in the set. So it's pretty much the size of the set. And we will express pretty much all quantities in terms of what we call here the coupon collector probabilities, which are like defined, like the classical coupon collector problem is you have Q coupon. Collector problem is you have Q coupons, you draw a certain number of times, and here we need the probabilities that in the end we have I out of these Q coupons when we draw L times and if we start with eta. Like the good thing about this, like you can compute all these numbers recursively in quite an efficient way. So once we express our results in terms of these numbers, we can compute them, compare, compute them. So like as a first part, Like as a first part, like this is like the asymptotic unit. So, this is like the mutual information under uniform input distribution. And pretty much here, this is the output entropy minus conditional entropy. And the output entropy, like because of symmetry, we can reduce it to the probability distribution over the general cardinalities. And these are given as a sort of a convolution between the binomial density and the distribution. Density and the distribution of the input cardinalities. And the input cardinalities can be expressed in terms of on collector probabilities. So we can quite easily compute this. You assume that the recorder knows the number of users? Yes, yes. For simplicity, yeah. Otherwise, it makes things more complicated. But in that case, you also typically know when an insertion happens, right? Not exactly, because let's. Not exactly, because let's say like you have a hundred users and your the number two queue is two hundred fifty six, like you will in general not see a hundred symbols because they may collide. Like you will, like I think the average number is something like 85 or something. So there will always be less than the number of users. What is delta here? Well, delta is the insertion probability. So each probability that the wrong symbol appears. Other questions? So then let's get to the finite block length results. And there we use, first we prove and use like a quite general like upper bound on the error probability. And we upper bound the probability that the decoder makes an error of size L so that he gets L messages wrong while K minus L messages are correct. And this is not the tightest possible bound. The tightest possible bound, but like it looks nicely, and you can fit it on one slide. That's why I prefer this one. You can make it tighter by doing more optimization into it. But also this bound shows you kind of what is going on. So if you compare this to a bound where for the sourced MAC, then what would happen is you would not have this term. So these like this log L faculty is precisely the difference between Is precisely the difference between m choose L and M to the power of L. So it tells you that you don't care about the order of the messages at the decoder, at the output of the decoder. So this gives you the error probability in terms of an error exponent. And the main thing to compute here is this E0L, which kind of looks like a classic Gallagher style error exponent. Only here we take it in a conditional We take it in a conditional channel where we condition on the values of k minus l symbols and compute the expectation over the remaining ones. Now if you want to apply this result to the HML, we pretty much need to express it in a way that we can compute it. So for therefore we call eta the cardinality of k minus l symbols, k0 the cardinality of the complete transmitted symbols and k0. Complete transmitted symbols and K, the cardinality of transmitted symbols and noise. And then we need to compute a joint distribution over all of those. So we start with the probability distribution over P eta, and then take the probability distribution of K0 given eta, and then also of K equal to K0, which is just a binomial, pretty much. If we apply that, we get this form, which, like, in addition to all the probabilities, there's some binomial. To all the probabilities, there are some binomial coefficients to account for the correct number of each type of sequences. So, this gives you a formula you can compute, and here I give you an example. Pretty much this is a block length of 20 with Q to the power of 8, 20 users, insertion probability 0.05, and like at the rate which is a little bit below the asymptotic capacity. You can see pretty much everything here, which is negative. Everything here which is negative will lead to a high, will pretty much lead to P, very high, so pretty much a user bound. Here you can see like pretty much the dominating contribution to the errors lies for error patterns between 11 and 80. For example, the chance of confusing the whole set of twenty code works with another set of twenty code works is pretty much neglig negligible. Make it should go. And this is strictly a finite log length effect. So the reason this happens, and this is not a smooth monotonic curve, lies pretty much in the log L faculty term we had before, because M contributes positively to the error exponent. And we can see this if we increase n, the influence of these terms vanish, and you get a nice smooth monotonic behavior in the like tail. In the tail here. And in this case, you would also result in a vanishing error probability. So, we can put all of this together and see how the rates per user behave as a function of the number of users if we put a limit on the final error probability of 0.05. And we can see there is a gap to the asymptotic mutual information, which gets smaller as you increase. Increase. And the other curve I plotted here is the so-called cover decoder, which I will explain in a second. And yeah, the cover decoder is an important concept in group testing. And what it does is, like it's it's a simplified decoder which goes through the code book code word by code word and checks if this code word is consistent with the channel outputs. Consistent with the channel outputs. That means that you check for each entry if it is in the output list. If that's so, then you add the code word into your output list. And if you do this, you may see these three are the only ones that fulfill these conditions, so they end up in the cover decoding properties. Thing is, this is like a sub-optimal decoder because it does not consider combinations of messages. And to give you an example, when this gives you strings. When this gives you a strictly better result, is here. Like I've added another code word here, which is 569, and it fulfills this cover property. So 5 is in here, 6 is in here, 9 is in here will appear in the cover decoder output list. But the joint decoder will be able to recognize that if you take the combinations of exactly three of these code words, then the only way you can end up with this channel output is by taking these three. Like this one together. This one together with two of the others will not be able to recreate your channel. So, oh, yeah, first of all, the cover decoder can be quite easily characterized in an almost closed form because the cover decoder adds each code word independently, and you can compute the probability that one of these code words will appear in the output list, and it has this neat little form. And it has this neat little form. And to get the distribution over the number of code words that appear in the output list, it's just a binomial distribution with the mean value of roughly m times this probability. Well, the reason this is important is because one of the things that kicked off this topic was the tree code introduced by our friends from Texas, and it was pretty much a low-computer. And it was pretty much a low-complexity solution for the cover decoder. And we've used it before also in our papers. And like there, we've shown it's optimal as Q becomes much larger than K. But this is a little bit misleading because even if Q is comparably large, as this example shows, so if we choose Q to be 2 to the power of 20 and number of users equal to 300, you might think, okay, 2 to the power of 20 is roughly. Pay two to the power of 20 is roughly slightly over a million, so 300 users. One million choices, you might think there's pretty much this should be close to the asymptotic limit, but in fact there is a difference of roughly 0.7 bits in the rates you could achieve with cover decoding and joint decoding, which may not sound too much, but if in a concatenated scheme the limiting factor is the altered code, it may actually lead to some big differences. So here, Differences. So here we use basically our machinery to analyze the inner decoder, which is like we compare an AMP decoding for the inner decoder and like an optimal theoretically optimal decoder that can be analyzed through the replica method. And we can see that these two lower curves here are the ones that use joint decoding, while the two upper ones use cover decoding. So if you can do joint decoding, you could get Can do joint decoding, you could get gains up to like 2 dB. But then the question remains: how do you do joint decoding? Because going through all possible combinations of messages has an heavily complexity of m to the power of k. So it's not obvious how to do it. But first of all, we can reduce the complexity by realizing one thing. Like the cover decoder is sub-optimal, but it doesn't do errors. So you can use the Doesn't do errors. So you can use the cover decoder to prune the list and then do joint decoding on whatever remains. You do not lose any information by doing this. And the second thing is to truly draw inspiration from group testing literature because group testing is quite a similar problem. And there is a way to find an approximate solution to the maximum likelihood, and it is to Maximum likelihood, and it is to formulate everything in this binary setting. So each number is represented by a string of length q with the one in the position of the number. And like A here is the binary representation of the coverage coding output list. It's a binary representation of the noise. And B is like the vector that shows us which messages have been transmitted and why is the binary representation of the noise. And y is the binary representation of the outputs. So if we search for the like jointly over B and Z for the solution that satisfies these constraints and minimizes the one norm of Z that this approximates the ML solution. So this is naturally like an integer linear program which is NP hard but we can relax it and ignore the integer constraint and it's no The integer constraint, and it's known that if you do this and you get an integer solution, then it's the current solution. So we can do this, and it works quite well up to a certain number. So we can see here for the settings above, like q2 to the power of 8, 20 block length 20, 5% error probability, and 5% insertion error probability. If we just use the tree code, we get this curve below, but if we But if we use this linear programming approach to prune the cover the 3D coding output list, we can gain quite a lot of advantage. Yeah, so this brings me to the summary. So we've given expressions for asymptotics and finite block length achievability for the HN with insertions. We've shown that joint coding can have a We've shown that joint decoding can have a potentially quite a big impact, and we've seen that linear programming gives a low-complexity approximation of joint decoding. The good thing about linear programming is that it doesn't use the structure of the code in any way, but this is also the downside. Linear programming scales with the size of the things you put into there. So if you want to apply this. To apply this to settings where the list you put into is exponentially large, then you need to use some of the structure in the code. And they're quite hard to even come up with any codes that are jointly decodable that do not use some form of interference cancellation. Because interference cancellation is hard to use on the A channel, because whenever you remove one entry, you might potentially. One entry, you might potentially delete some information from another user. So, yeah, if anyone knows some codes that are like jointly decodable, like multiple access codes that are jointly decodable, then let me know it pretty much. The only example I could think of was the construction by Chang and Wellen for the binary addermac, which is quite elegant. It really allows for truly joint decoding. Yeah, that concludes my talk. Thank you very much. Thank you very much.