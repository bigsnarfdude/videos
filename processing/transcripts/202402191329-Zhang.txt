to come here and give the talk and also especially to meet the new group of people because I work I originally I worked on I was just talking to Chris I worked on like 20 years of coyote dynamical system especially feeders and then so now I just I changed my my research field like before right before pandemic and then it turns out that I think this is really really amazing and I really want to know everyone here and I really wish One year, and I really wish to have more collaborations. So, just introduce a little about my university. My university is in Massachusetts, which is a very, very tiny state here. My city is called Amherst, and the city is like 90 miles from Boston to the west of Boston, and 200 miles from New York. So, that way you can imagine where it is. Imagine what it is. And my university is called University of Massachusetts, but people call it UNICEF. That's easy to remember. And so, one nice thing about my university is that it has the number one best cultural school for seven years. So it's national-wise. Sometimes we try to admit among students and we just put a commercial saying, okay, you know, kind of we're not the number one, we're not as good as MIT. As good as MIT, but in food, we're number one. So, I really hope if anyone is interested, you can wait, come visit me, and I can show you around the campus and also definitely we can taste the book. So, yeah, this is about my research. As I was saying, I really, before I really worked on chaotic dynamic system or chaotic theory, especially my expertise is in chaotic phenotypes. Chaotic failure system, and then right before the pandemic, and I realized that machine learning is a new tool to study chaotic system, and of course, complex dynamic system. And my ultimate goal is try to use machine learning tool to study calvin systems. So, I've been having some work, some recent work about study Celtic system using neural networks and also using graph neural networks. Networks. So, the topic which I'm talking about today is really related to discovering conservation loss. So, to study cut-like system to use neural network, and we have seen that there are many, many challenges. So, the first challenge is that, okay, how can you design a good, a good, very perfect, maybe very, very good neural network to learn your underlying dynamical system very. Underlying dynamical system very, very well, and that's what we have already heard several very excellent talks in the morning. And another challenge, there are many challenges, I just missed a few. Another challenge is like how do you learn physical laws, right? So this is related to my recent work with my colleague, Wei Chu, which is a young faculty, and also panels cover kids, and he's also, he works on PVEs and recently. Works on PVEs and recently he also changed its field to machine learning, so we form a nice team. So, mainly in this talk, in this work, we present a machine learning measure to discover conservation laws for physical systems. So, the thing is that we not only try to find one, we try to find all of them. But of course, you may think about what do you mean all of them? So, I will try to explain this to you. So, why can't the So, why can't the vision rows? So, for the real physical system, if you have a rotation, if your equation of motion is, your system is environment under translation, or maybe translation in time, translation in space, maybe under rotation in space. All these very nice properties are related to conservation laws can be described by conservation laws. So, I just gave you a more So, I just give you a motivation to start this. So, let's look at the harmonic alterator. And this is very, very simple. Simple example, right? It's a Hamiltonian system, has equation of motion. And then one thing, of course, we are interested is that, for example, just start with a toy model for your students. For example, you say, okay, just go back to home to design a new write a neural network and switch that neural network, and you first. Switch that the neural network, and you first of all generate some data set, and then I give you this data set, and you write a neural network, you try to learn orbit, learn orbit. It's quite easy, like easy, very easy case. But the problem here is that the orbit are close, so they are close orbit. So if you just design a very simple neural network, and it turns out that because the simulation, the learning has errors. So this is this. Arrows. So, this is this picture, this colored. So, you start from here and then you go forward and backwards. So, this is really one orbit, and you can see that no matter how well you design your basic neural network, still there are errors, as long as there are arrows and arrows grow in time, right? So, we can only force that arrows very slow. So, very small in the beginning, but as T goes really, really large, it's really it's not the close anymore. It's not the clause anymore. So now the question is like: how can we solve this basic problem? How can you design a neural network such that the orbit you find, you can see, you can learn, it's a close orbit. It has to be closed, right? So now we have to change our point of view a little bit. We don't want to just go follow this orbit. We try to learn the orbit very well. But instead, just realize, okay, there's another perspective because the hunt. Another perspective because the harmonic alternator is a harmonious system, so it has a it preserves the energy, right? So, this H is the summation of the connectivity energy and also the potential. So, this is the graph of this harmonic, this is the graph of the Hamiltonian function. And then, if you look at the vibral side, if you look at the vibral side of this Hamiltonian function, it might directly give you the orbit. So, from my perspective, So, from my perspective, that means that, okay, I don't have to learn orbit just from the data one by one, little by little itself. Instead, what if I just go had to learn a Hamiltonian function? If I can learn the Hamiltonian function really well, and then you just make the function to be a constant, and that gave you a circle, right? And that gave you an ellipse. Okay, so this is another perspective. That means that in order to learn the In order to learn the solution of that, the solution of this differential equation, and then it's enough to learn the Hamiltonian. And this is also related to one of our speakers in the morning. So how do we learn the Hamiltonian system? And that means that we have to use something called Hamiltonian neural network. Okay, so before I go that, I will explain to you what kind of conservation laws that I'm interested. Models that I'm interested in. So, the kind of composition mouse that I'm interested in are those smooth functions, smooth functions that is involved under the dynamical system. And of course, the dynamical system can be smooth or maybe discrete. Let's assume, just for now, it's smooth. And for example, in a special case, if your dynamical system is defined by the differential equation, differential equation, and then if there is a conservation law, Conservation law that means you take the gradient of the conservation law and that is perpendicular to a vector field. So, this is one of the definitions which I used in the example that I covered in this paper. Okay, so I'll start with Hamiltonian system. So, for the, so, um, so assume that you have a high-dimension dynamical system. I have a high-dimension dynamical system which is Hamiltonian. So the phase space is in 2D dimension. The phase space is in 2D dimension. And the Hamiltonian equation is given just by this ODE, right? And this is a symphonic matrix. And then the solution, we can write the solution for this equation. Okay, so in terms of the conservation laws for the Hamiltonian system, I just Hamiltonian system, I just follow the definition I gave you, right? So because Hamiltonian system, Hamiltonian system can be viewed as this ODE, this ODE. And this is the vector field. If this Hamiltonian system has a conservation law, that means that the gradient of that conservation law is equal to this vector field. So we have an equation. So in order to categorize the conservation model, we need to check. Well, I need we need to check this relation. So, keep in mind, this is one of the relations I have to use to design this neural network because I want to design a neural network, neural network that can be used to categorize all the conservation laws for Hamiltonian system. So, one of the first laws functions which I need to enforce is that I want to make sure that if I learn a conservation law, I take the gradient, it has to be propelling to this. I need to this vector field. That's one function that I will use in my logs, in my logs function. And okay, so what's the good thing about conservation also? And just for fun, I want, because I'm myself is a dynamical system person, so I want to just do a theory, which is a very famous theorem in dynamical system. It's called UW announced theorem. So it says that if you have a Hamiltonian system, That if you have a Hamiltonian system which is complete integral, complete integral, and also assume that for every energy source is compact, connected, manifold. And even though this Hamiltonian system may be very, very high dimension, but as long as it is completely integrable, there is a nautical transformation that changes this very complex dimensional system into a very simple action and angle core. Simple action and angle coordinate into this new coordinate because it's completely digital, right? So, in this new coordinate, it becomes just a shift. So, in the new coordinate, and then the equation of the motion become very, very simple. And this is really one of the motivation that I want to study the conservation laws for Hamiltonian system because, on one hand, if you're really lucky and you have a Hamiltonian system, You're really lucky, and you have a harmonic system which is completely integral. And if you can learn all the conservation laws, and that really saves your energy just like the harmonic alternatives case, right? And then you can learn the equation module very well. We don't have to solve the different equations anymore. And secondly, even if your system may not be completely integrable, but if you can identify several of the conservation marks, that will reduce the Mouse, and that will reduce the dimension, the complexity from high dimension to very low dimension, and that also makes things much easier. Okay, so just give you one concrete example. One example which we studied is called collageromolar system, and this is like a non-linear lattice. So, this system is like one-dimensional lattice. So, also we can see the So also we consider like periodic boundary conditions. So it's really one-dimensional ring and on this ring there are D particles. There are different particles. And so these particles and their positions and also their velocities and they are related by this equation that is described by Hamiltonian. So assume that I only have two particles which is the synthesis case and this two particle system already has two convergent system Already has two conservation laws. And the first one is the energy. Energy is preserved and the momentum is preserved. And in general, for higher dimension, for many particle cases, there are D particles. And then it was proven that the system has D conservation loss, functionally independent conservation loss. So this is a completely integral system. So this is one example which I So, this is one of the example which I want to include in the study. Okay, so what are we going to do? The main goal for this research is like, so we start with some data set. So, you have a dynamical system, which it may have several conservation models. But whether the number of conservation models is equal to the dimension of the position, we don't know. It may be the same, or maybe it's less. But anyway, we have all this data set. But anyway, we have all this data set. We want to cool up a neural network such that we are able to learn all the other conservation models. Once you know the conservation model, you can do a lot of things further, right? So the main goal is that how we can design this neural network. So this is motivated by two questions. The first question is that, okay, this also falls into a very general perspective. If I have a lot of data, data a lot of data, data set, observed data side on our physical system, but then but on the other hand we don't know the equation of the motion, so the question is like how do we predict more orbits and that's what we generally do. And second question is that okay if this system has some conservation laws, can we be a little bit more clever? We try to learn the conservation law just like the harmonic alterator case, that example. I don't have to really learn the album itself, but I want to learn the conservation law. Itself, but I want to learn the conservation laws once I have learned all of them, and that gives us a shortcut to learn the object. Okay, so the main goal is that we are going to learn all of the conservation laws. For example, if the position space is D-dimensional and there are number of conservation laws, and the question is that we try to learn all of them, and how do I? them and how do I and again this is related to the question which I was first asking how do you know they are they are different kinds of internals right so let me give you a kind of example again this example so um so for this CM system assume that you have two particles two particles of course it has two apparently clear conservation one one is the energy the other is the momentum but on the other Is the momentum. But on the other hand, when you try to use machine learning to work this conservation mouse, something really funny happens. Because if you take any linear combination of these two equations, they're still involved under the system. So how do you make distinguish about these two and that one? And also even worse, not only the lining combinations, but also I mean power, if you take the power, right? So that's even worse. And that means that, okay. And that means that okay, after you design a neural network, how many of them you learn? Maybe you can, even though this is equal to two case, even though there are two essential conglomerations of maybe you learn ten of them, right? It's right over 50 or something. So, okay, so the main goal for us is that we try to learn two of them, only two of them. Well, the question is that how can you make distinction between this and that, and so on. So that is a really difficult question for us. So it took us a while to figure it out. And in fact, this question was first considered by another group of researchers. So Liu Ji Chuning, Liu Varin Mandal, and Max Tegmer. I think you might have heard about their names. So they are a group of researchers in MIT. Group of researchers in MIT, and they are really physicists, and they know much, much better than us about conservation laws, and so on. So, they showed several papers, and this is one of them. So, they try to learn the conservation laws from different equations. So, the main idea is the following for them. So, they assume that you have this, the dynamical system is driven by this equation, for example, the Hamiltonian equation, right? The equation of motion driven by the Hamiltonian equation. The equation of motion driven by the Hamiltonian, and then a field that actually is known. And they try to learn some conservation amounts, such that deciding these two equations. So where is the first equation come from? And I already told you, right? So you want to show, so if this is a conservation law, and that means the gradient has to be perpendicular to the vector field. So this is one of the first requirements to make sure that the computer you learn is really invariant. It's small function. Really invariant, a small function is invariant under the flow. Secondly, just to solve that question, like I mentioned above, you don't want to find too many of them. So they assume that if they find two, and they assume that the gradient of these two conservation laws are perpendicular to each other. Is it guaranteed that every par if you have two conservation laws, at every point in space there exists like you can always find the more thoughtful level? Always find a more thoughtful? Yeah, that's a very good question. So, this is the method they propose in their use in their paper. And then, when we were trying to also figure out ourselves, and we also found it's not correct, right? Again, let's go to this example. And you can see that, so this is one colour, and this is one, there's another one, and you take the gradient, they are not perfect equal. Yeah, sorry, Crystal, and you're clear on that one. They were really quick. Thanks. Yeah, so this is like. Yeah, so this is like what a very, this is also a problem for us, and then we were thinking, how do I, okay, so it turns out that this measure doesn't, it doesn't, it's not like a strict, it doesn't work like very well. In some cases, in some special case, it works, but not always. So how can we replace this condition by some other conditions such that we can really find something like linearly independent, but we don't really know one right strong conditions. Conditions. So we were like digging into some physical paper and then realized: okay, oh, there's something called polysome bracket, which you can use. So for polython bracket, this is just the definition. And it turns out for us, we don't need this fancy definition because we're talking about the Hamiltonian system. So for Hamiltonian system, and also here we can extend our Hamiltonian system into general. Hamiltonian system into general Hamiltonian, not canonical ones. In other words, the kind of Hamiltonian system defined by scaled symmetry matrix. Assume that you have J which is a skilled symmetric matrix. And also the J, I think, okay, the J shouldn't depend on X, J doesn't depend on X. So if you have this skilled symmetric matrix, and then you can define your Poisson bracket by the following. For any two can smooth. For any two smooth functions, take the gradient and then you deal with this matrix and then the gradient. It turns out that this function is already, this operation is already defined, give you the Poisson bracket. And from that perspective, and we can revisit our Hamiltonian equation. So for the Hamiltonian equation, we have this gradient, which is really followed from that. It's really corresponding to that term. It's really corresponding to that term, and also we have J. This G here doesn't have to be symmetric matrix anymore. We can expand it to the skilled symmetric matrix. That means that now we are considering more general Hamiltonian equations. And also, so in terms of the independence of the conservation laws, and now we have a new definition of functional language. So, if you, so your system has So if your system has several convolution rules, and suppose I'm two of them, we say that they are functionally independent if there are two conditions. The first condition is that the gradient has to be independent. And secondly, they have to be Poisson commuting. So in other words, the Poisson bracket has to be zero. So that means that we don't need the gradient to be perpendicular, but instead we want the Poisson bracket to be zero. Write it to be zero. So, and then it turns out that this problem really, really helps us to solve, to overcome the difficulty. Okay, so this is really the main work, the main work that we have done with my colleague Wei and Wei Zhu and also Panos Carakidius. So, in this work, we designed a new method called neural deflation. So, the main idea is the following. Years the following, so we first, okay, okay, oh, all right, also, there's a story here. So, we have a we published the paper, but in the paper we published, we took a shortcut because we were trying to see if this method worked. So, one of the assumptions we made was that we assume that the equation, the function is already known, just like what the other paper did. So, in other words, you know the equation, I know the vector field F. I know the vector field F. But even if you know the vector field F, you know the system, but we still want to know how many conservation models. And there we try to design this neural network. But after the paper was published, and we got feedback, and then people were asking, well, what if you don't know the equation, but you only have a lot of observed data? And that way, so from my perspective, we also try improving the method. The method. So there are two perspectives. So one perspective is that if the vector is field is known, the other perspective is that if you only have observed data, you don't know the equation, what do you do? So how do you find the conservation law? So what I'm going to do is I'm going to go to the second part. I only have the data set. Okay. So here we mainly, so I have on the data side. We first divide the data side into training, validation, and also, of course. Validation and also, of course, and also test. So, what we do is that we design. So, in this neural network, it consists of many sub-networks. So, how many of them? If the position space is D dimension, and we design D different neural networks and called I1, I2, ID. So, we design D of them, and we try to check how many of them are really invariant, and in terms of, and also their functional. And also, they are functional independent. And we try to find that D0, the D0, right? So, and then the other network, they are not corresponding to the independent neural network. And those ones will figure out because those ones will have a loss function, which is really, really big. And then, from the loss function, we will see, we will count how many of them have small loss and how many of them have big loss. And then there is a jump here by the place of the Is a jump here by the place of the jump, and we will figure out how many D0 was the D0 there. So the main idea is that we are trying to cover neural networks altogether. Okay, let's do induction. So how do we design it and how do we put the loss function? So we're going to do by induction. So by 4k equals 1, for the first one, because we are working on having First one, because we are working on Hamiltonian system and we want to make sure that the first one, the first conservation law is really the Hamiltonian function. So in order to do that, we can just use this paper like done by Grey Dinos, and which was also mentioned by one of our speakers in the morning by using Hamiltonian neural network. Because now we have a data set, right? And using the data set and using And using the data set and using the Hamiltonian neural network, and I try to learn the first conservation loss, which we call H. Okay, so maybe, so this is just the main method, the main method for the hominin neural network. And then I have data side, and then we try to learn the hominomium by taking the gradient, such as the gradient is. such that the gradient is equals to, is as close as possible to the vector field. Okay, so after we learned the Hamiltonian, and that's really, really good, after we learned the Hamiltonian function, and then we can take the gradient, we have the gradient, and you times this Jacobi, the symbolic matrix, and that gives you the vector field. So once we have the vector field, and now we can go to the second step. So what now we learn about the first Now we learn the first congregation. For the second one, we're going to start from here. Again, as I said, we're going to do induction, right? For the second one, we try to make sure we design a neural network such that the second neural network is the second neural network, the gradient is perpendicular to the vector field, satisfy this condition. But if you look at this expression, that's really saying that any other conservation laws, and I Conservation laws and H, they are commute under the Poisson bracket. So, this is just one of the another equation which put into the loss function. And this is related to a new work which related to my collaborator and also we have a new student work with us. Um okay, so for after we learn the first one, we look at the second one and maybe assume that we have learned k of them. We have to learn k minus one of them and how do I learn the next one. We want to make sure that every time we want to make sure that all these new networks and then the Poisson bracket has to be close to zero and to make sure that they are function normally independent. This is uh related to the neural Is related to the neural deflation method. And okay, so the concept, so there are a few mouse challenges. Am I talking too fast? It's great. I'm loving it. Finish early, more topic discussion. Okay, that's good. So yes, we profiles. So, um, so but last but for this neural network. So, for this neural network, as I was saying, we designed these sub-neural networks. And then they are controlled by this loss function. So, maybe I'll just discuss, explain more a little bit about this loss function. So, for the loss function, so there are three types of loss functions, loss terms we put into the loss function. The first term is about we call conservation loss function. The conservation loss function, the main reason we put there is that because we want to make sure that each Is that because we want to make sure that each sub-neural network is related to a conservation law? So, this Ik, we want to make sure that I k gradient of IK, the gradient of Ik out with your vector field is almost zero. So this is by definition, right? I just put that here because that's exactly the definition of an invariant function, invariant function among the flow, right? And also. among the flow, right? And also the vector field we already figured out is the relative equation of the J copy of the Hamiltonian. And so this first loss is related to this, it's called conservation non function. Okay, and also here because as I was saying, I'm using the induction. Induction is just really assuming that for all the k minus 1 step, we already done. We already done we already learned everything, but now so the so I already learned k minus one translation rounds. I want to learn the next one, but the next one we don't know, but we don't know if it is a concentration. Maybe, for example, I'm going to give, for example, if you have a dynamic, if you have a Hamiltonian system, it only preserves energy. There's no other concern laws. And then what do you do, right? So for the second one, I still want to force, to force this one, I want to make sure that this is. This one, I want to make sure that this is zero. Of course, you can, it's no problem because if you have a Hamiltonian function, h squared also satisfies this equation. It's invariant. So this is the first loss. For the second loss, it is called involution loss. The involution loss, we want to make sure that you have learned k minus one of them, and now you have to read a new one. And I want to make sure that this new one, I take the gradient. Gradient, oh, not gradient. So, this new one, I want to make sure that this new one and the previous k minus 1 conservation models which we learned, and their Poisson bracket is 0, they commute. So that's also responsive here after I already learned t minus 1 of them, and the k is 1. And we take this Poisson bracket, and I want to force this one to be as small as possible. So, for example, then let's. So, for example, then respect to our question: what if I only have one conservation one and then you're learning the second? Now we are learning the second one, and you can see that if you only have the one and the second one, and you will see that this loss will be fake. You cannot make it to be zero. When you really train, no matter how you train, the second one will be really be fake. And that means that when you really you see all this kind of situation and then you know, okay, system only have one kind of issue. only have one confirmation both. Okay, and another important loss function, which we call the deflation loss. The deflation loss is that we also want to make sure that the gradient of this conservation loss function, they are kind of independent. They're kind of independent. So based upon that, so we define at k step, we already learned k minus one of them and we define k minus 1 of them and we define this function this Weiser space to be spun by the gradient of first k minus 1 first k star of 0. The first k conservation mass take the gradient so the spot of the gradient become become a linear space and now we only consider the complement of this space complement of the space. Space. So, what we are going to do is that now we are going to start a new one, right? We're training a new network. This network, we take the gradient and we project this guy onto that combination. If this one is already included there, when you project it, it becomes almost empty, right? Almost zero, so the projection is zero, and this one will be zero. But on the other hand, if you're really learning another independent congregation, An unindependent conservation mouse, take the gradient, you project on the complement, you have something meaningful. You have some meanings. So, this loss function is kind of different from our traditional loss because we'll not expect this thing to be zero. We expect this thing to be non-zero. If it's zero, it's really, really bad. Because if it's just zero means that there's nothing useful, this guy just falls into into that span, which is not in the point. One, which is not in the first place. So, to another question is that how do we deal with this loss function which is non-zero? Which is, we really want that to be non-zero. And then what we did is that we put that term into the denominator. This is mainly the loss function, the complete loss function, which we use to learn the case conservation law, and that is the summation of the conservation loss. Formation of the conservation laws, the evolution loss, and also we put this independent loss into our denominator. And it turns out that still sometimes it's a little bit tricky when you really try it. So we also, I don't know whether you notice that I also put a parameter here. Parameter here, this parameter is called a silver hyperparameter which we call the deflation parameter. So this differential parameter, for example, if alpha equals to 1, if alpha equals to 1, and alpha can also equals to 0.5 or maybe equals to 2, and then it gives you different strengths of this deflation. And by term, it's in the denominator. So by changing this deflation, By changing this deflation parameter, and we can try to, we can try to, we can see that we can control the speed of the convergence, and because the loss function is really, really, it's kind of, it's very important for us, and I will show you in the case study in the next few slides. So, in most of the case, we choose alpha equals to one, but if it doesn't, sometimes it's a little bit ambiguous and we try to use. Ambiguous and we try to increase it. Okay, so this is mainly the structure, the learning framework for this neural network. It's a little bit complicated, but it turns out that it works pretty well for most of the system we consider. So again, I come back to this example, which is my favorite example. So this example, as we already seen many times, right? We already seen many times, right? So, this example, this system is completely interglobal. It has deconservation loss, and also just for comparison, we also consider the so-called discrete Frank-Gordon system. And this system, if I spill, I also consider deep particles, and this is a Hamiltonian system, and it only has one conservation loss, one conservation loss. Conservation loss, one conservation law. So we try to pull these two systems all together, we do a comparison. The other has D particles, has D conservation loss, and this one has one conservation loss. So we train both systems, and then we put the loss function together so that we can see things are clear. So the orange, this orange color is the collagen system, mouse system, this system. So in both of the cases, So, in both of the cases, we assume that D equals to 6. D equals to 6. If D equals to 6, so for the CM system, it has 6 conservation loss. But on the other hand, for the same garden, it only has one. So, you can see here, this is the loss function for I1, I2, up to I6. And the loss is pretty consistent, 10 to the 96. And suddenly it jumps, it jumped to an a four. it jumped to and a for for the seventh and also we know that for this system for the c for the calvatino nozzle system it has six convolutional nodes. But on the other hand for the fine garden there's only one there's only one and which really works very well and that's just the Hamiltonian function. It works well but suddenly by two it jumps. It jumps. The main reason we put these two figures together just for comparison because otherwise it's still a Otherwise, it's still a little bit shaky, right? I jump here, and so, but the one designer just for demonstration. And we also checked for different number of D's for the same system. I just put a few figures there. This is the case when D equals to A, and you can see the CM system we perform very consistent. And also for the Sein Gordon, it's the same. So for Sein Gordon, we learned this one. Gordon, we learned this one, and so this five, we learned there are six of them. And yeah, we did mine experience, so they are very consistent. Also, okay, so in the first paper we also did several examples, including the harmonic system, and also three body problem, and also TALDA system, and they all performed very well. TOUDA system and they all perform very well, but there was one system that didn't perform very well, which is the FPU system. And this system, it has very interesting and we didn't know why. This system in our first paper didn't perform well. It has two types of conservation models. But no matter how we try, we can only learn one, and then the other we couldn't learn. And then the second time, and then after in the second draft, and we try to pull. Drive and we try to put out. So, this is one of the figures that we put out the, we only consider the FPUT system and then we try to increase the number of the particles. So, M corresponding to D, M corresponding to D. So we put all the figures together and we want to see what's happening. So, for N equals to 6 is the blue text, and then the orange case is the 7. seven and they're really they're kind of consistent so remember this guy has two conservation mouths and this one there's one here and there's one here they're jumps but still but this jump and also and then up to here and everything else they're also consistent so from that perspective and we can we are kind of we say we say okay there are two conservation laws we say it depends how we um variance them how we compare them um I think I think okay, so as I said, I was kind of fast. You're dead on time. Okay, good. So there are some open questions related to this. For example, there is another non-linear system called ablovic lattice. And this is like has an imaginary part, is a discretized malignant Schwittinger equation. And this system is mounted. And this system is one of the most amazing system because it's completely integral. It's very, very nice. So we were trying to try this system. It doesn't perform well. I think the main reason we managed to figure out that is that this is not a canonical anatomy system. It has a very special, and also because of this mandatory part, it's kind of difficult, but the world is currently underway trying to figure out how to deal with this system. With this system. And of course, there's another related question: when I was saying, okay, we try to learn, we learn the conservation law, and I'm pretty sure many of you are still like, oh, what do you mean you learn conservation law? So we just say, okay, we try to, we learned. So, for example, we say we learned D0 of the independent conservation law, meaning we called up D zero neural networks and each neural network corresponding to one. But then if you f really of physics But there's a diffuse really of physics, and you may ask, well, which one are they, is the second one corresponding to the conservation of momentum or angular momentum? Which one of them? And those are the question that we're going to consider later. And also, I hope that some of the audience will be interested in the job asking. Thank you very much. I have a couple questions. So the first is that how are it possible to optimize the LOS function? There are many competitive terms in the LOS function. For example, a couple of them vanish if you have a constant function. So it's just to understand how much you need to you need the loss. Right, yeah. It turns out that for our example we did the loss function there. Example we did, the loss function they covered pretty well. So, the only thing like not was like a little bit difficult is this guy. So, this one, that's why, so this is the one we try to use a different, like, also, okay, by the way, remember we have a trick here, which we have a very powerful parameter which is the alpha. Alpha is the deflation factor. We can manipulate that thing. So, this is the one for all other systems. We use alpha equals to one, but this is. Use alpha equals to 1, but this is the one we tried: alpha equals to 0.5. Meaning, remember, alpha corresponding to the denominator. If it's 0.5, we try to increase this power. So, square root of right, so because this is small. So, the alpha is a very important tool which we place. A lot of data points. Um, the the data point here we use, um a hundred we use uh So we use a box of 10. Versus 10. It's a very big box. Very big box. I think we use 50,000 points. But there's something very interesting. We tested all the different activation functions. So the activation function we use here is silo, which works very well and also it's very fast. So we tried many other activation functions and then we kind of Activision function and then it comes low, and I mean the scene works really magically in mics. Yeah, very nice talk. I was just wondering because you showed in the end that when the system is not canonical, that you get some you got some slight problems. I just wondered because your approach with the Poisson bracket is is that The Poisson bracket is only working when the Poisson bracket is defined from a sympathetic form, right? If you take a general Poisson bracket and you have non-trivial casing functions, for instance, it would not work, I think, is that so? Okay, that's a very good question. Because as I was saying, there are two schemes to be used. The first scheme is that if you if I know what the vector field is. And that one it works for very general Hamiltonian neural networks. But for the second one, if I only But for the second one, if I only give me data set, and remember there was one tricky part which I didn't show you. So that is the tricky part because I have to recover the vector field. But if you don't tell me, for example, I use the Hamiltonian neural network to find the Hamiltonian. But if you don't tell me what this is, and I have no idea what the vector field is. So this is the part I really need to know to know what this skill symmetric function. And so far, for this method, And so far, for this method, we used two specific metrics. So that's a very, very good point. I think the bell is rung. We better finish there. So thank you very much for having me.