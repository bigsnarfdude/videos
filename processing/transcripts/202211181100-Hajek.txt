It's been a great experience. I really appreciate the fact participants stayed the whole week and I worried. You know, when we had larger numbers, we had some people leaving on Wednesday, someone comes on Tuesday, leaves on Thursday, or they fly in and they fly out stuff. So it's been a real pleasure to get to know you over a whole week period. So it's exceeded my expectations a lot. I really learned a lot. I really learned a lot. So, thanks for coming. Okay, I'm going to talk about this work, which was published three years ago. So, it's the last, say, my most recent work on preferential attachment, on community detection. It's joint with Surya Sankaguri. This was based on his master's thesis, and then he went into blockchains after that. And now he's at And now he's at EPFL on a postdoc. So, first, and the idea was that there's a lot of work on Erdos-Renya graphs for community detection, and we were searching around for other models where we could say something. And the preferential attachment model seemed like a good one. So, I'll talk about that. I'll give an overview of community detection, which you all know about, and preferential attachment graphs with communities. Differential attachment graphs with communities, and then asymptotic properties of the model when we let the number of nodes go to infinity, and then a bit on inference algorithms, inference based on degree thresholding or the child sets, message passing algorithm, consistent estimation of a small number of vertex labels. So there's some theorems and results in it, but not there's really not anything proven about the message passing. So I have some numerical results about. So, I have some numerical results about that. I think there's a lot more that could be proven. So, Jamie can recognize this. This is in LaTeX. He wrote a LaTeX statement to generate that graph. Yeah, I was using the Beamer. And Beamer, right. So, stochastic block models. You have 200 vertices, K equals 50. There's a planted dense. Dense community in there with P and a Q. And so the advantages of this kind of model: there's many algorithms that we've seen this week. There's exact recovery. This is not really moving by itself. Okay, there's exact recovery algorithms. There's provable guarantees. A drawback of the Erdash-Merinia model is it lacks a time evolutionary nature. A time evolutionary nature, and maybe you might have that, for example, on a citation network is one example, where you've got some time stamps on the vertices. It doesn't have any heavy-tailed degrees, and it doesn't have short cycles, although you can put triangles in there, which is a way to get short cycles in there. So, preferential attachment graphs. First, I'll talk about the vanilla one without communities. Without communities. Vertices arrive one at a time. Okay, actually, this is with communities. The vertex labels indicate the community membership. And the labels are exogenously determined. So when someone comes in, you flip a coin and randomly decide what community they're in before they attach. And then each new vertex shoots out some M edges to the existing vertices. Choices are made independently and simultaneously for all the edges they shoot out. And the attachment distribution. And the attachment distribution is proportional to the degrees of the vertices that are already there weighted by the affinities to the communities. So the difference is from the stochastic block model, the vertices have times of arrival and the degree distributions are heavy-tailed. So here's the Barabas-Albert preferential attachment model. And so new vertex arrives. This is without communities. This is without communities. So you look at the degrees of the existing vertices and then you divide by the total number of the existing vertices. By the way, I'll usually start out with some initial graph where the number of edges is m times the number of vertices. And then m is also going to be the number of edges on every new arrivaling node. So then independently, we match those two edges to the vertices. Those two edges to the vertices with those that probability distribution, and we get the next graph. And then the next vertex arrives, decides where to attach. You look at the degrees, again, normalize them, and then attach at random. Okay, that's without communities. Now, if you don't want to put communities in there, suppose that the initial graph has communities already for the existing vertices, and then a new. A new vertex comes. So the blue one, it's blue, so it's going to preferentially attach to the existing blue vertices. So we take the degrees, so four, five, and three, but in this case, I'm going to multiply each of those by a factor of two. So the four there turns into eight because I multiply by a factor of two. Orange one is a different community, and then the three I multiply by two to get. I multiply by 2 to get 6, and then add those numbers up as up to 19, divide by 19. That's the probability distribution I'm going to use to attach those two edges. So that may be that one attached to this blue one and it happened to attach to the orange one. Then another one comes orange. So, you know, degree four, two, three, six, four, and two. So the distribution is going to be. So the distribution is going to be 4, 4, and 2, but the 6 gets turned into 12 because that's the same community as the arriving vertex. Right. So this is a model in the notation. And we consider the case with, could be more communities, like R communities. And then M is the number of edges that shoot out when a new. Edges that shoot out when a new vertex arises, and then there's a probability distribution for the color on the arriving vertex. There's only two types, then rho would be, let's say, probably it's a type one versus a type two. If there's R, then rho would be a vector of length R. And then beta is the affinity matrix. So beta UV is the affinity for an arriving vertex of type U to choose. Choose a vertex of type V. That's how you're going to weight it. And so for this example, I was using like 2, 2, 1, 1. So you're twice as likely. So you can think of you really choosing a node, but you could also think of it as choosing an edge. So, I mean, it's got four edges. And when you choose, you could actually choose one of the existing edges. And then that determines which node you're going to be attaching to. Any questions? Attaching to any questions about this? There was a work of Mosul and others where the communities were determined by who you attached to. So in fact, it's the same dynamics as the vanilla preferential attachment model. You color some vertices, and then when you attach, you join the community by taking a majority vote of who you're going to look at. And then. Going to look at. And then a question was: Do some communities like get overwhelmed? Like the ratio of the number of blues to oranges? And that was kind of the questions he's looking at. And then there's also been a lot of work going back to the Barabasi-Albert model where there's fitness. So there's like different fitnesses here, but it doesn't. But when the new nodes arrive, it's the same. So it's like we said in beta UV, it's just. It's like instead of a beta UV, it's just like having a beta V for different fitnesses. So we hadn't seen anything when we started this, but it turns out there's two days after we posted, maybe one day after we posted the archive, James Jordan from, I think from Sheffield wrote and said, I have some paper on this. So we had two papers and we condensed it quickly to one paper and built in his results. And built in his results. Okay, so as far as the recovery, I mean, there's several different questions you could ask. I mean, you could look at the whole graph without knowing what the timing is, which is like erasing the arrows about, you know, or you could have a directed graph, or you could see the whole, you know, actually, if you see the directed graph, then you can recover the timing. There has also been work about trying to recover the timing from the undirected graph. From the undirected graph. So you just give an undirected graph, the undirected graph at time t, and then try to recover as best you can the timing. That's a different problem. Okay. Okay, rho is a probability distribution that's determining the initial, when they arrive exogenously, their membership. So actually, I probably should have wrote rho here as a vector one half. Should have written row here as a vector one-half, one-half for the case of two communities. Yeah, so yeah, here I flip a coin, fair coin with rho is equal to half. If row is equal to one-fourth, one-fourth, one-fourth, one-fourth with four communities, and I choose color equally at random. Okay. Okay, so some asymptotic products. So, some asymptotic properties of the model. So, we can look at the fractions of half-edges from each community that converge. So, over time, if you look at the fraction of the edges that are attached to the orange nodes versus attached to the blue nodes, that converges and it's not hard to see. So, what you take all of the blue nodes and just combine them into big ones. So, these are all the blue edges and orange and all the one edges. It really doesn't matter very much. One edge is it really doesn't matter very much where a given edge which blue node an edge is attached to. If all you're worried about is the number of edges attached to blue nodes and the number of edges attached to orange nodes. So it's a lot, it's a lot like a polya urn scheme because when this edge it's like when you're choosing one of the either blue or orange, it's like if you forgot about the who this belonged to, if you choose an edge from here and pull it out, then you're going to add. Edge from here and pull it out, then you're going to add one more edge when you attach to there. Okay, so so you so that's so the you have more blue edges, so you can think of the or blue half edges because their color could be interpreted, you know, inherited from the vertex. If you have more blue half edges, you're going to get more. It tends to grow. But on the other hand, exogenously, half of the edges coming in from this part are going to be blue and half of them are going to be orange. To be blue, and half of them are going to be orange. So it's a variation of the Polya-Ern scheme. And if it was, you know, Poly-Ern scheme, the fractions converge, but the limit is random, right? In fact, it'll be like uniform from zero to one, but because you've got this exogenous choice here, then the limit is actually converges. So if you look at c of t is a vector of how many edges are of each. Of how many edges are of each type, and m is equal to two. So 4t is the number of edges you're going to have at time t. So this will be the fraction of edges that are in community one, the fraction of edges are in community two. So you can let a to t be actually, yeah, is the fractions. I don't know what I did before. So yeah, okay, that's, I call it. Yeah, okay, I call this eta t basically. So eta is the fraction of is a vector, thinking of a row vector, the number of edges on each communities that converges. The lining fractions of edges with given labels converges almost surely as t goes to infinity, where eta star is a unique probability vector satisfying h of eta star equals zero, where there is this thing here. Actually, we were having difficulty proving that this limit always exists, and but Limit always exists, and but James Jordan had a very nice Lyapunov function, you know, potential function to show this global convergence. And so, what this for eta fixed, so if eta was the fraction of edges of each type existing, then this would be, if you divide it by 2t, it would be the drift and the fractions. Okay, so you have it minus 28. You have a minus 2a to v comes from the fact that you're adding in more and more edges, so that dilutes the current ones that you have there. And then, um, so if a fraction, okay, and then this comes from the exogenous arrivals are increasing the number of edges of type V, and then an arrival of type U, which happens probably rho u, it will have affinity, beta UV. affinity beta u v and it will be attaching to each of the existing edges in neighborhood v. So you multiply by that and then divide by this to give you a probability distribution. So this is the probability that an edge arriving of type U is going to attach to an edge of type, you know, select an edge of type V when deciding where to attach. And when you average over all these, you get the sort of the arrival rate of attachment. Sort of the arrival rate of attachment to edges to a community v and set that equal to zero for steady state, and that gives you the fractions. There's a very, we put a proof in our paper of this. It follows from a standard result in stochastic approximation. Okay, we'll be better. We don't have a very good rate of convergence. That's kind of a problem. Kind of a problem would be better to, you know, to get more detailed results, I think we would need more on the rate of convergence here, which wouldn't be that hard to get. So let theta star UV be that ratio I just talked about, but divided by two, and theta v star given by this, then theta u v star, this thing divided by. This thing divided by t gives the probability that an arrival of type u is going to attach or choose an edge of a particular edge of type v to attach in the limit. Okay, so there's for t finite, there's an empirical version of these two things, but by the thing I just talked about, they converge to these constants. So you're going to do a cleaner analysis, you can just replace those. You can just replace those empirical things by these constants they converge to. So the limiting empirical distribution of degree for a given label is given by this. So for m is the number of edges you shoot out. Theta is just a parameter here. So you put in the limiting theta vector here, you get. So theta star v is the As the limiting strength of attachment to community V because theta star over V, if you have a vertex of type V, theta star of V gives the arrival rate of new edges to that particular, I said vertex, it should be for a particular edge, for a particular edge of type V, this gives the arrival rate of attractions by that edge. Uh, attractions by that edge, and then if you have a vertex that has you know four edges coming out, you multiply by four because each of the edges coming out of a vertex has the same rate of attracting new edges. Okay, and so n here is just the probability, you know, the fraction of vertices that have degree n. So that's this, theta is a That's this, theta is a half, and the if you if you just have one single community, theta is a half, and then you get one over n cubed. So the degree distribution, if you look at a randomly selected vertex from the preferential attachment model, the tail distribution is one over n cube. In the symmetric case, I guess. symmetric case oh yes right so in the symmetric case all these thetas will be end up being a half and then and then you wouldn't be able to tell by looking at the degree of a vertex what community it's in but but if it's non-symmetric then vertices in different communities would grow at different labels and you could do like a degree thresholding and get some some information about the degrees Okay. So, this is just an example of a graph of size 10,000. The average degree size is four. And there's two communities. And this is the degrees of, so just one community. And so if a vertex arrives from that community, it attaches to other existing vertices on that community. And the other vertices, the other half of the vertices, just randomly. Just randomly attached to any edge, not paying attention to the communities. And then the degree distribution of the vertices have one huge degree vertex, and it's more spread out. And then the degree sequence of the outlier vertices that aren't in the community are looking like that. Okay. So we did more things because we. More things because we wanted to find the short-time evolution of the degree of a vertex. So, so have this community model going and then look at the vertex that arrived at time t, and then additional vertices are going to be coming in, and they're going to be shooting edges out, and then the degree of that vertex will be expanding with time. So, if you fix t and then let y t denote. Let y denote the degree, sorry, fixed tau. So tau is the vertex that we're looking at. And then we want to see how its degree increases with time. So this is at some time t later. We can see what the degree of it is. And the dynamics are from t to t plus one. This law means it's a probability distribution of the increment here, given what the degree is, given eight of t. And L of T, and then that the label on that. So, L of tau is the label or the community that the vertex that we're talking about, time tau, is. So suppose you have a vertex from community V here, time T to go to T plus one. First, the random arrival comes with probability distribution rho u. It's going to shoot out M edges. Going to shoot out m edges, so the number of those that land up hitting this vertex has a binomial distribution with m. And then there's some really small probability that any one of those edges is going to hit exactly that vertex. And with high probability, the binomial is either going to be zero or one. So we can make that Bernoulli. And with high probability, by the way, this notation means this is a mixture of binomial probability. Mixture of binomial probability distributions. And that's approximately going to be, if at most one of those binomials is non-zero, which means you don't get hit by two edges, then it'll be a Bernoulli, either zero or one coming out. And so an approximation would be to let, okay, I also made an approximation here. I got rid of the T. This is the empirical UVT, but we know that converges to To u theta star uv, and then we added the sum out over u, and we just get theta star. So, so if we just look at one at this vertex tau, it looks like the number of, it looks like it'll grow by one with probability theta y over t for a fixed theta, or well, this theta should be equal to this, the theta. Should be equal to this, the theta star for that. Okay, but we define this general process using a general script theta. So for a given vertex that arrived at time tau from community, whenever it arrived, if it's in community v, and if it arrived, yeah, if community v, then the growth rate, it grows like this process with parameter theta. With parameter theta tilde equal to theta star v. Okay. Okay, well, this thing, just like in stochastic approximation where you have some parameter going to zero, like one over t, things get slower and slower and slower from the standpoint of one vertex, because there's more and more vertices out there and lots of edges. So if you speed up the time to have a constant arrival rate of edges, then you get a much nicer process. Much nicer process. So let Z be this very standard branching process going back. I mean, people in the 1940s were looking at this. So it's a Markov process, continuous time Markov process, where if you've got M individuals, you go to M plus one at rate M times theta, which means like each individual has a birth rate of theta, and you just look at the total population size. So that's a standard growth of a population. And then And then, if you sort of slow that down, so I use S for that time scale of a growth of a time homogeneous Markov process. And if S is the time amount of time we run that, then we want to run that for log of t over tau. So time t. So time t, it's like the integral of one over, running out of letters, one over q from tau to t is the log of log of tau minus, see log of t minus the log of tau or log of t over tau, which is what, that's how long you should be roughly, if you if you run this process for that amount of time, you'll see how much your degree grows. Okay. Degree grows okay. The marginal distribution of this for all time t is a geometric distribution, not a Poisson distribution like we see in the Erdashrenia graphs, but geometric. And the mean of this is e to the s. Very simple. And so if we so the marginal distribution of z of s is if you at time s is going to be the sum. Time s is going to be the sum. See, if you start initially with m variables, initially, then it would just be the sum of m independent geometric random variables, which is called a negative binomial distribution with that distribution. So roughly, if you want to get the distribution from a vertex here, what's the distribution of the vertex at time t? Take log t over tau as amount of sort of. Sort of time that this has been growing on the when you when you have a constant arrival rate and then you just plug in this distribution so um so we proved in here a total variation coupling between the original uh process the um so this is the actual random process from time tau to time t of how the Tau to time t of how the uh the number that the degree grows between tau and t. And then this is the y tilde, which is like Bernoulli growth discrete time process with the parameter equal to limiting parameter. That total variation goes to zero. If we let, as we look at t and tau both going to infinity with t over tau bounded, that's the Bounded because that's the limit regime. And then also, if we take the y check, this is the you take the continuous time process z, and then you put that back on the old slow time scale, and then you discretize time to make it discrete time. Then you can talk about the total variation distance of that, and that also converges to zero. So, in general, so if you're going to do any kind of estimation of the type of a vertex based on the original. Type of a vertex based on the original process, you could basically look at that Z process. So, and we can also generalize this to joint distributions of J vertices. If you take J vertices and look how their degrees grow, they're jointly in this limit independent, and they grow like independent branching processes that get slowed down according to that. The fact that time is like for any vertex is slowing down, sort of the rate of growth of time is one over s. So, this so this yields an accounting of the empirical degree distribution for each community. So, if you look, suppose you run this for some large t. So, t would be the total number of vertices that you're going to end up at time t, then. t then going from zero to t the probability that the vertices that arrived at time little t see yeah the vertices that arrived at little time t at time capital t they've been running for this amount s represents how much you run the time homogeneous Markov process for so that was there and then uh There and then theta would be the distribution for well, theta would be the type of one of the communities, the theta star, and then m here is the shooting out number. So if you integrate this, then you get the probability distribution I talked about before. About before. It's kind of a nice calculation. That's nice, it works out. So there's this pretty complicated distribution. Oh, actually, even, yeah, actually going me back further. Awesome. Yeah, this was the probability distribution from Jordan. This was the distribution of the total number, the fraction. The fraction, no, the fraction of vertices that have labeled v that have degree n. So, if you just chose a vertex at random from time one to time t and looked at the degree, you get this distribution. And then we can get that by integrating this distribution from zero to t because this basically gives you the probability distribution for the vertices that arrive at time t. And then when you And then when you integrate over that, then you get the total distribution for a randomly chosen vertex, and it works out to that thing. Okay, besides that, we also want to let capital T go to infinity for tau fixed. So if you look at the degree of a vertex time capital T as T goes to infinity, and then divide by how long the process has been running. The processes have been running on the S time scale. The ratio of the logs converges to that theta star, where L sub tau is the label or the community that that vertex is in. And if you, this is approximately, well, this is the same as y of t is equal to t over tau raised to the theta plus a little o of one in the exponent. A conjecture is that you can get a little. Conjecture is that you can get rid of the little O of one here and have a positive constant, much as you would have if you had a branching process. But the problem is, the empirical theta doesn't convert, you have to take that. There's a race between whether that converges to theta star to whether this really looks like a branching process with a fixed theta star to give you a constant out in front instead of this. But still, so if you look at a vertex. So if you look at a vertex in community L tau, so vertex tau and community L S tau, and you look at the degree later, it's going to grow like this. And so you can separate vertices in different communities eventually, almost surely, if they have different growth rates. Any questions about this? Okay. So now we talk about So now we talk about inference based on degree thresholding or children's sets, message passing, and consistent estimation. So, because we had these total variation results, we could start out just by trying to infer the community based on the degree of a vertex, or we could also look at the whole process of, I said, children sets, which would be the times that vertices are added because that core. Are added because that corresponds to the children because the children are actually vertices and that are numbered by the time sequence. So, one question is, can you get more information from knowing where the vertices are attached to than just from the number of vertices? Okay, and then message passing and then consistent estimation of a small number of vertex labels. So I'll talk about mainly numerical results. And also, there's a we'll get to that. Okay. So the estimation of a vertex label from the degree evolution, well, it's continuous time Markov process. You can look at this. Suppose you look at the continuous time branching process and you don't know what the growth parameter theta is. It's a simple thing to find the log likelihood ratio. Ratio, the log likelihood ratio is this. So, if you observe such a process from time zero to some s-bar, you look at the growth rate, this like just a branching process, and the log likelihood ratio is the total number here, which would be like the degree at the end times the log theta, but minus theta times the area under here. So it sort of makes. So it sort of matters how quickly it got to that degree. And this makes sense. I mean, if you want to look at the maximum likelihood estimate of theta, you divide by theta. So you get this divided by theta is equal to the area. So the optimal theta would be what the growth rate is divided by the area under it. Is divided by the area under it. And the area is like the integral of the intensity of jumps. Okay, so basically, the total intensity is something, and then the amount of jumps you really had is something. The ratio is theta is like the rate that you have of growth. So, anyway, that's an example of a likelihood ratio. Okay, I'm gonna say something. I should go back. I see. Oh well. Maybe it'll come up later. So message passing algorithm. One thing that's complicated about this, I wanted to say that you may be familiar with the weak limits. Yeah. I mean, it looks like you're going to get into more complicated algorithms. If I want to estimate, if I have two vertices and I want to estimate whether they're in the same community or not, let's say, can I do something interesting from just local observations without getting like kind of like what you were saying for just one vertex on the previous slide, without getting into like big, you know, global message passing? Well, I mean, that was. That was um did I did I missed that? Um, well, just from this, um, based on your community, you know how what the degree is going to be. Correct. So you could look at the individually. Yeah, so you could look at, I'll have some graph just counting the number of neighbors. Um, and and it doesn't work for the symmetric case. Okay, yeah, if those theta stars are the same, the number of neighbors isn't going to work, and it's like. Work and it's like the Erdash-Renya graph with two equal-sized communities. The community structure is the same, whichever community you're in. So you need some kind of seed or something random to jiggle it to get even the message passing to work. I guess it's not really the case that there are enough edges in one step that you can, you know, like if there were really a lot, maybe you could observe them connecting kind of simultaneously. Can observe them connecting kind of simultaneously to basically right. I'll talk about that later. So, in some cases, m bigger than one is easier because you can look. Occasionally, you might get two edges going to the same vertex. Or if I have two vertices and I see one arrival connecting to both of those, so it would be common ones. That's an indication that they're in the same community, and that could work even if you have the symmetric case. So, yeah, so. Okay, so yeah, so you'll see. I you can't, it's a little bit like Erdasharinia, but it depends where you go out on the graph because the degree at the beginning is going to be like a square root of n, and the degree at the end is going to be close to n. Most of those aren't going to be hit at all. So like 90% of the vertices, you're not going to be able to classify, but the first 10%, maybe you'll be able to classify. I don't know what the exact threshold is, and it depends on that beta matrix. Depends on that beta matrix. So that would be kind of an open problem to see how many vertices you can classify. And also, we're thinking about the weak limit and also the depth of this graph. The depth of the Erdash, of the preferential attachment does go to infinity, but very slowly. So if you look at a vertex at random at some large time, its children are pretty easy. It's like a branching process, but the parents, you know, some. The parents, you know, some note that note attached to some parent, and then that note attached to some parent. So there's always some path going back to the root, and it goes back pretty fast. So that's why the depth of the graph grows really slowly. And so if you want to ask what, if you just take zero to t and you map that in the interval zero to one and say something that occurred at the 60th percentile of the way through the end. Percent of the way through the end, the neighborhood will converge, but it's pretty complicated. And also, N will have to T will have to be ridiculously large to have a pretty big community because you might not have very many children, but you're going to have parents going all the way back to the root if you go back that far. So, I think that's the Um, yeah, the so we did basically do look local week limits to come up with uh a message passing. And the messages coming from the parents who you choose are different than the messages you send the other direction in general, if you if you know the time. But for the case of two symmetric communities, it turns out that the messages coming in both directions from the early arriving nodes and the later. The early arriving nodes and the later nodes are the same. So, in fact, you don't even use the order of arrival, which is kind of nice. So, that's a good thing about the case with two symmetric or even n-symmetric communities. You don't have to know the order of arrival. It's not used in the message passing algorithm. So, the message that goes from some vertex tau zero to tau is a sum of messages, other neighbor. Messages, other neighbors, of the messages that go to tau zero, and just add them up using this nonlinearity g, which kind of saturates at beta and minus beta, where beta, the fitness matrix or affinity matrix is beta, one, one, beta. So beta is maybe bigger than one, is how much stronger you are. Is how much stronger you are to attach to your own community versus attaching to the other community. Okay, so then to get, we didn't have any luck just using random initial points, getting any kind of this. I mean, it would be fun to get this two symmetric communities sorted out. And things didn't work out very well. So we went to something else, which is, and which only works for M bigger. And which only works for m bigger, two or larger. So m is two or larger, we look at the joint evolution of a finite set of vertices. Okay, so we take v to be a finite set of vertices, and then there's r to the v. So there's only a finite number of possible labels for those vertices. And then we look at the evolution of the degrees of that finite number of vertices and then. Finite number of vertices, and then do basically a maximum likelihood estimate of what the community detection, the community memberships of those vertices are. So we have A T tau is the number of edges from vertex T to vertex tau. All right, so now we look at these. So we look at the attachment of vertices from So, t-bar is sometime after the largest vertex in V. So, V has some set of vertices. We don't really look at the attachments of what's going on inside of V, but from time T bar onward to some large T, we try to sort out what the community membership is there by taking into account the joint evolution of those, which really depends in the limit when we were using the theta state. And the limit when we were using the theta star instead of the empirical thetas, it depends on the joint distribution. So, just want to go over some numerical results about this. So, this is recovering a single community. Half of the vertices coming in are in one community, half in the other. They shoot out five edges each, and they are four times more likely to attach to a given edge in the same community as in the other community. In the same community as in the other community. And so this is T over tau. So I put this on a log scale down here. So think of T being fixed. And the shape of these curves, like maybe it's a conjecture. Actually, maybe for this, it's not a conjecture. These curves don't depend on capital T and the limit. So because this is based on analysis using that Z. Based on analysis using that Z process, just the branching process. The uh, and forget about the bodhicharia upper bound, those are computable things. And if this comes from simulation, the well, so I guess because that, okay, well, simulation indicates that the probability of correct recovery for a given vertex depends on when it arrives. Okay, so if it arrives in the first one. first one so t over tau is um less than okay this is error probably so sorry t over tau so so if t over tau is is really small so uh 10 to the minus three that means you're in the first one thousandth of the vertices that means a thousand times more vertices are going so you look at say first 10 Are going to so you look at say first 10 and then you have 10,000 times after that, then the error probability for those is going to be uh really small, and then um then so if t over tau is between one tenth and one, that means this is where 90% of the vertices are because uh t over tau being one tenth, that means one tenth of the way through uh the air probably is up to um Up to between 0.2 and 0.3, and then it goes, air probably goes up to a half, which is guessing, you know, towards the end. M is the number of edges that are shot out. So when a new vertex arrives, it shoots out m edges. So the new vertex is equally likely to be from the first community or the second community. It shoots out five edges. And it's four times more likely to attempt. Four times more likely to attach to a given existing edge if that edge is from the same community. All right. Let's see. This has the degree thresholding algorithm. That's this one. If you just, because degree thresholding is going to work here because the degrees of the vertices in the community are going to grow faster than the. Community are going to grow faster than the degrees of the ones outside. If you look at the, not just the degrees, but which children they're attached to, that's the screen one a little lower. And then if you run a message passing algorithm for this, we get this probability of error as time goes on. So that's comparing the degree thresholding with Show. Actually, this one also has. Thresholding with children. Actually, this one also has degree threshold, degree thresholding. See, wait, algorithm. Oh, this is children, degree thresholding. This doesn't have the degree thresholding, children, and then lower than that would be the message passing. Yeah, what is identified? What is exactly the children algorithm that's really going to? Okay, so we just look at the We just look at the set of children of a vertex. So, a vertex arrived at time tau, then you look at the edges coming out and where they're located. And it's a maximum likelihood estimate based on that. So the degree would be equal to M plus the number of children. And degree thresholding would just use that. But the children algorithm looks at who are the children. At who are the children? Yeah, the arrival times is equal to their identity. Yeah, exactly. So, so when the children, your children, this nurse saying you can't choose your parents. But these children do choose their parents somehow. So the children are the nodes that, yeah, which children? Which which children chose you as a parent? Yeah. I see. So it's more like, I don't know, how it's like a viable time poll by. Yeah. And because back in the this the likelihood for this just the branching process, there's this area under here. So Z so if you have to have a branching process, you know, time homogeneous, continuous time branching process, and you want to estimate what the growth parameter is. Estimate what the growth parameter is. You can do better estimating the growth parameter if you take into account the whole trajectory of growth rather than just how many individuals there are at time capital T. And the difference is that just the integral of the size of the population because that whatever that happened, you might have been unlucky not to have too many children early on. And then for given theta, that should also slow your growth. Data, that should also slow your growth, right? But if you like caught up, then it might not true indication of maybe the parameter is actually higher than that. So if you take into account the whole trajectory, which in this case means which children attached to you, not just the number of them, there's more information in that. Check their birthdays. Check their birthdays, right? Yeah, exactly. Okay. So, this is a two-symmetric communities. So, we didn't have any luck recovering two symmetric communities without seeding. I don't, it's probably possible because once you do seed, we actually get a pretty good performance. And maybe there's some other way, and we tried some other things besides seeding. Tried some other things besides seeding, pretty heuristic things. But when things like sunk in, we get pictures that work pretty well. So, this is for two equal size communities. Each new vertex shoots out five edges, four times as likely to attach to an existing community edge. We ran this like 1,200 times, took 10,000 vertices, and then we took the average of those and did some. Of those and did some averaging over like a window of length five or so to get this purple curve. So, this is a rough estimate. This purple curve would be the empirical estimate of the probability of error as a function of the time of arrival of a vertex to say, is it in this? We start out with two vertices, one's in community one, one's community two. One, one community two, and then you just want to see: are you in the same community as the first vertex or the second vertex? Well, well, you know, there's you can swap the two communities. So again, if you're in the last 10% of the communities, the air probably goes up to 0.5. To 0.5, should be going up to 0.5, guessing, but if you're in the first, basically, if you're in the first 1%, then so there's like 100 times more vertices arriving after you arrived, then you can pretty much tell which community you're in. That's if you have this seeded vertices, and then Vertices. And then, so this is the results of simultaneously trying to. This is the seeding. So you may say, well, we're cheating by seeding for 10 vertices. But if we run that algorithm I talked about before, where you look at the joint evolution of the first 10 vertices, then you can get probability of error, which is like less than one. This goes like 2%. It's better for the earlier. It's better for the earlier arriving vertices. This is the green one, and we call that the complete data where we look at the joint. You know, there's two to the 10th possibilities for the labels of the first 10 vertices, and we just we have two to the 10th likelihoods, and we take the maximum of that. So it's if we keep that as a fixed number, it's still going to be like polynomial time. And then partial data means we just ask, is a vertex in the same community? Is the vertex in the same community as vertex one? Yes or no? So we're just looking at two vertices at a time: vertex one versus the given vertex, yes or no. And that did okay. I mean, it has error probably going up to 10%. This is, by the way, with the 10,000, capital T was 10,000 for that one. Yep. Is it clear that that curve should go all the way to the quick five? Right, in the bottom of it. Because you know, if you if you make a reasonable guess for the you know, a chunk of the nodes, yeah, the incoming node has some different properties that are attached to those, maybe you can get some kind of not perfect, but yeah, some I misspoke. Yeah, I think it doesn't get yeah, yeah, you could probably, yeah, you could probably integrate, probably integrate this, and there's probably some, yeah, you're right, based on how accurately you can classify the nodes that arrive. Accurately, you can classify the nodes that arrive before you. You could get, yeah, you're right. Yeah, so that shouldn't go up to a half. In fact, it looks like it's going up to 0.8 or something. And then here, the blue curve is for which algorithm? Pardon me? Sorry, so the curve that you're seeing, like yeah, so first we didn't actually do it in two parts. We just, this shows that it's, it looks like it's possible. It looks like it's possible to recover the first finite number of vertices if you want. And then this one, we did message passing where we seeded the first 10 vertices with their correct membership. Sure. And then here. Using message passing. Yeah, and then here you assume that you know the arrival times. Yes. Well actually the message passing part doesn't use the arrival times. Passing part doesn't use the arrival times. Yeah, so but we seeded 10 vertices. So, yeah, to do that, you have to know the arrival times for those 10 vertices. Otherwise, all you could do would be like the degree thresholding I talked about before. Or you could see, yeah, you just know their degrees, right? Yeah, so this is assuming you know the evolution. So, yeah, so if you have an unlabeled Erdash. Yeah, so if you have an unlabeled Erdash-Renya graph with two symmetric communities, can you recover? I don't know. Also, if M is equal to one, you can't, you wouldn't have this part. I mean, well, this comes from the fact that the set of vertices could be hit by two. Some nodes coming, we're going to hit. Some nodes coming, we're going to hit two of those at the same time, two or more. So if m is equal to one, it's still possible to distinguish between communities a little bit. But that gets into the difference between the theta star and the theta empirical. Like if you want to do detection, say, are there two communities, symmetric communities here or not? You can still do some detection, but it's pretty subtle. Okay, this recovery of four symmetric communities. We needed to seed more. We seeded 20 of those, and this is the message passing. It didn't, this one didn't work, but most of the time it works, and the average is kind of like that. Okay, then just have one more pair of things to talk about. If we had recovery of three communities, I thought it was interesting to look at three communities where two of the communities are semantic. Two of the communities are symmetric, and the third community is there. And maybe the third community is a large community, and then the two other ones are smaller, or the other way around, there's one small community, and the two symmetric communities are larger. So it has a little bit of the symmetric two communities case, but there's actually a third community in here. So the message passing works pretty well if we had 15 seeds. This is M equal to 5. This is m equal to five. In the paper, are the beta matrices. I think for the two large communities, I think, or two small communities, I think it was four, two, two along the diagonal, and then ones off the diagonal. Okay, then this is for the initial seeding air probability. Now, maybe I guess this is the I guess this is the total error. So, the error probability: we looked at small errors versus big errors. This is across all the vertices. And a small error means that a big error is making a mistake between a large community and a small community. Because if you've got two small ones and one big one, their degree distributions, their degrees are growing differently. So, say if you're Differently. So say, if you can't get that right, we call that a big error. And then if you make a mistake between the two equal size communities, we call that a small error. So in this case, we have one low degree community and two high degree communities. So there's some big errors there because the low degree communities have errors, and with the two high degree communities, High degree communities, there's still some small errors, but that's smaller than the big errors telling the difference between the big community and the small community. And then we looked at a different story where we have one high degree community and two low degree communities. And so the big errors were: can we tell vertices that are in this big community or not? And that was pretty easy. So it didn't make very many errors there. But then the two small communities, they're symmetric. It's difficult to tell the It's difficult to tell the vertices apart in the small communities. So there's small errors. We're failing to distinguish between the two smaller communities as the dominant source of the error when we have that set of that model. So I think that's it. So these are some references. So this is a paper of James Jordan. It's called Geometric Preferential Attachment Non-Uniform Metric Spaces. So labels here could be a metric space, not just. So, labels here could be a metric space, not just a finite set of labels. This is a. Oh, look at that, Mickey there. I didn't know that. Okay, so this is a coexistence and preferential attachment networks. So, this is one where, again, the membership is determined by a vote rather than exogenously. And we could consider, you know, maybe a mixture of that would be reasonable where you come in and you have some. Come in, and you have some opinions or something like that, and then you attach, and then you take into account who you've attached to. So it could be different. But the thing about this is the network itself evolves independently of these community structure because it's just like an overlay. And whereas, and that's why probably a lot of know, you know, the Bola Basha Riordan has a really nice, if you're just going to do preferential attachment, you should really look at. Preferential attachment, you should really look at the Bolobosh-Riordan model because it has a lot of exact formulas for these distributions. But those things break down when you do the pressure attachment with communities like we did. So we just went back to the, oh, that's this one, this paper here. So we just went back to like Baribas-Albert model because it was easier to talk about. Whereas, so there's some really nice exact structure, and that's, you know, there's a huge literature. And that's, you know, there's a huge literature on that. There's a huge, even a larger literature on FOLIA urn schemes, which is all related. And it's also related to the Graphon work. So it's an interesting area, and there's a bunch of conjectures there. But in the end, how representative is a preferential attachment network versus some other high-degree network? I don't know. So anyway, that's it. Thank you. Thank you. That wraps up our workshop. Okay. The bus is leaving, and for most of us, at 10 minutes after, that's 25 minutes. And we're going to have boxes.