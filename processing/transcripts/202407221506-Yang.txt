Introduction. I'm Yang Tan from the Hong Kong University of Science and Technology. I always try to introduce the name of our university because the university in Hong Kong pay more attention to the ranking, university ranking. So we try to highlight the names. I'm going to talk about cross-population by mapping. This is joint work with my former PhD students, Min Chuan, and Ming Quen and the current PhD student Xi Wei and Jia Shun and my postdoc Xiang Pong. And we also have these industry collaborators. We have the East Asian data industry collaborators. And then let me begin with a little bit of backgrounds. I think more than 100 years ago, people have started to collect genetic data from families. Data from families. And Dr. Francis Golden collected more than, I think, 200 family data. He illustrates that genetics play an important role in complex truth like human heights. But people noticed that maybe family data is delined. And in 1996, Professor Nielrich proposed to combine Propose to combine association study with human genome project, and then we have the GBOS project. After the completion of the human genome project, and then we can do GBOS now. We have the genotype chips, and we can generate a lot of genetic markers, like millions of genetic markers for, say, like nearly millions of individuals, and then we collect those. And then we collect those covariates and treats, and we can perform GBAS. This is a very simple genome model. So, patient study mapping methods. Basically, we are going to run those GWAS for a given trait one SNP at a time to perform this kind of marginal association study. Typically, we are using linear model or linear mixed model or generalized linear model. Linium X model or generalized linear mixed model to generate this kind of achievement results. And one issue is that for those kind of marginal analysis, we are going to have this kind of pattern, say like for a SNP in the LD block. If one SNP becomes significant, and those nearby SNP will be also highly significant due to the Significant due to the strong LD impact of those kind of local regions. So, one task is trying to prioritize the causal variance from those highly correlated signals. The purpose here is like we hope after the prioritization with this kind of fine mapping to try to identify the causal risk SNPs, and then we can perform more biological understanding, say, like. Understanding, say, like, what are those kind of genetic regulatory roles of those kinds of genetic variants in some type of specific way or some kind of from gene to variance to gene to functional understanding. And we hope we can use some kind of further methods to validate those kind of fine map evariants, for example, by doing these crisp target functional validations. Functional validations. So, what are those challenges of those kind of biomapping? I summarize the three major challenges. The first, of course, is going to be the strong LDE patterns for the given local genome regions, such that we can, when we do those penal GRs, we cannot really tell which is the causal signal. So, this is an example from LDL trait, and if we do those. And if we do those dynamic, you can see those competing posterior inclusion probability is going to be quite small, such that we cannot see which are the true causal signals there. A second challenge is that in the genome-wide association data, even though in the summary statistics, there are still a lot of confounding bias. Of confounding bias hidden there. So we hope if we have a method, we can also correct those kind of confounding bias from the summary GBOS, try to improve the replication rate of those kind of fine-mapping results. So that's a second challenge. The third one is that for the given local region, perhaps there exists multiple causal signals rather than the single one. So we need some kind of computationally efficient methods to identify. Efficient methods to identify multiple causal signals. So that's the computational challenge. There are several fine mapping methods. I think started in 2015. So by fine mapping and painter and so on and so forth. I think recently people pay attention to this fine mapping because of this kind of sourcey method. Because of this kind of SUSY method. I think Yang Pao is here due to his very nice work to introduce this new methods into the fine mapping fields. And then we start to look at this problem by looking at the multiple ancestry information and correct the company files. And then we have our paper here, and we also noticed that Xianla. You also notice that she unlike multi-ancestry, SUSY methods, and also multi-treat, SUSY methods for this kind of biomark. Let me briefly introduce what are the key idea of our XMAP. This is my PhD student and our industry collaborators. The first key idea is that we need The key idea is that we need to leverage the genetic diversity of those multiple ancestry. This is one example for European, the LD pattern for European population, and this is the LD pattern for an Africa population. And I use this LDL tree as an example. And here, say, in the European populations, it's not easy to. Populations, it's not easy to distinguish the true call of signal, but based on the Africa ancestry, we can clearly see this kind of true signal. So, we believe leverage genetic diversity can greatly improve the power of time mapping. So, that's the first component. And then we start to build our models. Basically, the model can be characterized by the following three components. The first component will The first component will be the SUSY component, which assumes those kind of sparse causal signals. And we assume in our papers, the causal signal crosses different populations are shared. So you can see that across different populations, we have the same gamma indicator that indicates the causal status. But we allow the effect sizes of different populations can be. Different populations can be different. Oh, by the way, here we are targeting the same trait, for example, human height in population one and human height for population two. So we are targeting the same trait. That's for the SUSY component part. But when we try SUSY for this kind of cross-population mapping, we find that the performance is not very, very satisfying. Very, very satisfying because we need to account for those kinds of two more components. The one part is the polygenic component. This is going to make the algorithm more stable, and it can also help us to correct the compounding of bias by modeling the strength of polyaging and the compounding bias. And one important thing is that for our methods, we For our methods, we need to, as you can see here, how to distinguish this kind of sparse term and this polygenic term. So we need to use the genome-wide information to pre-estimate the polygenic term and fix the polygenic term there. And then the SUSY part can be easily identified. So that's the computational part. And we also want to emphasize that when we try to handle those benefits, When we try to handle those bang or summary statistics, we first write down the likelihood based model, and then we need to use this intercepted terms like C1, C2 from the LT score regression to adjust the compounding bias. This is very important we found for the fine mapping. This is going to improve the replication rate. To improve the replication rate of the defined mapping symbols. And then this is a summarized version. Basically, this part just converted from the individual level to the summary level, but we need to pay attention to the LD score, the interceptor term, to correct the compounding balance. And then we run those variational inference programs to calculate the posterior inclusion probability for this gamma. And then we get. For this gamma, and then we get a fine mapping result. Let me skip the parameter estimation part. And I also emphasize that we need to have the really computationally efficient algorithm. So we tested this. So for the given region, if there exists multiple causal signals and the computational algorithm, the computational time. Algorithm, the computational count will be linear to the number of caller symbols. So basically, that's the result. And let me summarize how the XMAP overcome the three major challenges in financing. The first one, say like the strong LT, so we are going to overcome this issue by incorporating the genetic diversity from multiple ancestry. The second one is for the confounding bias. We are going to Bias, we are going to use those kind of LD score models to correct the population stratification bias and model the polygenic effects. And then when we handle multiple causal variants, we have those SUSY models combined with the polygenic term to make the SUSY model even more stable, and then our computational algorithm becomes quite efficient. And then let me briefly talk about the result. Yeah, again, I'm going to use this LDL as one example. See, this is the fine mapping result from UK Biobank with 340K samples. And you can see for these two risk variants, the posterior information probability is quite low, just below 0.12. Below 0.12. But if the sample size keeps increasing, and we can see this posterior increasing probability will increase to nearly 0.6. But if we combine multiple ancestry, for example, European ancestry with Africa ancestry with 340K European samples and 90k Africa. Africa ancestry. So you can see the posterior inclusion probability will be much higher than that, simply increased the sample size from the European ancestry. So that's the benefit from genetic diversity. The second is about the replication. So when we design our method, we always try to think about how to verify our methods. Here we try to use the discovery code or So like the discovery code order from uh say like four hundred fifty K European ancestry and we have the uh two that the twenty K Chinese population but we use like say like 72 K European ancestry from the civil ship. Why we use this kind of data to the replication? Because we believe that this kind of cybership GBOS is going to less suffer to the public. Less suffered to the population stratification. So the signal is going to be cleaner there. But we agree that because of the limited sample size from this kind of sibleship, we agree that the power of this replication study is going to be relatively low. So you can see that for different methods, we can have defined mapping results, but the number of variants can be replicated by this. Can be replicated by this simulation. It's going to be relatively small, but we can still see the replication rate for different methods. And at that time, we can see that XFAP has the highest replication rate. We believe the population correct the population stratification, those kind of compounding bias, play an important role to improve the replication. Road to improve the replication rate. And at last, I'm going to show you one concrete example. Say, like, here, if we do not correct those kind of population compounding bias for this type of assets, then there will be some kind of false positive signals. But if we correct those kind of compounding bias, and then the false positive signal is going to go. So that's the main result. And finally, we And finally, we also map those kind of use those fine mapping risk values combined with the single cell data from the blood tree. And we can calculate those treat relevant score for the 59 result. And we can show the single cell relevant cell types from the single cell data set. So basically, by Cell data sets. So basically, that's the result. And we also make our software, publicly available. And just now I heard from some of the you guys already try our software and I'm happy that the software works well for you. That's the summary. And finally, I would like to thank all our contributors and our industry. Contributor and our industry collaborators. And at last, I would like to welcome you to Hong Kong, especially Hong Kong USD. We have a very beautiful ocean view campus, and you're welcome. Thank you. But Facebook would file it's mostly driven by the European, not the Iver. But also there's simply GLGC and Book Hubbard. How similar are two class responses? Here it's about the LDL from the LDL, right? But the LDL is also the LDR itself. So I think that for the algorithm it's very clear at this point. For the microphone, it's very clear at this point. Yes, yes, that's a very good point. So, like, here, I think some other friends also asked, say, like, how much of the signal is driven by European, or how much of the signals are driven by Africa. In this example, I think Africa studies play a very important role. It can help us to distinguish the two signals. Yes, I agree. But for some other study, because, for example, for Study because, for example, for height, the sample size from the East Asian part is quite limited. So it can just help us to distinguish the true signal from the distinguish the true signal. But it cannot greatly improve the power because the sample size of the East Asia part is quite limited. So, just a quick question. So, you give us this. Just a quick question. So, so you give us this PIP here. So, PIP is the closer probability of being a causal SNI for which probability? For both, or for one of two, or for both? Because we assume it is shared in our model. Okay, that the caudal status are shared among different populations. Why do we make this assumption? Because when we start our work, we notice that the easter pop population is the sample size. Population is the sample size are quite limited. So we do not have enough power to distinguish those population-specific signals. Maybe Xiang, when Xiang started doing his work, he has more sample size. You have simpler than California or maybe right. So which one do you prefer? I'm just curious. That's the constellation. Now it's not the same. So if you have searched out in file support, it's very hard to tech tech specifics. So it's very challenging and it's very big because you have colour. Because if you assume colour, you can model information across. If you have more samples from other ancestors, then perhaps you have better chance to model population-specific signals. In the interest of time, yeah, thank you. Okay.