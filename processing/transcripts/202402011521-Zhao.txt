Organization of this wonderful workshop and also for inviting me. I so much enjoy this workshop. I cannot enjoy more. I met a few old friends, and also I know a lot more new friends. I learned a lot about sensory covariates because I did a lot of work on missing data, especially Nygonobo Missing before. And I just realized there are a lot of connections, although they are different disciplines, but also there are a lot of ideas we share with each other. Ideas we share with each other. So today I'm going to talk about something that is under the big umbrella of distribution shift or domain identification problems. In the previous talks, if you listen to Jim Bo's talk and also Leila's talk, they also talk about something on if you have two studies, you rarely have more information on one study and you're interested in the other study, study A, study B, where you have a biased sample person. You have a biased sample versus that you have a representative sample. How are you going to do that? So, my talk here is also very relevant. And before I give you more background, I will thank my collaborators. This is a collaborative work with Mi Man. Thank you very much for putting all the efforts into this paper. And the first author, Singh Hao, he will become a faculty member at University in Korea next month. So, this is the background, and we gathered. Background, and we get into this problem by reading a lot of conference papers in computer science. And they call they are pretty good at naming all the stuff. Although we are doing the similar things. They call this a domain adaptation problem, and they also invent a lot of terminologies on what type of domain adaptation it is. So, why it is domain adaptation? Well, you know, as a statistician, a big group of data scientists, let's call Big group of data scientists, that's called data distribution shift. So you think about normally, we have training data and test data. Every time we build an algorithm, we build a model, we train this model using the training data and want to evaluate the performance in test data. And we actually never carefully think about what is the difference between the distributions of training and test data. It could be different, right? If their distributions are different, If their distributions are different, somehow we need to think about how we can take care of this difference. Because you assume when we do this type of training and evaluation, we automatically assume their distribution is not the same. And more broadly, if we think about there is a source data, source population, and also targeted population, you can think about Leila's talk. And you have a lot more information on one sample, on one population, for example, a hospital in Boston. And then you have fewer. And then you have fewer patients in the hospital, for example, in Madison, Wisconsin. And we know that their covariate distribution might be different. Or you can think about the Florida residents versus New York City residents. However, maybe they are just working on the same disease, like pneumonia, like lung cancer. So these two populations, they are different for sure, but they must have some interrelations. So then the approach is we want to make all different types of is we want to make all different types of assumptions on the relations between two populations. The whole idea is we want to borrow the data and the information from the source so that we can do better on the target or we can do something that if we don't use in this source, we cannot do it. Okay, so the setting I'm talking about today, I'm focusing on today, is the so-called transductive transfer learning. By the way, I want to clarify, I know that people hear everything about I know that people hear everything about transfer learning, domain adaptation, data integration, data fusion, but I personally feel all of these terminologies are so heavily abused. So when you see such a term, be careful about what their setting is. Maybe different people use the same name, but they are doing totally different things. Okay. So the setting I'm talking about today is called Transductive Transfer Learning, or it's called Unsupervised Domain Adaptation. So if you hear about Application. So, if you care about it, let me first explain what this setting is. This setting basically assumes we have all the data from the source. In a simple case, we have all the X and Y information from the source. But in the target, we only have the covert information. And I use the gray color to denote this Y, the outcome in target. Basically, we do not need the Y information in target to build our own procedures estimation approaches. Approaches. If you do have it, if you do have it, like the example I will show later, we can use that as a benchmark estimator to evaluate how our method works. So if you care about this setting, in 2019, a few years ago, there's a very excellent review paper published by two computer scientists at the IEEE TAMI. It's pattern analysis and machine intelligence. It's a very topic. Intelligence. It's a very tough journey in computer science. Okay, I also want to make sure that CS people work on the similar problems, the similar setting, but there are two very different, distinct differences. The first one is they care more about prediction. The prediction basically we mean is target. What is the distribution of white data packs? So they use all the approaches to do the prediction. And they talk less about. And they talk less about model, model means specification, what type of quantity we can estimate, and what is the uncertainty that when we estimate those quantities. The other thing is, we talk about the label. What do they mean by label is the y is just discrete. For the simplest case, y equals 1 is 0, or y equals 0, 1 to up to k. They seldom care about the so-called regression problem in our terminology. Okay, so there are two distinct differences. So, there are two distinct differences. So, to make sure our notations are clear, so in this talk, we will use y and x to denote the algorithmic covariant, or whatever you want to call, response predictor, label, and feature, output or input. And because we have both source and targeted populations, we introduce the indicator, r, to denote whether it is the source, you know, r equals 1 or target, r equals to 0. We use p and q as P and the Q as the notations for source and target. So we also use the sample sizes, n1, n0. We use this. This type of notation, it means the marginal distribution. So this is a margin of y in source. Qy is a marginal distribution of y in target. And this is a conditional, this is the expectation. And also pi is a proportion. So this is the n1 divided by n in the sum of the two sample sizes. Okay. Okay. Any questions with this setting framework? Okay. And then let's think about how we can what type of assumptions people can make. Well, people just make whatever convenient assumption that they can. So in the literature, there are two main assumptions. The first one is people call this is covariant shift. This is a very meaningful name. That basically means we assume between the two populations, y given x is the same. Okay? And the marginal value. And the margin of x, this is a marginal distribution of covariate. They are different. So that means covariant shift. But the more important thing is they assume this y given x. They are the same. So on this slide, I write it as a conditional density. But in a lot of other situations, people just assume that the conditional expectation of y dimension are the same, depending on what type of problems people are working on. Well, this is not our focus. Well, this is not our focus, but basically this is a lot more extensively investigated from 2000. Actually, this paper is a paper published in the statistical journal. JSPI is working on something about the density ratio estimation, but people didn't realize its impact, I think, just in recent years, until in recent years. So the other one is we call the label shift. Well, it's a parallel. Well, uh it's a it's a parallel assumption. So that means x given y, we assume it is the same, but the margin of y is different. Because CS people call this is label. So the distribution of labels shift different. Okay, and also you can also create some examples for illustration. But I want to point out there are some, you know, every assumption could be interesting and for different applications. Particularly for label shift, you can think about the scenario. Suppose the source Think about the scenario. Suppose the source data is: we care about the proportion, the probability of developing pneumonia in a non-flu season, and the target is the probability of developing pneumonia in the flu season. So suppose we know a lot of more information during the non-flu season, we want to see what we can do for building a model in the flu season. And the X variables are just the symptoms of the model. So no matter if it's flu or non-flu. So, no matter it's a flu or non-flu season, once this person develops pneumonia and their symptom distributions are somehow the same. Okay? Well, of course, we can argue, but that is assumption. We assume no matter it's flu or non-flu season, if the person develops pneumonia or not develops pneumonia, the X distributions are pretty much the same. However, it's very clear that during the non-flu season, the probability of developing pneumonia is a lot smaller than during the flu season. So this is the In the blue team. So, this is the Martin distributions are different. Okay. So, we work on label shift and how the literature looks like. As I said, most of the cases, most of the papers that didn't list their journal conference, most of them from the CS conferences. And you can see most of them are for the discrete wise, which is really means labels. In our terminology, it's like classification problems. Problems. People propose different ways: maximum likelihood, moment matching, non-parametric. For continuous, it's just that we only find the two papers before our own work. There's a kernel mean matching and also some time matching assumptions. In all, what is the main difference of our own work is here, I wrote. We introduced a notation called a rho y. That is the difference between the two densities. So because we do not have the y data in q in correlation. Y data in Q, in correlation Q. So, this quantity, right, because you don't have the data, so it's arguably very difficult to model. Or even if you can model, it's pretty much very possible to misspecify it. But all of this work, they didn't really care about whether how we are going to overcome this type of difficulty. So, let me just briefly summarize what is our just a wrap-up. Wrap up, sorry, just to summarize our major contributions. So, we take a very different approach, which is basically we take the geometric, semi-parametric approach. And our main contributions distinct from the existing literature is we do not need to model this point with Go Y. As I explained, we do not have data in QYY, so this is very hard to model. But our procedure, we do not need it. And we focus on estimation. And we focus on estimation problems. This is trying to distinguish from the CSP, right? They do prediction and we do estimation, yes. I think I probably missed something if I'm confused by this, but if you know the distribution of X in the target, so if you know the distribution of the target. Y or X. Sorry, if you know the distribution of X in the target, and if you know, if you've assumed something about the conditional distribution of Y given X, then I. Then I guess I'm kind of missing why this is so hard, right? Yeah. So we assume that the share, the two population share, is x given y. Yep. And then even if you know x, it's not directly straightforward to estimate the y, the margin of y. But isn't it just by definition of the you can derive the relations. It's not that strict. I think I'm just going to. If it is a y given x, If it is a y given x share, that is very straightforward. But if it is x given y, it's not. Oh, I see. Yeah. Okay. So our approach is, you know, we work on the estimation problem. So this whole setting is very CS where you know machine learning, but we work on this estimation problem. We estimate the general characteristic of population Q. The simplest case is the mean of Y or probability of developing pneumonia in the flu C. Developing pneumonia in the fluid season. And we can, you know, technically, we can work, we can, our procedure works for whatever characteristics of the population one. And our procedure can accommodate both discrete or continuous. We don't care about whether it's really discrete label or a continuous one. And also we have some very interesting discussions on the model missetification and also the theoretical results. Okay, yes? So could you go back to the label sheet? So, could you go back to the label shift slide? It's our kind of pretty. Okay, so here it's okay if P of X is not equal to Q of X, right? P of X. Is not equal to Q of X. That's right. Definitely not equal. And then also that Py given X is not going to be equal to QY given X. Right, right, right. You're right. So in label shift, the only thing equals is this. The only thing equals is this, the x-bay. Any other yes? So, on the slide with the current literature, you mentioned that the goal is kind of to calculate or estimate rather that ratio row. So, once you have that row, what do you do with it? So, i in our case or in the researcher? I guess in your case. In my case, is uh you know uh our our final goal is we're estimated the mean of y in the target. The mean of y in the target. So you can regard this row as some kind of nuisance. So in general, if you want to estimate the mean of y in the target, you will have to know this quantity. So we are going to propose something that you don't need to know this quantity. So knowing the row there will let you estimate qy? Is that what you're saying? If you know the row y, you can straightforwardly estimate the qy. I will show you. Any other questions? Okay, so now I will continue. Okay, so now we'll continue. Okay, so over here. Yeah, so the methodology. This is our likelihood function. Well, this is a format I want to point out. This is very similar. This whole format is very similar to sensor covariate, right? So this part is not sensor and this part is sensor. But we do not have the lower bound, the upper bound. But if you have a sensor covariate, you will integrate within some intervals. But basically, our R here means this part is. R here means this part is from the source data. Source data, we have everything. But in a target, we don't have y, so we have to integrate to 1. So this slide is, we want to introduce some technical condition to guarantee the model is identifiable. So our approach is we borrow the completing this condition. And this condition was very popularly used in measurement error problems, missing data problems, and also in econometrics. This is not something we invented. So it's just a script. So it's just a speech. So to be clear, this is our target of interest. We work on a theta, and the theta could be just defined using this as median equation. The simplest examples include both the mean and also the quantile, the median, any quantiles. So our parameter of interest, if it's mean, we can explicitly write in this form. So this form combined with this likelihood together. Combined with this likelihood together is very important. So basically, this is a likelihood function, but it's a totally non-parametric likelihood. So in this likelihood, we have this x given y, we have data to estimate. We have py, we also have data to estimate. We have this rho y, well actually these two terms is like qy, we don't have data to estimate. And our problem of interest is SATA, it also just has this several components, x, gamma, y. Several components, x, gamma y and p y, and also no y. So if we put these two pieces together, this is a particular problem using the semi-parametric framework. So there are, in general, two types of problems in semi-parametrics. One thing is, another thing is, you know, if your parameter of interest is already appear in the slide. But here, we don't, it's slightly different. But the main idea can be summarized in this type of Can be summarized in this type of graph. So we consider all the possible approaches to estimate the private house of interest. And then because we have all the different nuisance functions, either x given y and p y that we have data to estimate, or rho y that we don't have data to estimate, we can derive this lambda space. It's called Nielsen-Tigen space. And by some ossebonality, we can derive this lambda perpendicular space. This space, it literally includes all. It literally includes all the possible ways, all the estimating equations for estimated C. And in particular, if you can derive the efficient score or efficient influence function, that is the most interesting one. So in general, in simple problems, this IF efficient influence function can lead us to the most efficient estimator of a permanent entity. Okay. So this is just a very rough. Yes. So the grain link would be other estimates. Green lane would be other estimators that you could use, but because the red lane is the shorter, it's the most efficient. Right. Right. But it's not always the case that we will have to pursue with this red one. Sometimes the green one is also interesting. It's actually, in this work, we use the green one, not the red one. Any other questions? Okay. So this slide is very important. So let me first explain. I will try to be slow to explain what is this. To explain what is this. So, this is the EIF, basically the green line in the previous graph. So, you can check, right? Again, the R part is from the source, one minus the R part is from the target. So, we have two functions. One is rho y, the other one is bx. For any audience who is familiar with like missing data and causal inference, this looks very familiar. It's just that we have this rho y, you can imagine like the proposed score model. And this bx is like the conditional mean model. But here, it's very different. Model. Okay. But here it's very different. So we basically, in a standard missing data causal inference, we either need this row y is correct or this bx is correct as a conditional mean function. But here, we can think about if the rho y is correct, right? So this bx could be anything. What I mean is, if you check this expectation, once the row y is correct, you can choose whatever bx you want to make sure it is mean zero. It is mean zero. And on the other side, if this rho x is wrong, is a round model, this bx is going to be complicated. Actually, it needs to satisfy this condition. So for any bx, once this conditional mean, even y equals to y, you can check the mean, it still keeps the mean zero property. But this type of restriction doesn't appear in the standard missing at random and also causes methods problems. And also, well, we will have to specify. Well, we will have to specify what is the optimum Bx within this type of family. It is this Bx. Well, this one looks a little bit complicated, but this will guide us. And this Bx, particular Bx, will introduce us some robustness properties so that we can move forward. Okay, so in the end, think about this efficient influence function again. If we know rho y and bx, If we know rho y and bx, if you know their two models, you can just plug in, you know, empirical variant equals to zero to get the estimator of zero. If we do not know, if we do not know, our story is now just give me any arbitrary model of rho y on any arbitrary model. And also, just give me any arbitrary model of how to evaluate this condition expectation of dp, something given x. This evaluation. X. This evaluation and this evaluation, it just relies on the model of X given Y in the source corporation. You can just simply fit a parametrical model of Y given X in the source, but just any working model. But the only thing I require is I need to solve this equation correctly. In a later slide, I will show you this just becomes an integral equation. So any two working models of this rho y and also of this EP, conditional mean function. On conditional mean function, both of them could be misspecified. But the only restriction we need is this rad equation, we will need to estimate it correctly. So this whole story is a key of this talk. And as you can see, it's different from the missing data and also causal inference. In the standard literature, in missing data, people call double robustness, because in that framework, these two working models, these two nuisance, you need at least one of them is correct. The one of them is correct. And then, if both of them are correct, it achieves the semi-parametric efficiency. But now we also have two nuisance working models, but we have this radio restriction. However, our story becomes two. Sorry, this row y model and also this conditional mean function of y d max in the source, both of them could be misspecified at the same time. That's why we named this double flux model. So try to distinguish. So, try to distinguish from the W robustities. Okay, so this is the, I just try my best to talk about the key idea. So, I'll introduce what is the W robust. I'll go back to this slide in a minute. So, this is the algorithm. I think once you see this, the procedure will become clear. This is the input. We have the data from P and Q, and then just adopt a working model, adopt another working model. This step is just the compute, just the plugin, very easy. And this step I will. Very easy. And this step I will explain in the next slide. We need to solve the integral equation. And then again, just the compute, and we'll get this output zeta hat. In the paper, we can show this zeta hat is asymptotically consistent with zero. That basically means this is an unbiased estimator. It's a valid estimator that we can use in practice. Okay, so how this integral equation looks like, it's a little bit complicated, but basically it's a Fredholm integral equation of the first type. You see, there is a kernel dungeon. First type. You see, there is a kernel function here, but this is just a one-dimensional kernel. The reason is, you know, we have, we need to approximate this conditional expectation of some function of x given y. Because this y just the one-dimensional. So this type of kernel, you don't have to worry about per se of dimensionality, all those stuff. It's just a one-dimensional kernel. And just to solve this integral equation, and we can get back to the algorithm to output the factor test. And this part is about the technical details of how to. Part is about the technical details of how to solve this kind of integral equation. The good news is for this type of equation, it's all well handled by applied mathematicians, another group of data scientists. So we don't have to worry about it. Okay, so let's get back to two slides back here. So this is the doubly flexible estimation procedure that we mainly propose in this paper. And the main features is we bypass the estimation of this row y. Of this rho y. And the double, again, what do I mean by doubly flexible? These two models, rho y and the conditional mean, or conditional density of y dimmed in the population p, both of them could be basis basically. Okay, and also you don't have to worry about our non-parametric estimation step here because it's just the one-dimensional. So we just use a very basic non-parametric regression technique and use the one-dimensional curl. And so I'll move from there. Any questions? Any questions? Okay, so maybe it's too early question, but uh so what if I want to estimate y given x, but do the earth brings something? The y game x in the target? Yeah. Okay. So we didn't do that in this paper, but we have a different work is suppose you know you can work, you can estimate this row y a lot better than the previous literature. And in terms of prediction, in terms of estimating y given. In terms of estimating y given x in the target, we can also beat a precision temperature. So we call that this type of estimation-powered prediction. But would you beat your estimator that is using a rawly specified? That setting is slightly different from here. So this doubly flexible somehow cannot be used in that framework. Because the other one we knew we really need an efficient estimator of the row y. Of the raw line in order to beat the previous prediction models. Yeah, but we do have something I can talk to you later in the model. But again, this work, you know, our ultimate goal is just the estimation problem. We don't do prediction. It's just the thing about we want to estimate something in the target. If you don't have y in the target, we cannot do it. Okay. So technical details, most of them are pretty standard. And the one-dimensional kernel, we need some asset. The one-dimensional kernel, we need some assumptions on the bandwidth H. So, again, this N1 is the sample size in the source, and the N is the total sample size. So, this is very common conditions. And so, for example, if N and N1 are in the same order, we can choose N1 equals to, or we can choose H equals to the N one to the negative a quarter sorry, one third uh power will be uh work that uh satisfy both situations. So basically uh it's not a problem at all. Basically, it's not a problem at all. Large sample properties, I will highlight, you know, the first one is, oh, I forgot to talk about the data. Data is because we will choose any working model of y given x, right? So you can just simply fit a purely parametric model. So this data is the parameter in this model. And our assumption on data is just to satisfy this root n1 consistence. So like MLE moment. Like MLE moment-based estimators, they all set. And then we will have this osmotic variance, you know, for any choice of these two working models. This theory guarantees the osmotic normality of the state hat that we propose. If you are very lucky or this is more interesting in theory, if both models are correct and we have this as part of the variance, then you can see this is the efficient influence function. But one thing I want to point out. Function. But one thing I want to point out is you realize there's a pi here. Pi is just the n1 divided by n. We write in this way, you want to highlight, no matter what, this theta hat is just the root n1 consistent. You cannot increase. This converted read cannot increase, even if you have a lot more data. Even if you are in the total sample size is much higher than on one. Why? It is because I have some reasons here. But the intuition is, you know, for in terms of Is, you know, in terms of outcome Y or label Y, we only have N1 observations from the source. We don't have any data, any information of like targeting. Even if you have a pretty large N0, it can help us to understand how the label shift maximum is, but it cannot increase the competence rate. Okay. And actually, this is very interesting. In the data analysis, I can show you some very interesting observations. Okay, so that's for the WF. That's for the doubly flexible estimation, and we have another one is the singly flexible. Just to think about, right? What is the difference? In a doubly flexible, there is a conditional mean or conditional density of y d max in the target. But we have all the data, right? So why not just you do a better model in terms of this white demax in the target? For instance, you can run a deep learning model of this white X, right? Random forest or whatever. That is our motivation of this single reflex. Of this singular flexible. I will be quick. The only difference is in the second step, instead of just choosing a working model of this white linear X, we are going to propose any non-parametric or machine learning algorithm to fit this kind of conditional expectation. Of course, we have some conditions later on. All other procedures are just the same. So the theory is we need this type of M1 negative a quarter rate with a little OP. And if you And if you are familiar with the double machine learning or debiased machine learning in China Zhuka's paper, they also have similar restrictions. Basically, they said in the literature, like random forests for some type of neural network models, some high-dimensional model, they also expect this type of requirement. Okay, so we didn't dip further into choose a particular method to do this kind of estimation. We just give this highlight couple. We just give this highlight computer. And then we also have the similar theory. And in this situation, if the row wire is the correct model, we can get the same result as before. Okay. Simulation studies. Okay, you know, simulation studies just everything works perfectly. So I think in terms of, I just want to make sure, I just want to highlight what type of estimators we compare. Estimators we compare. We estimate a so-called naive. You see, this naive is just to use a row y. But if you know the row y correctly, it becomes the shift zero. So this method is correct. But if your row y is wrong, this red method is wrong, right? It's biased. And this green and blue is we propose the double A and single A. They still use raw models, but they can give out as consistent aspects. And these are just for comparisons. This double A and single A Comparisons. This double A and single A deals are the two models. So basically, in theory, five and a six, they will both achieve the same interaction efficiency. Okay? I will skip all the details. You see, this is the H bandwidth, and this is the bandwidth in the kernel. But basically, this is the optimal order: 91 over 4 plus the dimensional of the X. We use 3 dimensional, so it's 91 over 7. I skip all of these details and all of this. And all of this. And this result, I want to just highlight: you know, if we compare these three, it's very clear. If you use the wrong model of rho y, this is biased. Even if you increase the sample size, it doesn't mitigate, right? It doesn't become smaller. But for the red and the blue, sorry, for the green and the blue, it works perfect. And for this, if you know the true model, of course, every one of them could be unbiased, but somehow the shape. Unbiased, but somehow the shift dependence seems like it needs a bit more sample size. And doubly flexible with the correct model is going to be more efficient than doubly flexible without the correct model. And similarly flexible. So basically everything is just match with our theory. In this type of simulation, we consider the same sample size of the source and the compilation. So there's no difference between NY and N. So you can see the Oracle estimator. So you can see the oracle estimator is always the best. This oracle is, you know, we generate the y in the target. We just simply report the empirical average. So this is the most efficient. I skipped the meeting. I really want to talk about the data application. So we use the Mimic 3 database to do some illustration of our method. The outcome is a type of so far score. This is the range from 0 to 24. I think the larger the number, it means I think the larger the number, it means the patient's situation, ICU patient is worse. And we include 16 covariance in this data analysis. So what drives the difference of the two populations, we use the insurance type. The source population is the private government and self-paid insurance. And the Q is either Medicare or Medicaid. So our argument is no matter this patient is within the government private insurance or within the Medicare. With the insurance or with Medicare, and once they, once you know, their so far score are pretty much the same, and their symptoms, sorry, once they're given the similar sofa score, right, given the value of one, all of their covariance distributions are pretty much the same. I also skip all of these details. So, this is, you know, this is in real data. We do have one target, so literally, we can do some testing of whether the Some testing of whether the label shift problems really exist. But you know, impurity, this p-value, we cannot reject the non-hypothesis. So what is the result? So this oracle here, again, is the sample average, just taking the y values from the target. So this difference here is all the three methods compared with our. So you can see green and blue, they have very small differences. You can just understand. Small difference. You can just understand why I'm biased. But shift dependent, because we do not know, because this is like the star model, right? The row model is a row, star model, it's raw model. So this is pretty biased. In this situation, the Oracle estimator somehow is still more efficient than these two estimators. And in this one is on the quantiles. We studied a few quantiles, 10, 25, 50, 75, and 90. Again, the data, the results. The results structure are similar. In some of the compiles, the shift dependent with the star model is very heavily biased, like 25, 75, and 90. In 10% and 50%, I think the reason might be, you know, in these two compiles, the row star model is somehow very close to the underlying two model of the model. So empirically, we didn't see a bias from the shift dependent. But the two estimators that we But the two estimator that we propose is always very, very similar to the Oracle estimator. And one very important, one very interesting observation is: you see, in some situations, this Oracle estimator is less efficient than the estimator we propose. The reason is in this data application, the N1 is much bigger than N0. So I think this is a very interesting phenomenon. Basically, if your N1 data is much. If your N1 data is much bigger in terms of sample size, even if you can get the estimator just from the targeted data, but you remember it's with the N0 to system. But if you don't use that N0 data, you borrow the information from N1, in the end you can get it with the N1 consistent. If your N1 is a lot bigger, it can improve the efficiency even more efficiently than the horror cost. In theory, basically, if your N1 is in a higher order than N0, In a higher order than zero, you can get an improvement of the copper. Okay, so take home messages. You know, this is a label shifter problem, but our main trick is we propose flexible, especially doubly flexible. And the role-wide plays a very important role. It's like our working model. And we have a lot of more direction that we want to work in the next few years. Where is this paper? This paper, you can use this. This paper, you can use this archive number to find more details. You can scan this code. And actually, we just last night we submitted a minor revision back to the journal. It's under Johnson. Okay, I'll acknowledge my founding agencies, and thank you very much. Want to go first? Yeah. So I think So, I think I may just still be kind of stuck on weird things, but I'm really struggling.