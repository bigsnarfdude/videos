Good morning. Let me begin with my big thanks to the workshop organizer for putting together such a great workshop and for including me. So my presentation today is based on joint work with Dr. Swasichek and Shung. Okay, so I have a very simple outline. I will begin with an introduction and then present a recent project in the area of large administrative health data analysis. And then I will share you. And then I will share you some of my thoughts about administrative health data analysis. So administrative health data are readily available, especially in Canada, because our healthcare system and it's universally accessible, government sponsored. So different Canadian And different Canadian provinces have their medical insurance databases that have very, very detailed information on the individuals' health care. And we also have like disease or patient registry, BC, British Columbia, and this province, this Canadian province. And for example, it has. And for example, it has its own cancer registry. And there are many different administrative health databases available. And also nowadays, many people have got interested in using EHRs, such as lab test results and so on. And also administrative records of Records of COVID-19 infection and the related nowadays are everywhere. It's a lot of information and even in a given administrative database and it affords you to look at or to analyze the data from different perspectives and may allow us answer. Allow us to answer different questions. So, many attempts actually, I mean, there are many attempts to use those data to achieve different particular research aim. Actually, there's no free lunch, and the challenges are first usually at administrative. Usually, administrative data are large and they have some issues very similar to big data issues. And they are large in size. Some of them probably also include many variables. However, because they are administrative, so and the database usually studied for administrative For an administrative purpose, not for a particular research purpose. And even for very simple questions, like, what do you mean by the time? It's not that easy for us to kind of make a choice. And not only that, to answer a research question, you may find the available data, administrative data, are not in the ideal Not in the ideal format and like IID observations. So, and they may be viewed as left and or right sensored data. They may be viewed as truncated data. It depends on what population, the target population is. So, I'm going to show you three. Show you three different research programs using large administrative health data. The first example is associated with BC Cancer Agency. And now they have a big cancer survivorship research program. It is started from a relatively smaller survival program called the CAAC program. Payad program and with focus on risk classification, assessment, and prediction. And now it has become a large, comprehensive longitudinal cohort study. The second program is led by my friend, longtime collaborator Rhonda Rausek with the University of Alberta. With the University of Alberta. And she started with records of mental health-related emergency department visit presented by Alberta residents younger than 18 years old using Alberta's ACCS data. And then now she has included much. Included much more information and big data like from national wise and from CHI. The further example is relatively new and is led by Dr. Bodan Nozick and it's on clinical management of Management of opioid use disorder and associated with BC Center for Excellence in HIV-AIDS. So, and the program actually has been using seven different linked population level administrative databases. And I listed the names and the year and the degree of my graduate student. And they were on or they are currently on those three different programs. You can tell and administrative health data actually have generated many interesting statistical Teen statistical problems. Otherwise, they wouldn't be able to get their degree. So, today I'm going to focus on the second program, which is mental health related and emergency department visits. I want to showcase what we do with administrative health data. So, I will begin with our formulation and then move on to present two different regression analysis. And I will show you some numeric outcome of the project. So, the goal is to The goal is to assess the need of pediatric mental health care and to improve the care system using mental health-related ED visits generated from individual residents of Alberta under age 18 years old. The specific aims in The specific aims include to evaluate the frequency of children and youth mental health ED visits, to examine the effects of risk factors and exposures, and to identify important covariates and time train. So the quick reaction we had is: okay, then we just need to come back. Then, we just need to conduct some analysis of recurrent events because mental health ED visits are recurrent. So, we checked what information is available. And so, in Ed Randa's lab, there were all the racks. The old records of mental health ED visits from Alberta residents younger than 18 years old during 2002 to 2011 as the first data set. And she managed to have second data extraction that is second time period from 2010 to 2020. 10 to 2017. Each record includes the starting and the ending date, the time, the age of the subject in year, a service, and some other information, including sex and proxy, socioeconomic. Socio-economic status and residence, region, and so on. And only the second set of the data includes birth date. So how should we summarize or process the data and to achieve the specific goal? Let's start with. Best style is taking individual and those different subjects as the study unit to begin with. So subject I, individual I, or a patient I, and I of T is the cumulative count of the MHED visit up to time t. And that I of T is the And that i of t is the potential covariance at time t to begin with. So, the question is: what is appropriate time? What do we mean by tea? What kind of covariate or time-varying covariate should we consider? And what a model and can be And can be appropriate, and what inference and can we combat using the data which is meaningful? And also, and it is feasible to make with the current data. So, what the information is available? Available and let's focus on the individual MHED records to begin with. Let's assume all the individuals are independent. Let's consider only external time independent covariates. So it looks like A. It looks like age is a natural, appropriate time to use. We cannot use the time, run that style and get the data, the data extraction time. It's not meaningful. It's different from a clinical trial. Naturally, you take time zero is the time and the person is recruited. Is recruited and studied the treatment. So then we use age. Now we consider zeta I and the covariate is not with an independent covariate. And because data extraction window is in Canada time, so if we use age and then each And then each subject has its own window. That's called CLCR. They are determined by, or because we were only interested in the subject under 18 years old. So this, the left and the right limit of individuals observation depend on not just the extraction window. Extraction window, they may also depend on the individual's birth state, BI here. And also zero, of course, and the person, after person's born and 18 under 18 years old. And also, as information, very detailed information on even up to minute. Even up to minute on the time and the subject started the visit. So I call it by T I J, subject IJ recorded E D visit. So the age actually is recorded in year. So year one and one year old and two year old and so on. So Tij minus birthday then The IJ minus birth student is the age of subject I8J visit. I borrow this diagram from Angela. She's not here. I find this is a very nice diagram to show what do we mean by different individuals have different kinds of observation window. You can see subject. You can see subject one's observation window is in between and still in between WLWR, the data extraction window, but in his own time scale is WL minus his birthday to WR minus his birthday. Then for subject three, for another example, and his observation window, then it's WL minus his birthday. That's and he. Thursday, that's and he became 18 years old before WR. So then his observation window is WL minus his birthday till 18. So if we present the available data and provided everybody's birthday is available, then you can see this is a collection of Collection of observations on a piece of a counting process. And it's a county person within this observation window individual eye, so collect all of them, the plus sate, the covary. We call it back, it's like we have a doubly censored recurrent event information. And I use O1 here to indicate, it's a set of all the individuals, and we have their records, mental health record. So for some, the first roundas data said birthdays are not available. If birthdays are not available, you can see now they just You can see not just for the individuals' observation time window, you do not know when it is start or when it ends. And correspondingly, the day you do not know what you observed, you know, the corresponding counts associated with exactly what year. Not only that, I mentioned. I mentioned I use O1 and for include all the individuals and they have ED visit records. O1 may be viewed as a sample from a population. That population includes all the individuals that have at least one ED visit. I call it by P1. So that P1, you know that apparently is the biased sub-population of the general population. If we take that general population contains the sample O, which I used as all the Alberta residents under 18-year-old. The 18-year-old during 2002-2017. So if we target MP, the general population, and then the available MHD data actually are truncated data. How to make the inference with the truncated data in it? The data and it on that general population. So, well, people will say, okay, let's model the trampoline to figure out how to overcome it. So, we kind of took a different approach and we considered to make use of the information. Of the information from the register of Alberta kids, which who didn't present the time period and it mental health for mental health. So that is okay, that is the count of them doing their own observation. Their own observation window for ED visit zero. That effect actually is there because of the Canadian healthcare system and it's pretty safe to assume if a person had an ED visit and then we have the record. All right. So then the information available is in addition to mental health ED visits, we use that effect that is the other young resident in Alberta didn't have mental health ED visit during the time. And together, if we use some population A population level demographic information on Z, and then we may manage to combat analysis and the target on the general population. Okay, so and this is This is one of the projects related to many health ED visits, and it's a two-sample problem. And we can see that to make a comparison, health ED visit due 2002 to 2010 and compare to the one during 2010 to 2017 because one is an earlier, the other is late. An earlier, the other is a late decade. And going to Star Wars, just to view the available data, Mental Health EDWA data has doubly censored recurrent events data targeting on P1, that biased subpopulation of the general population. Let's start with that. And then we're going to move on to conduct analysis of the truncated recurrence. Of the truncated recurrent event data and using population sensor information and targeting on the general population sheet. So we consider such a marginal model, that is the marginal rate. The marginal change of the number of observations at HA, conditional on that I, the time-independent covariant we identified, and XI. XI is indicator for subject I from late decade. We consider to form to model that a marginal. But to model that marginal conditional rate, assume it's in such the form, and probably you guys all are familiar with. We just instead of consider the regression coefficients are constant, we consider they are age-varying to begin with. This model This model actually is equivalent to the stratified and model if we consider earlier and the late decades and separated. Then you can see the model we consider is exactly to the strength. Yeah, okay, and with the correspondence of the regression coefficient together with the baseline function, and yeah, I just present it over here. And we may consider different specifications of the model. For example, we could choose all the regression coefficients to be constant. That model then reduces to a common. To a commonly used proportional mean and the rate model. Many people refer to as Anderson-the Gale model. And if we focus on P1, that biased sub-population, and come back to our analysis of the double listeners of the recurrent events. Events and consider use and the local partial score function as our estimating function. And then we can carry out our estimation and obtain an estimator for the age-varying regression coefficient. Coefficient. And of course, we need to take care of the first data set doesn't have birthdays. We need follow who and the Rosichek's paper, accommodate the situation, and try to overcome that difficulty. And also, of course, the two. Of course, the two sets of data and the individuals in the second, in the late decade, very likely were some of them were in the first data set. So we started with assuming they are independent, the two data sets are independent, and some simulation have been conducted to show. Back to show the estimation procedure actually is quite robust to that independent assumption. Or if we target the general population and we use the information, not just from MHED database, also use the fact. The other residents do not didn't have any visit during the time period, that effect. And then we can consider to use a similar estimating function. Just one term, that ratio of the two big sum in the estimating function requires every Requires everybody, every resident in Alberta under 18 years old information, coveted information that is not available. Then we use population level census information to approximate, aggregate, you know, to approximate the sounds and then have that estimate. That estimating function is applicable. So, of course, we need to check: is that approximation good enough? And also, we derived the asymptotic properties of the resulting estimator with this approach. And I'm going to share you some of the data analysis outcomes. So, this table presents the estimates for the regression coefficient. We consider all the regression coefficients as age-independent to begin with. So, the first column. So the first column of the estimate associated with the target population is P1, the second is P. And you can see alpha, actually, alpha is the coefficient to Xi. So quickly, based on alpha, you can see you can make a comparison whether the late and early decades, they are different or not in terms of the frequency of. And the frequency of ED visit. Apparently, it's not just if we focus on P1 and or even the general population, the two decades. And it looks like late group have higher frequency of EDVA. For another example, if you look at the estimate, The estimates for the regression coefficient to sex, and we can see that it's male versus female. So now you can see, and if we focus on P1, and then it looks like boys turned out have less EDVAs compared to girls. Even within P1, that subpopulation. And something interesting is if we go down to the third group of estimate gamma, it is the coefficient to the product of Zi and Xi. It shows how different in terms of the Risk factor effect between two rules. And then you can see if we will focus on P1 and Edmonton versus the other area, actually that's no big difference, no significant difference. However, if you consider the general population like Edmonton, looks like admonton and on average has relatively less and ed visit and and from later group compared to the early group. But the calgary area relatively in P, in P, the general population, much higher visit frequency in the nate period compared to the OV. Period compared to the early period. If we consider time-varying coefficient or age-varying coefficient, you can see you have similar estimates for beta and gamma are constant. It's just alpha is age-ivariant. Here's the plot, and the first plot is associated with the subpopulation, and it shows actually two estimates. The green line presents the estimate for alpha, which is a constant throughout all. constant I threw out all A and the blue band and present the confidence band for alpha as age varying covariate to the indicator of late decades. You can see the difference between late Between late and earlier groups, actually vary across different ages. And if you look at the second plot, this is associated with the general population P. You observe similar pattern, but you can see because the red dashed line is zero, then you can see the Is zero, then you can see the difference between the two groups is larger if you can see the general population. And these two plots present the estimate for beta, age-varying beta, the coefficient. Beta, the coefficient two sex. This is something and interesting, and I want some time to explain. The first one, you can see, and there are two straight lines associated with estimate for beta, for beta, assuming beta is constant and for earlier and the later. For earlier and later group. The two curves are estimated for earlier and later groups beta. And because they are coefficient to sex, sex, the boy versus girl, then you can see the difference between female and male and the female actually also varying, age varying. H variant and they are pretty much singing in the no matter in the later or earlier group. And then, and probably during the school age, and the boys started to have more issues. And you can see about 12 years old or a little bit earlier, and the girls catch up. So, so the curve drop below. So the curve drop below zero. So the straight lines, you cannot tell that much, how close or how different it is from zero, but the curves show that. Also, from different colors of the curve, you can see actually that change kind of a good catch up in terms of the ED visit and it's getting earlier in age for the In age for the later group compared to the earlier group. The second plot presents corresponding information viewing then the general population, I mean the target population is the general population. Then you can see similar patterns. However, the difference between the two groups is much bigger. Is much bigger. And so, what have we done for that project? I think we've provided some useful summary of the mental health ED visit data from 01. And then we mentioned. And then we managed to provide some summary on all that is Alberta and residents under 18 years old during that time. And if we can take O1 as a random sample from P1, that biased the sub-population, we may say we come back to some statistical inference using data from O1 on P1. O1 on P1 correspondingly and using O and that is all Alberta case under 18 years old during the time if it is a random sample of P, the general population of interest, and then we say we also conduct some statute inference because we provide estimate for the regression coefficient, we can construct confidence interval, we can make you know actually inference. We can make, you know, actually inference and then make a comparison between the groups. What if O1 OO is not really a random sample of the target population? Well, naturally say, oh, then probably we need to make sure all to look more closely and what all or what all presents. one or what all present that to me kind of um is an analogous to this this kind of toy example I want to show. If we have a collection of observations x1 to xn, the first thing we always do in terms of data summary is what we calculate the sample Summary is what we calculate the sample mean, X bar. No matter what, we calculate the sample mean. Probably somebody calculated sample variance. We use one number or two number to summarize the collection of data, observations. And if that collection of observations form a random sample of the target population, Of the target population with population mean mu. And then we may use X bar to estimate mu. We may provide confidence interval for mu based on the data. If D is a random sample from the population. What if not? People may adjust the floor that we may use inverse, for example, inverse probability, vinity weighted estimator based on the data, based on our understanding why B is not a random sample of the population. To provide a different estimator, Estimator not X bar give an estimator for me well what else to do for the program actually there are many interesting issues to further explore at least in very some very simple ones and the program is still in ongoing and and I'm going and there are many actually me and many still many interesting statistical problems to explore. So in general, for large administrative health data, and they're available and readily available, and they have a lot of information is just sitting there. But to analyze them and are not that straightforward. So, and we need So, and we need to carefully design for our analysis and with analyzing administrative data. I want thank Dr. Rossichek and Shung for the collaboration. And this presentation, the key part is based on a recent paper by Shung et al. 2022. And I also want And I also want to mention that a comment from Jerry Lawless on a previous version. Actually, I noticed this title. I tried to fix it, but I'm not able to update it. It's V-R-S-I-O-L. I know how to spell it. So, and a previous version of the presentation has led to a big modification of the work. Thank you so much for your attention.