I will be speaking about covering numbers of real algebraic varieties and some applications. The talk is really just about one paper and I may, time permitting, briefly mention some ongoing work. The co-author is my student, Ythan Zhang. He's a really impressive guy. Really impressive guy. Okay, so background number. What is the covering number? Given any subset of a metric space, you can consider the epsilon covering number. Here I'll think about sets V, which are in Rn. And the epsilon covering number of V will be the smallest number of balls. Will be the smallest number of balls, L2 balls of radius epsilon with centers in V needed to cover V. So here's a schematic that I just got from Google Images of covering number. So if you think of the interior of this black curve as V inside the plane, then I'm covering it by these disks. Okay, and in this example, it's actually, I mean, in this picture, it's not perfect because actually. In this picture, it's not perfect because actually, here the center is not in V, so this isn't quite what I would want. But it's just a technical difference whether or not you require the center to be in V or not. Okay. So the minimum number of such disks, that would be the covering number of V. So why is this relevant? Well, it can be used to bound the volume actually of two. Actually, of tubular neighborhoods of V. So, this is one nice and immediate application. So, the tubular volume of a set is when you thicken the set by some amount. So, you look at all points in the ambient space Rn that are within delta of, say, say within delta of V. If you have a covering number for V, then you immediately can get a volume bound for the thickenings of V. And that's how. V and that's helpful when V is a bad set. When V represents, say, the set of ill-posed inputs to some numerical algorithm. Then a thickening of V is sort of ill-conditioned inputs. So we might like to know how big such a set is, and we'll sort of see an application with tubular neighborhoods later. Also, the covering number appears in approximation theory and dimension reduction. And dimension reduction. So, if V is in a very high-dimensional space and I'd like to linearly project V while preserving pairwise distances with high probability, then actually the covering number, well, actually the Gaussian width controls how low a dimension we can project to, and this is controlled in turn by the covering number of V. So we'll also see an application with this. There's further applications that I don't know too much about in top. That I don't know too much about in topological data analysis, where one is trying to estimate topological quantities associated with a finite set of data points, like persistent homology groups. And a question that arises is sort of how densely do you need to sample the set to get the right topological invariance? This is related to the covering number of the set. Okay, so that's covering numbers. What about real algebraic varieties? The other part of my title. The other part of my title. So I claim there are strong motivations for studying these. So for me, here a real algebraic variety will just be a zero set of some system of multivariate polynomial equations in Rn. And I can also look at images of real polynomial maps. So I can look at a map from, say, some Rm to Rn, whose coordinate functions are polynomials in Rm. Then look at the image. You can look at the image of such a map, and that's very close to a variety, but it also is something we'll be able to say we'll be able to study. So these come up again as sets of ill-posed inputs for numerical algorithms. So for example, just the problem of inverting a square matrix, where that breaks down is when determinant equals zero. And determinant is a polynomial equation in the entries of a matrix. So you sort of naturally will get varieties. Naturally, we'll get varieties in the case of bad input for a lot of algorithms. The set of low rank tensors or tensor networks also, they're actually images of polynomial maps. So we'll see this in examples later. Basically, the tensor decomposition or network itself is a polynomial mapping from the factors to the tensor. To the tensor. This is less obvious. Actually, if you fix the neural network architecture, vary the weights, you get some function space. This can be closely tied to varieties as well. What's not obvious is that one often will use like sort of non-polynomials for activation functions, like rel u in practice, but then activation functions can be. Activation functions can be approximated by polynomials or by ratios of polynomials or rational functions. And that lets us say things in principle about generalization errors of neural networks. This is an application in our paper, the neural networks, but I won't have time to discuss it in today's short talk. So, you know, in these types of applications, actually, there are varieties or images of Actually, there are varieties or images of polynomial maps, and good bounds on the covering number of those sets let you say something meaningful about the application. And so I want to jump to the mean mathematical results. Maybe this slide is a little overkill, but I'm just going to give you a big table of kind of the state-of-the-art prior results on covering numbers. Uncovering numbers. So U here is some set. It's in Fn and Fn is F here, the field is either the real or complex numbers. U is a zero set of some polynomial equations or the image of a polynomial map or potentially described by polynomial inequalities as well, the semi-algebraic case. Let's say Fi are all the relevant polynomials, all the polynomials appearing in this description of U. And let's say that D is some unit. And let's say that D is some uniform degree bound. And for this class of sets, there's a well-defined notion of intrinsic dimension. And we're going to say that the intrinsic dimension is at most little n. We're really interested in the case when little n is much, much less than the ambient dimension, capital N. In this case, these are the sort of prior state-of-the-art results and covering number. Okay, so there's a lot to take in here. There's a lot to take in here. There's too much to take in. But I'm formulating these bounds in terms of the log of the covering number. So this is kind of standard in the literature, and the log of the covering number goes under the name of metric entropy. There's different cases here, if U is semi-algebraic, how many constraints it has. We need our set to be bounded, to have a bounded covering number. So we'll often intersect with the unit ball in our space, that's BN, the L2 unit ball, or scale it to the. L2 unit ball or scale it to the sphere, the unit sphere in some cases. Okay, the things to pick out here are in all these cases, there's this term on log covering number, n times log of one over epsilon. This term is easy to understand. This is the covering number for a unit ball of dimension little n. Of dimension little n. And you can see that that's correct just by comparing volume. So, like the volume of a unit ball compared to the volume of an epsilon ball, the ratio is epsilon to the n. So, you should need at least one over epsilon to the n balls to cover. Okay, and that's the contribution here. So this. Here. So, this is just if the set were linear, this is what you'd have. And then these other terms, they're actually incorporating the nonlinear aspects of the algebraic set. And the thing I want to stress is the dependence on the dimension, on the ambient dimension n here. In all these cases, there's at least a okay, there's at least a linear dependence on capital N. on capital N in these. And this is bad if little n is much more than, much less than capital N. Okay. So within this table, these are sort of our technical contributions. So there's one result that I'll say that includes Result that I'll say that includes all these, but for you know, as a point of comparison in the semi-algebraic case, we get some improvement in terms of the number of constraints. So there's kind of an improvement on this term compared to this prior work of Yamden and Komp. In the case of a just zero set of polynomial equations, we're just matching what was previous. We're just matching what was previously known by Basu and Liario. This entry is exactly the same. The proof is very different. Where we get the most important improvement is actually here. So for images of polynomial maps, we actually break this linear dependence on the ambient dimension. And we just have the ambient dimension appearing logarithmically. So that's the main upshot. That's the main upshot of what we did. So, we'll get the strongest results for maps for sets that have a nice polynomial parameterization. Okay, here's, I'm going to just go through kind of the proof at a high level. So, yeah, we have actually a really short and elementary proof. An elementary proof compared to what was out there. And the main idea is to slice our set down in dimension and kind of slice our set with linear hyperplanes and cut the dimension down and do an induction on dimension. So you can equip the space of affine planes of a given dimension in Rn with a probability measure that's invariant, or with a measure that's invariant. With a measure that's invariant to rigid transformation, there's a unique such measure up to non-zero scale, and then we're going to say that our so this is our definition, and our result actually pertains to these types of sets, not just algebraic sets. So we'll say that a set is Kn regular. It's basically it intersects with linear subspaces in a nice way. Precisely, so n here is supposed to be like the dimension of. N here is supposed to be like the dimension of our set. If you intersect our set with affine planes that are too small, their codimension is more than this little n, then almost surely this intersection is empty. Okay, so we want that to be true. And we also want that if you intersect our set with bigger linear spaces, their codimension is less than this little n parameter, then the intersection isn't too insane. Isn't too insane. It has it in most K-connected components. So if you have a set that intersects linear spaces in this controlled manner, it's almost always missing them if they're too small. And if they're big, it's sort of you have an almost sure control on the number of connected components. Then you can apply a recovery number result that I'll tell you in a moment. The main thing is all these algebraic sets, they do have this property. They do have this property of playing nicely with intersections of linear spaces. And you can get explicit bounds on what the K and N parameters are. Okay. All right. Yeah. Sure, I'll draw a picture. Small plane typically built an interface. Big plane. Almost always. Almost always don't intersect. So, like, um, yeah, space. So, consider like just a curve in 3D, a space curve in 3D. Yeah, lines almost always don't intersect the curve. That's the first bullet. Planes like intersecting this, but they intersect it in maybe at most some number of points. Yeah. And that would be the K here. Yeah. Would be the K here. Yeah. Yeah. Yeah, it's the technical intermission is this Milner-Tom theorem. Is this Milner-Tom theorem? Well, there is a little, there is some dependence still on the ambient dimension. Like we still have a logarithmic dependence in our result, but it's somehow well I haven't told you what these so k and n will depend on like the degree of the polynomials in the description of the algebraic set and potentially also in a weak way. Said and potentially also in a weak way on capital N. Okay, but yeah, the exact dependence is in the paper, and I'm not recalling it offhand, but here's the machine that gives you K and N, the K bound right here. It's the Milner-Thom theorem. This is how we control, the main thing is controlling this number of connected components in the intersection. Like we just have like three intersection points, say, in this picture. So, you know, there's just this really short paper by Milner. I think it's like two or three pages. And almost simultaneously, Tom put out a paper doing the same thing, also short. I believe it was written in French. And so they were considering zero sets of polynomial equations systems in Rm. And each polynomial had a, there was, they had a uniform degree bound, and the sum of the Betty numbers of the zero set. Of the zero set, in particular, the this is this, whatever this is, it's at most the number of connected components in the zero set. They give some bound just in terms of the degree bound and in terms of the number of variables. So, in particular, this is independent of the number of equations. Yeah, go ahead. Yeah. Yeah, exactly. Yeah. This does, yeah. So it's going to depend on the description through the degree and the number of variables. So if you, yeah, basically, if you can describe your set through a parameterization, then the key thing is you can decrease the number of variables basically in your description. It will just be the number of parameters rather than the number. Parameters rather than the number of variables in the ambient space. And so, yeah, it does depend on the description that you use. Okay, this is the machine that lets us control K. And then here's our general result. If you have your asset V and it's inside a box in Rn, so an axis-aligned box of side-length QT, and it's a Kn regular set. And it's a Kn regular set, so potentially, say, an algebraic set or some other Kn set, then here's a bound on the metric entropy. Let's look at the bottom line. You have n times like log of side length over epsilon. This is what you would get if it were just linear. And now, this n times log capital ambient dimension plus log k. Plus log k. So this is the general result in terms of little n, k, and capital. So you see that there's a dependence on capital N here, but it's only logarithmic. Okay. A couple words about the proof because it's really a naive idea. It's induction by slicing. So suppose our set is originally this surface in 3D. Surface in 3D. What we're going to do is create a grid of planes in 3D in all the axis directions. So in this direction, in this direction, and in this direction. We'll slice our surface up. And then each intersection, if you kind of randomly perturb the planes a little bit, then each, but then by our property, our regularity. By our property, our regularity property. Each intersection should sort of be a curve. The dimension should actually drop, that the little n parameter should drop by one. And then you do a grid on this. So this is a zoom in of the intersection with one face. We have a grid on the face now of lines. And in the end, you keep doing this down until you get down to points. So you get down to these red intersection points and you cover them. And you cover them. So you just draw balls that cover them, and you cover them with a sufficiently big radius that you're guaranteed to cover like the cell, all the cells that contain this thread point on their boundary. And then the other thing you could worry about is these green components. These green components down here, they are sort of not intersecting the grid lines at all. But then we have a control on the maximum number of such components. Number of such components. It's going to be at most k. So you just cover them as well. Okay, and so this, you know, I just gave a very rough outline, but it gives you this in induction, this kind of recursive bound chair, and you can build up a covering. Okay, so now I'm going to transition just applications of this result. So the first one is, I mentioned, whenever you have a covering number bound, you immediately get Have a covering number bound, you immediately get a bound on the tubular volume. So, and this is why. This is just the triangle inequality. If xi are points in V, so that balls of radius epsilon at xi cover V, then balls of radius 2 epsilon centered at xi cover the epsilon thickening of V. This is just the triangle inequality, and so therefore, the volume. And so, therefore, the volume of the epsilon thickening of V is at most the volume of a two epsilon ball times the number of balls here, which would be the recovery number. So, I get bounds on the tubular volume immediately. I don't have time to unpack this. Okay, but that should have been removed, yeah. But okay, let's just go on. So, I want to run through two applications in the last few minutes. So, one of them is applied to low-rank CP tensors. Okay, so I wasn't here earlier. I'm not sure if tensors have been discussed, but. Answers have been discussed, but here I'm just thinking about them as an array of real numbers, N1 by N2 up to ND array of real numbers. They're arising as ways to store data in a number of settings, rating data or video streams, 3D images. You can also make tensors as moments of multivariate data sets or as derivatives of multivariate functions, like high-order covariance matrices. So, like higher-order covariance matrices or higher-order Hessian matrices. And the thing I want to stress is that they're just big. Like n to the d could be a very large number, the size of these guys. So compression is needed. And one of the most basic ways to compress is the CP decomposition. This is writing the tense. This is sort of, this should be an approximation, not an equals. This is approximating the tensor as a sum of rank one. The tensor is a sum of rank ones. And a rank one is just an outer product of vectors. So on the right-hand side, you know, there are basically order Rn parameters, whereas on the left-hand side, there are n cubed. So if R is small, this could be a big compression. The CP rank is the minimum number of rank one tensors needed to add up and get T, the CP rank of a given tensor, like for an exact equality. So this is the Like for an exact equality, so this is the exact rank. So, I want to ask the question: like, what's the approximation power for this kind of model of compression? So, in other, and the way I'll think about this is if I form a random tensor, its entries are IID Gaussian, standard normal. How close should it be to being ranked, to being within epsilon? I mean, what's the probability it should? I mean, what's the probability it should be with an epsilon of rank R? Well, that's a tubular volume question. So I'll tackle it by figuring out what the covering number of my relevant set is. The relevant set here will be the set of tensors of CP rank at most R. That admits a polynomial parameterization. Yeah, I just quickly write it on the board. Right, so. This, you know, this thing is a polynomial map. So I'm in this nice setting of having an image of a polynomial map, and I can apply the result. This is what one gets. There's a slight improvement, actually, if the rank is less in the so-called under-complete case. In the so-called under-complete case, if the rank is less than or equal to the minimum mode dimension, because then one can actually use a slightly better parameterization of under-complete tensors. You get this reduction. Okay, what does it mean? So in this case, when I'm generating a random tensor with IID Gaussian entries, let me consider the event A R epsilon that there exists a tensor T prime of random. This is a tensor T prime of rank at most R within an angle of epsilon of my random tensor T. So rank is invariant under scaling, so I'll think about things in terms of angles. So when I want to be, you know, so this is what one gets in the under complete case if you plug in our covering number result for angles less than 30 degrees. And okay, here's just a numerical. And okay, here's just a numerical reference to unpack what this is saying. So if one considers 100 by 100 by 100 tensors, third order, 100 by 100 by 100, and rank 30, what's the probability that my random Gaussian tensor is within 30 degrees of rank 30? It's actually sort of astronomically small. It's e to the minus 40,000. And if I want to be within pi over seven radians, it drops down even like precipitously to e to the minus 140K. It's 140k. And so that's what one gets from these covering number results. And a takeaway for me is that to kind of use low-rank tensor compression, your data set shouldn't be at all well-modeled as completely random. You need there to be some strong structure in the data set, some structural reason why low-rank compression is reasonable. Otherwise, with very low chance, you'll actually be close to low-rank. Okay, just have a couple minutes. Just have a couple minutes. Do I have a? We started late, right? Do I have? Yeah. So, this is the second application. I wanted to talk about randomized polynomial optimization. So here I'm thinking about least squares, where the residual functions are polynomial in some variable x, in Rn. And I want to think about the case when I have a huge number of residuals. So capital N residuals. So I think about, I'm Residuals. So I think about I'm solving capital n polynomial equations in little n unknowns, but in the case I allow for there to be some noise. And so I set up this non-linear least squares. So kind of the computational goal here is I want to reduce the problem size and still have a good solution to this original problem. And I'd like to kind of make, if I do something like gradient descent with smart step sizes or constant step size or quasi-Newton method or alternate. Or a quasi-Newton method, or alternating these squares, or something. I just want to make the iterations cheaper. So, the basic idea here is sketching. I'm going to replace my polynomial system, P, by a smaller system, SP. And S will be a very long, fat matrix where S is M by N and M is much smaller than N. So random matrix. So the theory tells us that covering existing theory tells us that covering number. Existing theory tells us that covering numbers on this set, okay, this is the relevant set here. The image of this polynomial mapping, which would be in Rn, and then this pi thing is scaling it to the sphere. So then I normalize to the unit sphere in Rn. I need to have covering number bounds on this set, and then that will let me bound how little I can take m to be. Smaller m is better. Right, and this connection goes through Gaussian width. Gaussian wit. So, this is the result. If so, I take S here, this is just stated in the simplest case. I'll sketch with it a Gaussian matrix. So I'll take IID Gaussian linear combinations of my original polynomial equations. And then I want it, you know, with probability at least one minus delta. I want the objective function to be. Objective function to be everywhere within one plus or minus epsilon of what it was before. And then this will occur if you take the sketch dimension to be like so. So in particular, there's a very weak dependence on this n here. So n prime is where n is showing it. So there's a very weak dependence on the original number of equations. It's really just being driven by here the number of various. Driven by here, the number of variables. Okay, and we use this, we're implementing, we're using sketching in an algorithm for streaming tensor decompositions. And yeah, this is the part that's ongoing work. Basically, using the theory developed in the last slide. Developed in the last slide to come up with a new way to compute CPE decompositions for a tensor that's being streamed temporally in one mode. Okay, and there'll be more to report on this in the future. Okay, thanks for your attention. One quick question and then copy the podcast. So the results here for the endos were supposed to be lowering. What about the just make it the top? So do you have a NDIN or do you have anything new about that? No, we don't have anything new. I think we just recovered what was known. Yeah. Yeah. 