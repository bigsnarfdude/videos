But all that's that's all. Okay, well, first of all, thanks a lot for inviting me here. It's always a pleasure to be here. Well, okay, so that's about Distances of L R n squared to L Pn tend to epsilon L Qn. And so the question is, what is the Banachmato distance here of L? The distance here of L by n squared to L T n tens of epsilon cubed. What is what is the order? And well, actually we so we restrict ourselves in this talk to R equals. Equals one and two. And yeah, so you know, occasionally I have been approached with this question. I mean, what are these Vanahmaso distances? Because some people needed these for their research. And usually it comes with the complaint saying it's not an economic. The thing is, it's not in Nicole's book. This didn't work, this joke, right? It wasn't a joke, I mean, it was true, I mean. So what people usually do in this case, they look at Nicole's book. Well, okay, let me just explain first of all the definition. So the Vanah-Maso distance here. Maslow distance here on our Maslow distance is just sort of the distance between x and y is of course the infimum over all norm t times t inverse and then t maps from x to y and of course I'm in the operator. Of course, within the operator. And yeah, so then we have here these spaces. So LT spaces here. So the norm here. So A usually for me is an n by n matrix. And then the norm of A here in this space. Here in this space, this is nothing else but the operator norm from L P star n to L Q N. And this you can write out, this is just the subfremon of norm x and p star equal 1 and norm y and q star y. One, and then you sum here over all a ij x and i 1 of j. So this is, of course, the usual norm to have here. And then we have another tensor product, high tensor product. So the norm of A in L T and tensor pi. Pi Qn. This is just the infimum overall sums norm xi, norm yi. So this is here, this is in p, this is in q, and such that k equals the sum of the elementary changes. So it's just another tendon. Just another pendulum here, the space. And the interesting thing here is, of course, that L P N tensor epsilon L Q N. Consider the dual space, so this is nothing else but L P star tensor pi L Q star. So we have this LQ, of course. Well, okay, so the rating of the so how to compute the one of Mars of distance. One of the miles of distance or computer estimate, estimate from above and from below. And so to estimate estimate it from above Love indeed an intelligent or lucky guess this depends on your personality what which operates Operator gives you say approximately not the optimal order. So, in some sense, this is kind of clear-cut. I mean, you've got I mean but to estimate to estimate from below well what you have to do you have to check all operators which is which can be very tedious. So from below you look Look for an invariant that is smaller than the Weimar-Marser distance. And better to handle. Okay, so I don't talk about that. Okay, so yeah, so the case r equals 2, so this is when you consider L2m squared, LPN times epsilon LQM. So well, first of all, then this is basically it goes back to To join paper with people among the lists Chara Adelizi Gidun Take myself and the diesel bed. And Elizabeth. It's really puzzling to me that the three French people are following each other. Putting mixed up, right? Coincidence? Okay, okay, okay. So, any case, okay, so we are in luck. And why is that? Well, we can show that the optimal operator, and I mean here the operator that gives you the Banner-Mauser distance. Is the identity is the identity. So it simply means that I applied to this matrix gives you simply the matrix back. I don't need to specify this, I think it's clear. So it's not that it's of the same order, but it's exactly. Order, but it's exactly the optimal operator, and this is mainly due to the many symmetries we have here and there, too. And also here we have a lot of symmetries, and then by some averaging argument, we simply can show that this is the optimal operator. Yes. And uh yeah, this is it's it's easy to show if you It's easy to show if you have L2 and the space with a symmetric basis. But here you have tendered photographs. Okay, so then you have to compute the operator now and So you have to compute the operator norm here, and this is done by some old result by Hardy and Littlewood. Oh, this is Hardy and Littlewood. And there there's a big bunch of estimates there. Estimates there. And I just give you one example here. It says the following. So suppose that 1 of p plus 1 over q is less than 1 half. And you have the following. And you have norm of A and the alpha norm is less than some constant norm A L P N. Lpn times epsilon LQN. Where, you know, alpha, what is alpha? Alpha is given by 1 over alpha equals 3 over 4 minus 1 over 2p minus 1 over 2q. So you can, so you have here an estimate from below here for this kind of operator. At all. Actually, they wrote the title of the paper something like bilinear forms or something. And so, using these types of inequalities, you can settle that r equals two estimates basically from above here from this operator. And then, yeah, so this is basically what we had to do. And estimates from below, you probably need some random matrix. Look for plus-minus random matrix with plus-minus entries. Okay, so let me just quickly write down then what you get here. So then you get here that this phenomenon of distance. This is the following. So this is n to the 1 over p plus 1 over q minus 1 if 3 over 2 is less than 1 over p plus 1 over q. Than 1 over p plus 1 over q, and you get here p and q are between 1 and 2, and 1 over p plus 1 over q is less than square 2. Then you get here n n to the 1 over p star if If q is between 1 and 2, and this is less than p, and this is less than q, so and then you get n to the 1 over p, oh sorry, 1 over q is between 1 and 2, this is less than q star, and this is less than q. And this is less than equal. And finally, you get n over one over star u less than u less than so that you by this method you get this, so it's a little bit extensive, but well, I cannot help but this is the answer. So the more interesting case is probably Case is probably r equals one. And it's more interesting because of the strategy. So now r equals one. And so what do we do there? Well, first we observe the following. We observe that the distance here from L1 n squared to L Squared 2L Tn epsilon L2N. Now this is the same as the Mars of distance between the dual spaces. So in this case, this is L confined M squared. I'm only after L P star N times I star N. And then, yes, as I said, I mean for the estimate from above. Said, I mean for the estimate from above. So we want to estimate this one now. For estimate from above, we are looking again for different matrices among them are just watch matrices, for example, or random plus minus one matrices. And what about the estimate from below? So here we use We use the gamma infinity as an invariant. So, what is that? So, gamma infinity of x and y is the operator ideal of linear operator. Linear operators, say linear operators factoring through a china space. All right. So let me just write down what it is. Just write down what it is. So the norm gamma infinity of say for this t, so an operator mapping, mapping from x to y, this is the following. This is This is uh the following. This is the infinite over all norm A, norm B such that A is an element in L x L infinity. And then we take here any measure space to infinity. And it should not be too small the space, otherwise we don't have that. Too small in the space, otherwise, you don't have any vectorization at all. And V and L infinity Y, then T equals V A. Yes, that's it. So this this is the norm. So this this invariant has So, this invariant has some very nice properties. The main thing there is that you, because the Van Hamauser distance, you have to compute an infimum. And there you but here with gamma infinity, instead of computing infima, you can average. But this is, you know, it works beautifully. And well, And uh well before we go into that, well yes, okay, there's something more. Usually gamma infinity of a space, this is gamma infinity of the identity of the space. So this is how we find gamma infinity of the bar space. And now we need Space and now we need another operator ideal. So this is pi 1 of x and y. This is the operator ideal of one summing operators. And what is this? This is just the norm. This is just the norm, the pi one norm of t, an operator mapping from x to y. This is the infimum of all c such that for all n and n and for all x one up to x n in x we have that this. uh we have that the sum of the norm Txi is less than C times the supremum over all C and X star equal one sum of X I C so this is so you consider all constant C such that this is satisfied here Here and then you take the integral, and this is the pi1 log. And then, of course, also here you have that pi 1 of x. This is just pi 1 of the identity of x. So, and actually now we are working with this norm. Okay, so um yes, so what we estimate Yeah, well, okay, so there's one, yeah, there's more remark here. So of course, gamma infinity of x is less than the Banach Master distance of x to L infinity dimension of x. First here, the operator in its inverse, it gives you already a factorization through. Effectorization through network. And but then there's another thing, namely the gamma infinity of x times pi 1 of x equals the dimension of x if x has enough symmetries. Well, I don't want to go into this, but I don't want to go into this, but you can be assured that in this case of the spaces we consider that they have enough symmetries. And okay, so we estimate, this is what we have to do here, we estimate pi 1 of L P star and the pi L Q star and from the button. From above. Okay, so how is this done? Well, what do we have to do? Well, we have here the sum norm AI in L P star n tensor i L Q star n and this should be less than C times the supremum of elements from the dual space. Some of elements from the dual space here, which is just colour B, and then in L Pn times epsilon L Q less than one. And then the sum here overall AI. Okay, so we want to estimate so the norm. So, the norm is the intrimum of all C. So, what we do here is we replace this by something smaller. And what do we do here? So, we choose a plus-minus one matrix V well usually it should Well, usually it should be something like a Rawls matrix or so. And then we define the following matrices. We have here V, epsilon delta. Epsilon delta are sequences of signs, plus minus 1. So at i and j, this is just v i j epsilon i delta j and then Then this here, this thing here, this is bigger than 1 over the normal V here in our space L Vn tensor epsilon QN. Then we take the average of all epsilon and delta. Then we sum over all i. Then here we have theta. We have AI, V, Epsilon, and Z. Just plug them in. So we take here an average over particular matrices here. And this is, of course, an average is always smaller than supreme one. Now, if I now take this, plug it in here, this constant will get bigger. Will get bigger because we made this small. So, but if we compute here now, this average. So, we have here a matrix applied to this type of matrix here. This is the Vijaya ones, but here these are plus minus ones. So, we average here over plus minus one signs. Signs, row-wise, and column-wise. So, what we get here: this is bigger, this is bigger than one over this norm of B, and then we get here the sum of now the norm of AI, but because we apply twice Kinchin inequality, this gives us here the Libertrechnum. And now we are done, because on this side we have the summary. On this side, we have the sum of norms of AIs, and here. So it simply reduces to comparing the norm of the matrix in this norm and in the average norm. And we are through. So I'm also almost through, so let me just briefly. But this is the general strategy, and this is what we do. How we proceed here, and then I can give you some estimates here. All right, yeah, so for example gives you for L L one n squared, so L Pn times epsilon L Q n. So this is proportional to what? This is proportional to, for example, n to the 5 over 2 minus 1 over p minus 1 over q if 3 over 2 is less than 1 over p plus 1 over q. And then, let me have this in another one. We have n here, if t and q are both beyond 2. I have some more estimates there. I have one or two cases that are still missing. I'm working on those cases. That's it. Thank you very much. Thank you very much. Questions? The pi norm is much closer to the 5%. We also have something like 1%. Yes. Yes. Yes. So that's actually simpler because here if Because here if so if you have L infinity n, right, then squared L P n times epsilon. This would be different, right? Then because here then you know that gamma infinity of L T n and epsilon L 2n, this is just gamma infinity of L T n times gamma infinity of L 2n. Gamma infinity over q n and then then you're then you're done. Here in the other case the other case is a bit more involved. Other questions about this thing? We have a 