Yeah, on this image on image regression. So, yeah, my talk actually quite related to the previous two speakers. One is machine learning, another is multimodal imaging. So, basically, I combine the two together and kind of create this talk, and which is focused motivated by multimodal imaging modeling. And so, I think here many experts are actually in different Many experts actually in different fields, and I don't need to introduce, embarrass myself to different types of modality here. But basically, the data set we are looking at is sort of like for one subject, you can collect a different type of imaging data. And our goal here is trying to study the associations between the different types of modalities, just like Shore studied functional connectivity and structural connectivity. And here we are focused on more general concepts. There's any type of More general concept, there's any type of imaging data. And by the way, we're primarily focused on the kind of spatial-temporal nature of the data. So, and here, I think Shaw already revealed some challenging tasks here, like high dimensionalities and complex spatial temporal dependence and also the increase of sample size. And one motivating application we have been looking at is kind of study the brain activity during the The study of brain activity during the fMRI task performance, which can how much can be explained by the task independent image environments like resting state or structural imaging. And this actually is a science paper like in 2016 here, the Tabor's paper, and they study the human connecting project and they identify some brain regions that can be accurately predicted from the resting state FMR. From the resting state fMRI and the structure fMRI on the task fMI. So they're actually using very simple linear regression, but their sample size is very small. At that time, they only use like less than 100 subjects. And so there are some basic existing method, including this linear regression. I just mentioned the basic idea is partition the whole brain into multiple regions and for each subject. Regions and for each subject and each region, they can fit a linear model between the outcome image and the predictor image. So, and then they predict a new subject by averaging the fitted model across subjects. So, this is not really kind of prediction, like it's more like a projection, right? So, and another type of regression called voxel-wise regression, this with more like requires a predictor image and an outcome image. Image and outcome imaging have to be in the same imaging space, right? So they can match with each other, like do the walkthrough by walkthrough predictions. So for each walkthrough, you regress, create a regression model across all the subjects, right? So and then do the prediction. This one actually can do actual prediction. And here are some like basic limitations. It's not really can, I mean, lack of the flexibility, basically, this two type of method and not. Method and not explicitly model the spatial dependence. And actually, this problem, if you think about the not just motivated neural imaging applications, think about computer vision. This problem has been done by a lot of computer scientists motivated by the digital image, like pictures, right? So they have those type of like, they have different slides, different name, they call it image to image translation. They're trying to say, start from like the, they have an input image and output image. an input image and output image they try to label this uh a straight thing right so they they change the the the the the daylight to the to the night uh like uh from one image and uh then they may extract the the edge of the photo or something like that they they always think about this translation but think about a statistical problem this actually sort of regression problem right so you have a predictory image outcome image you can do you can do the same thing but but they they they heavily use these like a machine learning uh deeper neural networks to to complete this neural networks to complete this task, they focus primarily on the prediction, right? So whether they can actually reconstruct those images. And here we review some like relevant literature. One of the currently widely used is called conditional adverse network or conditional again. And there are some special kind of like very complicated architecture designed to make this neural network very kind of accurate. Of accurate this one of the representative method called RBDN. Basically, they recursive, I mean, incorporate recursive convolution and layers without any spatial downsampling. And basically, they actually can try to combine different tasks. It's a very powerful method in the field. And the limitation of this type of method is that they already require large training sample size. This is actually by nature the computer's visual. Actually, by nature, the computer's vision problem they can collect a lot of image, right? So, like an image net, those famous kind of like website, they actually collect the billions of the image. They can use that to bring their model. And they don't even actually care about the interpretation, about the model parameters. They care about more like a prediction accuracy. So, that's slightly different from our motivation. So, actually, to address the problem from the neural imaging data, we actually The neural imaging data, we actually propose one model called spatial Bayesian latent factor model. Then, actually, one student worked with me and the team together. We actually developed this three-level Bayesian hierarchical model. So we consider, start from outcome image, we basically try to summarize outcome image using some basis expansion approach and model the basis coefficient using like the latent factor model and also And also consider each latent factor as a scalar outcome. We do the scalar image regression. So, basically, three steps or three levels of the hierarchy, we make the connection between the predictor image and outcome image. And there are a few advantages of this type of method, like we can do some efficient dimensional induction and explicit model the spatial dependence. And we can actually by some analyze the model kind of formulation, we can reconstruct a voxel by. We can reconstruct a voxel by voxel. If you have one voxel from the predicted image, one voxel from outcome image, we can actually explicitly capture the correlation association between these two voxels in our model. And also, we allow the predicted image and the outcome image not in the same space. They can have different sort of like a resolution or different interpretations. And by the limitation here is in our model, we actually introduce some random effects. Random effects, and that's not very good for prediction. So, that random effects model can fit the training data perfectly. And then later, we don't know how to actually predict in the test data, how you actually predict that random effect. So, that prediction actually is challenged. And the post-pair computation, the three-level hierarchy model, you can imagine if you do MSM, it take hours and hours for the small data, for the large data set. And for small data, even take hours, hours, hours. small days even even take hours hours hours and uh we actually think about trying to extend this approach and uh and uh this is a new approach uh we we call it basic an image on image regression uh we are deep kernel learning based causing process and uh we try to come up some name you can you can you can memorize it's called a bird on the gp right so that's uh so so which is trying to uh uh reduce some like uh complexity in the three-level hierarchical model so we are trying to Three level hierarchical models. So we are trying to utilize some idea of the imaging projection, but we start from both the predictor image and the outcome image. So we have a two-stage, basically two-stage analysis. So stage one is we basically project the image from outcome image and also from predicted image into some small dimension space, vector space. And then in stage two, we just model the nonlinear association between these two projected image. To project the image. And in the stage one, we basically are using the Gaussian process to model the image. And in stage two, we use the deep neural network to model the association. But in stage one, the Gaussian process, you know that very important point is how you specify the kernel, a covariance kernel in Gaussian process, which actually heavily have impact on the performance. Impact have impact on the performance. So then we also follow the DP network help us to learn the kernel from the Gaussian product in a stage. So that's why we have actually utilized the DP network in both stage actually. And then we also developed some scalable posterior computation algorithm. It's sort of like using deterministic algorithm, not really deterministic, but the idea is to try to do the optimization for post-war. simulation for post-part distribution simulation instead of like DWAM SMC. So let's look at the stage one. So here I have some notation so that you generally use a Y represent the outcome image X representatives image I index the subject. And we have a one location in the image U represented in the outcome image V represented in the predictor image. They can be different. So basically the space can Different. So basically, the space can be different. But for the application, you definitely can assume they are the same as well. And so the first stage is pretty straightforward. Again, this is more like the basis expansion type approach. If you find a very good basis to reconstruct this image, right? So then we're trying to assume that all the subjects across different subjects, they share the same basis in the outcome image and in the predicted images, they share a different set of basis. Okay. So these two. So these two, but this basis can be general in general like the kind of like a space. The V and U here can be like a dimensional Euclidean space, a subset of the d-dimensional Euclidean space. And I mean, in theory, any also normal basis image can be applied here, right? So just depend on the choice of your which basis you can use. And then we basically convert the image. Then we basically converted the image from the original image space to this vector space. This bold xi is a vector. So the length of this vector is a number of equal number of bases you use to represent all those image. And in the second stage, it's more like if we under the Bayesian framework, those basis coefficients is unknown parameter, right? So we kind of assign prior, right? We decide prior, like a basically normal distribution had prior variance, the lambda x. The lambda x. We link the associations through this conditional expectation for yi. The yi is a base coefficient from the outcome image and given the xi and it's followed under normal distribution with mean initial expectation of y given xi deep neural network. Okay. So, and we also allow some prior variance. And so if you consider stage two, I think just the prior specification for stage one, right? So but everything we But everything we assume the basis are given, basis functions or images are given. And I think the first speaker in this session already explained what is deeper neural network of feed forward. So here we're just using a basic feed forward neural network. It's not a transformer, not fancy. But the key idea is we can replace different neural networks here if you want. So if we combine these two stages, you consider the Bayesian. These two stages consider the Bayesian framework, and if you link the basic theory of the Gaussian process, you actually can say you have a predictor image all a Gaussian process combined to and with the covariance kernel you can rewrite is based on this basis function, the privariance of the basis coefficient. And similarly, you can rewrite everything as original image observed image Y given the predictor imaging. So basically, it's also another conditional. So, basically, it's also another conditional Gaussian process. But the expectation basically, you can we have a specific formula, you can reconstruct from the predicted edge to the outcome, right? So we have the formulation under, and also everything is under the Gaussian process framework. And this becomes to, if you pre-specify this kernel here, then you can actually carry out analysis. You can, from the kernel, reconstruct the basis function and then do the two-stage analysis, and then you can get. Two-stage analysis, and then you can get the model fitting, and then you can do the prediction. So, here, the idea is that we want to do a little bit better on the directly specify the covariance kernel for Gaussian process. So, there's a lot of Gaussian process kernel can be used, like RBF kernel, the exponential square kernel, or like the maternal kernel. And here, we are trying to see can we learn the kernel of using neural network? So, instead of like directly specified, which actually can Which actually can capture some subtle kind of like a difference between different images and also and also have more flexibility. So the key idea here is that we basically stack all the image across subject, right? So across subject together. So here I just take the predicted image as the example, and then we basically consider feed forward, another feed forward, a neural network, and the input will be the location, right? Input will be the location, right? And the output will be the n-dimensional observations, right? So, and for each location, you actually can observe the n, uh, the image intensity value across n subject, right? So, then you basically can use a neural network to predict this n subject at this location. So, and once you get the neural network feeding, you can just do the authorization. So, one simple idea, you can use like a Gram-Schmidt process to organize those functions. Orthogonize those functions. It's not necessarily orthogonal to each other, but if you do STD, you can directly get the orthonormal vectors, right? So which could approximate this basis function. And the same procedure, you can apply that to the outcome image. So basically, the theta x here is a parameter, the weight parameter or bias parameter in neural networks. So that parameter actually can determine the sort of like the problem. Of, like, the property of this orthogonal basis function. And that is actually corresponding to the kernel learning because you can use orthogonal basis to reconstruct the covariance kernel. And for the posterior computation, we have this like a hybrid algorithm, right? So we assume this orthogonal basis has been estimated based on kernel learning first. And then in stage one, we just Then in stage one, we just do the given the basis function, you actually just do the keep sampler, right? So that because it becomes a linear regression, right? So you know the design matrix, you know the outcome, you can, for each subject, you can theta linear regression, get the basis coefficient. And this is easy to do the parallel computing. And then in stage two, we're trying to make it scalable. We actually borrowed a very interesting algorithm. It's called a stochastic variational grading descent. So the key idea is that they are. So, the key idea is that they do not directly sample from posterior distribution. They find a sort of approximation to the post-story distribution, but they do not. This is different from like a regular variation inference, have to assume the working density function, right? So you have to do that. But this one is directly sort of like a model sample of the run variable. Suppose you need a 10,000, maybe 1,000 sample, and 1,000 sample, and then you just explicitly assume those 1,000 samples here, and then you try to. Here, and then you're trying to move your sample according to the direction of the targeted density. So, so the key idea is like this. So, suppose your targeted density, this is one illustration. Target density is a bimodal mixture of normal here. You just randomly sample from one distribution, and then and then you get those points. And the initial distribution summarizes like this, right? So, the idea is that you can use a green design type. That you can use a green design type algorithm, decide where you move those points, and such that their distribution close to the targeted distribution, basically. So then you can see this is a basic like iteration based on this grading descent algorithm. You can gradually from the first iteration up to 1,000 intuition, you can actually get a very good approximation of your targeted density. So this is the diagram we actually adopt. And this is not our idea. Not our ideas, it's actually from this neuron paper. And there are some subsequent work has been developed, but this algorithm is scalable to our problem. And so here is overview of our procedure. Basically, we start from the predictor image and outcome imaging. We use kernel learning using GPU network to get this basis image and separately from the outcome image and the predictor image. And then we feed the deep sample. Then we feed the keep sampler using keep sampler feed the Bayesian linear regression to get the projected predictor and projected outcome. Then we feed the keep neural network to model the association. And so here are some examples that to give you some basic idea about this procedure. Like we have some synthetic data, the MNIST, very famous, like a computer vision benchmark data set. They are trying to To trying to, we actually create some new data set based on that data set. We try to mimic the calculation of a two plus one equals three and two minus one equals to one, right? So, and we basically randomly sample the one, two, three image from the database, and we design our own minus and the plus sign and shift and enlarge the size a little bit. And then we create this predictor image. So, this predicted image is a, we created 1,000 such predicted image. We created 1000 such predicted image and the corresponding outcome image that matched with each other, and then we just feed our model and also compare with other models to see whether they can predict accurately. So, this actually is a relatively easy task, right? So, if you imagine, I think the key here is that whether your predictor is a minus or plus, right? So, then you just identify it's one or three, right? So, in this particular task, it's very simple. But there are some variations here because two and one and three, this shape are so. And three, this shape can be different, can be varied. So, and it depends on different methods whether they can actually capture the key feature from this predictor and identify the association between the predict and outcome. So, you can see the comparison and here is trying to, we actually repeat this procedure 50 or maybe 100 times. And we just see which how many, this is proportional how the time that you can perfectly recover the association. You can see. The association. You can see, we actually try different kernel learning methods. So you can see that you can directly do the PCA on all the images, right? So we get those kernels. That will be also turning away. But you can actually get a very perfect training, but the testing is very bad for the PCA method because they actually fit a lot of noise in the data. And if you try different kernels, so this is a square exponential current. This is maternal kernel. So you can see the performance for this task is pretty good, especially for maternal kernel. It actually quite. From a turn kernel, it's actually quite comparable to our method. But if you try all different architecture of the deep neural network, include this state-of-the-art neural network, you can see this one is pretty powerful, but all other architecture cannot beat our method because, and also their number of parameters. So here, this is a number of parameters in those models actually is pretty large. And we have another example, try to create some like a slightly. To create some like a slightly slightly difficult scenario. So called it fashion amnest. So this is a sort of extension of the amnest. They have this t-shirt, a sneaker boot, those fashion products and a green image. And we're trying to basically create the prediction image. Again, we just create a predicted image by based on the quantiles of a non-zero voxel intensity, right? So the predicted image will be a larger size, right? So you have it just. So you have just for each sub-image, you show the different quantities of the original intensity. And we are trying to see whether the method can recover it, right? So recover the outcome image. And if we compare the performance of our method with the convolutional neural network, it's quite similar. And if you closely look at some like kind of mean square arrow, so our method actually can recover, have a best mean square arrow. Have a best minimal error in the testing data. For the training data, again, PZA can do better, but testing is a pretty sort of overfitting. So, and we also apply our method to this human connecting project and are trying to, this is a preliminary analysis. I think we are currently run more like an association study, but here again, we're trying to study the individual variability in the. Study the individual variability in the task function brain activity, and we want to understand how much they can be explained by this resting state functional activities. And we predict currently we choose this resting state FMI, but in order to compare with other method, we particularly using this low fractional amplitude of low frequency fluctuation, basically it's a matter of the proportion of low frequency fluctuation to the whole detectable frequency range of the Detectable frequency range of the F resting C Apple Matt Ham series. And this one, the advantage of this one is that you can get the voxel-specific values, right? So, of course, you also can get a weighted degree, different type of voxel specific values. And this primarily matters the local brain activity at that particular location. And for that outcome image, there are many different tasks. Here we focus on the working map. We focus on the working memory task, which is basically trying to check whether, like they have two unbag tasks, right? Two back versus zero back. The two back task is try to ask the subject whether they can identify the picture is the same as what was presented in the two pictures before. The zero back is basically whether they can identify it's presented at the beginning of the experiment. And we actually all the image we register to the same space for the comparison purpose. Same space for the comparison purpose, and we can we actually the data set we have around 900, which is actually 10 times larger than the original paper. And we do this, we apply our method with some specifications here, and we compare the different method, inclusive of our method, and some linear regression method has been used before. And we actually primarily apply this method, actually, summarize the result. We actually method actually summarize the result. We actually apply the method for each voxel, right? So and basically we can have results for each voxel, but we summarize with us by these AR regions and we identify two regions has a pretty relatively good performance in terms of prediction correlations. And this prediction correlation, I mean, this, sorry, go back. The selection of the region basically is based on all six methods. We have six. method we have we have a six method right so we we compute the prediction accuracy uh and take an average of all six methods and identify the region and do the comparison make sure it's fair comparison and identify the top regions here as two regions in the frontal lobe and which is quite related to this working memory task and and you can see our method compared with our competing method we have a on average the better prediction accuracy in these regions and also for those And also, for those, we can show the workshop as prediction accuracy. And in some regions, actually, actually here, the prediction accuracy for some method, they can actually have negative prediction correlation, which is bad, actually. So like those state-of-the-art deep learning methods, on average, they may have a small. May have a smaller prediction accuracy, but for some individual voxel, they may have a better prediction accuracy, but for some region, they really predict that. And our method is trying to sort of like, we try to understand why our method on average predict better. So we go back, look at some like working memory as a basis image for the outcome image, and then we can see our method can, as here is a top five, the basis image that we can identify the majority of our region in the frontal lobe. Region in the frontal lobe activities, and our this top five actually has a lot of variability in that regions. And okay, that's all so far we have done. And we plan to do more analysis on the HTTP data, even larger data set like UK Bell Bank data, potentially we can testify our method and also try to improve the posterior composition scalability. So, and yeah, our method has some. And yeah, our method has some kind of advantage. I already talked about that. And this is actually primarily again, my students' work. And initially, this was a course project. And the students actually become more kind of motivated to do more. And then it's developed this project. And it's an all three students in my class. And I mean this group project. And we wrap it up to a better project. And hopefully, again, publish in the future. And all my research is supported by those grants. Yeah, and all my research supported by those correct. Thank you all. Any questions? Thank you. Um, so we've got time for one question.     