Okay, so for our next talk we have Melanhe and he's going to tell us about upper tail scaling of continuum path measures in KPC. Hello everybody. I'm very happy to be here this week. It's been great so far. Happy birthday to T-Month. So I'll be talking about the upper tail scaling limit of continuum pathways in KPZ. By this I mean I'll be talking about zero temperature geodesics in the directed landscape as well as positive temperature polymers in the As positive pressure polymers in the CDRT. So it's very useful for me that I've come after Schwan. She's already introduced many of the things. So just to quickly go over it again. So we're going to look at one of the models is the direct landscape. So this is some continuum model of paths. And you have this four parameter family, as Sean was saying, L X S Y T. Just say that I switched the order of the coordinates because Sean. So this is the starting point, this is the ending point. And this is the outlet quote. And what this basically encodes is: basically, you can think of every path, every continuous path in this environment as being assigned some weight, some random weight, and you take the maximum over all of these weights. And so you've shown that there is some path which achieves this maximum, and that's called the geodesic. Looking at it as kind of like a metric space, essentially. So, in particular, if we fix the starting point 0, 0, then If we fix the starting point 0, 0, and we vary the ending point at a particular height 1, you get some function, some random function. This is called the parabolic A2 process, but basically it's the weight profile of the geodesic as the endpoint moves along this line. So this is the first model, zero temperature model. The next model is the positive temperature, CDRP, in which you have a, looks very similar, it says the exact same diagram, as you can see. As you can see, you have some space-time white noise, and heuristically you assign every path in this environment some random weight, some integral over the white noise, and then you can define a partition function as the exponential of this random weight, and you can use this partition function to define a polar measure in the usual way. And I'll be denoting the process in this situation, the free energy process, as H. If it's just the log. As h, so it's just the logarithm of the parts of the function. So again, 0, 0 is the starting point, this is the ending point, and if you vary at the x location at a given height, you get some profile. And you will see this is the KPC equation, started from narrow-edge initiatives. So the point, we are concerned about the path measures. And first thing to notice or observe again is that the path measure you can read all. Is that the path measure can you can read off after the path measure from these two processes? So, for example, if you want to look at the location of the geodesic at a given height S, you just look at the weight profile, so you want to look at it at height S, you look at the weight profile from here to some location Z, you look at the weight profile from here to some same location Z, and you want to maximize these two, these two, the sum of these two processes. And the location of the maximum will exactly be the location of the Will exactly be the location of the geodesic at that height. Similarly, if you want to understand the marginal of the polymer measure at that height, you look at this convolution. You can think of it as a soft maximization. You take the sum of the free energies up to that point, the free energy from that point to the end point, and then you normalize it by the partition function. And so, the point that I want to make here is simply that you can read off a lot of properties of the path measure from these two processes. And so, to understand the path measures, you have to understand these processes. well. Okay, any questions? Okay, so what is this process? So you know as you've already seen several times, it is the KPZ equation. It solves it as stochastic PDE. Compared to the usual coefficients, I changed it a bit. I put one fourth here. That has to make them both have the same parabolic curvature, which I have not really talked about. But basically, to have the k. Basically, to have the Hay-P equation converge to the directive landscape, you need to make the coefficients match, and this is the way to do it. And so the way to solve this is that it's just defined as the logarithm of Z, where Z solves this multiplicative stochastic heat equation. And so as Sean was saying, this is introduced by, this four-parameter field was introduced by Albert Spenner-Costelle, and the regularity of recent study by many people in this room, Albert, Chris, Ferras, and Matt Timo. And so, And so this is just the definition of this process. All right. So I'm going to be talking about the upper tail, the behavior of the patterns under the upper tail. And many after the upper tail have already been studied. So obviously there's a quick summary or some selection of results in the area for before. The area from before. So, one-point R deviations are per tails for L, which are basically the Facing Widow distribution, were known very long ago, basically from the very first work of Facing Widow. Timo and Kurt Johansson have studied one-point large deviations in different preliminary models in the late 90s, early 2000s, TASEP and geo-traditional PP. Li Cheng and Jeremy have studied the profile occupations of TASEP. There's been some work on the On the cadence equation, Either and Prohibitosho, myself, and Shoshin Du Dougli. I should say that all this stuff that I'm going to talk about for the results is work with Shoshin Du Damuli and Ling Fu Zhang at UC Berkeley. And there's also been some work on phenomenal models for ASEP by Chauiang Das and Wei Tao Zhu, and Solo Tan, Petrov, and Chilokong. It's not a complete exhaustive list, just to give you an idea of what some things are thinking about. So, as I said, those results were more about the large deviations of the profile or the one-point large deviations, but we are interested more about how the conditioning on the profile affects the polymer measure or the path measure under this coupling defined by this maximization process, maximization procedure. And so, what we're going to do is we're going to set this one-point value, which is, if you remember, this is exactly the weight of the If you remember, this is exactly the weight of the geodesic from 0, 0 to 0, 1. We're going to set that, or alternatively, in the positive temperature case, the free energy between those two points to be large. So we're going to say, we're going to make it large to be at least some parameter theta. And so what happens with the paths? There's some sort of energy entropy trade-off. If they fluctuate more, they have more environment to explore, but they also have to maintain this very high, they have to achieve this very high value. High value, which makes it very costly to do. And so the further up they go, the more they actually lose, and the harder it becomes for them to achieve this large number. And so the way out is to become more rigid. They will have much smaller fluctuations than they would normally. And as a kind of a side byproduct, the paths also become sort of highways. Other paths also want to use the same path to get to where they want to go. Let to go. So, in particular, it looks more like this. So, this was the more typical case, and now it kind of becomes a straight kind of thing. And so, we, what I'm going to talk about are two aspects. One is what is the size of this fluctuation? How wide does it fluctuate? And secondly, if you rescale by this width, what is the scaling limit of the process as you take theta to infinity? So, these are the two questions. So, this is the background. So, this is the background. If there are any questions, please ask me now. I'll move on to the results. So, you're saying that it's cheaper to kind of increase the weight in this one package? Yeah, exactly. There's a very small strip you'll want to increase along, and that is what the best way to do it. Okay. Yeah, so let's mention notation. So, gamma theta is going to be the geodesic in the direct landscape from 0, 0 to 0, 1 under this conditioning of having a very large weight. Conditioning of having a very large weight, and at least keep it. And so, the theorem, so this is a work in progress, it's not out yet, but anyway, the theorem is that the scale of the fluctuation is theta to the minus a quarter, and that once you rescale by this amount, you get a Brownian bridge up to some constant scaling factor. And this is the uniform topology, in the topology of linear functions. So, some background on the result. This is in the limit of data to And this is in the limit of theta to infinity? Yes, that's right. As theta goes to infinity, yeah. So, some background on this result. The one-point case of this result was proved earlier by G. Pengu using some exact formulas he developed for the location of the objects in the directed landscape. And so what he proved is that, exactly, the scale for the one point is 0 to the minus a quarter, and that it converges to the normal with the appropriate variance. And he conjectured that the same result will hold for the. And he conjectured that the same result would hold for the entire math. And this is kind of building upon or related to an earlier conjecture of Laso Bangui, who had studied the geodesic in exponential LPP again under this kind of large deviation conditioning. And I had again conjectured that in the limit, and n-go to infinity in that situation, you get this brownie bridge. So yeah, so this result is native, obviously, for the regular. Is stated obviously for the regular landscape, but it's kind of, you know, morally should be the techniques should be applicable to this space as well. That would be significantly more annoying to deal with discrete objects. Yeah. So if I do a different scaling where my scale space time and height of my theta, then the fluctuation will be over one is that correct? Sorry, so again, if you're not. But if I do a different scale in the newspaper, I think it's equivalent. Okay. I think it's equivalent. If you're space height and height of y theta, then the fluctuation. So you're going to make the distance as well to be very large? The height to be very large? Well, but then it's proportional to the landscape. Right, right. I haven't thought about it, but probably it should be equivalent there. But I mean, by rescaling, by symmetric of the landscape, we can break it down to something like this, I hope it's a scale. I'm sorry. The scaling of the half is given. Of the half, the speed of one actually? I could check it out. I'm not sure. Yeah. It's very simple heuristics and fairly. Yeah, right. So I'll. The second half of the talk is going to be about why these things, why you get the money support or why you get a brownie apprentice and so on. Does the landscape outside become independent? I would. Well, so the thing is that once, so well, yeah, so this is also what I'll talk about a bit. But if you go up, if you increase this thing by height theta, Up, if you increase this thing by height theta, you'll see a change. You know, the geodesics, if you start off, let's say from theta and a half away, they all kind of coalesce. So maybe outside of this, until you can look for something. But it has a pretty large effect, actually. Not on the landscape. The geodesic does want to take the highway with it, but the landscape outside might still be. Oh, that's true. Okay. If you put a tube or anything, like geodesics outside that's not allowed to put those tubes. Nerdled temperatures cheat scroll. Oh, I see. Okay, I have not thought about this, but I would think it should be like that here. Yeah. Okay, so that is the zero temperature case. We also studied the positive temperature, and it's the exact same result. What we look at, we look at the Aneels Polymer measure. Again, same endpoint, same height conditioning. And again, the same fluctuation scale, and get the same half of the gravity bridge, center gravity bridge. And so, yeah, so. And so, yeah, so Jeepeng's methods don't apply to bias temperature. And so, in that case, the one point was also not PhD. So, a natural question once I tell you an annual statement is, well, is there a quench statement? And so, this is not as written out, but this is what we'd expect to be true. Is that polymers are concentrates in a smaller window around some random curve. So, there's some kind of backbone curve. A backbone curve, which has the scaling limit, the same scaling limit. And around this backbone curve, the baller measure of concentration a theta minus half window around that. And you can kind of expect it to be theta to minus half as well. You don't think there are any log terms or anything like that. Yeah. Okay, so these are the results. And in the rest of the talk, I'll be talking about, I'll be talking lots of ingredients for these results and then how Of ingredients for these results, and then how these ingredients tell you various aspects of the result: y give to the minus a quarter, y bounding a bridge, and y given to the minus half. Okay. Okay, so the basic crucial ingredient is to understand what the profile looks like when you have the upper-head conditioning. And it's a very nice picture. What you do is, so I didn't say, but basically, if you look at the weight profile, The weight profile or the free energy profile at height one, typically it looks like a problem. So it's a minus x squared plus a stationary process. And so this is, so that's why I've drawn this minus x squared object below. And what you do is now you're asking for the process to go through this height point, zero theta. And so what you do is you draw the two tangents through this point to the parabola on either side. And the tangential And the tangency locations you can calculate are minus the other to the half, like plus the other to the half. And on this interval, minus to plus the other to the half, the shape of the profile is exactly this. And what's more, it has fluctuations around this shape, which are ACP Brownian. So it's basically the one-quarter fluctuation scale, which is exactly the fluctuation of a Brownian bridge on this interval. And you have some kind of sub-Gaussian type tails for this fluctuation. Functuation. So I should say that I showed him talking a lot about line ensembles. So the proof techniques for all these things are also line ensembles, but I won't have time to go into where these things come from. But essentially, that's why you're seeing these Brownian type of scales and tails. And so this is a crucial ingredient, and I'll point out that the slope of this line is of order of theta and the half. So this will be important later. So, this will be important later. Okay? Okay, so once you have a limit shape, once you have this kind of information about the shape, and you know you have some sort of Brownian type behavior, you can use this to kind of compute what is the probability of attaining this shape, what is the probability of getting this upper tail event in the first place. And so, in work last year, Shoshendu and I showed that using that picture that I had in the previous slide, you can get estimates for the one-point density of the. One point density of the KPZ equation. So, in this empower talk, I've been focusing on height one, but you can also get this thing for other heights kind of uniformly height. And you'll see that you get this, you get a short coefficient of minus four over three, which also matches with what it should be, what it is for the fracy-widdom distribution, fracy-widdom GUE distribution. And then you have this kind of sub-linear or second-order error term, which is not the correct error term, but it's what we could prove. And you know, this is a very useful result, but actually, it turns out that this is not enough for our applications. So, what will happen is that the path, we care about the path fluctuating on a very small scale. And so, what will happen is that when it does this fluctuation, it suffers some loss in its weight, but it's a very small loss in the weight. And so we want to understand what happens when you want to be at least theta versus when you want to be at least theta plus delta. And if you do that, if you use these upper and lower tailbounds, you can never get rid of this. You can never get rid of this theta power 3 over 4 error term. So even if delta is very small, the delta gets kind of washed out by the second order term. And so we need a much sharper comparison than what this kind of just upper and lower bounds provide. And so the next thing is something we prove in this paper, which is a comparison statement. And so not just using, if you know this limit shape and you can do some coupling arguments, you can kind of get rid of this error term. You can kind of get rid of this error term because you're looking at this ratio. So, somehow its error term is going to be the same for both of them. It kind of disappears. And then you get some comparison statement. And so if you look at theta plus delta versus theta, you get the main term is this 2 delta theta to the half plus this order delta. And I'll point out that where this term is coming from is exactly by the Taylor expansion of this 4 over 3 theta plus 3 over 2. If you look at theta plus delta and expand that, you will exactly get 2 delta theta to the half. And so, And so, essentially, unfortunately, you might expect that you could use this to improve this error term, but actually, that doesn't work for some reason. Yeah, so this is kind of the one of the Cushman gradients, more than this, actually, this is the Cushman gradient in this comparison statement between close by points in the table. So, you say plus order delta, so there's there's no theta in there? No theta there, yeah, exception. Um so that that is just constant widths. So that is just constant weights. So in particular, if you take delta to zero, you know, this is kind of the only thing that matters. Also, the point is that there's no, there's just a plus one. The coefficient here is plus one. There's no like constant term either. So because delta goes to zero, this thing just becomes one as it should. Okay, the next dimension gradient? No, that's it. Yeah, so I'll now explain how these Explain how these three ingredients, but more these two, the limit shape and the comparison statement, tell you, or you know, you can kind of guess from these two things why these objects are what they are. And how much time do we have? About 15 minutes. Okay. Okay, yeah. So, why is the physical theater the minus support? So, the first question. So, as I said, you know, if these two processes are basically stationary. These two processes are basically stationary after you add back in this parameter. And so, what that means is that if you look at the geodesic and you want it to go out by epsilon, that means that it has to suffer a loss of order epsilon squared. That's just the effect of this plus x squared over m. And so if you ask for it to go out by epsilon, but also to have a height of theta, that's basically kind of equivalent to asking for the original thing to be bigger than theta plus order epsilon. thing to be bigger than theta plus order epsilon squared without saying anything about where it should pass through. And so now if you, so now basically the probability of that happening is just the ratio of these two probabilities. And now you have this comparison statement. And so you get epsilon squared theta to the half. And so this is only a plausible situation if this probability is order one. It doesn't go to zero as theta goes to infinity. And so that happens exactly when epsilon is order theta to the minus four. So you see basically from this comparison. So you see basically from this comparison statement, or rather from the second-order behavior of the face-wood on the tail, you can predict that it should be theta of the minus a quarter as of the scale of fluctuations of the time. So, okay, so this is kind of the first order behavior. You now want to know how does this thing, you know, how does the process behave on this scale? And let's And let's do a similar calculation. So let's look at the situation where the polymer or the geodesic passes through location X at height S. So essentially, what should happen is that if you look at the entire uranium path, the overall weight should be theta, and so it should gain the weight basically linearly in its height. So if you look at the slope of general this height s, this should be about s theta, and this should be about 1 minus s theta. One minus n. And so if you accept that, then this should probably be more or less equal to this. At location x, you have these two proportions. And again, by stationarity and by independence of these two processes, they're in different temporal parts of the space, so they're independent, you get this plus s inverse x squared and 1 minus s inverse x squared. So again, this is something I didn't say before, but if you look at So again, this is something I didn't say before, but if you look at height 1, it's x squared, but if you look at a height s, it's 1 over s x squared, and similarly 1 over 1 minus s x squared. And similarly, the denominator should be something similar. You do the same thing, except now you don't have this x term, so you don't get a plus anything. So again, this is exactly the form of our comparison theorem, because these two things are independent, and these two are very similar locations in the tail. And same for those two. And same for those two. And so, okay, we'll scale x down by the thing we already saw in the earlier slide. And now, this is a bit of a couple of lines of algebra. But if you do this, you'll see that what happens is that you just get an s inverse plus 1 minus s inverse. And just by some nice algebra, this is just 1 over s times 1 minus s. And so if you write this out, you can put the x-ray on top, and putting it into the kind of the form of the normal density, you get 2. density, you get 2 times 1 over 4s 1 minus s. And so the exponent of the density is basically the same as the one point of half BS. And so you get this normal distribution. So in the true argument, we don't actually, we do something like this, but not exactly. We actually compare the probability of, we look at the ratio of this event versus the event where it has it to a different point y. Passes through a different point y, and then we get the ratio of the two densities. So we don't have to worry about like missing the polynomial terms and so on. And that is enough to conclude that it's Brownian, at least for one point. So, okay, so this, so essentially, for the one-point distribution, again, the comparison theorem tells you basically immediately that this thing should be normal. But we also need to know multipoint. It's not enough to identify the path notion from a single point. The path dimension from a single point. And so, for that, we need some new ingredients, we need some independence, and we need some decoupling. So, let me explain why or what happens. So, if you look at the joint, the formula for two locations of the geodesic at different heights, it's basically this maximization problem where you have two parameters, z1 and z2. So, again, you know, you're just looking at two heights, so you can be wherever you want over here, you can be wherever you want over here. Over here, it would be where we log over here, and then you can go back here. And so you get these three objects from Z just 2Z1S from Z2T. But the problematic term is this middle one, because now you have Z1 and Z2, which are both varied. So this profile, this limit shape picture I drew earlier, it had a fixed starting point. This is fine, and this is fine, but this is not handlable via that picture. But luckily, you have this other phenomenon of coalescence, which Of coalescence, which changes this, which simplifies this expression. So, by coalescence, I mean, because we've asked for this single path to be so good, every other path will want to use it to get to their destinations as well. So, what will happen is they'll all kind of, they'll actually, they'll get together extremely quickly and they'll stay together until the very last moment that they can. And so, what this means is that the LPP value, the distance from x1 to y1, plus from x2 to y2, is the same as the distance from y2 is the same as the distance from x1 to y2 plus x2 to y1. Just because they're using the same parts, the paths are all the same in those two sides. And so what that means is that I can pick x1 to be z1s, I can pick y1 to be z2t, and I can pick x2 and y2 to be just deterministic points, z0 and x0. And then I can write this two-part thing as a sum of two y. As a sum of two one-variable things minus a fixed thing. And that means that the double arcmax decouples into two independent or two non-interacting arcmaxes. And so now I can use these profile pictures from before. And of course, that's not enough because basically what I'll have now is like I'll have one arg max, I'll have one starting point here and profile up there. I'll also have a starting point. Profile up there, I also have a starting point here and a profile over here. And so, a priori, these are in the same, these are in the same kind of time strip, and so they're not independent. So, I can't really, you know, it's kind of hard to say how they're coupled together. However, coalescence again tells you that these two are also actually, in fact, independent. So, basically, what happens is that the first argmax will be happening in this part, basically only in this part where they're separate. The second argmax will be happening only in this bottom part. And so, they're well separated by the middle of the Separated by the middle of the strip. And so then they actually are independent, and you can do this computation. So, like I said, the two arguments occur in disjoint time stores, therefore they're independent. And so, yeah, so with these two things, the coalescence and the comparison statement, you can see why, even at multi-points, you should see brownie bridges on the right scale. Okay. And in the last bit, let's talk about why the polymerizer concentrates in this smaller window, t to the minus 1/2, around this backbone curve. So this is actually kind of very easy to see from this limit-shaped picture that I said earlier. So as I said, when you ask for the whole thing to be at least theta, the profile has slope minus 2 theta to the half. To the half. And so, what that means is that if you're looking at the free energy and you go out by distance epsilon, you go down by epsilon theta to the half. And so when epsilon is of order theta to the minus half, you put on by an order one amount, which means that for the probability distribution, the density has changed by a factor, a constant factor. And so these are all, you know, from the point of view of the polymer measure, these are all comparable. These are all plausible groups. And so for that reason, basically, you look at where this Basically, you look at where this profile is maximized, and then around the maximum location, you look p to the minus half scale width or around that, and those are the competitive locations for the odd merger. And yes, I don't know how much more time I have. If I have some more time, I can talk about the source of these limit-shaped pictures, if not all the colours out here. I don't quite follow. In this picture, what is S minus T? S and T are the heights. This is height S and this is height T. But in the middle somewhere. Yeah, right. So I'm looking at this situation. So I'm looking at height S and height T. So now, in this middle part, I have to allow both this and this to vary in the both theory. And so now that's why I have this delta large. And so now that's why I have this double argmax term here. And now I want to convert that into two single arcmaxes. And I can do that via this coalescence equality. Okay. Sorry, I forgot when I'm supposed to finish. Maybe I'm supposed to finish. Seven minutes left. Seven minutes left. Okay. Okay, maybe if there are no questions, I'll quickly try to tell you about where this. Tell you about where this picture comes from, where this slope picture comes from. Yeah, so I was worried about the time, so I kind of rearranged things. Okay, so as Sean was saying, these processes can be embedded as curves in Lyman samples. These are just some infinite collection of continuous random curves. And the nice thing about these kind of ostensibly more complicated objects is that they have a nicely Objects is that they have a nice resampling property. They have a boundary sampling property. So, what that means is that essentially, let me say in the case of the zero temperature process, if I erase the top curve on some interval, and I ask for the distribution of that erased part given everything else which has not been erased, it's just given by a Brownian bridge, which is conditioned to not intersect the other curves. And so it's a very explicitly sampling property in terms. Explicitly sampling property in terms of Brownian bridges subject to some conditioning. And so this is very useful because it tells us essentially that the limit shapes have to be convex. And so why is that? Essentially the point is that a Brownian bridge, if you look at an unconditioned Brownian bridge and you make it pass through two points, then it will follow the straight line between those two points. It will not do any other shaping. And so if you look at the case, suppose And so, if you look at the case, suppose for some, you know, somehow, suppose that the limit shape of this thing was non-convex. So, this thing here is the second curve of the Lyman sample. And so, because it's not intersecting, if the first curve has this kind of depression in the middle, the second curve has to have the same depression or something a similar depression on the same interval. And so now, if I resample this top curve on this interval where it has the depression, then And what the Brownian case property tells me is that I have to simply draw a Brownian bridge from here to here, which is conditioned to avoid this lower curve. However, because it's non-convex, this lower curve is already avoiding the first curve. It's already gone far down. And so if the Brownian bridge does what it wants to, which is just goes straight across, then it will already have avoided the second curve. And so there's no conditioning dissection. The conditioning does not have an effect on the profile of the Brownian bridge. And so that means that. And so that means that under the resampling, I've got a very different picture. Initially, I had this depression, but having done this resampling, it went away. So I got this straight line. And that's not possible if the true distribution of the Lyman symbol was to have this depression. So in particular, what this means is that there could not have been any kind of macroscopic non-convexity in the limit shape. And if you think about it a little bit more, you know, so I just said this very quickly, but if you think about it a little bit more, what Very quickly. If we think about it a little bit more, what it means is that the limit shape of the top curve will be the convex hull of anything you impose on it, as well as the second curve. So in particular, if you look at the second curve as a parabola, and you ask for it to go through a particular point high up, it'll adopt the convex hull of these two objects. And that is exactly this tangent picture that I drew earlier. And so that's where this picture comes from. And so that's where this picture comes from, and that's kind of what everything else follows from. Okay, so that's the end of my talk, and I'll just go back to the thank you so far, just kind of if you have one point up and one point down, you have complication and convex shit, I thought you could only use Barnabas to get I'm sorry, say again if you have what? I'm sorry, say it again if you have what? Like, you have two points, and one of them is concave, the other one convex. Right, the tent that you're drawing. Instead of one point conditioning. Oh, oh, okay, yeah, okay. Oh, you mean this? And one point is lower. So that one is. Oh, yeah, sure, sure. Wait, you mean like this? Yeah, I you said that has to be concave, but I thought. Well, so what happened? It'll just ignore this thing, it'll just go straight. Right, but if I ask to condition around that point, then it would become. Oh, sure, that's true. If you say, yeah, if you say I want you to pass through this point and this point, then it will have to write. That's true. And I thought, this can be using your method, this can be wrong. This can be shown, that's true. Well, it's a bit different from what I in the example I did over there because somehow you. You know, somehow you can't do a resampling over here, right? Because you've done this conditioning in the middle. So you cannot do the resampling over here. And so the output I gave golden applied. So essentially, the more precise statement is that anywhere where you can do a resampling, it has to be convex. So if you look over here, it has to be convex. If you look over here, it has to be convex. And so on. I'm wondering about this object that you had discussed, like landscape conditions on large scale. Is there some nice numerical methods to Is there some nice numerical methods to simulate those and that this seemed you know the computer would look like? Um I mean I suppose you could you could simulate you know discrete LPP models. Under condition, that's a question. Yeah, I don't know if there's a way to just avoid brute force. Maybe the best way is through the line ensemble. Gibbs not sure. Maybe you could do it that way. The line ensemble would be easier to condition some of the things. Yeah, I was wondering what's the fluctuations of the of the measure that you get at the end, the all the measure. So you say it's like take to the minus one half for the scale. Right. So you don't condition. Oh, you don't condition at all? Well, then it'll just be order one. I mean it's much smaller, much tighter. I mean just from these these heuristics I think I mean just from these these heuristics I think and I don't know a way to see it directly. Because this is not just an upper bound you this is you actually show that it's all the way up to the data minimum. We don't need to show this, we don't actually show that. Yeah, I mean we expect that to be correct, but we don't actually show that. What happens to the weight profile or to a density? Um yes, this is right. Yes, this is right. So, Xi Beng shows that in the zero temperature case, at a given height, it also has a Brownian-rich one-point marginal, and that's independent of the user location. Yeah, yeah. So, we don't need this, we don't prove this, but yeah, I don't know if we could prove it either. But anyway, so the point is that at a given height s, it'll be s theta plus. It'll be s theta plus kind of a normal something on the scale theta to the plus one quarter. So that's what it is in the zero terminal case. I would guess it would be similar for plus each other for shock. Curiosity, do you think you do similar methods instead of conditioning on a large TV and template profile, you do a difference and you look at what happens to coalescence points? Well, cost of points, you can't re-read them off of the Gibbs method, of the Gibbs line ensemble as easily. So I don't think it would be directly something an application of this method. But maybe you could do something, but that's. What would be like the corresponding statement for the coalescence argument in the cause of temperature? Right, yeah. Picture. Right, yeah. So I right. So in positive temperature, in positive temperature, it's basically also this, except you'll have some small error term on one side. So basically, these are equal up to some kind of exponentially small error in theta. This is the sort of improvement. If there are no more questions, let's thank our speaker again. So let's thank our speaker again and then we'll start back up at 4:50.