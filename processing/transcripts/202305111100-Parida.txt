I guess so. Okay. Is it here in one of the computers? Okay, perfect. Thank you so much. Last day of our talks here in Oaxaca. And today is my pleasure to introduce Dr. Lakshmi Parida from IBM, who will be talking about TDA applications. TDA applications in life sciences. Thank you so much. Thank you. I assume people can hear me. Thank you. Thanks to the organizers. This has been a wonderful visit, wonderful workshop, and I've really enjoyed it. Most of you know, I bored everybody to death talking about my experiences. Talking about my experiences here. But now we want to talk about science. So I'll talk about TDA applications in life sciences. So I'm going to assume that maybe some of you are not familiar with TDA. So I will. Okay, I got let me try to get rid of this one. Hi, Floating. So, I'm going to give one slide introduction to TDA, which stands for topological data analysis. So, what we are doing here is we are looking at the characteristic of the topology of objects. So, if you see in this two rows there, you see some objects in the top row, and you see some objects in the second row. The characteristic that we are looking for here distinguishes. For here distinguishes between the two rows, but not between the objects in the same row. So, if you have that object, you're allowed to stretch it, turn it, do everything, but not tear it. So, if you don't do that, the three become equivalent in the top row. So, whatever tool we are using, it's not going to distinguish. And the bottom row, assume that all the balls that you're seeing are solid, they are contractible to a point. And in the And in the top row, there is always a hole that is there. So that is the characteristic, or that is the invariance that we are looking at. But now you can say that, but we deal with data, we deal with points. How does it relate? And a simple way to see it is you have graphs. So you can have vertices in that, and you have these points, and then you have edges that connect this. So that is the notion of. So there is the notion of zero simplex, one simplex, two simplex, three simplex, and so on. Zero simplex is your point, one simplex is an edge between two points, two simplex is, let's call an hyper-edge between three, one simpleisis, and so on and so forth. So a point, a line, and this is a filled triangle. If it's not filled, it's a hole in the earlier dimension. And this is a tetrahedron, filled, and so on and so forth. Okay, so this is how we will think of mapping our Will think of mapping our data that we have onto this structure. And the characteristic that we will look at, easier for us to understand, is called the Betty numbers. Mathematically, it's a dimension of the homology group, but we can think of them as holes, how many holes they have. So let's quickly go through this thing. I'm assuming you can see this picture. Not my picture, I've taken a standard one, which is kind of very informative. So I have these different sets. Okay. And Sets. Okay. And B0, we call this B0, which is the number of connected components. You have four connected components here, one connected component, two, one, one. Now we go to the next level, B1, and look at the number of holes we have. This has no holes. This has one hole, two holes, no holes. And this has, you want to take a guess, how many in the first dimension? Actually, two holes. Actually, two holes in the first dimension. So, in B1 is dimension one. So, this is the holes that correspond to that. You can see this blue ring. This is an annulus. If it is not clear, it's an annulus, okay, like a donut, but a hollow donut. So, one hole is going around like this in dimension one, the other hole that goes around like this. And the most interesting one here is that there is a second-dimensional hole that is the hole of the annulus that you see. So, these are the holes. So, these are the holes that you'll see. So, this kind of explains the homology. And just like a graph is a collection of edges and vertices, you have complexes. Complexes, these are simplicial complexes. I told you these individual simplicial complexes, so you can have your data. It'll map into some kind of complex that you'll see here. There are names for it, Czech, Alpha, Viatoris, RIPS, and so on, but you can think of them as complicated. As a complicated, high-dimensional graph within quotes, if you want to have a mapping there. But of course, when you have a graph, you can ask questions about the graph. You know, is it connected? You can ask many things about it. And so can you here. But there is another component that makes it dynamic is with the notion of a threshold. So, what it means is that the threshold, I'm just calling The threshold, I'm just calling the thresholds like a knob. So you have a structure and you start turning the knob, and that whole structure starts changing. And the invariance that you will see as the structure changes is which were the homologies, because that's the characteristic that we are studying. So holes will be born and they will die and so on. And this, as you change the knob, you will begin to see those. And that's the more even more interesting characteristic that you can grab. You can always grab this B0, B1. Always grab this b0, b1. These are just numbers, these are your summary topological summaries. But you can make it even more interesting with some dynamism by doing that. And what I just explained of changing the knob and changing the structure as you go is called filtration. And what you see here generally is called persistent homology. And this is how we associate persistence: is that I had a hole. How long does it persist as I keep changing the knob? Changing the knob. And the notion there is that if I change a knob and I had seen a characteristic, a hole, and it just immediately vanished, they'll say, oh, possibly it was noise. And if you keep changing it and the hole persists, maybe that is your signal. Okay, that's one way of looking at it. So when you have this filtration and when you have the knob, you get an extra characteristic that you can work on. And these are called bar diagrams. Okay. And so this is kind of a one-page. kind of a one page one slide summary of a very complex deep field i'm just trying to get you to see how we will map problems into this so people who already knew about it thank you for your patience and i hope it helped the others who were not familiar with it okay so the norm what it is doing is that let's take this structure maybe let's so your Maybe let's. So, you're, this is an annulus. Okay, this is from Rob Greist's paper. And this is the shape that we know, but my data point is here. The points that you see is the data. And then you say that, connect this point if the distance between this is x. Okay. And then you start changing the x, increasing the distance. So at some distance, some points are connected, a little more distance, more points get connected, and so on and so forth. So the knob is a threshold. So the knob is a threshold, and the threshold you're free to choose based on what application you're doing. I'll show you some applications. So I'm going to, so I don't know why the top bar appeared. I'm not going to do the hide bar. I'm sure it's not hiding anything for us here. So I'll just continue. Is that in the omega space? I'm going to pick three problems, one from population genomics, one from the population. One from population genomics, one from metagenomics, one from phenomics. And I will very quickly go through it. I'm not going to do a depth, in-depth discussion of each problem, but I will show you how we map the tool that I mentioned to the questions we want to ask. If I have the time. If I have the time to go through all of them, otherwise I'll skip. But they're all so interesting. So first, I'll start with population genomics. I think the next speaker. I think the next speaker, Mashal, probably is going to talk about more in this space. So let me give the motivation. First, I want to say the problems we are looking at are important. So I'll first motivate the problem, that these are important problems we are looking at. And then I'll some sub-problem of it I will choose and I'll try to map TDA. So all of us here, of course, know that the DNA is like a palincest. That is, the DNA that I have, I am this collection of all these cells, but the DNA that I have. All these cells, but the DNA that I have is defining my phenotype, like my height, the color of my skin, my eyes, and so on. But it is also carrying the history of all my ancestors, going all the way back to when we were just hanging around in Africa in the savannahs. So it does carry all that history. And so this, you know, this is the sort of inspiration for the genographic project where we actually collected DNA from. DNA from around the globe, and we are trying to say, How trying to answer a question: how did we populate the whole planet? I think you are going to speak a little bit more detail. This is at a higher scale. So, I'm just kind of motivating the problem from there. I will not talk exactly about that. We had an out of Africa route, again, based on the data set that we had. And I'll quickly now jump to the model that we have. Simplistic model: this is a population model that I have, and then I'll ask the question. I have, and then I'll ask the question here. Every row is a generation, every dot is an individual. And when you go to the next row, you see those arrows. I hope you can see there are two arrows that is connecting the parent and child. So we make simple assumptions, panmictic population, that means any dot can mate with any other dot, no overlap of generations. You can actually show that even if generations overlap, you can reduce it to this model and so on and so forth. And now if I go. And now, if I take four, and this is the extant one, this is past, and this is extant because this is how the DNA material is flowing. And if I take four individuals from here, and I'll say, let me track their history back. Of course, this is all abstract. We do not have the data, but you see this purple structure here. And this purple structure that you're seeing here, because I just tracked from the entire thing, entire evolution I knew about this population, it is not a tree. Okay. Tree okay, and it is not that every single node has two parents, but there are nodes here with multiple parents, right? There are nodes here with multiple parents. You can just track them yourself and you will find. And then, you know, it goes on. We might go to a single ancestor or not. Those are different questions. And this structure is called an ancestral recombinations graph from the population genetics, genomics space. And then I'm trying to motivate my question: supposing you had two populations. So, a population evolves like this. Supposing you had a population evolving like this and a population evolving like this, and they interbreed. At some point, they get together and now they'll evolve together. The difference is that, so why would it be any different when they're evolving here and here? Because it is panmictic, any individual can mate with any other individual, but it is constrained to. It is constrained to only this part of the population. But now, when you mix here, they can start interbreeding with mating or whatever is the correct politically correct term. But they will start interbreeding. And this is kind of, we'll call as admixture. That means mixing of these two populations. And the question I'm going to ask is that if I'm given K populations, each has multiple individuals. Each has multiple individuals. They're defined as a sequence of polymorphic sites, SNPs. We don't have to use their entire genome because they're all similar. We just take the points where they differ. And then we say, is any of this population admixed? Is a question we'll ask. And then there is the other question also, which is the admixed populations. I just want to clarify that there are a lot of statistical approaches that actually look at this and similar problems, but I am going to try to apply topology here. Okay, that's a good question. If there is overlapping generations and so on, actually mathematically, you can show that it can be reduced to the model that I showed you. Okay, we just have to tweak the parameters a little bit, but it's going to be the same. So the simplistic model is more or less universal in that sense. You can think of a lot of other things also, but they can be reduced. So they can be reduced. Okay. So correct. That's a good question because when we are looking at discretely, we just take a generation when a child, when you have a child, that child has this two parents, that's a generation. And then when we do our calculations here and we have to convert it to time, we take an average generation like 20 years or 20. Like 20 years or 25 years or something like that, based on the problem. So, that's how you will convert that into time. Okay, so this is a model in principle. You can apply to humans, to animals, to whatever, as long as it's two parents giving rise to that. Because if it was only one parent, it'll be a tree. Okay, this makes it a little more interesting because it's an arg. It's not a tree anymore. Now, if I look at this structure here, and because I want to say that I want to use TDA, so I want to understand admixture. Use TDA. So, I want to understand admixture in this context. I have four populations here, and there I'm showing you four different scenarios here. So, the four populations are A, B, C, D. The data you will get is individuals from these populations, and individual is a collection of SNPs. Okay, so let's that is what you will get as input. But supposing this was, we don't know what the truth is, supposing this was, this is how they evolved, is there admixture here? Are any population admixed here? No. Actually, the answer is no. Is there an admixture here? Yeah. So this is an admixture because these two came here. And is there an admixture here? And which is the admixture population? Yeah. So by our definition, it will be only C, but D is not. Definition, it would be only C, but D is not an admixture, actually. D is not admixture. And you can do the similar exercise. So, to get an understanding, of course, we don't have the abstract thing. I'm saying, supposing we had that. And the characteristics, of course, I've already put it here, is that from that one, you get multiple routes, multiple paths to the root starting from there. And you're only climbing up. You don't go down enough. There is a direction, and you're just going up in that. Okay. So, this is the paper that we had seen by Rabada. Is the paper that we had seen by Rabadan on this topology of viral evolution, where you know they use topology to see reticulations in the structure, evolution structures. What it means here is that when B1, the Betty number, first-dimensional Betty number is zero, the structure is a tree. But if you have a cycle here, the Betty number becomes one and it actually captures the reticulations. It could have been admixture. Articulations. It could have been admixture, assortment, and other things, except that, you know, you'll get a hole in this one. But you could say that if I am seeing the hole, so what's the problem? I see that thing. The problem is that you are working in this bottom space. And in a sense, you're trying to reconstruct, if not explicitly, some implicit fashion, and then deriving your conclusions from that. So we had looked at this paper, we took inspiration from that paper. Okay. So you were going right ahead. So let me just gently take everybody there. Good questions. And I'm, you know, I'm going to focus on the tedia part, what changes we have to make to those and so on. So now let me let us look at our problem. Okay. Our problem, I showed you the arg. The arg, because of recombinations, is actually Of recombinations is actually full of holes. Okay, so it's not a question that is there a hole or not, it's full of holes because we are admixing, okay. And this is supposing, assume the truth that this is how it had evolved. We define a notion of scaffold. You can see what I mean by scaffold. We don't have to get into definitions of that, but this scaffold has holes, and this hole corresponds to admixing event, whereas these holes is part of the Is part of the arg because there'll always be holes because there is recombination. So, our problem becomes that: can I distinguish the small holes from these big holes? Okay, and again, in the question, you can say, I can see everything. You can't see everything because you'll be at the bottom level and you're only looking at the SNPs. So, there is this structure that is there from which we are trying to say, can we distinguish this? So, that is how we kind of pose the problem. So, there is a theorem which says that this, you know. Which says that this, you know, you can distinguish, and you know, there is persistence of these cycles and so on. I will sort of skip over that, and I'll just go to the result, one of the results, simulated result. So we use the same notion, and what we do is we take those and we do our TDA constructions and look at these kind of patterns that we get here. These are the bar diagrams, holes being born, dying, and so on. And then we say that was there an admixture or not? Say that was there an admixture or not. The real data that we applied is from the space of plant genomics. So, this is avocado. I mean, the land of avocado is coincident that this was used. And where we knew the answers, we knew which were admixed and which were not. And we could apply this technique and see that these patterns led us to that. And the only question it answered, is it admixed or is it not? And that's it. So, to the second part of the So, to the second part of the question, which are the admixed populations? You say that it was admixed, but who were they? Because the first one answered only. It's the same example as before. So when you say admix, you point at the population and say, that's the population that admix. There is a little bit of a mathematical problem here in TDA, because the holes, the dimension of the holes that you get and the holes, they're actually defining the basis of that space. Basis of that space and basis is not unique. So you can take your problem, go into TDA, work in that space, but you don't have a unique way of coming back and saying something about your data, which you want to answer. So we had to look at a construct which we call as essential simplicis and set the data that we have, the conditions that we have. So we define a notion of essential simplicis, which makes it unique. So this mapping becomes unique. So all I'm saying is. Becomes unique. So, all I'm saying is a lot of the tedious stuff we do, still, we have to build on some tools when it doesn't answer what we want to. Okay, so I'm really running out of time. So, I will quickly go over this. And this, believe me, answers the question of not only was there an admixture, but who were there? Because we have this notion of coming back to this. So, essential simplicities were used there. And there are simulations where we show the results. Are simulations where we show the results and so on. So, what I'm going to do is, since I have only five minutes, I had this metagenomics. Yeah. Not from this model, but you'll have to add more constructs to it to actually get that time. And that's a good question. Since we were not addressing that question, we didn't sort of look at it, but it is kind of possible because we had used in for human data. Because we had used in for human data asking different questions, and we know we had this generation and time conversions and going back and so on. So, what I'm going to do is I'm going to skip the metagenomics section, but it is a very good one. It is actually corresponds to the question that you had presented as a project. And I'll skip that and I'll go to the phenomics part. Summary, it's okay. Okay, I get very nervous when the clock starts. Okay. Okay, okay, I got the permission. So, okay, so I'm going to now focus on a metagenomics problem. Okay, so again, just to motivate the problem, to say that it is an important problem and so on. So, here is the thing. I'm born with a genome, which is my first genome. I got it from both my parents. But then I have a second genome, what is being called the second genome, which is these entire genomes of all the microbes on my skin, my mouth. On my skin, my mouth, my gut, and so on. And you can say, so what, right? And it so you we have on an average 37 trillion cells, but we have about average 39 trillion cells, approximately 1.5 pounds. If I remove all my microbiome, I lose weight by one and a half pounds. But the number of genes that they represent is in millions, whereas this is the number of genes is about 20,000. 20,000. And of course, this other thing is that I inherited it, at least when I was born, I inherited it from my mother, and I have changed it around based on what I do, my habits moved, and so on. So one of the things which is interesting about this is that you cannot change this DNA. At least we think we don't want to change, or we cannot, we don't want to change this one. But this is totally fungible in a sense. It's up to you and you can change it. And hence the interest of looking at gut microbiome. Looking at gut microbiome to affect your health and so on and so forth. And in this context, of course, the gut microbiome is considered the linchpin. So I'm just motivating that this gut microbiome problem is a microbiome problem that is looking at microbes from a space is important. There are other applications on soil microbiome, plants, and so on, but I'm just sticking with humans here. So, this is the problem that. Is the problem that Maribel had presented in a slightly different form? Which is so you have this metagenomic sequence probes a microbial community, and this high-throughput sequencing reads are a random collection from the sample's microorganism's DNA. And the short reads can match to multiple organisms in a database because there is sequence similarity and so on and so forth. And the question is: who are truly present and what is a false positive? Usually, the answer is given by. False positive. Usually, the answer is given by you get an ordered list, and towards the top, you have the true positives, and as you go down, you're probably having the false positives. Okay, so what we want to address is that there are a lot of other methods. So, again, let me repeat, there are many other methods. You can go investigate, look at the reads, k-merge, and so on, and do it, and they're all good. So, I want to, of course, apply TDA here and see whether we can get some results. And see whether we can get some results. We have a different answer there, but we'll take a look. So, the model is as follows. So, I want to kind of, so I'm calling this a signal enrichment problem. That is, I want to have more true positives than usual in my top part of the sequence. So, let's look at the problem. So, assume there are three organisms, A, B, and C. They're of different lengths. Because it was easy to draw this figure, they look like they're of the same length, but they're actually of different lengths. And you have these reads. Of different ones, and you have these reads. You have some reads that map only to A, the orange one, some map only to B, some map only to C, some map to A and B, and some map to ABC, and so on and so forth. So you can almost see where I'm going with this. And they are of different lengths. You can have genome coverage in the sense that you can also say it maps. Let's simplify it and say for the single read, what is the proportion of this entire thing it has covered by this read. So you have the genome coverage of that. Genome coverage of that, and of course, you can have a Venn diagram. So, each circle here corresponds to a microbe and the number of reads, but there are numbers associated. So, when you say SAB, that means those are the reads that mapped to A as well as to B. And each has a number, which is how much genome fraction it covers in A, how much genome fracture it covers in B, and so on. And this is a great setting for TDA because. Because I can take these sets as there's something called check complex you can use. So, when do I say you can take the Jakarta distance between these two and so on? And so, it gives me a way of converting this to a TDA problem. Okay. There'll be some details there, you know, but we can skip that. There's something here we call barycentric subdivision. Again, it's ideas from last century, barycentric subdivision, but something we had to apply it here so that our problem map there. So, there is a notion of filtration. So, there is a notion of filtration that I talked about, changing the knobs and so on. And we have essential simplicities because we are going into that space and we also want to come back to this space. And then I have the score. For this score that I have now, I'm going to use these TDA characteristics, which were those bar diagrams. And there is a magical formula. It is in a sense that, you know, we don't have any proof of that. The only proof we try to use is proof in the pudding. Proof we try to use as proof in the pudding, kind of a thing. So if it works, that's the score we are taking. But so this is what goes behind this, and we do the tests. So this is the score. Well, so you'll say, what is this? This is a score by which if you sort it, we say the true positives will go to the top. Okay, so this is how the TDA thing is used here. The TDA part, of course, comes in each of them and in the formulation of the score. And when we run it on the sample data, When we run it on the sample data, I see the Salmonella is here. This is the Salmonella data. And here we tested it where we knew the answers. And how does it work? So if you had one true positive, two, you know, this is under conditions where we know the answer and we are checking and it kind of works. And it is all great. And it worked. It works like the K-merse, although I didn't look at the K-merse. But just by these numbers and the depth, it is working well. I mean, I won't say well or not. I mean, I won't say well or not, it is working. Okay. If I didn't find my true positive at the top when I had one true positive, I'll probably go back to the drawing board and start doing things again. But I'm going to make it a little more, I'm going to twist the question around a little bit. And I'm going to say, I'm going to use less information. So we did take the genome coverage because we're all very aware of genome lens, how much it covers, and so on. And now I'll say, I'm not going to use that information. I'll say, I'm not going to use that information. Okay. And then I can have this Venn diagram here again. But here it just refers to the number of reads. I don't care about the size of reads. I don't care about what are the fraction. But if I just use only the number of reads and I have this overlap pattern, can I still answer that question? Can it still work? Of course, you probably know the answer that it will work. That's why I'm showing it. Yes. No, no, no, they want so when a read when a read is mapping to multiple organisms, so there is a threshold you're using by when do you call it mapped to this organism, right? When you're doing with the reference to the database. So we are using the regular parameters that is used there. So there is a knob there that can be used too, but I am going past that. Used to, but I am going past that. I'm just saying, now, if I have this and I have this big structure, does the structure tell me anything more than just the Venn diagram itself, right? So it turns out that this is simple. This is a simpler problem than before in the sense that I don't have the not the problem is still the same. My model is using less information than before. I am not using the genome fraction. But it turns out that it actually works. The data set that we The data set that we used, it actually worked. Okay, and this is an example. So, my conclusion is that I actually got more from less. I answered the same question by using less information. And I'm showing you our best results, of course. So, it even outperformed the regular ones. Okay, yes. Here's why I think the word I don't have the other part. Possible yeah, because at first glance it looks like you have to use information because you know if there is more coverage that organism exists, less coverage it doesn't exist and so on. Yes, it is quite possible. There are other things also that you know that can affect this. That can affect this model, but we can sort of discuss that often. So, there are other complications like if they're of different lengths, if there's a long guy, he's going to have more of that. You know, there are all of these, but overall, somehow you think every you're going to trip on each of these, but it turned out that you kind of ignore and go into the space and work, and it was. Into the space and work, and it was working for the examples that we were working on. Okay, not so bad. I have another, just another application, and I will, we have time for the last application, the phenomics application. Okay, so I'm good. Okay, yeah, I won't take too long. I'll just so this is kind of the philosophy underlying this. Let me just skip this, I'll just go to Like this, let me just skip this. I'll just go to what we did. Okay, so this is from the COVID data. Many of you may have seen this Venn diagram. So, what is the Venn diagram showing that patients were showing multiple symptoms? You know, there are six symptoms here, nosmia, cough, fatigue, diarrhea, you know, and so on and so forth. And then there was this beautiful picture, a Venn diagram there saying how many individuals had which of these symptoms. Okay, and the Symptoms. Okay. And the Venn diagram, and I think that was early times of COVID and so on. So that this was a good paper saying that, oh, people are coming, and not everybody is showing all the symptoms. This is kind of more or less the takeaway from there. And when I looked at this, you know, Venn diagrams are doing a lot of magic for me through TDA and so on. So my question was that, is there anything else that this is saying? I don't know what it is, but is there anything else we can learn just from this vendor? Just from this Venn diagram, just from a Venn diagram and no more information in the sense we go, we go to this, look at the samples, look at these numbers, and do our tricks. So this work was by Nagin. She was interning with us. Now she's a postdoc in Broad and she did most of the work here. So what we did is we actually went back to the paper and tried to get it. So of course we didn't get the same data. Don't get the we didn't get the same data. We got about, and what is showing in red is what? So, there were about approximately 1500 patients, and then we were getting different medical conditions. We didn't take the symptoms for some reason, you don't even see the symptoms there. Probably there was something in the pipeline that we didn't get access to. But we took the data and looked at these medical conditions that they had, and we trimmed it down to 31. To 31. So there were six there. You already saw it was looking complicated. So we took 31 of these. So these were the different symptoms with their codes. It doesn't matter what they are, but this is what you get. And what does the Venn diagram look like? It is 31 such. We don't even attempt to draw it. And because you're not going to visually see it, I just took a stock picture from the web saying, show me a complex Venn diagram. And here it is. Clearly, if we had that, we can't learn anything about it. We can't learn anything about it. This is a summary. That is, there were 651 patients with exactly one medical condition, 431 with two, but there couldn't be any two, and so on. So this is kind of a summary, doesn't make any sense, but just to get an understanding of this. And what we did is that we did a similar exercise of taking a check complex, defining on this, using our knob and turning it around and looking at this bar. And looking at this bar diagram, the bar diagrams that you get. And the interpretation for this is we are trying to look at these long persistent bars. You know, I mean, we can kind of discuss as to what we will get out of this, but what we did is we pulled some long bars out of this. And one of the hypotheses that came out of this is that the patients were coughing and the patients who were not coughing, they probably followed different paths to the disease. Followed different paths to the disease. It's, of course, an hypothesis, but this is a hypothesis that you get just by taking the so-called complicated Venn diagram. And there are a lot of other hypotheses that you can pull out and you can work with a clinician to see if it is telling them something, if it is serious. Of course, you have to follow up with other experiments, data collection, and so on. But this pulls this out. And we did a similar thing with the biobank data. And we did a similar thing with the biobank data. We're looking at severe COVID, about a thousand patients. We took about 16 labels. And this, what I'm just calling re-descriptions are kind of like logical functions, we can say. So which is a combination of and you know, A, and B, and C, and so on. Approximately significant 150 patients. You know, you can look at this picture. Even if you look at that, it probably doesn't make much sense. So I'll It probably doesn't make much sense, so I'll just take one of the hypotheses that we got out of it, which kind of garnered some interest even in the clinical community, is that a diabetes leads to severity, severe COVID, and obesity is a risk factor. But with respect to COVID severity, obesity and diabetes seem to be operating independently. That is, if you were obese, it did not increase your risk factor for getting. Not increase your risk factor for getting severe COVID. And this is something that we got only through TDA. And even if you just do odds ratio computations and so on, it actually evades you. Okay, it gets kind of complicated. But this is, again, I'm saying these are the kind of hypotheses that are non-obvious that you can pull out, which requires verification and further work and so on, which is interesting that you can pull these things out. Even certain pathways we were able to pull out, and then we saw that. We were able to pull out, and then we saw that there were other results through other means who were pointing at the same pathways. So I will summarize that we took problems from three different spaces. One of them we used topological signatures, the other one we used the formula in the bars, and the other one we use logic and topology. So we're mapping our problems into the space. Our problems into this space and trying to answer this question through topology. So I'll acknowledge my team here. And one of you is asking, so if you're working on topology, who is your topology collaborator? So we do collaborate with Shogata. In fact, a lot of the work that I, the results that I showed you, has many large number of collaborators. Since I'm pulling out only the math part, I'm acknowledging our math collaborator from Purdue. A math collaborative from Purdue, Shogada. And that's my team. That was a picture taken just before COVID hit us. So we could actually go run and so on and so forth. So that was a little bit of thing there. And thank you. There is a question from Zoom. Okay. Okay, thank you very much, Naktumi, again. And there's a question from Netli from Instagram. And Nelly from Zoom, from yeah, Dr. Nelly Salem. She will be presenting later and she wants to ask you a question. So, Nellie, please feel free to unmute yourself and ask the question directly. Nelly, can you hear us? Can some people hear us? Yeah. Okay. Nelly, do you want to ask the question directly here? Okay, I don't see that happening. So let's continue for now with the questions here at the audience. Yeah, Clara. Thank you very much, Sa. Amazing talk. What are you using? The TDA R libraries, because we used ones to classify micro and array structure and were working well, but I don't know if there is a new implementation to work again with this data. Work again with this data. So, we are working on our own set of tools called Matilda. And once it is ready, we have been working on it for a while. And we also occasionally use what is available outside. But we are working on Matilda and some of the math characteristics that I was talking about, like essential simplicities and so on, we'll make them available through that. And is Matilda open source or at this point? At this point, we're still working on it and so on. So, yeah, we will open source it and we will make it available. Oh, thank you. So, I would like to use it. Thank you very much. Yeah, thank you for your interest. Yes, excellent. Thank you. Really cool. If I wanted to start thinking about incorporating error models, like error in the sequencing reads, that might. The sequencing reads that might affect how mapping happens, error in measurement of on the COVID side, it might be incorrectness, but also sampling error on the population, size of population versus the size of your sample and that kind of. Yeah. So the way at this point, okay, so the way we are we say that we are taking care of the errors is in terms of the sizes of these bars, but we can have other ways. Sizes of these bars, but we can have other ways because if there are tiny bars, in fact, we are pulling out the bars which are big. So the noise to signal thing, we think if you have a lot of these little bars, a lot of tiny ones, they are probably noise. But you can have different models. So I'm just saying this is the most natural model that is coming from here, which is taking into account, say, the noise in the data. So you'll get these tiny, tiny things that you can get rid of. And the long ones, hopefully, are signals. And the long ones, hopefully, are signals. But it is possible to build on top of it other specific models. I just gave you the most generic and the way of handling noise there. So one comment. So the first example you had on admixtures, the problem at least looks a lot like the stuff that David Reich and Nick Patterson have been doing at Harvard. And so there are lots of data. And so there are lots of data sets around for that that would be potentially useful in a fair literature that didn't, I think, include any of the examples you had of how one would solve that problem. So it might be useful to look there. In the second two cases, I mean, my problem with the approach, and I think it's related to what Hector was asking about, is really the data you have in both of those cases is remarkably incomplete. Is remarkably incomplete. And it's hard to understand how that incompleteness manifests itself in the topology that you see, right? So, if you look at graph-based models in computational biology and people look at protein-protein interactions, for example, the fact that you don't measure them all and that you equate a lack of an edge with if you test it and you know there isn't. If you test it and you know there isn't one there versus you didn't test and you don't know there is one there, those have really big implications on the analysis. So, in the COVID case, for example, you got some data that said these people at some point in time had these symptoms, but you don't have complete monitoring of them. And so the data are just sort of incomplete. And I just wonder how you've thought about the fact that your topology has this, appears to have a very strong assumption that. Appears to have a very strong assumption that your data is complete. Oh, okay. So, um, yeah, great questions. Um, of course, I'm aware of the admixture thing. So, right. Like I said, in each of those, there are always other more conventional routes like statistics and so on. So, David Reich and others have been using that. And the admixture thing, we went to population, we went to the plant data because that is the only data we could get our hands on at the time when we were doing this. At the time when we were doing this. And then for the human data, we were trying to get some human data where we knew there was admixture and so on. It kind of didn't pan out, and then we sort of moved on. So the results that I showed you were mainly on simulated data and so on, because you could get answers on something and you don't know whether it's right or wrong. And the other point about completeness and so on. So I think, so here is what I think about the topology thing. It is not assuming and it's Topology thing. It is not assuming anything is complete, right? It is just saying if there is a lot of data, because completeness of data will always be a problem because you can always have more and more levels of data, right? You can always get more and more information about what you're studying. So what I think it works on is that your data can be incomplete and you can ask bigger questions. Of course, you're not understanding the pathways and other things that are going on, which will require. Other things that are going on, which will require further investigations, like in the phenomics, but it points in a direction saying, We don't know the details or everything that is happening, but if I look at this large space, squint my eyes, move back, and you know, look at this kind of structure, topology is saying, Oh, maybe there is something going on here. Could you go and check? But if you have to get the detailed thing, of course, yeah, you you need to do the usual route. One last question, right? Uh yeah. Where it starts changing your team. So, Prestar, go ahead. Nice talk, I wanted to know that if I want to replicate the results of the metagenomic paper, how much computer power do I need? Because I remember well, there were 36 strains in your looks strange in your in your so i have to with something like two to the two to the two to the power 36 intersections um okay so this is a this is a very specific technical question i have to talk to the team and see what it produced all i can say for that data set we probably didn't need very powerful uh you know or very uh intense computing so even if you have 36 organisms but if you do If you have 36 organisms, but if you don't have a read that is mapping to all 36, you don't go down to that depth. So the actual depth would be less than that, but it.