The tool from statistical physics to problems in combinatorics and computer science. But before I define anything sort of formally, I kind of want to spend a few times, a few slides, since it's Monday morning, just in a more relaxed fashion, maybe building some intuition, and so on and so forth. And I think this is supposed to be a sort of introductory lecture, so please just stop me and ask questions because I want everyone to understand what's going on. Okay, so. Okay, so let me start with a very old model of a gas in physics. It goes as follows. Take a big box in Euclidean space, and I want you to consider a random sphere packing of density alpha in this box. Okay, so what's a sphere packing? A sphere packing is just a collection of identical non-overlapping spheres. And when I say a random packing of density alpha, I just mean choose one uniformly at random from all possible input directions. From all possible infidi actions. Okay, so if alpha is small, then we maybe see something like this, and there's no sort of discernible structure to the position of these spheres. So these spheres are sort of supposed to model atoms, and that's why I said it was a primitive model of a gas, sometimes known as the hard sphere model. And now I want you to imagine cranking up the density alpha, and what you might see is a sort of some more structure emerging. Maybe it's sort of four. Emerging. Maybe it's sort of forced into a lattice-like pattern. And so we've gone transition from disorder to order. And this is what physicists might call a phase transition. But let me start right away with an open problem, which is prove that a phase transition exists in this model. So it's quite remarkable. So I haven't told you exactly formally what a phase transition is, but any sensible way in which you make that formal, it's open to show that such a phase transition Such a phase transition occurs here. But that's not the end of the story, so this is to spend a lot of time developing rigorous tools to probe these kinds of phenomena. And one of them is the cluster expansion, which I'll define a little bit later. But let me just say, so tracing the origins of the cluster expansion is, well, I find it a little tricky, but Meyer's name comes up a lot. So sometimes it's called the Meyer expansion as well. So you might have seen it in the lab. Expansion as well, so you might have seen it under Latin. Okay. Let me show you a picture actually. So, this I found on Wikipedia. So, this is people, people, you can try and probe phase transition in real life as well. You can take a genuine large box and throw in tons of identical spheres. These are called colloids. They're little glass spheres of diameter about 10 microns. That's about the width of a human hair. And maybe this picture isn't so illuminating, but what we can do is. Maybe this picture isn't so illuminating, but what we can do is draw a point at the center of each sphere and draw an edge between them if they're touching, and what we see is this. And so what's going on here? We see these, the colors represent the degree of the vertex. So if I have six white edges coming out of me, it means I'm touching six other spheres. And I get different colors for degree five, degree four, degree three, and so on. So why am I showing you this picture? So, why am I showing you this picture? Well, one, I think it looks nice. And two, I think it sort of shows the kind of complex behavior you have to capture with these models. It's sort of like a visual challenge. And also, I thought it's interesting because it shows that experimental physicists also think about these types of problems using ideas from combinatorics. In this case, kind of a sort of graph theoretically. A sort of graph-theoretic visualization of what's going on. So, I couldn't actually get the name of the team of people who made these pictures. Wikipedia wouldn't divulge this to me. I thought it was pretty cool. Okay. So, in this region where it is fully packed, it is really fully packed? Because it is never fully packed. So, why there are no small? So, yeah, these regions are fully packed where you see this white hexagonal. They're really touching. They're really touching. They're really touching, yeah. I guess that's the ones. Yeah. I don't really know how they went about exactly what means, like maybe they're almost touching. But yeah, we see these sort of hexagonal lattice type islands separated by these interesting marks of like less well-behaved spheres. Okay. Ah, so this is sort of. Ah, so this is sort of combinatorics coming into the frame, and I want to make another jump to combinatorics, which is slightly different. So, combinatorics is discrete mathematics. So, I just want to discretize the whole problem. Okay, what might that look like? So, instead of Euclidean space, let's take the integer lattice. So now let's take a large box in Zd, and I want you to think of these pixels here as little points on the lattice, and they're Points on the lattice, and they're colored blue if they're on. So Zt has even vertices and odd vertices, depending on the sum of their coordinates. So this is a bipartite graph. So I'll say a vertex is blue if it's even and red if it's odd. And so here it's small, and I'm going to take, what's my constraint? Two, I can't have two neighboring squares occupied at the same time. Okay, so this is one definition of a, or the definition of an independent set in Zd. And now I want you to imagine taking an independent set of density alpha in Zd. And at small alpha, again, we might see sort of no discernible structure like in this picture here. But what might you see when you crank up alpha? What kind of structure? So preferably someone who doesn't know the answer. For example, I stole this picture from Will, so he certainly knows the answer. Certainly knows the alpha. What might you expect to see now as increase alpha more and more? One colour dominates? Sorry? One colour would dominate. Yeah, great. Thank you. Yeah, one colour might dominate. So we're sort of i in this graph we know what the maximum moment independent sets look like. Independent sets look like is to take either the whole odd subletters, the whole even subletters. And so if we increase alpha, we're sort of forced into one of these extremal constructions. And we might expect something to see something like this. So here we have this sort of nice visual representation of the phase transition. And the cool thing here about discrete world is that we can prove these phase transitions exist. So this was first done by de Brucian, and then the location of the phase transition was. Of the phase transition was refined by Galvin and Karner and fellow in more depth. So now we can actually start proving things, which is one major motivation for passing to common star as well. Okay, any questions on this so far? Great. Okay, what I want to convince you today in this talk is that phase transitions are actually everywhere in combinants Rx. Maybe the first example that comes to mind. Maybe the first example comes to mind is sort of the emergence of the giant in GNP. But really, they're everywhere and often studied without using the language of phase transitions. But the tools and language of phase transitions can be very useful for studying these problems. And so I'm going to focus a little bit on an object very dear to combinatorialists' heart, which is triangle-free graphs. Is everyone happy with what that means? Happy with what that means. So, graph about three vertices all mutually joined by an edge. Okay, no click of size three. And we can play the same game. So, I want you to consider, as the thought experiment, to take a triangle-free graph from n vertices and n edges uniformly at random. So, just take a big bag of all the triangle-free graphs with exactly m edges, and then just select one uniformly at random. Okay, I should probably explain this sort of impressionistic picture to the left. Of impressionistic picture to the left. This is my interpretation of like a random-looking graph with no discernible order. It doesn't have triangles, but apart from that, it looks like a random graph. It's like an impressionistic graph on or something. Okay. So that's small m. I won't define small just yet. Who can tell me what the picture should look like for large m? Yeah, great. Maybe we look almost bipartite. Maybe we don't look exactly bipartite. Maybe we have a very dense cut and some vertices within the cart. But we might expect to look bipartite. This transition occurs around m as n to the three halves. And the intuition for that is that's when a random graph, that is Raymond random graph, wants to have a triangle at every edge. And so to avoid triangles, And so, to avoid triangles, you start to have to enter a sort of global conspiracy at some point and become a fall into a bipartisan structure. This transition is happening around m is n to the three halves. And of course, what's the driving force behind this phase transition? It's one of our favorite extreme or combinatorics results, which is Mantel's theorem, which says if you're triangle-free and you want the most edges, you need to get a complete balanced bipartisan graph. Complete balanced bipartisan graph. And so that's the kind of driving force behind this phase transition. You can play the same game now with any of your favorite extremal combinatorics results. So you could study from this perspective some free sets in the integers or some arbitrary group or anti-chains in the Helian lattice or look at error correcting codes and so on. And this isn't just a superficial analogy to the phase transitions I was discussing about like sphere patterns. Transitions I was discussing about sphere packings, some of the very same tools used to study random sphere packings can be used to study these problems as well. At least that's what I have to convince you of today. Okay, so I think this concludes the kind of hand-wavy introduction. I should start defining some things now. But before I do that, any questions? Okay, so let me introduce the protagonist of this talk, which is the hard. Of this talk, which is the hardcore model, and I expect this character will pop up a few other times this week. So, what is the hardcore model? It's a probability distribution on the independent sets on some graph. So, g is my graph, and I have some activity parameter lambda, and I'm going to select an independent set from the graph with probability proportional to lambda to the size of the independent set. And this normalizing factor, which makes this a probability distribution, is known as the. Distribution is known as the independence polynomial, the partition function of the Hardcom model. And notice this is a little bit different from the random independent set I mentioned a few slides ago, which was just to take one at a specific density uniformly. Here we have this parameter lambda, which sort of allows us to tune the density a little bit. If lambda is small, we expect small independent sets. And if lambda is very large, we expect larger independent sets. Okay, let me give you a very silly example, which will sort of be vaguely relevant a bit later, which is just take the empty graph. What if G had no edges whatsoever? Then every subset is an independent set. And so what's the partition function? I'm just summing over all subsets lambda to the size of the set. So the partition function is just one plus lambda to the number of vertices in my graph. And what is the hardcore model? Well, any set is valid, and I'm Is valid, and it's a binomial then subset of 1 up to n. There are no interactions, they each get included independently with probability, say, lambda over 1 plus lambda. Okay, and as we add edges to the graph, this becomes less and less true, and the hardcore model becomes more and more confusing. Okay? Very good. If you're a combinatorialist, you might be specifically or particularly interested in lambda equals one. Why is that? One. Why is that? Well, if we plug lambda equals one here, we're counting the number of independent sets in the graph. And so if you're a combinatorialist, or if you're a particularly rude combinatorialist, you might ask the following question, which is why would we bother with general language at all? And hopefully this talk will convince you that there are several reasons for doing so, but let me give you one immediate reason, which is you can write down straight away, which is this partition function at General Lambda encodes lots of useful probabilistic and Encodes lots of useful probabilistic information about the Harkonnel model, which is also relevant at lambda equals one. For example, if you wanted to understand the expected size of an independent set from this model, you can just write down the definition. It's the size of the independent set times the probability of picking that independent set, which is just some logarithmic derivative of the Classic function. So we can access moments of this model by derivatives, and we can access higher moments by taking higher derivatives. And you can access higher moments by taking higher derivatives. And I'll go one step further and tell you that it's also useful to consider a multivariate generalization of the Harko model. So here I'm going to put a different activity at every vertex of my graph. And now I can select an independent set with probability proportional to this product over all the vertices of my independent set times their activity. So these are all the same. This is just lambda to the i like we saw before. Eye, like we saw before. And we have this multivariate partition function. And an immediate reason for considering these things is that this encodes more intricate information about our model. For example, we can start talking about correlations by taking partial derivatives. And there's another more subtle reason why you might want to consider this. So notice that this makes perfect sense for. This makes perfect sense for complex lambda, whereas this doesn't. It's not a probability distribution unless all these lambdas are positive. And so it turns out to be very useful to study this, even though it doesn't have any probabilistic relevance at face value, one can study these partition functions at complex lambda. And the reason is there's a deep connection between the locations of the zeros of these partition functions and phase transitions. So I'll say a little bit more about that later. Bit more about that later, but just for now to say this is a large area of study to understand the locational zeros of these partition functions. And let me just show you a kind of famous theorem along these lines that constrains the zeros of these partition functions. It's due to De Brucian, which says if you are a graph and you have some collection of real numbers associated to each vertex of the graph, which satisfy this confusing inequality, then if each of the graphics are going to be Inequality, then if each of these lambdas, if your lambda is in this kind of polydisc, then you're non-zero. Okay, the exact content of this theorem isn't really the point, but I just wanted to ask, again, to those who maybe don't know the answer already, does this theorem remind you of anything? Yeah, great, thank you. Who is this, by the way, Seleneka? Sorry? Aya. Aya, thank you, Aya. Sorry? Aya. Aya, thank you, Aya. That's great. Yeah, this looks a lot like the, curiously, like the Lobus local lemma. And remarkably, this is equivalent to the Lobus local lemma. So building on work with Shearer, Scott and Sokol proved that this is precisely the Lobus local lemma. So I think this is a really lovely story that these two results were discovered independently in their respective fields and also two of two incredibly influential Ethereums in each respective field. Of theorems in each respect field. That was only sort of more recently discovered the exact same theorem. And it also provides some extra motivation for why you might care about the multivariate generalization, even if you're just a combinatorialist. So we've come quite far from just lambda equals one. Okay, but for now I'm going to actually focus more on the univariate case. So let me just remind you: this is the hardcore model. And let me just take stock a little bit. And let me just take stock a little bit about where we are and maybe a little look ahead. So you can study this model from three different perspectives. In statistical physics and probability, you might be interested in the question of what does a typical sample from the hardcore model look like and how does this change with lambda. In combinatorics, you're often interested in what does the typical structure of independent settings on graph or hypergraph look like. In particular, you might Look like. In particular, you might wonder how many are there. So, you might be interested in explicit asymptotics for ZG1. And just a little look ahead, something that I haven't mentioned already is there are two natural computational problems that come along with the hardcore model, which is can we approximately compute Zg lambda? This is some sum over potentially exponentially many things, so computing it by brute force takes exponential time potentially, but can we approximate it maybe in polynomial time? And can we approximately And can we approximately produce a sample from the G-blam? And studying the connections between these three, or all of these questions, and these perspectives has been a very useful field of study or direction of study. And this is really a complete digraph. You can kind of ask what one says, how it implies things in the other. But today I'll talk about how this tool, the cluster expansion, can be used to say things about combinatorics and computer science. These two edges of. Sides, these two edges of the title. That will be the remainder of the tour. What's the difference between the first and the third? The first. Sorry, this and this? No, the first item and the first one. Ah, yeah. Um well yeah, basically the same question, but maybe just phrase more in combinatorial language. Just phrase more in combinatorial language. Maybe in combinatorics, you're more interested often in the typical structure of a uniform independent set. So, again, lambda equals one. And here, often in statistical physics, you're interested in the typical structure at an arbitrary lambda and how this changes with lambda. Yeah. Yeah, so a lot of these questions are deeply connected and very much related. Okay, so I should probably tell you now what the cluster expansion is. It's perhaps a little bit anticlimactic, and it's I apologize for hurling a Taylor series at you on Monday morning, but it's actually very beautiful. So let's just spend a moment with it. So it's just a Taylor series of the partition functions that geode lambda. The Taylor series of the logarithm of the partition function. Now the fact that this has a Taylor series shouldn't come as a surprise. ZGu Lambda is a polynomial. That g of lambda is a polynomial, and when lambda is zero, it's one. So it has some Taylor series around lambda. But the interesting thing is the interpretation of what the coefficients are. So let me explain what this is. Here, C of K is the collection of all connected graphs on vertex at K. And the weight of a connected graph is something to do with subgraph counts in my big graph G. So maybe it's a little bit. So maybe it's a little bit more illustrative just to write out some terms by hand. It turns out to be useful to normalize by 1 plus lambda to the n, partition function of the empty graph. And what you see is that each coefficient of lambda, of lambda to the k, is some linear combination of subgraph counts in g, subgraph counts of connected graphs. Now this is an infinite series. Infinite series. I'm summing over all k here. So, of course, the question of convergence comes up. And it's a standard fact from complex analysis that the radius of convergence of this guy is determined by the closest root of Zg to the origins. So here again, we see the complex zeros of G coming into the picture again. And we have this theorem from De Brucian that tells us about or the lowest local lemma that tells us about the zeros of Z G. us about the zeros or zg. And so de Brucian and all the lowest local lemma tells us, or gives us a sufficient condition for convergence. So if lambda is small, in particular if it's less than 1 over e times the max degree of the graph, then I converge, this cluster expansion converges. So this 1 over e times delta is probably very familiar to you if you've worked with a low versus local lens before. It's the exact same factor of V. Perhaps a more kind of enlightening perspective on the cluster expansion is that it's useful for understanding situations where you have a collection of almost independent random variables, which maybe isn't too surprising. This is precisely what the logos local lemma is used for. And maybe very heuristically, I want you to think of this expansion as the following. It's sort of saying that the hardcore model on G is like just the hardcore model on the empty. Just the hardcore model on the empty graph, which remember was just independent percolation. Except I have some effect coming from edges. This is somehow the first order correction to independence. But then, of course, edges don't tell the whole story. Maybe there are some correlations coming from larger and larger subgraphs. And the cluster expansion allows you to precisely quantify how far Zg away is just from independent percolation. So maybe that's the So, maybe that's the more enlightening perspective of cluster expansion is useful whenever you have some perturbation of some idealized measure that you want to understand. Very good. So I want to discuss a sort of recent application of the cluster expansion, but also fairly, I think, hopefully fairly easy one to explain, which is the following problem in random graphs. So, this is a very old problem. So, this is a very old problem, random graphs, which is what is the probability that G and P is triangle-free? This goes back to Adish and Rainian and their very early studies of GMP. So this is very well studied, so let me just tell you some things that are known about this question. If P is little o1 over root n, then we know the asymptotics of the logarithm of this probability. And the dominating contribution from this probability is coming. Contribution from this probability is coming from these disordered triangle-free graphs. But if p is a lot larger, maybe it's going to infinity with 1 over root n, then we get a different answer to the asymptotics of this probability. And this is coming from the contribution from bipartite and graphs. And the proofs of these two facts are very different. So this is kind of the classical example of an application of Jansen's inequality. And this was proved by Rucek first using Spark. This was proved by Buchak first using sparse regularity, and now you can use it, prove it with a hypograph container method. And I should also say that Janssen's inequality tells you a little bit about what happens when we're inside of this sort of critical window. So suppose p is some constant over root n, then Jansen's inequality tells you that this probability tells you the order of it. But the constant in front is not known. So let's ask the question: what is this constant in front? Okay. So let me. Okay. So let me call that constant f of c. So formally, f of c is limit, it's a constant in front of this limiting rate function. So recently, Will and Aditya and Michael and I determined f of c when c is small. So if c is less than 1 over root e, we get this formula for f of c. We get this formula for f of c. So w here is the Lambert W function. It's just the inverse of the function x e to the x. So we have this slightly confusing formula which involves the Lambert w function for f of c and we also determine it when c is large, say bigger than 10 to the 10, and we get some other more confusing formula. So here x is some solution to some transcendental equation. But perhaps it's more enlightening to just plot these two functions and see what they look like. And see what they look like. Okay, so let me do that. So this is the blue curve is the kind of what we showed is valid when C is small, and the orange curve is what we show is valid when C is large. The blue curve again is covering from graphs that we think of as pseudo-random, and the orange curve is coming from graphs which we think of as supposed to bipartite. And of course, the question is: what's happening in between? And the easiest guess is just the following. Easiest guess is just the following, just to extend both and see where they meet. So, is this the picture here? And I think this is pretty cool that just a basic problem in combinatorics already has a very deep question about phase transitions at its heart. And I think answering this question will require some very new ideas. So, I have no idea if this is the correct picture. It's the simplest guess, so I would hazard that it is. But proving that That is a challenge for the future. But we do know there is a phase transition. But we do know that there is a phase transition, exactly. So observe that F of C can't be analytic because it follows the blue at first and then starts to follow the orange. There's no analytic continuation of these two functions. And so there is some non-analyticity going on here, which is one way of saying there is a phase transition intermodal. So phase transition exists, but the So phase transition exists, but the nature of it and where it is is still a mystery. Any questions? But does it mean that this non-alignment? No, it doesn't. We don't know that. So I wouldn't call it self-interface language. So there's at least one phase continuation and at some point it's analytic and and at some point it stops reading so Point install so you right, yeah. Okay, so I'm gonna try and sketch a proof of this for small, at least for small C. Okay, just to see whether the cluster extension comes in and how we can start to probe this question. Okay, the first step is to again statistically simplify the problem a bit and consider the following distribution on triangle-free graphs. Triangle-free graphs. So let's take lambda as some positive activity parameter again. I'm going to select a triangle-free graph with probability proportional to lambda to the g. Okay, and again, we can write down this partition function. It turns out this is nothing more than just GNP conditioned on triangle freeness. That's another way of looking at this probabilities distribution. Another way of looking at this is this is just a hardcore model on some hyperbara. So again, it's just a hardcore model in disguise. The hardcore model in disguise. What does this have to do with the probability that GMP is triangle-free? Well, let's just write down the definition. We can sum over all triangle-free graphs the probability I select in that graph. Ah, sorry, very good. Yeah, I should have said that. The size of G is the number of edges of G. Thank you. So if we write down this definition, we just get some scaling of the partition function. So to understand this probability, it's enough just to understand. Understand this probability, it's enough just to understand this partition function. And if we do this calculation we did already, we can look at the expected number of edges from GMP conditional triangle free from this model. And this is just, again, the logarithmic derivative of the partition function. So it's enough to understand the expected number of edges in this model. If I understand the expected number of edges, by integrating, I can recover the partition function. Integrating, I can recover the partition function, and therefore I can recover this probability. So I've just reframed the problem, as now I want to understand the expected number of edges this model spits out. Okay, so how can we analyze that? Here, I'm just reminding you of the model again. So I want you to imagine fixing some vertex V on my ground set, and I'm going to condition on the graph that you see off of V, and I'm going to call it H. So I'm just fixing H. So I'm just fixing H. And now I'm going to ask what can happen at the edge is incident to V? Okay, so H is fixed, and I'm going to reveal the edge is incident to V in this sample from this model. So notice that if UW is an edge of H, then I can't have both VU and VW, because then I've created a triangle. So what I want to is select edges, what I want to do. want to is select edges want to do is select edges incident to v that correspond precisely to an independent set in age if i select say 10 edges incident to v that comes with a cost of lambda to the 10. so this is just another way of saying that the edges incident to v are precisely the hard core model on the graph h. That clear? Is the other one? Okay, very good. Okay, very good. So we can ask what the expected degree of V is, and again, by our favorite calculation, this is just the logarithmic derivative of the partition function of H, the hardcore model partition function. And now our training kicks in, and we think we should cluster expand, because we see a nice log of a partition function. So let's cluster expand it. I have now a k minus 1 factorial because I took a derivative. And I have these cluster coefficients. But the key point here is that H came from a random graph. So in particular, it doesn't have many cycles. And it turns out that that means that you can discard just any connected graph, subgraph counts that contain cycles. And now I'm just counting tree counts in my graph G, in my graph H, rather. Rather. It's because C is small. That's because C. Well, C is small comes into the fact that this, so I require C to be small so that this converges. Yeah. C is small must mean you don't have a lot of four cycles, right? Yeah, that's right. Yeah, yeah. Absolutely. Yeah, so C is small comes in two ways. One, it's guaranteeing this convergence, and two, it's telling us that we don't have any cycles. Yeah. Is it easy to see if Can I say that yes, you get a big You say that if C gets a big sudden, you're almost bipartite. But without knowing that already, is it easy to see this? So here I'm saying C is small, so we're saying we're very far from bipartite. Okay, but is it easy to know that the C is small, and you're far from bipartite, or is that already a level up? That's already a bit of work. I'll say one more word about that at the end of this slide. Yeah, but that's a great question. Right, but yeah, we're in a pseudo-random regime now, where we're far from bipartisan. Regime now, where we're far from bipartisan. And so I'm just going to restrict this sum now to trees, but then I can count trees quite easily. We expect h to be roughly regular, so I get some factor of k to the k. This is coming from Cayley's formula. This is just the number of trees there are, and this is the number of ways of embedding a particular tree in a regular graph. Okay, and so you immediately recognize this as the Taylor series of the lambda w function. W function. And Bob's your uncle. There's actually a more conceptual way from getting from here to here and skipping this step, which is if I can just only consider trees, that means this must be the hardcore model the same as the hardcore model on just the regular tree. And I know what the hardcore model on the regular tree looks like, and I can write down what the answer should be. So you don't actually need to recognize this data series. Okay, and the final step is: well, this is my, I'm just pretending now sweeping a bunch of stuff on the carpet, everything is concentrated. So this should be, this is the degree of h, but h and g differ by one vertex v, so that should basically be the degree of g, and this is the degree of g, so we kind of get this implicit formula for the degree of the average degree of g. And remember, that's exactly what we were trying to find. The average degree of g is just Trying to find, the average degree of G is just the number of edges of G. So now we know the number of edges, and we can, by the calculation in the previous slide, recover the probability that G is triangle for you. Okay. So I did sweep a bunch of stuff under the carpet. One of them is how do we know it looks bipartite? How do we know these things are concentrated? And the way we did this actually uses ideas from computer science, which are quite nice. So you can. Are quite nice. So you can define the Galaubo dynamics, a sort of single-site update Markov chain for this measure, and use path coupling to show it's rapidly mixing. And that tells you a lot of stuff about the structure of the samples from this measure. In particular, it tells you everything is concentrated and there's no dense cut in this graph. So, using ideas from CS were actually some key tools there too. But yeah, it takes work, so it's not obvious to see it's right. Okay, this is probably the most technical slide of the talk. So if that didn't make sense, then you can re-engage now. But are there any questions on this? Okay. Actually, let me say one more thing about this is that we can also say something about lower tails for triangle counts in GN. For triangle counts in geometry. So, what's a lower tail? Now I'm not asking for my graph to be triangle-free, but rather that it has no, say, theta times the expected number of triangles in GPUs. So this is a classical large deviation probability to investigate. And one thing that we show, which is very different to when, so when eta is zero, we're asking for triangle freedness. When eta is, say, a little bit bigger than a half, so we're being less. Than a half, so we're being less restrictive on the number of triangles, then we can show that f is actually an analytic function of c for all positive c. And then we get some confusing formula for it in terms of the lambda w function that I'm not showing you, but let me just sketch it. It looks like this, say, when eta is one half. So this is very different from the behavior at eta of zero, where we do have a phase transition. Here there's no phase transition, and this curve is valid for all C. And that's because heuristically a different strategy comes into play for suppressing triangles, and that's to, rather than be bipartite, you want to lower the degree of your global graph. And when C is very small, there's a sort of other strategy, which is just to delete edges from each triangle, and this curve is somehow interpolating these two strategies, which is why you get a nice analytic function for that. So very different behavior to the E3 equals zero case. So the E3 equals zero case. Okay, so for the rest of the talk, I'm going to talk about a couple more applications to combinatorics, combinatorial enumeration. So here we saw a problem in random graph theory. Let me talk about a problem in combinatorial enumeration and finish by talking about algorithms. Okay. So remember that if you're a combinatorialist, you might care about this problem where we're trying to count the number of independent sets. Where we're trying to count the number of independent sets in some graph gene. And now we have our cluster expansion, and we might want to come and bash this problem over the head with our new tool. But we immediately fail because this simply doesn't converge at lambda equals 1. For any graph, as soon as g has a single edge, this isn't going to converge. So that seems a little bit sad. But luckily, the story doesn't end there. And to show why the story doesn't end there, Show why the story doesn't end there. Let me describe a classical theorem due to Korshinov and Saposhenko, which says the following: Here they're enumerating the number of independent sets in the hypercube. What is the hypercube? The hypercube is the graph on all vectors of 0, 1 of length d, and I've joined them by an edge of their handing distance as 1. So this is a bipartite graph again with odd and even vertices. And you can ask how many independent. And you can ask how many independent sets are there in this graph. And this was done by Korshinov and Chapachenko in the 80s. This is an early application of the graph container method. This is quite an influential proof. And of course, if you're asymptotically enumerating something, often you get some information about the structure of the thing you're not counting as well. So, what Slaposhenko and Porschinov showed is that a typical independent set looks very Looks very structured, looks very imbalanced. So, this is the two parts of my bipartition, the odds and the evens. And they showed that you have some maybe sparse set of defect vertices on one side, and then a ton of vertices on the other. So it's very imbalanced. So if we were thinking about the hardcore model here at Arbitrary Lambda, we've sort of sailed past the phase transition. Here we're very much in the structural regime. We're very much in the structural regime. And what you can do is you can look at this picture and say, well, even though we're sort of in this very structured regime, I can now use the hardcore model. I don't want to look at, or the cluster expansion rather, I don't want to think of this as a confusing structured thing. I just want to look at these defects and show that these are very easy to understand because these should appear, these are like a sparse hardcore model, which behave almost like independent. Which behave almost like independent calculations. But this is very unstructured, these defects, and so we can understand them in great detail. So you sort of flip the picture on its head, and rather than studying a structured independent set, you study the unstructured defects. So you can model these as a sparse hardcore model, and once you've fixed these defect vertices, it blocks some stuff on the other side, and then you just pick uniformly at random or percolate for the individual. Or percolate for the independent vertices on the other side. And so we've already seen that the cluster expansion is very useful for understanding sparse hardcore models. So if you use this perspective, you can get a very detailed understanding of independent sets in the queue. And maybe the quickest way to show you that we have a very detailed understanding of this is to show you a very complicated formula for the number of independent sets. But let me first say: what's interesting. What's interesting is, or an interesting aspect of this, is the container method developed by Korshinov and Sakashenko is precisely the tool you need to show that the cluster expansion of these defects converges. So these tools behave very nicely and fit nicely together. Okay, so here's this horrendous formula. So Will and I showed a few years ago that you can sort of get essentially arbitrarily accurate understanding of the number of independent sets. Accurate understanding of the number of independent sets. And you can view these as successive terms in some cluster expansion. So there's no reason to stop here except space. This is just to convince you that you can get a very detailed picture of what's going on if you use the cluster expansion to understand the alcohol model there. Okay. And now, so this is one application. Now there's been several others. One application, now there's been several others. I'll just throw the slides at you to kind of stun you. I won't go through them, but just to say that there are now several applications of the cluster of functions to classical problems in combinatorial enumeration. And maybe at the end of the talk, if you want to ask me about any particular one of these, then we can chat. Okay. So let me end. I've got... So let me end. I've got a few minutes, that's great. About the applications of the cluster expansion to algorithms, so the algorithmic perspective of things. Actually, maybe before we change subject, are there any questions? Okay, very good. So here's the hardcore model again. And as I mentioned before, there are two natural There are two natural algorithmic questions that come along with the hardcore model. One is: can we compute this partition function up to, say, one plus epsilon multiplicative vector? And can we sample from the hardcore model? So can we maybe output a random independent set that's close to the genuine hardcore model? Maybe at most at silon and total variation distance. And we want to do these two tasks efficiently. What does efficiently mean? Efficiently, what does efficiently mean? Maybe we want to do it in time polynomial and n, the number of vertices in my graph, and 1 over epsilon. So if we do the first task with these efficiency guarantees, we get what's known as a fully polynomial time approximation scheme. So an FP task or an FP rest if it's a randomized algorithm. And if we do the second task, we get an efficient sampling algorithm. Okay. And this Okay. And there's one very natural way to try and do this, which is to run a Markov chain whose stationary distribution is indeed the hardcore model, if we're looking at the sampling question. So this is the Monte Carlo Markov chain method, which I'm sure will come up a lot more this week. I'll just spend one slide on it. So what might the Markov chain look like? We can describe the Glauber dynamics of the Hark model, which is The Glauber dynamics of the Harper model, which is start with an arbitrary independent set, then pick a vertex V uniformly at random, and then just update the state of V according to what you see at its neighbors. So you're updating a vertex at a time, so you're walking through the space of independent sets, and the stationary distribution of this is indeed the Harkour model. And the point is, if this Markov chain mixes rapidly, then we get an efficient sampling algorithm. Algorithm and you can turn these into randomized counting algorithms quite easily as well. So sampling and counting for me will basically be just two sides of the same coin. So this is sometimes, well, this is an example of the metropolis algorithm, which was actually invented, this methodology was invented to sample from the hard sphere model that we saw in the very first slide. And this was voted one of the top 10 most influential algorithms of the 20th century. Some of you can see. 20th century, and so you can see that right at the top from 46. By all Americans in general at the presidential election, or by compulsory voting that year. Yeah. Yeah, so it's extremely, this was sort of the genesis of the MCMT methods, of course, extremely influential, except that. Except there are many natural instances where you might expect this Markov chain not to mix rapidly at all. So let me show you a picture where this might occur. So going back to this picture that we had on an early slide, suppose we were sampling independent sets on the integer lattice again, a very large lambda, so we're highly structured. So we might get an independent set that looks like this, or we might get an independent set that looks like this. Or you might get an independent set that looks like this. And a Markov chain is going to take a lot of time to travel from, say, this configuration to this configuration. So the phase transition here is somehow a natural barrier to this algorithm. And so this is just a glimpse at a more deep connection between phase transitions and computational complexity. And the cluster expansion has been useful for sampling in these structured or low temperature regimes. Structured or low-temperature regimes. So I'll just say a few words about that. Let me first just explain a little bit more abstractly why the cluster expansion is sort of useful algorithmically. Okay. So here's the cluster expansion again, just to remind you, some Taylor series. And I'm going to suppose that Lambda is such that this Taylor series converges. And the natural way to try and approximate Way to try and approximate this partition function is just to truncate this Taylor series at some point. And it turns out that to approximate zg up to a 1 plus epsilon error, all you need to do is to compute the first k equals log n over epsilon terms of this cluster expansion. But remember, terms of the cluster expansion just look like connected subgraph counts. And here it's really crucial that these graphs are connected. Because in a graph, it's very easy to, or one can show that. Or one can show that you can enumerate connected subgraph counts just by starting at a vertex and exploring vertex by vertex. And so to count connected subgraphs of size log n over epsilon, we expect this to take time roughly delta, with delta choices at each step of our exploration process, which is polynomial in n over epsilon time, if we want to enumerate these connected subgraphs. So this is a very happy story that this can be done. So, this is a very happy story that this can be done in polynomial time, and it really depended crucially on this combinatorial form of these coefficients. So, this is closely connected to a very powerful algorithmic method known as Barbanox interpolation method. And some, yeah, gives you hopefully a glimpse as to why this is relevant algorithmically. Any questions on this? Okay, so let me tell you a theorem that rather aptly is in a beautiful paper of two of the organizers and also Hughes in the audience, which was the first paper to give this combinatorial perspective on the cluster expansion. And it was also the inspiration for the combinatorial applications of the cluster expansion to combinatorics. So without this theorem, this talk would have been very short indeed. Being very short indeed. So, what does it say? I've said it just, forgive me for being very imprecise, but essentially says that there is a FP task for sampling from the hardcore model on Z D when lambda is sufficiently large. Particularly, we can sample from this picture. And let me just give you the kind of increcise reason for why this is. It's similar to the intuition for why we can count the independent sets in the hypercube. It's to switch. Hypercube is to switch the perspective and say rather than try and sample from this very confusing structured model, let me instead try and sample these defects first, these islands of red vertices first, maybe, and then once we fix those, it's easier to see what the blue vertices are doing. A little bit later, we showed, with an I and Peter showed that this is true for a bipartite expander. This is actually an easier theorem to prove. Somehow, An easier theorem to prove somehow expansion glosses over the more intricate geometric considerations you need for the lattice. So these are two examples where we can sample new structured low temperature regime. And sort of begs the question of what about general bipartite graphs? So let me end. I began with an open problem. Let me end with an open problem. We don't know. So take a We don't know. So, take a jet arbitrary bipartite graph. Can you approximately sample the independent steps in this graph? We don't know whether this admits a fully polynomial time approximation scheme, or maybe it's just as hard as approximating the number of solutions to a K-sat formula, or maybe its complexity lies somewhere in between. So this is known as, this is a well-known open problem. This is the complexity class sharp bis for bipartite independent scent. And I'll end just on a few. And I'll end just on a few more applications of the cluster expansion to algorithms. Again, to stun you, but also as an invitation to learn about the cluster expansion and ask you questions and potentially add to this list. Okay, so with that, I'll end. Thank you. Okay, thanks, Matthew. Are there questions at the end? Thanks, Matthew. Are there questions at the end? Can you also calculate the probability that G and M is triangle free? Yeah, yeah. Yes. This is the short answer. It's not so hard because we only want, let's say we want the logarithmic probability up to one positive one. And so then you just need to show that the probability. And so then you just need to show that the probability that GMP has exactly M edges isn't ridiculously small. And then that's not too hard to do. So it's easy to transfer. Other questions? Do you know that the cluster expansion can't make those two curves meet in a triangle free picture? Have you proven failure to converge at any point or anything like that? At either end of the story? That's a good question. So for C small, for example, the natural barrier is this one over E delta, and that's what this one over root E comes from. And that's the best radius of convergence for arbitrary graphs. But maybe, I don't know if one can use more knowledge about the space of graphs that you're considering. I would suspect that the cluster attaching can't do this, but then I'm not sure I have a rigorous proof of that. Rigorous people like that. I think they might be able to show that tree-like things. Yeah. Yeah, that's right. Yeah, so understanding the space of grass doesn't seem to help. Random grass are very tree-like. So for this triangle tree result, if you fix the degree to be like a cluster-like kind of degree sequence, would the argument there for the cluster extension be? There, for the cluster expansion, be easier or opposite? So, here you're saying degrees of Poisson, so like P is like constant over N. So, you're saying P is like constant over N, maybe? Right. So, here, yeah, the arguments would, I suppose, be easier, but in that regime, much more accurate results are already known. Then you can say you can actually get the precise asymptotic of the probability itself. Precise asymptotic of the probability itself, not just the random graph. And then you cannot infer from given degree sequence impact that you include. So you're saying a random graph with a given degree sequence? Right, conditional degree sequence. I think there are normal thing where she results. But I I'm not sure exactly like if they have a gap process or thing you actually you know Then you actually know integral on the degree sequence is then back to the gene. So here again you think of maybe fixed degree? Right, just the conditioning on the degree sequence. Yes. I haven't thought about that. And then go back to the gene. Right. Yeah, quite possibly that works. I'm sure this maybe Mortek knows better whether say you wanted the probability that the random regular graph or the six degree sequence is triangle three slightly closer. It's been slightly claims now. Square root n is a very critical value, like m around square root. Sorry, sorry, m around. Aha, you're saying square density is also where these methods break down. There's some way around maybe you cannot find the density of them. Very sensitive over there, so that's the same thing. Oh, yeah, that's an interesting thing. I'm wondering that if this class starts then some kind of IPUs or IBMs yeah so I think it could potentially be useful for that type of question so long as your degree is such that the cluster expansion converges but yeah I haven't thought about models with a fixed degree sentence it's an interesting question yeah So there you need some more complex like so there so I oversimplified things and said you sort of sample these red vertices. What you really should be doing is sampling contours which are separating sort of blue and red phases. And the definition of a contour, so this is like a Pearl's argument. Maybe you've seen a kind of similar thing in calculation. Calculation. So now you're sampling contours, and the definition of a contour uses some of the basic geometry of Z D. Exactly what you need from Z D is an interesting question, which I think Will and Tyler have asked. Theorem might tell you more about that later. Ah, good, good. So theorem is generalized for all data graphs. Right, so yeah. She'll tell you what exactly you mean. You'll be surprised. Any other questions? I have one quick question. So, you drew the curve where they meet. Yeah, from the picture, it looks like you're saying the prediction thing is that it's first order. Yeah. That's right. Yeah. So expect a first-order transition, but maybe at the transition point, you still expect to see the bipartite graph, just because there are two to the n ground states of bipartite graphs and just. Ground states for bipartite graphs, and just one round state for. So it's kind of like the Potts model with Q, but Q is exponentially large. How does it compare with the P seading works method? The desktop expansion method. I mean, in terms of the region where you can apply the use of self-aware reading walks from bytes. Sorry, trees. Trees. Whites is not fixed. Ah, whites is my favorite. Ah, right, right, right. Method. Ah, right, right, right. Yeah, so that's a very good question. So, this one over E delta is a bit frustrating because the reason for this is this is the conversion of the cluster expansion of the tree, but that's the closest root on the negative real axis. Whereas Weiss's method allows you to go a little bit further along the positive real line, because there you're interested in just probabilistic information. Then you can say things all the way up to E over delta rhythm, the uniqueness threshold for. The uniqueness structure of alcohol model material. Do you know if it's possible maybe to close the gap between these two methods? I mean, is there any hope for saying that the cluster method definitely rules? So I see somehow there is a fundamental barrier of this. Well, okay, so in some sense the answer is yes. The answer is yes. So, this is sort of what you can do clever tricks. So, the cluster expansion gives you like a zero-free disk, but you can create much more interesting zero-free regions that aren't disk-like. And so, you can sort of map a zero-free region around the positive real axis. This is a result of Hoost that says actually you have a zero-free sausage that points all the way out to the white threshold. And then you can map that into a disk. So, in some sense, that's a way of strengthening this cluster expansion method. This cluster expansion method using some checker techniques. So, yeah, this is what I presented is you're restricted to these zero-free disks, and sometimes it's much more interesting to look for zero-free regions with more exciting shapes. I think we should think better again. Let me have a coffee break for half a while.