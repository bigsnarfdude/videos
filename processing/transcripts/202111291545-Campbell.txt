Organizers for the invitation to speak. Yeah, so today I'll be talking about some very recent work on how to sample from complex, multimodal, weakly identifiable Bayes posteriors using parallel tempering. And the key, I guess, update from this work is that we are optimizing the path along which you're doing parallel tempering. So this project. Parallel temporary. So, this project was led by two fantastic PhD students at UBC, Saf Sayed and Vittorio Romagnello. And it was done in collaboration with my awesome colleague, Alex Bouchard-Cotet. So I suppose I probably don't need to review this, but just to establish a little bit of notation, in the setting of Bayesian inference, typically I have some data, let's call that x, and I want to infer some unknown parameter of interest, and let's call that theta. interest and let's call that theta and I treat both of those as random variables as Bayesian's because we're treating both the unknowns and the data as random we need to give things distributions and so the the distribution that we give theta before observing any of our data x we call that the prior and i'm drawing a little 2d cartoon here with some contours once i observe some data Some data, I express my uncertainty in theta using the conditional distribution of theta giving x. And we call that the posterior distribution. And I've drawn some updated contours there. Typically, the variance of that distribution on theta will reduce the more data you get. And the goal, sort of the holy grail of Bayesian statistics, is to be able to take random draws from that posterior distribution. From that posterior distribution. If you can take random draws, then you can do pretty much anything you want. You can estimate functionals of interest like means and variances and any other statistic that you care about. Unfortunately, actually doing that sampling is non-trivial. People have come up with all kinds of clever techniques to get around this problem, but one of the most common ones, and Julian has done an excellent job of introducing. And Julian has done an excellent job of introducing this: is Markov Gen Monte Carlo, MCMC. The gist of MCMC: I have my target distribution here in blue, the contours, and I'm just using some cute notation here, the density of theta given x, that's the posterior. What people do with MCNC is you design a very clever Markov chain that effectively, if you close your eyes for long enough and you run that Markov chain for a really Enough and you run that Markov chain for a really long time, the distribution of where the state of that Markov chain is converges to the posterior distribution that you actually care about. So, of course, if you run that Markov chain for a long time and then you open your eyes and you record what the state is, you can treat that like a sample from your posterior distribution. Now, in practice, of course, you have to initialize that Markov chain somewhere. And so people throw away the first some amount of the states to get rid of their. States to get rid of their initialization bias. And then with the remaining states, you throw away the order from the Markov chain because that doesn't actually matter. And then what you're left with is a set of states that look like they could be an IID sample from the posterior, if you're lucky. Unfortunately, not all posterior distributions are so kind to you. And typically, in the presence of multimodality and weak identifiability and high And weak identifiability and high dimensions and multiple scales, MCMC methods can run into some trouble. Just as a couple of little cartoon examples, if I were to run most MCMC algorithms on an example like so, where I have two modes in my posterior, two explanations for my data, I actually might, I'm very likely to get unlucky, right? If I initialize my Markov chain somewhere, I might. Markov chain somewhere, I might converge to one of the modes. If I initialize it slightly differently, I might converge to another mode. And this problem of getting stuck in modes is only exacerbated in high dimensions. Arguably, I guess, more common in posterior distributions is to see varying scale and also weak identifiability. Essentially, you've designed your posterior, your Markov chain to bounce around sort of Bounce around sort of correctly in some portion of your state space, but it is not able to actually explore the full mass of the posterior, for example, in this little funnel here. It's sort of hit a choke point. The step sizes are too big. And so in this talk, we're going to look at algorithms based on parallel tempering that resolve this problem by not just trying to sample from the usually very difficult posterior distribution, but actually start from something very easy. Actually, start from something very easy. We call this the reference distribution. You can think of this as being, for instance, an isotropic Gaussian. And you use an interpolation, sort of a sequence of distributions along a path that progressively get more and more complicated, like your actual posterior distribution. And then you actually run MCMC in all of these. So every distribution along this path of four distributions, you're actually just running MCMC. Distributions, you're actually just running MCMC, but you have one extra clever step where you take states from each chain and you try to swap it into the next distribution along the path. The intuition here is that if you look at these two distributions in the middle specifically, this distribution is in some sense kind of easy to explore. There's no sort of cutoff regions of mass. This distribution, you can see there's sort of a choke point here. So it's kind of difficult to explore both this area and To explore both this area and this area with just a standard MCMC algorithm. But if you can swap states between these, you can see that you can actually use the fact that it's easy to explore in this distribution to help you move states into the more difficult isolated regions. Okay, so that is the gist of PT. In this talk, we are specifically going to focus on the question, what path should I be using? And in particular, we're going to figure out how to optimize paths. How to optimize paths. I'm going to introduce a flexible family of non-linear paths. I'll introduce an optimization algorithm that you can use to pick the path, provide some quick theory on path efficiency, and we'll show at least empirically that our new method breaks the previous sort of theoretical upper bound on performance of traditional parallel tempering algorithms. So without further ado, let's set up some notation for parallel tempering. Notation for parallel tempering. In our setup here, we have a reference density, pi naught. You can think of that in your head like the prior in a Bayesian model. And we have our target density, pi one. This is the thing that we want to be able to sample from. You can think of this as the posterior distribution. In standard parallel tempering, you construct a path of distributions from your reference, pi naught, to your target, pi1, typically by parameterizing. Typically, by parametrizing the path with a variable t that's between zero and one, you raise your reference density to the power one minus t, you raise your target to the power t, and you multiply these two. Under very weak conditions, this creates an unnormalized density, which is all you need for most MCMC methods. And you can see in our cute little Gaussian example here, where I have a Gaussian reference centered at minus two and a Gaussian target centered at two, that this path creates a set of Gaussian. That this path creates a set of Gaussians that just sort of slide from left to right. Now, in parallel tempering, as the name suggests, we're going to invoke some parallelism. So we have, let's say, n computational threads. And so what I'm going to do is discretize my path to n steps, basically, between my reference and my target. And we're going to sample between all of these, sample from all of these distributions simultaneously. So let's just zoom in on two of these distributions. Zoom in on two of these distributions along the path. So let's say these two here. I've got distribution n and distribution n plus one. Each one has its own state because we're running MCMC in each one. So we've got a blue state in distribution n and a red state in distribution n plus one. The colors don't mean anything. I'm just using the colors to help you keep track of where things are going visually. Parallel tempering has two major steps. The first is sort of very straightforward. It's the exploration step. Straightforward. It's the exploration step. Basically, within each one of these distributions along the path, I'm going to be running any MCMC update that targets that distribution correctly. So here you can see maybe I'm running something like Hamiltonian Monte Carlo, and I'm sort of zooming around these contours here, and I've ended up at this point in my next state for the blue, the distribution n chain. And for distribution n plus one, I've sort of gotten unlucky. I've reflected off this choke point and I'm back in the. Reflected off this choke point, and I'm back in the main region of mass here. Okay, the second, and this is the second step, and this is kind of the secret sauce of parallel tempering, is what we call either a swap or a communication move. And essentially, what this is, is a metropolis-Hastings move on the joint set of all the Markov chains running, such that what I do is I move a state from distribution n to the same position in distribution n plus one. So the actual state itself, the value. So, the actual state itself, the value is not changing. I'm just shifting it to distribution n plus one. And then I take the state from n plus one and I shift it back to distribution n. And if you accept this kind of swap with just the right probability, in other words, the acceptance ratio of an MH move that targets the invariant distribution of all of these chains being just independent sample, sampling independently from each chain. If I design my swap probability, My swap probability, like so, then you can show that this sort of targets the correct big joint distribution on all the chains. And if we accept a swap, you can see, as I mentioned before, that being able to explore in one of the earlier distributions can potentially help you explore in a later distribution. So that's two. How does this work with n distributions along the path? Well, I kind of want to get rid of. Well, I kind of want to get rid of the sort of intricacies of how I'm actually sort of doing MCMC in each distribution. What I'm really concerned with is how do I assess a PT method compared to, let's say, other PT methods. So I'm getting rid of the drawing of the state, and I'm just showing you where my samples are along the chain. So again, the colors don't really mean anything. They're just there to help you keep track of things visually. I've got five chains between my reference and target. And what I'm going to do with n chain. And what I'm going to do with end chains is I'm going to propose swaps between distinct pairs. So, for instance, my first set of swaps that I'll try would be between the black and the blue and the orange and the purple states. And let's say I just accept both of those swaps. So I'll get two swapped orange and purple and black and blue here. Then moving forward, what we do is we actually propose swaps between TikToking pairs of chains. So my next proposal is going to be between the Next proposal is going to be between the other two, so like the even swaps. And then I'll do the odd swaps again, and then I do the even swaps again, etc. Intuitively, the reason why you want to do this sort of TikToking pattern in the swap proposals is because what you get is very nice consistent lines of motion for a sample that starts in the reference. It kind of just moves its way, assuming everything is being accepted, down to the target, and vice versa. Things that start at the target are. And vice versa. Things that start at the target are sort of successfully moving their way up consistently back to the reference. You don't get a random walk or a diffusive behavior. You get these sort of consistent lines of motion. Okay, so intuitively, what you want is for a state that starts in the reference to make it quickly down to the target, because you're sort of sampling independently in the reference. So you get some fresh information down in the target, and then you want it to sort of come back. If we want to evaluate the sample, Uh, we, if we want to evaluate a parallel tempering method, we're going to use that intuition. So, in particular, the typical metrics like effective sample size, etc., they are a bit too much influenced by exploration, by like whether you're using random walk metropolis hastings or Gibbs sampling or HMC or, you know, bouncy particle sampler or whatever. These metrics are sort of conflate, they conflate the exploration and the communication. And what we want is if we're just comparing between two PT. If we're just comparing between two PT methods, we want something that is a bit more isolated. And so this little figure from the previous slide maybe should give you a hint as to what we're going to do. We are going to look at how quickly a state makes it from the reference and gets swapped all the way down to the target and then comes all the way back. When that happens successfully, we'll call that a round trip. And what we want in a parallel tempering method is for there to be lots and lots of these round trips. This means that sort of, if you think of the joint Markov chain. Of, if you think of the joint Markov chain on all the chains, it's mixing quickly. Okay, and then the metric that we're actually going to use is just a frequency version of this. So just how often are around trips happening? Now, SAF and Alex and collaborators in some past work noticed something interesting about past PT methods: that as you increase the number of chains or the amount of parallel computation that you have, the performance of PT actually gets worse. The performance of PT actually gets worse. This is because past methods used randomly chosen swaps. Remember how I said that this sort of TikToking scheme is crucial? These past papers were all just picking the swaps to make at random in each iteration. And as it turns out, if you do that, your round trip rate actually gets worse as you add more computation, which is not good, right? You're doing more work, you're working harder, and you're getting a worse result. On the other hand, if you are careful, On the other hand, if you are careful and you do this sort of even-odd TikToking pattern, like I mentioned, your round trip rate actually converges to a constant. So that's obviously a big improvement over randomly chosen swaps. You might still complain. You might say, well, hold on, I'm still adding more computation. Why isn't my round trip rate going up? If you're clever about how you split your decision on what threads to use for multiple copies of PT, then you can actually bump this up to big theta n. To big theta n. But let's just analyze one copy of PT for this talk. So, what I'm looking for is getting this big theta of one, so a constant round trip rate. Okay, cool. So, that's parallel tempering. Works great. Everything's done, right? Well, let's take a look at a very, very simple example. So, here we've got a Gaussian reference distribution. It's got a mean of minus one, standard deviation of 0.01. I've got a Gaussian target. I've got a Gaussian target, mean is one same standard deviation. I'm going to use 50 chains between the reference and the target. So 50 little Gaussian sort of sliding from left to right here. And I'm going to give this algorithm the best possible chance. I'm giving it IID exploration. So of course, I can just sample IID from Gaussians. I know all of my distributions along the path are Gaussians. I'll just sample IID from all of them. And as I mentioned before, I'll be using this path where I take. Before, I'll be using this path where I take the reference to the power one minus t, my target to the power t. We'll call that the linear path. Here's how this operates. On the y-axis, I'm showing you the number of round trips. Remember, higher is better on round trips. On the x-axis, I'm showing you the number of iterations that we run, call these scans. In green and red, I'm showing you the parallel tempering method with the TikToking swaps, and I'm showing you the parallel tempering method. And I'm showing you the parallel tempering method with just randomly chosen swaps. The black dashed line is the theoretical best possible number of round trips that you're getting from either of these methods. And you might notice something about this plot. There are no round trips that happened after 50,000 steps. That's a lot of computation for what is effectively a failed parallel temperature. Not to mention the upper bound here is quite low as well. Bound here is quite low as well. So, what's going on? Let's dig into the math just a little bit here. So, recall this swap probability. At each step where I'm trying to propose a swap, I need to evaluate this. That's alpha n. Let's say that the expected value of 1 minus alpha n, if I'm sampling my two states independently from the two chains that I'm trying to swap, let's call that Rn. Okay, so if I, if I, Okay, so if I think that these two distributions are very close to each other, let's say these two distributions, hopefully you can see my mouse, then this ratio is going to be quite close to one. And so it's very likely that the swap is going to be accepted. And so the rejection rate will be close to zero. On the other hand, if I have two distributions that are quite far apart from each other, it's going to be very difficult for me to propose an MH move that accepts when I try to swap the two states between the I try to swap the two states between the different chains. So the rejection rate is going to be quite high. Saf and Alex and collaborators showed in their past work that actually these rejection rates for the pair n and n plus one are related to the total round trip rate for the whole method. In fact, if you take 2 plus 2 times the sum over all of the chains of the rejection rate divided by 1 minus the rejection rate for pair n. Rejection rate for pair N, and you take the inverse of that, that that is the round trip rate that you get under some mild assumptions. Okay, so you can see that if any of the pairs of chains along the path have a high rejection rate, this is going to work pretty poorly, right? Because there's sort of a bottleneck, there's a limiting factor. They also showed this big theta one result that I mentioned before, that as you take n to infinity, so the number of parallel chains that are working. The number of parallel chains that are working, if you take that into infinity, that the round trip rate converges to a constant. And SAF and Alex have labeled this the global communication barrier lambda. So if you go back to our Gaussian example, our Q-Gaussian example, what's actually happening here is that the reference distribution and the target distribution are almost, they're nearly mutually singular. There's almost no overlap between them. And if you chug through the math, you can show. Chug through the math, you can show that what this means is that the global communication barrier is quite large. And remember, the round trip rate, at least asymptotically, is basically inversely proportional to the global communication barrier. Intuitively, what this means is that, sure, I can add many, many distributions along my path here, but the more distributions I add, the more swaps I have to successfully make to make it all the way down to the target and then all the way back. And then all the way back. And unfortunately, the reference and the target are far enough apart that that trade-off does not go in your favor. You sort of, you know, no matter how many of these chains you add, you still end up having to do more work than you should to get a state to go from the reference to the target and back. And it just doesn't happen even once in 50,000 steps. This is specifically very relevant for Bayes because most of the time, Because most of the time, your prior, which you would use as a reference, and your posterior that you'd use as a target, they are almost always nearly mutually singular. It's very unusual to have a prior that looks a lot like your posterior. So then the question is, are we done? I mean, is PT just sunk by this incredibly simple Gaussian problem? And of course, the answer is no. We can actually do quite a bit better. Actually, do quite a bit better. So let's take a look at this example again, except I'm going to add some more sort of abstract parameters. Okay. So instead of minus one and one, I'm going to have mu naught and mu one. And instead of a standard deviation of 0.01, I'll call that sigma. The key parameter for this problem is z. It's the absolute difference of the means divided by the standard deviation. Essentially, z captures how separated these two Gaussians are from each other. So a large value. Each other. So a large value of Z means they're more separated. In the paper, we show that the linear path, so the one that everyone uses for PT and the one that I just used in that example, has that global communication barrier that scales linearly in Z. And not only that, but there exists a path of Gaussian distributions where the global communication barrier actually only scales logarithmically in Z. So indeed, we can do at least exponentially better than the Exponentially better than the standard linear path that everyone uses. I suspect that that's even a bit loose. I suspect that this is probably more like, if I extend to not just Gaussians, but anything, it's probably more like just a constant. Okay, so in this work, that's exactly what we do. We introduce a path of Gaussian's in this particular example, at least, that achieves that much. That much reduced global communication barrier. And the essence of it is that you squish the distributions down as you go along the path so that they have much more overlap, so that you reduce the rejection rates and you lower the global communication barrier. Okay, so the linear path is a problem, so we need to go beyond the linear path. Let's play with some ideas for what path we might introduce. Play with some ideas for what path we might introduce instead. Here's an idea. I have a mixture, right? I'll take my reference times one minus t and I'll take my target times t and I'll add them. This creates a path of distribution that looks like so. I start from my reference and I anneal down, or rather, I reduce the mass and I at the same time increase the mass on the target. This is a path, but it's probably not a good one for parallel tempering, right? Because critically, there's Right, because critically, there's this one step at the beginning where all of a sudden some mass shows up where it didn't exist before. It's going to be very difficult to swap states into that potentially isolated region of mass, right? So there's going to be a high global communication barrier and a low round trip rate. Here's another idea. We'll start from our reference and we'll start by annealing down before. Annealing down before we shift over, and then we'll sort of unanneal back up to the target distribution. This, I think, is probably a better idea because at each sort of sequential step, we maintain a decent amount of overlap between the distributions, the pairs of distributions. So, how do we codify this intuition a bit more precisely? In particular, what families of paths should we consider? Of paths, should we consider in practice? Well, number one, our family shouldn't be specific to a particular reference and target distribution. We want this to be black box, right? I would like to code this into a ProProg software system so that users don't really have to know much about what's going on behind the scenes. What that boils down to is, I really only want to rely on point-wise evaluation of my reference and my target. The linear path is great for this, right? path is great for this right because you just take pi naught to the power one minus t pi one to the power t multiply them almost always that will make a normalizable um density and so it's it's sort of you don't need to know much to use a linear path number two is that we should our family should include the linear path uh because i don't want to do worse than the linear path right and in particular probably i will initialize at the linear path Probably I will initialize at the linear path. And so if I start optimizing my path at the linear path, I'd like to be able to do that basically. And finally, using our intuition from before, whatever path family we work with, it should enable us to anneal the distribution. I mean, squish, right? Like, I need to be able to flatten the distribution out so that as I shift it around in whatever space I'm working in, I maintain overlap between the sequential pairs. So in this So, in this work, we introduce this family of paths called the exponential path family. The gist of it is that you take the reference pi naught and you raise it to a power, which is some twice continuously differentiable function eta naught. And you do the same thing to the target, and you multiply them. Okay, there's some mild technical conditions that I'm sweeping under the rug. You can see the paper if you're interested in them. They're not very enlightening, it's just various like differentiability conditions. Differentiability conditions. But if you squint at this for long enough, you can see that much like the linear path, this exponential path is black box. I just need pointwise evaluation of my target and my reference. Of course, I can set eta naught and eta one to one minus t and t to recover the linear path. And of course, if the sum of eta naught and eta one is less than one, and they're both not negative, this is of course. Not negative. This is, of course, annealing. It's sort of squishing the distribution down. And there's a big win here, right? I've changed my problem of designing a path of densities to designing a path in just R2, right? This eta naught and eta1 function. That is, of course, still a difficult problem. It's significantly easier than designing a path with densities, but I still have a little bit to go here. So in this work, we So, in this work, there's actually many choices that you could make. The choice that we make is to use a linear spline with some number of knots. The intuition for why that's even a reasonable choice is that parallel tempering can't really distinguish between a curvy, flexible path and a linear spline anyway. Because what you're first going to do when you run parallel tempering is you're going to discretize the path, right? So that you have n chains, you know, one, two, three, four, five, six, seven, eight in this example. One, two, three, four, five, six, seven, eight in this example. And so, parallel tempering, according to that algorithm, the blue path and the gray path here are identical. It can't tell the difference between them. And of course, the blue path is some kind of linear spline. So, okay, so linear splines are not, at least they're not like a limiting class of paths, let's say. But okay, so here's an example of how you might want to design a linear spline. On the y-axis here, I've got my function. On the y-axis here, I've got my function eta1. On my x-axis, I've got the function eta naught. And I'm drawing a couple of paths here. You can see that the linear path, this is where it gets its name, if you plot it in exponential coordinates. Here on the bottom right, I have my reference, coordinate one, zero. I have my target, coordinate zero, one. And the linear path just interpolates between them. The spline path gives you a bit more flexibility. So this is this sort of So, this is this sort of kinked path here. In particular, this optimized path that I've shown you, this little graphic that I've shown you a couple of times, is actually corresponding to this sort of kinked path in these exponential coordinates. So in particular, that might be my reference distribution. The flattened distribution in the middle is where that kink is. You can see that the sum of these two coordinates is much less than one. So I'm annealing. And my target is up at the. Target is up at the 0-1 coordinate there. So, actually, the linear spline, even with just one kink, gives you quite a bit of flexibility. And you'll see in our experiments later on that actually just adding one knot in the middle is usually good enough to get most of what you're going to get. So, there's one thing left. This is our path family that we want to work with. It's parametrized by these knots. And the question that's left is, well, That's left is well, how do I place these knots? Where do I put them? And the obvious thing to do, and the thing that we first tried to do and thought about was just maximizing the round trip rate, right? I spent a bit of time at the beginning introducing this notion of performance of parallel tempering, which talks about how quickly states are sort of being swapped down from the reference to the target and all the way back. So let's just maximize that. Saf and Alex and collaborators worked out the round trip rate for the linear path. And if you recall this formula that I put up earlier, it's two plus two times the sum of all the pairwise rejection rates divided by one minus the pairwise rejection rates, all of that inverse. As it turns out, for the linear path, the way that you derive that formula is just done by discretizing the linear path. So actually, it doesn't really matter. Actually, it doesn't really matter if you have a nonlinear path because the analysis works out to be exactly the same. You can just analyze these little linear segments instead of these linear segments, right? The fact that they actually all line up doesn't really matter. So indeed, the round trip rate formula is actually identical to in the linear case, which is kind of handy. Okay, cool. So let's just optimize that, right? We'll get some stochastic gradient estimates of this function. Of this function with respect to the not positions, and then we'll just use a typical SGD algorithm. Unfortunately, that does not work as well as one might hope. In particular, the round trip rate is a bad metric to use for optimization. If you squint at this formula a bit, you will see something bad here. I'm dividing by one minus r. When I start this algorithm, When I start this algorithm, I'm usually starting from a point where all of the rejection rates are one. So if I perturb my path a little bit, all of my rejection rates are going to still be about one, and I'm really not getting much gradient signal. At the same time, if I get unlucky and I'm exactly one, this blows up. So it's perhaps not even, like the estimated version of it is perhaps not even well defined. So, to study this, we did just a cute little empirical study: two Gaussians, one centered at zero, one centered at a mean of mu. I believe our standard deviation here was one. And if you look at the signal-to-noise ratio on the gradient, this is just a one-dimensional problem. It gets worse in higher dimensions. Basically, as you move mu even reasonably away from zero, your signal-to-noise ratio and your gradient just totally drops off. In other words, you're kind of sunk if you're trying to optimize the round trip rate directly. What else can we do? Okay, well, there's this other sort of version of the round trip rate, which is the asymptotic round trip rate. What happens as I increase the number of parallel threads that I'm working with? So I discretize my path finer and finer and finer. In this work, we In this work, we actually show that you get almost an identical result to the linear path. You get that the round trip rate, as you increase n, it converges to a constant. And we define lambda here to be the generalized global communication barrier for the nonlinear path. Let's break down this formula for a second here. Pi t, that's my density at position t along my path from t from zero to one. zero to one and that's proportional to exponential of wt so wt is like your log density of the at the position t along the path let's say i take xt and xt prime iid from that distribution pi t and i'll set my my global communication barrier lambda as the path integral from zero to one of the expected absolute difference in the pathwise derivatives of w. The pathwise derivatives of Wt using these two independent samples from πt. So there's some very nice features about this asymptotic formula that are in some sense almost an accident. They're almost just fortunate. In particular, the most important thing is that unlike the round trip rate, you don't lose gradient signal in the high rejection regime, right? So you start your path somewhere really, really bad. Somewhere really, really bad. All your rejection rates are something like one. This formula, if you squint at it, you'll see that the w's here are going to be log densities and roughly the same with their t derivatives. And so what you get is something that actually properly measures how much better you're doing if you make small perturbations to the path. Log densities tend to behave a little bit better for optimization in that way. Also, if you squint at this formula, this looks like a path length, right? Like a path length, right? If you maybe hit this with a Jensen's inequality, you get like a square and a square root showing up. You can kind of harken this to some notion, some weird notion of a path length. But the real win here is that unlike the round trip rate, you don't sort of saturate when you get a really bad path. This thing keeps growing. And it's a weird artifact of this asymptotic formula that this makes sense if you discretize it and use a finite n for optimization. Okay, so indeed, that's what we. Okay, so indeed that's what we do, right? Of course, when we're doing parallel tempering, we can't actually do this path integral perfectly. We only have access to this finite collection of chains that we're running. And so we use the sort of linear spline global communication barrier as our proxy. And you'll notice that this is actually totally kosher as long as there's not too much curvature in these paths, right? So as long as the linear spline is a good approximation. line is a good approximation. And finally, just to handle maybe some concerns about differentiability and smoothness, although I don't necessarily, I don't know if this is necessary, but you can get an upper bound on the square of this global round, sorry, global communication barrier with the sum of the pairwise symmetric KL divergences along your path. So this thing seems to be a little bit more well-behaved, we think, for optimization, especially. For optimization, especially when your path starts out as being really, really bad. And so we're going to try to minimize maybe the SKL instead of the, let's say, maximizing the round trip rate. Back to our little empirical example. So remember, we've got our Gaussian reference at zero and a target at mu. And I looked at the gradient signal to noise ratio on the round trip rate. Let's see what happens with the SKL. This is exactly what you want. This is exactly what you want, right? As your distribution gets worse and worse and worse, which is typically a good representative of where you're going to start when you do this optimization, your gradient signal to noise ratio actually goes up. So you're getting stronger and stronger signal to push you to a good path when you start at a bad path with the SKL objective. Now, you might notice that when you get mu close enough to zero, you might argue that you might want to switch your objective over to the round trip rate once you start. To the round trip rate, once you start getting successful around trips. But we didn't really explore that in this work. It's sort of, you know, left it as a little follow-up if someone wanted to do it. And as a very minor point, remember how I said you start with this path T from zero to one, and you have to sort of discretize it? Typically, you don't do that uniformly. You actually allocate different values of t along that path to different regions. That path to different regions where the distributions are changing faster. But there's an established algorithm that SAF and Alex and collaborators came up with a couple of years. So we just use that. Okay, let's see how this works. So I've got my setup from before, normal centered at minus one. For the reference, the target is centered at one. Remember this figure, round trips on the y-axis, higher is better. Iterations on the x-axis, more. On the x-axis, more work going to the right. I've got my two parallel tempering methods that just use the linear path and they get no round trips. I've got the theoretical best possible performance in the black dashed line. If you run our path optimization algorithm with varying numbers of knots, this is what you see. So you can see that we've done k equals 2, 3, 4, 5, and 10 knots. In the early stages of In the early stages of the algorithm, you see that it actually performs exactly the same as the other PT methods because it's still optimizing the path. In fact, there's a big region of really terrible paths that are right next to the linear path. And it takes a while to find a better setting. But once it does, the round trip rate absolutely kicks off. And the really neat thing is that if you just look at the slope of these lines, it doesn't really seem to matter how many of these knots you use. Even if you just add that one knot, you just allow the path to squish. You just allow the path to squish as it moves, that gives you most of the benefit. If you were wondering whether our symmetric KL surrogate is a good idea, this figure should hopefully put that concern to rest. In orange here, I have my surrogate symmetric KL that's decreasing as I'm optimizing. In blue, we have the inverse round trip rate. So this is, again, the thing that you would want to minimize if you were sort of not paying attention to how poorly behaved that round trip rate. To how poorly behaved that round trip rate is. And you can see exactly what we were talking about. There's this very heavy-tailed behavior. You get these big kicks that will absolutely destroy any optimization algorithm that you try to throw at it. So the symmetric KL is well-behaved, and they both track. As you minimize the SKL, you're actually also minimizing this round trip rate, just with better gradients. This behavior, where you see at the beginning, there's some time that the Beginning, there's some time that the algorithm takes to sort of find a good path, and then it finally figures it out. And then all of a sudden, the round trips start kicking off. This is not just specific to that Gaussian example. We've done it on some cute Bayesian model, beta binomial model, and a more complicated high-dimensional model, which is a big mixture. You see the exact same effect in both of these and other examples that we've tried. Finally, just with the last couple of minutes here, Minutes here. We also looked at how this method scales with the dimension of the state space. So, again, a cute example where I have a normal, all of my coordinates are at minus one for my reference, all of my coordinates are at one for my target, and I have a covariance matrix of 10 to the minus two times the identity. If we run 50,000 scans, we scale the number of chains with the dimension. Then if I plot the round trip rate for the method on the y-axis. For the method on the y-axis, and I plot the dimension of my state space on the x-axis, then if I compare the standard parallel tempering method that uses the linear path in green here, you can see that the performance decreases, but then it hits this inability to perform and it just completely drops off. Meanwhile, our method also decreases, but much less drastically than the method. Than the linear path. So the problem just sort of gets harder, but the relative performance of using an optimized path increases. Okay, and I think that's pretty much it. In this talk, we went over parallel tempering, which enables inference with intractable and multimodal posteriors. I didn't go into this, but actually there was an incredibly cool practical application of these exact methods in black hole. These exact methods in black hole imaging recently in physics, which is super cool. This is actually a picture of an accretion disk of a black hole, like 50 million light years away or something like that. But the standard PT methods use the linear path, which we have shown has incredibly suboptimal communication efficiency. So you're really not getting the benefit of parallel tempering in most cases, especially Bayesian analysis, Bayesian data analysis problems where using the linear path. Path. In this work, we introduced how to optimize the path in PT by creating a linear spline instead of just a line. And we also figured out how to optimize these paths in a tractable manner. And in case you're interested, there is a copy of this paper up on archive at the link below. There's also a QR code. And this was published in ICML 2021. Thank you. Thank you. Thank you. We have time for a couple of questions. Any question? Can I go? Hi, Trevor. I was wondering: do you have to make K depend something like either the dimension or the number of mood that you want to explore or something like that? Interesting. So I don't know for certain, but I suspect not. The big advantage of adding the non-linearity into the path is that you allow yourself to have this sort of kink where you anneal. This sort of kink where you kneel down as you shift from wherever the mass of the reference is to wherever the mass of the target is. In all of our experiments that we've done, we don't really see a huge benefit by increasing the number of knots, almost regardless of the dimension. It's just really that benefit of being able to anneal. And the linear path just can't anneal. And if you just add one knot, all of a sudden you can anneal. And it sorts out the vast majority of the problems that we've seen. Yeah, and actually, of course, if you add too many knots, then the optimization can get trickier and trickier. So it's, I would say, just having one. Oh, you can't get Peter here. Oh, hello. I noticed in all your cartoon examples, you had like a cadently silly reference. How much juice is there in trying to get like a smarter reference, like just by posterior optimization to start? By posterior optimization to start initially? It's a very, very good idea. So, the obvious thing that you would try to do, right, would be to do variational inference just with some really just whatever, right? Totally, let's say uncorrelated Gaussians, just something that pushes your mass to the right place. The problem with that is that if you were to do that, you would end up with a reference distribution that's too skinny, especially when you have very heavily correlated. Have very heavily correlated posteriors. So, Alex and I actually have a student working right now on how to optimize just the reference by itself. And we're trying to see whether you can actually recover most of the benefit of this more complicated path tuning with just, yeah, indeed, just optimizing the reference distribution.