Our next speaker is going to be Sergio Gomez. I also have a collaboration of two with Sergio through Andrea again. I guess Andrea is the center of a lot of this network. And so Serpio is going to talk to us about stress-related methods for the Schrodinger equation. So, Serfio, the microphone is yours. Okay, thank you. Good morning. Okay, thank you. Good morning. First of all, I would like to thank the organizers of this very nice workshop. And I'm truly sorry that I could not attend online in present. Sorry. So this is a young work with Andrea Moyola from the University of Pavia, Ila Perugia and Polish Soccer from the University of Vienna. So you know them very well, so they don't need introduction. So first of all, I would like to mention a couple of things about space-time methods. So some of the advantages of space-time method is that they Advantage of space and method is that they provide a higher accuracy in space and time at once. Also, compared to the method of clients, where we first discretize in space and then we use some time-stepping to evolve in time with space-time method, we get an approximation of the solution in the whole space-time domain without requiring a post-processing. Then, probably one of the most interesting aspects of the space-time method is the possibility to do some space-time adaptivity, which means that. Which means that we can do, for instance, some local time stepping or we can do some isotropic space-time refinements. So, the kind of meshes that we consider are prismatic space-time meshes. So, this means that each element of our spacetime mesh can be written as the tensor product between a polytoptin space and a time interval. Then we classify the mesh phases into groups. So, we have the space-like phases and time-like phases, and in this representation. This representation here we have an example of an element k, and here in blue we have an example of a spectral phaset, and in red we have an example of a time-like phaset. So we also have this kind of notion of horizontal and vertical phasets for these kind of measures. So this is on one problem. We consider the Schrödinger equation where omega is this special domain, it can be in any dimension. Domain, it can be in any dimension. And then we define also the Schroding operator S that is defined in this way. And then we have B here, and B is a given potential that could vary in space and also in time. So this model, of course, has many applications in quantum mechanics, optics, and the water acoustics. I would like to mention also that there are just a few words on space and method for the strong equation compared to the heat or the wave equation. As far as we know, there are only these words. As far as we know, there are only these words. So, there are a couple of words by Karakash and Makuridakis, where they combine some continuous coloring discretization space with some DG or CG in time discretizations. So, of course, in this case, it's not possible to do local time stepping. Then, there is also a work by Dencoeks and collaborators where they propose this continuous petroleum arching method. So, as usual, for this method, one has For this method, one has to set a trial space and then one has to build a suitable test space in such a way to get some if stability. And there is also work by Heidi Nurban where they propose this space-hyenotoic method that is closely related to the one by Dankowigs and collaborators. But in this case, the idea is to first set the test space and then to get a trial space so that one has a Uh, so that one has a insubstability. And very recently, there is also a space-time discourse method by some people here in Pavia. And of course, there is also our space-time interweight discontinuous harvesting method. So, I would like to highlight some of the features of our method. First of all, the variation of formulation we propose is well paused, stable, and quasi-optimal in any space dimension, and we can allow for very general discrete spaces. The method allows for hinge faces. The method allows for hinder faces. So this, of course, so we can have measures like this in space-time. So this, of course, is very suited for adaptivity. And finally, we use on the opening DG in time. So this allows us to decouple the global linear season as a sequence of blocker problems on time slab. So we simply need to solve in the first time slab, and then we transmit information, and then we solve in the next time slab, and so on, and so on. Up and so on and so on. Of course, at some point we need to choose a discrete space, but so far we are only going to assume that our discrete space is a subspace of this Bracken-Bachner subspace, which essentially means that on each element we're assuming some local regularity, and this is not a restrictive at all. So this is our space-time, this ultra-weight discontinuous caloric method. So we look for an approximation of this solution in the discrete space such that In the discrete space, such that for all the elements in the mesh and all the test function in the same space, we get this equation is satisfied. And okay, of course, we get this equation after multiplying the original problem by a test function, integrating by parts once in time and twice in space. And here, okay, there are many things that I would like to mention. First of all, you see that when we do the integration by parts, we end up with this volume term. The important thing here is that we end up with the central. Thing here is that we end up with the essential inverter pair, but now apply to the test function. So, this is kind of our notion of self-adjointness. And then we have direct terms that, as usual, they are numerical fluxes. We need to define them. We define it in this way. So we take some upwind numerical fluxes on the specific phase sets. And then we take some standard average on the tunnel phase sets with some penalty terms. And then there is also the Onish term. And then there is also the orange term that is probably not so mysterious for you, but okay, this is a local disworce term. This local penalization term, first of all, is consistent, of course, because if we apply the Schrodinger creator to the exact solution, we get zoom. But this term is necessary, at least from the theoretical point of view. Partly not necessary, but it is sufficient to get well pauses independently. To get well poses independent of the discrete spaces we use. And okay, an important thing I consider is that this locally square series, it has a weight and this weight mu is scaled like a square. And this helps so that the condition over this stiffness matrix is not spoiled by this least per term. So when we sun over all the elements, we did these ratio formulations. So we have this. Spatial formulation. So we have this equilinear form A, and then we also have the antilinear functional L. And of course, if we use some discrete trap spaces, the volume terms in the information vanish. So this is, of course, the standard thing. Then we also define some DG norms that are associated with the sesquininia form A in this way. So again, there are some terms that are no longer necessary. You have Necessary if you have a trap space, so we can drop them. And okay, of course, the main results are that we get coercivity with respect to this DG norm. We also have continuity of A and the antenna functional L in terms of these norms. So combining all of these, we have that for any finite-dimensional subspace PHP, there exists always a unique solution, and this unique solution, of course, at this point. Solution and this unique solution, of course, satisfied the quasi-optimality, this quasi-optimality bound. Then, of course, you see that we have the DG plus norm on the right-hand side, and we can bound this DG plus norm in terms of some volumes of the norms, which means that in order to get optimal error estimates, the only thing we need to do is to analyze the local approximation properties of this space. Of course, there are some terms that are necessary if you have a pure. If you have a pure turf space. Okay, so as I mentioned before, we have a lot of freedom how to choose the discrete spaces. So we have considered different choices. First of all, we have the pseudo-plane-weight term space, but this is space of functions that are okay, that is spanned by these complex exponential functions that are nothing but some product between plane wave in space and some complex exponential function. Complex exponential function in time. And these functions depend, of course, on the definition of some frequencies, KL, and some directions, DL. Then we also consider the polynomial tropespace. And okay, here is the right place to say it. Okay, it's been typically said that polynomial trip spaces are okay only for homogeneous differential operators. That means only for operators where all the derivatives have the same order. So, how this is not. Somehow, this is not true, and this is what we discussed in this work here. Of course, as you can see, we are already assuming in the definition of the polymetrop space that we have no potential, so the potential has to be at zero. And then we also have the quasi-traps in space that, okay, I'm going to talk later. So, I would just like to say a couple of things on the analysis of these spaces. Of these spaces. So, first of all, for the polymer space, okay, what is true is that the Taylor polynomial of the exact solution is not in our polymer space. This is true. Also, the average Taylor polynomial, of course, is not in the space. So, this is bad. But what we prove is that there are some extended Taylor polynomials. So, we have some high-order terms, and you see in the definition of these extended Taylor polynomials. Trillopolynoms and these extended Terrible polynomials belong not to the trap space of the polynomial trap space of degree P, but to the space of polynomials of TREPS polynomials of degree 2p. So with these extended tail polynomials, we prove these prior error estimates. So if we have some assumptions, we have some assumptions on the elements. So we have uniform star shapedness. Have uniform star shapeness, local cross-uniformity in space, and if the exact solution is regular enough, of course, we have optimal convergence of order h to the p. So you can see that we need to assume some extra regularity in space here that I'm going to also summarize later. In practice, what we see is that, okay, for a very simple example, we have the expected convergence in H, so we have convergence of order H. In H, so we have convergence of order HP for all p, and we also have the standard exponential convergence for tracks functions. So we have exponential convergence with respect to the number of degrees of freedom, and this is one plus one d. So for full polynomials, we only expect exponential convergence with respect to the square root of the number of degrees of freedom. Okay, so about the single plane wave drop spaces, okay, the ideas for analysis are inspired. Ideas of pro-analysis are inspired by those in the paper by Césanate and Dupre. And essentially, what we require from the discrete space is that for any solution to the Schrödinger equation, we assume that there exists an element in the Turk space such that the Taylor polynomial degree P centered at some point is equal to the Taylor polynomial. Okay, so the Taylor polynomial of this element in the turf space is equal to In the turf space is equal to the Taylor polynomial of the exact solution. This, of course, translates into finding some complex coefficients in such a way that a linear combination of the basis functions, when we compute the Taylor polynomial of the group P, at the end, is equal to the Taylor polynomial of the exact solution, of course. If we assume this, we can prove again optimal convergence in this DG norm. So we have convergence of. This Dig norm, so we have convergence of order SOP with the same assumptions as before, but now we also require this condition one. Okay, you can see that here we require some CPU irregularity. We can also improve this result and just require some so-left regularity. Okay, just for simplicity, I would like to summarize the main ideas in the proof. So, okay, as I mentioned before, it reduces to simplifying some complex coefficients. Simplify some complex coefficients such that this identity is valid. Okay, so the first thing we do is we regard the original problem as a rectangular linear system. It's rectangular because we are assuming that the dimension of our discrete space is going to be smaller than the dimension of the polynomial space. So what we know is that the coefficients of the detail of polynomials of atroc functions must satisfy certain relations. If we denote by d the space of complex vectors that The space of complex vectors that whose entries satisfy these relations. What we know is that the exact solution, of course, in each one of the basis functions are just functions. So this imply that each row of the matrix M and also the right-hand side vector B, they both belong to the space D, to this set D. So essentially, we have the image of the matrixes containing in the set. The matrix is containing in the set, and also the right-hand side is in the set. So, in order to guarantee that there exists this vector A, the only thing we need to do is we need to guarantee that the choice of our basis functions are so that matrix M is full rank. So, if we are able to prove that our basis function satisfies this condition, then, okay. Uh, condition, then okay, condition one is satisfied, and this means that we get optimal convergence, as I mentioned before. So, in one plus one D, we proved that as long as we take 2p plus 1 of the elements in the basis, then okay, essentially in 1 plus 1d, we only have two directions, so we have left and right. So, essentially, here we simply assume that the parameter scal must be all different. If this is true, Different. If this is true, then condition one is satisfied and we get optimal convergence. So we get the standard reduction in the number of degrees of freedom. And this is how the business functions look like in space-time. So in 2 plus 1d, we get something similar. We require p plus 1 squared of the element in the basis. And the conditions here are, of course, a little bit more involved, but essentially we need some parameters and some directions. we need some parameters and some directions. Okay, for the parameters, we assume that they are all different and none of the parameters are equal to zero. And then for the, okay, also we also need to associate some directions to each of these frequencies. Okay, these directions are defined in terms of some angles. So essentially, if this is satisfied, then condition one holds, and we get again. Hopholds, and we get again optimal convergence. So we get the standard, the usual reduction in the number of degrees of freedom. And here there is an example of what we mean here. So essentially, we need to choose some frequencies, for instance, k0, k1, and k2. And to each one of these frequencies, we need to associate a certain number of directions. So, the number of directions increases like this. So, for k0, we need one direction, for k1, we need to take K1, we need to take three directions and so on. Okay, and this is an example. Okay, this is for a fixed time. We get again essentially clean waves. Okay, this is a numerical experiment in one plus one D. So this is a problem in one plus one D where we have a square well potential. And this potential depends on this parameter V star. And okay, you see that in this experiment. You see that in this experiment when we increase V star, then the extra solution oscillates faster in space and also in time. Okay, so first of all, for a small frequency, we observe the expected convergence of order rates to the p for p, but and we also observe the same exponential convergence as for the polymetric space. Okay, but in this particular experiment, for instance, we particular experiment for instance we we already know we we can shift a little bit uh because uh we already know the frequent the frequency of the exact solution so uh what we what happens is that if we choose the frequencies uh in a kind of arbitrary way then the rates of convergence of course degradate but then if we choose the frequency in such a way that we can capture the frequency of the exact solution then we recover and you can see in dashed lines we recover the expected converge Cover the expected convergence predicted by the a priori R estimate. Okay, finally, I would like to also say a couple of things on the quasitroff space. Essentially, this space is tailored in such a way that the Taylor polynomial of the exact solution belongs to the quasitroff space. So this, okay, in the case of the Schrödinger equation, even if the potential is pieceways constant, this doesn't mean that the quasi-trap space coincide. turf space coincide coincides with the polynomial turb space okay and here we can like of course allow for smooth varying potentials and we get the optional convergence again so okay this is an example so we have different potentials here this is this is the real part of the solution of for some standard problems and here we obtain the We obtain the convergence rates, and in each case, so we always get the expected convergence. In dashed lines, we also have the result for the full polynomial space. So, you can see that we don't lose so much in terms of accuracy. And okay, of course, the real gain is when we look at the p-version of the method. So, in the p-version of the method, we obtain the exponential convergence, we expect the number of degrees of freedom, while for the full pre-member space, we only expect the We only expect the exponential convergence with respect to the square root of the number of degrees of freedom. So, another important thing is what happens with the condition number of this matrix. So, in this case, you can see that for all these spaces, except for the pseudo-plane weight trap space, but this is not surprising, of course, we always get a conditional number that looks like x to the minus one. While for the plane, While for the pseudo-plane wave turb space, we get this exponential, sorry, the exponential number of the gross, like h to the minus 2p plus 1. But you know, this is the usual thing. So this is the issue that has been mentioned many times in this workshop and probably it's going to mention a couple of times more. So another interesting aspect, I think, is what happens with singular solutions. So here we consider this problem where the exact solution has just some Has just some disregularity here. You can see that for the polynomial trap space and the second point trap space, for instance, for p equal to 2, we only get convergence of order h, while for the full polynomial space or any quasi-trapped polynomial space, we get convergence of order h that five-fourths that coincide, that agrees, let's say, with the regularity of the extra solution. So in this case, Of the exact solution. So, in this case, it seems that the quasi-trap space and the full trendless space works better than the pure trap spaces, let's say. Okay, and this is just a table when I summarize all the different spaces. Essentially, for the polymetric space, we can only allow for zero potentials. When we look at the dimension, what kills us is that we also have these two here. That we also have this two here, which means that when d grows, so for large dimensions, but already in 2D, let's say, the local dimension of this space is too large. So, asymptotically, of course, we're always going to win against full-ponemption space. But, for instance, in 2D, for instance, you need to use approximation of degree 6. 6 is to beat a full prenemous space. So, okay, in for in 3D, for instance, you need to use approximation of degree 21 to beat the full prenormal space. Of course, this is not reasonable. And in addition, we also need to require some additional regularity in this special direction. And okay, you also have that the condition number is good. For instance, the plane wave threshold space. For each of the plane wave turb space, instead we can allow for piecewise constant potentials. The dimension of the space grows like p to the d. The regularity of the solution, we only need HP regularity. And then, of course, the condition number is really bad, as you have seen. As for the full plane space, we can allow for pieces of smooth potentials. The dimension of the space looks like. Of the space rows like p to the t plus one. Okay, the reality of the solution we require is just hp, and the conditional number is good again. And then probably the most robust in this case seems to be the quasi-trop space because we can allow for a piece of smooth potential. The dimension of this space is the same as for the exodoplaine-trop space. Of course, we need to pay the price of some CP regularity here. Uh, CP regularity here, but the indecognition number is still very good. And with this, I would like to end my talk. Thank you very much for your attention. Thank you. Perfect timing. I have to say I'm gonna start by your comments. I have to say I'm always happy to see good results from quasi trust functions, so thank you for sharing that. And uh do we have any uh other questions? Do we have any other questions in the room or online? Can I ask you a quick question about the basis function? So the set of functions that you use, maybe I missed it, but do you get the fact that they are linearly independent for your choice of angles from the rank of the matrix, or do you prove that separately? Separately okay, no, no, yes, they are linear independent. Okay, we prove it here, right? Because otherwise, the metric sampl would be would not be full rank. So, yes, it's somehow contained in the proof of the condition. Yes, so in the proof of this result, let's say. Inside, we also need to, yes, it's equivalent, let's say, to prove that also they are linear independent. All right, thank you. I see that Ralph has another question. You can go ahead and unmute yourself, Ralph. So hi, Sergio. Thanks for the impressive talk. I suppose in each time step you've got to solve a linear system of equations. Yes, yes. And does this have nice properties or well, ugly properties? Well, ugly properties. What is the structure of that linear system? Okay, yes, of course, if we don't change the mesh from one time slab to the other one, okay, the mesh remains the same for all the time slabs. Of course, if you do adaptivity, then you have to compute the stiffness matrix on each time slab, which is probably a little bit more expensive. Okay, I think the nice property is the condition number, because the condition number is better than I expected. Number is better than I expected. Yes, so it's not like H1 minus two for analytic problem is H2 minus one, which seems to be okay. Of course, in the numerical result, we are not considering how to solve the linear system. We simply use a Gaussian elimination, and that's it. But yes, I think one of the interesting aspects of this equation that, by the way, the same condition over you get for the heat equation, it shouldn't. The heat equation h to the minus one seems to be nice. So, no special properties like symmetry or other structures? No, it's not symmetry, it's not symmetric because the operator itself is not symmetric. No, no, okay, it's just that is non-singular. Okay, for the direct properties, this is enough. Yes, exactly. All right, thank you. Thank you. All right, do we have any further questions in the room or online? Nothing suggests we thank our speaker again. Thank you, Matthio. And we have five minutes left.