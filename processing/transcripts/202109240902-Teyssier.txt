Thanks a lot for the invitation. I'm very happy to be able to speak about random transposition in the workshop. Okay, so the title says everything. We are going to speak about random transposition and we are going to show the cutoff profile and give some Give some ideas about the proof. Okay, so first you all play cards, or I guess, from time to time. And sometimes you have to mix the deck of cards. So there are two good ways to mix cards. The first one, if you're skilled with your hands, is you do like Percy Daeconium. Like Percy Taiconis, you split the deck of cards in two and you riffle it. So this mixing is so emblematic that it has many names. So it can be dovetail, riffle, parole shuffle, American shuffle sometimes. And if you're not skilled, what you can do is that you can put the deck of cards on the table, you spread it and you mix it with your hands. And this works also very well. This works also very well. The other ways to mix are quite bad. Okay, so today we are not going to consider those shuffles. We are going to consider maybe the most elementary shuffle, which is the shuffle by random transpositions. So what we do, we take a deck of n cards. So you see my deck. Okay, here I have six cards. Have six cards, and at each step, you pick at random two cards. So, say you pick this one and this one, so those two, and you swap them and you reiterate. So, at the next step, you can take those two and you swap them. Okay, and you reiterate. So, what I said. So, what I said is what's written here, and another way to see the work is to as okay, the host, okay, something was written on the screen, sorry. And you can see this work as a random walk on the symmetry group with an increment measure, which is this one. And you can see also You can see also this, so it defines at the same time a transition matrix. So, what you do is that you start from the identity, and at each step, you pick a random transposition. You pick a transposition uniformly at random, and you multiply. Then you pick another transposition, you multiply, and yeah. And when we mix the cards, there's some probability as we pick the two cards independently that we pick the same card and we do nothing. We pick the same card and we do nothing, so that's why there's one over n for the identity. So we pick the identity with probability one over n. This doesn't really matter. It gives some laziness to the workers. Okay, and if you look at it as a random work, so on a on a group, on a Kellogg graph, for n equals three, it looks like this. So you have six vertices, which are the elements of the symmetry group, and you jump. To you jump from a vertex to another, multiplying by transposition. Okay, so the work is quite elementary. There's some noise in the room, I think. Yeah, thank you very much. Yeah, thanks. What did I say? Okay, so you have a walk, which is A walk which is very nice. So, transposition they generate the group, and so you will get close to uniformity as the number of steps tends to infinity. So, to which sense do we get close to uniformity? I won't explain anything about mixing times here. I will just define the quantity that we will be interested in, which is Which is the distance to stationarity? So, this we will reuse during the whole talk. So, remember this definition. It's the distance in total variation between the probability measure of the walk after T steps and the stationary measure, which is here the uniform measure. And yes, so we take the distance in total variation, which is The distance in total variation, which is up to a constant factor of the L1 distance. Okay, any questions so far? No. Okay, good. Okay, so the first important result, very important result for this shuffle is the cutoff result by Tiaconis and Shashani, which was proven at the beginning of the 80s. So, what it says is that what it says is that if you take a deck of n calf you need approximately one over two n log n transpositions to mix it to which sense so to this sense so if you are a little before one over two n log n your distance to stationary will stay close to one so you won't have mixed deck of calves and if you're a little after you will be Detail after you will be very well mixed. Actually, what they proved is a bit more precise. They proved that to mix the deck of cats, it takes one over two n log n and that everything happens in a window of size n around this mixing time. So this okay, can I move? Can I move? Okay, this zoom bar was a bit annoying. Yes, so this threshold phenomenon is what we call the cutoff phenomenon. And something very interesting which we can do for not so many walks is to try to understand what happens in this window. So if we try to zoom in the window, in the cutoff window, can we understand? Can we understand the full profile? And the answer is yes. And here's the formula. So yes, for this mixing by random transpositions, if we are close to 1 over 2n log n, so at this time, so c is a fixed real number, it's a fixed parameter, then the distance to stationarity will converge to a function. will converge to a function to some function we can which can be written nicely as a distance between two probability measures and more precisely it can be rewritten as a distance between a well-known probability distribution and a distribution which is a deformation of the first one. So as c tends to infinity, you can see this term goes to Infinity, you can see this term goes to zero, and so the distance to stationarity goes to zero. A few comments. So we can interpret this distance in terms of fixed points. So we can try to guess the interpretation and we can come back to it at the end of the talk. Some comments. So yes, so this function is what we call the cutoff profile. cutoff profile. The question to know if we can find the cutoff profile was asked by Nathanel Beistiki, who's my PhD advisor in 2016. And there are other works for which the profile is known. And every time we can rewrite, it's not always written in the article, but every time we can rewrite the profile as a distance between As a distance between two probability measures. So, most often instead of having a low of Poisson, we have a normal low and a shifted normal low. But yeah, most profiles can be written in a nice form. Okay, very good. Do you have questions? No, not yet. Okay. Okay, some bibliography. So sorry, this slide is a bit heavy. There are many names. So this random transposition shuffle, it has a quite long history and only for the random transposition shuffle. There were many articles. So sorry if I forgot your article in the list. So what we said. So, what we said, at the beginning of the 80s, there was this cutoff result by Deconis and Shashani, which is a starting point also for the theory of cut-off phenomena. Then there's an article which I like a lot, which is this article by Matthews. So unfortunately, so he gave another proof of the cutoff, not using representation theory as the Diaconistan chest. Theory as Diaconis and Shashani. Unfortunately, his upper bound contained a mistake which was fixed by Graham White. But his lower bound is excellent. And in particular, his lower bound is exactly the limits profile. So he really understood how fixed points worked to give a precise Lawabound. Then there were other Then there were other articles by Peristiki, Jourette, Alon Cosma about some other observables. And so my contribution to this story is the cutoff profile. Okay, so there are many generalizations. So for example, instead of taking transpositions, you can take K-cycles and or you can take conjugate. Or you can take conjugacy classes in the symmetry group. And so there were cutoff results for those works as well. And for the K cycles, so this was proven this year actually by Nestoriti and Olisker Klog, sorry. The cutoff profile is also known now and we can even take a K which is equal to Is equal to a little O of n. So the best result for K-cycles that we can hope is known now. And there were other generalizations. So Oliver Mato-Raven in his PhD thesis, he studied biased random transpositions. So you give more weight to some transpositions than others. And with some co-op both from And with some co-founders from the University of Ausch, we studied the cutoff profiles on quantum groups. And we also found a profile for quantum random transpositions. I don't explain what it is, but it's a variation on the symmetry group, more non-commutative in some sense. Okay, so now we are going to explain the ideas. The ideas, the main ideas of the proof. And to begin, I think it's pertinent to explain the method by Daikonis and Chashani, as what I did is based on their work. Okay, so a wonderful idea that Daekunis and Shashahani had. And Shashahani had was to use the Fourier transform to study the random walks. You all know the Fourier transform on R. So you have such formulas. If you take a function from R to C, probably, you give to it a frequency and you have such formulas. And actually, if you are not on R but on a finite. Are not on R but on a finite group, you have similar formulas which exist. So there's also Fourier transform on finite groups. And you can see the formula is really similar. So instead of plugging frequencies, you plug irreducible representations. So in the talk, if you are not familiar with representation theory, you can just imagine that representation theory, the representations are the frequencies. Representations are the frequencies after taking a Fourier transform. And you have such a formula. So you take the Fourier transform, you give it an irreducible representation or frequency, and the sum, the integral, is the same, f of x, f of g is the same. And here in the real case, we have an exponential, which is a complex number. And if you consider groups which are commutative, Groups which are commutative, then you also get exponentials. But we are interested in the symmetry group, which is highly non-commutative. And in this case, those terms are actually matrices. So they are more complicated than in the real case. So what we can say about this Fourier transform is that it's as useful as on R and in. It's as useful as on R. And in particular, the full theory can be adapted. So there's an inverse Fourier transform. If you define the Philbert spaces properly, the Fourier transform is an isometry between them, and you have a possible identity. Yes, and if you're curious about those Fourier transforms, those generalizations of the Fourier transform, there's the book by Carlo Icimelio, who was written. Carlo Aquelio, which was written in 2017, I think. It's quite recent and it explains everything super, super well. Okay, questions about the Fourier transform? No. Okay, let's continue. So now I will explain the method of Daikonisan Shashahani. Honey. So the lower bound of the lower bounds, they are quite simple and you can give probabilistic arguments. So if you have done not enough steps, if you are a bit before one over two n log n, then you will not have even touched all the calves. So the resulting permutation will have a lot of fixed points. Fixed points and it cannot be close to uniformity. So, simple probabilistic arguments give good lower bounds. The upper bound is a bit more complicated, but I will try to explain it to you. You take the distance to stationarity. So, the distance to stationarity, we said it's an L1 distance between probability measures. So, if Probability measures. So, if we take the function f to be the difference of our probability measures, we can rewrite it just as a sum. Then you apply the Cauchy-Spars inequality to this. So you get a sum of the squares of a function. And here you have to believe me that this is the correct formula, but I told you there's a Pasabal formula in the Fourier transform, in the Fourier theory. Fourier transform in the Fourier theory of finite groups. And so you get this. And when you rewrite this, okay, you can kill the uniform distribution and it just kills the irreducible, the trivial representation. So you end up with a formula which is the sum of representations of some coefficients and a trace of the product of matrices. And the trace of the product of matrices, this is not particularly nice. So you don't really know how to handle this. But in the case of random transpositions, there's some extra property which is very useful. It is that our increment measure is stable on conjugacy classes. So it means that we take all transpositions with the same probability. And in this case, the Fourier transform of our increment measure for each lambda, it's a multiple of the identity matrix. So it's much simpler. We can write it as so this coefficient, this S lambda times the identity matrix of size d lambda. And this ugly. And this ugly formula becomes much nicer, and we just have a sum of squares of some coefficients. Last thing to say is that we are working with representations here, but you can interpret those coefficients in terms of the transition matrix. And so the S lambdas are just the eigenvalues of the transition matrix, and d lambda squared, they are just the equation of the equation. And d lambda squared, they are just the multiplicities. So you are bounding your distance to stationarity just by some sum of eigenvalues to some powers. Okay, questions about the DACUNINIS SHASHAHANI ABAMAN LEMA? No. Okay, so I will continue and now I will. And now I will explain how to improve this method to get limit profiles. So, this is one key point. So, the next slide will be very important. Something that you can observe if you read the article by Deknis and Shashani is that all their asymptotic bounds are perfect. They lose no information except at the very beginning, at the first inequality when they apply the Cauchy-Spart inequality. So, if you want to If you want to improve to have a better bound, you cannot afford using Cauchy Schwats and you have to stay with L1 type sums. So what we do is that, as I said, we stay with L1 type sums. So our distance to stationarity is just this sum. And then we apply the inverse Fourier transform. Apply the inverse Fourier transform to this function, you get an ugly formula. You can believe me that it's the correct formula, but you get an ugly formula. And as I said, if you want to erase the uniform distribution in this function, you just take the trivial representation out and you have your formula. So you have an average over all permutations of Of some formula still with traces of a product of matrices, which you don't really know what it is. And same thing, as our increment measure is stable on conjugacy classes. You can simplify it and you get a nicer formula. So here it is. And okay, so you can rewrite the distance to stationarity as an average over all permutations. As an average over all permutations of the sum, here you have the eigenvalues of the walk, of the transition matrix. Here you have the multiplicities, essentially, of the eigenvalues. And here you have the characters which corresponds to the eigenvectors. And this is this term which is a bit hard to handle. The two other ones are easier. Easier. Okay, and now we have this sum, and you need to approximate this sum. So you need to understand which irreducible representations matter and those characters. Something worth mentioning, so I spoke in terms of eigenvalues and eigenvectors, so I don't write their formula. don't write their formula but they generalized nestori and polisker taylor they generalized in their article this approximation lemma to all reversible irreducible periodic markov chains so if you have if your favorite markov chain is nice and you know its eigenvectors then you can try to compute its cutoff profile it should be possible if you have enough Be possible if you have enough information on the eigenvectors. Questions on this part? No? Oh, okay. Yes? So which representations matter? Okay, so this we will see after. So you know the presentations of the symmetry group, they are actually Jung diagrams. Jung diagrams. And those which matter are the Jung diagrams with a very long first line, with the first line which is almost n, so n minus j with some j fixed. Okay, thanks. Yes. This was what I what I said was also understood, I think, by Jacobis and Shasharani. They grouped the representations. Group the representations according to the length of the first line. Okay, so now let's speak about the proof. So this formula, we will reuse it, but I will rewrite it when we need it, so you don't need to remember it. What I just said is that the irreducible representations of the symmetric group Of the symmetric group, so the Fourier frequencies of the symmetric group, they are actually in bijection with the partitions of the integer n, which you can write as Jung diagrams. So, for example, for n equals 5, the representation lambda will be written as this. So 5 As these, so 5 equals 5, it's also equal to 4 plus 1, so 4 on the first line, and 3 plus 2, 3 plus 1 plus 1, and some others. And I will add a definition, which is the definition of a truncated partition. So if you have a partition like this one, you define the truncated partition just killing the first row. Killing the first row. So, as I answer to Gilles, Gilles or Gil? Gil. Gil, okay. Thanks. So as I answer to Gil, we are going to consider partitions which have a very, very long first line. And so when you truncate the first line, you have almost nothing left and you can study what's left easily. Uh, easily. Questions about this? No? Okay, let's continue. I told you that I would rewrite the formula. So I will give the ideas of the proof. We start from this formula and then we want to estimate the different terms. So we have three things to estimate, this d lambda, this s lambda, and D lambda, this s lambda, and these those characters. And what I said also is that we are going to pack the representations according to the length of the first line. So if we fix a j, we are going to take all the young diagrams which have the same first line length together. Okay, so the S lambda, I told you they're the eigenvalues. They were already known by Daekonis and Shashani. And you can... So the S lambdas, if the first line is almost n, they are just these. So this is easy to handle. There's no problem with eigenvalues. The multiplicity of the eigenvalues are as follows. As follows. So Japanese and Shashani, they showed this bound, which is true for all Jung diagrams. And actually, if you fix the length of the first line, if you fix J, this is not only an inequality, but it's an equivalent. And I rewrote the lambda one, the n2. lambda one the n choose lambda one differently but it's just that this inequality is an equivalent in our case so we know s lambda we know d lambda up to some good error and we have to understand the eigenvectors okay i'm putting some suspense on the eigenvectors but yes um and Yes. And yeah, I forgot to mention this, but for the multiplicities, we find this using the Hookeleng formula, which was also mentioned by Igor Pack yesterday. And for the eigenvectors, we have some formula which is not so simple, which is the monagamna-chema formula, and we will speak about it at the end of the talk. Now I will give the second main part of the proof. I will tell you the second main part of the proof, which is how to deal with those characters. So I will first give you how they compensate and then I will try to explain a bit how it works. So if you fix a day, so if you fix the length, Fix a j, so if you fix the length of the first line to be n minus j, then you can consider a weighted average of characters. So remember, this is the truncated partition. Okay, I'll let you read this formula. And the thing that we can observe is that this average can be rewritten with an equal. Can be rewritten with an equality, so it's not an equivalent, it's a rigorous equality for almost all permutations as a polynomial. So we don't care about the polynomial. What matters is that we can rewrite this complicated average on characters as something depending only on the number of fixed points of the permutation sigma. So fixed sigma is the number of fixed points. So fixed sigma is the number of fixed points of the parameters in sigma. So this seems a bit magical. And what I meant by almost all permutations, it's permutations which have at least one cycle, not too small. So this is really almost all permutations. Permutations which have all cycles of lengths less than 10, they essentially do. And 10, they essentially do not exist. Okay. I still have around 10 minutes, right? Yes? Yes. In the remaining time, we will see how this formula implies the cutoff profile, allows us to find the cutoff profile, and then we will try. Profile, and then we will try to explain how to derive such a formula. So, first, I won't read everything here. As you know all the parameters, if you place yourself at the correct time, at the time you want, you can plug the values that we showed earlier. Earlier, and you can see that the distance in the distance to uniformity at this time in the window can be rewritten as a sum which depends only on the number of pix points of the permutation sigma. So, here we have an average over all permutations of something depending on the number of pixels. And what you probably And what you probably already know is that the distribution of the number of fixed points of random permutation, it converges to a law of Poisson of parameter one as n tends to infinity. So yes, do you know this or should I explain rapidly why it's true? Okay, this is true essentially because the probability, imagine the probability that the first point is a fixed point, it's one over n. The probability that the second point is a fixed point is also one over n. And it's true for all points. And so the number of fixed points is the sum of n Berni variables of parameter one over n. Of parameter one over n, which are not exactly independent but almost independent. So, if they were exactly independent, you will have the convergence to the law of Poisson one immediately, and they are almost independent, so it still works. And this way, after some computations, which are not very interesting, you get the limiting copy. Okay. Okay, now let's try to explain my favorite, I think it's my favorite formula in mathematics, which is this formula of Monaghan Naquema. So the formula is a bit complicated, so I will try to give you an idea how it works. The Monaga-Nakayam formula gives an exact formula for the characters. The characters. Counting how many ways you can cover a diagram, a young diagram, like this one, by rebounds. So rebounds are things like this. So you are connected. And you don't contain any square. Okay, is it clear what a ribbon is? It's something which looks like this. And here I wrote sigma. Actually, what matters is only the cycle structure of sigma. So the character at some lambda. At some lambda, at some Jung diagram of a permutation which has one four-cycle, one two-cycle, and one fixed point. You try to cover this Jung diagram, putting first a ribbon of length. A ribbon of length four at the bottom or on the left, then a ribbon of length two, and then a ribbon of length one. And you have three possibilities, which are those ones. And then if you have ribbons which are flat, they have something which is called the height is you will have. You will have a coefficient one, and if you have ribbons which have an even height, then you will have a coefficient minus one. And so summing those things, you get the value of the characters. So you probably have questions. It's a bit hard to explain this formula. Is it clear or did I give you an Or did I give you an idea which is precise enough of how it works? Okay, so you count how to cover young diagrams with ribbons or with unions of ribbons. Now let's give another example where the first line is quite long. So if you take this character This character, and you apply it to a permutation with one eighth cycle, one to a second, two cycles, and three fixed points, then there are many possibilities. But what we are interested in is that some possibilities are in some sense equivalent. So you put the ribbons in the same order, but in this In this order, but it doesn't really matter whether you put the orange here or here. So you can see those two have the same shape. It's just that we didn't place the ribbons at the same place. And for this class of ribbon, if the word class has meaning here, how many equivalent Coverings do we have? Can you tell me? Was it a followable? So how many ways can we cover this diagram with those types of ribbons such that we have this structure up to colours? So, yes, for the transpositions, so for the ribbons of size two, here we have two possibilities. And for this part, we have three possibilities. And so, as a total, we will have six possibilities in this equivalent class. Okay. Okay. And okay, this is a bit complicated to explain, but this one, if we had more two cycles or more fixed points, we just pick one among them. So we have, we pick one ribbon of size two and one ribbon of size one. One and so this is a more general formula for if we had another permutation here. Another, yes. Okay, and here if we truncate the first line, here it's just a covering with ribbons of the truncated diagram, which is linked with the Monagamakema formula, but for the truncated diagram, For the truncated diagram, which itself gives exactly the characters for the truncated diagrams. And playing this way up to some coefficients, instead of having representations of the symmetry group of the full symmetry group of size n, we can restrict ourselves to formulas on representations of the Of the truncated Jung diagrams, and this is much more pleasant. Yes, so this is the main idea. So we find a way to consider the characters not on Sn but on SJ. This is okay, you don't really need to read this in detail, it's just an example. So this average. So this average for j equals 2, this character applied to sigma, to any sigma will be the number of transpositions in the cycle structure of sigma plus the number of fixed points of sigma choose 2 plus other coefficients. And this one you will have a formula. And what you can see is that. And what you can see is that everything which is not related to the number of fixed points will cancel out, and in the end, you will have something only with the number of fixed points. And if you take J larger, you have horrible formulas. So yeah, I don't write more about this. A last thing that I want to write and that I did not write on my slides is that the interest here The interest here in this formula, we want to transfer this lambda so that it's a lambda star. So we said we want to restrict ourselves to representations of the truncated diagrams. And if here you add a star, you have a formula which is, you can recognize something. So d of So d of lambda star is actually the character of lambda on the identity. And so what you get is essentially a sum of CH lambda star over all lambda star of the identity times CH lambda star of the permutation and And there are orthogonality relations for characters. And if sigma is different from the identity, then this will be equal to zero. So that's a way to understand why everything which is not connected to fixed points will cancel out. Okay, I hope the explanation was okay-ish or that you could get something from this explanation. Get something from this explanation, and yeah, thank you for your attention. Thank you. Questions and one, one question that okay. One question. Okay, so one question concerns. You mentioned there are some calculations that you didn't really describe where you get the Poisson of one plus e to minus two C from the formulas. So is there some easier way to see what is Poisson one plus e to the minus two c? So this was fans to the fixed point, but is there an easy way to see this from the formulas that you get this? From which From which formulas? So you had some formulas with the sums over the representations, and then yes, and then you say, okay, so this converges to the total variation between the two Poissons. It's just computations. So if you want to know the story, actually I made the computations. I had some big formula. And one month later, I realized that it can be written as a distance between those two Poisson distributions. Between those two Poisson distributions. So here, at this point, between this and this, there's no intuition to get it, just computations. Then you can interpret the profile, but between those terms and these, it's just computations. Does it answer? Well, it's not the answer I was hoping for, but yes. Forward, yes. So, so excuse me, if I may continue. So, does this computation that that's because you have some formula for your functions t sub j, I guess? Ah, yes, yes, yes, yes. So, in this, the form of the polynomial matters. Yes. What I say is that here you have a double sum, then you replace this by a Replace this by a Poisson law of parameter one, and then it's just an inversion of sums and you do computations. But I did not include this because you need an explicit formula to do some calculation with it. Thank you. Can I ask another question Um so uh well first it was a beautiful talk um and uh you I think you mentioned uh some subsequent work so maybe it answers the question but is there any similar result when you don't take the uniform distribution on the transpositions but some other graph or weighted graph. So I don't know. And so the whole technique works because we are giving the same weight to all transpositions. And what I said in the method is that if you're not conjugacy stable, then you stay with matrices. And so the method does not apply. What is in the case this is What is in the cage this is of Oliver Matso Raven? So he has the cutoff. There are interesting cutoff results. But the technique they use in this case is the technique of lifting eigenvalues, which is also used by Nestoridi and Bernstein for the random to random shuffle. And then you get recursion formulas for the eigenvectors, but it's very complicated. But it's very complicated. So I think not. So I think there's no other profile. What I can say, which is maybe funny, if you put more weight on the identity, if you add a lot of laziness, if you give a weight almost one to the identity, then you may have another limit profile. This is pointless, but yes. Uh there are things in the chat. Oh, okay. Is there a last question? So the papers by Ernest Ridi and I don't remember what the other name. Yes, where they do this for K-cycles, do they also rely on representations? Or is this? Yes, they also rely heavily on representations. So what they do is that, where am I? Okay, they use this formula. So they use the same approximation lemma and then they use bounds on the K-cycles, which were proven by Bob Ho, half. I don't know how to pronounce it. I don't know how to pronounce O U D H. So he and they do similar computations as the one I did. It's a bit more complicated because it's K cycles and not transpositions, but the method is similar. And yeah, it relies heavily on Relies heavily on representations. I would be very, very happy if someone can find a proof of the cutoff profile without relying on representations. So one of my hopes was the article, the upper bound by Braham White with the strong stationary time, but I could never ask him how precise it is. It seemed to be a very, very good approximation. Very good approximation. So I don't know what it gives on the profile. If you know him, you can ask him or tell him to tell me. Peter, you've got a hand up. Yeah, look, what is the behavior of this interesting function, which is that the total variation distance between Poisson 1 and Poisson 1 plus e to the minus 2c? Is it something like constant e to the minus 2? Is it something like constant e to the minus two c or? Is it what? Is it something like a constant times e to the e to the minus two c? If you want an asymptotics as c tends to infinity, yes, so maybe it's equivalent to exponential of minus 2c or in this if you add the two here. But the function itself. But the function itself, if you look at different profiles, you see nothing on the function and you just see a function which is decreasing from one to zero like this. So yes, the asymptotics are actually as c tends to infinity, what matters are only or is only Or is only this representation, right? Basically, that the yeah, so the representation of your Poisson distribution. Yes, because here you have a sum of the exponentials of minus 2 J C, and if J is greater than 2, all the terms are negutable. And something I can mention is that Is that when Diaconis and Shatani do the Cauchy-Spart inequality, so on the equivalent, on the asymptote, on the equivalent of the profile as C tends to infinity, they lose a factor one over E. So even on this first term, you can already see that the Cauchy-Spass inequality has some impact. Is fast inequality has some impact. Interesting, thank you. I love the theorem. So I guess we can thank Luca again. Okay, so thanks again for the organization and everything. And if people still have questions, I will go during the