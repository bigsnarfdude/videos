It was a very nice initiative taken by Emmanuel and Cosme and Dimitri and Sanyo. I think you're also one of the organizers. It's very nice to be here, to be in a workshop where there is no machine learning. I really appreciate that. And people are doing numerical analysis. Okay, so I'm going to talk about endorian event preserving. To talk about end-oriented event preserving techniques and essentially, I'm going to focus on the compressible Lavier Sox equations. So, this is a joint work with Martin Kronbischer, Matthias Meyer, Boyan, and Ignacio Thomas. Here, I want to acknowledge the support of the Air Force, the Army, Lawrence Livermore, and the NSF. So, this is the organization of the talk. I will give some background. The talk: I will give some background for the work and then I will focus on the compressible Navier equations, then we'll give some numerical illustrations. I am sorry, but I will repeat a few things that have been already said during previous talks, in particular the talk of Manuel and Dmitry and Laura. So, the objective of the research program is as The objective of the research program is as follows. We want to develop numerical techniques for solving non-linear conservation equations, PDs that have a dominant hyperbolic feature with guaranteed properties, that is to say, being in domain preserving, sympathetic preserving, be somewhat discretization diagnostic, that is to say, I want the method to work with finite volume, finite differences, DG, CG. I don't really think that the I don't really think that the discrimination is important, actually, provided that it is done properly. And also, we want to satisfy some outro inequalities. The key challenge here is that we want the properties to be guaranteed and certified through some theorems or very strong numerical evidences. So we think that methods with certified properties are robust. That would be, let's say, for us, a definition of robustness. Definition of robustness. And this method can be used in confidence with very little known how from users. And I think it is very important to develop such methods. And we also want to involve as little numerical parameters so that we have nothing to learn. Okay, so field applications. So far, we work on the compressible oiler. Euler Euler Poisson equation composite on the Naviestox, creation, ideal MHD, radiation transport, multimaterial foot flows, and the shallow water equations, the Saint-Vernon regime and the SER dispersive regime. So some results here. I'm going to skip that. So currently what we're doing is we are working on the demonstrating. Working on demonstrating that some of these techniques can be scaled to very large scales on massively parallel computers. So essentially the method has been reprocessed by Matthias and Martin using DIL2 and using MPI, multi-threading, and SIMT vectorization. So this is a So, this is a current work. We are also working with Bennett Clayton on the approximation of the Euler equations with tabulated equations of states. So, essentially, our goal is to solve the triple point problem that Laura was mentioning previously. And essentially, now we have the technology to do that. This problem is essentially solved. Essentially, solved. And topic of the day: well, we're working on the compressible Lavi-Stokes equations using some implicit time stepping. And we're also working on the grey radiation, Euler Poisson. And yes, there is a problem here that maybe we should discuss this afternoon in the discussion session about constructing methods that are invariant when preserving for mixed approximation. I really think that this. I really think that this is an open problem that is not really clearly solved. Although yesterday, Remy said that it was easy to do. Maybe we would need more discussion on that. And also we're working on Lagrangian hydro and staggered grids, which is the same topic. How can you do that on staggered grid and still prove a theorem? Okay. Okay, so here is an illustration of what has been done by Matthias. So this is the Euler equations solved with the methodology that we have proposed with Boyan using a DL2 with Q1 elements with 1.8 billion degrees of freedom. Okay, so this computation really shows that the method scales very That the method scales very well. Okay, let me continue. Now I want to talk about compressible Navier-Stokes equations. Okay, so the compressible Navier-Stokes equations, we all know what this is. So the dependent variable is a triple composed of the density, momentum, total energy, and we have the conservation of mass. The conservation of mass, conservation of momentum, and conservation of the total energy. Some boundary conditions, some initial data. I'm not going to spend much time talking about the boundary conditions. And to simplify the presentation, although it is not really a limitation, we're going to assume that the Fourier is Newtonian and the heat flux follows Fourier's law. Fouriers law that simplifies the presentations. Okay, so here we have appreciated two invariant domains that can be identified. If we just consider the Euler system, the invariant domain is positive density, positive internal energy, and we have also a minimum principle on the specific entropy. Whereas if you look at the Whereas, if you look at the Navier-Stokes equations, the invariant domain is smaller because the Navier-Stokes equations violate the minimum principle on the entropy. This fact is not very well known, but yes, the entropy, the Navier-Sox equations violate the minimum principle on the entropy. So, a priori, we have two different invariant domains. So, when we So when we started this project, we were not exactly sure how to handle that because what is the environment that we want to preserve? Do we want to enforce a minimum entropy principle? Yes, we can do that for the Euler, but we cannot do that for the negative equations. And which variables should we use? The right variables for the Euler system seems to be the conserved variables. The conserved variables. The right variable for Naviestox seems to be the primitive variables, and some of the people in the literature advocate entropy variables, they advocate entropy stability, etc. Why? I'm not sure. So here the situation is a bit fuzzy about the variables to use. And a third point here is how. Point here is how to do explicit and implicit time stepping. At the beginning, I was thinking that taking some IMAX techniques from the shelf would work, and it turns out that it's not the case. So, here I want to mention one result. So, when we started this project, to our knowledge, there was only one result. Published in literature with a mathematically precise slash correct result about the environment environment domain preserving approximation for Navier-Stokes equations. It's a paper by Zhang and Shu and the restriction, the parabolic time step restriction. So our goal is to essentially reproduce this result, the result of Zhang and Shu, but with the hyperbolic CFL. So we're going to prove. So we're going to prove environment preservation. I'm going to be precise about what I mean by environment preservation. And the DPABCFL, that is to say, the time step will be bounded by some velocity times the mesh size only. Okay, so this is the plan. So the strategy will be as follows. We're going to use an operator splitting technique to separate the Splitting technique to separate the hyperbolic and the parabolic part. So the hyperbolic part is the Euler system and the parabolic part is what is left over. Essentially, this is the heat equation with here a very a term that is annoying. Yeah, no, yeah. Yeah, no, yeah, this is that. So, so the idea of using a parator splitting actually has been used by Demkovitz, Oden and Rachowitz in 1990. But the problem is how to do that properly. Because remember, the good variable for Euler is the concept variable, the good variable for the parabolic part is. The parabolic part is the entire entire primitive variables. And so the problem is really to blend these two techniques to be conservative and to be positive. Okay, so we're going to combine the explicit and implicit part in using the strong splitting in some clever way. And again, as usual, the devol is in. And again, as usual, the devol is in the details. So we're going to use the concept variables for the hyperbolic part, and we're going to make it explicit using the invariant domain technology with convex limiting that is now quite standard, I would say. And for the parabolic part, we're going to use the primitive variables. Variables, we're going to make the viscous term implicit in some clever way. Just using an implicit Euler will not be enough. And we're going to make the implicit algorithm environment preserving up to second order. All right, so for the hyperbolic step, essentially everything has been said by Laura. Been said by Laura. So I'm going to repeat what she said. We're going to use the old technology, although now it seems that Dmitry and Manuel have a new way to do that. But we use the old way. We construct a low order invariant domain preserving technique. Variant domain preserving technique that we call a GMS GV, so a guaranteed maximum of speed graph viscosity. We construct a high-order scheme that may be that may not be invariant domain preserving using an entropy viscosity commutator. And then we combine these two solutions using convex limiting with correct bounds inferred from the lower the solution. Inferred from the low-order solutions. That is to say, we're going to use the bar state from the low-order solutions. All right, so I illustrate the method here with a continuous finite element, but actually the method works for finite differences, finite volume, Tg. It is essentially the same method. So we have here a sequence of shape-regular meshes, phi sub-area, the shape functions. And we define the p tau h to be the finite element space using these shape functions. Here I'm going to assume for simplicity that the finite element space is conforming in C0, but it is But it is just for simplicity for the formulas. And I'm going to make a D plus two copies of this finite element space. And I'm going to approximate the dependent variable in this bold space. And here I want to emphasize that this is a weakness of the method at the moment, because it means that all the variables composing the Composing the dependent variables, density, momentum, and total energy, are approximated at the same nodes, if you will. We're using the shape functions. So the method essentially is collocated. So here we exclude absolutely any staggering. So this is a current weakness. And that's that. So the method. So the method depends on essentially two parameters, the coefficient Cij that comes from the transport term and the lumped mass matrix. And these are the only two coefficients that comes into play. Well, of course, with also the consistent mass matrix. So now we define a time step, we approximate, we expand the velocity, the We expand the velocity, the state over the shape functions, and you see here that the coefficients here are collocated because everything is expanded with respect to this scalar, the scalar shape functions. And then we have seen this formula already a few times in the workshop, so I'm really sorry to go through that again. We essentially approximate the flux using the Lagrange interpolation of the flux. We use a forward order technique for the time stepping and we introduce a graph viscosity here with a coefficient Tij, which is a low-order viscosity. The key here is to estimate this viscosity. So Yes, I'm not to scheme the boring details. So essentially, the idea comes from Lags, Pertam, Tadmor, and Shu. The idea consists of constructing a Riemann problem. So we define a unit vector, Nij. By normalizing Cij, we consider two states, UI, UJ, and we consider this Riemann problem, which is one dimensional, with the left state being UI, the right state being UJ. The right state being uj. And then, so this is the left state, the right state. The solution is in the middle, and we have here we have a constant state, here we have a constant state, and in the middle it is complicated. But the idea is not to compute the solution to the Riemann problem, but to estimate To estimate an upper bound on the maximum wave speed. So the key is to estimate an upper bound. So to make sure that the solution is contained in this code. And estimating Lambda marks is often, most of the time, easier than solving the Riemann problem. Okay, so once we have Okay, so once we have this founder mass, we define the ij like so, and also we define the bar state that has been mentioned already a few times. And the key property of this bar state, of these two definitions here, is really this one: that you take any convex set in the admissible set, assume that ui and uj. That ui and uj are in this convex set, then the bar state is automatically there, and this is just a consequence of Landa Marks being an upper bound on the maximum wave speed. So, this is really the lemma that makes everything work. Of course, for the lemma to be true, lambda max must be a guaranteed upper bound, and then eventually you And then eventually you show that the lower solution is a convex combination of bar states. Okay. So we have this theorem under the CFL condition, then let be any convex set in A, the admissible set, if the uj's in the stencil of I are in B, then the low order stays in B. the low order stays in B. And this is true for every convex set. So this method essentially is a generalization of the this statement is a generalization of the maximum principle for any discretization and any mesh, any space dimensions, and actually for any hyperbolic systems. And so this method is a bulletproof method. This method cannot fail. Well, not exactly. It could fail, but by delta t actually delta T actually dying, delta E going to zero if your PDE is actually ill-posed. This is the only way that the method or if the method is not well programmed. Okay, now the higher the viscosity, the idea is to reduce the graph viscosity and to make it as small as possible and to be as close as possible to the galaxy solution. And here I want to To make a few comments here. Some words of caution. You don't want to be too greedy because actually reducing dij is pretty easy. You just have to take dij equals zero. Okay, it seems to be a very good idea. Unfortunately, this idea is very good only if you work with linear equations. So if your world is linear, then it's a good idea and move on. But if you want to solve non-linear concepts, But if you want to solve non-linear conservation equations, then this thing is not robust. And we have proved that we construct counter-examples in this paper with Boyer. So we have counter-examples that if you do dij equals zero, you converge to solutions that actually are weak solutions but not entropic. Again, linear stabilization has been mentioned a few times already. Again, it seems to be a good idea. Be a good idea if your world is linear, but it is not robust with entropy inequalities. And actually, a little bit like Volcker mentioned this morning, actually the linear stabilizer has to be removed close to shocks. And again, we have investigated this problem with Alexander in a paper in 2013. In a paper in 2013. And actually, the idea is that the linear stabilization should be removed where you have entropy production. Again, you can produce control examples that if you use a linear stabilization and you don't remove it, then you can converge to solutions that are weak but not entropy satisfying. And again, you can also. And again, you can also use IPR viscosities, and this is even worse unless you remove it close to shocks. So here it's a word of caution about the higher the viscosity and extra viscosities like inner stabilization and API viscosities. These are nice tools, but must be removed when you are close to regions where you have a production of entropy. Production of entropy. Okay, so there are many ways to construct higher-order viscosities. It could be a Lax Van Dorf or whatever smoothness indicator. Here I present a method that we use that we call entropy viscosity or commutator-based entropy viscosity. You define some entropy, entropy flux, and actually this pair could depend on the degrees of freedom. So this pair could be local. And the idea is to. And the idea is to just verify how well the chain rule is satisfied. So we construct this ratio here that is between 0 and 1. And then we multiply the lower the viscosity by the maximum of Ri and Rj, and that gives us the higher the viscosity. Viscosity. Okay, so the formal accuracy of the method now is only dictated by the approximation properties of the finite limit space. So this viscosity is potentially higher. So the third item now is the convex limiting. And we use, yes, convex limiting. And it's convex limiting, not in the sense that has been mentioned. Not in the sense that has been mentioned by Ennes two days ago. We use convex limiting because we limit functionals that are quasi-concave. And so we assume, so the strategy is as follows. We assume that the low order satisfies some disinequality, where psi is a quasi-concave functional. It could be density, internal energy, entropy, mechanical. Maginetic energy, whatever, anything that is quasi-concave. And we're going to limit the high order, we're going to transform the high-order update into something that satisfies this inequality. And here to do this program, you cannot use FCT-based technique or anything that is based on the coordinates of the variable because the box. because the box or the manifold generated by the zero isovalue of psi is not is not an hyperplane, is not an hyperplane in the phase space. So to do that, what do we do? So we define the local minimum. So we construct the bar set. So we construct the bar state, we construct the minimum of psi over the bar state, and we translate the psi by this minimum because we want to actually enforce a lower bound with zero as being the lower bound. And then by definition, actually, by the lower The lower solution satisfy this inequality. And so, what do we do? We follow the FCT technology. So, we construct the lower-order update with fluxes, the high-order update with fluxes. We construct the skew symmetric matrix that connects the low order update to the high order update. So, this is the usual FCT technique. Usual FCT technique and we are going to compute limiters. Okay, and this is where we use the convex limiting. So first of all, this program is actually feasible. It is feasible because the limiter L equal to zero is admissible, because the low-order updates satisfy this inequality. Okay, so the strategy is a divide and concur. Strategies that divide and conquer, so we actually work with the IJ pairs separately. So, essentially, we, and this is where the convex limiting strategy appears, is that the Aij is actually split through convex combinations like so. And then instead of doing the limiting with the IJ, we're going to do the limiting with pairs. With pairs, UI and Pij. Okay, and so we take the largest Lij that satisfies that. And again, the fact that Psi is quasi-concave is essentially important to prove that actually this problem is always well posed, that you have only one object that satisfies that. Because the zero-level set. The zero level set or upper level set of psi is convex. This is actually the property of psi being quasi-concave. All right, and then with this limiter, we prove that this holds, and then that's it. So the idea is we have the low order in the convex set. This quantity, the high-order update could be outside, and doing a line search, we put it back on the manifold. But here, you notice that the manifold is not an upper. You notice that the manifold is not an upper plane. Okay, some notes about the convex limiting. It is important to relax the bounds using estimation of the local curvature. So you have to estimate some second-order derivatives here. And that gives high-order infinity norm in space without sacrificing the CFL. So up to third, fourth order, etc. So, up to third, fourth order, et cetera, et cetera. So, in practice, what we do, we do the convex limiting on the density. And here, of course, you could do FCT because the density is positive density is just a linear constraint. But to do the limiting on the specific entropy, well, we do the convex limiting. And of course, And of course, here I'm talking about entropy, and a problem arises when you use a tabulated equation of state. And now, actually, we do that. And for a tabulated equation of state, there is no notion of entropy whatsoever. So, actually, we can construct a surrogate of that thing, and we do the limiting on that thing. And again, we have a paper with Boyan and Ben. with with Boyan and Bennett, we will prove that actually the internal energy is positive and the density is positive. But again, we don't limit on the internal energy, we limit on the surrogate of the entropy. All right, so this is the end of the algorithm for the hyperbolic step. So we have formally an operator SS1H that solves. S1H that solve the Euler system, and this operator is environment preserving. But here, the environment domain is the environment domain of the Euler equations. Now, we have to deal with this part, and this is the innovation here: how to actually solve that in a way that is compatible with what has been done before. So again, we use the same finite element space and we update the density so that the density doesn't move in time. Well, the update is easy. And then we have to update now the momentum. So we construct the bilinear form associated with the viscous term in the momentum equation. And here And here I construct the stiffness matrix associated with this bilinear form. And here we use a Kant Nicholson to step in time. So we update the velocity because remember that we update the velocity. The density is constant. So updating the momentum is equivalent to updating the velocity using Krant Nicholson. So we define the velocity at the midpoint. At the midpoint, and then we update the final time using just continuing the continuous extrapolation. Okay, and now we need to do the same thing with the internal energy. We construct the berliner form associated with the internal energy. This is the stiffness matrix associated with the balancer form. Here, I'm going to assume that beta ij is negative. Beta ij is negative, so this which is true up to the acute angle condition. This assumption is not really necessary, it can be removed by applying a local corrections to the thickness matrix using the usual LED technique by Roe, Jameson, etc. Although we are not exactly sure that by doing that this is consistent. By doing that, this is consistent on very historic pressure. Okay, so now we need to solve the internal energy. And for that, I construct an object here that is non-trivial. And this is the very, this is one part of the technique that is non-trivial. So this is the dissipate the production of energy due to the viscous dissipation. To the viscous dissipation. Okay, and I define it like so. So it is defined at the mid-step. And here, notice that this thing is not variational, actually. It is not variational. You are not going to able to define this object by just looking at the problem and making by using the Geracking approximation. Okay, anyway, so I define this object. Define this object and we update the internal energy using again quantitylson with this object and again solving the internal energy solving the for the total energy is equivalent to solving for the to the internal energy because the density is constant in time. And then we update the internal energy. Okay, so far so good, but actually caution. But actually, caution: here there is no guarantee of positivity of net energy. So, this scheme actually is not positivity preserving, and this is due to the Kant Nicholson time stepping. And just like Emmanuel was saying two days ago, we need to do a limiting in space and time. And this is essentially what we do here. So, if the minimum of the higher So if the minimum of the high-order update is positive, you move on, you don't do anything. Otherwise, we have to use the backward Euler and to construct a low-order internal energy and to use the FCT limiting. We can use FCT here because the internal energy is a scalar here. So I'm not going to going to skip the boring details because Because there is an important detail here. So we use the backward Euler for the lower update. But notice that here in the right-hand side, I use the same source as in the previous scheme. I use this source here. So we use this source. Of course, we can prove that due to the acute angle condition, condition this system satisfies a positivity you cannot prove a minimum principle here well yes you have a minimum principle which is this one but essentially this is a positivity of the internal energy all right and then we do the usual thing to to do the limiting we apply the FCT FCT with the mass lamping correlation here. So this is the correlation due to the mass lamping. And we use FCT and that's that. So the point that is crucial here is the definition of this K sub i and using this K sub i here in the right hand side of the lower order update. And this K sub i has this special form here. Here. So if you do that, then the scheme is positive on the density, positive on the energy, and is also conservative on the total energy. And this is the key here: that the scheme is conservative on the total energy, and this is due to this special choice here. Okay, so to summarize, we have an operator, a discrete operator for the update of the hyperbolic part. We have a discrete operator for the parabolic part. And we use now strong splitting. So we do an hyperbolic step with delta t over 2, a parabolic step with delta t, and we finish with an hyperbolic step with delta t over 2. Essentially, this is how this is programmed. This is a program. You construct an hyperbolic update. Using this hyperbolic update, you construct the parabolic update, and then you finish with the hyperbolic update. So the theorem is as follows: that provided you have a discrete space as defined as above, if UN is in a In B and B here is only positive density and positive internal energy. And if delta T is less than the maximum delta T allowed for the hyperbolic step, then the method is invariant end preserving. That is to say that U n plus one is in P. Okay, and so we have lost the minimum principle on the specific entropy. Minimum principle on the specific entropy, but the minimum principle on specific entropy is anyway enforced at every Euler step. Okay, and the algorithm is conservative. So we have a global conservation of mass and total energy. Okay, some numerical illustration. So we tested the scheme on an exact solution by On an exact solution by Baker 1922 in 1D, and here I construct a consolidated error estimator that combines the density, the momentum, and the total energy in the LQ norm, Q equal 1, 2, and infinity. So what do we observe? So this is a number of grid points, and we observe that we get second order in the L1 norm, in the L2 norm. Order in the L1 norm, in the L2 norm, and the N infinity norm. Okay, we can do the same thing in 2D. So this is essentially the same solution, but on the mesh on the domain that is 2D using P1 finite element on triangles, the meshes are non-uniform and the load and they are not nested. So this is the number of grid points for CFL04 and CFL09, just to show that the And CFL09, just to show that the results are independent of the CFL. And again, we get a second-order convergence in the L1 norm and the infinity norm. Okay, for small CFL and larger CFLs. Okay, now a second test that illustrates that it's easy to go wrong with this with Navier Stokes. So it's A benchmark proposed by Virginie Daru and Christian Tonneau in 2000. So it's a shock tube. So it is a box, a 2D box with a jar fragment at the middle. We have a left state and the right state. And we break the jar fragment here. So there will be a shock moving to the right. And as the shock moves, it creates a boundary. Creates a boundary layer at the bottom and at the top. Okay, but the shock moves and then creates this boundary layer, and everything moves nicely. Of course, we have the expansion and the contact discomfort moving to the left, but the left part is not important. So, what happens is that the shock reaches the right boundary and bounce back. And while it goes back, it is Goes back, it is going to interact very, very strongly with the viscous boundary layer that it created on the way in, and it is going to create a lambda shock. And here we have a lot of interactions going on. So, first of all, I want to show you what can go wrong if you use an algorithm without that is not certified. Okay, so these pictures have been taken from a paper by Have been taken from a paper by Elene Yee and short Cogrin. So these are the plots of the density with various schemes. So here this is the muscle scheme with 4000 by 2000 grid. So, okay, this is some, and of course they refine the grid, and this is what they get. Now they use another method with whatever 66 with a different. Whatever 66 with a different grid, and look at the solutions, they are fundamentally different. Now you use ACM whatever, and again, so here it seems that it's almost the same, but it's different with muscle. And then you use wino and you get something else. Okay. And the problem is that. And the problem is that if you have a method, you do not guarantee anything, you do just slope limiting whatsoever, you produce garbage. Now, this is what we believe is the correct answer. So, I show three plots. So, this is a density computed by Christian Tonneau and Virginie Daru using a method that they call OSMP7. So, this is the solution. So this is the solution computed with a P1 finite element with 860,000 grid points. And this is a solution computed with the algorithm that I just showed. So these two methods are the same, but here we're using DL2 with Q1 for an attainment with 128 million grid points. And visually we see that the solutions coincide. We can be more precise. We can be more precise. We can compute here the viscous stress coefficient at the bottom for various discretizations. So L1, L2, L3, L14, these are various mesh refinements. And this is the friction coefficients for all these refinements. And we observe that we converge. And we observe that we converge and the crosses, the crosses, I don't see the crosses, are from this OSMP7. So here we claim that, well, we claim, we think that we have the correct answer. Okay, so I will finish with some tests that we are doing now with Matthias with solving the flow around the OATA50. Around the OATA15 airfoil from the ONERA at MAC 0.73, at Reynolds 3 million, and angle 3.5. This MAC is and this angle are very special because the flow is unstable. We have the so-called buffeting effect. So the problem is solved in 3D. We're using 274 million grid points. The grid is heavily graded in the Heavily graded in the y direction. The smallest grid size is 2.1 micrometer and the largest in the X direction is 60 micrometer. The length of the foil is 30 centimeters. And there is no turbulent modeling whatsoever. So this is direct numerical simulations. Okay, so. Okay, so this is here a shearing plot of the density at some time. This is the pressure at some time. These computations at 2D here. Again, this is a 2D computation showing the grid refinement. So the grid is really, really fine. And this is the 3D computation. Here we show the Here we show the stereo plot of the density at various locations. Okay, and the result that is interesting is not this one, it is this one, the computation of the pressure. Of course, the pressure is unsteady because the flow is turbulent, but we average the pressure in time and in span-wise, and we compare. Wise, and we compare two experiments. So, the crosses here are the experiments, and the green line are the averages obtained from the computation, the 3D simulations. And here, again, I recall that there is no turbulence modeling. It's and the Reynolds is 3 million. Finally, some scaling tests to investigate how the parabolic step and the hypocrisy. The parabolic step and the epolic stage behave. So these are strong scaling tests that show that, so what is it here on the left, this is the Onira foil that shows that the Onira foil, actually, since the Reynolds number is very, very large, actually solving the parabolic problem is quite easy. And the problem is essentially dominated by the air parabolic step. But now when step but now when we solve the the uh the the 2d shock tube dimension actually the viscosity is actually is is is is quite large and this is the parabolic step oh hold on what am i saying here oh no no okay sorry i am sorry yeah it is again no no the power no sorry yeah the the the the renaissance number is three million but actually since we uh we refined in the in the book Since we we refined in the in the boundary layer, I'm sorry, actually, yes, we solve the boundary layer, so it is the parabolic step that is actually the most that takes the most time. Okay, but anyway, that we see that the curves are a linear over a large range of core numbers, so the scaling is quite well. Okay, anyway, current work, we are now working with D2, we develop a new DIL2, we develop a new version of DIL2 doing a CLD that is called Ryogene. And we're working on gray radiation and Euler Poisson and then the tabulated equation of state. And that's it. Thank you. All right. Thank you, Jean-Luc, for this really impressive talk. Okay, questions? All right, I see several. Dmitri? Jean-Luc, I have a question about splitting. If I think of projection methods for the compressible Navier-Stokes equations, then splitting the problem into an inviscid step and the viscous step equations. And the viscous step creates pressure boundary layers. Can this also happen for the compressible Navier-Stokes equations? You took me splitting for the incompressible Navier-Stokes, huh? Yes, but here it's the compressible, so it is fundamentally different. For the incompressible Navier Stokes, the boundary layer comes from the fact that the pressure is a Lagrange multiplier. Here, the pressure is not a Lagrange multiplier. Pressure is not electrons multiplied here. Okay, but maybe other variables can have boundary layers because you're using different boundary conditions at the hyperbolic step and at the parabolic step. So I will. Yes, yes. Okay, thanks. All right. Next one, Manuel. But just to come back on your question, I do not claim that operator splitting is the way to go. This is the first technique that. The first technique that we have found actually to solve the Navier Sukho equations, but now actually we have something else on the way. Oh, I see. That would solve exactly your question. Okay. Hey, yes. Hello, Janluk. Well, first of all, thanks for the talk. Really nice talk, really impressive results. So I just have a quick question about this schematic that you have, where you were showing the invariance. where you were showing the invariant set and then you showed that the low order solution is inside the set, the high order solution is outside the set. And then you mentioned that once you do the flux limiting, you basically send the solution to the manifold. So I just want to make sure that I understand this part correctly. So I mean I suppose that because of the fact that you symmetrize the limiters in order to have conservation of mass, you introduce some extra dissipation that is going to push the solution inside the set so that the solution Inside the set, so that the solution is not exactly in the manifold? Am I right here, or do you use something else to make sure that the solution is actually on the manifold? Inside only. Yeah, I suppose it's inside, right? It's inside the cell. Eventually, when you have done the limited everywhere, you're going to be inside, yes? Yeah, okay, yeah. I just wanted to make sure that that was the case. And that is not on the manifold, yes. It's not on the manifold, but eventually, eventually, the limiter is. Eventually, the limiter is going to be one almost everywhere. You know that if the limiter is not equal to one almost everywhere, you are low order. So, since we observe high order, it means that the limiter is inactive almost most of the time. Okay. Okay. Yeah. Thank you. All right. Any other questions? I can make a comment just to explain. Can make a comment just to explain the splitting, maybe. The way to think about the splitting is that you make an Euler step. What happens after the Euler step? Of course, the density stays between the bounds that are given by the equation of state. And then the specific entropy is has a minimum principle, but unfortunately the internal energy can decrease. It will not get negative, but it goes down. It will not get negative, but it goes down. And then you move to the parabolic step, where, of course, now you have a different domain. The domain is basically saying, just for the parabolic step, the internal energy has a minimum principle, but unfortunately now the entropy can go down. So in one step, one quantity can move down, and in the other step, the other one. And then after you combine, you end up with this smaller, well, not small. End up with this smaller, well, not smaller, actually, bigger set. It is more relaxed, the whole domain. And maybe this is a deficiency of the splitting, but this is the way it works. You have different domains for the different steps. You have one minimum principle on Euler, and you have a different minimum principle on the parabolic step. And that's the way I think about the splitting. And of course, after you combine the two steps. And of course, after you combine the two steps, maybe there are other things to do, but we don't know how to do it. That's the way we enforce it. That's all. Thank you. All right. Thank you again to the speakers, all the speakers this morning. We're going to take a lunch break now.