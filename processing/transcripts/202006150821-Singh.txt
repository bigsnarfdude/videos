Came on previously on Diablo, which is an integrated method for multi-omics data sets derived from bulk tissues. So this is my first attempt for single cell. So just a disclaimer. Okay, so this is the data sets that have previously been mentioned. We have the seek fish data, the single cell. The single-cell RNA-seq data set. Both of them are measured for the same set of genes here. This is 113 genes, a different number of cells in both of these data sets. In the single-cell RNA-seq, we have the cell-type labels for the various cells, and these are just a few labeled here. And for the seek fish, there are no cell-type labels. So there was a bunch. So there was a bunch of questions that were to ask for these benchmark data sets. One of them here is: can the two data sets be overlaid? And what is the minimum number of genes needed for integration? So the way that I sort of look at this question was really: is there a gene signature that can be used to predict the cell type labels? Cell type labels, and can this be done into an integrated fashion rather than applying a model building in one data set and applying it to the next data set? This is just a quick overview of what the paper did. They had the single-cell RNA-seq data set. So, this is your expression data, your cell type labels. They did some differential expression analysis to narrow down the number of variables. The number of variables and came up with a 43 gene classifier, and they applied it to the sake fish data set. And they essentially impeded your class labels for this data set. So my sort of goal was, can we improve this? So, again, as I mentioned, this is for me sort of like an imputation problem where you have a data set with no labels. A data set with no labels, and since they're measuring the same thing, is it possible to sort of combine them to sort of improve performance? And this technique, semi-supervised learning, really tries to make use of the unlabeled data in determining your decision boundaries. So this is an example where on your left you have a small labeled data set. So these are your two classes, and this is the decision boundary. The decision boundary, and then on the right, you have a larger data set where many of the points are unlabeled, and you can see that the decision boundary is changed, but it may be better generalizable to additional data sets where those data distributions do follow this particular nature. So I'm going to be using a wrapper method called self-training. Training. And essentially, the goal of this method here is to start with the labeled data. So you have a few cases that are labeled. You build a classifier and you apply it to the unlabeled data. And you're going to have class probabilities for each of these unlabeled data points. And you essentially retain the ones that are of high confidence, so based on some probabilities. So, based on some probability threshold, and you retrain your classifier. And then, this is what you get. And then you essentially repeat this process a given number of times when you're sort of confident on the majority of your class labels. So the only thing I switched here was I switched the classifier to GLMNet, which is a penalized regression method and it allows for simultaneous model fitting and variable selection. Model fitting and variable selection. And I used this, well, there's a library for semi-supervised classification methods. And I'm only, and it has a lot of different semi-supervised methods. And the one I'm looking at in particular is called self-training. Okay, so the first things first, before I integrate the data set, does the self-training actually work? Data set, does the self-training actually work? Right, you can send me like data to show that it does. But the first thing I wanted to do is just look at the RNA-seq data set here. And these are the number of cells in each of those cell-type classes. I removed one of the classes here just because of the few number of samples that I had, and that might run into issues with cross-validation of the hyperparameters, so I excluded that one. So I excluded that one. And I essentially just simulated, not simulated, I guess, I just deleted some of the class labels here just to simulate those unlabeled data sets. And this graph really is showing the output of that. So on the x-axis is the percentage of missing data that I'm generating. So this point here is when 10%. point here is when 10% of the data set is missing and here is when 70% of the data in the RNA-seq data is missing and this is 70% of each individual class otherwise I would get some classes that are totally gone and that would defeat the purpose and I would apply Elastic Net classifier to the label data and then develop the model and then apply it to the unlabeled data and see what To the unlabeled data and see what my error rate was. And then I apply the semi-supervised version. And then once I compute all the class labels, I compare it to what the true values is. And as you can see, as the amount of missing data increases, the error rate of the supervised method is the balanced error rate, which is the weighted error rate of the individual classes, is a lot lower. And again, this is just Lore. And again, this is just one iteration because each of iterations of the semi-supervised version has a lot of iterations. So obviously I need to repeat this and have error bars around these. So before I integrate it, one thing that has already been mentioned is how do you actually scale the units of these two data sets. So here I'm sort of showing the amalgamated The amalgamated data matrix here, and this is just the PCA plot. And already you can see that the data distribution of the RNA-seq is quite different from the seek fish data here, although they're centered at zero. And the approach that was used in the paper was called bias-corrected quantile normalized data. And essentially, you can see here that the quantiles of the two data sets here are like basically the same. Here are like basically the same, and essentially, they're trying to make the two data sets similar. But you can see that there's still some scaling issue that can be corrected, and it's really coming from the overall data distributions, right? So normally when you run sort of your standardizations, you're scaling at the variable level, but you can also go ahead and scale at the data level. And scale at the data level, and some of the current approaches out there actually do that. So, how can we sort of mitigate this problem? The data set that I had to work with was already normalized, so I need to go back and work with the unnormalized data, but I haven't done this for this exercise. But so I'm just going to list a couple approaches that can be used. The first is to treat this as a The first is to treat this as a batch correction problem, and then back here you would correct for that batch problem and then move on to the classification methods. Another approach that's used for methods that look at multi-group variables is called multi-factor analysis. And what they do is that they'll do initial standardization, but once you standardize your data, But once you standardize your data sets, they'll standardize each matrix by its corresponding first eigenvalue. And the reason for that is to really scale at the component level. And then other approaches, such as extensions of canonical correlation analysis or Diablo, do a further standardization from the usual one, which is the standardization. The standardizations by the number of variables. So they're basically saying once you standardize your initial data sets, also standardize by how large your data set is. So here I'm color coding by the cell types here, right? So the big ellipses that you see here are the RNA-seq, the single-cell RNA-seq. And that's what my classifier is based on. And then I'm going to be predicting on. Is based on, and then I'm going to be predicting on this little blob here. So you can already see that since the data distributions are quite dissimilar, my semi-supervised method may not actually do that well. So this is the approach that I used for estimating the performance. And essentially, what I did was I took the label data and just split that. And then one of the And then one of the splits, I combined it with the unlabeled data set. And then for this matrix here, I applied the semi-supervised approach. So fitting a GLM net on the label data, estimating the labels of the unlabeled data sets, and then retraining that many times, and then applying the final model to that labeled data set, and then determining the error. set and then determine the error rate. So these are the results. So here you can see elastic net versus semi-supervised elastic net for the individual class cell type classes. This is the air rate and this is the balanced air rate. And as you can see that the super semi-supervised method does not do as better of a job as the better of a job as the usual unsupervised method, sorry, the non-semi-supervised method. And this is something that I want to try after I look at the various data normalization techniques that I mentioned. This plot here called an upset plot is showing you the overlap between pairwise panels. So this is the FSVM 43 gene panel that was reported in the paper. 43 gene panel that was reported in the paper. And then these are the other two panels that I identified. And you can see that the intersection across all three is at 31 genes, and we have some unique genes as well. So with that, I'm going to just close with some summary points. The first is that semi-supervised method improves upon the classification performance, but it has its caveats, right? Performance, but it has its caveats, right? So the data distribution of the unlabeled data, if it's very different from what you're training on, then your performance will be lower. However, if you did have seek fish data or additional unlabeled data sets that did have labels, it may actually generalize better than the non-semi-supervised method. The limitations of the present study, I would say, are. Limitations of the present study, I would say, are the different data distributions. And as I mentioned, trying different normalization strategies to see if the performance improves. Some future directions. The first one is to use observational weights. So as previously mentioned, there's a big class imbalance and whether or not we can sort of weigh the different observations based on the inverse of the Observations based on the inverse of the class size and see how that affects performance. I mentioned the data strategies already. And lastly, a word of caution for the current implementation that I have, don't use it with R 4.0 or QLM 4.0 at the moment, just because there's some changes that have broken my code. So just a word of caution there. And with that, I want to thank my supervisors, Dr. Bruce. Uh, supervisors, uh, Dr. Bruce McManus is my postdoc supervisor, Scott Tibbett was my PhD supervisor, and um, Kiman Lakau has really um brought me into this multi-omics space, and without her, I wouldn't be analyzing such data sets so thank you. Wonderful, thank you so much. All right, great talk. Uh, so GC has a question. He was wondering how platform-specific bias might impact performance. Might impact performance of the classifier? Yeah, so the platform-specific bias really goes back to the correction for batch effects and whether you can identify the platform-specific biases and correct for them. Like in the bulk tissue genomic space, batch effects are quite common, and there's a lot of Common, and there's like a lot of different methods out there that allow you to correct for them. So, this, like with any other technology, can be impacted by that, right? So, even when I guess you're running the samples, who's running it, like the date it was run, are all the classes of one cell type run on a separate day? And so, these kinds of technical artifacts will influence your resulting expression. Resulting expression values. So I think these types of artifacts should be corrected for before applying any classification method. So can I just add a little bit of clarification about that? So this is a little bit different than this batching factor correction, right? Because the label data all come from one batch. And how do you make the inference? The inference to the other batch and knowing that the performance as well. I'm not sure if I'm clear or not. By batch, you mean the technology, right? So we have labeled only for one technology. And the difference between the other technology, the data in that technology, could be driven by badge and could be also be driven by And could it also be driven by some of the cell type differences basically that you don't know? There are two unknowns together. Right, so the way that I look at that is if you look at it from a clearly classification perspective, if your data sets, your training and tests are very different, then the result is going to be very poor, right? So in the deep learning space, you have this test data set, and that's so. This test data set, and that's sort of like your data set that you want to optimize to, right? So if that data set is very different from the training, then it won't generalize to other new data sets that you find. So I think, obviously, I don't know what the real answer is here, but just comparing the approach that was used in the paper versus trying to Versus trying to make the data distributions of the two different data sets similar and then trying to estimate the performance. I think that that may sort of answer the question they're asking. Great. And I think there's sort of some follow-up questions is like Alex is asking, how different would the output be if you correct for each variable independent or when you take an Variable independent, or when you take into account the whole data set? Right, so I think the correct for each variable independently is really what the quantile normalization. I'm not sure if it was done like on the sample basis, but you can see that the like the data literally is the same because the way that this normalization works is that the quantiles of the data become the same, right? But as I showed in the PCA plot, that necessarily. As I showed in the PCA plot, that necessarily does not mean that the latent variables of the two data sets have the same scaling factor. So you will get different results if you do one or the others. And these are just methods that they have been being used for quite some time now. These are from, I think, France. And again, I haven't tried this on the data set, so I'm not sure how the results will. I'm not sure how the results will change, but I think based on just my experience with this machine learning, if your data distributions are very different between train and test, you're going to get very different answers. Great. I think we should move on to Josh, but before we do, I just want to bring up David's question, the baby's question, because I think it's really great both for your talk and for the whole session that probably you could help as a brainstorm. I'm right. As a brainstorm, I'm right. But one of the questions, what he's asking is: if we're treating the labels in one of the data sets, said the single-cell RNA-seq as the gold standards, since they're also data-driven, I think this gets a benchmarking too. Is there any way to include the uncertainty in the labels in the training? And I think that's a really important question for all of us to think about, you know, with the lack of gold standards. So I'm unfortunately not going to give you time to handle. Not going to give you time to answer, but I think that that would be great for us all to keep going on Slack as a discussion point. Okay, great. So, without further ado, thank you so much, Emri. And now