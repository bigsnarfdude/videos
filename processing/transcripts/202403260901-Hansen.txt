And I'll be talking across a few different papers, but but maybe a couple common themes will go along with it. Okay, um, and this is a topic I've been thinking about for a while, and it's showed up in multiple papers, and so today's talk is trying to crystallize some of the basic ideas and kind of where things stand conceptually. So, I like to think about it. Um, one of the applications that's important to us is potential policy problems. For instance, think about climate change. Policy problems. For instance, think about climate change. This is a case where historical data is of limited value because we're thinking about pushing economies into places they haven't actually visited before. Subjective uncertainties are important, potential model misspecifications important. And so here I'm quoting from a recent paper about the challenges here. There's complex risks with climate change. They can't be currently quantified, but they should still play a role in evaluations and decision making. Evaluations and decision making. The paper makes a statement. It doesn't really show you how to do it, but it makes a statement that I'm sympathetic with respect to. And by quantified, I think I mean here fully quantified. I don't want to throw out any prior information, but I don't want to presume the quantification is precise. So I just noticed something. I've only got half my slides here. Can I take a second and do a quick repair? Yeah, of course. Yeah, I'm sorry about this. This will take me just a minute. No problem, no. Oh man, how did I really botch this? Sorry. I really bought it. Sorry. Yes. This is embarrassing. Hadn't happened to me before, but anyway. Let me just see here. Maybe you can still hop on a plan and let me just check something. I think I'm okay. Just a second. Let me just do Antani, can you get me back to my slides? Slides over. So just do control until. Oh, no, you have to click on it. It has to like acknowledge that you're just a simple click. Whoops. Yeah, but I need to make it full screen. Click on this. Let me just make sure I got more than 16 slides here, though. No, it's not full screen. Yep. View full. Could people close their eyes for a minute to see if... Yeah, I've got more slides than it says. I'm sorry. I got fooled by the. Okay, so apologize for the delay. So Hayek wrote this very interesting. Wrote this very interesting discussion about limits of knowledge. This was his so-called Nobel Address. And he's pretty tough on econometrics. I don't agree with everything in here. He said kind of econometrics hasn't really taught us much over the decades. So I don't agree with that. But there's a quote here that kind of resonates with me. And I think it's an important challenge if we want to, and why I think it's important to think about these types of issues and policymaking problems. Issues and policymaking problems. That even if true scientists should recognize the limits of studying human behavior, as long as the public has expectations, there'll be people who pretend or believe that they can do more to meet popular demand than what is really in their power. And the idea is that, you know, for scientific communication, there's this kind of concern that if you acknowledge too much uncertainty, you'll just be dismissed and ignored and everything. And so I think it's really important to instead integrate uncertainty formally into the decision-making process to. Decision-making process to make it clear that you don't just get do-nothing or degenerate outcomes out of it, but to think how to do this in sensible ways that kind of resonate. So in our work related to climate change, I'm talking about that example a little bit later. There's important uncertainty trade-offs show up in our policy problem. And I'm spending more talking a little bit about the first bullet point here than the second one, but. Bullet point here than the second one, but and you know, as economists, we always like trade-offs. The trade-offs here are kind of um, you make best guesses when you use econometric model, you can make best guesses, and you can also use it to assess potentially bad outcomes. And how do we trade these things off? Models can be used for both purposes. Both are interesting by themselves, but in policymaking, we have to think about trade-offs here between the two of these. Because if we focus only on the best out guesses, then they're not going to. You know, best out, you know, guesses, and they're not quite right, so something bad could happen. If you only focus on really bad outcomes, you may not want to get up in the morning and just throw up your hands. So it said that the trade-off is really central. The other thing that's important in addressing policy questions that I'll say less about here, but we said in some of our other work is do we act now or do we wait till we learn more? So, in the climate change context, this is again quite an important question because it may be very, very costly to wait. And so, by the time you learn stuff, To wait, and so by the time you learn stuff, it may have done so much irreversible damage that it's very hard to respond down the road. And so, it's trade-off, even down the road, we may well know more about climate change. But this trade-off is really an important one in decision-making. So this line of research kind of began with positing some interesting representations and now has gone backwards to. And now has gone backwards to produce to think about axiomatic foundations for it. And the stuff I'm talking about today are some recent work I've been doing that's going on after the axiomatic foundations. It's interesting that there's this Savage in his original elegant axiomatic paper. He has this, that there's this kind of precursor of it that was kind of an unpublished working paper version in which he makes this comment: once you've got the representations, maybe you don't need the axioms anymore. Need the axioms anymore. It's kind of an interesting observation. Since, in some sense, we've been beginning with rep, we were thinking about representations before the deep axiomatic defenses. But I think going back and forth can be very useful. And so this is about that process. So let me, this model misspecification issue, of course, I'll appeal to statistical authorities here. Box is quoted, George Box Smith. George Box makes quotes. There's multiple versions of quotes like this by George Box that models are wrong, but can still be useful. What you got to figure out is whether they're importantly wrong. So here he makes a statement about it's inappropriate to be concerned about mice when there's tigers abroad. So that model misspecification could be a big deal. And it's important to think about it. And then Cox points out: lots of people say models are misspecified, but that alone doesn't get us very far. Get us very far. A model we know is a simplification, it's an idealization, it's not a very deep insight that is misspecified in some sense. So, the construction of these kind of representations, idealized as they may be, are still an important aspect of doing quantitative research, addressing substantive problems. And so how do we use substantively interesting models that we think are misspecified is this interesting challenge. Is this interesting challenge? So, what I'm going to be talking about next is that's based on material across three papers. Of course, I'm not going to develop any of these in great detail, but I'm going to try to distill some of the ideas from them. One is structured ambiguity and model specification, a JET paper that Serge and I recently published. Another one is based on a lecture I gave at a conference last summer that just got published in the Journal of Applied Econometrics. Just got published in the Journal of Applied Econometrics: Risk, Ambiguity, and Misspecification. And that's a paper with Sargent, where we kind of build connections to not only statistical decision theory, but robust control and rust control theory, as well as abstract decision theory and economics. And finally, a work that's that earlier versions have been around, but we've just done, we're about to complete a major revision of it is making decisions under model misspecification with. With Correa Biglio, Mashaboni, and Marinacci, my Italian friends, who are all expert decision theorists. So the aim here is we want to allow for a broad perspective of uncertainty and one that's broader than is typical in decision theory. We want to have three components: risk, ambiguity, and misspecification. And a lot of the existing discussions. The existing discussions of uncertainty kind of capture two of the three, but not all three. So let me just add some specificity to these words. One is the term risk. So, you know, risk is, I think of that as known outcome, unknown outcomes with known probabilities. So in macroeconomics, you write down a model under rational expectations, economic agents are only confronting risk. They know probabilities, but not outcomes. You know, coin flips and dice throwing and games of chance are typical. Games of chance are typical, or certainly other examples. Ambiguity is something that we're very familiar with from statistics. We have alternative probability models, maybe indexed by some unknown parameter or something, and how much weight do we assign to each. The so-called Bayesian prior addresses that, but then how confident might we be in those prior specifications? The third one is: if I take each of the given models. If I take each of the given models, I expect it might give flawed probabilistic predictions. And I may well think that none of the models in my parameterized family or my set of models of substantive interest is correctly specified. And so in many respects, a third one is probably the trickiest one to address in ways that are fully coherent, but it may be also, you know, in light of the comments from Box and Cox and others, it might well be the most important. And again, what's important for the research applications that I care about. For the research applications that I care about, these have to be dynamic and recursive. The dynamic is important because a lot of policy problems are explicitly dynamic. Recursive makes things conceptually simpler along some dimensions and certainly computationally simpler along some dimensions as well. So the approach I want to talk about for the initial part of this talk is I'm going to draw on and develop modifications of savage style axiomatic formulations from decision theory. From decision theory, but I want to push things beyond savage. I want to go both beyond risk and I also want to be in a contact with applied economics and with applied challenges and economics and elsewhere. And again, a key part of this, you know, I talked about ambiguity versus misspecification. From a statistical terms, you can kind of think about this as I want to distinguish between misspecifications of likelihoods and misspecifications of priors. Now, in our applied work, these type of characterizations are opening the door to what I think are better ways of conducting uncertainty quantification that are important for both private sector planning and kind of government policy assessment. So if you just do uncertainty quantification alone, you're kind of quantifying where uncertainties are big or small, but you're not going to, but the decision problem makes you. But you're not going to, but the decision problem makes you think hard about what uncertainties do you really care about. Maybe they're big, but they're largely irrelevant to the question at hand. And so that's what that's the piece that I think is important to incorporate into uncertainty quantification. So the targets here, computationally tractable methods. I won't be talking so much about the computational tractability, but that certainly has to be important for applications. And then we want to do two things. We want to assess the impact of uncertainty on proof. Assess the impact of uncertainty on prudent decision or policy outcomes, but then we want to also isolate the forms of uncertainty that are most consequential to these outcomes. And so in some sense, these methods are meant to be distributionally robust, but distributionally robust along two dimensions, the likelihood type dimension and the prior type dimension. And then as I'm looking at the distributional neurobust issue, once I have this decision problem on the table, I can start. Decision problem on the table, I can start figuring out which where the uncertainties really matter. So, inputs, of course, probability and statistics, we need to limit or at least bound the amount of uncertainty entertained, but we also have to have aversion. So, we know that in kind of risk problems, we have to specify risk aversion. For these problems, this dislike of uncertainty about probabilities over future events comes into play. And so that So that type of aversion becomes an issue. And here there's an interesting question because, of course, for a policymaking problem, it's not my job as an economist to tell policymakers or the public how averse they ought to be to uncertainty. The thing we can do, though, is use methods like this and trace through the consequences of various different aversions and do sensitivity analysis. But the sensitivity analyses are now in a very low-dimensional way. Very low-dimensional way because they're over kind of how averse the decision makers are to various forms of uncertainty. So, here's the Anscon-Allman framework, which has been used a lot in decision theory and economics, draws this distinction between horse races and lottery uncertainty, where the lottery uncertainty is the risk component, the horse race is the ambiguity component. I can put probabilities over the lottery, but the horse race's subjectivity has to come into play in some form or another. Has to come into play in some form or another. So, Anskanauman, their original paper, which is a very elegant paper, was really just going after a different axiomatic characterization or a way to derive the savage representation result. But it's been used a lot in decision theory under uncertainty, pushing harder on this distinction between lotteries and horse races. So, roughly speaking, we're going to define preferences over acts. These acts are going to map so-called states into probability. Are going to map so-called states into probabilities over outcomes, which you might care about. Jenskenalman referred to horse race versus lottery for motivation. The probability distributions over outcomes, we can think of those as lotteries, if you like. And the probability over states, you can think of as the analog to a horse race. Today, I'm going to be focusing on a static setting. I don't want you to think that's enormously constraining for the following reason. If I think about If I think about a dynamic decision problem under commitment, I can always formulate that with as a static problem, a date zero problem where I'm letting things like decision rules be contingent on stuff that happens in the future. I'm solving it from an ex-ante perspective as a type of commitment problem, but can still allow these decision rules to depend on information as it comes down the road, and I can still allow for learning in that sense. Now, the more recursive Now, the more recursive dynamic stuff, there's very important issues there. The first paper I talked about with Sargent and Jett really gets into some of those, and there's a big literature on the challenges there, which I'm going to be putting to the side for today's talk. So there's two approaches which have been developed separately in the two papers. And so the first one is in the Journal of Applied Econometrics paper. And this was sorry. Sargent and I had done this earlier work on model by specification, and I got picked up by decision theorists, and there are multiple papers that use our work in their abstract and econometric to motivate their representations. And we went back and read that work, and we were not completely convinced that it was addressing the problem that we had in mind. And so it was addressed, which we thought it best to think about as addressing a related problem, but not. Related problem, but not model misspecification, but more like prior misspecification. So, to make this point, in a static setup, we take the state now in that ANSCA-Oman framework to be the parameter vector. It's going to index alternative models. It can be discrete, continuous, whatever. And each statistical model is going to induce a probability distribution over outcomes. We can think of that as giving rise to a so-called Anscombe-Alma lottery, if you like. Anscombe Alma lottery, if you like. And a probability distribution I put over states becomes a prior. Okay, and then misspecification then becomes issues about that probability distribution, which you put over a prior. So I can just throw some formalism on the table here. I'm going to call L a likelihood function. W is a bunch of stuff over future events. I'm solving this problem. Over future events. I'm solving this problem X ante, so I could actually include data that gets observed in the future. And that's going to be conditioned on a theta. Now, I actually want something a little bit more general than a lottery here because I'm allowing for states to be unobserved. I'm allowing for not all the omegas to be observable and stuff like that. But anyway, let me just think about this for the moment. L is a likelihood. It's parameterized by theta. The way we took up the parameterization of theta, we give it some type of common measure here. Give it some type of common measure here, d tau is zero, and then and then we get all these, yeah, then we can construct these different implied distributions depending upon what data is. Now, for this robustness analysis, you might put a baseline prior over the parameters, but for now, we can just think about conditioning on theta for the moment. But then there's going to be a rule that's going to map W's into objects I have preferences over. Objects I have preferences over. So we can think about theta here as the outcome of the horse race, and we can think about this gamma of theta conditioned on theta, this gamma of w conditioned on theta as inducing a lottery. Because so far, once I know theta, I know everything. Now, it's going to be important here going forward as I extend this, the parameter space can be infinite dimensional. It doesn't have to be finite dimension. Infinite dimensional. It doesn't have to be finite dimensional. When I do the prior robustness analysis, though, I'm going to imagine maintaining absolute continuity across the different infinite dimensional spaces. And we know mathematically that's a very, that that putting proper priors on infinite dimensional spaces is very informative along some lines. And that maintaining that absolute continuity will kind of keep that informativeness. Then, typically, in a decision problem, these gammas of all prize rules that are going to be restricted. Prize rules that are going to be restricted to a set of functions. So think of D as indexing different actions, and then I can get these different gamma w's depending on which D that I might have. The ambiguity aversion in the earlier decision theory then becomes tied to prior uncertainty with links to robust Bayesian methods. The misspecification under this interpretation refers to misspecification. Interpretation refers to misspecification of a baseline prior. So, if you think of this as a way to reinterpret some of these earlier papers in decision theory, we talked about misspecification as really misspecification of a baseline prior. Now, in that same paper I mentioned, the Journal Applied Econometrics paper, we also build in similarities and differences to machine learning methods, specifically to PAC methods, probably almost. PAC methods, probably almost correct Bayesian methods. There's a lot of mathematical similarities, but there's important conceptual differences, which I time constraints I won't delve into right now. Now, under this setting, one can use ANSCOM-Ammond in different ways. Under this use of ANSCON-Amman, likelihood misspecification is not included. Because if I want to think about once I know theta, if I get this, that induces a probability, then there's then. then there's then then that becomes the um that becomes a lottery and there's no scope for misspecification anymore so in order to in order to proceed with this setup that that's been commonly used that that's been commonly and it's a mathematical setup that's been commonly used in decision theory one would have to extend it and right and we're exploring ways of doing this in future work to allow for misspecified lotteries okay because the likelihood function this likelihood function is you know for given things This likelihood functions for a given theta, for a given value of theta, is telling us what the induced probabilities are over the outcomes that interest us. So there's ways to extend this, but you have to push yourself outside the Anscombe-Alman framework in this way by entertaining a counterpart to a misspecified lottery. So there's approach two, and this I've done with my Italian colleagues. My Acadian colleagues. This one here is going to flip it a little. It's going to flip the use of Anskan Almond in quite a different way. It's going to build off some language that Sarge and I put in our JET paper with distinctions between so-called structured models and unstructured models. So by structured model, I don't mean structured models. So, by structured model, I don't mean structural in a connection sense necessarily. I mean a model that has a priori justification in some form or another. It could be a reduced form specification, but with a motivated parameterization or the like. And these are the ones that have substantive interpretations. I'm going to call those structured models. Now, at the same time, I want to allow the structured models to be misspecified. And the way that I'm going to do that, just a minute. The way that I'm going to do that is, I'm going to use those unstructured models as to entertain model misspecification. So the unstructured models are going to push me outside the set of structured models. The structured models are ones I substantially motivate. The unstructured ones are going to push me into a bigger place. Now, in this approach, we're going to have a similar starting point to what's happened in decision theory analyses. What's happened in decision theory analyses of misspecification? States are going to be of direct interest to the model builder. Uncertainty aversion is now going to be represented as so-called variational preferences defined over unstructured probability over unstructured probability models. So here we can think about there being a single theta. There's a single model here, and we're allowing it to be misspecified. And the unstructured models, unstructured alternatives give me a way to think about that misspecification. And then I want to look at. And then I want to look at look, you know, I'll look over a set of a whole set of unstructured models. Now, variational preferences are ones that basically involve penalization. So, as you do this search over these unstructured models, your hands have to be tied or else you get uninteresting answers. In earlier decision theory, Gilboa-Schmeiler, that type of calculation was all done within a constraint set and the variational process. Set and the variational preferences kind of move you beyond constraint sets are special cases, but they allow for more general forms of penalization and the like. And for a lot of applications, that generalization would be quite important. Okay, so now the uncertainty aversion is, yeah, this use of Ansconamen now delivers on model mint specification, but it doesn't, but it, but it basically conditions on a model. And there's some, you know, and these variational preferences, you do some type of minimization, you do a minimization. Some type of minimization, you do a minimization over the unstructured model subject to penalization. So, so I think of this as confronting likelihood misspecification conditioned on a parameter. So, for this setup, too, we have to push beyond the usual on-ANSCOM-Almond framework. So, the way we do it is we add a set of so-called structured models to the ANSCOM-Almond framework. There's earlier literature that was, I think, motivated somewhat differently, but Motivated somewhat differently, but by Gilboa and co-authors back in 2010, they define these two different preferences. One set of preferences are so-called mental preferences. And these mental preferences for us are going to induce a partial ordering by a decision maker, have a unanimous ranking over all the possible structured models. So I've got a set of structured models on the table. I'm going to ask that, I'm going to say that. I'm going to ask that. I'm going to say that I'm going to rank different types of acts by partially by saying, see whether one actors prefer the other overall possible structured models. So this is familiar from treatments in statistical decision theory on admissibility. Admissibilities based on a partial ordering. Now, the second one are the behavioral preferences that are complete. At the end of the day, we have to make a decision. So the behavioral preferences. Make a decision, so the behavioral preferences have to go beyond the mental preferences to actually kind of complete things. Okay, so now what we now the way that we're extending this type of this type of apparatus is we're going to incorporate model misspecification concerns, model by model. Then we're going to engage in this look over these alternative structured models. And the axiomatic inputs in this paper provide links between the two preferences in the presence of model specification. In the presence of model misspecification, and to kind of entertain various forms of model ambiguity. So, we want the behavioral preferences in some sense to be consistent with the mental preferences, and then some other properties that seem of interest in order to address formally this issue of kind of model uncertainty in conjunction with model mispecification. So, we lay out an axiomatic framework that's kind of designed to do that. Again, you have to push beyond the Anscombe almond. Again, you have to push beyond the ANSCOM-Amman framework. So, in the first case, ANSCON-AMAN handles prior uncertainty, you know, prior ambiguity. The second case, it handles kind of a model misspecification. And now to put the two, we got to push beyond Ansconum. We have to push beyond Ansconum. Okay, so let me just put down some representations that we can start motivating by either one of these type of setups. Which by either one of these types of setups here. So, this gamma is the gamma over w is telling me my kind of the outcome, the outcome of interest as a function of these w's. And as I say, gamma can actually be parameterized by different decisions. I've got this L function, the L, it's a counterpart to a likelihood, although it could be over objects that we don't have data on, shocks and macros. on shocks and macro models and the like, L of W given theta. And then I'm going to integrate that over this detau. So that's the that I think of about as a risk assessment. And then the subjective uncertainty, then subjective probabilities come into play in the prior, in the baseline prior here. And that's D Pi zero. So this is what Anscombe Alma and Savage were basically after in their analyses is to get representations of this type. To get representations of this type, the expected utility representations extended to subjective probabilities. Now, you might say savage axioms solve the problem. So why are we even talking going further? Again, I have to go back to my friend Stephen Stigler for interesting historical perspectives. Savage and Popper had this exchange, and Savage says the following: if I knew any good way. And Savage says the following: If I knew any good way to make a mathematical model of these phenomena, in this case of vagueness and indecision, I would adopt it, but I find despair of finding one. The consequences of vagueness is that they were unable to elicit precise probability by self-interrogation in some situations, but not others. So I'm not saying Savage would live, would endorse what we're doing here, but the question seems to me like he didn't view his axioms as fully solving everything. Fully solving everything. There's some of these ambiguity questions were left on the table. So, in our actual calculations, we often use statistical divergences. I'm going to give the representations in terms of these divergences. Of course, there's other type of distances which we can explore on REM as well. But I'm just going to develop a notation here in the context of so-called B-diversion. Notation here in the context of so-called D-divergences. But yeah, there's nothing that there's aspects of this in which you could extend a Vostracine or to other types of distances. Now, in my own view, if we're thinking about likelihood misspecification, kind of preserving some form of absolute continuity makes sense. But on the other hand, if you look at overclasses of models, then it may be important to push further. So, you know, standard approach to phi divergence is you write down this convex function, you make it zero. You make it zero, you make a one is evaluate a one is equal to zero. You can normalize the second derivative if you like to one. And then you can use this to explore prior ambiguity. So priors, I'm going to make absolutely continuous with respect to a baseline prior. I represent these by these n's. These n's are relative, relative probabilities. So they all integrate to one against d pi naught. And I'll call this collection script n. And I'll call this collection script n. Now, the divergence then, for each n then, I certainly know that this phi sub p for these priors here, evaluated n integrated d pi naught has to be non-negative. That's the usual kind of Jensen's inequality argument with phi divergences. And then I can use this as a way to penalize decision making. And this would be an example. And this would be an example of these variational preferences that were kind of originally an axiomatic treatment of these came from Masheroni, Marinac, and Rustakini. And then the representation takes the following form. I mentioned penalization here. So in the case of fee divergences, I compute the fee divergence. I weight it by a penalty parameter, C sub P, and then I minimize the kind of expected utilities subject to this penalization. Subject to this penalization. Maximum utility function of Gilboa-Schmeiler, then, what I would do instead would just announce a set of possible priors in N naught. And then I would just do the minimization over N naught. So the Gabosch-Meither stuff came first. Then came the variational preferences. The variational preferences kind of nested the Gabosch. Meither ones, the Gabosch-Meither ones are the ones that are kind of even more familiar to min-max theory in a lot of settings. Theory in a lot of settings. But as I say, I think the penalization flexibility is an important one. Now, how about instead I want to do misspecification concerns? So here's what I'm going to do is I'm going to take this L, this L density, this L function, and I'm going to multiply by M. And M can depend on theta. This could give me a way to entertain a misspecified likelihood. Hey, I have to restrict the M so that they integrate to one. The M's so that they integrate to one. And then I don't, and then I get the whole set of all such M's. Now, for a given theta, for a given structured model, if you like, I can solve the following problem. I'm going to minimize over these M's. Again, I'm going to have a penalization. I'm going to use Phi sub M for the penalization here, and then I'm going to integrate this. So, this is the first one. Is the first one gets at prior misspecification, the second one gets at model misspecification. Okay, and the first set of axioms that are the first application of Anscon Almond kind of delivered a counterpart to the first, the second one to the counterpart to the second, that kind of delivered a justification for the second. Now, the key thing here is when we're looking over model misspecifications, I'm not going to impose a prior over the set of all M's. I'm going to be looking over much bigger spaces. So it's when these influences. Bigger spaces. So, so in these infinite-dimensional spaces, priors can be, you know, are necessarily informative. And I'm going to, for misspecification, I'm going to look even bigger. Yes. The CM and the PIM there, they don't even have an M, right? M is just a name. It's not the M that follows it. When you take a minimum. Yeah, I'm sorry. I'm having a little bit of a hard time hearing this. Could you just repeat that statement or question? We're going to take a minimum over a small M. Take a minimum over small m, you have these are two functions or two numbers, one number and one function. They don't depend on m, right? They're the m is just a name. So m is not the variable. No, no, m is it's i'm gonna announce a uh i'm gonna announce a fee sub m and for feed divergences, I'll just announce a fee subm and I'll just announce a fee sub p. They could be the same fee, they could be a different fee. Right, but it don't depend on the small m that is the variable in the mean condition. It's gonna depend on fee for sure. It's going to depend on fee for sure. I'm not sure what small f is. It's going to depend on the choice of fee, absolutely. Correct. The fee subm, that could use a different sub, a letter. That doesn't hit hard as a minimization. The minimization is what happened. Let's see, I guess I'm still having a little bit of trouble. In the display, there is a psi m and there is a phi m, and those subs. Oh, I'm sorry. That's bad notation. The fee sub M is, I'm using a common one for all M. I'm sorry. And the size of M is common for all M. They're not going to depend on M. I'm sorry. The M was there to just remind me had to do with misspecification and it's not dependence on M. And the same with C. So I'm going to take a fixed convex function phi, and I just called it V sub M, which is, I guess, exposed to mistake, and its corresponding penalty parameter. Corresponding penalty parameter. But yeah, they're going to be invariant. They're going to vary over the different M's, but I'm not going to vary phi sub m or psi sub m. Yeah, thank you for the clarification. I agree the notation is not great in that regard. Okay, so the key thing is we're not going to impose a prior, we're going to do this search over a whole bunch of probabilities, but we're going to, since it's misspecification, we're not going to presume we can resolve it with a prior or a family of priors. They're absolutely continuous. Family of priors are absolutely continuous. So, this is the approach that has the closest links to robust control theory. Now, if I want that, now given I know how to do each of those separately from a representation standpoint, I can just combine them. And these axiomatic approaches are meant to help us think about this combination. So, what I could do is I can, for each theta, I can minimize over M, okay? And then I can look over, and then I can look over a Gilboa-Smither type. A Gilbois Meiler type set of these different ends. And so, and that would give rise to this representation. So, I'm doing misspecification model by model, okay? And then I'm looking over the alternative possible choices of the prior. Now, now, prior here, you have to remember is a prior over a misspecified model. So it's not, you know, it says, it's, it's, it's, you're, you're not taking the view, you're taking the model as an interesting one and a good approximation in some sense. And a good approximation in some sense, but not that it's literally the data generation. Then, of course, there's a more general version or the alternative version of this where I have these two divergences in play. And now I'm going to look at, I'm going to compute this. Again, I'm going to compute the model misspecification penalization. I'm going to integrate that over these different alternative priors. And then I'm going to add on a penalization over the different priors. Priors. Okay, so this is again, this is a way to construct a joint divergence over the models as well as priors. And for a lot of problems, I think this distinction would be quite important. And so this is a second type of representation trying to get at this notion of treating prior uncertainty and this kind of likelihood or model uncertainty in kind of distinct ways. Um, in kind of distinct ways, okay. So, so, so the prior uncertainty is n is, you know, the way I've done it, the n is always absolutely continuous spec of pi, so it's going to restrict the set of alternative models you're looking over in contrast to the choice of m. Okay. So let me now, in my few minutes remaining, talk a little, just mention dynamics and talk about a couple of different applications which we've been doing and have completed papers on. Doing and have completed papers on. One is we can roughly say we can start, we can go dynamic. A different way to do dynamics is to exploit dynamic programming methods and to imagine that you're going to do a conditional counterpart. And so you might have misspecified Markov transitions and you might have misspecified in a dynamic context, priors and posteriors get kind of blurred because tomorrow's posteriors, today's prior, but then we're going to explore the consequences. But then we're going to explore the consequences of these in a kind of a recursive way. There's issues that come up there in so-called dynamic consistency, but there are ways to do this recursively in ways that we think are sensible. But I'm not going to have time to go into all those issues. The paper I have with Sergeant and Jett discusses this tension between dynamic consistency issues and admissibility, which are kind of non-trivial issues. Which are kind of non-trivial issues, and you have to think your way around it. Now, for uncertainty quantification, and this is important for our applications, there's two things that we want to address. The first is how much uncertainty aversion should we impose. As I say, I, as an external researcher, shouldn't be telling society or the decision maker exactly how averse they should be. But the thing we can do is do a sensitivity analysis. We can change those penalization parameters and show you what the. Change those penalization parameters and show you what the consequences are. And so that reduces what could be a very, very high-dimensional uncertainty problem into a characterization that's much lower-dimensional. The other thing what you can do is you can investigate which uncertainty matters the most. And I'll give you an example in just a minute. I can activate robustness concerns. I've got multiple sources of uncertainty. I can activate robustness concerns one at a time, compare decision outcomes to those that would happen if I activated them all. That would happen if I activated them all simultaneously. And that's revealing to help me understand which channel of uncertainty might be actually the most consequential for the decision maker. So let me just talk briefly about some results, two applications. One is social valuation and climate policy under uncertainty. You know, of course, as we all know, there are many calls for immediate climate implementation. It's quite clear this. It's quite clear there's a case where there's limits to our knowledge about the timing and magnitude of climate change. And some people use this as a reason not to do things. That's not based on sound decision theory, but certainly these limits are there, and I think it's valuable to integrate them into the policy problem. So we study these problems where decision maker constructs uncertainty in a situation where in the future, we're going to know a lot. In the future, we're going to know a lot of their damages to the overall environment, to the economic opportunities. In the future, as we damage things more and more, we're eventually going to know a lot more about damages than we do now. But we still may want to act now. And it's also a setup in which the value of further empiricism in the near term is limited. But down the road, we might really get a big dose of learning as extreme events start playing out in response to climate change. So this is joint work I've done with BuzzBrock, Mike Barmett. Joint work I've done with BuzzBrock, Mike Barmett, and Amon Hongzhang. So, the challenge here is we put in four sources or channels of uncertainty. One is we build this macro style model that has productivity uncertainty, capital investments today will alter future outputs, and there's uncertainty as to how that plays out. Geoscientific uncertainty. If I omit carbon in the atmosphere today, it's going to have an impact on the future of climate. Impact on the future of climate. Economic uncertainty shows up because climate change in the future is going to alter economic opportunities and social well-being. But, you know, economies historically have been good at partial forms of adaptation and other responses like that. So that induces uncertainty. And then finally, there's a type of technology uncertainty. One way out of climate change is to do research and development and hope to develop new economically viable. Develop new economically viable technologies that are clean instead of dirty. So, the research question that we pose in a stylized model is: which of these four sources is really the most concern for designing policy? And so, the type of R D event that we can explore there is one like nuclear fusion all of a sudden becomes viable. It's a big discovery type of R D. And our initial research shows that this unknown. And our initial research shows that this unknown R D outcome is really the most potent contributor to uncertainty and actually dominates the other ones. But the other interesting part of this is the following. You might think that the more uncertainty of verse I get, the more averse I get, the less I would do. But in fact, it's just the opposite here. Because of the nature of the R D and everything, you actually, there's a range of versions under which you actually want to be, you want to increase R D investment. You want to increase RD investment, not decrease it. You want to be more active rather than more passive. The next type of application we've recently investigated with some other scholars, Aceano, Munson, and Shankman, is land allocation in the Brazilian rainforest. So what's the challenge here? Land can be allocated to plant trees that are absorbing carbon, or it can be allocated for agriculture, including cattle grazing. There's uncertainty about the land use productivities. About the land use productivities comes from multiple sources. Yeah, we do our productivity measurements are already imprecise, and we may not have measurements of both types of productivities in all locations. So this productivity uncertainty was a big deal here. We built a spatial dynamic model with prudent land use under uncertainty in order to measure the social cost and benefit of preserving the rainforest. The way we do it is we imagine, you know, that's. The way we do it is we imagine that the rainforest has social implications beyond Brazil, but for the entire world because climate change had just externality. So what we do is we impose certain sources of social cost of carbon and then trace out what the prudent policy is in response to those sources. And it turns out that the source of uncertainty of matter will be sensitive to how big or small the social cost of carbon. To how big or small the social cost of carbon is. For very small social cost of carbon, you really care a lot about the agricultural uncertainty. For the very high price of the social cost of carbon, then the carbon absorption uncertainty comes into play in a big way. And we also kind of show, and we actually measure costs that we at least argue that in principle, because agriculture is not all that productive in the Brazilian rainforest that a relatively low social cost, one can address climate change in a very productive manner. One can address climate change in a very productive manner. Let me just talk about concluding Mark's remarks. I think uncertainty matters for policy turtles like the close social cost of global warming, the social investment of green technology and development and the like. And I think understanding the source of this uncertainty broadly conceived, used by the private sector and governments will make economic policy all the more effective. So this is all about how do we integrate this uncertainty analysis into the decision-making process, which is, of course, one of the big themes of this entire conversation. One of the big themes of this entire conference. Thank you. I'm glad to answer more questions or address more issues you want to talk about. Very much awesome. Maybe I'll open one question. In your framework, what is the role of data in choosing the type and extent of misspecification as opposed to prior choices? Yeah. Okay, so this is a this plays back on that comment I made about where we can think of the static problem as kind of an ex-anti-commitment problem, in which along the way, there could be decision rules that at each date and time in the future depend on data that might become available. So that means that different forms of learning around that, you know, potential learning is on the table because some of the data you're looking at could be like signals about parameters of interest or work. Parameters of interest or different information about models that might blog in terms of trying to sort out models that are more or less plausible and the like. So data can play a role here. If you look at Ferguson's book on decision theory, it's very much an ex ante approach here under which you solve the problem ex-ante, and then you allow data to kind of emerge along the way, and then you analyze prudent. Emerge along the way, and then you analyze prudent decision making. So, our commitment problem is in that X ante perspective. Sorry, our static problem, one has to take that X ante perspective. Now, in the dynamic problem, then you what you want to formulate, what you formulate recursively, then there's recursions for dynamic programming, there's recursions for learning, and you want to kind of combine those. And then you want to both explore ramifications for misspecification in the learning. For misspecification in the learning algorithm, as well as in the conjectured models about the future uncertainty as it plays out. So these two forces can still be in play, albeit now in a kind of a very dynamic programming recursive type specification. Yeah, thanks for the talk. I have a question. Can you thought, and this question actually is exposed. This question actually is exposed clearly in the dynamic framework, but already in the static framework. When you form a prior, I mean, there are settings in which I can imagine where the prior is formed based on past observations or something that has happened and people might be based on surveys or questions or things that sometimes. You know, that some sort of intervention you have done. And you know, the prior might have a misspecification. I think, but it's kind of before, like, it's not, you know, it's the misspecification happens before you actually make the decision. So you have an opportunity to correct the misspecification in the prior. It's like the adversary has played first for the prior part of this, and then for the likelihood. You know, for the likelihood, that's okay. So, that I would imagine would lead to a very different type of decision-theoretic set of axioms in a situation where you know that you go after the adversary for one of the components of this. I mean, then in the dynamic case, it goes, everything gets mixed, but the issue in the dynamic case is that you perhaps always have a confounder because you, you know. In founder, because you would, you know, the adversity might be able to see something you don't see all the time, right? So, and so, you know, have you even solved a lot of issues? So, let me, of course, that's a very important question. Again, in this static framework, I can handle dynamics with learning from an ex-ante perspective. That is, I can entertain decision rules and feedback on data that the data that can, you know, information become available in the future. And so they're. Become available in the future. And so they're like the robust, even the simplest case, it would accommodate robust Bayesian learning where I've got an unknown initial prior, but for each prior, I'm updating via Bayes' rule. We, of course, will want to add misspecification into that type of setup. So we're not closing the door on learning at all. But the statement of commitment happens to be really important here because the place where things get tricky is. Tricky is you can solve the problem at date zero. Okay. What happens if you re-evaluate the problem at some future date? Now, dynamic programming methods, usually the preferences are represented recursively so that that's not an issue. It's all built into the preferences. But in these models with the ambiguity of version, misspecification of version, then things, that's where the dynamic that. That's where the dynamic dynamic consistency does not follow immediately. So, the place where this comes into play is how much you want to entertain this fact that the person making the decision in the future is going to re-evaluate things, which in a lot of settings would seem to make sense. So, then you get pushed more towards recursive representations of these problems, both model misspecification and priors. As I say, the priors are the prior at date T was. The prior at date T was a posterior at date has a posterior that comes out at date t minus one. So there's kind of dynamic learning going on. And then how do you want to model that future decision maker on reevaluating things? And over what set of objects does he reevaluate them over. So they're the issues. Yeah, those issues in this ambiguity setting in general, even without misspecification, are tricky issues. Without misspecification, are tricky issues, and so they remain a little bit tricky here. Um, we have our own take on what we think are useful ways to address it. But there is this tension that we talk about in our chat paper between ex-ante-admissibility from the standpoint of date zero versus these dynamic consistent solutions. So, I um but but but we certainly want to include and do include the idea that in the future you can learn. The idea that in the future you can learn stuff. But in a dynamic context, you may not learn your way out of everything because models in the future could, you know, the world could change in the future in ways that aren't explicitly tied to the past. So you also want to allow potential misspecification concerns, I think, going forward as well. Thanks, Larry. Thanks for the excellent talk. I have a question regarding the relationship between distributional robust optimization and theoretical foundations such as actions that lead to virational preferences. From the ecometric paper 2006, we know for divergence measures, then Measures, then there is the equivalence between the reprinted visa and the actions. So I wish you mentioned at the beginning of your talk that we could just from the reprinted distance, then we could easily just replace, for example, diverted measures with, say, watches and distance. But I'm curious, is there sort of a link between? Of link between better representation and original practices. If I remember correctly, I don't think this is done in the metrical paper. Maybe some subsequent papers. So I should be clear how what you can and can't get out of axioms. The axioms right now are not telling us how to what type of penalization when I'd be using. When I'd be using, it's it's it's uh it's yeah, just like you know, savage axioms don't aren't gonna tell us utility function curvature for risk aversion. So in that sense, they're weak. They're not as helpful. That's obviously a very important part of the implementation. You know, your choice of how to penalize things. That's a very important part of that implementation. Today, the axioms that I'm aware of and the ones that you were That I'm aware of, and the ones that you were also making reference to, and all their various extensions, really don't get into that question in a way that's directly usable on applied work, which is too bad, but I think it's because it's obviously a very important part of the implementation question. The absolute continuity one I've kind of wrestled back and forth with. Here, conceptually, we're imposing absolute continuity. And I think if you want to. And I think if you want to really think about a misspecified likelihood, it kind of fits, but there's no reason why you have to think about misspecification in only a straight likelihood context. Just like prior misspecification, the ones that we thought about are the priors that are mutually absolutely continuous, but in general, one can entertain other ones as well. So now we have, I mean, we do have in my paper with my Italian co-authors, we do have one. And co-authors, we do have one result that pushes one towards absolute continuity, but I don't think it's totally compelling. So I'm at this point in time open to various different types of divergences. I think there's trade-offs in using the different types, but I don't have a, unfortunately, I think your point was these axioms don't really help you to settle how to complete the variational preferences. And that I would agree with. Did I interpret your question correctly? I'm not sure. Yeah, okay. Very good. Thank you again, Nas, for a wonderful talk. Yeah, well, thanks for inviting me. I think these are important questions. So I look forward to further advances on them. So thanks. 