We're recording? Yes. Okay. Yeah, fantastic. And I'll start by thanking the organizers for putting together this workshop and, of course, for inviting me to speak. So I'm going to be talking about basically some techniques for computing things related to random quantum circuits. And this is largely based on a few papers. This first one from a few years ago, and the second one more recently. And the second one, more recently, kind of using two different ideas from statistical mechanics or quantum many-body physics to basically compute the rate at which these random circuits generate pseudo-randomness. And I'll say very precisely what I mean by that in a few slides. But I'm going to sort of focus on a somewhat abstract property of random quantum circuits, namely that they converge to something called unitary designs. And while this is sort of abstract, I think this is behind a lot of the interesting properties. lot of the interesting properties and interesting features of random quantum circuits, namely that they quickly scramble and generate entanglement and sort of this ability to rapidly generate pseudo-randomness, I think, is also behind a lot of the interesting feature, like interesting properties of random circuits with respect to the recent supremacy experiments, these Google experiments. So at the end, I might mention basically the application of these random circuit techniques to some interesting problems there. To some interesting problems there. And so this is joint work with Jonas Haferkamp, who's a grad student in Berlin, as well as Alex Dozell and Fernando Brandau, who are at Amazon and Caltech. Okay, so as I said, I think the very, very broad takeaway from this talk is that ideas from STATMEC and quantum mini-body physics are extremely useful for solving what I see as fundamental problems in quantum information theory. So random quantum circuits. Random quantum circuits, I think, are very interesting to both quantum information theorists as well as quantum many-body physicists because they're sort of a rare instance of a solvable model of chaotic dynamics. So they guess they'd be in the second row here, but they're good toy models to kind of investigate the emergence of thermalization from unitary dynamics, the onset of quantum chaos, as well as out of equilibrium properties like diffusive hydrodynamics. And for a quantum information theorist, these random circuits are really nice because they're low-depth implementations of unitary designs. They rapidly scramble and decouple quantum information. And certain properties of their output distribution are provable properties are, you know, kind of give evidence that their sampling from these distributions is a hard classical task. And this is sort of the basis of these recent Google and USTC experiments. Okay, so I'll first. Okay, so I'll first tell you what I mean by random quantum circuit. And I think we've heard about this a bit already this week, but I want to consider local random quantum circuits on n quits of local dimension Q. So we have, for instance, we could have a 1D chain of n q dits. And the time evolution I'm going to consider is this evolution, for instance, by staggered layers of two-site unitaries. So we have our 1D chain of qubits down here. Of qubits down here, and this entire thing is one unitary evolution which acts on you know the n-qubit system, and where each of these two-site unitaries is drawn randomly from some gate set, which for simplicity, I'm just going to take to be the R measure on the two-site unitary group, which is u of q squared. And then time evolution to a time t is given by a depth t circuit, which is the product of t layers. Okay. Okay, and for simplicity, I'm going to focus on these 1D local random quantum circuits, but actually, quite a bit is known about, for instance, higher dimensional circuits or more general graphs. So you could consider more general connectivity graphs for systems of n qubits. And you could also consider more general gate sets. So instead of uniformly harrandom two-site unitaries, we could consider drawing gates randomly from some discrete universal gate. From some discrete universal gate set. And sort of for simplicity, I focused on the simple example, but feel free to ask if you're interested in one of these other particular cases, because quite a bit is known about which results generalize to these more general random circuits. Okay, good. Yeah, so these random circuits have been, there's been a lot of interest in the literature because you can sort of use them, as I said, to Can sort of use them, as I said, to probe things like operator growth or quantum chaotic dynamics. And there have even been generalizations of these things to study the diffusive spread of charge, or you can intersperse measurements in these things to observe an area law to volume law phase transition. So there are lots of generalizations that I think are interesting for many condensed matter problems. And as I said before, also like sort of the things you can prove about. Also, like, sort of the things you can prove about the output distribution of these random circuits means that there is this task, random circuit sampling, which for which there is evidence that this task is classically hard or classically intractable. Good. So my goal is going to be to kind of study in this talk is going to be to sort of study one property of random circuits, which I think is behind a lot of these interesting features. And that is that these random circuits sort of And that is that these random circuits sort of are pseudo-random in a very precise sense. So they converge to ensembles, which are called unitary k-designs. And roughly speaking, this is the circuit depth T, which we start approximating moments of the unitary group. So just so all of the parameters of the talk are on one slide, I'm going to consider, again, systems of n quotes. Okay, the local dimension is q. So that's actually going to be sort of a convenient knob. Uh, sort of a convenient knob to tune. So if I have local qubits, this is q equals two, but more generally, I can just take some local dimension. Um, such that the total Hilbert space dimension d is q to the n. Okay. And then we have depth t circuits, and we're going to be interested in when we start forming k designs, and I'll tell you what those are. But are there any questions about the model I want to consider? Any questions about random circuits? I'll pause. I'll pause. So I'm interested in the ensemble of all random circuits. So you can kind of imagine I have a one parameter family of ensembles of circuits, you know, indexed by time. All right, I'm going to assume there are no questions. So I'll start by telling you what unitary designs are. Designs are and basically, I'm going to prove that these random circuits converge to these nice ensembles called unitary designs using some ideas from statistical mechanics as well as quantum anybody physics. And then sort of at the end, I'll apply these techniques to certain problems which I think are interesting from the point of view of like these noisy, these noisy random circuit sampling experiments. Okay. Okay, so recall that the Haar measure is the unique left and right invariant measure on the unitary group. And for a general ensemble of unitaries, or for what I mean by that is a very, for any distribution on the unitary group, we're often interested in moments of that distribution. And one object which sort of captures the k-th moment of some distribution or an unitary group or some ensemble of unitaries is this thing called the k-fold channel. Is this thing called the K-fold channel, which is the sort of like k-fold adjoint action thing here? And we say that an ensemble of unitaries is an exact k-design if these kth moment operators are equal for all operators O. So this is basically saying the kth moments match. Okay. And so basically Pauli's form an exact one design. Form an exact one design. But in general, very little is known about very few exact constructions are known. So there is some existence proof which says that for any dimension in any moment k, basically these distributions, these set of points on the unitary group need to exist. But very little is known beyond like k equals three. So we can relax this definition and sort of to make progress, just ask when an ensemble is close to forming a k design. So just to be very precise, and I'll So, just to be very precise, and I'll give a more intuitive definition in a few slides. But we say that an ensemble of unitaries is an epsilon approximate k design if these k-fold channels are close in some norm. So this is kind of a very, very tight super operator norm, but it's just some norm. And we're just asking when these two ensembles are close. Okay. Nick, I want to ask you a very simple question. Oh, please, yeah, yeah. Simple question. Oh, please, yeah, yeah. Thank you. In this case, right here, actually, Kfault channel in green. I'm trying to understand your integral. So, what size Hilbert space does O act on? And when you take that tensor product, how's the action working? So, is like O acting on the size of U to the power K or it's some local operator? Yeah, it's any operator that could be that acts on the K-fold space. So, here. So here, I mean, basically, this is just like a super operator way of writing polynomials of degree or like homogeneous polynomials in U and U dagger of degree K. I've just sort of written that in a K-fold space. But this is right. These are any operations. The K-Co acts on a K-fold space then. Right. And that's K copies of like, you know, UD or like the D-dimensional Hilbert space. U D or like the D dimensional Hilbert space. Perfect. Thank you. Yeah, so there's sort of an equivalent definition you can give for designs, which is just that any polynomial, which is homogeneous to degree K in U and U dagger or U conjugate, averaged over the Haar measure is equal to the average over this other ensemble, which could be discrete or continuous. Which could be discrete or continuous. So I've written this other ensemble here as an integral, which means I might have some probability distribution, but I could also imagine some sort of weighted sum over some discrete set of unitaries. And you expect that these sort of discrete sets of unitaries exist, which also are exact or approximate designs. So your notation of du is in this K-fold channel is just some yeah, this is uh yeah, yeah, I like This is, yeah, yeah, I probably should have written like for some, yeah, I'm sort of mixing the discrete and I'm being sloppy about mixing the discrete and the continuous, but for this continuous case, I really do mean some probability distribution on the unitary group. So I should have like d nu over u. Yeah, yeah. Okay. Yeah, yeah. Sorry about that. But you could imagine some sum over a discrete set of unitaries. And I'm also interested in those. Right. Right. So these notions of designs are extremely powerful. So, I mean, yeah, there are obviously designs. I'm interested in designs on the unitary group, but there are, you know, designs for spherical designs come up, obviously, in other areas. And they're sort of, I guess, interest in designs across many areas of math. But in quantum information, we're usually interested in designs on the unitary group. So these are, you know, sort of things which capture uniformly random. Which capture uniformly random operations on a quantum system. And they're very powerful because they sort of allow us to wield the power of like fully harrandom unitaries with sort of like low complexity. So they're sort of like basically a hard random unitary is extremely costly to implement. It's like an exponential operation, an exponential in terms of the local operations. And designs sort of give you the power to implement these things with like a basic lower overhead. That's one way of thinking about it. One way of thinking about it. Okay, so yeah, just more informally, we say that an ensemble of unitaries or some distribution on the unitary group forms an approximate k-design if the average over that ensemble is close to the average over the full unitary group up to the k-th moment. So, you know, we have some set of points or some distribution on ud. We average over, you know, average some polynomial of degree k, and it's close to the average over the full. And it's close to the average over the full unitary group for all such polynomials, all such expressions involving k of these guys. Okay. Another way to think about this is sort of asking about how random the time evolution of a system is compared to the full unitary group. So if you consider an ensemble of time evolutions at a fixed time t, right, so these could be depth t random quantum circuits. These could be time, like some ensemble of time evolution. Time, like some ensemble of time evolutions generated by disordered Hamiltonians. So you take some spin system and you disorder the couplings. That gives you a family of Hamiltonians, Hermitian operators. And at some fixed time t, you have some distribution on the unitary group. And you can sort of ask about how random that distribution is by asking, you know, how close you are to forming a k-design. So at time equals zero, you just have a bunch of points setting the identity, but you might expect that chaotic time evolution is going to move you rapidly and ergodically over the unitary group. ergodically over the unitary group. So you can sort of ask, like, at what times do you start, you know, forming approximate designs and start approximating moments of the HART measure. Good. Are there any questions about designs? Yeah, maybe I have a very simple question. No, please, yeah, I agree. Yeah, if you take H to be some spin glass type of random Hamiltonian. Glass type of random Hamiltonian of some sort. Are you sure as time goes to infinity, you actually recover hard measure? I think if you start, right, you see what I mean, right? If you have a tangent of the point and you run time, you kind of explore the poles more than you do the equator. So, this will not give you some unitary hard measure as t goes to infinity, but right, I think it's basically. But right. I think it's basically, I'll say that energy conservation, I think, means that the phases of the time evolution operate of the IHT will never be harandom as t goes to infinity, which I think is basically what you're saying. But if you, I think, I do think generically, though, if you make your, you know, your disordered Hamiltonian a little bit time dependent, then you do recover the Haar measure. Or you do recover a lot of if you make it time dependent, right, then you make, yeah, I could have. Right, then you make, yeah, I could imagine. Like, if you make uniformly on the yeah, yeah, if you kind of uniformly explore the space, discretize it somehow, then it does, I agree. But that doesn't preclude possibly if you have some, you know, some disordered Hamiltonian, you could also ask like you get approximately close for some low moment, right? So that's not getting the full hard measure, but you might approximately reproduce some very low moments or something. So the way to read this is perhaps, you know. This is perhaps you know, for certain times you still get some kind of a design, and that'd be interesting. I mean, I was just, I was just phrasing, I was just phrasing this as you could ask this question, but I do, I can tell you what I believe. I believe that sort of designs are sort of not quite the right question for time-independent systems, but I do think you form kind of weak approximate designs, or you form approximate designs as sort of a weak norm. Yeah, good, good, okay, I'm totally on board. Thanks, yeah, yeah, good, yeah, but uh, yeah, this was maybe. Yeah, good. Yeah. But yeah, this was maybe just like a setup for a question you could ask. But I do think that designs are really, really asking something about time-dependent systems. Sorry to interrupt again. Oh, please, please. I mean, you're talking about approximate designs, and that's with respect to the diamond norm, but you're not claiming to me that any realistic time-independent Hamiltonian would actually be close in me to a K-design? A K design? Right. So I'm defining this thing. I'm talking about approximate designs, definitely. And by approximate here, I mean diamond norm. Yes. And you could, it certainly, I think it's an open question if even like you get approximate one designs in any sort of norm for some time independent evolution. I think you do get like a low. You do get like a low moments in a weaker norm than diamond norm. And this is sort of what I meant by: I think sort of the design is not the right thing for time-independent systems. But I do think if you have very generic time-dependent systems, you'll converge in diamond norm. Okay. Maybe just a comment, Simone. So, like, suppose H is a local Hamiltonian and the number of random parameters going in is, I mean, say you have a spin chain. You know, you have something that grows linearly with the number of spin chains. That grows linearly with the number of spins in terms of the number of random parameters that go in. And you have a unitary that acts over the exponential space. So, and you know, if you really have a truly hard unitary, you would expect the exponential number of random parameters. So, in some ways, like from a parameter counting perspective, we can also see that it won't quite explore the full space. Although you could form some kind of an epsilon net, like Nick was saying, if you make h to be time-dependent. Saying if you make h to be time-dependent and you move around, you move around the space of unitaries and then you run it, you know, then you could cover with some approximation. Definitely. And what I'm going to be talking about is basically, you know, these random circuits are a time-dependent evolution, which does, in a very precise sense, you know, kind of fill up this space. Can I just ask? Yeah, please digressions, but I'm just trying to connect this to other notions of randomness and how. To other notions of randomness and Hamiltonians. And you know, then, if I'm thinking about random Hamiltonians for spin systems or local Hamiltonians, I'm thinking about many-body localization, which seems like it's sort of a very different phenomenon because it cuts up the space into regions that don't get connected. So, could a system that's many body localized ever be one design? Yeah, I think, uh, yeah, I think probably not. Somehow, all these notions of like chaos, which you know, so quantum, quantum many-body chaos is. So, quantum, quantum many-body chaos is a very touchy subject because everyone, you know, no one really knows what it means, but and everyone has strong opinions about it. But all these sort of symptoms of chaotic dynamics we see, you know, that like this, this is form factor growth, there are, you know, certain decay, like decay of correlation functions in a very specific way. They all sort of, you know, seem to happen for generic systems in a certain way. And for MBL systems, that doesn't really happen. So I would say that when I say generic or So, I would say that when I say generic or chaotic, I mean sort of not integrable and not MBL. But obviously, all these, you know, I would say we don't understand many-body chaos. So, you should take everything I say with sort of when I say, you know, expect for generic, it's just sort of wild conjectures. And part of the reason random circuits are nice is because we can actually compute these things extremely precisely. Okay, thanks. Yeah, yeah. But I would say that, like, yeah, MBL systems are sort of not chaotic in that they don't do a lot of things. And they don't do a lot of the things I'm going to describe. Okay, good. Right, so I'm interested in when random circuits form approximate designs, right? So when sort of the ensemble of random circuits converges to the Haar measure, and like a precise approximating the kth moment as a function of the circuit depth. So, actually, quite a bit, a little bit is known about this problem already. So, does this Problem already. So, there's this seminal result by Brandau, Harrow, and Horodetsky from now 10 years ago, which says that these local random quantum circuits, 1D local random circuits, form approximate K designs and a depth which scales linearly in N, so the number of qubits, and this extremely high degree polynomial in K. So it's like K to the 11. And actually, their proof for this, which I won't go into, makes use of the sort of a bound on spectral gaps called this martingale method, which is due to our previous speaker. Martingale method, which is due to our previous speaker. So there's sort of already some nice connections to many body physics there. But moreover, they gave a lower bound on the K-design depth and showed that you can't possibly do better than linear in N and K. So this is the circuit depth at which you form a K-design for N qubit systems, qubit systems. And there's sort of been a folklore conjecture in the field for the last 10 years that local random circuits form approximate K-designs. Random circuits form approximate K designs in a depth which is, you know, kind of saturates this lower bound. So it's like sort of an optimal implementation of pseudo-randomness. And obviously, there's a dependence here on epsilon, this approximation error, which I'm suppressing, but it's like a it's a log one over epsilon factor in here. Okay, good question. Is this architecture agnostic? Like irrespective of whether you're going to treat you irrespective of right, so yeah. Right, so yeah, no, no, yeah, right. So, um, uh, I can, there's sort of two answers: is what we know and what you expect. So, what we know is that basically you replace this n here in the depth with like a one over d, where d is the dimensionality of your lattice, your spatial lattice. And the actually the polynomial in k actually, do I have that right here? Yeah, fantastic. Okay, so this is what's known. So, this is what's known: that a d-dimensional RQC, like on a d-dimensional spatial lattice, formed designs in sub-linear depth, but an even worse polynomial in K. And what you expect is that this, even for higher dimensions, you should also get something linear here. Right. And you also expect, and this is something that hasn't been shown, is that the complete graph case, so cube. The complete graph case, so qubits with all-to-all interactions, as in, like, you can apply a two-site unitary to any pair of qubits, you expect that depth to go like logarithmically in n. And that's, and that's not currently known. Yeah, so I would say that that's sort of in the n dependence, that's the that's the main open result. And then in the k dependence, it's, I guess, there's like a pretty large gap for all the cases. Good. All right, so there's sort of been this folklore conjecture in the field for a while that. In the field for a while that random circuits are sort of, these local random circuits are optimal, and at least in the 1D, that this should be the correct depth. And just as Ramix was pointing out, if you have different spatial dimensions, like a different arrangement of qubits, there's going to be some change in the end dependence here. But I've just sort of quoted everything for 1D for simplicity. So, yeah, and maybe it's also interesting just that there's sort of another tack you can take for efficiency and so that it's. Efficiency. And so that it's known that if you take basically random Clifford circuits and intersperse non-Clifford gates, you can also converge to K designs in a number of non-Clifford gates, which is end-independent, but scales kind of polynomially in K. Good. But that does break up. You put these clipers in random locations in the circuit or like Random locations in the circuit, or like no, so imagine if you aggregate all of them in one place, it could yeah. I mean, basically, they have it, I think their construction is they have a depth n random Clifford circuit, then they do one non-Clifford gate and then another depth and, you know, another depth and Clifford circuit. So you're sort of scrambling the single non-Clifford gate, and then you can ask about, you know, how many non-Cliffords did you need to get to a K-design and the total depth of that circuit. So there's some kind of a structure to where you put the Cliffords. To where you put the Cliffords. Yeah. Yeah. I mean, presumably, if you randomize the Clifford locations, like, should also be fine. But yeah, you're basic, like there's their construction was structured and you're sort of like waiting a scrambling time almost before you put another one in. Sounds good. Good. But I'm just going to focus on these other random circuits, the kind of unstructured, completely like our local random circuits. Local random circuits. Okay, and designs are extremely useful in quantum information. So the kind of breadth of the applications involves things like state distinguishability, decoupling, randomized benchmarking, and other sort of interesting physics things like generating entanglement or showing OTOC decay. And while some of these results only require two or four designs, basically higher degree designs are often essential for concentration arguments. So if you want to make a statement about a single So if you want to make a statement about a single random quantum circuit being close to the random circuit average, like utilizing higher degree designs is very useful in that context. But maybe one physics application I'll mention is that higher degree designs are really useful for understanding like long time or kind of post equilibrium properties of many body systems. So what you could imagine is if you take a state which is acted on by a design element and look at the entropy of a subsystem. Look at the entropy of a subsystem, so some small subsystem A, and ask how close that is to the thermal entropy. What you can do is you can bound the probability that that entropy fluctuates away from its maximal value over the design instances, and you can upper bound that in terms of basically powers of the total Hubert-based dimension. So if you have some time-evolving system, which is forming designs as you time-evolve, Designs as you time evolve, what you can then show is that basically the probability of a fluctuation away from, you know, of some subsystem entropy, away from equilibrium, gets increasingly rare, sort of up to extremely late times. So even after your system has equilibrated, there's still some like long-time dynamical process. And that's like, you know, basically your subsystems are sort of getting more and more and more static. And then one other application of high-degree designs is sort of. Application of high-degree designs is sort of which equilibrium are you talking about? Right. So, this would be so, strictly speaking, here, I'm like the for these sorts of systems, the marginal is approaching just the identity on the subsystem. It's like the maximally mixed data on the subsystem. But I would say that, you know, I expect something like this to hold more generally for, you know, just this would be like the. For, you know, just this would be like the, so, right, so in that case, if you have the maximum mixed state, obviously the entropy here is the, uh, is like log of this size of the subsystem or log of DA. But yeah, so more generally, this might be something like the thermal entropy of the Gibbs state, but maybe that's, yeah. I mean, yeah, strictly speaking, you can prove something like this for random circuits and for designs. And in that case, the subsystems approach the maximum mixed state. The maximum mixed state. And then the second kind of interesting property you get from high-degree designs is improving lower bounds on the circuit complexity. So yeah, I probably don't have time to really go into depth about what this is, but this is basically you define some set of simple operations like local gates, and then you ask about the minimal circuit, which implements your unitary or Circuit which implements your unitary or state. And this is called the circuit complexity. And there are all these conjectures in kind of motivated by the high energy physics community about kind of the long time linear growth of the circuit complexity. And specifically, you know, it's even been conjectured that most local random quantum circuits of depth t have a complexity which scales linearly in t for an exponentially long time. So this kind of minimal circuit complexity needs to grow linearly. Needs to grow linearly, and what you can prove is that complexity needs to scale linearly in the design order k. So, if you can prove that your system is forming designs as you time evolve, you can actually establish lower bounds on the on the complexity growth with high probability over the ensemble. So those are sort of just two maybe physics applications of high degree designs, which are which you can kind of establish rigorously using designs. Designs. But I'm going to focus. Okay, quick question. Sorry. Definition of complexity is a lower bound of the depth to accomplish a task. No, no, no. Right. So maybe you said it, then I would. Yeah, that's fine. So fix some set of gates, okay? Some local gates, and like two local gates, and then fix a tolerance. And then for some unitary, the complexity would be the minimal number of those operations you need to approximately implement that unitary. Implement that unitary. Yeah, basically what I was saying. Yeah, yeah, I think that's what you're saying. Yeah, and but so with no restriction on the unitary or? Well, so it's the question, given some, right, all right. So the question is, what you ask about the complexity of a fixed unitary. Of a unitary, of course. Okay, great. I'm happy. Thank you. Yeah, yeah. And then the conjecture is that, like, for some time evolution, which is like, you know, some fixed unitary, this needs to be, this grows linearly. Needs to be this grows linearly in time, and I think this is believed to be a generic property of chaotic systems, and it's that has all these connections to basically ADS CFT. But I think it's just sort of interesting from like a many body physics point of view. So this, so how do I read this? So this is there for most unitaries, this figure is correct? Or? Yeah, right. So you can never really hope to prove this, like establishing lower bounds on arbitrary unity. Lower bounds on arbitrary unitaries, circuit lower bounds on arbitrary unitaries is extremely hard. But what you can do is you can make probabilistic statements about ensembles. So if I have some ensemble of random quantum circuits, the precise statement that you can prove is that with probability exponentially close to one, right? Because you might get extremely, you might get exponentially unlucky and like implement the identity, right? You might just accidentally pick the identity everywhere. And obviously that will have a complexity, which is just always zero. complexity which is just always zero but so with with probability exponentially close to one you can lower bound the circuit complexity um probability with respect to uh with respect to the space of unitaries that no no no no your ensemble your ensemble so that's what i mean right so it's measure on the space of unitaries from which you pick you right right right right that's right good yeah exactly uh yeah sorry this was just supposed to be sort of like some Sorry, this was just supposed to be sort of like some quick motivation for a physics connection why designs are interesting, but I'm happy to kind of give as many details as you want about this. Right, and that allows you to, so I guess what's, I should say that we don't exactly know for qubit circuits, for instance, you sort of need a linear growth in this design order k. So t and k need to be linear, and that gives you a linear complexity growth. But if you have some this higher degree polynomial, you get some. Higher degree polynomial, you get some sub-linear complexity growth. Like if t goes like k to d11, the lower bound you get is like some algebraic growth in t. So, you know, if you improve your bounds on the design time, then you, you know, you give probably better lower bounds on the complexity growth or more accurate lower bounds on the complexity growth. So I think what we sort of know how to prove saturation and some like growth, but maybe we don't have all the, there's some details about basically this plot that you'd like. Some details about basically this plot that you'd like to prove that maybe you're not quite nailed down yet. Good. So, the two things I was just going to briefly discuss are basically proofs that random circuits form approximate designs using, and I'm going to kind of give sketches of the arguments, but these are things you can prove rigorously, using two quite different ideas from either StatMech or many body physics, or two different. Either stat mech or many body physics, or two different ways of thinking about random circuits. And the simplification I'm going to use is basically taking the local dimension to be large. So if I take it sort of a little bit large in some way, you can actually get this conjectured linear design behavior. So this sort of optimal behavior in T. And this is going to use some statmec mapping. So I'm going to write basically the distance to a design in terms of a statMec model. And then there's a second approach, which has a much Second approach, which has a much more reasonable local dimension, but still something k-dependent. And they are going to use basically lower bounds on the spectral gap of a frustration-free Hamiltonian to compute the design depth. These are on linear architecture? Yeah, actually, so this first, that's an excellent question. So this first theorem here, it's actually these brickwork circuits are actually important. So I do need 1D brickwork for this. 1D brickwork for this theorem to be true. For this second one, the constants I've quoted here are for these like 1D brickwork, but sort of this, the spectral gap approach works for general dimensions. And sort of you can pay the price of some factors of n and just it kind of works for everything. And you can also generalize the second theorem to arbitrary universal gate sets, like very, very general gate sets at the expense of some constant. So if you don't want hard. And if you don't want Ha random local two-site unitaries and you prefer to draw them from some universal gate set, that'll just appear as some constant here. So, in some sense, the spectral gap approach is much more general and works for. And would you conjecture that these formulas would change to n to the one over d in R dimensions, giving the specific dimension? I think that's the true behavior. The sort of the thing which I think is not true is that, right, so I think. Not true is that, right? So I think the diamond norm basically that n to the one over d is probably true. I think the spectral gaps might be too weak a norm to see that, but I think that's still an open question. Yeah, so sort of this approach that Aram and Saeed took to prove this one over, this n to the one over D behavior for local qubits, they did need to use some fancier technology. So they couldn't just use this nice spectral gap approach. So there's sort of some open question. Approach. So there's sort of some open question if the spectral gaps actually know about the optimal end behavior. But I'll sort of maybe what I mean by all that might be clear when I start displaying formula. Good. So yeah, as I said, these are sort of two different approaches for computing the design depth for random circuits, so the depth at which we start forming k designs. And one involves upper bounding the partition function of some lattice model. And the second one involves lower bounds on the spectral gap of a Hamiltonian, a local Hamiltonian. Hamiltonian. Okay, so I'll just give an overview of the first approach. So the idea is that we had this distance deforming design in terms of the diamond norm, and we can upper bound that in terms of a weaker norm at the expense of some dimension factors involving these things called basically moment operators for an ensemble or some probability distribution over the unitary group. And basically, you can rewrite this two-norm in terms of this very simple quantity, which is sort of like a double. Quantity, which is sort of like a double average over the ensemble. And this thing is called the kth-frame potential. And what you can do is you can show that it's lower bounded by, and this is just some number which you can compute for any distribution on the unitary group and for any k. And basically how close that number is to k factorial tells you how close you are to forming a design. So it's just some number. And basically, the difference between these two things upper bounds the diamond norm at the expense of a dimension factor. So you need these two. Expense of a dimension factor, so you need these two things to be really close to overcome the dimension factor. But, um, right, the important thing to remember here is that I just have this function, this kth frame potential, which is the distance to forming a design. And, you know, it equals k factorial if and only if that ensemble is an exact k design. So the goal is basically, yeah, please, please, please. Wait a second. It's already slow. No, no, no, no. Why do you get involved? Why do you get this lower bound? Because I mean, you know, you don't know a lot about your measurement at the moment. Yeah, so there's the way to prove basically, so the proof of this is pretty simple. And if I had an iPad, I think we could just do it out. But the way you prove this is basically by taking this operator here and then taking the one. And then taking the one for the hard measure, and then you wait, or is it just waiting? Ah, wait a second. Is it just the second line which you have? I mean, namely the... Almost, yeah, yeah. The fact that norms are non-negative, therefore, whatever you have, I mean, is totally. There's another way of writing. Just a norm being non-negative. I mean, that's a restatement of this k-factorial business. So, once, yeah, once I understand. This is once, yeah, once I understand this middle line, the third way. Let me just, yeah. So, if I just take this as an operator, basically, I take the square of that or the two-norm of that, which is, you know, it's some positive semi-definite thing. So it has to be greater than zero. Basically, like this times its conjugate and take the trace of that is something greater than zero, right? Which is exactly what you said, that this thing has to be. This is an orange, right? Right. Right. And then, so the way you get this is if you just, right, because this is just a trace of this thing squared. So the way you get this is basically just using the fact that the Harr measure is left and right invariant. So if you just sort of expand out the square, you get one frame potential for this ensemble here. And then the cross terms just become the Haar measure. Okay. So basically, this moment operator, this HAR. Basically, this moment operator, this HAR operator hits the ensemble. It just absorbs it due to the invariance of the HAR measure. Right. I think I've, you know, I. And then the non-trivial thing is that this thing equals k factorial for the HAR measure. Yes. Right. And that actually comes from a diaconist result from the 90s involving moments of traces of hard-rand immunotaries. So I think Diaconus and someone else showed that. And someone else showed that, yeah. So, if this is the Haar measure, you can absorb the U in here, and then it's just moments of traces of Harriman immun series. And it's known that that's k-factorial basically as long as the moment is less than the total dimension, which is a caveat I didn't write on the slide. Okay. But hopefully, at least that sort of sketchy sketch helps. Are there any more questions? Okay. Okay. So the goal is just to compute this frame potential for depth T random quantum circuits. And to do this, we basically have to do a k-th moment calculation of this thing. So we have to basically average over k-th moments of these random circuits, so k-copies of the circuit and its conjugate. And I won't give too many details on this, but Give too many details on this, but basically, so you're averaging over all of these local two-site unitaries, and there's a way to think about kind of har averages as a prescription for index contraction. So, basically, if I average all these gates here, it's just going to tell me I have to kind of connect these like legs and these legs in basically in certain ways with respect to some permutation. And then there's going to be some weighting factor, which is a function of that permutation. Permutation. So, and this is kind of this whole approach is called Weingarten calculus. And it's basically just like, you know, a way to take our averages and write it as a sum over different index contractions or some over permutations. So you can replace this sort of average over gates in terms of basically an effective vertex, which is just sort of keeping track of the weights and keeping. And keeping track of the permutations with respect to which we contract these indices. And the end result is that you can actually write this frame potential as the partition function on a triangular lattice. So what happens is that this kth frame potential is then basically you sum over all possible spin configurations where the spins, these local spins, are actually elements of the permutation group SK. And then there's some Uh, uh, there's some plaquette term, which is a function of three of the spins, which you can write explicitly here. But the point is, it's just some function of three spins of three permutations, which you can compute. And this lattice has width n over two and depth two t and periodic boundary conditions and time. So the nodes at the bottom here are identified with the nodes up there. But it's just it's just sort of some exact way to rewrite this kth frame potential for random circuits. So, yeah, I definitely. So, yeah, I definitely skipped over details, but is sort of that roughly clear that I can basically rewrite this distance to har randomness in terms of a lattice partition function? I have a question. So I've actually, so I've done this calculation with Weinggarten functions, and it rests pretty heavily on these har integrals giving you a bunch of deltas. Do you get any sort of, I guess, any analogous sense? I guess any analogous simplifications for any other interesting distributions or yeah, so this is sort of going back to what Ramish asked a few slides ago. So, this approach really doesn't generalize to like arbitrary gate sets as easy as easily as the second approach I'll describe. This kind of the next approach I'll describe in a bit does generalize to universal gate sets very nicely. But here, yeah, basically, there's no simple way of just replacing this with some universal gate set. But you can, sort of what you're asking is, You can, sort of, what you're asking is: are there other interesting distributions which do give you some maybe interesting partition function? And the only ones I've thought about are basically orthogonal gates. So, you can define some partition function for orthogonal gates. So that would be the HAR measure on like the two-site orthogonal group. And there's another, it's sort of a more complicated statMec model, but you know, it's well-defined and you can show that like orthogonal random circuits converge to orthogonal. Orthogonal random circuits converge to orthogonal designs. And I have no idea if that's interesting or not, but it's something we can show. Gotcha. Thank you. But yeah, I think what you're pointing out is very astute that sort of this approach is specific to our random gates and this approach is nice and you get this beautiful static picture, but the gate set is very important. Whereas sort of in this next approach I'll describe after this, the spectral gap approach, the gate set is not as important. Yeah. Yeah. Okay, so you just get some lattice partition function. And the first thing you can show is that basically, if you plug in the same permutation everywhere, the same spin everywhere, you get a weight one. So I told you that these local spins are just elements of the symmetric group SK. So there are basically k factorial ways of plugging in the same spin everywhere. So what you can think about that is that they're k factorial ground states of this lattice model. And remember, I said that the And remember, I said that this thing was lower bounded by k factorial. So we already see where sort of the minimal HAR value comes from, sort of comes from the ground states of this model. And then the decay to HAR randomness must be the decreasing contribution of all the excited states. So that's sort of the first hint of where we're going. And what you can then figure out is that basically all non-zero contributions to the basically the excited states. The basically the excited states of this model can all be thought about in terms of domain walls, which wrap the circuit. So, for the second moment, you basically have two spins. These are elements of the symmetric group S2. So that's just the identity and swap. And all non-zero configurations can be thought about in terms of domain wall configurations between regions of identities and regions of swaps. And the periodic boundary conditions in time means that these domain walls sort of need to wrap the circuit. Circuit. So, you know, here's some single domain wall configuration or a double domain wall configuration. And basically, to compute the K design time, you simply need to count all these domain wall configurations. You know, so you need to, you know, all single domain wall configurations and compute their weight all the way up to, you know, end domain wall configurations. But as I said, the decay-to-Haar randomness is going to be the decreasing contribution from these excited states in this lattice, in this lattice model. In this lattice model. So, actually, it's pretty easy to come up with an upper bound on this partition function. And you basically just do that and you know sort of what the weights are for these single single to single domain, well, yeah, for these domain walls in the second moment case. And you know, you get this upper bound on the on the frame potential, and that just immediately gives you the two design time. So, the circuit depth of which you form right. Circuit depth of which you form, right? Because this is the distance to form a design, and you basically care about how you decay to this minimal value of two, which you which you're kind of decaying exponentially, that minimal value. And that just gives you the circuit depth of which you form an approximate two design, which is something linear in n. But you can actually, so this is sort of a very simple upper bound, but if you want to be kind of more exact, you can actually compute this partition function pretty precisely. Partition function pretty precisely just by solving the problem of p non-intersecting random walkers. But you know, this upper bound is kind of simpler. So this was for the second moment where we just have a kind of simple set of domain wall configurations. But if we go to higher moments, you have much more complicated domain wall configurations, and they're allowed to kind of do more complicated things like interact and pair create. And basically, I won't. And basically, I won't give you too many details, but the combinatorics of domain wall counting in the second moment case is very simple and just becomes extremely unwieldy in higher dimensions, or sorry, sorry, in higher for higher moments for more general k. But what you can prove is that for any k, you sort of always have this contribution from the ground states. And moreover, you can also prove, basically, you can prove some properties of these Plaquette terms for any. Of these placette terms for any k, and you can prove that you always have this contribution from the single-domain wall states. I call this like the single-domain wall sector or something, and it's these higher-order terms that you really don't have any rigorous control over. But you can then say that basically for there's a finite number of them. So for some large queue, this single domain wall sector will dominate all the other sectors, and that gives you basically a linear design time. So you show that the So, you show that the circuits form k designs in a depth or scale linearly in n and k. But unfortunately, you needed to take some unwieldy large q. So you didn't know how big q needed to be. So that allows you to show that these random circuits are optimal, but sort of with some unwieldy large q limit. But there are a few reasons I think this limit is likely not necessary. Like I think this might actually work for constant values of q, but I can. Know constant values of q, but I couldn't, it's hard to prove that. So you just left it as a conjecture that these single domain wall sectors always sort of dominate. Good, so that was basically k designs for from the stat mech approach. And just in the last few minutes, I'll describe the second approach, which involves the spectral gap of the local Hamiltonian. Okay, so in the previous section, we were basically bounding. Section, we were basically bounding this distance to a design in terms of the diamond norm. We're bounding that distance in terms of a two-norm. But we can kind of retreat to another norm, which is this operator norm between these moment operators. And it's just another, you know, it's another upper bound on the diamond norm at the expense of dimension factors. But this operator norm has some extremely nice properties. So for depth T random quantum circuits, there are kind of two really nice properties that you can do. Of kind of two really nice properties that you can use. One is this amplification, which is that basically the operator for every time you act with another layer of your random circuits, the operator norm difference between the moment operator for a layer and the HAR measure just sort of gets amplified by another power of t. So you don't actually need to think about the whole random circuits. You can just focus on the action of a single layer. And then this other nice property. And then, this other nice property is that the moment operator of a layer can be kind of rewritten in terms of the spectral gap of a frustration-free Hamiltonian. So, I'll say a little bit more about this Hamiltonian in the next slide, but basically you can see here that if basically this Hamiltonian has grounds to energy zero, and if you don't know anything about the gap, that means that this quantity here is one, and if I exponentiate one to the One and if I exponentiate one to the power of t, I don't know anything. But any sort of non-trivial lower bound on the spectral gap gives me something which I can just, you know, you know, take to the power t and then get some convergence to design. So, yes, please. Is this inequality which you're just showing? Uh, is that easy to see? No. Yeah, so this star here is basically. Is basically that I've hidden a few steps. So the way to think about it is like a layer is sort of like a product of gates, and then you can use something called the detectability lemma to break that into a sum of gates, sum of moment operators for gates, basically. And then once I have that sum, you're basically reinterpreting that like sort of, you kind of want the second highest eigenvalue of that sum of moment operators. And you sort of rewrite that as the second lowest eigenvalue. Second lowest eigenvalue of a Hamiltonian. And what you get is this thing. But yeah, this precise expression here comes from something called the detectability lemma, which sort of allows you to break up products in terms of these kind of operators in terms of a sum. Can I just ask a follow-up to that? Oh, please. Since there's a converse to the detectability lemma, does this mean, and you mentioned that the spectral gap may not detect everything about unitary K-design? Does that mean once you go to the That means once you go to the infinity norm, you're already sort of bound to having to bound a spectral gap. So I actually not sure I understood your question. The thing I have thought about, and you can tell me if this is what you're asking, is that if I already know something, say, from the two norm, I can use the converse detectability lemma to kind of like guess about the behavior of the spectral gap. Is that sort of what you're implying, or are you implying something different? Yeah, well, basically, I was wondering what the converse label is. Well, basically, I was wondering what the converse detectability lemma says in this context, which I guess you answered. So, yeah. I mean, yeah. So, the input info, some input information could be this two-norm behavior. Yeah, and but it's sort of something, yeah, actually, I like played around, Jonas and I played around with this a bit, and it was sort of really confusing. So, I'm actually, yeah, I was sort of unclear about what the converse detective alabilum is actually telling you about what these gaps can do. Okay. Yeah, I need to revisit that, but it's a fantastic question. It's a fantastic question. And that's definitely, I'd love, like, yeah, I really want to know. There's still some things I'm very confused about these gaps. Like you can get lower bounds, but yeah, I'd love to know what's optimal and what's possible. And maybe the detectability dilemmas, like the converse detectability dilemma, is the right approach there. Because that might actually tell you if these gaps know about some behavior. Good. So it turns out, yeah, there's this non-trivial step here, but in this upper bound here, you do have a spectral gap of this kind of local Hamiltonian. And this Hamiltonian turns out to be a 1D translation invariant sum of projectors, which has ground state energy zero and is frustration free. And I probably don't need to tell this audience much about frustration. This audience, much about frustration-freeness because we have experts listening. But frustration-freeness, it just means that the ground state of this Hamiltonian is also the ground state of each of the terms, or that the intersection of all the kernels of these projectors is non-zero. So, and you know, and then the Hamiltonian is not frustrated because these terms aren't all fighting to lower the energy. So, there's this beautiful theorem by Kanaba from the 90s, which says that a 1D translation variant. A 1D translation variant frustration free Hamiltonian, you can lower bound the spectral gap using spectral gaps on small subsystems. And I'm sort of glossing over, it is actually sort of important what the boundary conditions of this Hamiltonian are, but I can just sort of quote these nice two papers involving one by Marius, who's listening in here, or just ask a question, which says that basically this works for open and periodic boundary conditions. So, yeah, you can lower bound the gap on n for the Hamiltonian on n sites involving the gap on three sites. And just before I go on, I'll just sort of say, give a picture for what's going on, right? So I cared about properties of these like depth T random circuits, and this amplification property says, well, I actually only need to consider a layer. And really what I meant by a layer was sort of like an odd layer and an even layer, but that's fine. That's fine. And then I kind of like, you know, kind of broke up this product of gates into a sum and reinterpreted the upper bound on some on some moment on some operator norm in terms of a lower bound on the spectral gap of some Hamiltonian. And then Kanabe told me, I don't actually need to consider this like n sites, this n site Hamiltonian. I actually only need to consider sort of the gap of three sites. Of three sites, which is just like these two terms. So, sort of a drastic simplification of the problem. And this series of steps allows me to, like, I say something non-trivial here, I might be able to just boost that all the way to some nice statement about how these random circuits converge to designs. So what you can actually do is exactly compute the second moment gap. So this is for the second moment, and you get three fifths, and then this is the gap on three sides, and then you go up. The gap on three sites, and then you go up here and you notice that three-fifths is larger than one-half, and so we're done. Right, I just needed something larger than one-half to get a non-trivial statement, and that allows you to show that you know, random circuits form two designs, which we already knew. But moreover, what you can you can use some nice property of the ground states of this Hamiltonian to show that if the local dimension is larger than this function of k squared, six times k squared, then the gap. Then the gap on three sites is lower bounded, it has to be greater than three-fourths, and three-fourths is greater than one-half, and so again, we're done. We sort of prove that at least for a random circuits with large local dimension, you know, we get some conversion to designs. So you plug through this series of steps and what you show is that basically these two lower bounds or these two calculations of the three-site spectral gap, the subsystem spectral gap, give you. Or the subsistence structural gap give you conversions to approximate designs. And notably, that you know, what you can show, what we've then shown is that for local dimensions which are larger than k squared, we get this optimal design behavior, which is linear in n and k, albeit with some log q factor, which might depend on how we chose the local dimension. But we could imagine either taking it to be k-dependent or some. Imagine either taking it to be k-dependent or some like large constant, and then we get this linear design behavior up to like square root q. Okay. And I should also mention that for kind of near-term applications of random circuits, this gap approach is really nice because we just analytically and numerically can like compute these gaps. And that, and we actually care about the constants in these gaps quite a bit for near-term applications because those kind of propagate to constants in the design depth. Constants in the design depth. And if you want to, you know, if you care about something which is the Google experiment is doing, the difference between a constant of 20 and 5 million is actually extremely important, you know, sort of the difference between tractable and intractable. So it's sort of interesting that, you know, I think in the many-body literature, we only really care about the scaling of this finite size threshold, or sorry, the threshold for the finite size gap. But actually, for random circuits, these constants are sort of important. Okay, and I've sort of run out of time, and that's fine because I just wanted to overview these nice techniques for computing design depth. But I'll just mention that you can sort of apply the same techniques for random circuits to other interesting problems. So this will just take me like two minutes, I think. Yeah, so I'll just proceed for two minutes. But as you probably are aware, there have been these recent superconducting. There have been these recent superconducting qubit experiments involving 2D random quantum circuits. This is Google Sycamore and another experiment out of the University of Science and Technology in China. And they actually ran 2D random circuits and claimed that this basically, and then measured in the computational basis. And the claim was that, well, one conjecture that the hardness of random circuit sampling is relying on is that the output distribution of these random circuits is hard to sample from. These random circuits is hard to sample from on a classical computer. Okay, so what you're doing is the Chinese experiment 60 or 66 qubits? Yeah, I think it, yeah, I think they have two plots in their paper. One is 66 and one is 60. And I can't remember if, I think it might be that the error rate that I might be quoting is their 60 qubit one, but I think the title does say 66. Yeah. Say 66 minutes, yeah. Yeah, that's what I remember. That's what I said, yeah. But I think maybe, yeah, but I think it's almost more useful for, yeah, I mean, I'm not going to go into this, but I think you, the fidelity is, is sort of more non-trivial for some statement I want to make for for their 60k. But maybe I might be misquoting, but yeah, thanks for all good. Yeah, yeah. Anyway, there are a few basically, so this is definitely still looking. Basically, so this is definitely still a conjecture, and there are a lot of underlying things I would say we still need to understand to prove this conjecture. But one kind of ingredient in the hardness arguments for this task of random circuit sampling is that the probability distribution over measurement outcomes, right? So for a fixed random quantum circuit U, right, measuring in the computational basis means I get some classical probability distribution over, you know, measurement outcomes, which are like, you know, just bit. Like just like bit strings of length n. And one ingredient in these hardness arguments is that this probability distribution over measurement outcomes is sufficiently spread out. And this often goes by the name anti-concentration. And you basically care about the average variance of this outcome distribution. But one thing you can show is that sort of Can show is that sort of random quantum circuits anti-concentrate extremely quickly. So, kind of before they've scrambled any sort of local information. And you can do that using these statmic techniques. So, you can show that 1D and all-to-all coupled random quantum circuits, so both 1D and complete graph case, like achieve this anti-concentration property in log depth, log n depth. So, you know, that might have some implication for, I mean, yeah, this anti-concentration property is expected. This anti-concentration property is expected to be an important aspect in the hardness arguments for this random circuit sampling task. And one more thing is that another key ingredient in these hardness arguments is something called the white, the validity of something called the white noise approximation. And basically what you can show is that random quantum circuits scramble local noise in a very precise way, such that this white noise approximation is true. So you can also define these sort of stat mech models. You can also define these sort of stat mech models for noisy random circuits where you have these local hard random gates, but you also have like single qubit noise after the application of each of these gates. And what you can then show is that basically some white noise distribution, which is just that some linear combination of the ideal random circuit distribution plus the uniform distribution, that this approximation is basically roughly true for log n. Roughly true for log n depth circuits under some mild assumptions about the noise rate. So, you know, that the true noise distribution, at least for this local single-qubit noise model, is close to this white noise distribution in TVD distance between these two probability distributions. So, I mean, I know that was kind of quick, but these are just supposed to be sort of some more practical non-design applications of the statMech techniques to actually prove things about random. Techniques to actually prove things about random quantum circuits which are maybe relevant for these supremacy experiments. But there are a plethora of open questions that I'd like to understand, some involving this design growth and kind of like generalizing these results to other gate sets in certain ways and also understanding non-local random quantum circuits. But I'll just end there and say thank you very much for listening. Well, thank you, Nick, for that very nice talk. Are there some questions? We had a few during the talk, but maybe I can ask a question. Yes. So I'm surprised that you can show log n anti-concentration in 1D, but somehow can't do it in 2D. You would think like 2D is harder to do. Maybe not harder, but you know what? You would think you could do 2D as well. could you could do 2d as well what is the what's the yeah fan it's a fantastic point i think it's really just um uh just sort of how limited our proof techniques are i mean yeah right so 1d and all to all are sort of like the two extremes of dimensionality right yeah one is you know kind of infinite dimensional the other one's 1d so you sort of expect that everything in between should also be log depth um that's a good argument uh but That's a good argument. But yeah, I think it's really just that in 1D, you really can understand everything in terms of these domain walls. And sort of in 2D, bounding the, like you can define another set of domain wall configurations, but they're sort of like, you know, right. So in 1D, you really know that these domain walls can't cross, and that's actually really important. But in 2D, bounding all the configurations is sort of impossible because you don't have this nice 1D combinatorial problem of non-intersecting. Of non-induced. I think there is any way to argue like this. Suppose every time you add a random gate, you can only help anti-concentration. So this is a big, oh, it's a big, I have a big proof sketch. So suppose, you know, you grab your 2D grid, you approximate it as n 1D lines. Each one, I think, concentrates in log n. And then you kind of argue that if you were to couple them together, that could. That if you were to couple them together, that could only help you or something. Yeah, it's actually interesting because these something like that does exactly work for these spectral gap things. So I mentioned like lower bounds on spectral gaps, and this is exactly how you can generalize those to any graph, basically, because if you ever add a gate, you can show that this gap only needs to increase. So if you have a lower bound on the 1D gap, you can bound basically any dimension with like a 1D as a subsystem at the expense of some dimension of some. At the expense of some dim factors. So, yeah, in that case, you do pay the price of some factors of n. In your case, would you? Like, am I gonna, by breaking it up into like n 1D strips that are anti-concentrating in log depth, do I get an n factor? Which would, you know, I might accidentally be proving that it's like a linear depth or something, right? Or something, right? Maybe not, though. I agree with the intuition, though. Yeah. Hi, Nick. This is Nicole. I just have a general question, Nicole Barberis. But I had read that some of these unitary designs are used to study entanglement and entanglement spreading. Can you say just a few words about that? Because I know you're at the end of the, but. The end of the, but um, are you exploring any of those areas too? Sure, so I mean, I think uh, the best way to answer that uh question is uh, it sort of depends what you mean by entanglement, but you can definitely ask questions about like uh subsystems of states evolved by random circuits. And then I might care about the von Neumann entropy or the Rennie entropies of those subsystems and how it's like decaying, which is sort of, or sorry, how it's growing in time. And of course, you can. Growing in time. That makes sense. That makes sense. Because over the circuit, you start out with a uniform distribution, which is maximum entropy, and then you're slowly. Well, no. So I think here the question would be, I have some, say, initial unentangled state, which has, you know, maybe the subsystems there will have zero entropy because they're pure states. And then I care about kind of evolving with a random circuit, which is doing a lot of entangling. And that'll. A lot of entangling, and that'll increase the entropy. And I might care exactly what that function looks like and exactly what time I reach sort of maximal entropy. Okay, all right, thank you. And yeah, you're totally right that basically designs are sufficient but not necessary conditions for kind of maximal entropy. So you can show that once the random, these designs are powerful because they sort of give you all these things, but it could be that you actually get that property earlier than you might have thought. So if you reach a depth where you form a design. A depth where you form a design, then you might know that the entropy is saturated, but you, you know, this could have happened at some shorter depth. So sometimes, like, designs are really powerful because they give you everything you want, but you know, they might be overkill for some questions. Okay. Thank you. Very nice talk. You're an excellent presenter. Thank you. I appreciate it. But yeah, just maybe one more thing. You can actually use, you can use some of these stat techniques to actually study the entropies precisely. So you can. Precisely. So, you can actually compute purities of subsystems for these random circuits kind of exactly, and then watch how the purity decays. And that actually gives you good bounds on how the entropy grows. Okay. Thank you. That's a good question. All right. Well, why don't we, I think we can transition easily into open discussion here. So let's thank Nick again, and then we can continue talking about. We can continue talking about various things. So, thank you, Nick. And I'll stop the recording.