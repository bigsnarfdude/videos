Federico Ke Merlengi, who is at the University of Miran Bikoca, Anto√© Tamira, who is in Lugano. And Ben Laura D'Angelo is a PhD student in Padua, who is going to also join Mirano Bicoca. Looks like everybody's going Bicoca. Zosha Yu, who is a colleague of mine at UCI, and Antonia Canale, who everybody knows, is in Padua. So I'm going to talk about something that I'm going to talk about something that I've been working on lately during the pandemic, was a bit earlier, related to basically Bayesian parametric analysis of nested data. The starting point is actually, since this is a workshop, I wanted to make some comment. So if you think about clustering distribution of features and we look beyond the vision world, right? World, right? And look in particular to the frequencies world, we can see that there are indeed quite sparse, the number of approaches that deal with class distributional features in the frequencies literature are kind of sparse. Till a few years ago, actually what we could see were classing methods related with symbolic statistics, but they didn't allow a problemistic assessment of cluster uncertainty. More recently, I've seen I've seen that the problem of class distribution of features in a frequency perspective has been approached from an optimal transport perspective. And this kind of relates with Marta's talk on yesterday, right? It was just yesterday. It's many talks in these two days. So I think that probably we can also say something at this regard. And maybe later, maybe by BMP. Later, maybe by BMP 2022 or 2025, we will get maybe some talks in this regard. Was it an earthquake? I don't know. So from a patient perspective, though, we are familiar with the problem of clustering distributions. And in particular, the Nested Digital process was proposed by Rodriguez, Gelfand, and Danson back in 2008. And there has been And there has been multiple extensions, multiple uses of the NASA DG process in the literature to identify distributional groups based on the data. And here I mention just a few of the ones that I like. For example, Rodriguez and Danson, a paper that I did when I was still at MD Anderson with Rebecca Graziani, basically, but also some other paper by Peter and Zonetti and And young. So, the necessary process we know leads to a two-layer clustering. First, it allows grouping together similar distributions. That's what we call distributional clustering. And then also clusters, similar observations within each distributional cluster. And what we may call observational clustering. And we were all very happy to use this model, which I'm just going to. Which I'm just gonna present briefly. We are all familiar, but just to fix a bit of notation and the point that I want to make later. So, this is a mixture model using the nested process as a mixing distribution. So, our data come from distribution FJ. And then we basically assume, oh, this works, a mixed distribution GJ that is essentially a random. That is essentially a runbruity measure with atom GK star. And notice the GK star are also a representation as a random measure with the atoms theta LK. Notice here that the atoms come from a monatomic distribution, but also they are specific right to each GK star. So if the distribution atoms, Distribution atoms with this GK star are characterized basically by different atoms for each distribution. And we were all happy about this model, right, till Igor, Antonio, and Federico told us that there was a problem, right? So in a recent paper actually published in 2020 in Bayesian Analysis or 2019, December 2019 in Bayesian Analysis, they proved that the inference, I should know exactly. I should know exactly. But for me, you know, the past two years have been kind of blurry. So they basically proved that the inference obtained using the NASA digital process may encounter a problem of degeneracy. And the degeneracy is simply that even if two distributions share only one atom in the support, the message process basically is going to collapse. Is going to collapse basically the two distributions to the same cluster. And this is true even in the nested digital process mixture model formulation. So even not only if you use directly the NDP structure on the data, but also if you use the mixture model formulation that we have seen before. So technically, right, the partial exchange of partition probability function. Partially exchangeable partition probability function basically collapses to a fully exchangeable case when there are ties among the observational atoms. So, and one may think basically why do we care about this, right? So, I'm going to try to present today two case studies. A first case study that I'm just going to present very, let's say, in not much in detail, but just to give the idea. And the second case study. Just to give the idea, in the second case study, but instead I will present in more details where I think it will become clear that we may want to, but this is actually an important problem that we need to address when we consider distributional clusters. So the first case study relates to the study of microbiome data. Microbiome data are becoming fancy and used a lot in many you know in many uh in the literature today uh they basically uh rely um they basically um study the composition of the microbes in uh for example in our gut and try to relate uh the microbiotic composition to some clinical outcome for example um the ability to um you know um to um let's say um Um poster smaller. What's that? Okay, exactly, exactly, exactly. Be able to process more, for example, but or other or even more even more important issue, for example, developing side effects during chemotherapy, for example. So, this is this type of thing. Because microbe has been associated with the regulation of the autoimmune system. And microbio. The microbio ecologists and microbio investigators, microbiome investigators, they are really interested in studying, in particular, the richness and diversity of the microbe. In particular, the richness and diversity, more richness, more diverse, more a larger number of microbiome species and more diverse microbiome species have been associated to a healthy microbiome. Healthy microbiome. So now the point is that the microbiome data are quite challenging. And there are many ways in which they are challenges. First, we are dealing with high-dimensional data. Second, they are very, very sparse in the sense that there are many zeros into the data, right? Many micro-biospecies have been observed in some sample and not others. They are characterized by over-dispersion and... They are characterized by over dispersion and skewness, and they are very heterogeneous within and without and across subjects. But in particular, if you look at the typical characterization of, let's say, the distribution of a microbiome across two subjects, right, here in the x-axis, I have the distribution of the number of zero, of ones and two, and so on for subject one of, so basically, of Michael's. So, basically, of microbes that have been counted zero times, one time, two times, and so on. And this is a typical object that microbiologists actually are interested in to study diversity and richness. You can see that these two subjects, representative subjects, are characterized by distributions that are quite similar, right? Except for some very, let's say, big. say big for micro some micros that have quite a few number of counts that have been observed more so if you use a naive necessary process in this case these two subjects may actually be the distribution of these two subjects may actually be collapsed together or instead separated when they shouldn't right in particular this is actually what we see when we apply ina even See when we apply in a even ester process in our analysis later, we will see that these types of these two distributions of these two subjects that belong to two different populations actually are clustered together. So, this is basically what I was just to say, that in both microbiome and RNA-seq studies, the distribution profiles of sequences data are expected to be quite similar across individuals. And basically, there's going to be just a small fraction of differential abundance sequences that are going to Are going to characterize the distributional profiles across different conditions. So, in this case, Venesti Digital may provide reliable inferences when comparing distributional patterns across individuals. And I'm just going to show this slide for the approach we propose by using what I'm going to show you in a little bit. We are going to be able to actually characterize well the distributional profiles of two populations of individuals, one population of African Americans. Population of African Americans and a population of rural Africans characterized by two different types of diets: one high, one which is high in fat and protein, versus another which is high in fiber. I will let you guess which one is, right? What? So there is also a single term here, in single subject, but actually we studied as a sort of a mix of the two types of microbes in. Types of microbes in their gut, right? So it makes sense actually to create a single cluster which is separated from the others. So we were happy with this type of results. And this is the first type of example where it is important to take into account the degeneracy property of an SGL process, not just use the NSGL process as it is, although it may be tempting. It may be tempting. The second example is actually the one that I'm going to discuss more in detail today, and it's actually related with some work that I've been doing more recently on neurosciences. And there is this new paradigm in the neurosciences, which is related to miniaturized microscopy. And what this entails is basically a poor mouse is put on On the head of a poor mouse, they put a microscopy, a microscope that detects the activity of the neurons during an experiment. And I'm just going to show you the type of data that we get from this study in a simple. So this is the brain of a mouse, right? And you can see that at some point, right, if I keep this going. These going, right, you can see that there are some spikes, right, of fluorescence that you can see in the video, right? And so focus for now, so we have multiple neurons, of course. This is what goes under the name of calcium imaging microscopy. And the idea here is that we want to measure the intracellular calcium concentration of neurons in awake animals, while the animals do. Animals, while the animals do their thing, they move around the space and so on. So basically, the idea is that when a neuron fires, calcium flows the cell and produce a transient spike in its concentration. What we get is a movie of time varying flow intensities, the one that we saw before, for each observable neuron in a targeted area. And for us, what we have to deal with, this is just a small snapshot of a long time series that we record basically. That we record basically at very small millisecond scale. And we detect, we can obtain the time series of the calcium level at the different times. This calcium level concentration is just a proxy of the neural activity. What we really want to study is actually when the spikes happen and the amplitude of the spikes. So you can see, for example, here that there is this big spike. Here, that there is this big spike. It's not that the mouse got a fantastic idea and his activity level spiked at a point. There are instead multiple spikes subsequent to each other of a small amplitude. And we want to be able to also get this sort of behavior. So, there is a slow decay of a calcium concentration corresponding to a sequence of spikes. So, we need to decode. Spikes. So we need to deconvolve the calcinetes and identify the precise spike times of observable neurons. The issue that we have is also that what actually we are dealing with is a complex experiment where the poor mice has multiple stimuli. So, for example, is put in the box and has to look at different pictures or look or have different types of images that is. Of images that it is presented with, or different types of odors. So, the idea then is to try to deconvolve the spikes at activity across the different conditions. Here I have, I don't know if you can see the colors, but basically each color corresponds to a different condition. For example, natural type of scenes, or instead, for example, this white here, but it is here in the middle, is a static image. This is like a control type. Is like a control type of condition, we wouldn't want to see any spike here because we don't show anything to the mouse, right? So, is this like a liver of a neuron or is it like a liver of the region? So, this is each one of these sensors is at the level of a neuron. And so, for each neuron, we have a time series of spikes. And for now, we are just looking at the problem right from the Problem right from the single neuron perspective. One issue, as you have probably noticed before, when we were looking at the movie, right, is that the activities are very sparse, right? And it's not like there is any known network of neurons. You actually have to identify the neurons. And then, so the spatial analysis here requires some more care than typical, right? For example, for other types of brain imaging data. Does that answer? Data. Does that answer your question? So, the important point here is that we have different conditions and we want to study, for example, the distribution of the amplitude in each of these conditions. And there are some scenes and some conditions that are very similar to each other. So, this is a case where we have actually continuous data, right? And it turns out that it actually may be important to be able to categorize distributional spikes. Characterize distribution of spikes that can be similar across different conditions. And again, the use of a nest division process may not be optimal, as we will see later. Questions? Of course, when Federico, Igor, Antonio noticed the problem, also proposed a solution, and their solution was to have a class, a new class of latent nested processes. New class of latent nested processes that consider a latent mixture of shared and idiosyncratic processes. The issue with that proposal was that it was very computational complex, so you can only deal with only small data sets with few groups. Also, Mario Beraja, Alfando Guglierme, and Franco Quintana proposed a variation of hierarchical digital process. In their case, they consider a base distribution which A Bayes distribution, which is a mixture of ADP and a non-atomic measure. And to create the clusters, we consider a random partition model so that different populations could be grouped in cluster, but could be internally homogeneous. And our proposal is a bit different. And it's basically based on a silly variation, if you want, of an SD division process. So we call it Pamaton's model. The name kind of gives it away. Model the name kind of gives it away. So we consider an NSI data set. In this case, I'm going to consider the case of continuous measurements observed over J units. Here, the units can be subject in the case of microbiome, different samples in the case of genetic data, or different conditions in the case of neuroscience data. And we found out that the degeneracy can be avoided. That the degeneracy can be avoided if the prior explicitly models commonality of atoms between the groups. So, the idea here is at the first level, the data come from distribution GJ for different conditions, right? So, each sample is subject is categorized by a sample-specific distribution GJ, exactly as the previous model. And we also have the same second first level of an SC digital process. Level right of an SD digital process, where G comes from a distribution Q, and Q is basically a non-probability measure with atoms GK star, exactly as the first slide that I showed you. What we do different is simply to say instead of having theta L K here, we have only theta L. So there are atoms that are shared across all the GK star. And silly as it is, stupid as it is, this actually is enough for solving the degeneracy. Solving with degeneracy. And of course, the model is completed by appropriate distributions on the atoms, sorry, on the weights at the first level and second level, right? And in particular, notice the parameters alpha of the distribution and beta of the distribution of these two weights. I'm just making a note here because they will become important soon. And again, we And again, we probably visit was nice to see, but indeed, other people have worked with this type of object before. For example, Attis, Pyros, Nicolais, and Walker, they actually investigated the use of a common atom structure before, but they considered M known subpopulations. So they didn't consider, like we do, basically infinite mixtures here, right? Also, Also, if you are familiar and we have paid attention, these sort of idea may seem similar to the Aracho-digital process by T, because basically we have still the same atoms. But the difference here is just here. So we have Q, which is run for the measure with atom GK star. And this is the difference with respect to the RKRDP that does not allow clustering of distributional units. Of distributional units. So basically, this is a sort of a silly but new model of nested random probiotimation. Questions? And to show that everything works, I'm going to borrow the discussion with yesterday Igor tools that from the discussion with yesterday Igor started. So we can study the partial exchange of a partition probability function for this. Of a partition probability function for this process, and how much time I have. Okay, so basically, this is basically the probability of the observed allocation of the distant elements in the process, right? And we can show that the, so in general, right, for our nested, for our also for the necessity of the Also for the Venetian digital process, the exchange of partition the partial exchange of partition probability function can be expressed in this way. I want to point out for a single sample, right, this is the if you consider j equal to one, one can obtain the usual exchange of partition probability function for individual sample, which is this one. For the common atom model, just for the case of two Just for the case of two experimental units, for simplicity, the partially exchangeable partition property function can be expressed basically as a mixture of two components. This is very similar to what Federico, Igor, and Antonio found for an S-digital process, except for an important difference, which is here. In their case, when so Q1 here is a probability of two distributions being the same, G1 equal to G2. The same, G1 equal to G2, they found that, as I mentioned, if the two distributions had even one atom in common, right, this term here on the right was going to be zero. So everything was going to reduce basically to this component, which is the typically fully exchangeable case. In our case, simply, this term does not reduce to zero. Simply, this term does not reduce to zero. So this term remains positive even when the atoms, I mean, even of course, in the presence of common observations across the two samples. Questions? And we can do and can study a bit the properties of distributional and observational clustering, and in particular. Clustering and in particular, we can study probability that two distributions are the same across subjects or samples or conditions. And also, we can study the probability of a tie between two data points in two separate units. We can cluster together notice both at the distributional level, but also we can cluster together observation, allowing for boring information across the two layers. This is also quite important because the NASA digital process does not allow that. Does not allow that, does not allow cluster observation observations across groups. Oops, going too fast. We discussed a lot about correlation about random quality measures yesterday, and we saw how probably this is not the best way to describe dependence across random volume measures, but we computed it anyway at the time, and we found out an interesting And we found out an interesting aspect that the correlation actually between to run fluid images in this comradal model is actually bounded below by one half, between one half and one. Why is this? Well, because of a community of the atoms. So that creates a necessary dependence between distribution. Is this a problem in our case? For example, in the example of microbiome that I discussed before, it is not really a problem because It is not really a problem because we actually expect the distribution profiles to be very similar across subjects. And so we thought that this was a good, I mean, could work. And also, notice that also for the NASA digital process, right, the correlation actually is not, I mean, is depend on the parameter alpha, which characterizes the first level of the hierarchy. Level of the hierarchy. So we can, of course, extend our framework and instead of considering the model simply the Y's as a random sample for the G's, we can just consider a non-parametric mixture model, right, as we are used to. And this is actually what we do. And in particular, the type of development has been done mostly by, I mean, Mostly by, I mean, I said completely actually, by Francesco Denti and Feduke Merlenghi. You know, I'm an accessory in most cases, and it's been recently accepted to JASA. This is the paper where we actually developed the microbiome data example. In the follow-up paper, actually, we discussed and we thought that this type of model could also be useful for the distribution, for the studying the distribution of the amplitude in Carlson flow. The amplitude in calcium fluorescence data. How much time do you have? 15 minutes. Okay, perfect. I think I have more than enough. So an important point that I made before and I want to make again is that in these cases, we observe the casino traces, but these are not the target of analysis. What we really want to study are the spikes, times, and the amplitudes. And we have a problem of deconvolving the calcium level intensities of what we see to find and to understand how many spikes we see, for example, in a small interval, right? And their amplitude. So we have some limits of a technique because, of course, there is a lot of measurement noise. There is the issue that I told you before that again, we don't have a super spike. Again, we don't have a super spike here, but we have multiple activity spikes in this interval, and we need basically to capture the presence of those spikes and the corresponding amplitude. But also, this is just a snapshot of a long time series. So, we also have a computational issue that we need to take into account. So, what we want to do is to identify the spikes times, so discriminate. Five spikes times, so discriminate between signal and noise, estimate the distribution of spikes amplitudes, in particular in the different conditions, and take into account basically the effect of a different stimuli on the distribution of spikes amplitudes and see which distribution of differences we can see. The model that we are actually going to use to tackle the problem, the basic model, is a calcium concentration model that has been present in the literature, neuroscience literature since some Literature, neuroscience literature, since some time for some time now. And it's a very simple model that has been used also by others, for example, Danira Witten and others, that have addressed this problem from a frequency perspective. Here we have that the observed trace is basically related to an underlying AR latent model to describe the true calcium concentration CT. And if you look at And if you look at the AR1 model here, we have basically that the AR1 model is extended, let's say, by considering a spike AT. So in the absence of neuronal activity, we should expect AT to be zero. And so we would have only the AR1 process that describes the usual decay of the calcium concentration over time. Casting concentration over time. So, or more than you can, the usual time dynamics. See, if there is a spike, instead, AT is going to be positive. This is actually what you're interested in also to study these amplitudes that determine basically the extent, the amplitude, the level of the spike. Of a spike, and again, we do not expect these amplitudes to be somehow regular. So, we want to characterize the neural activity under different experimental conditions, as I mentioned before, because we have different types of scenes, natural, different types of natural scenes. And let me see here because that's natural moving, natural scene, right? And below, everyone is. Right, in the lad everyone is study breaking here, right? And have different conditions. Did I kill anybody with this? No. So we want to characterize the neural activity at time t under each condition. Sorry. I promised that this is going to be entertaining and Entertaining and okay, so we're going to just use our framework, right? We want to consider in this case the spikes AT as coming from a stimulus-specific distribution, let's say GJ. And you got it. Basically, we want to be able to characterize the distribution with each experimental condition, possibly borrowing information across groups. And we also want to cluster the Also, we want to cluster the amplitude across and within distributions. So, try to discover similarities in the activation response to different stimuli. And we're going to do this through a slight variation of the common atoms model that we have described, just to fix the idea once again. So, we have different conditions, and we believe basically that the distributions across conditions. Across conditions are described basically by stimulus-specific distribution Gs. And eventually, we want to study when these distributions may be similar, right? By using a common atom model that basically discriminates across the distributions only by the weight that we put on a common set of atoms. Articles. Oops. So what we do differently in this case is that we need computational efficiency to tackle the problem that we are looking at long time series in this case. So instead of using the fully non-parametric framework, if you want, that we had before, where we're considering a possibly infinite number of Infinite number of clusters, and we are actually using slice sample techniques to be able to do posterior inference in the MCMC. We are going to just be using the generalized mixture of an admixture framework of Rubijin Arter and Massiner-Wally, also published recently in 2021. You can see that being an editor has helped me trying to understand the literature and see the literature. But so we basically have the same structure again. Have the same structure again. This time we consider k a finite mixture of a possible distribution g k star that identify the clusters of distributions across conditions. And we basically use for this model the generalized mixture of finished with level the generalized mixture of finite mixture framework of Fluvich Nature. So basically we have a Dirichlet distribution and we basically use a beta negative binomial framework, a beta negative binomial distribution to To make inference on K. We didn't worry about the number of components or number of clusters with time, right? Like, so we didn't try to connect with the talk this morning, but we didn't really think through that, right? So, and for the distribution atom GK star, once again, we are basically just considering. Once again, we are basically just considering a set of common atoms across the distribution G1 star, GK star. And these common atoms, these amplitudes are obtained as IID draws from a base measure of D0. What is important here is that if you remember, our problem is actually to determine when we have a spike. We have a spike in the common formulation, right? So far, I haven't really told you anything about the AJ star, right? So we want to be able to have a sparse representation of positive amplitude. And of course, you see where I'm going here. Basically, we are going to basically use a model that allows the AJ star basically point mass at zero here, to have point masses at zero. And that's actually what we do to enforce sparsity. What we do to enforce sparsity. Basically, we use a Spikens-Lab sort of model, essentially, where we have a point with the base measure. It's an inner Spike and Slab model because we're assuming that the base measure in the Kommeratom model formulation has a point mass zero, and then we're going to use a gamma distribution to describe the positive spikes. In particular, we're going to choose the gamma parameter. particular we're going to choose the gamma parameters so that these spikes are going to be far sorry to the amplitudes we're going to choose the gamma parameters so that the amplitudes are going to be far from zero right so that only large amplitudes larger than zero are actually going to be chosen so and this is kind of a sort of a mimicking the behavior of a non-local prior densities and so we choose the parameters of a gamma so to be to allow us To allow us this behavior. Okay, so I have five minutes, I think. Correct? Perfect. So I'm on time for once. So we have forest mental conditions. And again, we have different stimuli, static grading, natural scene, natural movie. And we also have a period of spontaneous activity that you want to. To be able to capture. In terms of the inference, right, first you may see here, probably a tone of a sea better than here, the yellow spikes here. So the yellow terms here, these are basically the spikes, the corresponding amplitude, right? And of course, there is an issue of time resolution here. Of time resolution here, I should probably create a and in terms of clustering assignment, right? If you're going to try to get a cluster assignment here based on our posterior based on the MCMC, right, we can see that we can find through actually using the variation of information criteria by Sara Weden and Garamani that we're able to identify three natural clusters. Identify three natural clusters. One cluster actually puts together natural scene in natural movies, which is kind of understandable. The second cluster basically captures the static gradient. And the third cluster, quite interesting, captures the absence of stimuli, which is our null condition. In terms of distributions, right, we are going to look now at the distribution of the spikes amplitude, in particular, static grading, natural scene. Static grading, natural scene, natural movie are here. And again, the color choice is probably not the best for these representations here in person. But basically, again, remember, natural scene and natural movie are actually clustered together, right? So, and it looks like this is actually captured in this marginal representation that I'm showing here. The static grating instead of the distribution is quite different than the others. Different than the others. And in particular, we can also see these from another point of view: the so-called estimated firing rate, which is an estimate of a number of spikes per second. You can see that these numbers, the estimated firing rate is quite different indeed between the first and the other two. Of course, these are proxy. Again, remember that based on our inference, these two conditions are characterized more or less by this. Um, are characterized more or less by the same distribution, but we can cluster them together. The other point that we can see is the spikes amplitude. As I mentioned before, we have the issue, right, that we have to deconvolve the calcium traces and the neural activity. And quite interesting, again, our model, except for the use of the old calcium concentration model that we showed before, right, doesn't really doesn't really have anything in it to allow these, but we are able to capture multiple spikes at subsequent times, characterized by relatively small amplitudes, which is quite nice to see. So this is in submitted work with Laura D'Angelo, who did most of the work, all of the work again, and Antonio Canari and I were just We're just looking at her Zoom sharing most of the time. So, anyhow, so I thought that this was an interesting type of model to present and more actually than what we've seen so far, right? We can, I think, think about the issue more generally of clustering distribution. Generally, of clustering distributions. In particular, for the type of applications that we have discussed here, so we discussed the come in the context of, sorry, here the fluorescence data. And here we wanted to class the spikes over multiple conditions. We are looking at only single neurons, whereas instead it would be interesting to look at multiple regions, right? But this is compounded, the problem is compounded by the type of data that we have. Type of data that we have, but also by the computational complexity as well. The other point that I want to make is we can also, of course, add in covariates, right? And there is also a lot of covariates that are available on, for example, the mouse movement throughout the environment. And these are typically not taken into account and they could be taken into account. But in general, I think that the problem of clustering distributions is quite. Of clustering distributions is quite interesting. And this was just a simple modification of a well-known nested digital process that seems to be effective. And that's it. Thanks. So, you're going after the marginal distribution of the amplitudes. Could it be an adjunctive field to try to consider like joint distributions? So, for a situation where the mouse gets first shocked, gets first startled, and then in choice, like when a gueta goes off or something, we can't represent that, right? Because you assume that the E sub G, or you assume that the amplitudes are IID from the same condition of G. idea from the same condition given the given condition correct so yes that's another another in so in this in this work we're basically just being very naive right in the sense that so if i understand your question correctly right that we were basically just considering the conditions as given basically we were basically having the gj condition on describing the different conditions right is that uh w what you're getting at okay So this computationally, how does it scale if I like if I have lots and lots of my oh so this is the issue right and that so probably you have so on one hand from a modern perspective what we really did and I'm ashamed to save it this but basically what we did was simply just to change the nested nested Nested process framework, right? By considering simply common atoms, right? So it gets so computationally is as challenging, if you want, as the next usual process, probably a bit less because of the common atom structure, right? But still, the bulk of the work, which is the cluster assignments, is still there. On the other hand, right, the usual answer is there is Answer is there is a lot of variational approaches that can be used to make these computationally more scalable. I think. I know, simple answer, but I have one more question. Why should you consider the finite version of the model and not the English? Oh, because yes, that's actually connected with this computational scalability in the second one, right? We wanted to just make Just make sure that if we had used in this case possibly infinite number of components, sorry, for the slice sampler, right, still it would have been computationally intensive. So by this way, I mean this method by Hugh Natural with telescoping sampling actually worked well in this case. Well, in this case, so they actually have a specific algorithm that we follow. As I can see, you didn't consider mixtures, so you don't have a kernel on top of that? We do, actually, yes, we are. So the kernel, right, is actually given by this model, right? Baby's model, right? So how do you do the inference for the spikes? Do you do like a post-MCMC analysis? No, no. CMC and all these things? No, no, it's the inference for the spikes comes. That's also another computational complexity, right? We're basically, more or less, right? We're basically using the posterior probability, let's say, of inclusion, right? From the spike on the spikes of the smart. Yes, I was cheap here, and probably I was not completely clear, but basically, what you Not completely clear, but basically, what we do is if AT is equal to zero, there is no spike. If AT is positive, there is a spike. So I was trying to go for the cheapest way to get out of the problem. Okay. So