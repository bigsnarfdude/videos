Many faces, even so over Zoom, but it's a great and interesting group. I'm very excited to present to you today new work from my lab on legging circuits in recurrent neural networks. This work is led by Chris Wendin, who is a postdoc in the lab. And first, in this presentation, I would like to mention some of the problems we discovered using conventional dimensionality deduction methods to understand the complex mixture. The complex mixture of behaviorally relevant signals in neural activity. It could be neural activity from the brain or neural activity in training record neural networks. And to overcome some of those difficulties, we developed a new approach, which we call a latent circuit inference, which is an inference operating circuit mechanism from neural responses during cognitive task. And in the last part of the talk, I will show you how we tested this new approach. This new approach using the card neural network models of context-dependent decision-making. So, to start off this talk, I would like to first ask a question of what do we expect from a successful theoretical framework where we can say we have an understanding of how a system functions. There are such successful theoretical models in neuroscience, like here is one example, which is a Which is a somatogastic ganglion circuit of a lobster. It is a very well-studied system, which consists of a few nodes, few neural nodes, which interact with each other. And in this circuit, we can record activity from every of these nodes. We know the connectivity, how these nodes are connected to each other. And we have good models to describe how the patterns of activity of. How the patterns of activity of this circuit arise from the circuit connections to produce this pyloric rhythm, which is a function of the circuit. It's a digestive system. It needs to generate periodic oscillations. And the advantage of putting all these components together, activity, connectivity, and function, is that we have a full understanding of the system. And first of all, we can ask new scientific questions just as Scientific questions just as the questions of parameter exploration in this model. And moreover, we can test our theory by predicting outcomes of perturbations of the system. For example, in this case, we can change the temperature of the environment, and we would be able to predict how the function of the circuit, such as this rhythm which the circuit produced, will respond to the change or to this perturbation. Or to this perturbation. This is the ideal level of understanding we would like to achieve. However, when we think about cortical circuits, we are not there yet. Here is an example of how we can model cognitive functions. I would particularly focus today on decision-making. In decision-making behavior, we get a stream of sensory stimuli and we need to produce a categorical choice. For example, based on the pattern of moving dots on the screen, we need to pick left or right. We need to pick left or right target depending whether the net direction of motion is to the left or to the right. This type of behavior can be modeled with neural circuit models, which are rather classic models, which consist of just few nodes of excitatory and inhibitory neurons, which interact with each other through very interpretable connectivity. And here is one of the well-known model neural circuit models for decision making. In this system, Making. In this system, two excitatory populations compete to produce a categorical choice, which will be indicated by elevated firing rate in one of these populations. In this simple model, we have the same level of theoretical understanding as I showed you before in the lobster example. Because using dynamical systems theory, we can directly analyze this system to understand how these activity patterns arise. How these activity patterns arise, and they arise from the attractor structure in the phase space. And we can exactly understand how the pattern of connectivity is related to these dynamic attractors and how it will affect the function of the circuit, which is the ability of the circuit to produce decision-making behavior. These models are very useful. For example, we can also make testable predictions, such as if we change the excitation-inhibition balance in this circuit. Balance in this circuit, this model is capable to predict how the decision-making ability, the speed, and accuracy of decisions will change. This is very powerful. However, you can already realize these models are very simplified. They have just, in this case, three nodes, and every nearing in, let's say, population one, have exactly the same response profile. It elevates firing rate to a specific choice. However, where we will However, when we look in recordings from the brain, typically we find that neurons multiplex many different kinds of signals and information on the single cell level. To give you an example of this mixed selectivity, I will use a task which is by now is also become classic, very well studied, which was pioneered by Valerio Monte in 2013. It is a context-dependent decision-making task. This task is a This task is a little bit more complicated because the stimulus has two features, color and motion direction of the dots. And there is a cue which indicates which feature you should be paying attention to. In the motion context, you should discriminate left versus right motion direction of the dots and leave no color. But in a color context, you should be able to detect whether there are more red or green dots in this stimulus. In this stimulus. As shown in this example, there is this complicated situation, ambiguous situation, where, for example, dots move to the left, indicating left target, but dominant color is green, indicated you should pick the right target. So on these complicated trials, you really need to use the context queue to resolve the ambiguity. If you look in neural recordings during this task, we will. During this task, we will find that in prefrontal cortex, neurons multiplex information about all the task variables. This is a figure from the very same paper where there are multiple example neurons from prefrontal cortex, and their trial average responses are shown sorted by different task variables, for example, by choice or sorted by motion and choice in one context or another context. And as you can see by this, And as you can see from this figure, these neurons modulate firing rate of their responses split and separate according to all these task variables on the level of single neurons. This is a complexity and heterogeneity, which at this point is not matched by simple circuit models. And therefore, another approach emerged to understand this type of mixed representations using trained recurrent neural networks. In this approach, we can In this approach, we can take a neural network which consists of many nodes connected to each other, and we can use machine learning approaches to optimize the connectivity of this network to produce the desired task behavior. And it turns out that in these networks, neural responses show very similar response properties in terms of their complexity as we see in the cortex, which seems like an advantage. But on the other hand, because this network, a high-dimensional network, Because this network is high-dimensional, they consist of many units with complex responses and complex connectivity. We cannot just easily apply the same dynamical systems tools which we have in small circuits to understand how connectivity of this network give rise to their dynamics to accomplish the function. So, usually to understand the complex neural recordings and recurring neural networks, we rely on variance-based based dimensionality reduction approaches. Based dimensionality reduction approaches. In this case, we think about neural activity as a trajectory in the high-dimensional space where each neuron corresponds to a separate axis. But we think that because of constraint imposed by the task, this trajectory is confined to a low-dimensional subspace. So we are looking for a projection Q to a low-dimensional subspace where different axes correspond to task y. Axis corresponds to task variables. In the case of context-dependent decision-making, this would be motion and color, feature of the stimulus, context, and decision. So when we perform these dimensionality reduction methods, we obtain this matrix Q, which gives us the projection from high-dimensional space to low-dimensional space. And this matrix allows us to understand how different kinds of signals are mixed on the level of individual need. Mixed on the level of individual neurons. In particular, if we look in one column of Q, for example, column representing motion direction, we will get a direction in a high-dimensional neural space which mostly correlates with motion. Or you can think about it as an activity pattern, which represents the motion stimulus on the level of population. And if we look across rows of Q, we will see how different kinds of signals are mixed. Kind of signals are mixed on the level of this particular nearing. For example, this nearing I highlighted here will be modulated by both motion and choice information. Well, this is great, but at this point, we only use statistical approach to find these directions. I would like to go and ask, do these directions which we identify have behavioral relevance? Do these directions really drive behavior of the circuit? To test this idea, To test this idea, we decided to train a recurrent neural network to perform the context-dependent decision-making task. And here is a psychometric function of the network, which shows that the network is indeed implements the task correctly. I show you only psychometric function on color context trials. And you can see that when this plot probability to choose right based on the color feature of the stimulus, the network shows very steep psychometric curve, but it is influenced. Curve, but it is insensitive to motion stimulus because motion is irrelevant in this context and the network is successfully ignoring the motion. And this is the connectivity which we obtain from this network. So again, in this network, we can apply one of the existing dimensionality reduction techniques, for example, GPCA, to obtain a set of axes representing different task variables. For example, we can obtain a context direction. In a context direction and project activity on this direction. And you can see that activity nicely separates according to the context which was shown on each possibility trial. We can also project on choice direction, and you can see that the neural trajectory will separate according to the choice. Similarly, when we project on motion and color directions, we obtain a set of neural activities. And an interesting finding from previous work was Finding from previous work was that irrelevant stimulus feature, although it doesn't drive behavior, as I showed you on the previous slide, it is still represented in network activity. You can see it, for example, focusing on this projection on the color axis. The red to blue lines show the motion context trials, but you can see the trajectory still split according to the color information. So the color is still. So, the color is still present on the level of neural population and can be decoded. And it becomes a puzzle: how is it possible that sensory information, which is irrelevant, which is encoded by the network, nevertheless doesn't drive the choice? Well, before asking this question, maybe we should ask ourselves to what extent these directions which we identified are actually expected to drive behavioral outputs because we didn't have any mechanistic description. Had any mechanistic description of trajectories, we just captured some variance in the data. So, one way to do it would be to perturb activity of the network along those directions according to patterns which we inferred in the projection matrix queue and see what would happen. One prediction would be that if I activated motion context direction on color context trials, because I'm messing up with the Because I'm messing up with the context mechanism in the network, I should make the network sensitive to the irrelevant motion feature of the stimulus. However, when we try this in our simulation, it is not what happened. If we simulate the network in a pattern of activity identified by DPCA as a motion context direction, we don't see any behavioral change in the psychometric function. So it seems that So, it seems that when we use dimensionality reduction, which is purely based on capturing variance in the data, this variance does not necessarily need to be behaviorally relevant or really relate to the mechanism which the network is using to solve the task, which is in contrast to simple circuit models, which provide this link between activity and behavior. This motivated us to think about imaginary. us to think about imaginary reduction methods one step further to maybe bridge between these two perspectives on one hand understand mixed selectivity of heterogeneous neural calculations but on the other hand also provide mechanistic description as it allowed by the by the interpretable neural circuit models so in our latent circuit inference approach we also seek to find projection We also seek to find a projection of high-dimensional trajectories on a low-dimensional subspace, which I denote here by X. And the objective function for our optimization would be very similar to the usual dimensionality reduction methods, where we try to find a projection in which we get the best fit of trajectories within this low-dimensional subspace. However, in addition, However, in addition, we require that latent variables against which we perform this regression interact with each other in what we call a latent circuit. So those latent variables x are not arbitrary regressor factors, but they are dynamical variables which are part of a neural dynamics with a latent connectivity w, which we would like to infer simultaneously with the projection matrix Q. So in the So, in other ways, our latent circuit approach doesn't only infer the embedding matrix as the usual dimensionality reduction methods do, it also has a latent circuit dynamics component, where the latent variables are constrained according to the dynamical systems equation of a neural network. And additional component which we infer is the connectivity of this. Is the connectivity of this latent circuit or latent connectivity WREC, which will constrain how these variables interact. So when you think about mixing of neural signals on the level of single, of different behavioral signals on the level of individual neurons, just like usual dimensionality deduction methods, we have mixing because of different contributions from the embedding matrix Q. But in addition, because this But in addition, because these latent factors interact with each other through circuit dynamics, there will be additional mixing. For example, in a circuit model, the context node can inhibit or suppress another sensory node, and this will result in time-dependent modelation, which provide additional source of mixing of different kinds of information on the level of individual neurons. When you think about this latent connectivity matrix, one usual way to Activity matrix, one usual way to visualize it would be really like a simple circuit diagram, which you can interpret and understand how the dynamics results from excitatory and inhibitory interactions in a network. To test this idea of inferring interoperable circuit mechanisms from neural trajectories, we decided to use recurrent neural networks trained to perform behavioral tasks. Because in these networks, we have full access to the activity and Full access to the activity and connectivity, and we can perform different perturbation experiments to test the latent circuit inference. In the context of recurrent neural networks, now you can think about latent circuit inference as embedding of a low-dimensional latent circuit in a high-dimensional space of activity of a large recurrent neural network. We do this embedding by finding the mapping between trajectories of a large neural network. Between trajectories of a large record neural network and a small latent circuit. And this mapping, similar to conventional dimensionality reduction methods, provides us mapping between each node in a latent circuit, an activity pattern in the recurrent neural network, which corresponds to one of the columns of the embedding matrix Q. In the context of RNNs, we can take this argument one step further. One step further, we can take the mapping of trajectories and differentiate it to obtain the mapping of vector fields between the small circuit and a large recurring neural networks. And by driving this calculation a few steps further, we can derive a relationship between the latent connectivity structure, double recurrent small, and the connectivity of the large recurrent network. So, what this theory predicts that So, what this theory predicts is that if we do find a good mapping between trajectories of a larger NN and a small circuit, then in this case, there should be present a latent low-dimensional connectivity structure in the connectivity of the large network, which we could reveal by conjugating this connectivity with the embedding matrix queue. This relationship is also very useful because it allows us to map not only It allows us to map not only node on an activity pattern in a large RNN, but it also allows us to map between connectivity of the latent circuit and a large RNN. In particular, we can derive that one entry in the latent connectivity matrix corresponds to a pattern of connectivity in recurrent neural network, which is given by the outer product of two corresponding columns of the embedding matrix Q. Embedding matrix Q. So, this would suggest that if we perturb the connectivity of the latent circuit, for example, this particular connection, then corresponding perturbation in the large RNN would be given by this pattern of changes in the connections between recurrent units. All right, so this is the theory, and then we wanted to test it how it works using recurrent neural networks trained to deform context. Works trained to perform context-dependent decision-making tasks. So, as I told you, after we train the model to perform the task, the model exhibits rather complicated connectivity structure, and it's not really obvious how this connectivity generates behavior of the task. However, we can apply the latent circuit inference to learn this latent connectivity structure from neural tragedy. Connectivity structure from neural trajectories. And for this specific model, this is what we obtain. When we perform the inference, we simultaneously infer the embedding matrix Q as well as the latent connectivity of the circuit, which consists of recurrent connectivity, inputs and outputs. When we look in this latent connectivity structure, it suggests a relatively interpretable mechanism for how the model can solve the task. In particular, In particular, we see inhibitory connections shown here in blue, which go from the color context node and target two sensory nodes representing motion. So in this mechanism, the relevant motion information will be just suppressed by the context node. And similarly, the motion context node targets sensory populations representing stimulus features. Stimulus feature. So, this simple circuit mechanism is based on suppression of irrelevant stimulus information. And then, the checkered board pattern of connections in the latent circuit shows here corresponds connection from sensory to choice populations with a corresponding choice preference. For example, sensory population representing right, color, and motion stimuli project to the choice right node, and the same is true. Note, and the same is true for the left node. Maybe to make this circuit mechanism a little bit more clear, I laid it out as a circuit diagram here. So, this is a pattern of connections which we see in our latent circuit. And then when we look in these connections going from sensory to choice nodes, these are connections from right motion and right color populations to the right choice and left motion, left color to the left choice. And when we look in this. Um, and when we look in these contextual connections, we see that the color context node just suppresses all motion information and motion context node suppress color information. Well, this circuit mechanism is very interpretable. We can understand it. But so far, we didn't really prove that this is the exact mechanism which the large RNM is using to solve the task. It could be that the simple small circuit can implement Circuit can implement context-dependent decision-making only in this simple interpretable way, but larger and using a different mechanism, and this is just the best approximation which the small circuit could find. To test whether this is true or not, we can use our theoretical relationships between connectivities of two networks to ask whether we can find this latent circuit connectivity structure within the connectivity of the larger network. Of the large charner. So, what we would do, we would just use our embedding matrix queue to conjugate the high-dimensional connectivity of a large RNM and compare how it relates to the low-dimensional connectivity of the latent circuit. And when we do it, we find a very good correspondence with very high correlation between the inferred latent connectivity structure in the RNN and the latent circuit with the same pattern of. With the same pattern of connections, which are the signature of the interpretable circuit mechanism, which we found. We can further ask whether we can see the signatures of this circuit mechanism in activity of RNN by projecting this activity on directions given by our embedding matrix queue. In this case, it's very similar to how you would use a DPCA, for example. In DPCA, you would also project activity on some low-dimensional space. Activity on some low-dimensional space. The matrix Q, however, which we find is different than the one you would get by applying DPCAs. So we would project the same activity, but on different set of directions. So when we do this, we can also project activity along the context axis and choice axis, which are identified by our method. And you can see that activity will separate according to the motion of color context, and it will Color context and it will also separate according to the left or right choice. But what is interesting is that when we project activity on color axis, for example, you can see that in motion context, which is shown here with a solid line, where color information is irrelevant, we see a clear suppression of the color information. This is a picture different from what would PCA suggest you. However, this picture is consistent. However, this picture is consistent with the latent connectivity structure which we identified in our latent circuit. And similarly, motion information, when it is relevant in color context shown here in dashed line, is also suppressed in this projection. So in this way, the circuit mechanism, which we identified in the latent connectivity structure, is consistent with the picture which we obtain by projecting dynamics of this network on the set of dynamics. Of this network on the set of directions identified by the matrix Q. But of course, the ultimate test of the circuit mechanism should be perturbation. We should be able to perturb activity and connectivity in the large RNN and obtain very clear changes in behavior of the network. So far, we are not aware that there is any technique which would allow us to identify patterns of perturbations to the network connectivity. Perturbations to the network connectivity, which would predict the behavioral outcome of these perturbations. And therefore, I will focus on showing you perturbations of the latent connectivity. So, the latent circuit mechanism, first of all, can suggest what should this perturbation do. For example, here in this perturbation with highlighted with two red squares, I would weaken the inhibitory connection from the motion. Inhibitory connection from the motion context node to the sensory nodes representing color. It is a similar perturbation which I used in the beginning of the talk, but there we were perturbing activity and here we are perturbing the connectivity. This perturbation is expected to weaken the efficacy of the motion context mechanism and as a result makes the network sensitive to the irrelevant color information on the motion context trials. And this is exactly what we see. And this is exactly what we see in the latent circuit. The latent circuit is a small but still functional model of the same task. So we can ask directly what happens in the latent circuit to its behavioral performance when we do this perturbation of connectivity. And you see that the more we weaken this connection, the more the network becomes sensitive to irrelevant color information. But what is especially interesting is that using our relationship between connectivities of a small and big network, Connectivities of a small and big network, we can translate this perturbation in a patterned perturbation of connectivity in a large RNN. So you can see that this connectivity is very distributed across the entire network. And now when I'm going to do the perturbation, I will not use any I will not make any further use of the latent circuit. Latent circuit just suggested me to perturb this particular pattern of connections, and it predicts that if I do so in large And it predicts that if I do so in large RNN, I should expect the very similar change in the behavior of the network. And this is indeed what we observe. When we apply this pattern of perturbation, we can make the network sensitive to the evolving color information. And it's not unique to perturbation of the context. Context note, we can perform all different kinds of perturbations. Here is an example, another example where we can weaken the exact three connection. Can weaken the exact three connection from the node selected for the motion left stimulus to the node representing left choice? And if we do so, we expect to decrease the proportion of left left choices only on motion context trials. And this is exactly what we see, plotted a choices to right. And you see that the network is really becoming biased towards right choices when the perturbation strength is strong. And it happens exclusively on motion context trials. I didn't show you how. Motion context trials. I didn't show you color context trials. And when we translate this perturbation to the larger and m, we perturb this particular pattern of connectivity and obtain very similar results. So at this point, we validated the latent circuit inference, can indeed find behaviorally relevant patterns in activity of large RNN, and we can validate the inferred circuit mechanism both by obtaining the lignitive activity. By obtaining the leaving connectivity structure in the large RNN, also in the RNN activity, and also the adaptations of activity and connectivity in a large network. So lastly, I would like to talk about how unique is the circuit mechanism for the context-dependent decision-making, and also how unique is the circuit mechanism which we infer from the activity of one specific RNA. To answer this question, what we did, we trained 500 recurrent neural networks to perform context-dependent decision-making tasks to the same level of accuracy. And then, on each of those recurrent networks, we perform multiple circuit reductions initialized in random ways to obtain multiple latent circuit solutions for each of those recurrent networks. Then we took latent connectivity matrices of each of those circuit reductions. Each of those circuit reductions and just performs the principal component analysis to visualize the diversity of latent circuits which we discover. So, as you can see, there is quite a bit of variability in the solutions in the recurrent connectivity of these networks. But nevertheless, this variability is interpretable. And also, an important point that if we consider one individual network and perform multiple reductions on the same network, we find a very tight cluster of solutions. So, it seems Clusters of solutions. So it seems like for each individual RNN, we do find the same latent circuit, but across RNNs, there is some variability in this latent circuit. To interpret this variability, we clustered these solutions in just three clusters. And I will show you the mean connectivity pattern for each of those three clusters. So the major cluster, which we have here, where the most networks resided, corresponded to the First, recited corresponded to the exact same latent circuit solution which I explained to you in the earlier slides based on suppression of the relevant sensory information. When we look to the mean of the cloud number two, we find a slightly asymmetric solution. It uses the same suppression-based mechanism, but in this case, you see that one of the sensory population has slightly stronger recurrent self-excitation. And to counterbalance this, the network uses. Balances, the network uses slightly stronger inhibition both in the context and recurrent connectivity. And the mean cloud 3 has very similar pattern, but biased towards different population. So overall, although there was some variability in the space of the latent circuits, which we infer, all of them use relatively simple and interpretable mechanism based on suppression. So at this point, I would like to conclude. Would like to conclude. So, what I showed you today is that we developed a computational framework for inferring a low-dimensional connectivity structure, which we call the latent circuit, from high-dimensional neural responses. This approach allows us to predict changes in task performance under perturbations of the network activity and connectivity. We also found that large RNNs implement simple and interpretable circuit mechanisms for context-defending. With mechanisms for context-dependent decision-making, and that across a large variety of networks trained to do the task, we identified very similar mechanisms. This last point suggests that utility of recurrent neural networks may be not in discovering new mechanisms, but rather in developing our understanding how a simple circuit mechanism can be distributed across large networks. So, at this point, I would like to thank everyone. So at this point I would like to thank everyone in my lab, in particular Chris Langdon, who performed all this work. And I would also like to thank our funding sources. And I'm happy to take any questions. All right. Thank you, Tatiana, for a very nice talk. I'll just read the questions in the chat. So Sir John, and then I guess Karina had the same question, asks, is the latent connectivity constrained to be constant across content? To be constant across contexts? Yes, the lacking connectivity is treated as a connectivity. It is not functional connectivity. We think about it as a structural connectivity. So for all task conditions and for all trial types, there is a single connectivity structure which give rise to the latent trajectories. And then Christina Saban has Christina, Savan has a rather lengthy question. Maybe Christina, do you want to unmute and ask? Yeah, so I was a little bit confused about what makes the latent dimensionality reduction result in individual latent dimensions that are interpretable. In general, that's not the case. So if you think about an LDS kind of projection, there's a degeneracy in the model that says that any rotation of the entire space is equal to Of the entire space is equivalent. So I was just wondering, and I should apologize, I missed the very beginning. So you probably said this, and I just didn't hear it. What makes the individual axis have a meaning in the task space exactly? Yeah, so maybe this can help to understand because when we use this latent circuit, latent circuit is also a model of the same behavioral task and it receives. Behavioral task and it receives task-related inputs and it projects task-related outputs. So the identity of the nodes is pretty much defined by what type of inputs they receive or what type of outputs they produce. For example, context nodes will receive context input. So the same way when we model BKRNN, we provide the network with various inputs, which are task-relevant variables. The same inputs also The same inputs also receive our latent circuit receives the same kind of inputs. Okay, so pragmatically, this results in regularizing the W in and the W out to have a unique solution. So like zeroing out a whole bunch of stuff in W in and W out. We don't need actually, because in Large and the inputs, if we just used, let me find the relevant slide. Maybe here. On slide, maybe here. So, if we use just a diagonal matrix to represent inputs and outputs, we would be zeroing out a lot of stuff. But it's obviously not always the case in a RNNs. In a RNNs, different inputs do not need to be orthogonal to each other, right? So, in principle, and we also did this, we can learn of diagonal entries, but those entries could be also inferred and they can. Also inferred, and the correlations between inputs present in the large RNN will be inherited by the latent circuit. So, I guess the regularization here is not necessarily, well, then if we don't constrain the problem anyhow at all, then we still deal with non-identifiability because we can swap the noise and latent circuit and it would be still equivalent circuit. So, what we do, we can prime the circuit initially by training it without allowing any correlation. It without allowing any correlations between inputs, and after the nodes gain their identity, then we can enable to learn the correlation structure, and then we can infer the correlation structure between inputs, which was present in the large RNN. Got it. Now, I'm asking because I just wonder what it would take to use this methodology on actual neural data. Because, in general, there you don't get interpretable axes. Yeah, we have some thoughts about it, but it's still. Yeah, we have some thoughts about it, but it's still work in progress. I thought it's a little bit too early to talk about it. Thanks. Right. A few more questions. So I'll ask, Adrian Valente asks, can those different mechanisms be distinguished behaviorally, for instance, on edge cases or the modified task? I have to say, we didn't think about this very hard, but when we train RNS, But when we train RNNs, we require all of them to perform the task to the same level of accuracy. So if you look just in like very simple tasks, like how do optimometric functions of this network look, they all look the same. But circuit mechanism is slightly different, right? So at this point, it's something I'm not ready to answer precisely. Maybe there are corner cases which you can decide, design to distinguish between these different. To distinguish between these different circuit mechanisms, and maybe they differ on the level of neural trajectory, which suggests they are, because for different networks, we do infer different weighted circuits and we infer them from trajectories, which suggests that there is an imprint of these mechanisms in your trajectories. But whether you can distinguish them behaviorally by some clever task manipulations, I didn't really think carefully about it. So then a few more. So Sir John asks: under which conditions is the model reduction exact? And how do you determine the number of latents to use? Right. So the model reduction is not exact. It's approximate. But in all animals we've trained so far, on average we capture with a latent circuit model, we capture way above 90%. Way above 90 percent of the total variance. These models are now strained to the behavioral task, show relatively low dimensional neural activity. You can just do a PCA and see that in our task, you will only require eight dimensions, and these eight dimensions correspond to the number of task-relevant inputs and outputs. So, the problem of selection of the number of nodes in the latent circuit is not hard because neural activity is low-dimensional, and dimensionality. Is low-dimensional and dimensionality coincides with task dimensionality. However, for the case of actual neural recordings, we know that there are extra dimensions, and we are working on the problem how to work with it, how to do model selection so that we can still infer interpretable and testable circuits in situations where we do not know a priori the number of necessary latent nodes. But it's also a little early to talk about it. So, I think maybe that hits Guillaume and Tatiana Chimichiko's questions, but feel free to chime in, either of you, if you want to follow up. Guillaume, you good? Okay. Tatiana? Okay. All right. And then I can ask my question. Yeah, go ahead, Tatiana. I'm curious about the PC role in discovering. Role in discovering the low-dimensional structure in a single context. Would the standard PCA work in discovering the manifold or not? Because you said at some point that you need DPCA, but not PCA, if I understood you correctly. Right, so for the case of RNNs, I feel because neural activity is very low-dimensional, then all these methods would. Then all these methods would converge on a similar manifold. It doesn't have to be the exact same manifold, but they will be very close to each other. They can differ quite a bit in case of neural activity because neural activity shows variability in other dimensions, which are not necessarily task dimensions. And if you take a recording from the frontal cortex, for example, during context-dependent decision-making tasks, and just do PCA on it, you will find more than eight dimensions. Find more than eight dimensions, right? So, like for RNN, the subspaces will be very similar, but for neural recordings, the identification of task-relevant subspace becomes a more intricate question. Okay. All right. And then, Sir John, I saw you had a follow-up, but maybe. I saw you had a follow-up, but maybe that's resolved. Okay, I'll assume it has. All right. All right. Well, thanks very much. Tatiana, sounds like there's a lot of good discussion with that. And maybe I'll just make a note for tomorrow that, yeah, definitely sounds like there's some topics that people would enjoy discussing further. So thanks again, Tatiana, for a very interesting talk. And up next is. Up next is Age Altan from North