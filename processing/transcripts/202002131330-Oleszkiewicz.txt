So today I would like to talk about harmonic analysis on the District Cube. I would like to say sorry to all of those in the audience who know already the basics of the subject, because I assume that there are some people who don't know the basic notions. So I will make a quick introduction. Um so uh as a notation uh Uh as a notation, uh the this bracket n will be just uh uh integer interval from one to n. And by discrete cube we will mean the two points at negative one, one in power n. And for the sake of today's lecture, I will always consider that this cube is equipped with the normalized probability uh measure, which is just the product measure assigning to measure assigning to each vertex probably one over twin power vertex. So now this object can be considered as well when n is infinite and this is a well studied and classical object in analysis and also in topology. But in all that I will be talking about today and will be assumed to be just a natural number, uh a finite integer. Uh Finite integer. The reason is that, first of all, most of the applications really deal with the finite dimension discrete cube. Another reason is that technicalities are a bit more unpleasant in the infinite-dimensional case. But most of the notions can be actually transferred to the infinite-dimensional situation. So the usual distance that can be considered on this discrete cube is Uh considered on this discrete cube is so-called Hammings metric. Uh so we just take two vertices and measure on how many coordinates they differ. That's the most natural metric uh on this uh object, on the discrete cube. And once we have uh this uh probability measure we can talk about expectation. So expectation for a function, a real variable function on this discrete cube, expectation is just the arithmetic. Expectation is just the arithmetic mean of its values in all vertices. So, this is a very simple object. Also, we have a standard, the standard L2 structure. Having two real-valued functions, we can define their scalar products simply as expectation of the pointwise product, which is just the arithmetic mean of products in all vertices. And the standard notation is that the piece normally is the number of. Standard notation is that the pth norm of such function for some positive number p is just expectation of the absolute value of f in pth power, and we take pth out of that. This is of course usual standard norm when p is greater or equal than one. When p is between zero and one, this is only a quasi-norm, it doesn't satisfy the triangle inequality. But anyway, for some applications. Anyway, for some applications it's convenient to consider this expression also for p below one. And F infinity is clearly just the supremum normal, just maximum of the absolute of f over all vertices. So now once we consider such a scalar product, it's easy to see that F to norm is just a hil the Hilbert norm coming from this scalar. Coming from this scalar product. So these two notations are compatible. And as I said, we can consider the family, the linear space of all real-valued functions on the discrete cube with this scalar product structure just as a Hilbert space. And obviously the dimension of this Hilbert space over realities. Space over real case as huge as the number of points in the description, which is 2 in the power n. So now another notion that is very standard is the notion of a Boolean function. So Boolean function on the discrete cube is the function taking exactly two values, negative 1 and 1. And the main motivation is coming from theoretical computer science. So this is simply general functions. So, this is simply a general function that takes n bits of information as an input, and as an output, it gives a single bit of information. Okay, so for computer scientists, this is clearly the good model, the most natural model for any computer algorithm, any procedure that takes any bits of input and produces all. n bits of input and produces one bit of output. Of course in computer science and in logic sometimes the most the more natural and more classical notation is you consider 0, 1 instead of negative 1, 1. But of course these two approaches to notations can be easily switched from one to another. In analytic applications usually it's much more convenient to have negative one one so I will keep Negative 1, 1, so I will keep to this notation. This simplifies most calculations. Another application is so-called social choice theory. This is a branch of microeconomics and also with some very mathematical parts. And basically this looks at the the function f as a voting process. You have n voters, each of them can make a decision them can make a decision, a binary decision, either yes or no. And then you have some voting procedure, some voting model, F, that aggregates the votes of single voters, knowing which voter voted in what way. It's not symmetric, not necessarily symmetric, and then we produce an outcome. So you can think about presidential election, for example, among two candidates. So quite many things are modeled in this way. Of course, this is a very simple model in practice. Situations are much more complicated than that, but that's one of the reasons to look at such functions. And there are some classical important examples of Boolean functions. The most important ones are Walsh functions. So once we have a subset S, So once we have a subset S in this integer interval bracket N, we can consider the product of all coordinates of the vertex x whose indices come from this set x. And this product is called the Walsh function. So obviously it takes values in negative 1, 1 set, just because all coordinates are negative or positive ones. And to this collection And to this collection uh the natural extension is when S is the empty set, uh I just put identically one. So uh when the index set S has just one element, it's a singleton, then such a random variable is usually denoted by Ri or in some other situation just by Pi, and this is just a projection to the I squared. So then when we So then when we take all such Walsh functions induced by singletons, what we get is a rather market sequence, independent and independent symmetric plus minus one Bernoulli Real. Okay? So this is the general setting. And now this Walsh system, the collection of Walsh functions, is especially interesting because they have very nice properties. They have very nice properties. So when s is non-empty, then expectation of the Walsh function is equal to zero, just because it's a product of independent mean zero random variables, so it must have expectation zero. And of course for the constant, for the empty set, this is constant one, so expectation is one. And then it's easy to see that this translates to the orthonormality of the Walsh system. It's obvious that when we take It's obvious that when we take the pointwise product of two Wallus functions, we again get a Wallus function. And the index set is just a symmetric difference of s and t. So let me give an example. When you take Walsh function index by 1 and 2, Z and 2, 3, these radi mahels that are in both Walsh functions, like R2 in this case, will appear space. In this case, it will appear squared, and because Serv the Mahara takes only values negative or positive one, after squaring it cancels out. That's a simple observation. So this means that the scalar product of two Walsh functions is equal to one when s is equal to t and zero otherwise. So this is the orthonormal system, and actually this is also the orthonormal basis, because you have two in power n. because we have 2 in power n orthogonal vectors in the linear space of cardinality of dimension 2n. So this is also spanning all possible functions. And now there is also much more elementary way to see that all functions can be spanned by Walsh functions. But I will skip this part because we don't have that much time. And anyway, Much time. And anyway, this means that every function, real-valued functions on the DC cube, admits exactly one expansion in terms as a sum of Walsh functions with real coefficients, and these coefficients are denoted by f hat. So of course it's easy to read this F hat coefficient just as a scalar product with the Walsh functions in the Walsh functions index by S. And we have some special easy cases. So, expectation of F is a scalar product with the identically one function. So, it's just the Fourier Walsh coefficient indexed by the empty set. And we also have the Planchard identity with the standard proof. So, I will skip this. So I will skip this. So in particular, if f is a Boolean function taking only values negative one and one, f square is identically one, expectation is equal to one, which means that the sum of squares of coefficients of any Boolean functions is equal to one. Since this is just the Planchet identity. Okay? And now we can talk about frequencies just by analogy to the trigonometric system. So if this S So, if this S is a set of small cardinality, then we are talking about low frequency, and if it's large, then we talk about high frequency. And there are many natural analogies to gonometric system terminology, but I'm not able to to get deeper into that. Uh there are many ways in which uh high frequency Walsh functions uh are similar to Are similar to high-frequency trigonometric functions. So now we can talk also about Eradamacher chaos. So if such a function, sorry, here n is missing. This is discrete cube. If we have a function, real valid function on the discrete cube and all coefficients for high frequencies disappear, so f. disappear. So f hat of s is zero for all s with large coordinate grid and needle. We call such a function a Rademacher chaos. And of course the family of all such chaoses of degree not exceeding D form a linear subspace of Hn. And also there is a complementary notion of the Taylor space. So the kth tail space is the linear subspace spanned by Linear subspace spanned by all Walsh functions with high frequencies, say higher than k. So, this is all very standard setting. And just one short side note that I will mostly skip is that this is consistent with the usual way harmonical analysis is done on uh locally compact abelian groups. When you look at negative one one as a Z two, just use pointwise Just use pointwise, or if you look at this as Z2 in power n, just take two points, two element group with one as a neutral element and this as the other one, and you take point one multiplication, then everything that I described is consistent with the usual terminology in L. Terminology in LCA group harmonic analysis. And this is very simple thanks to the fact that characters on Z2 group are real valued. So in general, characters take value in the unit circle on the complex plane, but in this very special situation they are real valued and this makes things very simple. So this harmonic analysis, especially harmonic analysis of Boolean functions, Analysis of Boolean pensions is a quite old subject. It develops quite rapidly for the last 30 years, more or less. Of course, some theorems are older. And it has a lot of applications in combinatorics, in computer science, in many branches of mathematics. And I would like just to describe some relatively recent results. Results and these are not the most important results. Their importance is not the reason I'm talking about them. That's just because I was engaged in research in this direction. So I would like to talk about several possible ways you can work with descriptive analysis. And actually they are not very recent. All the things that I will be talking about are All the things that I will be talking about are at least three, four years old and some of them a bit older. So so this is not very very recent, but I I I just want to give you some view on what type of things people are doing in this area. So FKN, I will explain in a moment. This is the common custom of people working in combinatorics and theoretical computer science that they name CRMs by Name theorems by abbreviations, just taking initials of the names of the authors. So the FKN stands for the Friedgud Kalei and the Naur. So this is the very classical, that's a very classical result. It comes from 2002. It was proved by Echut Friedgut, Gilkalai, and Asaf Naur. And this tells you the following: consider a Boolean function on the discrete q and denote. On the discrete cube, and denote by rho square root of the sum of squares of all frequencies higher than 0 and 1. So you take all sets of coordinality at least 2 and you add those squared coefficients. So as I told you, for any Boolean function, if you take squares of all possible coefficients, they will add up to 1. So here, you denote by rho the square root of, sum of squares of all. The square root of sum of squares of all coefficients associated to the sets of coordinality at least two. And this row will be assumed to be small. That's the typical situation in which this theorem is applied. So now the FKN theorem says that there exists some set B with small coordinality, meaning that it's either the empty set or it's a single atom, such that the sum of squares The sum of squares of all remaining coefficients different from B will be bounded by some universal constant times rho squared and this single coefficient will be huge. It will be greater or equal than one minus this universal constant rho squared. So these two things are equivalent just because as I said the sum of squares of all coefficients for a Boolean function is equal to Boolean function is equal to one. So the real statement is that once you exclude a single small frequency, a single just constant function or some of the rather maheres, then all remaining coefficients will have small sum of squares. So you assume this only about frequencies higher than two, and together with the fact the function is Boolean, this shows you that And this shows you that actually everything is dominated by a single low frequency. Either this function is very close to one, or it's very close to negative one, or it's close to a Radema function Rk or minus Rk for some great. This is what the FKN theorem says in a quantitative way. When you write A different from beta, you mean A. You take all subsets, uh, no, just different. You uh you can consider all possible subsets of this integer interval, and a single coefficient will be huge, will be very close to one, and all the remaining ones will be small. This is what the theorem says. Very nice? Is there an analog of this theorem in the usual world of normal VM? Sorry? Is there an analog of this in the usual world of usual function to n? The usual mode of visual function, the usual function? I don't know about trigonometric system because I didn't work in this, but actually this paper that I give reference to is extending in five or more ways the classical FKMCR. So, for example, to other product spaces. So, if you are interested in generalizations have a look at this paper. Is there a reason why this is uh surprising? I mean, because you Surprising? I mean, because some of the coefficients, and uh this proves to be a very uh it's like with analytic functions. Uh this uh the way I see this theorem is that actually the assumption that a function is Boolean is very strong. Okay. There are not so many of them actually. Of course there are two in power two. N of them, but still they are very special among all possible. Very special among all possible functions. Okay? So I will not show you the proof. The proof is on the slides if you would like to see. And actually, the first proof by FKN, by Frederic Carai and Narr gave some very huge concern. That was a universal concern, but really big, like 15,000 or something like that. It was quickly improved by Tinler and Saffer the same year, and actually some small gap in the proof was also fixed by Guy Tinler and Murray Saffer. Geithindler and Maurice Affram. And uh well the main idea of this is to show that if you have uh some sum of white Raden-Maher variables, meaning just the Raden-Maher sum, you take Raden-Maher variables with real coefficients, add them, and this is called the Raden-Maher sum. If the variance of the absolute value of this sum is close to zero, it's small relatively to the variance of the sum. Relatively to the variance of the sum. This means that a single summon dominates the sum. That's the morale of this theorem. And actually this is the way it was proved, more or less. So I will make a geometric digression. I wanted to speak about at least one thing that might be interesting to geometers in the audience. So this is one thing that came from one of the generalizations of the One of the generalizations of the FKN theorem that we did with Onof Wojtaschuk and with Yasaki Andre. So this is a side effect of our work on the FKN theorem. So we consider a normed linear space, maybe finite dimensional. Now, when you have a subset of this space, you will call it one separated if for any two distinct points of this set, the distance between them The distance between them in terms of this norm is at least one. And now we have the following natural question that we don't know the answer to. So we have two finite sets, A and B, that are one separated in some normed linear space. Does the Rominkowski sum A plus B necessarily contain some one separated subset of cardinality power of coordinity of A plus cardinality of B minus B. Cardinality of A plus coordinity of B minus 1. Is it true or not? We don't know. So the canonical example, of course, that one should have in mind is just the real line and integer intervals. And then obviously you see that you cannot beat on this number. And the reason why we asked this question is that the theorem of this type was needed in our work on FKN theorem in some more general situations. And theorem in some more general situation. And well, we know that the answer is yes if one of these sets is small because cardinality at most two, and this is very easy. This is just Hanbanach theorem and one line of proof. And we also know, and this is less obvious, that this is true if the norm is Euclidean. And the question is, is it true for general normal space? Normal space. We don't know. The simplest open question is when both sets have cardinality 3, and even then, I don't know the answer. Actually, this means quite many possible five-element subsets, nine-elements. Minkowski sum, so this is not very easy in combinatorial terms. So one would like to have something more regular, just not to consider. More regular, just not to consider 1000 cases, but once things become regular, actually, usually you can find a subset. So it's not a good counterexample. I don't know. I think that this conference may be a good place to popularize the question. Okay, so now uh I will very quickly uh show you uh what can be done. So one of the things that can be done uh this FKN theorem can be strengthened. Theorem can be strengthened, and actually, the bounds on the coefficients may be very significantly improved. So, when you look at the coefficients of coordinality at most one, the ones that don't appear here, you can find some small coordinality B such that for all such low coordinality, A is different from B, the sum of squares, depended not by universal constant. Not by universal constant times rho squared, but by this expression, which is much, much, much smaller. And actually, this is of the optimal order. We prove this bound and notice that this is optimal together with ye and Ray and Vojtaschuk. And independently at the same time, Ray and Nodoner prove the same result. And the function showing that this cannot be improved is also. To be improved, it's only this font. This has a very simple font and can be easily checked. So, I will not go through the proof. It's on the slides if you would like to have a look at it. Actually, I thought that I will be speaking faster, but it didn't work. And the one of the key tools in the proof is so-called Bonami-Bechnery inequality. So, this is uh a hyper contractive inequality. Contractive inequality that is equivalent to the logarithmic Sobolev inequality for the discrete cube. And this fact was actually proved by Bonami independently at more or less the same time in a different form. It was proved by Gross. And the name of Beckner is somehow added later because Beckner used Bonami's observations when proving In his famous paper, the optimal bound on the norm of Fourier transform. So then he slightly extended Bonam is inequality, but actually it's rather Bonam is inequality of London outgrowth. So I will not go through the details just running. And just to give you some more Just to give you some more notions, what people are interested in in Boolean analysis. So when you have a real valued functions on the discrete cube, you can ask about the influence of the ith variable. So you can measure how much this function depends on the change of sign of the ith variable. You fix all coordinates except the ith, then you measure. Then you measure the variance with respect to the ice coordinate and then you average releasing remaining coordinates. And this can be easily expressed in terms of the Fourier-Walsh coefficients. It's just sums of the squares over all sets to which i belongs. And this is obviously an interesting object, measuring how much function depends on individual coordinates. Individual coordinates, and one of the classical theorems, and actually the one that probably was most responsible for the start of the whole branch of mathematics that is now called Fourier or harmonic analysis of the Boolean functions, is due to Khan, Kali and Linear, and of course it's called KKL. And they proved that if you have a mean zero function on the discrete cube, that is a Boolean The discrete cube that is a Boolean. Then there exists some index in this integer interval such that the influence of this function with respect to the ith coordinate is bigger than some universal constant log n over n. And that was a huge breakthrough because combinatories were working on this question in a slightly different uh formulation for a long time and For a long time, and all results before that only work with constant divided by n. So, getting this logarithm was a huge breakthrough, and this somehow convinced people in this discrete world that analysis may have some nice applications also in combinatorics, and this started the whole branch of research. Let me just mention that this assumption that expectation is zero can be weakened, but you cannot completely. Can be weakened, but you cannot completely drop this assumption uh just because for a constant function, for example, f identically equal to one or f identically equal to negative one, obviously all influences are zero. Such a function doesn't depend on the ith coordinate. So you must somehow exclude in this theorem this constant functions and neighborhood from consideration. Uh neighborhood from consideration if you want to get uh this type of out. So assuming mean zero is just one of the ways this can be done. Okay? And also this uh order of this uh bound is optimal. This is indicated by uh the classical so-called Tribes function. I I don't have time to describe this function, but it's very elementary and can be easily checked that if you choose it properly. If you choose it properly, then uh equality theory is achieved. And actually, for the times function, all influences are equal to some constant times logarithm more or less. So this is optimal example in a strong sense. Okay, so let me just make a one-minute digression. So recently we have been targeted, I don't I don't know whether I properly pronounce his name. He was a PhD student of Michel Ladoux. He worked on some generalizations of this theory and he asked about the influence indexed by two elements. So you take ij between n and one, and you assume that they are not equal. You have two distant indices. So then you can This is so then you can define the influence of f indexed by i and j just as the sum of squares of coefficients over sets to which both i and j belong. That's the natural extension. And there are some natural reasons to ask about uh such higher order uh the this is related with higher order partial derived values of the discrete cube. This is a natural object to to study. And to study. And uh he proved several results. Uh I provided some uh alternative uh quite different proofs uh of these results. And basically they say that if for all i and j these influences are small, are bounded by say one over a thousand log of n divided by n squared. And this square is natural because for the Is natural because for the same tribes function that is considered in KKL, you can prove that these second degree influences are of the order logarithm n over n squared. So this is a natural order of magnitude. So if you assume that all these second degree influences are small in this sense, this implies that f is close to plus minus one or plus minus radio half. or plus minus radar marker for some k. Okay, so once you exclude identically one, identically negative one, and radar markers and minus radar markers from your orbit of interest once, then there are at least two or three ways to describe what it means that it's close. They are not compatible. One of them does not Compatible, one of them doesn't imply another. So there are different languages in which you can describe this closeness. But once you assume that f is in some sense far away from constant functions and plus, minus r the minus, you have an analog of the kn. So this result has some higher degree analogs, and that's one of the things that was investigated in recent years that I wanted to mention. Okay. So thank you very much. Okay. So thank you very much. Any questions? So so was there a particular reason why people studied these questions of harmonic of Boolean analysis? Yes. So some questions in combinatorics naturally translate. So once you have a subset of some n-element set, you can identify this subset in a natural way. In a natural way with a vertex of the descriptive tube. Right, and so there is a natural connection between these objects. And also quite many natural objects in computer science. Stability questions. This is one of the reasons why people are interested in influencing and bounding. Uh you assume about some computer science procedures, uh but they are stable. uh but they are stable, right? And you need some tools to measure this and some theorems to deduce some good properties. And influences are one of the ways you need to measure this. And KKLL theorems tells you that in generic situation you cannot expect too much, for example. Any other questions? Okay, if no question then let's thank the picture. 