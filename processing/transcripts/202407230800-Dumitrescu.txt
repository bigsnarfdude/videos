Screen Yes, could you make a full screen? Yes. Do you see in full screen? Not yet. Oh, yes, no. Oh, no. Yes. Thank you. All right, good. So I think we can start for the part two of the mean field gain theory computation and application given by Rosatna from King's College London. So Loxanna, please go ahead. Okay, so thank you very much. I will. So yesterday I didn't do everything I wanted for the theoretical part in the lecture. So I will Lectures. So I will do a very brief recap of the formulation of meaningful games and of the approaches that I didn't cover yesterday, basically. And in yesterday's lecture, I covered mean field games with control. So you have seen that I have different variants of this type of problems. Of these types of problems. Now I will concentrate on another type of mid-field games, which are the minfield games of optimal stopping. And then I will present, let's say, an easy approach to solve this kind of games from a theoretical point of view and a numerical point of view. And for the exercise session, I will discuss an application, as I did also yesterday, to energy transition. To energy transition. So, this will be a long-term model related to entry-exit games. So, entry of renewables on energy markets and exit of conventional in the electricity markets. Okay, so first of all, I would like to remind which is the formulation, the general formulation of a meaningful problem, but as you have seen yesterday in the application. In the application part, we always have a fixed point, but this fixed point problem might be differently formulated depending on the problem. But let's take the general framework. The general framework is what you do. First, you fix the environment, the distribution of the population, which is given by mu t, which is a flow of measures. Okay. Then you solve the standard stochastic. Standard stochastic control problem in this environment mu t. Okay, so this is using the very first part of yesterday's lecture. We know to solve such a problem. So the representative player wants to minimize or to maximize this type of criteria given by the object of function j for the fixed flow of measures mill. Of measures will. Okay. So assuming that we have an optimal control for this problem, of course, that this optimal control will depend on μ. Okay. And we say that we have a Nash equilibria if the law of the process X along the optimal control alpha star coincides with the μ t that I fixed at the beginning. So if I have At the beginning. So, if I have such a fixed point, then I have a nash equilibrium. I want to emphasize once more the fact that these mean field games are useful tool for stochastic differential games, but with mean-field interactions, which are symmetric, the symmetry is extremely important, okay, in order to be able to make such an asymptotic approximation. Approximation. Okay. What is L? Excuse me. What is L at the bottom of the slide? L? Yes, L. Calligraphic L. Ah, L is the law. Okay, okay, thank you. L represents the law of X T. Okay? Okay. So. Okay, so which are the approaches to solve such games? So, the very first approach which has been introduced by Lastrillience is using the PD theory. So, this one consists in a coupled system of equations, an HGB equation that we have seen yesterday in the stochastic control part. You remember that I said that the value function of a standard stochastic control problem satisfies. Control problem satisfies an equation which is called Hamilton-Jacobi-Pelman equation and which is backward in time. It means that you give the terminal condition. Okay, but here you have another equation which gives the evolution of the distribution of the population over time. And this one is forward in time. So you have an initial condition. And what makes this problem difficult is that first you have this forward. You have this forward-backward nature of your system, but also you have the fact that these equations are coupled. Okay, and we see this approach a bit more in detail. Then later, six, seven years later, after the introduction of minfield games, Carmona de Laru proposed a probabilistic approach, a forward backward approach, where you characterize the equilibria via. Characterized the equilibria via four backward system of SDEs for which the coefficients depend on the law of the solution. And yesterday, for example, in the application part, I rather used this FBZ approach to solve the problem. It was a framework with common noise and the forward backward approach works very well in this setting. And then another way to solve these problems. Way to solve these problems are the so-called compactification methods. I will talk about one of them today, which why are they nice? So, first of all, this PD and FPC approach are quite demanding in terms of assumptions. Okay, for the forward-backward approach, for example, if I want to use the Pontri-Argent principle, I need convexity, okay? I need regularity of the coefficients, so it's quite demanding. But in the case when these assumptions are. But in the case when these assumptions are not satisfied, we should be able to do something, and the compactifications methods allow to relax the assumptions at the price of relaxing the concept of equilibrium. And I will explain what it means. Okay, so let's see how this mid-field game PD system looks like. Okay, so as I said, I have two equations. As I said, I have two equations. One for which I give the terminal condition, which is the Hamilton-Jacobi-Berman equation that we have seen in the very first part of yesterday lecture. And then we have this Fokker-Planck equation, which gives the evolution of the distribution of the population. And the equations are coupled. And we'll see how they are coupled. So let's start with the value function of the representative 8. Function of the representative eight. So you remember that when I described the program to solve a mean field game problem, what I said is that I fix the environment mu t and then I solve the problem of the representative agent. So this is the problem. So I have mu tx represents this value function that I which is equal to the infimum over all admissible control. Over all admissible control processes of this objective function, where you can see that the terminal reward depends on μt, which is the final distribution of the population, and f depends on μs. I mean, f is the current, is the running reward, which depends on the entire flow of measures. Okay, so using what we have seen yesterday in the lecture, Have seen yesterday in the lecture. We know that using dynamic programming, okay, we can show that this value function u solves this HGB equation, okay, where this part, the infimum of alpha times the gradient of u plus one over two alpha squared plus f represents the Hamiltonian of your problem. Okay, so in this very particular setting that I'm given, you can. Time given, you can have a C12 solution, but in general, you should not expect to have C12 solutions. And there is a very nice theory about viscosity solutions, which help you to solve this HGP equation. So to address the well-poseness of the equation in some sense. So, and you have the terminal condition of this value function, which is u of capital TX. of capital T x is equal to g of x mu capital T. Okay, so first you observe the coupling already because you see that the f depends on mu, the g depends on mu, so this solution u of my LGB equation depends already on mu. Okay, so you see the dependence. Okay, let's see now. I said that we have another equation which gives the evolution in time of this mu t, okay, which is a which is Okay, which is a flow of measures. What does it mean, a flow of measures? It's basically a function which depends on time. So, to each time I have mu t, which is a probability measure in this case. Okay, so you need a PD characterization of this mu t, which represents the law of x of the optimum. So, first of all, you also remember that here. You also remember that yesterday I said that we can compute the optimal control. Okay, so it's the one which minimizes the Hamiltonian. So here in red, I put the optimal control. So when I write the dynamics of X at the equilibria, I write down the optimal control, which is minus the gradient of U. And then I will not enter into details now because we don't have the time, but what you should know is. But what you should know is that we can show that the law of x satisfies this equation that you see, the second one on the last one on the slide, which is called the Fokker-Planck equation. Actually, in the sense, what we call in the distribution sense. It's actually not difficult to obtain it. What you do to obtain the equation, you take a test function psi of t and x t star. t and x t star and you just apply ito's formula okay and by applying it's formula you can deduce this foker planck equations you saw a simple application of ito's formula but i will not enter into detail what you have to observe here is that this foker planck equation you see depends also on u as well because it depends on the the optimal control okay which is the gradient of u so you see that they are fully coupled if you want this They are fully coupled if you want to this equation. So, this makes this problem quite hard in general to solve. Excuse me. Yes. So, in the previous Fokker-Planck equation, so mu t is given, right? Mu t is given. Mu t is the law of x t star mu. Is it given or not? So, this will represent the equilibria, yes. It's given. So, the annoyance. So, if you find a solution, sorry, if you find a Sorry, if you find the solution of this system, okay, then you find the equilibrium. So, mu, you try to find is part of the solution. You see what I mean? A little bit confusing. The solution, the solution, so you are looking for the equilibria. And you already equilibrium. Yes. I thought you said like muti is the environment, right? You fix the environment, but then you want to find it. Then you want to find it. So at the beginning, you fix the environment, mu t. You solve your control problem, but then you are looking for the mu t, which is the fixed point. So the law of x along the optimal control, where the optimal control depends on mu coincides with the mu. So I'm looking for such a mu. So fixed point means like mu t is equal to the law of xt, mu. Yes, exactly. Okay. Yes, exactly. Okay, so this is what I'm trying to find. I'm trying to find the fixed point. Then, then mu t is not fixed, right? Mu t is unknown. So, when I solve the control problem, I solve for a fixed mu. Oh, okay, I see, I see, okay. And then I'm looking for the fixed point. Okay, okay. Okay. So, uh, here, what we are trying to do in order to be it, so the notion at equilibria using So, the notion at equilibria using the PDE system is defined by two objects: by the value function u and by the flow mu. So I'm trying, so if we are able to prove the existence of a solution of a such PDE system, then we can say that we have an equilibrium. Okay. And now what is nice and it's nice and Nice and it's nice, and also very important, as I said, is the fact that how can we use our result obtained? So, the fact that we get a mean field equilibrium in order to say something about the end player games, because this is finally the point of interest. So, I have shown you yesterday in the application part that we can how I I showed you in a linear quadratic case the fact that I can use the DMI to construct a prox equilibria using the forward-backward approach. Using the PDO approach, I can do the same. So how it works. So when I solved my MINFID problem, you see here I have a generic W, a Brown in motion, and I have this controlled SD. Controlled SD given by X. So, what I do when I consider the end-player games, I just replace the W by the W of each player, because I consider here that I'm in the setting without common noise, and each player has his own noise, and the noises are independent. So these are called idiosyncratic noises. So these are the dynamics of the players. Okay, then the cost functionals, you see that the You see that the cost function of each player i depends on the controls of all players through the empirical distribution. So, this mu bar n represents here the empirical distribution. So, at the limit, I have the measure mu. Here I have the empirical distribution. And how I construct the Nash equilibria, you remember that in the limit setting, my optimal equilibria was given by Equilibria was given by. So if I go back here, you see my control was given by minus the gradient of u applied to tx t star. So here I say that each player will have a control which is minus the gradient of u of t sorry in the end player game. I use alpha n i which is minus the greater Alpha and I, which is minus the gradient of u, applied to x i t. So you see, I have alpha star in the limit setting, and I show in the end player game how I adapt it to the end player setting. I just replace x by xi. And this can be shown to be an epsilon-nash equilibria in a rigorous way. We can show epsilon-nash equilibrium, it means almost Nash. It means that the cost decreases at most. Decreases at most at epsilon n under unilateral deviation. Okay, when so when epsilon n goes to zero. Okay, so this is how we can use the Minfe game using the PDE system to get the proxy equilibria for the end player game. Now, the second approach that I mentioned was the probabilistic approach. We have seen We have seen yesterday, but a brief reminder. So, using actually, there are two ways to write the probabilistic approach. You can write the equivalent of the PD approach in a probabilistic setting. You can do this, but you need the volatility to be non-degenerate, so you need some assumptions. Another approach is to write A system where x represents the state process and y will basically represent the derivative of the value function that I had in the probabilistic approach, in the PD approach. And for this one, you need also quite strong assumptions because you need your coefficients to be differentiable. You need convexity, okay? So it's quite demanding. So, in this case, when use the Stoccati-Pontraguium principle, so I repeat y hat corresponds to the derivative of the value function from the PDE approach. The mean field gain problem amounts to solving this fully coupled system of four backward equations, okay, given by x-hat and y-hat. And y hat, where this alpha hat here represents the optimal control, if you want. It's the one that we have seen also yesterday in the lecture, is the one which minimizes the Hamiltonian of your problem. Okay. Yes? Sorry, is there any question? I didn't hear well. Okay, so this. Okay, so this is the probabilistic approach. So, now what we will do about the compactification methods, the third type of method to solve this mean field games, I will talk in a specific case of mean field games of optimal stopping that I will introduce now. So, before doing this, I would like also to say there are many numerical methods to solve the PDE system and the The PD system and the forward backward system is a there are many algorithms for the PD system using fine difference methods and also for the forward backward system. For example, I present yesterday a method using using deep learning, okay, which is useful, especially when you are in large dimensions. So there are many, many ways to solve these systems, also from a numerical point of view. So now about So, now about the Minika games of optimal stopping. Before introducing them, I would like to ask if you all know what is a standard optimal stopping problem. You know what it is? Okay, so maybe I will share just my tablet. I will write quickly what an optimal stopping problem means. Um I suppose you see the tablet yeah yes yeah okay so actually to introduce the the optimal stopping problem the best way is the the pressing of an American option so uh for the pressing of American option you know that the You know that the price of the American option is given by the supremum over all stopping times of the expectation of G of X tau, where G represents the payoff. X is the underlying and G is, for example, if you have a call option, G of X will be X minus K plus. So this represents This represents you can choose the duration of your contract and you decide when to stop in such a way that you maximize your profit. Okay, so this is the American option. Okay, and it can be shown that the first moment when it becomes optimal to stop. So if I write If I write here, I wrote the problem at time zero, but I can write as I did yesterday in a dynamic way at any time t. So if I take toll larger than t expectation of gfx to, and the diffusion process will start from t and x. So x is a standard AZM. Are the SEM, as we have already seen. So, the first optimal stopping time is the first time when V of T X T we maybe it's not. Good that I selected t is the first s when v of s x s t x is equal to g of x s. So it's the first time after t. So if I want to compute the stopping time after a given time t, when the value function coincides with the payoff. Okay, this can be shown. Okay, this can be shown. So, this is a standard optimal stopping problem. So, you see that I don't have any control process alpha as I did up to now. My control process is the duration of the game. Here is no gain, but for American option is, for example, duration of your contract, okay, which is given by two. Okay, so now that we have seen in the case the standard optimal stopping from how it looks like, I will go to the problem of the optimal stopping. To the problem of the optimal stopping problem in the case of n agents. So I will change the sharing. Do you see my Do you see my screen? The slides? Yes, okay, good. Okay, so how it looks the end player game. So each agent, each player will have his own control strategy, which is the stopping time. So he will decide when he wants to. We decide when he wants to exit the game. So I take the state process of the N agents are given by these standard stochastic differential equations. I'm again in the setting when each agent has his own Browning motion, Wi, okay, but you can consider also the case with common noise. And each agent wants to solve the optimal stopping problem, which is written at the very last line. So you see that I have, okay, I write the maximum because I will know that I will have a, I will find a stopping time which will maximize the value function. So I have the maximum over all stopping times of describing. Stopping times of this criteria. So you see that this is the optimal stopping problem that I have written to you. But the only difference is that this I have here a running reward. In the example that I gave you, I had a terminal reward, but you can also consider a running reward F. Okay, so I integrate the running reward between zero and the time tau when we stop. We stop. And we see that F will depend also on this term Mn. So, this Mn is similar to what we have seen in the first part of the lecture, where it was the empirical distribution of the state processes, worth joint state processes, and controls. But here, it's a bit different because I will take the empirical distribution of those players who are. Those players who are still in the game. And how I indicate the fact that those guys are still in the game? I put the indicator function here that T is smaller than to I. So TOI is the stopping time of the ICE player. So in order to say that I take into account only those who are still in the game, I put the empirical distribution, but I add this indicator function to specify. To specify that those players are still in the game. So the main difference with respect to regular case with regular control that I presented is the fact that this flow of measures, Mn, so it is again a flow of measures because for each T, it takes values in the space of measures, but there will not be probability measures anymore in this case, because I have the possibility. Because I have the possibility that the players progressively leave the game. So I have a decreasing mass of players. So this will be a sub-probability measure. Okay. And now how we try to write the Minke game formulation, so how it works. So first of all, using very similar arguments as in the case of regular control, since our game is symmetric. Since our game is symmetric, so in the sense that it's symmetric interaction, and you see all players are identical in the sense that they have the same drift, the same volatility, okay, the same running cost. We can expect that this Mn will converge to some limit, which in this case will be deterministic because I don't have any common noise. In the case, when you have common noise, this limit will be random and will be a And it will be measurable with respect to the common ocean filtration. So we try to write the mean field formulation. So we write, as we already know, we have a representative agent in this case. So the dynamics of the representative agent of the state process follows this stochastic differential equation. And the optimal stopping problem for the agent is. Problem for the agent is exactly the one that I had here on the last line. The only difference is that I replaced the empirical distribution Mn by this measure MT. Okay. And okay, what I wanted to say that you can consider more general, I'm presenting a very simple setting. You can consider also more general settings where B and sigma depend also on M, but we don't treat this case here. Don't treat this case here. And you look, we try to look now for the fixed point as usual. So, what we do, you remember that in the case of control, we were saying that if we have an optimal control, so in the case when we have an optimal control, we are looking for the law of x along the optimal control. So, what we do here is assume that I have a stopping time which maximizes my objective function here, okay. Yes. Could you explain what is the law of X? The law of a random variable. Distribution of a random variable. Okay, I will. Sorry? Okay, thank you. So let for a fixed Let for a fixed M uh so I solve the problem for fixed m okay and for initial condition X so what we are trying to do is to compute now the distribution for a given set A, which is a Borayan. Okay, we want to see which is the distribution Which is the distribution, the probability that x t belongs to a for t before tau. Okay, so if I have a fixed point here that the m that I gave as input when I computed my optimal stopping time, okay, I get the fixed point for equation three, the m on the For equation three, the m on the right-hand side coincides with the m on the left-hand side, then I say that I have an equilibrium. And here I integrate everything also, of course, with respect to the initial distribution. Okay. So now this is how the How the end player game looks like, and how the mean field formulation. What is M star here? Is that the initial distribution of distribution? Given initial measure, yeah, M0 star, in the previous slide, when we solved it for a given M, was that a flow? M is a flow, yes. M is a flow, yes. Okay, so I'm a bit confused with the m star notation because the m star zero, I put a zero here, is the initial distribution of the population which is given. And then here, when I solve for a fixed m, there is a fixed flow of measures which starts for m0 spar. Okay, I see. I see. Yeah, that's what I wanted to clarify. Thank you. Yes, you're welcome. Okay. Okay, so now I will talk about this method to solve mini field games of optimal stopping, which is based on a compactification technique. So first of all, the minfield games of optimal stopping is usually in practice a bit more difficult to solve. The first difficulty, as I said, is the fact that, as I mentioned, the players can exit the game. Can exit the game. So you have this. You can imagine that you have many players who exit the game suddenly. Okay? So there will be a discontinuity in the flow of measures. So it means that this function m will not be continuous in time anymore, as is in the case of control that we have seen yesterday. The flow was continuous in time. In this case, here it will not be the case. It might not be the case. The case. So the PD approach, which the equivalent system for optimal stopping of the one that I have presented in the case of control, it's much more difficult to obtain. So now I'll present an approach which helps to deal with this lack of regularity and to avoid these issues. So, how it works. So, first of all, what is a linear programming? What means linear programming? It means that you optimize. Programming. It means that you optimize a linear objective function which is subject to linear equality and linear inequality constraints. Okay, so linear objective function and you have some constraints which can be equality or linear or inequality constraints and all of them are linear. So, what I want to do first in the case of optimal stopping just to Just to rewrite to transform my optimization problem into a linear programming problem. So this was my optimal stopping problem. You remember the supremum over two of the expectation, and this is the SD. Okay. Okay, I assume the work positions of the SD. I will not spend time on this. So let's introduce. So let's introduce this measure M T. So again, A is a borelian set of R. It could be also other spaces, R D and is defined the expectation indicator of A X T. So X T belongs to A for T smaller than toe. So this is a measure. So for each stopping time to. So, for each stopping time toll, I can define such a measure. So, using this definition of my measure, I can rewrite the expectations for a given toe, the expectation of the integral from zero to toe of f tx t dt. I can rewrite it in this form as the integral of over zero capital T times O of F T X M T D. So this can be rewritten easily just using the definition. Now, if I just write this integral of f m, m I say, okay, is a set of is a flow of measures, but I need to link my flow of measures to the initial process, okay? Initial process, okay, to say basically that these measures characterize, if you want, the process. So, how do we do this? Is we use the ItOS formula. So, we take a test function u for the given test function u, we can apply it to the formula. It's exactly the same method as in the case when you want to derive the Fokker plus. In the case when you want to derive the Fokker-Planck equation. Basically, it is a variant, if you want, of the Fokker-Planck equation. So, what you observe, so you take the test function u, you apply Ito's formula between zero and time to u of t xt, and then you take the expectation. Okay, so take the expectation and using again the definition. Expectation, and using again the definition of the flow M, I have this equality, the last one that I have. Now, here I take only positive functions u. So this means that the right-hand side is positive because the test function is positive. So what I get as a formulation of my problem, I get that I want to optimize over all measures. Measures. Okay, so you remember that I said that to each stopping time, I can associate a measure, and for that stopping time, I also have this constraint satisfied, okay, for the measure associated to the stopping time. So now what I do, I define my problem. So I define my problem as the supremum over all measures m, which satisfy this. Satisfy this constraint that I have deduced here for a given stopping time. So I say that I want that my measures, my flow of measures, satisfy this constraint for all test functions that are positive. And of course, that I need these integrals to be well defined. So you take that, you can take, for example, the coefficients to be bounded, but we can extend this. So it's not. Okay, so the first observation. Okay, so the first observe. So I have a linear programming problem because the objective function is linear with respect to m. So m is the parameter over which I will optimize. You see, I take supremum over m. And also the inequality constraint is also linear with respect to m. So this is a linear programming problem in the sense of the definition, standard definition that you can find also in With. Definition that you can find also in Wikipedia. And the first observation is that, of course, you have seen that to each stop in time two, I can associate a measure such that I have the constraint satisfied. So this space of measures is larger than the measures associated to stopping times. Okay, so the first observation is that the value Is that the value, this value is clear to be larger or equal than the value of my standard, this optimal stopping problem. But as you'll see, we can show, I will not give the proof, but we'll be able to show that actually under reasonable assumptions, the two values will coincide. Why do we do this? Why do we do this? So, we do this because when we consider the initial problem, this one, supreme mover stopping time, the set of stopping times is not a very nice set. It's not convex. So if you take a convex combination of stopping times, you'll not get a stopping time. And it's not, you need also compactness, okay? Because when we want to do this fixed point, because we always have a fixed point with this mean field game theory, we need some. Game theory, we need some nice properties of the space over which we optimize. We need convexity, we need compactness, and the set of stopping times is not a nice set. However, if we do this, we transform our problem into a linear programming problem, this set A of measures which satisfy this constraint can be shown to be very nice. First of all, it is very simple to observe that is convex. To observe that is convex. So, if you take two admissible flow of measures M and you take a linear convex combination, you'll obtain that the convex combination also satisfies the constraint. So the first thing that one wants to show is the fact that I get a maximizer as usual. So, if the supremum is attained, and this can be shown, not it's, I mean, it depends. I mean, it depends on the situation. It could be less difficult or more difficult to show. But what you have to retain is that the main property which allows us to conclude that we get a maximizer for VR M0 star. So maximizer means that there exists an M star. M star is a flow here. It depends on T, which is admissible. So it satisfies the constraints such that Satisfies the constraints such that when I take the integral from zero to t and the integral over all of f m star, I get the value of my relaxed problem. This can be obtained because we get the facts that the set A is complex. Okay, so this is the main property that we are able to obtain and which is essential to get this characterization. To get this characterization, we have to be careful when we do our proof. If we consider f, which is continuous with respects to t and x, is very simple to show by weak convergence of measures. But what you will observe is that the continuity of f with respect to tx in the mean field setting is not enough. The mean field setting is not enough. Why is it not enough? It is not enough because you remember that my running reward F has to depend also on M. So I have an F T X M. You see here, F T X M T. But as I already said, due to the fact that maybe there are many players who exit the game suddenly, this MT will not be continuous in time anymore. not be continuous in time anymore so we shouldn't expect that f of t x m t to be continuous in time so we need to find a way to deal with this discontinuity of of f and for example if we take f of this form f bar tgx it's quite um it's quite uh simple to to show but we have also more general cases but this is a case i'm interested in because it's also for the application interested in because it's also for the application use okay so uh i don't know how much time i still have so that i uh decide what i um you have a little bit more than half hour okay so uh to prove the existence of the solution in the single single agent agent case uh in this case the In this case, the main property is to show that this function, any function of the form gx mtdx, which will integral over O of Gx MTDX, is a finite variation. Okay, for any m which is a measure which satisfies the constraint. So this is the key argument, which allows us. Which allows us to use a very nice theorem, which says that if I have, let's say that I have Mn, a maximizing sequence, which converges to some measure M star, the convergence in the weak sense, okay, you know what it means, convergence, weak convergence of measures, it means that for any For any continuous bounded, so maybe I will have Mn converges weakly to M in the weak in the sense of weak convergence of measures if for all If for all test functions U which are bounded and continuous you have the integral, so assume that here a man are measures over R. And our measures over R. Let's say take the integral over R. So I have U of X M and D is integral over R U X M D X. So this means weak convergence of measures. And now I will go back to my I will go back to my slides. Okay, so if I take Mn, a sequence which converges weakly to a measure M in the sense that definition that I gave you. Definition that I gave you. Now, if I define these functions g and t, which is the integral over o of g x m and t dx, okay, so I take functions of this form. So, depend the dependence on the measure will be of this form. Then, by using this result here, this basically will mean that these mappings G and T have uniform. and t have uniform bounded variation over the time interval zero capital T and then is a very nice theorem from the book of Ambrosio which says that from a sequence of functions which have uniform bounded variation and which are also bounded and in this case it is the case because g is bounded and the flow of measures are also bounded then we can show that Then we can show that these functions will converge in L1 to some function g star. And then you remember that also my function g was assumed to be continuous with derivatives. So if I take the convergence of gx to mn dx using the weak convergence of the measures mn, I can identify g star with identify g star with the integral over O of Gx m star. Okay, so this is a bit more tricky than in the continuous case. And then from the L1 convergence, I can obtain the whole convergence, the integral from 0 to capital T of F bar T, the integral over O of Gx M and T D X D T, which converge to the same thing where I replace the M n by M star. The M by M star. Okay, so this gives me the existence of a maximizer in the single agent case. And what I already told you is the fact that we can establish, before going to the mean field formulation for using this linear programming approach, we can also show the link to the strong formulation. So, for example, in the case when sigma is unified. For example, in the case when σ is uniformly elliptic and the domain is bounded, but we can extend also to unbounded domain. So, if I take the value function of a standard optimal stopping problem where I optimize over the stopping times, what we can show is imagine that the initial distribution is just a Dirac, M0 star is a Dirac measure, then what you get is that the value function V0x will be V0x will be equal to the value of your linear programming problem, okay? The value function. And then you can also show if you take the unoptimal stopping time from the standard optimal stopping problems. You remember that I told you that an optimal stopping time is, for example, the first time. Is for example the first time when the value function is equal to the terminal reward, but here the terminal reward is zero. You see, I have running reward. So if I take the measure associated to this stopping time, then I can show that it will be also a solution of the LP stopping problem in the sense that will be the one in which the supremum will be attained. Okay, so. Okay, so this is important because we want to show that we solve the same problem, okay? We change the formulation, but we don't change the problem that we solve. And now let's see which is the definition of a Nash equilibria in this case. So, again, I'm giving an initial distribution of my population, which is M0 star. Then star, then we say that a family of measures m star which satisfy the constraint. So always when I have the fact that it belongs to the set A, it means that it satisfies the constraint, is a Nash equilibria for the LP MFG optimal stopping problem. If what happens if I put M star, this M M star, this m star in the running view world. So I made f depend on m star, then I obtain that m star is also the one which maximizes this integral from zero to capital T integral over omega of f mt. Okay, so you see that on the right-hand side of my inequality, I have on both in the running reward. In the running reward and outside, I have M star. On the left-hand side, in the F, I have M star, and here I take any M. So for any M which satisfies the constraint, if I take M star, okay, is the one which maximizes this quantity. So actually, I can write this as a fixed point. Actually, I can write this as a fixed point of a map. Okay, so if I introduce this, I take this function for a given m, if you look on the last line of my slide. So if I take the integral from zero to capital T integral over O f t x m if I forgive f and for given given f and for given m and i take the arc max over m hat uh this gives me uh theta m okay so for each m which is fixed in f i can compute theta m which is the argmax so corresponds to those flow of measures which maximize this basically the objective Basically, the objective function, okay? So, yes. Could you help me interpret this definition, this inequality? Because what I see is for the running reward, you fix your distribution as M star, but when you are computing the expectation, it's just an arbitrary MT on the left-hand side. Left-hand side. I'm trying to figure out what that means. So, I didn't hear well the question. I think you are too far, and I really don't hear well. Can you please repeat being closer to the yeah, hello? Can you hear me? Yes. Okay. I have some trouble interpreting this inequality inside the definition of Nash equilibria. For the left-hand side, you fix your distribution. You fix your distribution as M star inside the reward function reward, but you're sort of computing the expectation using an arbitrary M measure. And like, what does that mean? Like, you're it's exactly as in the case of when we were looking for the fixed points for regular control. So basically, this This term here, the integral from zero to capital T integral of over omega of f m represents the objective function of the representative agent okay when f depends on m star so the population is m star okay so if i so if i find the fixed point as in the case Point, as in the case when I have written, I will go back here here when I had standard stopping times. You remember that I solved for a fixed M and my fixed M was appearing in F. Okay? F was depending on M, which represents the population. And then I take the problem of the representative agent, and I'm looking for the optimal stopping time. So if the distribution of the process X for T before the stopping time to, that distribution coincides with the M that I have in the reward function, then this is an equilibrium. So this is exactly the same for the linear programming problem. It's my fixed point. It's the analogous. It's the analogous. Do you understand now? So, M is the population, and this criteria is the one that corresponds to the representative agent that he tries to optimize. So, if the best response, so the best action, I don't know how to explain, of the representative agent, which is he optimizes over M. And if he's best. And if his best response will coincide with the one of the population, then I have the midfield equilibrium. Thank you, thank you. Okay, so basically I can rewrite it what I said as a fixed point of this mapping, theta m. And what I also want to say is that you see that the spaces that I use might seem to you strange, not the first one but the second one. Not the first one, but the second one. So, the first one, I say that theta is defined on the set of measures which satisfy the constraint, so belong to A. And then I say that it takes values in the power set. This is when we denote 2 and AM0, this is the power set of A. So, why it takes place in the power set? Because we do not necessarily have uniqueness of what we call the best. The what we call the best response of the representative agent. We don't have uniqueness of the best action that the representative agent can take. Okay, so he might take different actions which he might decide at different moments to stop and it's still optimal. Okay, also for the optimal stopping time when for the American option, when I introduced it, I told you that the stopping time, the optimal stopping time that I gave you is the first optimal. Time that I gave you is the first optimal stopping time when he should stop. So he never should stop before. But there are also, there might be also stopping times after that one when it still might be optimal to stop. Okay, so what I want to emphasize is, okay, you don't have uniqueness of the best response. So that's why we need to define this mapping to take values in the power set of the set of constraints. And now, And now, if we take in this particular setting where f, so in the mean field setting, it depends on t, x, and m, so we take f to be of the form f bar, which depends on t and the dependence on the measure is of this form, as I did before, times gx, where g g prime g is nice, it's of class C. is nice is of class c2 uh g prime and g the second derivative are bounded f bar is also bounded and so on so for an f of this form what we can show is that we have at least one equilibria what does it mean equilibria again it means that this mapping theta that i have introduced has at least one fixed point what means a fixed point m is a fixed point is a fixed point m is a fixed point if m belongs to theta of m so i don't have m equals to theta f m because again i don't necessarily have uniqueness of the best response but um in this case we write m belongs to theta of m so we show this we use um a known theorem which is uh the so-called kakutani The so-called Kakutani-Fanglisburg fixed point theorem, which is adapted to the case when we don't have uniqueness of the birth response. And again, the main ingredients for capital Ni Fan this fixed point theorem are the fact that my set A of measures which satisfy the constraint is compact, this is essential. Uh, essential and also that is convex, also, it's uh important. And then, what we basically need to do is to show that this fixed point map theta has a closed graph, okay? But I will not enter into details. If you want to know more, we can discuss. Okay, so we show the existence of an equilibrium. Uh, here again, I present the case when I take a special. Case when I take a special structure of f and the special dependence on the measure, but we have done in much more general settings. This is what I need also for the application. And now about the uniqueness. Yesterday it was a question if we have uniqueness. So, in general, we shouldn't expect uniqueness of the equilibria is difficult to get. You need a strong assumption. We have the strong. The strict monotonicity assumptions introduced by La Srilience, but these assumptions are not compatible, for example, with such a dependence on the measure. We don't necessarily get the uniqueness of the equilibria, but we still can get some uniqueness results. And which are the uniqueness results that we get? What we get is that if we take If we take, so assuming that there is an assumption that f bar is decreasing with respect to the second component, and this corresponds to what we call a war of attrition game, which is a game which is of war of attrition type. What does it mean? It means that it's good for you to be the last one who leaves the game. Okay. And the opposite of war of attrition type games are the so-called preemption games, where you are more interested to be the first one to leave the game. So, here it's for you as a player, it's better for you to be the last one. And for this particular structure, what we get is that if we take M star and M bar to be M bar to be two equilibria, two different equilibria. Then, what we get for all t, we can show that this integral of gx m star coincides with integral of gx m bar. Sorry, there is no prime, it's just m bar. So what we get is that the equilibria, the reward of the representative agent will be the same. So for two different equilibrias, he will have the same. Different equilibrias, he will have the same reward. So, the value of the representative agent will be the same for all equilibrium. So, you will have for all equilibria, you'll have the same value for the representative agent. So, the value is this, I go back, is this VR? Yeah, is this VR? This is the value. Vr. This is the value. So the value, whatever the equilibria is, Vr will be the same. And you'll see for the application that we have a different type of unique results, but for the application, we'll see that this will be enough in order to have an application which is well posed, to have the problem which is well posed, to make sense. Okay, so I think I will not. Okay, so I think I will not enter into the sketch of the proof. What I want to say is that all these approaches that we use to solve minting games are related one to each other. So, for example, when I introduced the probabilistic approach, I said that it's related to the P D approach in the sense that the backward component was corresponding to the derivative of the value function for the P D approach. Value function for the PD approach. Also, this linear programming approach, we can link it to the PD approach. What I mean by PD approach, I mean a system of PDs, as I explained today at the very beginning for the case of regular control. In the case of optimal stopping is a bit more tricky. So this is the PD system. This is the PD system. So, what you can show is that if you take M an equilibria that we obtain using this linear programming formulation, and then you define V star as being the value of the representative agent, what you can show is that this couple of functions will satisfy a system. So, what is the system? I will start with a very simple one, which is the third one, the C item, which is the version. The version of the HGP equation for optimal stopping. This is a standard which also corresponds, for example, when you want to compute the price of an American option. So you have variation inequality. Here I take the terminal reward to be zero. So you take mean between V star. So you should normally have V star minus the payoff, but you don't have any payoff here. And then on the V star, when you When you take the minimum of these two quantities to be equal to zero, basically, what you are computing is the minimum between the exercise value, so the value that you get if you exercise, and the continuation value. This is how they are called. And then we have, you remember that I said that we have a sort of Fokker-Planck equation, which gives the distribution of the population. And this is what we also get here. Also, get here. So, for items A and B. For B, we have, if you want the standard Fokker-Planck equation, the only difference is that the test functions for which we are writing this are those for which the support is in the continuation phase. What it means? It means that this Poker Planck with equality is satisfied in the region. The region for which is not optimal to exit, okay? Because over the whole space, it was an inequality larger or equal to zero, which means that this shows that we can have some loss of mass because the players can exit. But here, on the region in the space, which can be defined by here by C, we have C star, we have equality to zero. Okay, so it means that it's not optimal to stop. And then we have a characterization on the region where it is optimal to stop, S star, which is characterized as those points Tx where V star is equal to zero. So this is the stopping region. So in this case, you have this condition, the A condition, which is satisfied. It might be It might be a bit difficult to understand, but assuming, for example, that f is strictly different from zero, what you obtain is that m star is different, is equal to zero. So on the stopping region, m star is equal to zero. What it means? It means that on the stopping region, no player is there. All of them exited the game. But you might have also some interesting phenomenon. Saw some interesting phenomenon, which is very natural in the midfield setting, is that different players might decide to, if there are several optimal strategies for them, they might decide on different times to exit the game. Okay, and that's why it's not absolutely necessary. This is what condition is saying, that m star is zero on the region on the On the region as time, because the players in the midfield setting might decide to leave the game at different moments if there are several optimal strategies for them. Okay, it might be a bit difficult to understand this in just one hour of lecture, but what you have to maybe to retain is the fact that these approaches are not completely disconnected. No, they are really. No, they are really connected between them. You can, from one approach, to get so if you get the existence of an equilibria in using one approach, you might get equivalence to another approach. Okay. And this is the PDE system. This PDE system was actually studied in a paper by Charles Beth Touchy, who is a former PhD student of... With a former PhD student of Kerry Lyons, where he actually addressed this PDE system. And what we wanted to show is that with this linear programming approach, we recover the system that he got. Okay, what I also want to say for the end is the fact that we can do also numerically to solve the linear programming problem. As I said, there are. As I said, there are many numerical algorithms using the PD approach, the probabilistic approach, especially in the case of regular control. In the case of optimal stopping, there are much less, actually, there are very few. And the algorithm that we are trying to propose is the same, the following one. So it's very natural in the context of minifield games to solve the problem by iteration. Iteration looking for the fixed point. So, how you proceed? You start the first item with a measure which satisfy the constraints. Okay, so you can just take the measure which represents the law of the process, which belongs to the set A. And then you fix a number. And then you fix a number of iterations. And what you do is you see I computed the argmax of this objective function of the representative agent, where in the running reward f, I fix m at the previous iteration, the m that I had the previous iteration. Imagine that you are here at the first iteration. I had the m zero that I fixed. I had the M0 that I fixed at the beginning. So here I'm just computing the R marks over all admissible M of the integral from 0 to capital T integral of omega F of M 0. And I optimize over M. So here I get an M tilde N. So the first iteration is M tilde 1. Okay. I get the best response. And then I have to. And then I have to, I will update the measure flow, which will be a convex combination with well-chosen parameters between the previous M, so it will be, you see, M of zero, and M tilde N, which is the best response that I computed at the previous step. So M n. Step. So Mn will be just an average of all best responses that I got from the first iteration up to the nth iteration. It's very important to do this convex combination and to take into account also the M that you had before, not only the last best response, because otherwise the algorithm would not converge. Okay, so to ensure the convergence, you need to, and also with You need to, and also with a bigger weight to put bigger weight to the last value of m. You see, it's n minus one over n and m the previous m and then with much little weight you take the last best response. So this is basically when you take into account all best responses, it means that you learn. Okay, so you improve for M more you make progress. You make progress in the number of iterations, more you learn the equilibrium. And in order to assess the convergence, you monitor the exploitability. This is a notion of error, which has been, which is used quite a lot when we talk about this kind of algorithms, which basically measures how far is the output of your algorithm from being. Of your algorithm from being the Nash equilibrium. So, in our case, what we observe is that if we monitor this error, we see that it's of order one over n. What I want to say is that, okay, the algorithm looks very simple. It's not very difficult, actually. Of course, that here I didn't introduce the discretization part because you have to discretize your Discreticize your measures, your flow of measures. And also, what you should know is that there are special solvers, for example, in Python, that you can use in order to solve linear programming problems. For example, there is the Gurobi solver, which is very fast, is very efficient. Okay, because basically what you have numerically to do at each iteration, you have to solve a linear programming problem. A linear programming problem. Okay, so at each iteration, you solve a linear programming problem. So when you compute the best response, this is what you do. Okay, so this is the algorithm. We get also the interrupt. You may want to wrap up maybe a minute or two. Sorry? You may want to wrap up in a minute or two. If I want to. If I want to, sorry, I don't hear you. So you've got one or two minutes left. Okay, okay, thank you very much. Yes, I will finish now. Okay, so I've not entered into, we can also prove the theoretical convergence of the algorithm. And just to conclude, which was the advantage of this linear programming approach? First of all, if you look at the third item, is the fact If you look at the third item, is the fact that it is very well adapted to deal with mid-field games with optimal stopping because you have this lack of regularity of the flow of measures, which makes it difficult to use the PDE system. There are many interesting applications to investment timing, industry dynamics, and we see an application in the exercise session. And also, what is nice is you have seen. And also, what is nice is you have seen that, for example, when we work with the PD approach, we have the equilibria is characterized by two objects, the value functions and the distribution of the population. While when you work with the linear programming formulation, we formulate the problem exclusively in terms of the population measure flow m. So you have seen that I have been interested in only one object. In only one object. This was the distribution of the population M. So this simplifies at some point the analysis. Then, what it was essential is the fact that you have to relate your measures to your initial problem and to your initial process. And that's why we have a linear constraint on the measure flow, which expresses the fact that the measure flow is the flow of. That the measured flow is the flow of marginal laws of a controlled stochastic process. Also, the theoretical analysis was much simpler because my new set of controls was compact and convex, while the set of stopping times does not have these nice properties. We can also do numerical algorithms as the one that I Numerical algorithms as the one that I presented. Then, what I said is the fact that using this approach, we can also take into account the fact that different players might take different decisions if there are several decisions which are optimal. And this is completely natural. Okay, when you have a population of players, they might decide differently if there are several. Differently, if there are several optimal options for them, and we also recover the initial strong formulation, and this is, of course, extremely important because we need to solve the initial problem. Okay, we have a list of publications you have on the slides also on the folder if you are interested to look at the papers and the applications, and I will discuss a bit more. And I will discuss a bit more about the applications during the exercise session. Okay, so I think I will stop here. Thank you very much. Any questions? Oh, could you go back to page 31? Which slide? 31. 31. Yes. I'm wondering how you obtained. I'm wondering how you obtain the art maximum. Is your set a finite element? So, okay, the set of elements is not finite because you work on infinite dimensional space, but as I said, when you do the numerics, we have to discretize your measures. And then we use a solver. There is a solver Guru B, who computes the arc max of this map under the constraint that. map under the constraint that m belongs to a so this is a standard linear programming problem uh that we uh recast in a fine fine uh finite dimension setting and we used the special solvers for in python it became finite dimensional because you did the discrete value yes yes yes yeah uh could you go back to uh page 17. Yes. Yeah, I'm wondering on the left-hand side of the last equation, I'm wondering why the time interval for the integral is from 0 to capital T instead of from 0 to tau land capital T. Because here I write those formula from u to 0 to time. From u to zero to time tau. And then the integral from zero to toe can be written also as the integral from zero to capital T. And inside the integral, you have the indicator that t is smaller than to. And that's why I can, when I take the expectation to use this measure m, where you see that the measure m, the definition is the expectation of indicator of axt, indicator of t smaller than to. I see. Thank you. Any other yes um I guess this technique is more uh efficient than deep fictitious play, right? Because like in a deep fit uh deep b with a deep BSD, we have to solve for each player in parallel. In parallel, and then no, no, because when you used, for example, what I did yesterday, okay, where I used the deep BZ, I didn't do for each player. I mean, when I want to compute the midfield equilibria, you have the system at equilibria. Right. And you have one system. Then, of course, I also had some plots with different trajectories. Had some plots with different trajectories for different players, but this is: I don't solve the system for each player. I just take the final, I just take the mean field solution and I apply to the individual dynamics of the players. As I explained before, when you want to go from the mean field game problem to the end player problem, you get a proxy equilibria. Okay, okay. Okay, I see. Yeah, so you are so only here. So, here, where I did this, here, you see, here I explained that you get the solution, a title two, and then you just apply to the dynamics of each player to get the proxy equilibria. So, for the DPSD, this is exactly what we do. We solve the system for the limit problem, but not for each player. So, just for the representative player, and then we can. And then we can obtain some proxy equilibria, some proxy equilibria for the end player game by doing this kind of thing. Oh, okay, okay, yes, yes. It's clear now. Thank you. You're welcome. Any more question from the Zoom? Any more question? All right, so if now let's All right, so if not, let's thank Rosanna again for the two lectures on the minfield gain theory.