So, something that if you haven't seen this talk before, you might not know about that we can represent graphs and networks with point processes and why this is an interesting way of doing that. I'm going to go on with my laten space networks model. So, telling you about the model, asymptotic properties, some posterior inferences, some real data examples. And if you already heard this talk, the real data examples is something that I just done. So, there's something. So, there's something new for you as well. And I'm gonna conclude with probably not being able to, but like these overlapping dynamic community networks that I'm working on with another collaborator. Okay, so let's start with the idea. The idea is that if you look at networks, you can look at a lot of different characteristics, but one of them is the density of the edges. So, the intuition is kind of like simple. If you've got a lot of edges, you're in a dense. Got a lot of edges, you're in a dense setting. If you don't have that many connections, you're in a sparse setting. There's, of course, a formal definition, which is an asymptotic one. Okay, so if you look at the number of edges as the time grows across by, so this P is the time, and if this grows as the number of nodes squared, which is the maximum rate that you can have because edges connect nodes, then you're in the net setting. Otherwise, like if it's a little low, then you're in. If it's a little low, then you're in the sparse setting. Now, the vast majority of real-world big networks are sparse. Think about social networks. Let's say that you're very, very popular on Exxon slash Twitter, slash Instagram, whatever. Like, even if you have a lot of followers, I'm sure that the number of people that don't follow you and don't know about your existence greatly outnumbers the number of your followers. So, sorry to be harsh, but the word is maybe sparse. Like really sparse, and so we really want to like a lot of models that we're going to see are able to capture very well-dense networks, like the small ones and very dense, but they don't really do well with sparse networks. So, you really want to have something that can take that into account. So, what's the problem here? Well, we start with the adjacent symmetrix representation, which is the most classical one, and it's great, and we like it's super simple. And if we want to mutation, we usually use some exchangeability assumptions. Exchangeability assumption. So, this was mentioned yesterday in Francesco and Beatrice's talk: that the exchangeability assumption here is that if we permute the labels of the nodes, so we permute nodes and columns together, then the distribution of your adjacent symmetries is not going to change. That's great, but there's a theorem that proves the Aldo Silver theorem that with this setting, you can only describe dense and empty networks. So, you're like, there's nothing you can do there to describe sparse networks. I mean, you need to like. describe sparse networks i mean you need to like come up with like weird way not weird but like more refined ways but like in this setting like forget about it uh like the proof is like literally like two lines but if you want to have a more overview about this problem this is nice later by the channel that i think is always been just an archive um so what carone and fox did in this paper in 2017 was to say okay let's forget about the adjacency matrix and let's use point processes and so we got z Processes. So we got Z, our network, and now it's going to live in a positive plane. So these theta, i, theta, j are labels of nodes. So in position theta i, theta j, you have a spike z i j with maybe one. If there's an edge between i and j, zero otherwise. Okay, so it's kind of like, I mean, it's kind of simple. The like really smart thing here is that for point process, you have a different notion of exchangeability, which is basically like look at a wind. Which is basically like look at a window in your plane and then shift the window, and the distribution of the points is not going to change. Okay. So turns out that they can prove in the paper that with this notion of exchangeability for point process, you can model both sparse and dense networks. This is not the only way to solve the problem, but it's the way that I'm going to follow. And I think it's like, you know, kind of like a neat way of doing it. So Caroline Fox proposed that. So, Caroline Fox proposed a base model to do this, which was the following. So, we need to specify z ij. So, zij is edges, so it's either 0 or 1 in this case. So, we're going to get a vernally, fine, with a certain function here, which is an exponential function of this w. Now, w is what we call sociability, and this is a function that is increasing in w. So, guess what? The more sociable you are, the more connections you're going to make. This is a latent variable, okay? Is a latent variable, okay? Further note we kind of want to discover it, and it works well if you want to model, like especially if you want to model scale-free degree distributions. So things like power loss, because there's this idea of W is that it's kind of like how sociable you are is going to be related to how many nodes you have, how many connections, neighbors you have. And by putting a prior that I'm going to tell you about later that mimics this power law behavior. That mimics this power law behavior, you're gonna be able to describe power law networks, sparse and power law networks. Now, this is great, but of course, it's a bit limited if we think that degrees are not the only, like sociabilities are not the only thing that is driving the connections. So, for example, like a classical thing that people in networks do is to think that there might be some similarities between nodes that is going to tell you, like, if two nodes are very similar, they are going to most likely be connected. So, let's Connected. So let me go through the spoiler of the two real-world data examples that I'm going to look at. So on the left, you've got a network, a sub-sample of a network of like one year of United States flights. So what does that mean? There is a connection between two airports if there wants at least one flight. And here on the right, I got commuting in Italy. So this is commuting for work and study in municipalities in Italy. I'm going to tell you more. In Italy. I'm going to tell you more later. But so basically, there's a ledge if there are some people commuting. Now, like the Italians are like, what is happening? Like, nobody's commuting from the north. No, this is like subsample to be centered around Nazi, which is the region of Rome. I'll explain you later. But so the idea here is that, of course, like a city like Rome or like New York, Atlanta are going to be big apps and they're going to have huge degrees. So having a high sociability is going to be very good. Having a high sociability is going to be very good for them. But there are other things that are influencing the connections in these networks. We cannot be so naive that they just like think that it's just how big the airport is influencing all these connections. Maybe it depends on your geographic location. Or maybe there's something else that we are not even able to account for or think about. We might not have covariates about. So, what we are going to do in this talk, and what a lot of people do, is to take these networks, don't say anything about like. Don't say anything about like geographic locations or covariance, but try to embed it inside a latent space. And then we're going to use this latent space to figure out what are the driving forces of the connections. Okay, so this is hopefully the title of whatever I'm going to put on archive, I hope soon. Modeling sparse latent space networks with patient on parametrics is joint work with Francois Caron, the same guy of the paper, of course, of the 2020 paper, and Juliet Rousseau. They were my Paper and Shuddhrousau, they were my supervisors at Oxford during the PhD. Okay, so the model looks very similar to what we've already seen. There's a point process, the only thing that I've added is this location xi, latent location, so no information about geography. I'm going to modify the function that like tells you the probability of connection. So it looks very much similar to before. Like you see this part here, the 2w and wj is exactly the same, but then Is exactly the same, but then I'm dividing by this function of a distance between xi and xj. Now, this function is decreasing in the distance, so if the nodes are really close, they're going to, in the latent space, they're going to most probably connect it. If they're very far apart, they're not going to be. That's it, yeah. Now, we want to do inference on this W and X, so this is what we really care about. And we are going to put a prior on theta, W and X. Now, theta, you can kind of forget about, it's always this label. You can kind of forget about it's always this label, but it's not super important for this talk. Focus about w, sociabilities, and x, data locations. Um, so this is the BMP part. So, we're going to use a completely random measure there, in particular, a Poisson-point process with a certain intensity, which is nothing fancy for X in particular. It's a unit rate Poisson process. We could be a bit more fancy there, admittedly, but let's stay simple. And then there's this rho dw. Now, rho dw is. The raw DW. Now, raw DW is going to be a levy measure and it's going to be the distribution of my social abilities that it's going to enforce the power law. How am I going to do that? Well, here, VNA's talk is going to help us. So if you listen to that, she talked about generalized gamma processes and she said, well, she was working in the normalized version, but she already said that they are regularly varying families. So, what does that mean? Like, for me, the intuition is the following: that you see that You see that? No? Okay. There's a like we managed to send the like the men to the moon, but we are not really doing great with pointers. So there's this part, the Eilig one, y to the power of minus sigma. So that's a power law function with exponent sigma. Wonderful. That's what I've been waiting for. Okay, we got a power. Like, be mindful. Like be mindful. So we got a power law with exponent sigma, and then we got another function. This function is a slowly varying function. What does it mean? That it behaves not widely at infinity. So it can do two things. Either like go towards infinity but slowly, like the logarithm that's slowly varying, or go towards a constant at infinity. So what this actually means, so regular variation, what it means for me is that it looks like a power law. Is that it looks like a power law, kind of, but then we are allowing for some flexibility around it, which is given by the slowly varying function. So, power law should ring a bell because I already told you about degree distribution. So, this is again the same thing that with W, with suitabilities, we want to model power low degree distribution. So, this is how we're going to achieve it. But it's not the only important bit here. So, we got some theorems there, proving some asymptotic results. Remember that the notion of density and sparsity is asymptotic. Density and sparsity is asymptotic itself. So, if we look at the number of edges and e here, um, now this behaves in three different ways depending on the value of sigma, so of the power log exponent. If sigma is negative, it grows like the number not squared. Now, that's exactly the definition of tensor. First slide. Okay. If it's positive, but it's between you see zero and one, it behaves like NT2. It behaves like nt to the power 2 over 1 plus sigma. That exponent is less than 2. So it means that it's a sparse network. And it also means that this sigma is actually tuning the sparsity level, not just like if it's dense or sparse, but also how much sparse it is. And then there's sigma equal to zero, which is something in between. If you want more results about all of this framework, this is a real parenthesis. There's another paper that we published last year where we work in the more general Where we work in the more general setting. So we don't specify this one minus exponential, blah, blah, blah. We leave it free to vary. And this frame is actually called the graphics process. So there's a more theoretical paper there to prove a lot of stuff. Okay, so for the BMP completely random measure VX here, there's an if and only if results that tells us that a graph is dense if and only if the completely random measure has finite activity and sparse if and only if it is infinite activity. If it is infinite activity, so the number of jobs at the complete random measure. Okay, so that's what I promised you that we were going to be able to describe both tests and sparse networks. And then I promised you also a result on the degree distribution. So the number, the fraction of nodes with degree j is bounded upper and lower by this j to the power one minus sigma, which is again a power law. And again, expectedly, it depends on sigma. So sigma tunes both, the density, the density is. The density is partially level and the power low degree distribution. Like, if I simulate different networks with different signals, I get different slopes. That's the degree distribution. Okay, not log logs. There is something more, but other like results about clustering, but not going to mention them here. So, regarding like posterior inferences, this is, of course, the painful part. Now, we are not going to be like Now, we are not going to be like super BMP here. Like, you can do exact inference here of the infinitely many parameters that you got, but we're going to use this finite IID approximations of completely random measures. So, this is a nice, like easier, let's say, parametric setting that we are guaranteed to converge to our infinite measure, completely random measure. And I encourage you to check out the paper by Julie and Cotters that provides some approximations, IAD approximations. Provide some approximations, IID approximations to complete the random measures. Now, the algorithm, this was basically what I did during my PhD, was to confirm it to somebody else the other day. I basically spent my PhD computing derivatives and making mistakes and recomputing derivatives. So it's a mix that gives HMC and metropolis things. Unfortunately, it's not super, like, it's the complexity is the big piece of networks. So it's big of n square. Of n square. It can be lower down if you're smart about it with n log n, but I didn't like really focus on that. But there are things to do, and I'm going to solve into suggestions. Okay, so we want to do it just to like, we want to do inference on w and x, okay? So shabbability is a location, and then hyperparameters, so sigma, and then there are others that are kind of like under the carpet, but they're there. Okay, so let's go to the examples. The US airport data set. US airport data set. So it has like: sorry for Alaska and Hawaii being excluded here. Okay, so we got 800 nodes here and something a bit more than 11,000 connections. So flights among them. So again, what we are going to do is that we're going to give the adjustments to matrix. If the adjacent symmetrix to the model is not going to know anything about the geography, for example. And we kind of want to know if there's something else rather than being a hub and not that we can model with it. Okay, so first thing is the effect of sociability. So the true, like the dots here are the true GB distribution, and the band in yellow is my credible intervals with the posterior predicting, which are a bit too confident to be fair, but it's kind of like. To be fair, but it's kind of like okay. So, this is what I told you about the fact that with W's, we are modeling the power loading redistribution. So, that's something. Now, the latent space, which is the new part, if you want, it looks like this. So, this is I arbitrarily decided to pick a two-dimensional latent space. And so, each of these dots represents a lamp in my latent space. As you can see, it's a perfect representation of the United States. Okay, not really, but like, let's see if it makes sense. Not really, but like, let's see if it makes any sense, okay? Uh, we don't really expect it to look like the US map, okay? Um, so I'm gonna color things by longitude and latitude and degrees. So, what do we see here? So, we see that it's kind of like very able to distinguish between what is west and what is east. So, this is, I think, expanding given how the US look like. There's a lot of stuff like closer to the coast. Uh, and so a lot of people are traveling on their side of the US. On their side of the US, I think. Not so much, like, so there's not so much division in the way that connections are constructed between south and north. Although I think there's something interesting, I think, see this like bunch of dots here and here. Also, this one, they're kind of like colored a bit differently. Low degrees and high degrees. So, we get a lot of like high degrees along the perimeter. Then there's something like even going on here if you want, and then like. On here, if you want, and then like low degrees of an are kind of like staying in the center, which is expected. Um, so the model, I think this latent space is trying to tell me to tell me something. So, I'm gonna like look at like clusters of nodes and see what they mean. So, let's zoom in the latent space. So, if I look at like this latent representation here and I map them back, so I like I put in black, if you can see, what are the inputs corresponding to this square here. Corresponding to this square here. So, well, we got like a bunch of like nodes from the west side of the US, and here a bunch of nodes from the east side that's expected. But then there are other things that I think are interesting. So, this thing always comes up. So, it's a bunch of airports in the northeast of the US. So, this is like New York, Maine, Rhode Island, Massachusetts airports. So, my guests there, I can actually tell you. So, these are the airports. So, these are the airports. So, the first thing that I noticed was that there's a really high edge density, like 0.56. Like, the total edge density is 0.02. So, there's a lot going on there. So, people are flying a lot there. And I think, like, you can correct me, it's it might be, well, there are some islands there, so it might be convenient. I feel like there are a lot of rich dudes there flying. Because you need to think that this is not just commercial flights, this is also like private. Short flights use also like private, like Taylor's Wave or whatever. Okay, so then there is a so this section here, which was already in what, like in the north-south, it's like Seattle. So like it's very connected there. And then I'm ah yeah, I should have checked this before the talk. Can anybody tell me what is going on here? Nothing. That's where you live. I think there's something going on. I think there's something going on. The please go with me to check some politics. I'm gonna like think there's like a bigger airport and then uh well I'm gonna I'm gonna double-check. But there are some like things that the model is telling me. Okay, so like does it make sense to do all of this? So here I'm confirming my little space. I highlighted the house, so the 20 highest degree airports, and they're kind of like you see a bit spread out there. And then I used And then I use this multi-dimensional scaling algorithm. Now, this is an algorithm that doesn't account for switchability, it just accounts, like provides you a little space. You just need to give it like the adjustency metrics. And this is how it looks like. So what do I see here? Like all the high-degree nodes are in the middle. This is because it's not able to disentangle the contribution of like having a huge degree, because these guys have a huge degree because it's power low, okay? So there are a lot of degree one loads and then a few like high degree. And then a few like high-degree notes. So, this is what it's telling me. Oh, like, I'm just able to say that, like, there are some very important notes here. While you see that we are like spreading things around, and this is a bit more informative in terms of understanding how the network works. So, this was for the American side of the room. Okay, now we're going to go to the Italian side, which is also heavily represented here. So, this is this Latin community in data comes from the Italian census. Data comes from the Italian census of 2011. So I told you that this is like people, so these are commuting between municipalities, so not municipalities, commuting either inwards towards this region of Lazio, which is somewhere here, or outwards. It's a much sparser data set than I had before. I decided that I needed at least 10 people commuting between municipalities to include an edge. Okay, so let's look how it looks. So it's a bit more difficult. Let's look how it looks. So, it's a bit more difficult than the airports example is kind of neat and works well here. And things are a bit more like it's still, I think it still works well, but I wouldn't expect it to do so necessarily. So, what is happening in this latent space is that I clearly got four corners of notes. And so now I'm referring mostly to the Italian. So, what is, I was surprised to see that they're kind of able to recover, like sometimes it's, I mean, almost perfect, the province. Almost perfect, the provinces of that. So that is this part here. Okay. And so the black lines are provinces. So inside the region, there are provinces. So this is Frozilone, this is Viterbo, this is Rome and Latina. So there is like a lot of so again, I didn't give any information about the geographical structure, but still the model is kind of able to reconstruct the people are commuting a lot. People are commuting a lot inside their province, which is fine. From a local authority perspective, you can really say, okay, so it makes sense that I focus on doing a lot of province transportation, for example. But it's not the only thing that I think is interesting. So I looked at high degree notes there. So forget about the corners, we kind of already got them. And then there are like four towns here. I'm going to focus on a few of them. So in the center, we got Roma. In the center, we got Roma. Now, that's expected because the way this network is constructed is that, you know, the old roads lead to Roma, and it's also true in my concept. So it makes sense. It's like kind of like in the middle. Great. And then I was like, what the hell is Casino? Now, I knew that Casino was famous because of Saint Benedict's. Like, there's a monastery. It's like very famous from the Middle Ages. But it's like, is this why people are commuting? I don't think so. It's like either work or study. Turns out that there's actually like a lot of people way more than the rest of Latsi commuting. Way more than the rest of Latsi computing there because there's a university, so people commuter for study, and they're coming from like other regions, so they're they have a different structure of commuting with respect to like more industrialized parts and towns. So these are people that are commuting for study and they're coming from far away. Okay, so this is why it's towards the center. And then, as for example, Formia here. So, Formia is a town on the coast, and it's the biggest town. On the coast, and it's the biggest town in the south of Lazio before Campania starts. Campania is where it's the region of Naples. So, formia has a lot of connections with the south of Italy that other cities around there don't have. So, I think this is cool because compared to, I mean, I still need to do the experiments, but compared to something like a stochastic block model or like a community detection, you wouldn't be so able to distinguish like single series there. While here, like it's While here, like it's more flexible, so we are able to interpret things with more flexibility. Yeah, there's a bit of a ball in there, but I think it makes sense, so I'm pretty happy with this. So I'm just gonna say briefly nothing about this, apart from the fact that I'm also working with Sayamis Gridu, she's from University of Cyprus at Imperial College in London, about dynamic communities in sparse networks. So this is another sort of model that belongs to this graphics framework. In this case, we're gonna Graphics framework. In this case, we're going to do dynamic communities. Not going to say anything about it. And I'm just going to conclude. I don't have the time. I don't want to run longer. So I'm just going to conclude saying that graphics is, well, I think they are kind of like nice frameworks for networks. Of course, there's the BMP part, which is kind of like pushing people outside of that. But I think it's still like a very, you know, it works well for like sparse and dense networks. Like even big networks, if you're able to see. Like, even big networks, if you're able to scale the computations, it might not be true in my model, but other models that have been done are much more scalable. It adds some nice things like the power law or positive clustering that I didn't talk about. There are some interpretable parameters like sigma, I open convention, like this parsity slash power law thing. Of course, you can do a certain quantification, facilitation. And the last point for me is that, like, a more user-friendly. For me, is that like a more user-friendly but less computationally heavy inference would be a dream, and that is kind of like what I'm looking forward to. I think that's it. Thank you very much. Thank you, Francesca. Any comments, questions, reactions? Everyone is happy. Very cool. Thanks. I have a question, sort of. I have a question sort of vaguely about identifiability of like separating sociability and the location problem. I guess I imagine because the location problem is just as fight in such like a distance-oriented way that that really helps with that. I was wondering if you could just like talk about that. Yeah, so I've like the most the biggest problem of the identifiability is between the axes actually because like first being like locate like it's the location so like if I shift it's not rotate. So what I'm doing is that I'm using this thing. Rockrustian transformation, which is you're basically taking a set of like initials. So I start from these MDS locations, and at each iteration of the MCNC, like apostate or they don't need to do it during the chain, I'm going to rotate everything to match that thing. So it's okay because of that. Very cool. Any other reactions, comments? Yes, very nice talk. So, uh, so I guess the sigma parameter detros the sparsity of the network. Do you have to fix it? No. No, no, there's a there's a prior, it's a like a Vega upper prior there. It's a kind of like flat one in proper prior. And so, yeah, no, it's kind of that parameter is not really an issue. There are other parameters which are a bit more. Other parameters which are a bit more not mixing super well, but that one is fine. So, there is an IPR prior, so we don't really fix it, although there are like some, there are otherwise some estimators that you could use that have been like fine in the literature. I was wondering about the parameter sigma, which you put input to zero. What is that? That's not sparse, that's not dense. That was not bents, yeah. So, that's something in between. I think it's called like almost sparse or almost sparse, I think. Uh, we don't really like, so most of the like all the examples that I'm gonna work in the like in the applications are from sparse networks because this is kind of like the new thing that we're able to do. So, we don't, but you could, I guess you could like look at like at least like the credible intervals of your estimates to see if like it gets into that regime or not. Also, the approximation to the complete random measure only works for sigma positive, so that's also nice. Thank you for the super nice talk. I'm curious again: how crucial is the cross-run process as something? Recently, I saw things like combining attractive and repulsive processes, like Argentine folders. How did you call them? The Argentina photos try to combine different point processes. things like that globally are repulsive and locally attractive okay so can you do something similar obtaining something that globally sparse and local locally stense for example uh so yeah i could change the the prior and the locations absolutely uh that would be a smart thing to do actually because it's not really like maybe this is like now too naive um there are other things that like you can also do that for the w's though for example like the the prior on the w example like the the prior on the w so there's this other work that looks at like core periphery structural networks so like a lot of like bunch nodes connected in the core and like in the periphery so there are some things that you can do there so yeah there's flexibility you know i but it's uh it'd be a totally different model and set up right you would actually use the locations and no notion of social variables yeah so that's the that's the what i hope to do with a phd student to actually incorporate covariates because here like everything is latent so Because here, like, everything is latent, so I don't, I'm not gonna input any covariant there, but my hope is to be able to actually do that because that would make it more applicable. I'm waiting for the next couple. Maybe like two or three next up. Any more comments, reactions? So let's thank Francesca again for the night. 