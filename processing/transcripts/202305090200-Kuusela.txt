Right, so okay, the last speaker of this session is Mikel Kusala. So he will be talking about something very similar. Not the same, of course, but quite related. Neural likelihood surface estimation for intractable spatial models. So the floor is yours. You have 30 minutes from that. Thank you. Just make sure this works. Yes, it does. Yeah, okay. So I'll be talking about very related topic, but here we actually like to. But here, we actually do like local. So, the setup is similar. We assume that we can easily sample data from a spatial model and we want to do parameter inference. The question is, can we do that using neural networks and use neural networks to basically learn the likelihood function and do all the useful stuff using likelihood functions? So, the first thing I should point out, this is joint work with Julia Balchison, who's one of my students at CMU, and Ahmed Dad, who's also here. Also, also here. So, so Julia has really done all the hard work here making this work and implementing it, and done all the experiments here. So, really, a huge thanks for this work goes to Julia. Okay, so what's the background and what's the motivation here? So, they are all interested in fitting spatial models to increasingly large and complex data sets. If you know about my work, my favorite is. But my favorite example of a large and complex data set are the Argo float data. Here's an example of what these data look like. These are floats that are deployed in the global ocean and they sample profiles of temperature and sanity every 10 days on a global scale. There's about 20-25 years of this data right now. There's altogether 2.5 million profiles, each of which might have up to a thousand observations. It's a huge data set of the global ocean, which is really important for understanding the For understanding the global climate. And fitting spatial models for data like this is complicated. These data are very large scale, they have non-stationarities, they have non-Gaussianities, it's multivariate, there's all kinds of complications you can imagine, which makes it very fun to work with. And really sort of the motivation for this protocol work goes back to the Argo data. So, one of the things I've been wanting to solve with this data for a long time now is the non-Gaussianity of these data. We actually know these are non-gaussal. Of these data. We actually know these are non-Gaussian. Some of my previous work has shown that they're non-Gaussian here, which we are not really able to capture right now. So, a couple of years ago, we started to fit increasingly complex non-Gaussian models to data like this. And we kind of quickly found out it's very difficult to do. We really don't have very good methods for fitting non-Gaussian models. Like, basically, anything that's available is either slow to use or is based on approximations that are not very high quality. So, the question is: how do you fit? Is like, how do you fit models like that to data like this? So that's indeed the motivation here. So, what we want to do, we want to do like huh-based parameter inference for large and complex spatial data. And you all know what the problem is. The problem is that it's either slow to do or your likelihood function might be actually completely intractable and available to you. So, for Gaussian processes, we all know that they scale badly. So, if you have a large amount of data, they become slow to fit. Large amount of data, they become slow to fit. And then you don't really need to go much beyond Gaussian process to kind of end up in a situation where you like your functions simply not available to you. You either have some kind of integrals which are intractable or you have some kind of sums that are intractable. Even things like, you know, just change like the nugget to have a non-Gaussian distribution, then, you know, you need to use things like Laplace approximation and so forth to fit this. And in this workshop, we've heard a lot about max table process. So that's a very nice example of a process that is intractable. Process that is intractable. You have the sum that is exponentially growing, and you can't really evaluate likely functioning for moderate sample sizes. So, a lot of the past research has focused on various approximations for situations like these. Here's a list of just some of those. You all probably have your favorites among those approximations. So, a huge amount of work has been done to kind of provide approximations to active function. So, now, kind of, recent interest in this phase. kind of recent interest in this space has kind of focused on machine learning models we just heard a talk about that uh so so how do you use machine learning to to to estimate parameters in models like like this um so um so the kind of the relevant connection as was nicely outlined in the previous talk is into something called simulation-based inference or likelihood free inference these are they're basically the same thing as far as i know or as far as i can tell um uh and um and and there has been some work uh in you There has been some work in using methods from likelihood-free inference and simulation-based inference for spatial models. There are at least three papers that do it as far as I know, and we just try to talk about the last one here. But really, this stuff actually goes back to a whole different community. There's this whole community of people who focus on methods for simulation-based inference, like to free inference. And these kind of tools and methods have actually been developed in. And methods have actually been developed in the physics community, and this is because when you work on various kinds of physical systems, you usually use simulators to model your data. I work a lot with orgo physicists, and they have these huge simulators for simulating particle collisions. And the question is, how do you do inference in situations like that? When basically your statistical model is a simulator. So you don't have like a likelihood in sort of the classical explicit form. You cannot write down a likelihood. Form, you kind of write down a likelihood, you cannot even evaluate the likelihood. The only thing you have is the stochastic simulator. That stochastic simulator somehow includes a likelihood function. And the question is, how do you do inference there? So the setup really is that, so we have, and so one thing, this term is a little confusing in the sense that likelihood free doesn't mean that there's no likelihood. There's still likelihood. It's just that this likelihood you cannot evaluate. It's not directly available to you. So the setup is that the likelihood function, which still exists, but is simply not... It still exists, but it's simply not available to you. But what is available to you is a way of simulating data from your forward model. So for any given parameter, you assume you can simulate data from a statistical model or simulator or whatever model you have available for your data. And then the question is, how do you do inference even in the situation where you cannot access the light? So there's a lot of work that has been done in this space, especially within the Been done in this space, especially within the physics community. There's this nice review that you can have a look at. And basically, the idea is to use machine learning to one vertical, either learn this likelihood or learn things like likelihood ratios or cost errors or other kind of classical tools for statistical inference. So, and this now directly connects back to these three papers. They are really the key insight was that basically statistical modeling of spatial data. Modeling of spatial data is really a likelihood-free internal or simulation-based transform. In most spatial models, it's easy to simulate random models, even though the likelihood might not be available to us and we don't necessarily know what it looks like, it has interactable things in it, it's usually fast to simulate data from our models, just the way they are set up. So that now brings up the possibility of doing simulation-based inference, or likely free inference for spatial processes. Microphone transfer spatial processes, which these papers have done before. So, what has been done in those papers is to do something I would call neural prediction. So, the basic setup, and we just heard a talk about this, so the basic setup is that you take a spatial field, you give this as an input to a neural network, and the neural network kind of spits out a point estimator of your parameter. And typically, the way you train these neural networks is that you use the mean square error loss. Means per error loss, which, of course, for infinite samples is simply this expectation. And as was really nicely explained in the previous talk, this basically corresponds to using the conditional expectation as your parameter estimation. So for infinite samples, these neural networks are supposed to converge to the conditional mean of your parameter theta given your field y. So, of course, in practice, we do this for finite samples, and here's kind of the This is for finite samples, and there's kind of the you know all the stochastic gradient design and all those you know things kind of going on as well. So, ultimately, what this really means is that these theta hats that come out from this neural network, there's some kind of regularized finite sample estimators of this conditional expectation. So, and you would hope that for infinite samples, they hopefully would converse to this conditional expectation. So, as far as I can tell in these previous papers, this is what has been done and. And so, this exists already. So, first, we want to do something else. Let me just tell you what are the limitations of the previous approach. This is very impressive work that has been done at this really prime-creative work, but there are some limitations. So, the first limitation is unstream quantification. So, we just heard that in some of these papers, there is bootstrapping for unstream quantification. There might be ways to get critical intervals, but you cannot get kind of fantastical, likelihood-based confidence. For example, getting guaranteed. For example, getting guaranteed coverage confidence sets for projects like this is not immediately clear. Actually, there are things you can do, but it's not immediate and it requires kind of further work. The other thing is that, as was really nicely mentioned or explained in the previous talk, these are really basic estimates. Like this is really kind of a posture or mean, right? So this means that it depends on a prior. So what is your prior in this case? Well, your prior in this case is the distribution from which you simulated your training parameters. Your training parameters. So the prior is simply whatever distribution you use to simulate your training parameters. Usually it tends to be uniform or like a Lacan hypercube, but it's still a prior. It's still a prior distribution that is kind of driving those base and posture and mean estimates. So, I mean, so it's clear that this condition expectation has to depend on this prior because it's integrated over the posture, which is, of course, likely times the prior. So there's no way around the factor. times the prior. So there's no way around the fact that these point predictions depend on a prior. They're really Bayesian estimators that are done in those previous papers. And then the last complication is how do you handle multiple realizations? So that was the motivation for the previous talk. And at least in some of these previous papers, what you would do is you kind of use like a larger neural network if you have replications. I'm not actually not entirely sure if this is still true for the permutation invariant approach. To further permutation invariant approach, but, anyways, it's kind of a complication. How do you handle multiple replications? So, the motivation for our work was that if you were able to do likelihood-based inference, you wouldn't have these limitations. And if you were able to somehow learn a likelihood function, you would be able to do all the kinds of things we do with likelihood functions without these complications right here. So, what I'm going to do in this talk is to tell you how to do that. To tell you how to do it. So, we have a way of training a neural network to learn the likelihood function for spatial processes. It has all the benefits of classical likelihood-based inference without this limitation. So the syllabus is basically this. So, the left-hand side is what has been done before, and the right-hand side is what we do. So, what we want to do is to train a neural network that will learn the likelihood function of our spatial process. Of our spatial process. So, the likelihood function is really kind of a function of two things. It depends on the parameter, which is, of course, the way we use the evaluator function, but of course, it also takes the data as an input. So, if we want to train a neural network to learn this object, this neural network will have to take both the field and the parameter as an input. So, that's the first difference. For us, the input is both the parameter and the field, while in the previous work, it has been just the field. Been just a field, and then for us, the output is going to be something that's proportional to the likelihood function. And then, once we have that, we can then extract from that a likelihood surface. We can extract from that the surface of the likelihood function as a function of the parameter. And then we can do whatever we always do with likelihood surfaces. We can do maximum likelihood, we can do approximate confidence sets, likelihood ratios, you know, anything you do with likelihoods. That is now all doable. All doable once we have this neural network. So, okay, so what we want to do here is to have a neural network that is going to one way or the other learned likelihood function. Especially what we are going to do is to train a probabilistic classifier that takes the field and the parameters and input. And the output is going to be something that is proportional to, well, the output is something whose odds. Something whose odds transformation is proportional to the likelihood function. So, the neural network B train has the property that if you compute its odds, which is simply the output over one minus the output, this is going to be proportional to the likelihood function. And once we have that, what we can simply do is to plug in the data y that we have observed and then evaluate this over theta that we trace at the light wave surface. And we are not too conscious. And we are not too conscious about the proportional thing here because the proportional constant doesn't depend on parameter, it only depends on the data. Remote things that we want to do with livelihood functions is really not a problem. So we can do confidence sets, we can do point estimates, we can do all the useful things. So the benefits of this, we can do all the user likelihood-based things like maximum likelihood, confidence sets. It's also, it has the benefits of neural estimation in general, these are very fast to develop once they're trained. It doesn't depend on a prior. Train. It doesn't depend on a prior, and this is hiding the text here, but we have no trouble adding multiple replications as well. Because if you have multiple ID replicates, the likelihood function is in the product of the individual likelihoods. This is training the individual observation likelihood function. We can just evaluate this from multiple observations, take the product, and that's how you handle multiple calculations. So, what are you classifying against? It's on that next slide. Oh, okay. Okay, so how do we do that? Okay, so how do we do that? So, okay, of course, the magic is how do you train this classifier? And really, it's right here, is where the magic happens. So, okay, so what are we going to do? So, what are our training data? So, the first thing we do is that we simulate parameters from a distribution. So, for now, there's a distribution. I will show in a second that it will go away. So, we need to generate our parameters somehow. We do it over a boundary parameter space. That's a general assumption and all these like little free things is that you need a boundary. Things is that you need a bounded parameter space. And then once we have simulated our parameter, we simulate from the space approach with the parameter. So this, and then we evaluate that on a grid. So we are assuming right now that our observations are on a fully observed regular grid. So for each of these thetas, we assume like our gridded or like our spatial observations on a grid. And then that, so now we have these pairs of the field and the Of the field and the parameter, and that's our first class. So that's class one. So, what is the second class? So, for the second class, simply what we do is that we permute the pairing of these two things. So, we simply pair the y i, the i field with like any randomly permuted kind of collection of these parameters. So, the same parameters as before, we just apply a random permutation on them. So, what is going to happen because of this permutation? So, what is going to happen because of this permutation is that you basically get independence between the y's and the theta. So, once you permute these two things, you break the dependence. So, now what you have is that in the second class, the y marginally and the theta marginally, they have the same distributions as here, but they are independent because of the permutation. And then, simply, what you do is that you train a classifier to separate this class from this class, and that's the classifier. And I will explain in the next slide. The classifier, and I will explain in the next slide why this gives you the life field function. But it's really as simple as this: you just simulate, you simulate data like this, you do this permutation, and you train this class versus this class, and this basically gives you like the function. There's a small detail here in practice, but we do be actually simulate multiple y's for each parameter to find that that works a little bit better, but the idea is the same. Yeah, do you do the permutation? Do you keep the same in the last one? In the multiple y's, the same theta for all y's simulation or cube permutation. Yeah, so we so we account for the fact that you basically so we permute within the sort of the account for the multiple recognition permutations. It's not like a full permutation, it's like a constraint permutation that are transferred back within the multiple representations. There's a backup slide that actually kind of shows like that. Okay, so this is how the classifier is trained. So, now why does this give you the likelihood function? So, it's actually a very simple argument. So, I will go through the argument because it's actually very simple and illustrative. So, okay, so we train this classifier. So, it's a probability classifier. Its output is a probability. It's a number between zero and one. And if we train it using the cross-entropy loss, which is the extended loss for binary classification, what Classification, what that neural network will asymptotically converge to is the class probability. It will give us the probability of class one, given the field and the parameter. And now the claim is that this gives us the likelihood function. So why is that? Well, it's actually very simple. So now if you look at this class probability, you just apply the base rule there. So then the probabilities of each of the classes is just one half because the same size. So these go away. And then This goes away, and then we kind of write this like this. Okay, so now we have these two ratios here. We have the ratio of the pair under class one under to the so we have the probability of the pair under class one over the probability of the pair under class two. And now, so what are these two things? Well, the first one is just some joint distribution of the two. And the second one, well, it's the same margins, but they're independent. Same margins, but they're independent because of the permutation. So, in class two, this is simply the product of the margins because of the permutation. And now, what happens now, if you use just you just write this as like y conditional theta, then the priors cancel out, the p theta cancel out. So now you get this thing. And now you notice that this is the likelihood function. So, you seem to have the likelihood over the margin of y, and the same here. And now, what we can do, so being a basic. And now, what we can do, so we can basically solve this for this ratio. So, you give this ratio a name, we call it a psi. And now, so you have the psi here and psi here, and you have the classifier right here. So, now what you can do, you simply solve this thing for the psi, which has the likelihood in it. So, the solution is that the psi is the classifier over one minus the classifier. And that psi, it's proportional to the likelihood. I mean, this likelihood is the psi times the. Likelihood is the psi times the margin. So, so this means that it's a little hidden there, but the likelihood is the margin times the psi. But the margin, like this doesn't involve the parameter, it is just that basically a multiplicative constant that we really don't care about. So, this means that the likelihood is proportional to psi, which is which is simply this h over one minus h. So, so that's the that's the that's the key trick. And this actually, I mean, these both. Key trick. And this actually, I mean, this goes back to the fact that classifiers really learn density ratios. So basically, whenever you want any kind of a density ratio, you can just train a classifier to learn that. This is just learning a particle density ratio where we use this permutation trick to kind of get excess the ratio we want to get the lightweight function. So it's really these two slides, like how you generate the training data and the connection to the classifier. These are the core of the method. It's really simple. It's as simple as this. Simple, it's as simple as this. This is extremely simple to do. You can, I mean, for any model, you can do this. You can, you just need to simulate data, you do the permutation, you train, you know, these two classes, one person after, and you apply this transformation to your classifier, and that's it. That's how you get the light information. So, the rest of this talk is the kind of technical details of how we actually do this, and then results that actually works. So, okay, so the first data, what is the classifier we use? So, we basically use what Amada has done before. So, Amada had this convolutional neural network that basically would take the spatial field as an input and would speed out the parameter, in her case, the parameter as the output. We use the convolutional part from there, and then our input is both the field and the parameter. So, we basically take this CNN part and we concatenate it with the parameter. So, now at this point, we have basically a So now at this point, we have basically a neural network that has both kind of congruence something for the spatial part, and then kind of the parameter is kind of concatenated with that. Then we have a fully connected classical neural network here, and we train it as we just discussed, and this gives us the likelihood, or sorry, as proportional to likelihood as the output. So that's the neural network. And CNNs are really kind of a useful tool here because spatial fields are basically images. Because spatial fields are basically images, and CNS are very good for images. So it kind of seems to fit this problem very well. So, another technical data that they actually found was very important is that, so ultimately, if you think about this argument of how the likelihood function arises here, it all kind of relies on learning this class probability. So, the probability of class one given the pair. So, we need to be able to learn this accurate. So, we need to be able to learn this accurately in order to learn likelihood accurately. So, what we found out was that, so it's kind of well known that classifiers actually not very good at learning these probabilities or they have some biases in learning this. So, there's a kind of well-studied solution to this called post-op calibration, which is that we can do things to the trained neural network to kind of estimate these probabilities better. We use something called plot scaling, which simply means that you do apply logistic regression to the trained neural network. logistic regression to the train neural network to improve the these probabilities uh to to calibrate the neural network it actually really makes a difference here just to show you like i'll show you the case studies in just a second but for gaussian process like so you can easily check the calibration of the neural network you just simply uh put the uh kind of the pre probabilities against the improper probabilities if you don't calibrate the network it's it's having uh trouble getting the probabilities right once you've calibrated it's doing a very good job Is doing a very good job. And then, okay, so can this give us basic likelihood function? What do we do with the likelihood function? So, we can do all the usual things. So, we can get the log likelihood surface. This simply means to just evaluate this classifier and this transformation of the classifier for multiple parameters. You do it on a grid. And one thing that's important here: we need to do this in a vectorized way. So, neural networks are fast for vectorized evaluation. Evaluation: so the p to the speed is the vector st evaluation. So, we do that's how we get the surface. And once we have the surface, we can find a maximizer on it. I mean, we find the maximizer on the surface, which is basically the same as the maximum acclaim estimator on that grid. Once we have the surface, we find the maximum likelihood based on that. And similarly, you can find approximate confidence regions. So, basically, one way to get approximate confidence regions from a likelihood surface is that you basically look at the two times delta local. Basically, look at the two times delta log likelihood. Because of Will's theorem, this should roughly speaking follow a chi-square distribution, and you can use that to get a confidence set. It's approximate, of course, but it actually turns out to work very nicely, as I'll show. And the key point is that, so if you were worried about this multiplicative constant that we have, the nice thing is that neither this or this depends on that, right? It cancels out. You just kind of see that, you know, we do this sighting, which is the likelihood up to the constant. Which is the likelihood up to the constant, but they are the same because the constant bounces out. So, really, it doesn't matter that we have this constant there. Okay, so let me show you that this works. So, we have two case studies, one for a Gaussian process and one for a Brown-Rasnic process. So, the Gaussian process is really kind of a sanity check. You know, for a Gaussian process, you can actually evaluate function. So, we just use this to check that the neural network is actually doing what it's supposed to be doing. So, maybe in the interest Maybe in the interest of time, I'll skip something. Basically, we're using this to learn the parameters of an exponential covariance under a certain setup. We can discuss the setup separately if you're interested in details. I'll just show you that this works. So, here are, so okay, so here's three parts. The left one is the exact Gaussian process likelihood function. The middle one is the uncalibrated neural likelihood, and the right one is the calibrated neural likelihood. And depending on And depending on the projector and the color scales, like you, you know, you may or may not see differences between these, but basically, the setup is that, so, first of all, you see that it's kind of putting the concentration where it should be. And remember, this is a very difficult problem. Like the field had, there was 25 by 25 grids. So the field has 625 input dimensions, and then we have the parameters. So really need to learn this really high-dimensional function, and it's doing that very well. So it works here. It's doing that very well. So it works here. I'll show you kind of more convincing evidence that calibration makes a difference. But you see that already the calibration is slightly different from the uncalibrate one S3. And the key point is that it helps to get the high likelihood correct, which turns out to be important. So, you know, here's another realization. So you kind of see that it's tracking the true likelihood. Here's another one tracking the true likelihood. Here it's actually more obvious that the calibration really helps. The high likelihood region is right. Likelihood region is rather similar between these two figures. So we can check the quality of the point estimators. We just find the maximizer of those surfaces. This is showing that, okay, this is on this project, this is not displaying very well, but basically there's a black dot here that shows where the true value is. And then there's clouds of points with two colors, one for the neural estimator and one for the exact. And you see that they're basically producing similar quality estimates. So it's basically give a The estimate. So it's basically giving us point estimators that are of similar quality as the exact likelihood. Then we can get the likelihood of the approximate confidence regions using basically two times the altar log likelihood. These are the 95% approximate confidence regions for one of the studies we just looked at. Here's the exact one. This is the uncalibrated, this is the calibrate one. Of course, from a single realization, you need to be careful to interpret that. So we did a coverage study, and so you can see it's on the top. So, you can see it's on the top. You have the coverage for the exact likelihood, and this is the uncalibrated, and this is the neural. So, you can see that, first of all, the calibration makes a big difference. So, it really helps us get the coverage right. And you can see that the coverage is basically more or less correct in both the exact latitude and the neural latitude. And also, the size of the confidence sets are seen. Okay, this high index is saying that's the area of the confidence region. Region. Okay, so that was the Gaussian process. And so, what's the benefit in this case? Of course, we knew the likelihood function in this case. So, what's the benefit? The benefit is computational. So, these neural networks are much faster to evaluate than the exact likelihood. And this was a relatively small scale study, but already we see a big difference. So, the exact likelihood, this was the evaluation time there. And then, for the neural likelihood, depending on whether you do it in a vectorous or unvectorous way, you see that it's. Vectorous way, you see that it's much faster if you do the vectorized evaluation. And this was a small-scale situation. I expect that these differences are much weaker from large-scale situations. So already for Bausian process, this seems like potentially helpful. But then where this gets really interesting is when you look at intractable models like Brown-Brasnik model or other max-stable models. And again, so nicely, we have had previous talks explain the details. So I wanted to explain what is a Brown-Brasnik frozen and. A brown Bresnik prosin and that it has an intractable likelihood function. I just explained that, so of course, here we don't know what the true likelihood is, it's intractable, meaning that it's unknown to us. So we compare what we are getting against the pairwise likelihood, which Rafael explained in his talk. And it has this tuning parameters. You need to pick this cutoff for picking your weights or basically the pairs to improve. And B and others find that it's quite sensitive to. Find that it's quite sensitive to that choice. So, I'll show you results for a few values of this cutoff. So, again, I'll skip the details. We can discuss the details separately if you want. So, again, I'll just show you that this works. So, okay, so here's, so this is a pretty small cutoff. This is basically a situation where you only look at the kind of the pair, like the neighboring pairs to the left and to the right. So, the pair was likelihood is not supposed to work very well here. And you see that the pair was likelihood, this left here is the pairwise likelihood surface. Here is the paravas vagule surface. You see that it's kind of all over the place. Here's the neural surface, uncalibrated, and this is the calibrate surface. And you see, it's giving a fundamentally very different answer from pair of eyes like Lud in this case. If you do it, the pair of eyes likelihood for a different cutoff, you get a very different result. And of course, for ACID, it doesn't matter. You don't have that tuning parameter. So, and based on the arguments that, you know, if you believe that these neural networks are fitting well, this should be a reasonable. Uh, this should be a reasonable accuracy of what the true likelihood function looks like in this case. Um, so you can do you can check the quality of the point estimators. Again, it depends on the choice of that cutoff. So, if you have this kind of bad cutoff, the pair was likelihood is really bad, it's kind of all over the place. The green here is the neural estimate, the other color is the pair of likelihood. If you do it for delta equals two, which seems to be a good choice, you find comparable results. You find comparable results. There are some cases where one is better than the other, but it's basically comparable for most situations. And this was more or less the best delta we found. If you increase, it actually gets worse to the worst. And of course, for the neural estimator, it doesn't matter what you do. Then we can do confidence regions. This is what the confidence region would look like for delta plus one for pairwise likelihood versus what we do. Of course, the idea here is like you're thinking of this as like an approximation of likelihood function and then using the same way of getting the approximate function. Same way of getting the approximate confidence region. This is for the different choice of delta. And we get a coverage study here. And so in the first two cultures, showing the coverage for the different curvas lactes, you see that it's having problems getting the coverage right. While the SP the calibrate neural lactu is getting exactly more or less 95% coverage, but it's supposed to get 95% coverage. So it basically gives us reliable confidence sets based on those. confidence sets based on those neural estimates of the of the likelihood surface. Here are the sizes of the areas. Again, you see that it's a bit like too large in terms of the area. And again, there's a computational benefit. So the neural network, again, if you do a vector stimulation, it's actually faster than the thermostat. So it's higher quality and faster than the therapy. So basically what we have done, we have developed a way of learning the lightfield function using classifiers. Using classifiers, uh, it provides uh fast and accurate parameter estimates and reliable answering quantification. And uh, one thing to point out: this is an it's an what's called an amortized neural network. I also saw this was in the previous talk, these terms used. So, there is an overhead cost from training the network, but once the network is trained, you can just use it as many times as you ever want. You can build it into an R package. You know, I think that was the vision that was on the last slide and previous talk. So, like, once the network is trained, you can easily use it as many times as you want, I speak. Use it as many times as you want, and it's fixed to do. There are some limitations here. We assume that we have a fully observed grid. So, we are right now thinking of ways to do this for partially observed grids. We're also thinking of ways of doing this for things like the RCO data, which is not on a grid. So, that's a whole different challenge how you do that. And also trying to figure out how to relax this assumption of the bounded parameter space. I can use some high adapter to figure out what that space should be. And really, the big picture here is that, like, so if you Like, uh, so if you think about the only thing this requires is a way to simulate quickly from the model, so this applies to basically any statistical model, it applies to any spatial model, but this applies to any sensible model from which it's fast to simulate. So, so I guess the question is, like, does this kind of lead to kind of a wider use of this kind of likelihood-free techniques for statistical modeling? Actually, some of my colleagues are already thinking of like using these kind of things for like epidemiological models, for example. Um, so, um, yeah, so if you want to see more information, uh, we have a If you want to see more information, we have a paper on archive since this morning, so brand new. Of course, this thing is hiding the archive number, but there's an archive number there if you find the slides, or you'll find it on archive. But yeah, so I'll finish here. I'm happy to take any questions. Thank you very much.