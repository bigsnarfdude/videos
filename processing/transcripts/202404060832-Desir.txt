Thank you for putting me first when I'm not too jet-like. And so today I'm going to talk about some recent work trying to merge and to bridge some classical Review Myodrome problem. I'll talk about the survey optimization problem today and deep learning. This is one joint work with Axel Parmontier, who's a faculty at Icar Debaume in France. And what I'm going to talk about today is quite preliminary, so we're hoping to have a paper by the end. We're hoping to have a paper by the end of the spring, so any feedback would be more than welcome. All right, so I debated whether I should have this slide, but then since I was the first one, I was like, okay, let's just get on with this. So we're going to talk about the assortment optimization problem, which is the problem of selecting a subset of product law for customers. And the two key challenges are: one, to capture choice or substitution behavior from data. So, you know, if I From data. So, you know, if I'm choosing my flight from Paris to Calgary, I only had one option, so I didn't have much of a choice. But if you're coming from, say, New York, maybe you have different choices. You can stop at Toronto or Seattle. And so that creates a subset of choices. And so being able to learn these choices from data is the first problem because, as you see, the demand from customer or the choice depends on what's offered. The second main challenge is that, assuming you have learned the demand, how do you Have learned the demand. How do you choose which subset to offer? And that, as you can imagine, quickly becomes a combinatorial optimization problem. And so I changed the title of this slide to reflect the title of this workshop, which is, in fact, we're solving a combinatorial optimization. And most of the applications nowadays are for online platforms. So you need to be able to solve these problems at scale and very efficiently because you may want to decide what subset of product t to show when you someone when you pull on a web page that needs to be corrected. A lot of web page that needs to be corrected. Okay. And so the motivation for, maybe let me start getting into some notation, and also, I guess, the motivation for using deep learning is that nowadays we are living in a world where products have a lot of information. You have a lot of information about customer, about product. And so, in fact, a product, I'm going to say, is defined by a set of features, X. And so, you want a model that's able to leverage the richness of this feature space. And so, that was the motivation for us. feature space and so that was the motivation for us to use to use Deeplink. And so the notation I'll be using is pi for the probability that a customer chooses product X, so defined by a feature vector X, if the set S is often. So this is how I encode the dependence on the set. And I'm going to assume that there is some priority data that you may want to learn. Okay, that's the that's just the space of the model you're working with. If you're working with an M and L, for instance, this would be how you would write the probability. And so your goal is to learn data. So, your goal is to learn theta. Theta is the sensitivity to the different product features. You want to learn this from the experience. Once you have this, the next step is you plug this into your favorite assortment optimization problem, and you're trying to maximize over all the possible assortments where you may have some constraints on the assortment. So, I'm using a strict F for the set of feasible assortments. Then you want to maximize the expected revenue, or each product here has a revenue bias. And so, if you think about this, you know. And so, if you think about this, you know, so typically we have these two steps. First is the estimation part. You estimate your best parameter data from data. And this, depending on the class of choice model that you choose, may be easy or harder. For instance, if you have a mixture of M and L model, this can lead to a non-concave maximum like you would follow. So this is the first step. Second step, you have to solve this combinatorial optimization problem. Again, depending on the class of choice model as well as the nature of the feasible. Podel, as well as the nature of the feasible set, this may be a bit hard. So, we're solving this sequence of complicated problems. And I guess this is how we were able to write lots of papers because these are hard problems. And so, this has given us a lot of problems. And so, typically, let me try to summarize like this. So, basically, there's a first step where you use data to learn data, and then you take this as an input to your optimization problem. And so, for every given new sort of Sort of set of products, you would want to make a decision of a sort. And typically, we can think the estimation can be done offline, what you learn from historical data, and then the optimization may have to be done online, if you're thinking about showing a new assortment to someone who think about this, what can go wrong with this approach? If you think about this, what can go wrong with this approach? So, I'm not going to cold call, but let me tell you a few things. Got a cold call, but let me tell you a few things that can go wrong, and maybe you have other ideas. But what can go wrong is two things that one can think about. One is, even if the problem is well formulated, so let's say I tell you that the data comes from a mixture of an MNL model and you're fitting an M and L model, it's not clear that better parameters, in the sense of the likelihood that we're solving here, actually leads to a better decision. So that's number one. So you are That's number one. So you are choosing the parameters to maximize or to maximize likelihood. And then what you care about is the revenue. So it's not clear that the better model in the sense of likelihood actually leads to better decision. And then the second problem is most of the time the problem is not misspecified. So in fact, the data doesn't come from not in a logic. And so to illustrate this, and this is going to be the motivation for my talk, let me show you a small toy example that I constructed that illustrates this. So the first one is one specific So, the first one is a well-specified problem where I generated data according to a multinomial logic model and I fitted the multinomial logic model. And so, on the left-hand side, you can see the estimation. So, I'm using great and decent algorithms. So in each hypocrisy, the logo equal to breakdown. And so, what you can see is on the left, for every epoch, I'm using the parameter that was chosen, and I'm solving the assortment authentication problem. Orientation problem. You can see the optimality gap. And so here, what happens in this picture is at some point there's going to be some overfitting, and so that's why there is a gap here. But what's interesting here is you see, for different types of assortment problems, you see that the best choice of parameter actually may be reached in a different epoch. What this is illustrating is, in fact, the estimation is done independently of what the downstream optimization. And so, in fact, depending on what's what So, in fact, depending on what problem you may want to solve down the line, you may not want to choose the same problem. So, this is a simple example that I took at in a well-specified case. Now, things can be even worse if in a miss-specified case, so this is a similar example where instead I generated data from a mixture of MNL model and I tried to fit an MNL model. Okay, so here you see it seems like I'm doing better in terms of fit, but if Terms of fit, but in fact, you know, I can't predict anything in terms of optimization. In fact, sometimes better model in terms of likelihood in terms of prediction actually leads to worse decision in terms of optimization. Yes, but if I change the left picture and look like to something like L2 or L infinity more of the error for actual choice models, would you create the same thing? You. So, what do you mean by so? Are you saying plotting a different model? Are you plotting a different model, or are you saying I'm optimizing a different model? I'm saying I'm a ground truth choice model, and let's say you estimate that and you minimize the editing. Oh, I see, I see. Yeah, yeah, yes. So you expect some sort of like non-deep shift behavior, yeah. Yes, but then you're saying you need to be able to optimize over that. Right. Because I'm playing, I'm just like changing the notes of like. Yeah, and this is where we're going anyway. So this is where we're going to try about the different losses. So, basically, we're going to try about a different loss function that is able to encapsulate the optimization. So, if you were able to, I guess, minimize some sort of norm infinity, I guess you would perhaps this would mitigate some of this. Yes? Just clarification about the optimality gap. So, is this that you estimate an ML model, find the optimal assortment using ML, and then compare it and compute the verb with the groundwork model, that's right. Integral towards this. Okay, and so what I'm going to present today is: okay, I'm going to try to present one idea for an integrated approach that integrates the estimation and optimization. And it's going to, you know, it's going to be, you know, I'm going to propose a different loss function, in fact, that's going to be aware of the optimization. It's going to be leveraging ideas from something called combinatorial optimization layers in machine learning. So I'll tell you what this is. I think there are many optimizations. What this is. I think there are many applications, many potential follow-up applications, so I wanted to share some of that. And so, in the end, what I'm going to propose is what it's a heuristic algorithm for assortment that is contextual because it's going to be based on deep learning, so it's going to be able to leverage the feature space. And it's going to be very fast in terms of optimization. I'm going to be able to use this to return an assortment very fast. Okay, so this is what I'm going to present to you. Okay, and so, okay, we are building on classical result from assortment to get inspiration and using some recent advances in Kubernetes optimization. Now, you can think of this as, I guess, building also on this smart and predict literature that started from the news vendor problem and linear program. And here we're building on these ideas in the context of revenue management, and this will be not too much. Okay, so here's the outline. So I'll first give some motivation for how I think about the algorithm that I'm going to present. Then I'm going to tell you how I'm going to use those layers to estimate these models. And finally, something very cool. Okay. And so what I'm going to do is I'm going to go back to our favorite example, the sword and optimization under MNL. For some reason, I always go back to this. I remember when I was sitting in Fricard's class doing my PhD, we had a guest deal. During my PhD, we had a guest speaker, Pat, came, and gave us a lecture on a summary optimization of ML, and he presented five different proofs. And when I was working on the graphics chain paper, in fact, we came up with a new proof. And so what I'm going to present here is yet another approach for this, which I think is quite useful. But for some reason, it was published in a paper I wrote with Santiago called Incentive Compatible Assortment Optimization. It was not the main sort of results of the paper, so it ended up in Appendix M or something. Ended up in Appendix M or something. And so now I'm happy that I can share it with you because I think it's quite interesting. Okay? And so I want to revisit this classical result of Tallou and Verizing about the revenue order nature of the optimum software. And to do that, let me do a picture proof. So think about the objective function as having two components. One is the numerator, which is this summation RIUI, and the other one is the denominator, which is the summation of UI. And if you plug these in a 2D map, so here each 2D map. So here each point is an assortment. It has some value for this numerator and some value for this denominator. And so if you write the assortment, you're trying to maximize A over 1 plus B. And so the key sort of realization, and I guess all the existing proofs use this idea in some way or another, but the key observation is this is a quasi-convex function as a function of A and B. And so because it's a quasi-convex function, there exists an optimal assumption. There exists an optimal assortment which is on the convex hull of this set. And so if you have an optimal assortment which is on the convex hull of this set, it means that there exists a supporting hyperplane. Right? And so this means that there exists an optimal assortment that solves, that maximizes this interactive function. Okay, and so in fact, we can write this more general result, which is if you can rewrite your assortment optimization like this, which is maximize a function f where a is F, where A, S, and B S are some functions of your assortment. If this function is quasi-convex, you can in fact rewrite the optimization problem as the way I think about this is a linear objective function, right? You're changing the assortment objective into a linear objective. And so the way it gives the revenue order result is that for MNL, this is RIUI, this is UI, so you factor out UI, and you get RI minus lambda, and so your optimal assortment is all the revenues that are higher. Assortment is all the revenues that are higher than a certain threshold, so this is the revenue order. But in fact, this also gives the revenue order optimality for a welfare objective because this objective is quasi-convex. This also gives the revenue order for a sub-problem of nested logic, if you look at this function, which is also quasi-complex. And in fact, this also applies if you have constraints. So, for instance, if you think about the cardinality constraint result, you can write it like this, which is you just need to solve this linear problem on this set. Problem on this setting. Okay, and so this is the motivation for us to think about. Okay, if you think about this, this is saying you can, in fact, write the assortment problem as a problem where you're maximizing a linear function over some feasible set that represents the set of feasible assortment. Okay? And so here's an idea. Can we learn, you know, and so those weights we know how to write them for certain choice models for sure, but you know, can we learn? Model for sure, but you know, can we think about learning those weights for a general choice model? Can I think about learning a set of weights from data so that instead of solving the solver problem, I'll solve a proxy solver problem, which is going to be given by this linear function. And so I call this a linear surrogate objective, and you can immediately see that if I'm able to do that, or if I give you a set of weight, then the online optimization becomes trivial. Then the online optimization becomes trivial, right? You just need to solve this. So if you have unconstrained problem, you just take all the products with the positive weights. If you have a cardinality constraint problem, you take the top K weights. If you have a capacity problem, then you just solve an AppSack problem. This may be an IP, but it's just a new problem, right? And so the question is, can we learn weight in a data-driven way to replace the assortment with this problem? So this is the motivation. So, this is the motivation. This is why I call this a smart greedy greedy in the sense that I'm solving this instead of the Astronaut problem. Okay. And so, here is where those combinatorial layers are going to be interesting. So, what is a combinatorial layer? Think of this as, so we have a typical neural network architecture where you have an input x, which is transformed into some vector theta. And a combinatorial layer is basically a mapping from a vector of parameters to, in our case. A vector of parameters to, in our case, a solution where the mapping is done by optimization. So, on the previous slide, can you go to the previous slide? So, this one? Yeah, so the linear combination actually depends on the object, the constraints are constrained. Yes, it may depend, yes. So, you can't learn a single set of digits and then say so yes. Yes, can we hold on to that question? And I'll come back to this in a few slides. But yes, you're right. You're right. I'll tell you how we get around this in a few slides. Okay, so basically this is a transformation from a parameter to a solution. And so the goal here would be to learn theta to produce good solutions as opposed Good solutions as opposed to good predictions. Okay, so that's the goal here. And so this is what I mean by combinatorial optimization data. Okay, so there are two learning frameworks that have been proposed. One is it basically depends on what data you have, right? So there is one framework that's called learning by experience, where you assume that what you have is an oracle that can tell you some revenue or some profit for every solution. Revenue or some profit for every solution that you give it. Another framework is something called learning by imitation, where you assume that in your data what you have access to is examples of optimal solutions. Okay, and there are methods for both of these, but what I want to highlight is a key challenge with these combinatorial layers is the following: is that if you think about the optimal solution as a function of theta, which is the objective, in fact, the The objective. In fact, this is a piecewise constant function. So, if you change theta by a small amount, the optimal solution is not going to change. And so, that means that the gradient is going to be zero almost everywhere. And so, the key idea to learn this method is basically what we're going to do is we're going to perturb the objective function. This is what I'm going to show you next. And the context in which I want to show you this is in the learning by imitation setting. Everything that I'm going to say, also, we have extension for the learning by imitation. We have extension for the learning by experience, but I think it's easier to understand the limitation setting. So, we're going to assume that we have access to optimal solution in the training set. So, the training set is I have assortment, sorry, subset of product, and what is the optimal assortment? And I want to learn this. The loss that I'm going to use is the non-optimality of the target. So, it's, you know, what's the difference between theta y star, which is what I predict, and the theta y true, which is the true solution. Yes, it's related to the smart. True function. Yes, I'm. It's related to the smart predicate optimize objective for it. Yes, so they have in the smart predict and optimize, it's related. So I think here they use the true data, if there exists such a true data. For us, there is no such true data, so I'm using the predicted data. But it's related. The way that they construct their SPO plus loss is slightly different than what I'm going to present to them. Yes. And isn't it concerning if they like, can I cheat by picking a bad data just so that number? Picking a bad data, just so that number is low or what do you mean by picking a a small data? Because you want to minimize this loss, right? You want to minimize this loss, yes. Because if you choose theta, basically y star is going to be a function of theta, right? It's computed by this combinatorial layer. So if you can control it so that you anticipate what this y star is doing, in fact, you have to write the stress. And so the key idea is the following. We're going to perturb the loss function. Okay? So what we're going to do is the following. We're going to construct a new loss, which is, I'm going to call the perturbed loss, which is what? Which is, I'm just going to add some noise to the objective function. And if you do that, there are a few nice things that happen. One is it becomes very easy to get. So, first of all, this is this gives. So, first of all, this gives a very nice convex and differentiable loss in theta. Second, it's very easy to work with because you have an expression from the gradient. So, what you can do is you could just use Monte Carlo, you sample a few epsilon, you solve these, and then you get an estimate of the gradient. You can do by population. And so, the way that you should interpret this is as follows: which is, in fact, what I'm proposing is I'm perturbing the combinatorial layer by adding. The combinatorial layer by adding this noise, and as a result, what the output of my layer is not anymore an assortment, but rather a distribution of an assortment. And perhaps the best way to think about this is, in fact, if you think about the softmax layer, this is exactly what this is, where the optimization is just a simple R max. You're picking the highest component, and the noise is going. But for the softmax, what you have is you have. But for the softmax, what you have is you actually have a closed expression for this mapping, which is this actually the MNL, the MNL form. What I'm proposing is, in fact, you don't need the closed form. What you can do is you can just use Monte Carlo. Just sample a few epsilon. As long as you have an oracle for the optimization, you get an estimate, you get a distribution over solution, and you also get an estimate of the gradient, so you could use back multiplication to train this. And in fact, my co-author. And in fact, my co-author has used similar ideas, and based on this, he actually won the New York 2022 challenge for vehicle routing, where the optimization layer was about solving a routing problem, and they were trying to construct costs for the different edges of a certain graph. And they have implemented, there is a Julia library that they developed. But in fact, you see, it's quite easy to even implement it from scratch. You just need to sample and take expectations based on that. Take expectations basically. Yes, that's right. In your setup, do you, for the observations, do you need the optimal solution, or is it enough if you have the optimal revenue? So here I'm assuming that we have the optimal solution in the training. But the only thing you have is an oracle that tells you the revenue for that solution, then you can also So you are using y star, right? So not y star, y true. Yes. The theta transpose y true may or may not correspond to the actual revenue. That's right. So we're saying we're hoping that this linear proxy that we're learning is a good proxy for it. So in a way, we're trying to learn y true and not the revenue, right? We're trying to be as close as possible to that. That's a y true line. But but if you have access to a normal particular revenue, that would be slightly diff different interpretation, but So it depends on what you have access to. So it will in MNL, right? It will in MNL, right? And in fact, so this was quite new to me, but in fact, so another interpretation of this is, in fact, thinking about this loss function as a decision aware loss, where if you think about this loss function as measuring some kind of loss between the parameter that comes out of your The parameter that comes out of your layer, and the true, because this has this nature, some information about the optimization that you're solving, right, because we're solving, you know, we have the constraints, for instance, then one interpretation is that you're using a loss, which is decision of the weight. Back to your point, right, where, you know, here we're embedding some kind of, we're encoding the optimization, the downstream optimization in the loss function. And in fact, there's a huge literature on this equivalence between perturbation. This equivalence between perturbation and regularization. So it actually dates back to this paper by Henderson et al. that actually showed that another interpretation of the MNL was through this idea of a representative customer that chooses a distribution of a product with a desire for variety. So there is a regularization term. In fact, this representation is equivalent to this noisy version that we think about with this random utility framework. So there's built The mutility framework. So there's builds on this literature, and also in online learning, there is this equivalence between regularized per turbiner, I think. No, sorry, follow the perturbed meter and follow the regularized leader, which are also building on similar similarities. Follow-up question, Sarah. If I just do some off-the-shelf on a layer optimization, not necessarily follow the current because something else in the last layer, would that be similar? So what do you mean exactly? So basically, your loss is kind of like regression. Your loss is kind of back regress, right? So you're just so in order to learn the data, I just use only new. So you're basically running follow-up query with query. Yes, and I think that should be equivalent. Is there a reduction basically if I just give it a bit? I'm not sure, but I think we are exploring this or not. You might see the feedback, right? You. We know why true though, actually. So in this case, I think it's convex. So in the case of, if you only have an oracle, then this is not complex. It may not be complex. So I'm not sure. Okay. So um okay, so I get now get back to your question, Vinit, about you know, what do we need to what what what should we, you know, how should we think about this data? So another thing that that that that that we that we do is we leverage the structure of the optimization, of the problem. And in fact, the way we implement this ML layer is similar to our, you know, what we do with our paper. Similar to what we do with our paper with Ali, which is in fact what we're learning is a score or weight for each product, and we're using these boxes to encode the nature of the problem. And so, I think one key observation that is related to your comment about how to make this work is somehow, I think in the ML, those weights are just a revenue, or some shifted version of the revenue. Some shifted version of the revenue. So they could be, so and the shift should be a function of the whole assortment. So in a way, if we want this to work, there needs to be some encoding of the assortment in this vector x. So you could think of this x as, you know, maybe it's all the product features, but maybe I also want to encode some information about my assortment. I have to tell you the problem that I'm solving before you put this. Uh Right, or it could be some general information about what's the, you know, maybe it's the average revenue of the some kind of information about the assortment, maybe not the optimization, but at least about the anyway, the more information you put in there, the better this is going to be, right. But you're right, if we're hoping for these weights to be somehow dependent on either the assortment or the problem, then you need to somehow encode this. But a good thing about this representation is that you could actually representation is that you can actually learn so you can actually learn this on this on an instance with a small number of products and then you can use it for large large products size instances right because once you learn this box you know if I give you an instance where you have more products you can just repeat this and you can see it's two instances is it possible to get around theta at all and just say using X to predict Y true because you know Y true so Y true is much easier than one so in a way what we're doing So, in a way, what we're doing here is we're putting structure on the network in the hope that this will help this map. So, basically, we're putting structure on this map, but you're right. So, instead of having this, you could have exception of black box maps. So, your comment about small number, learning it on small, that means theta, if you learn it in a formulaic way, but this will just give you some numbers. Theta will be some scalar, some, not a function. Not a function, not in a functional way. No, no, it could be a functional way. So, for instance, let's say here I encode, you know, let's say, so you know, the key is as long as your features don't depend on the size of this assortment. So let's say it's the average verb, right? Then you'll have some okay. And the last thing that I think is quite nice is also it gives you a principle docal search heuristic in the sense that because you have a distribution of assortment, what you could do is. Of assortment, what you could do is you could generate a few solutions and return the best one instead of just, for instance, doing some kind of local search or just based on the zero one vector. Here, you have a principal way of doing this local search basically. So, what I want to finish with is revisiting the first example that I showed you. So, if you use this approach in this misspecified example that I showed you, here you see that in fact the loss that I propose, and in fact, it's related to what you're saying, right? What you're saying, right? Is actually, you know, when it goes down, actually, the optimality goes down. So it helps reconcile these two steps. And we have started to do some experiments. So in different settings for MNL, also with Ranking Base. And in all the cases, we are getting some very good results, especially after we allow for what I call post-processing, which is this local search heuristic, or generating a few solutions and taking the best one, we get less than 1%. Less than 1% optimality gap, and this is a preliminary, so we're hoping to make this even better. So, just to conclude, so I talked about proposing an integrated learning approach that directly learns an optimal assortment. And the way I do this is by embedding some combinatorial layer that I can learn using this perturbation idea. The method is contextual because it leverages the feature space. There is some engineering work that needs to be done. Engineering work that needs to be done in terms of how do you think about creating those inputs. But the good thing is the online optimization is fast. I'll just mention a few things that I'm doing on this page, which is obviously what happens when you don't have access to the true optimal solution. This is what the polarium experience. I want to do also more exhaustive numerical experiments. So, two ideas that I think are quite interesting. One is, because of its, you know, we can use this feature set, I think we could solve the solvent problem where, say, the input. Sort of problem where, say, the inputs may be given by images or text. So, I think I want to see whether there are some interesting applications where we having this neural network model would be useful. The second thing is, if you think about, say, I want to learn a mapping for the mixture of M and L model, then we know a lot about hard instances. So, maybe what we could do is we could leverage this knowledge and put those hard instances in the training set so that the algorithm may learn. So that the algorithm may learn about these corner places and these corner instances and provide good solution. And finally, I'm working with other students and postdocs on whether we can use this idea for network programming management problem or even bundle chips. So think about, instead of maximizing a linear function, maybe you think about this as customer is maximizing some linear utility objective with some budget constraint and that the output is naturally going to be a bundle. Is naturally going to be a bundle of nuts. And so maybe we could learn these models using it. All right, that's it for me. Sorry, I went to the door. Over time, we need to ask