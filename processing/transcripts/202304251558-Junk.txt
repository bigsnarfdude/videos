How many people know logistic regression? Okay, so I don't need to explain that, all right? So oftentimes in logistic regression, you have kind of fixed effects for the regression parameters, right? But there are also random effects associated with subjects or some other feature of it. So we can think about going and measuring, you know, things over time and doing it repeatedly. So you want to account for the core. It repeatedly, so you want to account for the correlation between those measurements. And so you use these random effects to do that, but you have to integrate them out of the likelihood. Because you have essentially a hierarchical model where you have something like this, where u are random effects, x are fixed covariates, and beta are parameters. And then you have some distribution for the u. And so to calculate the likelihood, you have to integrate out the conditions. out the conditional you have to integrate out the use okay to get the marginal likelihood and this in turn in fact turns out to be really a really big problem the result here for software is not just for logistic regression but for exponential family regression in general so like we were talking about linked variables yes there's a right it's a lot that i kept saying could just be very loud but there's a lot of problems It's a lot of problems. You can't do it analytically, so that it comes with something like that. Exactly. Can you say something about item number one up there when you only have a small number of samples? I have a lot of half-formed thoughts about that. You know, I talked to some people here about using machine learning techniques in some of these settings. These settings. I think there's an argument made about using the Monte Carlo data as data to model and predict. I think there's some hay that could be made from that point of view. So I could talk in generalities. I haven't tried any of it. And I don't think many statisticians have. So about the ratio of data to Monte Carlo sizes, these N and Ms. There's a use case in physics where we're looking for new phenomena and we require enormous significance, five sigma in order to claim discovery. If you want to discover with just one instance in the data, you have to have a background prediction of three times ten. Okay. Which means that you need a lot of Monte Carlo to be able to. We need a lot of Monte Carlo to be able to tell that there's very little in those bins of that distribution that look like the same bookie. Is that encoded in there somewhere where there are parts of the distribution where we care more about than others and we have stringent criteria whereas most of the distribution? Well, I mean, I did not say much about, I broke the rule that. I broke the rule that I tell my students that you have to describe everything on your slides. And I didn't. Look, J is essentially minus the expectation of the second derivative of the log likelihood. V is the variance of the score function, and W is this variance of the deviation of the score function. These are complicated formulas, which is why I didn't write them down. And so all the properties of an underlying distribution are encoded, all the relevant properties in that. I just was trying to make I just was trying to make the point that suppose you have small m, right, but you have large m, then you still can have this asymptotic normality, right? And you can still use this to make inputs about your MLE. On the other hand, if you have a large M but smaller M, you can still do the same thing here. But you have to incorporate both in order to make it work. I think that in statistics, we often do I think that in statistics we often do the Monte Carlo and assume we've got the answer and then we go to the next step, right? Because frankly most of our models, not all, but most of them, the vast majority, we can make this so small it's negligible. Are there some scenarios in which Poisson will work better than governments? I think there is no algorithm that can't be beat. Okay. All right. Yeah, I mean, I think there are demonstrated cases where quasi-Monte Carlo is more efficient than doing random sample. That is without a doubt but true. There are theorems about that. They tend to be settings where the functions are simpler to evaluate. Where the functions were simpler to evaluate. Right, because with random sampling, you can have clumps and not explore the whole thing, but with quasi-Monte Carlo, because it's deterministic, right, you explore the whole space. So when you were looking through some of the, I think you looked through some papers on particle physics, were there things that you saw there that surprised you? There, that surprised you, or what were your reactions? There were a few things that surprised me. Very often, especially when people talk about Monte Carlo, they rarely specify what type of Monte Carlo they're given or the details about it. Or the details about it. In my world, MCMC is really different than just standard Monte Carlo because of the dependent structure in the simulation. I pay a lot of attention to that. But often in the physics papers I read, nobody even mentioned that. Like, how is the money, they were just, we have Monte Carlo samples. And so I had no idea how they were being produced. No idea how they were being produced. That was one thing that surprised me. I mean, maybe it's obvious to all the physicists that you're doing one particular type of simulation, all right. It wasn't obvious to me. What do you use for pseudo-random number generators? Yeah, I use the ones that people who know what they're doing, bro. So most of So, most of my code is written in R in Python. Summon C if I have to get serious about things. And so, I use the built-in pseudo-random number generator. The ones in R are pretty good. This tuck one in C is really bad. Yeah, well, I don't do that stuff in C much. But they're really good logics. That's kind of below the level where I usually. That's kind of below the level where I usually work. So can I go back to this this thing with the MCMC where you're you were trying to you you had the various techniques for for how to find the width of the proposal density? Yeah, sure. So I mean one of the things that might come up in particle physics example would be that we know what the target density looks like at least near its maximum and so we have the width, some measure of the width of the target density. The width of the target density, but only locally near its maximum. And is there just a rule of thumb? I know you said it was complicated, but for the simplest example where I say that the target density is multivariate Gaussian, is there not some rule of thumb that tells me what the scale factor should be? Okay, so let's put the multivariate Gaussian right in the view. Okay? And what you're going to get is the this is just the normalizing constant from this proposal. Right? And so essentially what you're going to get is the And so essentially, what you're going to get is the normalizing constant in the numerator here for that, right? And so you're going to have a variance to the d over 2 from that multivariate Gaussian. And so it's going to be the ratio of that variance to H that's important. Okay? Right? And so that means that you're going to have to have it be the right size relative to the target distribution. That I've encountered that 0.38 squared over. Oh, no, no, no. You need an acceptance rate of 0.234. 0.234. 0.234. I can't put it on my t-shirt. That was created by some good friends of mine. Okay, so let's explain how they created it. They assume the target distribution has completely independent components. It's multivariate, but all the components are just a product of marginals. It's just a product of marginals. Then they let the dimension of that go to infinity. They do it at the right rate. So it converges to a diffusion process. And then they optimize the speed of that diffusion process convergence as a model for the underlying Markov gene. And they come up with an acceptance rate of 0.234. What's wrong with that? Well, what's wrong with that is rarely do we have independent components, right? Independent components, right? And all of these optimal scaling results rely, and they've written hundreds, it seems like papers on this. So they all rely on some notion of independence among the components. And so they're just not realistic. They are really just rules of thumb. On the other hand, when I said H has to be something like B to the minus a power, that coincides exactly. That coincides exactly with their result. And I made no assumption about that. So, despite all their assumptions and asymptotic approximations, they might be on to something. So, if you just want to monitor it, that's fine. But you can also say, I know before I start that if I pick H too big, I'm in trouble. What I really wanted was just a sort of a first guess. So you're saying actually the kind of words is a first guess? That kind of works as a first guess. It works as a first guess. But then I would never start with something. I mean, if you're just going to start with something, I would never start with h bigger than 1. So in the beginning, you talked about the problem of performing these simultaneous confidence sets. You described that you have a conservative version of your confidence sets and anti-conservative version, something you search between those. I think, honestly, Bit with those. I think, unless I missed it, I think you never explained how you would do that, sir. So, how do you actually do that? What is kind of the key idea? I went through that really quick. So, the basic idea here is if you look at this line segment, we just do a bisection method here. And so, you know, and then calculate a normal probability using quasi-Monte Carlo, actually, because you can do it very fast to get the right coverage for that. How do you know you have to? How do you know you have the right coverage? Because of the construction. You know it's between these two. You know, this one's too big, this one's too small, and you just find the right one. What is the kind of the objective? Like, how, like, what is the underlying thing that tells you that now I'm... It's the normal distribution is underlying it, right? We have this asymptotic normal distribution. Oh, so it's sort of based on the ellipses in some sense, like you somehow based on that, you get it? Oh. The ellipse is just a construction of a confidence region. We're assuming we have a CLT. Okay, I think I see. Okay, so you have that distribution. Somehow for that distribution, you get that. But is it that that's a really high-dimensional Gaussian, which are kind of difficult to do? Yeah, I mean, we've done it for oftentimes that Gaussian is quite high-dimensional. Dimensional. In the paper, we did it for, I think the highest dimensional one was over 100. All right, so that was a pretty routine Bayesian problem. But we had, because we have means and quantiles for each marginal and other features, the things you're estimating get really, big, really critical. Can you share about the number of monthly family events you need for the year? The current events you need from the Richard distribution. I missed the last part. No, no. If you need to estimate sigma for well-behaved densities, you will get a Wischard distribution, right? Okay. Like a fiducial distribution, you said? Wisher distribution. Wishard. I couldn't figure out how we were getting the fiducial. Okay. Go ahead. So by studying the properties of the By studying the properties of the distribution, can't you figure out the number of molecular events you get? I mean, in a sense, we're trying to do something like that. For me, Monte Carlo, the goal is not just simulation of new, of Monte Carlo events. There's a feature of something I want to estimate. I want to estimate. And that thing here is theta. It's a vector typically. And so I'm just trying to say that with these confidence regions, I can say how far I can assess this error. So that's how I figure out. It's a sequential procedure and the properties that we establish in our estimation of sigma allow us to check the data over and over again and have the right coverage. Right, but you need a reliable estimator of sigma. Reliable estimate of signal. And we have them. We have them. I wrote like 14 papers on this. Okay, but it gets complicated. Okay, it just gets complicated. I'm trying to figure out what's the advantage of the squares, I guess they are, as compared to the ellipse. There are situations where the ellipse makes a lot of sense, okay? If you really care about the direction in the underlying distribution, because it captures the Underlying distribution because it captures it. But if you want marginal interpretations or interpretations around smaller subsets and not the whole distribution, these things can give nice marginal interpretations. I'm not saying that they should be used for everything. Confidence intervals of each, but don't you lose the correlation. But if I want to make inference about just one About just one marginal, and I have a thousand irrelevant parts of the distribution, then why do I care? Just to follow up on it, so I think the point actually is that at this point you don't care, but like if the correlations aren't matched, this would be this, like imagine you have a histogram and you plot these confidence inflows in a histogram, those are the correct confidence inflows without having to think about correlation. At that point, the correlations don't matter anymore. The correlations don't matter anymore. The correlations only matter if you look at these ellipses. But if you look at these boxes, then the correlations are not meaningful. You don't need to think about the correlations. The point density rules themselves give you all the information you need. That's correct. Now, the correlation is captured in our estimation of signal. For people to complain that there isn't enough correlation information, you can get correctly because they want to put a function to it correctly. Well, but that's exactly the point, right? That's the difference between simultaneous. That's exactly the difference between the lower button and the upper button here. That's the difference between simultaneous and pinwise. So if you do only pin-wise, then yeah, there's not enough information. And if you do simultaneous, then you can use that information to fit some goodness of fake best. I'll also say in a forthcoming paper, we've shown how to generalize this to create confidence tubes around functions. Tubes around functions, simultaneous conference tubes around functions. You talk kind of assumed you have like a fixed sample size, the side M, and then you're simulating, and then you're off to go. You could also imagine situations where you kind of dynamically start the simulation, and then once you're you know, your estimate shrinks, you're like satisfied and you stop kind of dynamically. And so there are systematics associated. Okay, so the properties that we established on the correlation matrix guarantee that will work. That the resulting intervals will all have the right coverage probabilities and all that. So our method explicitly allows a sequential dynamic check. Statisticians last some years. in the last some years I have become focused on models in which they think parameters as given in their model are the right and relevant ones. So the coordinate axes in this graph are inherently more important than any other coordinate axes in this graph. This is, I mean, this is really literally what a high-dimensional regression does all the time. Regress on something. Time. Regress on some predictors. You've got way more predictors than you have responses. So the model is not identifiable, but you fit it anyway because you believe in some way that most of the predictors are irrelevant. And your goal is to find only a very small number of active variables. So the the inference is focused on the margins in a heavy way. On the joint behavior, because most of the confidence intervals are around zero. Yeah. Almost all the confidence intervals will include zero. Thank you. That's right. That's a good way of saying it. Could you maybe say a bit more about the bottling of error? When we have the Monte Carlo, you want to estimate a smooth curve or a sort of distribution. From a statistical point of view, it seems crazy to do a billion simulations. Why do you just do a million smooth resulting boxes? I'll give you a skimmed headache of what Schematic of what we've been thinking about. Suppose you simulate however many you simulate, right? And you just want to estimate a function. It could be a likelihood function like Charlie's doing. It could be just a function, but it has to be a bit, you have to integrate it, so you have to restrict it to a compact set or some finite set where you can integrate it, right? Some set of interest. Of interest. And so if you have this set of interest and you have a bunch of samples, and you can just do a non-parametric estimator of your function of interest, you can also take and just pick points along here. And then these points become the elements in your vector theta. And then we've got a proof that shows that if you go through and do the multivariate If you go through and do the multivariate error, these calculations that we talked about with estimating sigma, that we can kind of connect the errors. I maybe should have drawn a piece of dots or something here. That simultaneously over that region is going to be the right coverage probability of tube around that. So we thought that we had invented that. It turns out some economists did something pretty similar. They proved no theorems about it. Similar. They proved no theorems about it. They just did it 25 years ago. But we have ways to estimate Sigma in more general settings than they do, but I have to give them credit for... I mean, I have to give the economist credit once in a while. So that term formulation looks very similar to the Gaussian processes where they make a problem on the SI bottle of all the curves. It is related. It's distinct from that particular process, that particular formulation. I'm giving you just the highest level. Kind to the economists. It looks very similar in their 95 paper, which is more than 25. I think that the distinction we're making is we're The distinction we're making is we're allowing the underlying simulation to be much more general than anybody else. We've got conditions in that setting. It's not just even related just or just from Markov change. It's much more general processes. We tend not to use simultaneous confidence intervals. Well maybe we should be. I agree. Can you say something about pros and cons? About the pros and points that we should know? Well, I mean, the cons are, right, is that you often, if you want to make inference about a single parameter, fine, you need one confidence interval. But if you have two and they're all related based upon the same data you're estimating, then your coverage is going to be wrong if you don't incorporate that. Your joint coverage over both can be wrong. And the simultaneous intervals allow you to give constant. Simultaneous intervals allow you to give confidence statements for functions of arbitrary functions of all parameters. Right. So if somebody said, well, the physical quantity I'm interested in depends on this list of parameters in this call in a complicated way, simultaneous continuous integrals give you overbounds on the value of the function. I mean, they might not be truly able to evaluate, but there's some sort of thing that guarantees. That is true. You can then generalize this to more complicated things. Just in front of my answer. Just influence that. So Louis, in some sense, you are actually using simultaneous influence and you are accounting for the coverance matrix. Like, you know, whenever you do witness of fit test or error propagation or anything, you always account for the correlations between the bins, which is the heliosis here. So in some sense, that's actually what is done when the cognitive stranger is built into any of these inference procedures. This would be an alternative way of doing that. Now you wouldn't have to worry about the co-agraph strangers. You just take the set, take the You just take the set, take the individual BNA bias intervals and propagate those or do fix or win as a fit based on those. So in some operation, it might be easier because you only look at the marginal BNW intervals and it all works as you would expect without having to worry about the dependence. But at the same time, you have sort of dealt with this machinery for dealing with the co-responses and propagating turbulence through your analyzes. So it sounds as you can also do that. And I guess there's a bigger question. Would there be Would there be a benefit from doing the simultaneous intervals in this case when there's already an existing kind of practice of using the full coverage, which is every just using the elements, and that is kind of built in both the pipeline. So, I mean, do you think there would still be a benefit from switching from the elevators to the volunteer species? I mean, our motivation was simply what was described before, right? Very often in statistics, we're just interested in a marginal. interested in a marginal. We want to make inference about that marginal because we think that parameter has something to do with the world that we're interested in. And so if that's the case, then the elliptical regions kind of are opinion, right? And they don't have great... You can turn these ellipses into marginal intervals, but they all tend to be very conservative. All those standard techniques tend to be very, not all, but they tend to be very conservative. Very conservative. Well, I'm confused because we take that ellipse and basically do what's equivalent to profile likelihood. We get a, if we're only interested in component one, we get a vertical stripe up there, so plus and minus an ending of component two, and it exactly covers, and you get it by the usual way for The way Fred James talked about it years ago was we find the right size ellipse and then we take the extreme of those two ellipses. Yeah, those are going to be too conservative in general. Well, it depends on what number you use for the ellipse. I mean. If you use the kind of the standard type ellipse, based upon the Wischer distributions and things, that's going to be too conservative. Once we've profiled that we kind of Once we've profiled out the component two, we can use plus one and then uh delta chi-squared. Coverage is perfect. It boils down to taking the parameter values for which the log likelihood is equal to the max of the log likelihood minus one half times the okay, but keep in mind, I'm not doing a likelihood-based analysis here. I'm not assuming any underlying model, right? I'm simulating it through a process and then using that game. It through a process and then using that game to start intervals for my estimate. So I was thinking of those ellipses, that ellipse as a contour of it not only encodes the correlation structure of the underlying distribution, it encodes the correlation structure of the simulation process as well. So those are both encoded in Sigma. How much difference? How much different? We're talking about different things. The issues are kind of, some of the issues still come up. They do. I mean, it seems to me if you're going to write in your abstract, I mean, if you can deal with the ellipse, which physicists can, it's kind of all the information is there whenever we're in it. But if I say abstract, you want to say there's two parameters in here, they're intervals. You're writing it in English, plus or minus, probably when you want to do the simultaneous ones instead of artificial ones. Or do you make a copy of the channel that you think of? I'm following that. You think of like unfolding and unfolding histograms, like those are the case where you have strong dependencies within your units. So, like, if you only report the ellipses and basically keep the marginal intervals, then you would also have to report the cover as matrix, which no one is doing right now. But if you instead report basically these boxes, which are simultaneous intervals, then that histogram you put on the paper would have all the information you need for drawing simultaneous inferences from the histogram. So, I think at least for a policy. Instagram. So I think at least for like a blocking purpose and kind of easy communication, that would be the case for if you really want to simulate though, wouldn't you want the ellipse? Because you want to take into account the correlation then. Right, right. But if it's just like in one picture, in one histogram, you want to summarize your inferences while accounting for this simultaneously, then you might want to report to simultaneous postings instead of the. Honestly, that was our entire motivation. You just said it better than I did, okay? It's better than I could. We frankly wanted to draw this picture so that people could understand it better. That's interesting because I don't know that we've ever done anything like this. We basically draw three reasons. We have an infinite vertical stripe, an infinite horizontal stripe, and the little. And those are our doing for Newsence parameters. Part is we sh we sort of do it for Newsence parameters because we have a list of dependences of the parameter of interest on variations. Parameter of interest on variations each and this is parameter. But it's single numbers, it's not rows. And that's one at a time. And we have to teach our students the width of the vertical stripe is not the width of the ellipse. Yeah, we have situations where we're extracting two parameters of interest and we usually compare it with standard model prediction. So the standard model prediction is somewhere. So the standard model prediction is somewhere on this plot. Right. And we would look to see whether it came inside the ellipse or not. I mean, in two dimensions, I think that's perfectly fine. Most of the time, like in a lot of Bayesian models, you're dealing with much higher dimensions typically.  