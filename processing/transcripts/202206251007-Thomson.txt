Was going to be long distance transmission. We got it working. It came in, you know, under budget, ahead of schedule, double the performance. And then they discovered that no place in the world needed that kind of capacity. So it got mothballed. And I moved over to cell phones and worked on that. No publications there. I had a department. I had a department head who said, Oh, if you've got a good idea, when it kind of works, you can publish it in a journal. If you've got a good idea and it works really well, you publish that too, but you do it through the patent office. So cell phones, I've got a dozen patents and no publications. But the 82 paper came out then. And the next year, I went out to Scripps and talked about it. And talked about it. And so I've kind of changed course. And then came back from Scripps and was in actually what had been Shannon's department in the mathematics of communications. And so I worked on climate and laser reliability and various things. Various things. And then, of course, ATT managed to bankrupt itself. And so I ended up here at Queens. And let's see. Page down. So little introduction. Top there is a spectrum due to Beethoven. So it goes time left to right. So it goes time left to right and of course frequency goes bottom to top and things change around and it's not exactly it would be pretty boring if it were stationary. And just as a reminder, you get the spectral representation. And if it's stationary, of course, different frequencies can't be correlated. So we expected of Of dx at f and dx at chi are the spectrum with a delta function. So that makes it a bit suspicious. And if you then look at the what's known as the Wiener-Kinchen theorem, or now the Einstein-Wiener-Kinchen theorem, it was actually discovered by Einstein and By Einstein in 1914, I guess there was some minor disturbance in Europe, so it kind of got ignored. And so a little history of the spectrum, of course, discovered by Newton, you know, quite a while before Fourier and then Kelvin and Stokes. And Stokes have a series of correspondence on it. Kelvin had actually helped translate Fourier's book, so he knew about it. And then Stokes suggested the form of a periodogram. And Schuster named the periodogram and published kind of the first example. And he has an interesting paper. He has an interesting paper. You should read that if you haven't. It's just saying, yeah, do a periodic autograph. And it was actually written in terms of integrals and time. And so a lot of the early work you kind of have to think of in that term. And then, but really actually knew that it didn't converge and published that actually before it was published. Published. And but Raleigh is an interesting person because Fermi used to say when he was at Bell Labs, I didn't see him personally, but I heard it quoted by three or four different people. He would say, most great scientists have one idea in their working lives, except for Lord Raleigh, who had two. So let's see here. Let's see here. So, a spectrum basically the problem is you get really very short series. If you look at a music C D, it's 44 kilohertz per second, two channels parallel. And if you look at our longest climate record that's actually measured, the daily one from Uppsala, that's a That's 110,000, 112,000 samples. And, you know, so typically climate is the first couple of notes and music. And Doug Martin, this is a quote he made actually in a bar after the Royal Statistical Society meeting in 1979. But time series. But time series is the worst subject to teach. First, you have to teach all the standard methods, and then you have to tell the students none of this stuff works. So, and this is what people really do. So, what happens? This is a just first year, 10 years or so of Kingston data. We get it from NOAA as a possible. Possibility. If you compute a periodogram, well, it's got problems. It gets at least the length of a year right. Maybe not much else. And there were problems that Stokes had noted, that Raleigh noted, that Tukey noted, lots of them, etc. If you go to one of the Go to one of the worst estimates that would be an autoregressive one. I guess it's just an AR2. Same data. Well, you look at the problems. It was a little later than the periodogram and gets the wrong frequency. Pretty much everything else is wrong. And that's been noted by two. And that's been noted by Tukey Arado here. If you don't know him, he was one of Cole Mogarov's students. So that's one to take seriously. So what can be wrong is with an auto-regressive model as an example. So come up a little bit to about 1950, and you get. And you get Hamming with the tapers, so the tapers here in the top window, dated Kingston data times the taper here. And if you do a spectrum with that, well, it still gets the right length of the year. It's a little not quite so bad on the side lobes. And, you know, not so bad. Not so bad. So, this is the two estimates together. So, if you look, the Hamming window, of course, is a little wider than the periodogram. But then if you look out past the first side globe, the periodogram is obviously biased by an order of magnitude too high. And even if you talk to somebody. Somebody in parliament or Congress talking about a budget of order of magnitude mistake is kind of large. So, but it's a pretty good estimate for 1950. Okay, so it's got a lot of problems. The worst one is probably that when it comes to theory. That when it comes to theory, is where did the taper come from? It's a combination of John Tukey and Norbert Weiner saying it's a good idea. Not much more on way of theory. Berg in his Stanford PhD thesis down here at the bottom kind of goes ballistic about it. Generally, not a great estimate. Now, a couple of comments from my Bell Labs department heads. First one here, you know, consider if a problem is worth doing. And he said, do it back to the envelope calculation. And if it's not going to save the company at least a million dollars, don't waste time on it. Don't waste time on it. This was Biscoborn in 1965. He was my first department head, and I was just kind of being trained. And of course, in 1965, a million dollars was a fair amount of money. And then coming up, but you know, it does kind of cover your outlook to think. When Dieter Allsberg was made. Was my department head when I was working on WaveGuide? And he says, I can find at least 100 different ways of estimating a spectrum in the literature. What I want to know from you is which is the right one and why. And then he added, just for emphasis, and if you're wrong, we are talking about a measurable fraction of the US GNP. USGNP. So no pressure, at least not much. So how do we get to the spectrum? Well, we've got the Fourier transform of the data, just y of t here is df t of x of t. So we've got n samples. If we look now, go back to the spectral representation. To go back to the spectral representation, of course, that's the integral of dx, which generates a whole series, whether you observe it or not. And expected value of dx squared is the spectrum, just by definition. Now, if you take these two, plug them together, and I ignored the phase, you get that the Fourier transform is a convolution. transform is the convolution of a Dirichlet kernel here sine x n x over sine x of times dz or dx and so that's kind of awkward so you don't really observe this directly now the same kernel this derisha kernel shows up Kernel shows up in the Slepian sequences, or as they were known when they were published, discrete probaspheroidal wave functions. And Bob Parker out of script said, you know, that's kind of a mouthful. Why don't we name them after Slepian and continue? So we do that. So we now find that do just normal orthogonal expansion. expansion of y of y sub k the co eigen coefficients are integral minus w to w the spectral the y the Fourier transform of f minus chi times the Supian sequence of VK and you wave your hands a little bit and you find that y sub k is sum from 0 to n minus 1 to n minus one of data x of t times the Slovian sequence V sub t and k and Fourier transformed. So it's you can say it's Fourier transform of data times a taper or it's an orthogonal series expansion of x of t times e to the minus I2 pi f t. And so then the simplest multiple And so then the simplest multi-taper estimate is the spectrum estimate of the spectrum is average on k of the y sub k, the eigen coefficient up here, magnitude squared. And so the important thing is k should be more than one. You can still use one, of course, but it's not a great. Not a great representation. So it's derived from first principles. It's not ad hoc. You get two free expansions, kind of, of a block length, the N, which is often dictated by that's all the data you've got. So that's not really free. So that's not really free. And then the time bandwidth product, the end determined and W. So it's optimum energy concentration. It's numerically stable, so that's good. You can compute it with an FFT, so it doesn't take any time. I still hear people saying, oh, you know, FFTs are slow. Well, that might. Well, well, that might have been true in 1960 or 65 when they came out. But you know, I timed one several years ago working on, I forget which one of the Cray, probably the Cray one. And it was still, it was all of a few milliseconds for the FFT part. So, and it was 10,000 point transform. Transform. So that's a kind of a silly argument. It's a paper by Stoicha and Sundon. It came out in 1999. It says the multi-taper estimate, at least in cases that are simple enough that you can prove it, our maximum likelihood. And then this quote by J.J. Thompson. This quote by J.J. Thompson. People spend all their time on convergency and never learn how to use the series. So that's kind of an interesting viewpoint. Well, what do they look like? These are the first four, I guess, Cepian sequences of, I think it's NW is four or five. Forgot to mark that on here. And their Fourier transforms magnitude squared. So zeroth one here kind of drops off the cliff, goes up to just about 100 here, because n is 100, and hits 10 to the 10th. And the zeroth one is down a couple of magnitudes below that. Let's see. Let's see, this is the so you can see, you know, you've run into side robes at some level, but 10 to the minus 10 is pretty small compared to the main lobe if you compare the ones from the periodogram. So, we can do the same spectrum of the Kingston data that we had before. You get a pretty narrow line, and it's basically a smooth spectrum with bouncing all over the map. The problem with single taper, of course, is it's chi-squared with two degrees of freedom. So the most probable estimate. probable estimate or most probable point is at zero. So you get a rather rough spectrum. And so you have to get up to the average to keep it, even if it's not a bad spectrum or a difficult one, to keep the bias away, but it's most likely is at zero. Not a particularly useful thing. This is a series from England. It's the longest series. It has some problems. One, it's monthly in the early part. I think the daily data was lost in the Hampton Court fire. And then there's another one. So again, Other ones. So again, rather sharp line. A couple of other little ones here. Two cycles per year, and I cut it off because it gets boring. This is looking at the annual cycle around one, which is interesting. Now you have to look at the frequency range here. This is 0.999. 99981 and 1.000, I think, one, two, yeah, three zeros to two. And the peak in the spectrum is definitely below one cycle per year. And so, you know, if you look at the F test and do all the variants of that, you're about five standard. About five standard deviations away from one. The thing to remember about an F-test is the variance is proportional to one over T cubed. So the spectrum itself, basically the variance is independent of, you know, just k or one over k and times is. And times the spectrum squared, so it doesn't converge with increasing sample size unless you increase the bandwidth. But the F-test is a one over T cubed variant, so it's pretty strange. Basically, the same reason that we know planetary orbit so accurately and things. So continue on here. If we look at the phase of the Look at the phase of the Kingston series or of the Central England series. Sorry, what you see is the original is the red line down here, and then it jumps and continues going along in the same trend. So if you look at this, this was in 16, no, 1712, I think it was. That's what England changed from the Julian to the Gregorian calendar. So that's, yeah, that's not a mistake in the data, just changing the calendar. And so you fix it by 11 days and it. 11 days, and it kind of goes along. And the dotted blue line in here, the trend, actually isn't a fit, except I kind of drew it through the middle, so eyeball fit. It's got a slope of 50.29 arcseconds per year, which is the general precession constant. So In the Central England series and a lot of other places, the length of the annual cycle is not a cycle per year. It's offset by precession. And that occurs through a lot of Eastern Europe or Western Europe. Paris kind of bounces back and forth, and then when you cross. And forth, and then when you cross the Alps, it changes to a normal year. And so this got so this is now quality control data from when I was doing WaveGuide. Periodogram is up here for For reference, pretty awful estimate. This is the raw data series. Now, on here is a little bump. This is from the machinery that made the waveguides, and you knew what caused that. Then, this ripple out here is it was pretty early series, and there were two samples that, or a sample that Or a sample that it took long enough to transfer to disk, but it suppressed a voltmeter at lower priority than the tape. And so it got suppressed and one sample is just repeated. So you see that. And so you fix this and you're down at a couple of more orders of magnitude. The thing that's interesting is this outlier when you're Is this outlier when you go and plot it, and I don't think I included a page of a plot, you can't actually see it. It's, you know, just same sample repeated. It's not, and yet it causes a two-order of magnitude error. So you have to be careful with time series. Another thing that Another thing that to watch out for is a lot of series have a lot of periodic lines in them. This one is from GOES-10, so it's a geostationary satellite. So 1998 to 2004, and this one. And we're up here. This is Up here, this is the 99.99 three nines percentage point and four nines on the top one. So you're getting significance level, which is more than the number of samples. And this is a pretty narrow band in any case. It's only what's 60 de sentence. 60 to 360 to 370 microhertz. So you can actually pick out things. Charlotte and I published this a bit ago. Okay, so what have we looked at? Kind of the other side of it is the autocorrelation. You get, you know, a lot of papers. You know, a lot of papers about autocorrelations. Most of the estimates are bad. Some of the things we've done here are listed on this. One of the current problems as I've been playing with is running into data with a Laplace distribution. So just double exponential. And it was discovered in front of... Discovered in front of before Gauss, but the theory for it in time series, multivariate Laplace, is very recent. And so still an ongoing problem. And it was a little of a surprise to see data that had the class distributed data. And so That looks wrong. So, if this is now looking at the autocorrelation, so it's just Fourier transform of the spectrum. And it's kind of curious. A comment by Bartlett says it always has less damping. Says it always has less damping. That was fairly early, right after World War II, and he may have known better at the time. And it was still classified. Woener's book was certainly classified for several years. So this is just a very quick look. If you take a multi-taper spectrum, take its Fourier transform, or do the equivalent in the time domain. In the time domain. This first frame is the regular Bartlett estimate. The true spectra or true autocorrelation is in red. The estimate is this kind of ratty one in black, which goes out through a thousand points in the series and it rattles around out to the end. If you do a multi-taper estimate, Estimate. Again, the true estimates in red, the estimates in black, and it actually follows it reasonably well. So, you know, don't, you know, there's a folk theorem that says bad spectra go with bad autocorrelations. We know the ordinary periodogram is pretty bad. Periodogram is pretty bad, and the Bartlett autocorrelation, which is its Fourier transform, is equally bad. So it's a poor thing to use. Okay, so something that what I've tried to do with this is just a summary is convert time series from a black art into something that you. Start into something that you can actually do reliably. Let's see. Yeah, you can teach the multi-taper one going back to whatever quote it was I had there earlier, Doug Martin's. Without telling the students that none of this stuff works, it's well, you can read these probably better than. Well, you can read these probably better than I can. That's a bit of a problem. I put them on here and now I can't read them. So see what's going on. So, but it gets one of the places this where you've likely used this is in touch tone. Craig Lindberg and I worked that out when he was doing a postdoc at the labs. And it's a The labs, and it's about a factor of 10 to the fifth improvement on false detections and so on. So definitely worth while. And a couple of maxims. I always like to read people about, or what people read these. These ones are more or less about spectra, but But they're useful. So, you know, pay attention to the kind of some of the wisdom from the past and so on. And analyze data. Don't believe dogma. That's amazing. You know, if I believed the Central England series was a cycle per day. series was a cycle per day, probably wouldn't have figured out what was really going on. And then there's the corollary to Fermi's quote that most great scientists have one idea, so that says, expect about 0.01 idea per paper. Let's see, I think there's another page here. I will let you read these for yourself. Any questions or so on? I don't know how I am for the next one.