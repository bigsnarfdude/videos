Thank you for the organizers for putting this together and also Group Eight, which I have now named the Beasts of Burden, or looking in Burden. So one of the advantages of going kind of later in the week is that we all kind of are on the same page. We've sort of set all the groundwork, so I can kind of jump right in. The thing I want to talk about is experimental design and reinforcement learning, which is, of course, a very big topic. So I'm really just going to talk about two open questions that I've That I'm interested in working on, currently working on, but don't really have good solutions to. So, the first part is just what do we do when we have data where there's no, what I'll call, forced expiration. So, this is what happens when you have, you design an app and then you want to try the app out in a population of patients. So, you just put an expert-derived policy in there, but there's no expiration at all. Sometimes this happens because you just want to test out the app and make sure that it works. That's what I'll call a low-risk setting. That's what I'll call a low-risk setting. And then there's also settings where they are really afraid to deviate from that policy because they think it's unsafe. Those are very different settings and they need different approaches. And then the second thing is just about kind of bringing back classic design principles into these decision problems because I think that in health, there's lots of situations that are quite different than designing a computer player to play Go, for example, where we can just dump a bunch of data through and we don't care how inaccurate it is early on. Inaccurate it is early on, we know eventually we're going to convert. And so we sort of focus on first-order updates. And so, how do we take the perspective that's sort of closer to like destructive testing or really small sample designs and carefully consider each randomization? Okay, so some of these slides, some of you may have seen before. This is an example that I like to use that shows Novax Force exploration. So, this is an app that was developed as part of a pilot project at the UNC, and now it's And see, and now it's being extended to a bigger trial. It's called HIDRA. This is a weight loss after people type 1 diabetes, and so we heard a bit about that. And so what it does is it tries to give you insulin and food recommendations, but also is centered around exercise. And so those who are familiar with T1D know that it can be very hard to control insulin or glucose levels when you're exercising. People react very differently. People react very differently. It depends on the type of exercise, it depends on your diet and other things. And so this is a new domain and it's high risk. And if we recommend changes to the insulin level, we sort of explore too much, right, that could be very dangerous. And so the first version of this that rolled out had a deterministic rule that took every possible patient state and mapped it to a recommendation. And so what we really want to do, though, is we want to optimize this in personal. Really, want to do those, we want to optimize this and personalize it to patients. So, this is an example of an online decision problem. We've seen a lot of these now. The goal was to maximize some long-term cumulative utility, and I'm just going to assume that this is known and fixed, and we've all agreed that we've captured every feature of the decision problem, which of course is a difficult thing, but for the purpose of this talk, we'll assume that's fixed. And that at every time step, we want to use whatever information we have to select an action or make a decision. So, in this case, it's push something out or not, and then if we do push something out, what are we pushing to them? Is it eat a certain amount of. For them, is it eat a certain amount of carbs? Is it change their basal insin rate? Whatever it is. And we need to balance taking actions that we think will give us a high utility, given whatever we think the dynamics are, whatever our model is, and then also choose actions that will give us information that improves the model. That's how we get an optimal decision asymptotically, or as information accumulates. So there's lots of problems in precision medicine. Sort of the classic case is adaptive clinical trials, response-adaptive clinical trials, just-in-time adaptive intervention. Trials, just-in-time adaptive interventions in mobile health, which is sort of the focus here, infectious disease management, computational advertising, I guess that's not precision medicine, but lots of decision problems that have this form. And the way that people typically deal with this issue is what I'll call force exploration. So the thing that we're most familiar with in statistics is randomization. So it doesn't have to be pure randomization, it could be restricted. You could just take a rule and perturb it a little bit, or you could just select it random from feasible actions. Random for feasible actions. And the idea is that you have some kind of distribution of reactions or decisions that may depend on the state. And the hope is that if it's adaptive, then it will concentrate on the optimal action as information accumulates. Now, you may prevent it from becoming a point mass, as we saw in Susan's case where they had these safeguards or the guardrails, or you may hope that it eventually settles on an optimal deterministic strategy. And so the things that are used heavily in literature are epsilon and greedy. So you pick what you think is the best action probability. What you think is the best action with probability 1 minus epsilon, you randomly select from the remaining choices that are feasible with probability 1 minus epsilon. Thompson sampling or posterior sampling, which we've already seen. In math stats and biostat, we see ERN models used a lot. I don't think those get used in the mHealth setting very often, at least I've never seen it. There's also what I'll call conditionally deterministic action selection. And this is where, conditioned on all the information you have, the action recommendation is a deterministic function of the history. Of the history. So things like upper confidence bound sampling is an example of that. So the idea there is you take a mean, estimate of the mean utility, and then you add a term that might be like a scalar times a variance or something else. And so it's a known function of the data, but it forces you to choose things that you think have potentially lower utility but have high uncertainty. And things like mutual information, adaptive designs, Bayesian dynamic programming, and other things also have this feel. This field. Okay. So getting back to this, in the first set of data, the system was based on an expert-derived policy. There was no randomization and there was no exploration at all. So if a patient presents with state S, there's a map that takes state S and maps it to action A, and that's it. So this is what the app looks like. Actually, it just comes up a little bit from this now. This is the earliest version. So you get this action. So this says, This action. So this says, maybe you can't read this, but it says: eat a snack, 25 grams of carbs, and this gives you that recommendation for adjusting your insulin. And this is based on state information from the patient. So here's some of that state information. How much time it is until they're going to exercise, calories, glucose level, and so on. This is connected to a glucose monitor. So the question is, what can we do with this data? So we've run the trial, and the idea is partly just do people like the app, do they use the app, right? Does it work well with their pump? Does it work well with their pump? I mean, their glucose monitor, can it read that and so on? Can it interface with like my fitness panel to get diet information? But then we now want to learn from this data. And this problem arises in a lot of different cases. And I find myself encountering this problem in other settings. So infectious disease management is one area I work on. Other sort of treatment of serious illness where people don't really want to or don't feel comfortable exploring. Banking is another one. Tracking nuclear. Tracking nuclear materials. I think in another part of my life, I work in that problem. But the point is, you have to learn without inspiration. So there's sort of two ways that can happen. One is greedy selection. And this is done a lot in industry, where you just have a model and then you estimate what's best and you just do that. You just keep doing that. But in our case, I'm going to focus on following a deterministic expert-derived policy. What do we do in that case? So, what can we do? So, what can we do? Well, we know from causal inference that the answer is kind of nothing. Positivity assumption is completely violated here, right? There's no variability in action selection given state. Variance is zero. But the mathematical answer is, well, if you're willing to make a lot of very strong assumptions, then you can learn as much as you want. And people have even shown you can learn as fast as you'd like as long as you trust your models completely. And so, this is a paper that came. And so, this is a paper that came out in 2018 in contextual bandits, where they sort of show that they can get consistency, they can get rate optimality. There's a lot of papers like this going back to adaptive experimental design in the 70s and 80s, where they show that greedy procedures like this can be optimal, right, in terms of rates. Yeah. But it's like dependent on for example in that specific paper, it's heavily dependent on having uh sufficient excitation in the in the context space, right? In the context space, right? Yep. That's a small one. Perfect lead into this one. And so the idea here is that you hope that you get some variation in the system, that you get these perturbations in the system. And those perturbations, say, around, if you imagine there's a threshold, if it's above a threshold, you give treatment one, if it's below a threshold, you give treatment zero. You get these perturbations around those thresholds. We can learn something just from the variation in the system about what would have happened under different treatment assignments. And so, you know, that's this idea. And I, you know, originally got excited about this idea because it seemed like the only way forward in some of these problems. And so, you know, as I mentioned, they showed consistency and rate optimality under greedy action selection. We extended this to do consistency and asymptotic normality under MVPs and POMVPs, not only under greedy selection, but under these deterministic policies. And so I kind of got excited about this. I thought this was a cute mathematical result. Mathematical result. But now I have to tell you, you shouldn't do this. It's a very bad idea. And the reason it's a very bad idea is it relies so heavily on extrapolation that you have to really trust your models, but also because there's no overlap, you have no way of knowing if your models are wrong either. Do you know what I mean? So you're trusting the models and you have no way to know that they're wrong. And they could be arbitrarily wrong. So this leads me to the This leads me to the regression discontinuity designs, are they trusting their model that much? Or why isn't there an analog of the regression discontinuity design here? Okay, so in the extreme case, imagine we have a linear model and your treatment rule, and you have binary, let's say you have binary x. And you just say if you have x equals 0, you get treatment one, you get x equals 1, you get treatment 0. And then you just assume there's some kind of linear model. And then you get an X that's a half, which you've never seen. You have no idea what's going on in between there, right? And so that's one of the reasons. Well, there's no points in the margin, you mean? There may not be very many points in the margin. And even if there are, then you might feel good about, in effect, sort of near the margin, but then what do you do is you go farther away. Yeah, and that's extrapolation. And that's total extrapolation. And these are in high-dimensional settings. Yeah, yeah, yeah. High-dimensional settings. So, yeah. Okay, so what do we do? Well, in a low-risk setting, so there's another study at UNC called Nudge, which is currently under review, where they also have a deterministic policy they use initially. But it's on weight loss, and they're just messaging people, take steps, eat right, this kind of thing. That's what I would call a low-risk setting that we can just take the policy we learned from this data, we can estimate it, even though our assumptions may all be incorrect. We take that as a warm start. Be incorrect. We take that as a warm start, and then we just learned from there, right? That's our starting point. And if we're way off, then maybe it's worse than starting at a random policy, or maybe it's not. The hope is there's some signal that helps us, right? That's one way we can use the data. But in a high-risk setting, we really have to be cautious. We don't, you know, as I mentioned, you don't want to estimate the policy and treat it as if it's correct because it relies so critically on the correct model. So, how do we create algorithms that give performance guarantees, that give us the right answer, so we keep running them, but they also have safety guarantees. But they also have safety guarantees that they don't deviate too far from the expert policy and they do things the expert thinks are safe, at least. So now you might have constraints that come from the expert that tell you, okay, these are safe things. I'm willing to experiment in these areas and not other areas. How do we make this all principled? There's lots of ad hoc things that can be done. But how do we sort of mathematize this and formalize this and come up with an answer that we think is sort of satisfactory? And that's the first problem I'm really interested in thinking about, currently working on. And I would love to talk to you. And I would love to talk to you all about that. And on the other sort of side, the sort of classic design, and I don't have a lot of time left here, but I only have a few slides. I'm just going to show you an example. This is not from a smart, but it's an adaptive clinical trial, and the ideas apply. And this is a trial that's going on, that's why I wanted to show it. So I'm sure you've all heard of the iSPY clinical trial series on breast cancer. So there's a whole bunch of these, and we're involved in one that's iSPY 2 Plus, but the design is similar to ISPY 2. But the design is similar to IS by 2. The idea is that this is a response adaptive design. Patients are coming in, you're assigning treatments with varying randomization probabilities according to how efficacious you think the treatments are. There's also this extra little feature, I guess, that effective treatments can be graduated so that they become salvage therapies, and bad treatments can be removed from a trial. Let's ignore that for now. And so the idea is: patient comes in, they get assigned a treatment, something like Thompson San. Assign a treatment is something like Thompson sampling. You observe their outcome, you update the probabilities, you may throw out some treatments or add new treatments, then sort of peanut butter the probability and start over with the next patient. So the goal there is to balance current and future patient outcomes. That's what we think about with the responsive adaptive clinical trials. It's the same problem in mHealth. It's just immediate versus future utility, right? The information in an adaptive clinical trial, you think about out-of-trial patients versus in-trial patients, here it might be immediate utility for one patient versus their future utility. That's same for the Patient versus their future utility. That's same for the same patient, but it's the same problem mathematically, or at least in a lot of ways. So here's the idea initially was to use some variant of G computation and Thompson sampling. That's what was originally proposed. And this is a simulation of that, of Thompson sampling based on a generative model. It's informed by existing data and so on. So they have sort of carefully constructed simulator. And so we're going to look at a burn-in of 100 patients. The first 100 patients just get randomized, 50-50M. Patients just get randomized 50-50, and I only showed two treatments here to make it clear. And what's up? Here, the y-axis is what's the probability you get the optimal treatment given your covariates. So, not for you, it's not for your specific counterfactual outcome, but for your covariance, you look at the average or the conditional average treatment effect, did you get the right treatment? So, early on, it's 50-50 because you're just tossing a coin, and then you start doing Thompson sampling. And what you see is there's some concentration of points here. That's good. There's also this concentration of points down here, that's bad. And this is That's bad. And this is what people in the response to the advocate of the trial world sometimes call stickiness. You get stuck in a bad spot and you assign the wrong treatment for a while. And eventually, if you run this long enough, it will become more concentrated in your one. You won't have this issue. So it doesn't seem to be very adaptive, even though it's adapting. It just takes a long time. It's very slow. And so this got us thinking about some ways in which we can try to be. Some ways in which we could try to be safer in these settings and try to make sure that we are being a little bit more careful about how we balance in trial versus out of trial. So, I'm going to show you, this isn't a graph, it's a schematic. So, imagine you have a patient that presents in the trial, middle of the trial. You have some estimate of their efficacy. They have two treatments, right? Efficacy. And then you also have some measure of how much information you get from each treatment you give them, right? You give the treatment one or treatment two. In this schematic, you can see treatment one is more effective than treatment two. More effective than treatment two. It also gives you less information. So it improves your models less somehow. I'll talk about how to mathematize that in a second. So in this case, we can probably all agree that you should give them this treatment. It's better for them now, it's better for the future patients because it improves your models more than if you'd give them treatment two. An alternative is this situation. So now you have this trade-off. Treatment two is maybe more effective, but treatment one gives you more information. So now you do have. Treatment one gives you more information. So now you do have this trade-off. What should we do in this situation? And here, maybe you say, okay, well, select the Thompson sampling or UCB or whatever other method. Okay, so this very simple, stupid idea, which is why it's so open problem, you can implement this. You can say, okay, I'll measure information in terms of, I'll look at the KL divergence between like a prior and a posterior. It's a frequent disant, but that's the idea. And if I'm pretty confident, so of course these things have error, right? So if I'm pretty confident I'm in the situation, Error, right? So if I'm pretty confident I'm in the situation, I just won't give them this. That's it. It's a very, very simple thing to do. Here's the original simulation I showed you. Here's what happens if you do that. So it's a very trivial thing to do, right? Trivial change. And you can see that there's a much thicker line here. You don't have nearly as much of this sort of stickiness, right? Eventually you sort of stop having these point masses at the wrong treatment. And it seems to give a big improvement. To give a big improvement. So in this case, this just is one example of taking this idea. These are all over the place in classic experimental design. These are old ideas. Maybe not exactly this sort of domination, sort of removing treatments, but this kind of thinking, right? Balancing information gain and efficacy, thinking about how to combine them. These are all old ideas. And so if you do this, it seems to give some benefit and worth maybe. Benefit and worth maybe digging into deeper. So, in online decision problems like responsive data clinical trials, micro-randomized trials, and so on, we know we have to explore to learn. But if we take forced explorations like Thompson sampling or UCB, and you get the same plot, by the way, you do these other methods, they're ad hoc in that they're sort of just designed to somehow balance the two and they give you the right rates, but they don't, they can perform very badly in small samples. They can be very unstable. They're based on these stochastic approximations that are hard to tune. And they often select dominant. And they often select dominated actions. So if you run these and you pretend you know the truth, they do that all the time. And so, can we come up with principled methods that allow us to do that trade-off? And what you really want is you don't want information there in like KL divergence land or some other kind of information metric. What you want is something that captures the actual benefit in the future. So if you're just talking one patient, if I give them what I think is the worst treatment now, how much potential utility can I get in the future from the information I get now? That's really what you want to do and translate that information. That's really what you want to do and translate that information. So, can we express the cost of exploration in terms of expected cumulative utility? And come up with sort of schematics, like I showed you, and design methods around that. And most of the problems that I work out, and it may be different for you all, is that I'm willing to trade a lot of computation power for this. I don't need linear updates. Of course, we have to be concerned if we're running things on phones and so on, but I would much rather have this and have it running on a server, a bunch of computations. Running on a server, a bunch of computations, and get a single linear update because it's not really the world that at least I live in. These decisions are made once a day or a few times a day. There's time to make those computations. So I think I will stop there and even hear your thoughts. So thanks. He says the proposed mascot for the beast of burden. Questions? I don't want to ask you. So Eric for the first problem, are you still in the batch of data or are you thinking about online? Ah, we have a batch of data in that case. So when you have a batch of data, what you're still thinking is improving the actions kind of by moving. Oh, because, yeah. I didn't mean to cut you off. I'm just going to repeat what you're saying. So if you have a batch of data, then how does that be, what does that have to do with an online problem, right? So you have the batch of data, like in Heath. So, you have the batch of data like in HIDRA, that was the first trial. Now, you're going to run another trial, and in the second trial, because it maybe looked promising, you want to allow the algorithm to start learning. That's what you want to do. So, you want to take that batch data and somehow have it inform where you start in the follow-up trial. That's like a warm start, but then also the other issue is how do you make sure you're safe when you start learning. So, these are things that are happening over many. These are things that are happening over many trials. And you want to see how you can tweak the actions so that it's not hard. Right. There's some kind of guarantee. So it's not just choosing the wrong. When you learn the guarantee, everything is deterministic in your batch. Saying it? In your batch of data, the actions are deterministic for you. Right. So when you learn the... Oh, yeah. So you're not learning the, you're not. What you want to do is you want to see if you can estimate the optimal policy, right? You can estimate the optimal policy, right? And of course, you can do that if you're willing to trust your models. So maybe that's your starting point. Then what you want to do is you want to roll it out, and the safety guarantee.