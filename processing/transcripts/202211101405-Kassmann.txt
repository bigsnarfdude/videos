So thanks very much. First of all, let me apologize for the delay. I in fact had some technical problems and then I used also wrong here. I missed the whole bottom of Mester, so sorry I messed that up. I would like to thank the organizers for setting up this workshop and for inviting me. It's one of the best places, right? And I'm happy to meet people I know, but also people I have not known. And I will be in contact with in the future, so that's perfect. The future, so that's perfect. In the abstract, I had said something about non-local Neumann derivatives and the probabilistic interpretation, but I realized when I looked at the program that there are many very active young colleagues from Biedelfeld here who are not presenting, and so I decided just yesterday, okay, come on, I'll take 20 minutes from my talk and present their works because I have the feeling many of you are interested in their result and you can connect to them. So I will now, for 20 minutes, So I will now for 20 minutes speak about their results. Before we start, okay, let me advertise the workshop. It's not the first one. We have created a new series in Biedelfeld, non-local equations, analysis, and numerics. And if you have a cell phone, you can just scan the barcode, and then you have all the information. So please spread the word. Spread the word and send colleagues, students, or collaborators to Biedelfeld. And if you want or need funding, please let us know. We are right now in a pretty good situation. Okay, so the news from Biedelfeld. I have to... Oh, this was now I get it. So let me explain the words of four people who are on here, and then you can connect to them. I call it the B2B. It's like the B2FL2 band. And some of you know that Marvin has moved to Barcelona, but he's also B, so it's perfect, right? So let me explain some of these words. So this is about divergence from non-local divergence from equations and this classical question when you have diversion of something equal to right-hand side, what regularity do you have from the gradient if you know regularity on the right-hand side? So here Simon has studied this for non-local. Studied this for non-local equations. So you set up what I would call a non-local operator with indivergence form with some coefficients Axy. And let's keep it simple. We assume that A is just bounded. And you want to study the equation L A U equal mu and mu might be, let's assume it's a radon measure, of course it can be a function. Then there is by now a famous result by Kusiming Joe and Sierra 215 that says that you have this information. That you have this information, that you gain differentiability for some small number epsilon. And they were an alternative proof by Armenikova and others. And so this is, of course, a very interesting non-local effect. And the question is, can you get better regularity if you know more about A? So here A is just bounded measurable, and the question is, if you throw in some extra regularity, can you get more? And here's an answer by Seeman. By Seeman. So you assume A is in BMO, vanishing mean oscillation, and then you gain regularity exactly in the sense where the range of T is depicted here. And this is a very interesting result because again that is purely non-local because for a local equation such a gain is not possible. So in the easiest case, I like that example very much. Look at this equation here in divergence form. Here in divergence form, this equation leads to this here. So, and whenever you claim some better regularity of u prime, then basically by choosing b you can kill it. So, in the local setting, no differentiability gain is attainable, so even if your coefficients are better. Now, there's a forthcoming work by Seemon, together with Tromo and Yannick, where they go deeper into the potential theory work. Potential theory world, and maybe you want to read the applications here. This is about gain of integrability for the gradient. So you set up a system in a Lorentz space. Here is the Lorentz space, which is in between Ln and Ln plus epsilon, where you would have C alpha. And here you get, in this framework, you get even Lipschitz regularity for your function. Regularity for your function u, and this is just let's say for the experts to read this quickly when you know the least potential approach. And of course, Toma has worked on this before, then you can easily grasp this. If not, it's not a big deal. So Minjung is also here. Ming Jung has spent a few years in Biedelfeld, and I want to present two of his works. You might want to relate to them. This is a very classical. This is a very classical problem. It's about the green function and the bounded domain. So you have a nice bounded open set, and you define the green function in a classical way. This is again related to an operator and divergence form. The coefficients were a xy before, now it's kxy. And of course, what you want, you want to pick up the regularity that you know from the fractional Laplacian. And let's not go to the boundary if you're staying inside. Then obviously, this is exactly. Side, then obviously this is exactly what you expect from the green function. And you might wonder why is this now interesting? Because that's, of course, ancient even for non-local operators. But what is interesting here to try to prove it in a robust way so that those constants don't depend on s. Why is that a little bit interesting? I mean, first of all, you might want to use the results, but if you go through the heat kernel, through the fundamental solution, and you define g xy as integral in time of the fundamental solution, you cannot prove such a result. You cannot prove such a result because your estimates on the fundamental solution are, of course, not robust when s goes to 1, because you change the behavior from exponential to polynomial. So, under rather general assumptions, we were able to prove those estimates. And we want to continue, of course, we want to have robust boundary regularity results. That seems to be a little bit tricky. Now, I asked Minium also to meet with Norval Wina, not only with Green, and Only with green, and he delivered quickly on this. So, that is about a very classical problem. It's about regularity of boundary points. So, you know, when you have the original problem and you have a boundary point, then with the help of the capacity, you can derive an if and only if condition for the regularity of boundary points. And in this situation, what you do, you jump into this point, and then you have a small. And then you have a small ball and a large ball, and then you look at well, so this is the domain D, and then you look at this here, this is the small ball outside of D, and you look about the capacity of this in the large ball, in this large ball, and then you check the behavior for small radii. And what you can prove here, also for the non-linear case, P doesn't have to be. Non-linear case, P doesn't have to be true. You can prove an if and only if condition. And I think this is a very interesting result, so I wanted to show it to you. Now let me mention results of Marvin concerning a project that I have been involved in for many, many years. So I got stuck with non-symmetric coefficients. So again, we're looking at non-local operators in divergence form, and this theory, of course. And this theory, of course, has been developed very well. But I was never able to do it for non-symmetric coefficients. And I would love to explain where exactly my problem was. It was to show that log U is a BMO. And so this is how this all started. And it has led to several articles. And let me quickly jump through it. So the question is, you write down an operator, again in divergence form. Now we have capital K inside. We had capital A, we had little K, we have capital K. We will later have J, so we just say flat. They have J, so we just say flexible things. So now we don't assume symmetry of KXY, and we want to see how far we can get and under which assumptions we can prove results. And of course, what we want, we want to run the De Georgian National Regularity Theory, and also ideally, we want to prove the heat-kermal balance. So if you write down the quadratic form, or here's the bilinear form, and you split K into its symmetric part, K into its symmetric part and to its anti-symmetric part, you see that this you can write this bilinear form into two expressions. The first expression you like very much, and you have used many algebra lemmas, of course, for this expression. And this is then a little bit different with a plus here, and this is our troublemaker. So, in short, the result is that if you impose a sector-type condition, then everything is possible. So, that's like the meta theorem, I would say. Theorem, I would say. So, what is this sectotype condition? I don't want to give it in terms of fonts, but in terms of these kernels. So, this is the anti-symmetric part, and this is the symmetric part. So, we assume this integrability in this first variable for theta in this range. And what you will see, you will see a perfect analogy of Trudinger's results from the late 60s for non-symmetric operators. So, we thought. Operators. So we thought it's I decided not to give you all the assumptions, but to give one example, which is a little bit cute. So you might want to look. It comes later. So we take a function here, which is a Lipschitz function. So that's a very innocent function. And we define Ks to be our standard kernel and Ka to be the difference. It's like gradient of V, so it's like a ground standard. Of V, so it's like a ground state transformation, what I'm doing here. This is like a discrete gradient. And then you can show that this condition is satisfied if V has a certain holiday regularity. And then what is interesting, now this has not changed, but this has changed, then your operator is like a fractional Laplacian, if you just look at constant coefficients, plus a lower-order term, which, if you use the Carrié Duchamp, is of this type. Of this type. And when alpha goes to 2, which is S going to 1, then this operator converges, or solutions would converge to this operator. So, and all these lower order terms can be treated. So, I guess you want to see the results. So, the results are as follows. Under usual bounds on Kxy, I wrote here point-wise bounds. You do not need point-wise lower bounds. You have coercivity in a much more general framework. But under usual assumptions, we can also allow for. With assumptions, we can also allow for t-dependence. You can prove the following results. You can prove a weak parabolic Hanack inequality together with Hölder regularity estimates. This was part of one paper which is under review. You can also prove parabolic Hanack inequalities. This was part of a second paper. And up to now, in some regime related time and space, you can prove the heat kernel estimates. You can find this in the PhD thesis of Mars. Find this in the PhD thesis of Marvin, and hopefully, this will become an independent project of his soon. And what I would like to stress is this was an interesting development because for the heat kernel bounds, we ran into the problem that there's no PDE approach to heat kernel bounds for non-local operators. I mean, there are many works, of course, by Gen Goya and Hu, but somehow we wanted the Arwonson bounds. We wanted really the classical theory of Arwonson to work with non-local operators. And this turned out to be challenging. This turned out to be challenging. If you like this problem, then you can go to this article, another article by us, where we were able to run the Argonson idea on non-local operators. So this was challenging because, of course, it's related to Liao, and no one knows what is Liao exactly for fractional Laplacian. So, but you need a sort of Liao, and then it works. So, we were able to establish Apple. Worked so we were able to establish upper estimates also in metric measure spaces. And so we caught up with the people from stochastic analysis, which made us a little bit proud. Yeah, so by now you might have read a comment. We are of course not the first one, and there's a certain overlap with Sigurd and Louis's work related on Boltzmann equation where non-symmetric operators are also propagand. Alright, so All right. So yes, the last from Bielefeld is Solbeck. And there's not yet a paper out, but I want to maybe increase a little bit the pressure on us in Bielefeld by telling you what we are doing. So this is a problem that I like very much. So we had this problem long ago, but there was more energy put into this problem by the paper of Louis and Cyril, and it's about And zero, and it's about the following. Assume you have a family of measures. So at every point in space, you have a measure. And as a family, it's a symmetric family. So you see here AB becomes BA. And then there's a conjecture. The conjecture is if at every point the measure is a non-degenerate two-stable Levy measure, then you have coercivity in this way. I chose to write V and W because in the kinetic world basically your operator acts on the Basically, your operator acts on the velocity and not on the state variable. So, this is a conjecture which I like very much, and there have been contributions. I have a paper related to this with Martik Dider, and the best, so to say, result in terms of why it lacks assumptions is by Jamil Chaka and Louis Silvestre. Although, also their paper, even when you stick to Stay to stick to the regular case, that means the measures have densities. It's not optimal, but no one knows how to do it better. So, this is a very involved technique, which I like very much. But we can do it a little bit better now. We can go to singular measures. We can discuss and treat rotating singular measures. So, let me tell you this is one second. Assume, I do it with a jump process, it's easier for me. Assume you are in space and you can jump along a coordinate. And you can jump along a coordinate axis. And this is like: I call this a frame. And assume at every point in space, this frame is arbitrarily rotated. So that means here you can jump up, down, left, right, forward, backward. And here you can do it too, but the frame is rotated. And you have no information how the rotation goes. Some measurability will be involved. And the question is: can you jump through space? Can you fill out the space? And can you prove coercivity? And yes, I think we can really claim that we can do this. Think we can really claim that we can do this. There's parts missing in the analysis parts, but there's some algebra involved that actually is very nice. And if you think about how to fill space with these rotating frames, it gets very interesting. In two dimensions, there's no question, right? In two dimensions, you easily can jump from one point to another by just one hit. That's true. In three dimensions, it's already complicated. All right, so I wanted to talk about. So, I wanted to talk about non-local binary problems. Here we go. I do this. So, maybe I should start differently. So, I guess many of you know the paper by Enrico Valdinocchi, Serena di Piero, and maybe Jammer Rossotton was a member, was also an author, where they introduced and studied some non- And study some non-local problem with some non-local Neumann boundary conditions. And then there is a half a page of the probabilistic interpretation. If you think about the three authors, it's of course interesting to read their probabilistic interpretation of what's going on. And it's like complicated. And I wanted to understand that. And I get out of post-real. And if you don't know it, well. And then if you don't notice it, will it just stay in the background maybe? And now go back to the full screen view. Top menu, right? Top view. Yep. Go down near the bottom, full screen. Near the bottom, full screen. Like this? Nope. Nope. Go back. Full screen. I guess it's the last one. Yeah, I see. Okay. So I wanted to study this problem and I wanted to understand what this stochastic process is really doing. Process really doing. I'm not the first one that was in the French school. There were some papers on different boundary conditions, also, some sticky boundary conditions. I can give you the reference. So, for us, like PDE people, here's the problem. So, you want to solve a parabolic equation in a bounded domain. Equation in a bounded domain, and you have some initial conditions, you naught. And in the complement of the domain, you prescribe some Neumann boundary conditions. Now, what is this N? This N is a non-local Neumann operator, which is written here. Let me explain that to you. Because for people who have not seen this, this is maybe a problem. So let's use this picture. It's perfect. So this is D, and you know what the normal derivative is. If this is Lipschitz by Hadema's theorem, you Theorem, you know that almost everywhere you have this vector, and then you can differentiate, you know what the normal derivative is. But now, in the complement, I define a non-local operator, and this looks exactly like a fractional Laplacian, but you integrate only over d. So the differences close by don't show up. You integrate over all points x here, the difference uy minus ux in this direction. Now you want this to converge to a This is to converge to a classical normal derivative. Okay, so wait, can you just go away from that so fast? And now we call it JYZ. So we had already four. Yeah, no, then I guess I was looking for something in the slide that corresponded to the words you used. We only count the differences in that direction. I mean, direction, I would be careful. I would be careful. The direction, this is u of y, this is u of this is z. So you write u of y minus u of z and you weigh it with whatever. I understand that. So if it is exactly as you say there, then yeah, good. Okay. I was just confused by what you said. So in order to study this problem, we have to set up function spaces. And I want to show you something which I find incredible. There's an interesting search for a function space that I would never And I would never have thought that in the year 2022 we could find new functional spaces. I thought they were found in the Soviet Union or in Jena and they're hidden in Kleebel's books, of course, and so on. I mean, this is, I mean, I always like to make the claim it's in Kleebel's book because you cannot go be one of us. But I consulted with our colleagues in Vienna and the students, the former students of Tiebel. Performance groups of people. No, this is not known. This is exciting. So let me, and it has something to do with the probabilistic interpretation of the normal problem. So that's why I get so excited about it. Okay, let's set up quickly function spaces. So we start with what is the density of a Leby measure. So this is just this classic integrability condition. Think about the density of your fractional function. So we have a bounded open set, and then we define what I call the V-space. It's a function space on functions defined on all of our Functions defined on all of Rd and in the domain D we know they're L2 and we know the same I know is finite. Now this is an integral dRd. That means here you do have all combinations of xy but not x and y both outside. So there's no regularity assumed here. There's regularity, the classical Soberlev-Slobodecksky regularity inside. Plus there's something going on across the boundary which is interesting and will be important. Is interesting and will be important. But here it's just weighted L2. Okay, so this is a nice function space which you can study. It relates to an energy. Here's the energy. So this is a bit crazy notation. So again, you take all pairs, but not x, y, both from the contraband. Okay, so this is now the energy where we fix our. Where we fix our measure. So this is like constant coefficients, right? This is a translation invariant object. The measure is like an order of differentiability, but it's a measure, so it's more flexible. You cannot parametrize it by numbers, but it's a measure. Okay, now we can relate this to our Slobodetsky spaces. These are all there are two spaces where this is finite. I think by now this is pretty clear, and you can study these spaces, whether there are several. You can study these spaces whether there are several Hilbert spaces and so on, and whether you need some mild extra condition on this measure. New, I mean, so far it was just a Levy measure. So there are many works. I like, there's a recent reference of myself together with De Fogemm von Resten, and I think we summarize very nicely the literature. Okay, now let me just we can skip this, but it's fine. Okay, so there's a huge group. It's fun, okay? So there's a huge group of people in the US doing period dynamics. This is like a lot of genome mechanics using differences or integral operators. And they always have trouble when they see my function space and the results that there's Rd. So because for them, they have some equation here, and then they have what they call the interaction domain, is where somehow the outer world influences the inner world. But of course, you don't have to use omega and Rd. You can replace Rd. Rd, you can replace Rd by the union of omega and your interaction set, and then everything applied. Okay, so now these V spaces are interesting spaces and of course you have to study all the necessary properties and like density of smooth functions and so on. And this, I want to highlight this point number four. Point number four, so I think we have understood the V space, and now you ask yourself whether functions defined in R D, smooth, or compact, but in R D, not in D, but in R D, whether they are dense. Yes, that is true, and this defines a stochastic process, because then you have a regular Dirichlet form. So this result about density of functions tells you that the energy together with the functions such that the energy is finite. Such that the energy is finite, gives you a regular Dirichlet form, and then by this whole theory of Dirac deforms, there is a Stohaski process, a strong Markov process, it creates a Hamp process. And that is an interesting process. That's a process that jumps in the domain D as it wants, like this alpha-stable guy, and it can jump out. But when it is out, it has to come back because there's no outside jumps. Okay? So we have that process in our hands. It's an abstract definition, but it's there. When it jumps outside and it comes back here, When it jumps outside and it comes back in, it comes back in exactly where it was? No. Where does it? Oh, okay, according to the jump. Okay, the jump kernel. It's another jump kernel, it's a new game. You jump, I think. It's like a... Now I can spoil basically the whole probabilistic interpretation. And now, if you consider the trace process of this process, that's your guy. And this is currently leading to a lot of discussions in the probability world because they have. Discussions in the probability world because they have a notion of an active reflected Dirichlet space. So there is a notion of reflection, but what is happening here is a different type of reflection. So whether that's why I don't dare to call it reflected jump process, because this is somehow could be misunderstood. But the wording is maybe not so important. All right. Oh, yeah. So I wanted to send two. Wait, can you just go back one second to the last? One second to the last. I just didn't give compact. Okay, thanks. So I wanted to share two good messages concerning the non-local Neumann derivative. Now, when you teach PDE, what do you do? You minimize the H1 norm on the ball, and out of the blue, you get Neumann zero. So the Neumann conditions are natural boundary conditions when you minimize H1 energy. So it's like a present from somewhere that. Like a present from somewhere, that the minimizer has Neumann zero. And this is true for non-local operators too. There's no difference. So that's my feel-good message number one. So if you minimize this functional, this was this energy where all pairs are considered but not the pairs complement complement. When you minimize this energy in the V space, that is your the replacement of H one. The replacement of H1. That's really how we in Biedelfeld work with the space. It's not, I know it's like global or anything, but it's something like H1. Then you get quite easily, you get that the normal derivative is zero in the complement. Proof is exactly how it is for local operators. No difference. Two steps. Wait, but we still don't actually know what the normal derivative is, right? I defined it. And you. The function of the complement. But he's not like you don't know what the act. Like, you don't know what the act. Normal, normal term is. No, no, no, no, wait. Sorry, you said it's just a J. But the J is here also, the J is everywhere. That's the data. Let's fix the measure new. It's like you could... Let's fix the measure nu. Is it defined? The j? The n. On the first slide, right? Yeah, exactly. Yeah. So maybe. So if you give me, right? So if I give you... Okay. You okay? Never mind. Never mind. Never mind. I did not realize it was J and full. Yeah, yeah. I think you realize it was J and J. I didn't read carefully. It's fine now. You thought you had a K and a J. You just had a J and B. Do we go ahead and k somewhere? No, no, no, no. You're always like, A, little K, big K. Now I just, yeah, I get it. We're good. Thank you. I deserve this. Okay, so this is the field-good message one, and the field-good message two is also cute. So, we can prove the classical divergence theorem in bounded domains just deriving it from the Fubini theorem with the help of mod over S. And that's like a very cute thing. So, I show you how to do it, yeah. Most important sentence in case of political. Most important sentence in case of problems ask for she likes talking about it. So, how you do it? So, you give me a vector field F, C1, then I cook up. No, first of all, we decide how we want to approximate. This is not an approximation of the identity, but something very similar. You give me F, and then out of this F, I build, I call it a non-local collective unit, but in fact, what I'm building is just a function of two. Building is just a function of two variables which is anti-symmetric. So I call every function in R2D which is anti-symmetric a non-local vector. That's bad naming, but people like it. So and then when you have this, then you can show this convergence. So this is your normal derivative, but now the new epsilon depends on epsilon, the f epsilon depends on epsilon. So and what you will see, everything shrinks. And what you will see, everything shrinks together so that this complement integral converges to this integral over the surface, and then you get your divergence here. So, and I will really teach it this way. I like this proof. I like it very much. I haven't seen a better proof in bounded digital max. If you know one, I will be. Could you say a little more about what what how is FF F F's not defined? I knew you would ask. But this anyway, give me such a suggestive name. Give me such a suggestive name. I'm worried about this here. He should write on this stuff. Okay, I didn't prepare. That's maybe bad. So now you don't see any epsilon, right? This is artificial. Now the epsilon, please don't. Now, the epsilon, please don't ask me how I choose it. That's related to my new epsilon. So, but this will give an anti-symmetric function how I think about it. If you give me a vector field, and then I take a point x and a point y, and I just go along a straight line from x and y and just measure not the angle, but I measure the scalar product that goes the vector between the vector field and this vector xy. And I just uh average this out. And I just average this out. This gives me a non-global vector. Okay, now this is the exciting question. Now after these two field loop messages, you like the V space. But then the question is, what is the trace space of the V space? The trace space lives on the complement of the domain. Like the trace space of H1 lives on the boundary. So here the trace space has to live on the complement. Live on the computer. So let's not treat this for general new because it's already pretty complicated in the simple example that we know from the fractional P-dapless. We can choose P equals 2 if we want. And this 1 minus S is just there to prove robust results. Now, let me recall what is the definition of the V space. So VSP, so I think now the domain D from here on is called omega. Omega. Okay? So the VSP space is the space of all functions in L D such that the same enormous finite, and again I have this integral here where I take all pairs, but not the pairs complement complement. Okay, and now the classical result that we want to copy is the result from W1P to go to this W1 minus 1 over PP, and you know that this trace of And you know that this trace operator is linear and continuous, and that you have an inverse which is then the suburb of extensions. So, we have tried this in the last years for these V-spaces, and there were two groups. So, they were so interested in our result, they all visited Biederfeld. They came and wanted to see how we do it, and then they developed another technique, but none of these two approaches is perfect. Because the paper with Wartik Dieder gives an answer to what is a trace space, but it's not. To what is a trace space, but it's not robust. When s goes to 1, I don't capture my half. I want very much to capture, to get h a half on the boundaries. So their result, in fact, is robust, but no one can tell you when if you give me a function defined on the complement, I cannot even decide whether it's in the trace space yes or no, because it's an intrinsic definition. So, and of course, we wanted something extra, still explicit. So, but I've learned. So, but I've learned from them the Douglas identity from the beginning of the 20th century. So, it's an important contribution, but it didn't answer the question either. So, and now Florian and Tom from Bielefeld, they really understood how to do it. And I will show you the result, but I will not show you their result. I will show you now an upcoming result that I'm working on together with Florian, which generalizes the idea. So, I'll show it to you for general boundary Lipschitz. I'll show it to you for general boundary Lipschitz domains and the non-linear version of it. So the answer is here. If you look for the same I norm. So presumably you just look for the same enormous norm. So if you choose here omega complement, omega complement, then you see the same norm of the functions that you want to work with. So I'm happy to discuss this expression later, more in detail. But what I want to show you is, so here Show you is so here these measures they do something very similar, simple. They in fact work in a one-dimensional setting. So this is the discs to the boundary. You sit in the complement and you measure the disc to the power minus s and you integrate with one minus s. So from one-dimensional integration, and in the end there's a lot of one-dimensional going on, you know that this helps you to transform an integral over a line. Integral over a line to an evaluation in a point. And this is exactly what's happening here. And this just takes care of long-range jumps. It's not so important. This is much more important. So this is the semi-norm, and this semi-norm really has the following properties. So if you define, or this is, this is, I'm sorry, this must be the V space. So Florian likes sometimes W more than V, and I do some always change it. But he agreed. He sent me an email and said it's perfectly fine. So, yes, the trace map is continuous and linear, and there is also a continuous right inverse, which is the extension operator. And what is nice, these operators are continuous with regards to S. That means when S is moving, so the operator norm is independent of S when S goes to one. Okay, so in order to complete, and I think I have to stop soon, I want to show you now what. To stop soon, I want to show you now what's 10 minutes. Plus steps longer. Okay. So, yeah, I want to finish with a probabilistic interpretation of this, of the non-local Dharma problem. So, yes, so as I have said before, we are very much interested in robustness results, so we always try to get the limit. From the limit s equal to 1, what we know. So, of course, then we want that the trace operator converges to the classical trace operator. And this is true. Okay, now I want to answer now the question about this jump process. So I have shown to you the parabolic Cauchy problem. Problem and I want to have a probabilistic interpretation. So ideally, the solution U is equal to something where I see a stochastic process and I take expectations and so on. And the process that we need is the one that I have described is the trace process of the process that belongs to the V space. The process from the V space, let me repeat, jumps around here, when it exits, it jumps back. Exits, it jumps back. When you consider the trace process of this process, then you redirect those jumps. So a jump of this type is illegal and will be replaced by a jump of this type. You know that the Cauchy process is a trace space of Brownian motion. That's why Kirichlitz-Neumann map works. And so I think you all have a good understanding what the trace process really is. Trace process really is. Now, this trace process is very similar to the original jump process when your jumps happen far away from the boundary, because it's unlikely to make this excursion. But here, of course, there's something going on. There's an extra term from this excursion coming. And this term, although they were not interested in probability at all, appears also in the paper for my. From my host Octon and two Italian colleagues, and I'm sorry I forgot their names. They treated the homogeneous Neumann problem by transforming the homogeneous Neumann problem into a problem for the so-called centered or regional fractional abduction, and there this term appears. I actually didn't know, but that's exactly the same term, and this is not a surprise. So what is the result here? What is the result here? So now you have seen this already. So we want to do this for the case of bounded measurable coefficients because then our jump processes are rather general. So again, we start with our jumping kernel, which is bounded between the two kernels that you know from the fractional Parson. We set up this form, and we know the function spaces, and we know from our density result that there is a Our density result that there is a strong market process related to this theoretical form. Okay, maybe we I mean now you have to be careful when considering the trace process, but I think this picture says much more than the restrictions which are worked out here. So now there's something missing that happens on the last On the last slides. So now we have defined a process which lives only here in D. But our problem is non-local, so somehow we have to, we want to define UTX, right? UTX. And needs to be defined everywhere. So you have to somehow transport everything from inside to outside. And this is for in particular if the right inside is inhomogeneous, remember? Side is inhomogeneous, remember? So you want nu equal g. So this intensity which lives here, let's call it g, which tells the process something about its jump back. I mean, this must be somehow reflected in the solution. And that is a bit complicated. Now we go to this direction. This is nothing but the weak solution. Nothing but the weak solution of our parabolic problem. You will immediately understand this when everybody who works on parabolic PDE. So you just test the equation with phi, here you have used partial integrations, and the inhomogeneity of the Neumann type comes here as a term on the right-hand side, like F. And for very long, we were struggling for. We were struggling for which we can actually do this. So, and there, the measure that you have seen before, this very fancy measure mu from this extension result, pops up again because this space is then the correct space from where you chose the Neumann data from. And that actually took us very long to understand this, and all even. This and all even this minus one is actually correct there, so it's strange that you invert this because the space at infinity rules out several cases. But okay, now we need a mechanism to go back and forth. Kappa, let me say what kappa is. Kappa is just an integral over JXZ. So when you x sits outside, and when you integrate this expression, Integrate this expression over all z inside, then this is kappa. It's just a normalizing quantity. We define the mechanism to extend functions and to restrict functions where J enters these operations and then don't hold it. Sorry. Hold it. Sorry, don't hold down on the button, just click through. Thank you. Okay, this is the result of me clicking. I understand that. So I think you might have skipped something by. Did you try using the computer? It froze on one of the earlier speakers. You should probably use the arrow. Hold on, I remember. Alright. Yeah, thank you. Now it's moving. I was actually up here. I was doing that. So it was on one slide, but I want to say this is a project which will hopefully finish soon. It's totally on me, Subin is just waiting for me. So this is the result. So this is the result. So if you have this parabolic Cauchy problem, inhomogeneous in the Neumann data, where the Neumann data are L2 in time and in this space here, and your initial conditions are in L2, then here you have a probabilistic interpretation of your solution. Now, it is easy to read this for homogeneous. Read this for homogeneous Norman problem. This is much better because g is zero then. And then, now just forget about this. If you forget about this, then I think you are very much okay with this expression, what is written here, which you know from Feynman Gatz, for instance. So working in a G and leveling it out again is is what these guys are doing. What these guys are doing, and then you get the classical a priori estimate, which is L infinity L2 plus L2 L just so there's nothing fancy, so we are not claiming anything but the closeness and the probabilistic implementation. Yes, I had one backup slide, but I think we should just stop here. Thank you very much. I have a wish. Are there any more questions for it? Sorry, Mr. I have too much time. Actually, so, Freddy, you were talking about this process where you jump on a lattice and then you rotate the frame. You said there's a difference between two dimensions and three dimensions? Is there. Is there. I think I don't understand. Okay, super easy. Assume we are in R2 and you know that you have like such axis lying around. At every point there's such a thing. And I'm just asking you, if Nestor gives us two points, with how many jumps can you connect the two points? And you just need one intermediate jump, because here is a certain frame, here is a frame. Frame, here is a frame, they will have an intersection at some point, so we just jump to that point and there. And this result is actually even better because the length of our jump is linear dependent on the length of x, y. That will be important later. So we need to control the number of jumps and we need to control the length of the jumps because there could be configurations in space which take you somewhere very far. Somewhere very far, even when X and Y are not so far. And then our analysis results will not work. So, where we are right now, so we can agree or disagree, somehow these mechanisms in R D we have understood. So, the result goes like this, where we have a full proof of. Let's play the same game in R3. At every point in R3, you have a frame that's sitting there. There. And I make one assumption. I just want that one type appears in an open set. So you can do whatever you want to do, but let's call a rotation a type. I want that one of all these infinitely many types appears in an open set. Then the mechanism works. The mechanism works. And this mechanism that we had just discussed about running around. And foolproof is not so complicated, but somehow I think it took us quite a while to understand it because when you do this in R3, I mean, you know, aligns don't have to, I mean, OCLET tells you that there's a lot of possible configurations where you do not find each other. And I think we agreed that I will not talk about this today, right? That I would not talk about this today, right? I think I messed it up. You have a second question? Yes. Or is that a fake one? I actually do have a second question. Can you say something about the... You said this thing about getting the balance on the fundamental solution for the local molecular problem. Could you say like a few words on how you get the balance? Yeah. What the proof goes on? It's a pity that I d don't have uh I have them on the computer. I have them on the computer, basically. I would use exactly those slides. So you prove. So, I mean, you have to find the core idea of Aronson. And the core idea of Aronson is a weighted L2 estimate. And finding the weight is the main problem. And what I found surprising, what we do, we first We first cut the jumps, that means we have, let's say, a threshold row, and we do everything with a fixed threshold row. And then you know that the long jumps are just a perturbation of lower order, and you want to work them in later. But we get a wrong exponent in the approximative thing, and so I thought this is dead. So once we have a wrong exponent there, and it turns out that this. There and it turns out that this perturbation technique is so strong that it can cope with any exponent. So, I think Marvin would explain it better than me, but somehow I thought we are lost when I saw that wrong exponent. But somehow, unfortunately for us, the perturbation technique that means working in this long-range jumps, which are of lower order, is miraculously, can cope with any. Miraculously, you can cope with any exponent that you get in the approximative result. And we don't learn anything about Liao. I was hoping always that we learned something about Liao for fractional operators. And there is some sort of Liao result there, but I think we don't learn about Liao from the R. Marvin, you might want to add to this. The problem is that we have. Yeah, the problem is that we have to truncate, right? That there's this number rho, so you don't see any jumps that are longer than rho. And that the fundamental solution to the classical fractional heat equation satisfies the Lao inequality. Only when we cut off the long jumps we could prove something. Are there any other questions? Esther? Thank you, Russell. I have two questions. I have two questions. First question: Do you notice any difference if you, for example, look at kernels that have a rate of order less than one or bigger than one? Oh, okay. Now we are talking about S less than half or larger half for normally. Right, right. Here? This this is rather a very postless plus probabilistic interpretation. It's irrelevant. Interpretation, it's irrelevant. I mean, what there is something, these excursions, they are much more dominant when S is close. But these are two effects. We have long jumps versus this. Right. Okay, I gotta be careful. So the correction term, unfortunately, I cannot pull out directly. But in the proofs, there is no let's do. No, let's do case one and case two. Okay, we don't. Okay, so that answers my question. Yes, thank you. And my second question is: so, is there for this Neumann problem, is there a property like the reflection property? Like, like say what Luis was talking about? I mean, what we wanted, of course, we wanted to have some score of what the equation. So, when you do reflecting Browning motion, and reflecting Browning motion in itself is not, I don't find it's a trivial object. So, Elton Shu was maybe the first. So Ailton Shu was maybe the first one to to any people have published about the reflecting bound emotion, but I think the PhD thesis of Ailton Shu was the first one that I think answers many questions. And when you see it, then you see a scorehot equation for it. And of course we wanted something like this. And we were on the wrong track for a very long time because we somehow were always trying to understand probabilistically what the probability is. To understand probabilistically what the process in terms of waiting times or what it does, that it's just that we just need the trace process of this process that comes in the V-space that we learned somehow rather late. What was your question? What do you mean? If you look at this in the half space, right, if they're solving the problem with still anointment, if there's some kind of trying to think how Google got, right, because this is not locked, right? Go, right, because things are local, right? There's like a kind of reflection property and a lot of some weird thing where you say, okay, I have my function, this is zero anointment, and I'm going to then make a copy by a reflection, and then that's also an equation. Okay, I think now I understand in which direction you think. We have a new community to move on that true non-local one. Right. I mean, it's weird, yeah, but I mean, okay. Of course, we have it. Now, let's say this is the half-space, yeah. Now, what is This is the half space. And what is the DND to Neumann map? So here you prescribe data. You solve the Poisson equation here. And then you evaluate the normal operator here. So you get a Dirichlet True Neumann. So functions living here are mapped onto functions living here. It's a very interesting object. It's a Dirichlet True Neumann, which when s goes to 1, is, so to say, goes exactly. So, to say, goes exactly to the classical one. So, in the paper with Guy Fogem that I had mentioned, we re-study this directly to Neumann map and some very first spectral properties of it. What one can do with this thing, I don't know. For example, yeah, right. Of course, I mean, that's what Turin and Ankhan are also. what Tuin and Ancana also somehow had in mind, but I'm not I didn't see it clearly. Of course I because in the if the TV term maps a pseudo into a pseudo piece into a pseudo piece regardless of the rest, then everything should vanish. So if your deviate thermal map maps some function that vanishes somewhere into something that vanishes in a new place, you should try to prove that everything is here. Yeah, okay, that's a good statement. But now what what Okay, that's a good statement, but now what to do? So still, I have to prove this property, right? Shuttle with that. But okay, but I play very interesting about an object is that maybe that's a good interesting or the question of which operators have an extension. But here there's a kind of like a like a easy relationship between the relationship between