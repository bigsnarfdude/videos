Again, to thank Kaulina for this meeting. She was really leading all this with fantastic enthusiasm and has done fantastic work. And we owe her many, many thanks. Thank you. Now, my talk essentially, in one way, is continuing Continuing what Marcus has started, but actually, I think it is useful first of all to understand what we are doing. So, what do I understand by a spectral method, which is perhaps very different? So, I reread a few days ago the foundational paper of Steve Olsen from nineteen seventy seven, where he defined spectrum neglects. And definitely what I'm doing here goes way beyond conceptually. Way beyond conceptually what he has done. The whole framework is different. And anyway, I'm interested here in spectral methods for time-dependent problems. Now, the problem, you open books on spectral methods, and invariably the books are about pure boundary value problems, and somewhere it is written briefly, like you say, in Nick Trafatten's book. Nick is a great guy, but Nick is a great guy, but it is written somewhere. And if there is also time, then we can always use something that could end of the reference to time-dependent problems. And what I claim is that time-dependence creates a completely different framework, and this framework was motivated by the linear Schrodinger equation, but actually is much, much wider. So, what do I mean by Schr√∂dinger? So, what do I mean by? So, suppose that you have for simplicity a linear PDE, a general linear PDE, and L is a well-posed linear differential operator, which creates a strongly continuous semi-group, and we have some initial and boundary conditions. A spectral method is simply an autonomous system in the underlying input space. That's it. Space. That's it. So, in other words, we take a sufficiently smooth autonomous system in L2, by sufficiently smooth, I mean that it has similar regularity or more than we need to describe the solution. And the system is in L2. I'll do everything in L2 for reasons that should be clear to anybody interested, say, in linear threshold. Let's say in linear Schrodinger, because we want to conserve mass, it is L2. So we have autonomality in the usual sense of the word, in the domain with respect to L2. A spectral method simply approximates the solution of the TDE as a linear combination of basis functions, a truncated linear combination of basis functions, where the coefficients are time to. Where the coefficients are time-dependent and they are determined using Galotian conditions. That's it. End of the story. This spectral band. And so we need to choose an autonomous system, and now we have a number of different requirements that we need to impose. The first requirement is non-negotiable, we want stability. Negotiable, we want stability in the sense of lux, otherwise the numerical solution will not converge to the exact solution. Or, to phrase it in terms that usually you will not find in textbooks, we want to replace a well-posed PDE by a well-posed system of ODs approximating the PDE. And stability is non-negotiable. We want mass conservation if the exact solution consensus. If the exact solution conserves mass, for example, Rihanna-Schrodinger, but not only Rihanna-Schrodinger, we want to conserve mass and maybe ideally some other features, but mass is the most important here by the numerical solution. On the other hand, if the exact solution dissipates, for example, we are solving the diffusion equation, we want the numerical solution to dissipate as well. So we wanted this basic So we want at this basic level to conserve some important structural features. We want fast convergence. So we want, once we're expanding our solution in the underlying basis, we want fast convergence. And of course, fast convergence translates into small systems, which are easier to solve by data simulation. And we want And we want low cost. So, in other words, when we are time-stepping, we want each time step to be very, very, and this essentially reduces the cost of linear multiple. So, if we have n degrees of freedom, we don't want big O of N cube operations per step. This would be crazy, especially for multivariate programs. We want Mac to spend much less per step. And also, this means that we want to evaluate fast an expansion in terms of our basis, autonomous basis of different functions. So, for example, if we can use FFT or something like this, wonderful. If we need to use Gaussian quadrature, this becomes expensive. Now, and And parts of it, I will simply remind you elements from Marcus's talk. So, we have an autonomous system, and suppose that all the functions there are C1, so we can talk about derivatives. Because it is an autonomous basis of a space, every derivative of a function there can be expressed in terms of a basis. So, this gives rise to a mapping from the vector of fun. The vector of the basis functions to the vector of their derivatives. And this linear map is represented by an infinite matrix, infinite-dimensional matrix, which is called the differentiation matrix. And once we have either zero Dirichlet or periodic or Cauchy boundary value problems, automatically the Solving the linear exponential of the differentiation matrix will be one in L2 norm because in that case the exact differentiation matrix is skewed symmetric. So in other words, if we can maintain skew symmetry with the spectral basis, we are automatically ensuring that the exponential of T. exponential of Td is of L to naugh one and the exponential of E T D squared, which corresponds to the diffusion equation, is dissipative. And this is almost an if and only if statement. Okay, so I will not go through T systems. I just want to remind you that once we have Cauchy boundary, Once we have Cauchy boundary conditions, we can define t-systems using the Fourier transform. We can also do it in finite intervals, but then this will not be a basis of the Hilbert space. And this is a very neat and nice theory, but all this was in Malcolm's stock. But I also want to remind you something else from Malcolm's Talk: namely that we have an alternative way of constructing these systems using the differential Lambda. Using the differential Lamtros algorithm, and the differential Lambda algorithm doesn't depend on being on the whole ring line. So now my problem is as follows. Suppose that we have part of the ring line, and the two standard parts one can consider, because everything else interesting can be mapped to this situation, are either interval minus one, one or interval zero, infinity. Now, the Fourier trial. The Fourier transform trick doesn't work in that case. However, the Lantros trick still works. So let's see what we can get with the Lantros trick. It will take me. Oh, okay. So suppose that we have zero boundary conditions at a point A. So it can be said interval minus. So it can be said interval minus one, one and plus one and minus one, or zero infinity at zero. And the L derivative, if the first derivative is d, the vector of first derivatives is the differentiation matrix times the vector of the values. Then the second derivative is d squared times the vector of values. The 27th derivative is d to 27. Derivative is d to 27 times the derivative. And now suppose that we have zero boundary conditions at A. This means that all the phi n's are zero at A, but this means that all the derivatives are also zero at them. Not only the first derivative, the second, all the derivatives. And for example, if, as usually will do, our functions are analytic functions, then the only analytic function that vanishes. Only analytic function that vanishes and only the derivatives vanish at the point is a zero function, which is not a terribly good choice of a function. So this doesn't work. How can we take care of it? There is one simple trick to take care of it. Don't work with analytic functions. So we still want our function to be analytic or at least say infinity in say minus one one, but let us put an assumption of singularity. An essential singularity at the endpoint. And so, for example, if we have integral minus 1, 1, we can take phi 0. Remember, all we need is to provide phi 0, and then we are using the Lantrus differential Lantrosch algorithm to manufacture the subsequent terms of the basis. So, let us choose phi zero, which is some normalizing constant times the exponential of minus one. The exponential of minus 1 over 1 minus x squared. So essential singularity at plus 1, at minus 1, analytic insight. And let us generate the first whatever functions using the differential natural algorithm. How do these functions look? So this is phi 0. And all of them, by the way, they belong to the same scale. That's important. Phi 0 is an. Phi 0 is a nice function. Try to featureless, but that's okay, it is only 5, 0. Phi 1, nice function. Phi 2, nice, there are two hills here, but we can leave it. 5, 3, these hills go. 5, 4, 5, 5, 5, 5, 5. I've done it up to 10, and essentially, what we have is. Essentially, what we have is a boundary layer at the end. The essential singularity spoils everything, and we obtain a system of functions which are formally orthogonal, but they cannot approximate anything useful. So, this is not the way of doing things. So, what to do, and this leads us from T systems to W systems. And the idea. And the idea behind W systems is, oops, why do I always pause the wrong button? The idea behind W systems is very, very simple. The idea itself is almost trivial simple. Suppose that I have an Where I have an interval a b, finite or infinite, doesn't matter. And I have a weight function in this interval. So, weight function is a function which is sufficiently smooth, integrable, non-negative, all its moments are bounded, and the zeroth moment is positive, excluding the zero function. And this weight function, you can use it to generate a system of autograph. A system of autogonal polynomials in the usual way. So we have an autonomous polynomial system denoted by pn and it is very simple. Define phi n as a square root of the weight function times the n's autonomous model. And this is a system which is orthogonal in L2. And mathematically, this completely trivial idea. However, analyzing this idea becomes far from trivial. So the first question, and this is very easy to prove, what are the conditions? So we generate again the differentiation matrix. What are the conditions for this matrix to be still symmetric? And the answer is we have the weight function. To have the weight function has to vanish at the endpoints. So if the interval is finite, it has to vanish at the two endpoints. If it is half infinite, say zero infinity, it has to vanish at zero. And this is an if and only if condition. And in that case, the differentiation matrix is skill symmetric. Now, the differentiation matrix, and this is important to understand, in general it will not be bounded. Generally, it will not be bundled, and to the very good piece of news that it will not be bundled. Because what caused the failure of T functions, T systems, in the example I gave you, is the fact that all the powers of the differentiation matrix are bounded matrices. Because the differentiation matrix was tri-diagonal, so all the powers are bounded. So, all the powers are bundled matrices, so they're finite. And however, what we want is actually the situation whereby a certain power of the differentiation matrix goes unbounded, because this stops this trick that caused this function to be dysfunctional, so to say. The functions have to be identically zero. So, we want actually a certain A certain power of D to blow up. And by the way, sort of as a footnote, you may ask: what are all the W systems, so functions like that, which are also T systems? And the answer is essentially a mean polynomial and a little bit more. So T systems are generated by a weight function, which is an exponential. Function which is an exponential of a polynomial and is itself intangible. So the polynomial has so-called Freud weights or Freud metrics. That's it. Otherwise, we are talking about completely different structures. So, for example, what about D square? What are the conditions for D square to exist? And essentially, And essentially, we can prove that this square is a bounded matrix if and only if the square of the weight, of the derivative of the weight function divided by the weight function, is itself a weight function. Remember, weight function, non-negative and all the moments bounded, and so on. And the proof is via the Christopher-Derbeaux formula and the Christopher Derby formula and reproducing kilometers. And in general, a good way of generating this sort of weights is take some weight w tilde and multiply it by say x minus a to power alpha, b minus x to power beta, so to cause it to be zero at the end points. And then if alpha and beta are greater than one, Greater than one, this square is bound. You may ask, what about D to five or d to six? How far can we go? All this can be determined. So essentially a necessary condition for the S power of the differentiation resistance to exist and the S plus first power perhaps to grow. S plus first power perhaps to blow up is that alpha and beta are greater than s minus 1. This alpha and beta here has to be greater than s minus 1. Okay. Now, we have two intervals of interest, and other intervals can be always mapped into these intervals, minus 1, 1 and 0, infinity. And in each of these intervals, there is one natural choice of weight functions and of the normal polynomials. And it turns out that these choices have absolutely amazing properties. These choices are These choices are it's zero infinity. The Laguerre function, the Laguerre weight, x to alpha e to minus x, where alpha in our case will have to be greater than zero for it to vanish at the origin, greater than one. And in this case, Pn, the n's function, is the generalized Laguerre polynomial, but I need to multiply it by coefficient. Coefficient so that it is autonomous. It is normal with respect to this weight. And in minus 1, 1, we are using ultraspherical weights. So we are choosing alpha. In principle, alpha has to be greater than minus 1, but in our case, it will be greater than 1. And take the weight function 1 minus x squared to power alpha in minus 1, 1. And in this case, our polynomials are. Our polynomials are altruistical polynomials, which are the Jacobi polynomials with the two coefficients equal, P and alpha alpha, times again this normalizing constant so that everything is of normal. And now, what are the features for that? And this becomes interesting. And it takes some 10, 15 pages of dense algebra to prove it, and the proof is And the proof is not terribly interesting, just laborious. So you'll have to believe me that I'm right. For the Laguerre family, an alpha positive, the elements of differentiation matrix have wonderful structure. So for m greater than n, otherwise simply flip it, flip m and n and add minus and for, dmn. DMN is minus one half times AM BN. Now AM and BN are just two numbers here. One goes, one is roughly speaking, one over m to alpha over two, and the other one roughly m to alpha over two. That's it. And turns out that this is, and I didn't expect this to happen when I started to play, this is the This is the most important feature. This leads to so-called semi-separable matrices, which make our linear algebra as easy as working with direct angle matrices. And the same for the ultraspherical family. So again, BMN for m greater than n is minus half a m b n. Otherwise, you flip it and remove the minus. Well, am and b n. Where AM and BN are of different form. And we'll see soon why this is so important. And there is a very long combinatorial proof that for Laguerre W functions and also for ultra-spherical w functions. W functions, b to power s is bounded if and only if alpha is greater than s minus 1. That's it. So, remember, we don't want all the powers to be bounded, but we want enough powers to be bounded to approximate our differential operator. So this can be different for different situations. For shredding here, we want to approximate the Laplacian, so S has to be equal to. So, s has to be equal to and so we have this sort of condition and this is not an easy proof and for both for Laguerre and for ultra spherical the condition is exactly the same. Alpha has to be greater than s minus one. And so have a look at how this functions say look for Laguerre value functions. Formula Geoda value functions alpha equal. So here we have alpha equal 1, alpha equal 2, alpha equal 4, s equals 1, s equal 2, s equals 3. And you can see that the growth, for example, the growth here is a nice growth, they are all nice functions, but when you are taking higher and higher powers, here the power Here, the power really blows up, it goes up to 400, here 150, here only 30, 75. So the right choice of S prevents things going infinitely too fast. And the same for ultra-spherical functions. So if you are in the Roman regime, it should. You are in the wrong regime, it shoots up very fast. But which alpha to choose? It turns out that there is always a sweet spot, the correct choice of alpha. Because on one hand, we want to choose alpha so that we can recreate enough derivatives correctly. So, this is problem between it. On the other hand, turns out that there is always one Always one type of choice which gives completely different. Well, let me perhaps show to you examples first. So take the function e to x sine x at zero infinity, approximate by the first 70 elements of a W system, a Laguerre W system, and in blue. And in blue, we have the results for it is the log to base term of errors, so the number of significant digits we are getting, approximating over the interval 0, 70, which my approximation to 0 infinity. And blue is alpha equal one, green is alpha equals three, alpha is red is alpha equal two, and see the difference. See the difference. Suddenly, choosing the right alpha gives us incredibly small arrow by orders of magnitude. Here the arrow is actually speaking 10 to minus 4, here it is 10 to minus 8. And the only difference is different choice of the product. And this becomes even more pronounced for altospherical functions. So here is the altospherical function function with the endpoints. At the endpoints, alpha equals one, alpha equals three. This is the arrow here. Alpha equals two suddenly gives us 23 significant digits compared to four significant digits. It doesn't cost more. It is simply a choice of different parameters. That's all. Now, why? And there are really two ways of explaining. Really, two ways of explaining this. So, one is that if you are taking alpha less than twice, so we have the number s, which is the number of derivatives in water approximately. If alpha is less than two times s minus one, then the s derivative blows up at about it. If alpha is greater than twice s minus one, then One, then we have too many zero-directed boundary conditions in the boundary. So we don't have L infinity convergence. We have L2 convergence, but not L infinity convergence. If you are choosing alpha equal twice S minus 1, this is the sweet spot. We have exactly the right amount of regularity to approximate everything correctly. Another way of looking at it. Way of looking at it is as follows. If we are in a compact interval, the theory of convergence of orthogonal systems to functions in presence of analyticity is well known for more than 100 years. In terms of the balanced ellipse, so we have an ellipse around the domain, and if everything is analytic in the ellipse, we have Ellipse, we have exponentially fast convergence, and the speed of the exponential convergence is essentially the ratio of the major and the minor axis of variance. But this depends on the analyticity of everything. In and in our setting, the phi n's need not be analytic at the end points, because we have one minus x to the power. 1 minus x to the power. And we are taking the square root of 1 minus x to the power, for example, or 1 minus x squared or x to alpha. We are taking the square root of that, clumping it into our functions, and this can destroy analyticity. But an elasticity is recovered when alpha is twice an integer. Because then when they are taking the square root, you have the function. you have an analytic function one minus x square or e to minus x x to alpha e to minus x the square root of that and the square root is itself an analytic function so we are maintaining analyticity and this maintenance of analyticity this is what is making this difference this is what leads to This is what leads to this amazing difference. By the way, if you are taking alpha equal to four, you have almost the same good. Slightly worse. What is important? Analyticity is what creates fast convergence. This is a very simple room. And if you choose an even integer alpha, you have fast convergence. Now, what about What about the algebra? And now, and this I didn't expect when I started it. So I didn't know what would be the differentiation matrix, and I didn't appreciate this form of differentiation matrix until I started to look at it more carefully, and also, to be honest, until the referees started to look at it. So we have several Several computational steps here. The first is we need to expand F in a basis. And this will be different in different situations. So to expand it in ultraspherical basis, there are fast methods. Essentially, you can expand it as fast as FFT, or even faster than FFT. There are unstable methods due to me and to other people, and there are stable methods due And there are stable methods due mostly to Alex Tauden and Srihan Orwell. And this works beautifully well. And essentially, it is even faster than F of D. So this is goodness. Next, we need to be able to form a product of differentiation matrices times vectors. Why? What is a vector f in our world? This is the vector of a This is the vector of expansion coefficients of our function. b times f is a vector of derivatives. d square f is a vector of second derivatives. And so we must be able to generate derivatives to generate our linear system. So we want this product to be done fast. So if we truncate everything to an n-by-n matrix, we don't want it to cost all the n square operations, we want it to cost all the n operations. And vergence. And solving linear systems generating for the matrix D. Again, we want to do it triple. Order n, order n square, but definitely not order n. And finally, we'll need to compute functions of the differentiation methods or of a polynomial generated with the differentiation matrix. So exponentials, or perhaps if you are solving Klein-Golf. Or perhaps if you are solving Klein Borden, sines and cosines and so on. And there are different ways of computing functions of matrices, or function of a matrix times a vector, which is always much triple. And probably the most versatile tool in this situation is the Danford formula from analytic for functional analysis. And using the Danford formula, And using the Danford formula, essentially I reducing everything to multiplication and FFT. So if you can use FFT and so on, to solve linear systems and solve fast linear systems, you can approximate very, very convenient, for example, the exponential, the transformation. So, I'll give you an example how to form D times F. And the whole idea is what saves us is semi-separability. Now, what is semi-separability? In general, semi-separability is the following thing. We have a matrix, finite or infinite, I don't care, we say that the matrix is semi-separable. Matrix is semi-separable of rank R if it has the following features. Take any sub-matrix above, wholly above the main diagonal or wholly below the main diagonal, no matter how loud, the rank of this matrix cannot exceed R. Then we say that the matrix is semicircular of rank R. Now, remember what we had earlier. Above the main diagonal, Main diagonal, the elements of the matrix are products of AMDH, and the same under the main diagonal only with the sign change. So everything for Laguerre is semi-separable of rank one. For ultraspherical, we have to separate odd and even coefficients, and then we obtain two matrices, which are also secondable for other one. So let us see how to multiply the line. How to multiply the lanc one, make it say for Laguerre. Here we have the Laguerre coefficients of the matrix. How to multiply it by a vector. And to this end, let me define Hm. So we want to form this product. We have some capital M and capital M. And typically N is much larger than M. So we want to evaluate. So we want to evaluate an an n plus one by m plus one matrix, but we want this product to be much longer than because the longer they are, the more precise they are. So we want to formulate this product for some very large capital N, for M between zero and capital N. And how to do it fast. So define sigma m sum from 0 to m minus 1 b n f n rho m sum from m plus 1 up to capital N of a n f n. And we start from h0. H0 is simply b0 rho star. And it is very easy to see that hm is minus am sigma m plus b m rho M rho. But both sigma and rho can be obtained by this fine simple recursion, each one using a single product. So all you need is to form this product and so on. And all this, all together, while playing a matrix by a vector, although the matrix is dense, costs only capital N plus 5N flops. So all this is very. So, all this is very, very fast. Slightly more complicated argument works for ultraspherical W functions. And solving linear systems and so on is very, very similar. People have already done this sort of stuff, or general sense of okay. So, I'll say a few words about the multi-government case because I have really finished. So T systems generalize by tens of products because we are working in the full space. W systems, W systems in parallel peoples can be generalized by tensor products, but that's not very interesting. In unit balls, and this will be next talk, this can be done by generalizing Zernike polynomials. Polynomials, W systems in simple access, generalizing code window polynomials in space. And very, very soon, this is really from a week ago, well from a week ago, instead of Galerkin conditions, it includes Galerkin-Petrov conditions, a la Tachyo Karguton stock, and things are more complicated by. Things are more complicated, but all the steering can be extended to this situation. We have two maintenance, and but we can survive. That's it. Yeah, questions? So, this was very interesting that now we covered all. Very interesting that now you covered all types of intervals. Can you please remind me again the meaning of this alpha parameter that you chose? It has to be above s minus 1. What is the alpha parameter? What is the alpha parameter? Alpha is built in into the wave function. So essentially, alpha is, say, for Laguerre, the wave function is x to alpha into minus x. Alpha spherical is one minus x squared to the power alpha. So essentially tells you how many zeros, fractional zeros, you have it there. Are there any other basis sets for a given differential operator of ICL guarantee? Guarantee this any separability because this is really extremely efficient. Do I have to go to those specific basis sets or does there exist a class of basis sets? Can I somehow beat my system and do a semi-separable differentiation? Okay, so let me tell you what I know, which is probably one minus epsilon times what you might use. So, semi-separability is quite rare. The one general situation in which semi-separable matrices occur quite widely is a theory of H matrices, theoretical matrices, the Bolkon Harbournages. But otherwise, it is quite relevant. Now, you cannot really bully a system into being sensitive. You have either to You have either to design it like this, or in my case, to be incredibly lucky. 