Okay, by these guys. In particular, I want to recognize the award by Jalen Lee, who was a PhD student at UCI. He has been coadivated recently by Brian Jess Brian, who is a student of Abak Shaba. The Department of Statistics at UCI. Most of the intuition behind our work and the results of our experiments is actually due to Aaron Berstein, who is in the Department of Cognitive Sciences at UCI. So today we're going to work and discuss fMRI data. I don't think there is any need of introduction for the data to this audience or with Scoop Forward. Going essentially Scope forward, going essentially to the essence of the talk. What we're going to discuss today is PPI psychophysiological interaction analysis. This analysis was first proposed by Frieston in 1997 to investigate task-specific changes in the association between activity in different regions of the brain. The idea is to use The idea is to use a seed region and essentially try to identify boxes of regions in which activity is more related to this seed region. So, think about, for example, as in region A. So, there is, for example, no task resting state. We may expect that the region A may be associated with some other region B out of spontaneous activity. Then, in the presence of a task, something oops. Oops. Sorry. Perfect. In the presence of a task, something happens, so there may be new connections with other regions, but there may also be a reinforcement of connections of associations with the pre existing regions. So the spontaneous fireworks are the model essentially tries to capture, so the effect on one hand. Captured so the effect on one hand of these physiological effects, the spontaneous correlation in the absence of a stimulus, and what they call psychological effect, essentially the effect due to a stimulus or to the existence of other covariates that you want to understand. Now, if we look at the model proposed by Frieston from a statistical perspective, it's quite easy to understand. We have the Bohr response for region R, the C region, at a particular time, and this is going to be associated. Time. And this is going to be associated with the other regions, the bulk response in other regions. This is what we call the physiological effect. And then there's going to be a psychological effect due to the stimulus. What we're also interested in, and this is typically one of the main goals of the inference, is the PPI effect, the physiological effect. This is the interaction between the stimulus and the ball response in other. Stimulus and the bore response in other regions. Yes, moderation, right? Right. Yeah. So, so, so, the first slide that you can change, but shouldn't it be like the G is actually pointing to the green arrow? Because that's the green arrow. It's changing, it's not changing A, right? Correct. Yes, this was just to give an idea of the effect of it. It's the green arrow that is impacted by D. Correct. Yeah, in CBS, it was just. Correct. Yeah. You see, this was just this was just to clarify the psychological definition of psychological and physiological effect, right? It's kind of in terminology which we are not used to. So that's why I was going with this simpler simplification. But good catch. I remember correctly. But that tells me that I need to be very careful in the future. But Martin, in the non-similar respect, we have D2A, but the simulators we use. So it it would be nothing uh so I guess I think it's a standard way of doing it is the effect of D is on the A B connection, right? Right, so exactly rather than on the A activation. This is what it looks like to me is the separate connection it's more like yeah this is with the with the A should be moderate moderating the ADBP yeah this is basically just to show that when there is a stimulus right there's gonna be When there is a stimulus, right, there's going to be a physiological effect due to the stimulus, right? That's going to add to what response activity we had before. Just fair, but good catch. So essentially, so we are interested in essentially conducting inference when on the physiological effect, and in particular, also on the interaction. Now, the let's see if we can move this one behind. How can I hide it? Okay, perfect. So, PPI tackles task-dependent functional connectivity. And essentially, here when the first it was defined, it was defined by Frieston to assess connectivity differences between two task conditions, basically a rest and the stimulus. Now, later on, McLaren have extended the virus framework to include more than two conditions. The virus framework to include more than two conditions. So, and again, this is just a simplificatory picture to show that we are going to need to consider also the effects with respect to other conditions. Now, the point of this talk is that we all know by now that there is understanding that brain activity is indeed not static. In all the cases that we've seen so far, in this type of models, the betas were static, right? The coefficient that were capturing the connectivity associated with. The connectivity association between two regions in a static wave over time. But by now, we have an understanding that brain connectivity is by nature, dynamic, and fluctuating. And so we may need to think about functional connectivity and connections that may not be stationary over time. Why this might be the case? Well, on one hand, there is the combined result of the stimulus, but also The stimulus, but also spontaneous fluctuations in neural activity over time. Some examples of these may be the following. For example, while I'm talking in front of you, my heart rate and other psychophysiological mechanisms are increasing quite substantially. So these may affect the way in which I respond to any of the slides that I see. But also, while I'm here talking about the slide, I'm also thinking about the next slide. I'm also thinking about the next slide, right? And so there is also some idea that there may be just dynamic brain activity and dynamic association because we are learning and we are basically trying to predict what's going to happen next. And indeed, at this point, I have no idea what the next slide is. So let's go and see it. So here, I'm trying to essentially describe the model that tries to describe the association. That tries to describe the association between a seed region and other regions in a dynamic way. Let's consider some terms. Now I'm going to collect, I've been considering more, before I was just considering one region, right, R, and C region and another region. Now I'm going to consider all the regions together. So we can collect the responses with the regions in a vector Z. We can consider a convolution of a stimulus K. A convolution of a stimulus K with a modernization function, and I'm going to consider the interaction moderation terms, essentially in a vector xk t for each region where I basically convolve the stimulus with the neural signal. In the common general PPI model posited by McLaren, what we have is essentially a What we have is essentially an expansion of the term that we've seen so far. So we can recognize again the physiological effect, we can recognize the psychological effects of the stimulus, and we can also recognize the PPI effect between each region D and the stimuli. What we're going to propose is a simple modification, at least in theory, of this model. And in particular, what we're going to do is just to think at the time-varying physiological effect. At a time-varying physiological effect. What does this correspond to? Well, the idea is essentially the following. Of course, I go back to the general DPPI model if I just take a beta t constant over time, right? The basic idea here is the following. Let's consider two simulated time series, and of course, we have a stimulus effect. So, on one hand, when the stimulus happens, when there is Stimulus happens when there is a stimulus, at that time there is the effect, the PPI effect, and may bring in the more increasing of the bulk signals between the two time CI. What we are trying to model is indeed the changing of these effects of the overall effect on one region over the seed over time. And this is captured essentially by allowing Essentially, by allowing the beta t coefficient to have smooth changes over time. So, essentially, we are basically assuming, for example, in this case, that when the stimulus is active, we may have some sort of decrease of the association between the two regions in a smooth way. So, in order to do this properly, we need first to have some priors. To have some priors on the beta coefficients over time that allow to describe these changes over time. So to do that, we rely on well-known tools in Bayesian analysis. And in particular, we are going to look first at the linkage prior proposed by BIT2INFLUGNATO recently, which is double gamma prior like this in 2019. 2019, but basically posit that the behavior, the dynamics of the beta coefficients in our case, are modeled through essentially in L1 model with the air coefficient equal to one. The attention though here is the attention is on the doubt on the error, right? And the error is And the error is modeled through what may be described as a scale mixture of normal. The idea is: we're going to look at the variance of the error terms. When the variance of the error term is small, we expect essentially the process to be essentially very much close to constant. If we allow the variance to be large, then we are allowing deviations from the curve. Are allowing deviations from the current values and so the possibility to move and to have smooth changes over time. Um, so this is one of the possible characterizations by which we could have gone through these dynamics of the betas. It's actually a very simple one, especially if you want to do this computationally and have a computational speed. We're also interested, of course, in sparsity and trying to understand which regions are more associated to others. Are more associated to others. So we also have linkage priors for the static effects, which are both the main effect of the stimulus and the PPI effect in our case. And for these, we just use, again, we rely on well-known tools in the Bayesian framework. In particular, use the Oshu Schlinkage prior. And again, the idea here is to look at the variances, right? Whenever the variance of each of these coefficients, PPI. Of each of these coefficients, PPI effects or main effect, is small, we expect essentially that the effect will be small, so we can probably avoid considering it in the model. Whereas if the variance is large, we allow the coefficient to be possibly large, and then we can likely include them into the model. So, this again is a type of model that has the advantage of being very quick also to run. Quick also to run on a computer with R. Are there any questions so far? So, so far, I discussed with you the case in which we have a one sea region over all the other regions. But we would like to maybe run this model, we could think about this model, is that we can actually vary the sea region and so try to understand then the association between each. Each region without being, let's say, linked to the fact that we have the seed regions versus others. So, the possibility of this model and the reason why we are really looking to the computational speed in this case is that we can fit our versions of a model by allowing the seed regions to vary across the regions. And basically, we can describe the associations between the regions together. Here, in order to look at the association between Association between various regions, we look at a measure of the partial correlation. And a proxy of a partial correlation in this case is given essentially by the combined effect of the physiological coefficients and the PPI effects. Here I put in red what we need to be wary of, aware of. On one hand, the C region is R and then we have the different regions D. We have the different regions D, and of course, the idea here: we want to go for the different regions, assuming each one of them a different state region, so we will eventually permute these coefficients. Okay, so good. Now, what is the issue here? That let's look at a simple example. We have, for example, a model for seed region one, and we get an estimate of partial. An estimate of partial correlations, right? The proxy for partial correlation for this model, where we get the solution between region two and region one, region three and region one. We vary R and we get an estimate. We get this, we vary the seed region, right? And we get an estimate of partial correlation between seed region three and two and region one and two, and so forth, right? Well, Well, you know where I'm going to, right? We expect that essentially, in principle, we should get some sort of the same conclusions if we look at the different models. That's typically not the case, right, in practice. So we need a way to combine inference across models. And what can we do to do that? So to avoid or to take into account discordant decisions? Well, typically what it is done in this Is done in these cases when you run this type of models to use adoc conclusions, like you know, if they are both, you know, if you see both associations in both, okay, that's fine. Otherwise, no, right? This type of ad hoc conclusions, we decide to go in a different way this time and basically to put these in a decision-they framework. And in particular, we can put these into multiple comparison framework where we are faced with. Framework where we are faced with dependent hypotheses. So we have hypothesis for the case in which region R is the seed region and D is one of the other regions associated with it. And vice versa, we have the hypothesis which is associated on when D is the seed region and R is now one with predictors, right? So we are going to use this framework developed by China. Framework developed by Chandra and Bhattacharya in EJS in 2019, where we basically look at non-marginal decisions. And here, the idea, since the time is running fast, is simply to define a modified post-discovery rate by taking into account the dependence across hypotheses. And here it's a very simple case in which we have essentially four hypotheses or conclusions that can be dependent. The case in which we do not see any Which we do not see any association when seed region is R, and the case in which we don't see any association when seed region is D, and vice versa, the case in which we see associations. The difference here is that in a decision-theoretic framework, we have to consider also what we decide in the two cases. So, we have also, in addition to the usual compound loss. Compound those functions, we take into account the truth in our decisions. We also have to take into account what we decide in the dependent hypothesis. And so this essentially means that we will have to describe a number of false positive, to describe our loss function, and consider also different types of false positive cases. We take into account also the mismatch. Into account also these mismatched conclusions, and we get essentially eventually to simple expressions, at least in practice, but a more complicated loss function, but essentially depends on the decisions that are taken in the different cases. And in particular, after some computations, we can essentially boil down to the system that depends on the decision that depends on the posterior of the joint probability of deciding to reject the null hypothesis of null association in the two models in the two cases right um this seems uh like a very uh simple probability to compute it actually is not because we have to look at a john probability um we kind of used an approximation in our case of this model taken to into into put into account the fact that we are modeling and running the models independently. So I'm going to write this, it's not exactly correct, but we are essentially estimating this probability by considering the positive probability of having an association in the two models. And essentially, like the two models are actually independent and this probability is computed from independent models, right? We can discuss about that later on. About that later on, but so in order to provide inference in this case, we had to take some shortcut. But at the end of the day, however, we're going to use a modified force discovery rate, technically count these probabilities. And based on this post-discovery rate, we're going to obtain a threshold on these probabilities of the alternative. And blah, blah, blah, we're going to decide our associations as a consequence. I see that I have four minutes, so I'm going to go a bit fast in the next slide. Go a bit fast in the next slide because I want to go to the real case analysis, which I think is interesting. So we also need to estimate the partial correlation. So far, we just decided there is a zero or one in the associations, right, to the loss function. So we do that by simply using the usual Gaussian characteristics. So we have to from our decision-free framework, we essentially get the zero and ones in the precision matrix of Matrix of the model you're looking at. And this is exactly what we do. And eventually, we just estimate whenever we say that there is an association, we partial the value of the partial correlations essentially by averaging out the values that we see in the two models. And again, we can discuss about that. It's a shortcut, but it works and seems to be fine in practice. Simulation study. Simulation study, we show essentially that we do better when there is indeed changes over time of the connectivity in a model that is not the one that we are actually using with respect with the generalized TPI. One thing that I wanted to notice from this emotional study is that if we actually have a case in which an association is actually not time dependent, we'll actually be able to recover it. But that's it. And I'm going to move to the study, real case study. So, the reason why I like this study is that in this case, we're actually discussing with Aaron, and he was very interested in trying to learn predictive associative relationship between what an individual does or an experiment proposes, and essentially decision making and rewards. And rewards. So here we have a simple study, well, it's not really not a simple study, we have a study, which is actually quite convoluted, where we have essentially eight subjects lying in a scanner. They are presented with one of four images, and basically they are told essentially to push a button for each one of the images. Unknown to the individuals of the study, the way in which these images are shown is governed by Is governed by an underlying Markov process. So the idea here is that as the experiment goes forward, goes on, we expect, hopefully, that the individuals will learn the sequence of images over time. Unknown to the individuals in the studies, what happens is that sometimes Aaron was a bit evil and changed the sequence order of the Markov extraction probabilities underlying the study. So the idea here is that to learn. So, the idea here is that to learn essentially how the individuals are doing in terms of predictive learning, the sequence of images, and through the use of their reaction times. So, these reduction times may indicate have learned the distribution that is line the sequence showings. The relationship between time The relationship between time and the stimulus probabilities is typically captured by the rate. This basically measures how a system learns about new information relative to previous experience. And so we are interested essentially in how our activity is modulated by the predicted of a sequence of short buttons and Sure, but it's sh and we are creating essential main processes: two learning processes: a learning slow process, which believed that to be associated with campus, and the faster learning process, which is associated with the CAU data. We are also interested in what happens in the association with anterior corpus, see, that is essentially considered as a region, again. A region again. I'm sorry, you can see better. Yeah, okay. I'll tell Haran. This is what you wrote. Trusted him. Thank you. This is why I'm here. So anyway, the Anti-Republican Court is what we understood as a mediator, if you want, of the two regions. So the idea. So, the idea here is that these are the action times of different trials, and we can see that sometimes there are some, let's say, higher numbers, or these numbers in any way connected with the predictability of the process itself. To do that, we need to construct essentially some predictor of the predictability. And this is done, I have 17 seconds, so I'm going to go faster. Basically, by using known tools that Andrew, sorry, has. Andrew, sorry, Heron has developed before. So, by using the Riskorda-Wagner equations, so we can get to a measure of entropy. This forward entropy is the spectator price of the next image given to the participants based on their experience and the image that they're currently viewing. And in order to do that, you need to estimate some probabilities of transition from one of the images to another. Of transition from one of the images to the next, to the one that you've seen based on these equations. We can discuss a bit more about these in the discussion part, but basically we're looking at look-ahead activity expected in anticipation of the upcoming schemes. We have four covariants that are considering. So, on one hand, an indicator for a participant see an image, an indicator for a participant. An indicator for a participant pressing a button, and then we have the slow learning rate and the fast learning rate. We use standard techniques to build these interaction terms based on recent work by Bush and Siesler and others. The type of inference, I'm going to just show one of the examples that I had, the type of inference that you're interested in. The type of inference that you're interested in is essentially to see how the correlation between regions varies. For example, in this case, we're looking at the anterior single versus the Caldata as a function of the entropy. In this case, we're assuming this low learning rate. And again, we'll go back to Aaron about that. But we are expecting essentially here is a comment that was that we saw that there is. That we saw that there is a functional activity that seems to be increasing for most subjects between the anterior circular cortex and caudate. And in particular, it seems that this suggests that the two regions are more tightly coupled with the activity in the ACC when the hippocampus has greater difficulty predicting the next stimulus in the sequence. And similar conclusions. Conclusions we can somehow do for the other types of regions that we are interested in. I am beyond my time, so I just want to say to reiterate. So, what we have proposed is essentially an extension on the general, on the usual generalized BPI model that allows for time-varying functional connectivity, taking into account time-varying spontaneous fluctuations in the activity. An important point. An important point is that the metal can be parallelized using associations between regions and using a multiple comparison framework that takes into account non-marginal decisions. We have shown and tried this in application to predictive learning experiment. There are many ways in which you can do extend this type of model. On one hand, we can try to learn across all the same regions. Learn across all the same regions in a single model. It's more challenging than it seems. And also, we may also try to learn across the subjects, multi-subject inference at the same time. The paper has been written up, has been submitted, but so Brian, so Jaylen has gone to other endeavors. In Deborah, Brian is helping us to get it first GitHub and then archived. Okay, and that's it. Oh, yes. Yeah, I think um I'm good to ask questions. Go ahead, okay. Yeah, so yeah, thanks for a nice talk. I'm just wondering, you have this decision theory. You have this decision theory that you are working to acquire probability of the null hypothesis. I think you have shrinkage, yes, probability, right? So, yes, yes, you have shrinkage, right? You don't have sparsity there. So, how you your prior probability? So, yes, I didn't go through it, but essentially there is a decision, right? On the does not exist in association. So, the multiple comparison. So, the multiple comparison framework comes in after running the model from each model, a very decided region, right? For each model, there is, and you're right, you caught me, there is a decision part there where you decide, okay, is this beta recognized by the, is the pass correlation actually different from zero based on this, right? From based on the From me based on the single model based on the shrinkage price, right? So there is that part, right? But that so once you have decided that that doesn't take anything out of the second part because you still have to make decisions across all the models, right? So the non-margin decision case comes in when you take a decision across all the models. Does it make sense? So essentially, you have a decision. You have a you have a decision, you have a model, you run it as usual, you take decisions, and you estimate those comparisons in the first model, right? Based on that, right? When whatever is out of the model is not going to come back, essentially, from this model, right? The issue is only for those that for which you have a discord decision. So some models is positive, some other is negative, and you want to understand exactly. We are fit, some refit, right? Exactly. That's exactly the point. Thank you. Yeah, thanks for catching that. Any more questions? So I'm wondering, do you by using PBI, do you get something more than other time-varying models that use partial correlation? Simplicity. And the fact that it is something that is used by practitioners, right, quite extensively. I think they can understand it probably better than models. Models that we have used earlier, that are basically related on this type of relationship, graphical models, and so on. It's probably something that it is more easy to run on a single region, right? And then take it from there. So typically, you just run volunteer regions versus the others, right? So that's and so. But then you combine them into a person in this right, exactly. Right, exactly. And so, I guess what I'm asking: do you get something different in this than you would in a other time-varying model? I think that depends on the definition of an alternative time-balanced model. Like your work. Oh, my work. Well, my work was so far, the type of work that I've been doing, right, are based on. Doing right are based on Eiden Markov models, for example, for functional connectivity, right, or other types of models that are essentially state-space models, right? So, in this sense, these are a bit difficult, be different because I'm not actually expecting any state-space association here, right? The beta t is a physiological effect that changes, we have smooth changes over time. So, in this regard, they are quite different types of inference that I expect, right? I expect, right? Doesn't make sense? So the issue may be then to try to find an alternative model that allows these type of smooth changes over time in a functional connectivity setup, right? Of course, you could think about athletic connectivity, but that's a type of inference, right? Does that make sense? Other question? A lot of interesting stuff, Michele. Definitely, I'm glad you're looking at partial correlations and not marginal correlations, which is the right thing to do. I agree. I agree. But so in the I'm asking about the symmetry. So if you go from C region one to region. Go from C region one to region two, or from sieve region two to region one, as you said, you should expect to get the same result, but you're not enforcing it. There shouldn't is any reason why you wouldn't want to or you can't enforce that symmetry in the results? The symmetry of the partial collision you should obtain by taking region seed region one and going to region and estimating what happens in region two as opposed to state region two. To region two, seed region two, and estimating the partial correlation to region one. So that would probably, I'm sorry. So if I understand the question correctly, which is a big if, right? I think that in that case, however, the inference, let's say, on the model that has seed region D over R, let's say, that will depend on the inference that I found in the previous model, right? Would it create like a sort of order between the models that maybe I don't want? Between the models, but maybe I don't want to have a priori. Does that answer your question? Thinking about it. Me too. That's good. I think I missed the part where whether or not you fit the model to every subject separately or to all subjects. That's exactly in this model. In this type of in this model that we've done, we are still fitting the model separately for each subject. And we hopefully in a future extension, maybe considering all the subjects together. Of course, that creates additional challenges, especially from a computational perspective. I'm glad to leave Norbert in charge and I'm gonna check with Aaron about that. Okay, so our next speaker is Albert Working from UC Reporting, professor in Europeology. And quite interestingly, the experiments that he has conducted on. The experiments that he has conducted on grass has a very similar flavor as the one that Andre just described for humans. Robert has been a longtime collaborator and is working on really exciting properties. Robert, thank you. All right, this is going to be quite different. Okay, so my goal here is actually, well, first of all, thank you for the invitation. 