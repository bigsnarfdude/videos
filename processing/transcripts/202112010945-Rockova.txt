Thanks a lot. So, first of all, thank you so much for having me. My first talk, in-person talk since the beginning of the pandemic. So, really excited to be here. And my talk today is kind of one of my pandemic papers. It's not about COVID, but it actually happened during the pandemic. So, this is an example of what can happen if you're locked down in a dark room with an internet connection for a long time. Time and it's a collaboration with actually two of my colleagues from the University of Chicago. So Tetsuya is assistant professor in my group and UAC is my student. And for those of you who have seen the talk, a version of it earlier this year, this is technically not recycling because I have four new slides. Okay. And those are from actually a paper that we just submitted last week. And it's called Approximate Bayesian Computation Via Classification. So it's, we kind of have like those two do in tandem. Like those two will do in tandem, but I'll spend most of the time talking about a metropolis, and then indeed those four slides will be about the extension. All right, so the way I typically introduce this talk is, you know, with the mixology metaphor, because one of the skills I acquired during the pandemic was making old-fashions. So I would like to, you know, think of this work as a kind of like an old classic with a new twist. And the old classic is obviously the metroblisk, right? Oh, it doesn't really. Kind of weird. I'll just use, I'll just do this. So, Metropolis Hastings, right? And then the mute vist will be the classification aspect or machine learning aspect that be incorporated in it. Okay, so what we are looking at here is a classical Bayesian, not really classical Bayesian situation, but setting where we have IID observations from some model and that's parametrized by And that's parametrized by okay, I will have to adjust the fact that I don't have a pointer. So, by parameters data, right? And we want to learn about them in a Bayesian way. So, we are focusing on situations when the posterior obviously cannot be easily computed. So, we would want to use some maybe sampling technique to approximate the posterior. But we have the added complication that the likelihood function cannot be easily evaluated for each given. Evaluated for each given parameter value. So, those kind of settings happen a lot in practice. I listed three examples, one of them I'll be analyzing later on. So, it's a Lodka Volterra model, which is kind of prescribed by a solution to a differential equation. In practice, it would sort of like a noisy solution to that equation. And it's not necessarily kind of a friendly model where any typical Any typical metropolis type of algorithm would not be feasible because the likelihood cannot be necessarily evaluated, right? So another model is from finance, the Hester model, which is prescribed by stochastic differential equations. So in those kind of situations, we have discretized actually observations of a continuous process. And in those cases, it's kind of difficult to also do basing inference. I have one actually kind of a simplified version of the Heston model, the CIR model. Version of the Heston model, the CIR model, as an example. And then, you know, you can think of any kind of hierarchical model where after you integrate out some hyperparameter, you get marginal likelihood, which you cannot compute. So it's not necessarily a very uncommon situation. So what can we do in those settings? Obviously, this kind of problem has been around for long enough time that people have thought about solutions. And one of them is the ABC solution, so approximate Bayesian computation, which relies on the fact that Which relies on the fact that we cannot evaluate the likelihood, perhaps he can sample from the likelihood, right? And the ability to sample from the likelihood can be translated into the ability to sample from a posterior. And it's kind of really neat the way it works. So the way it works is you basically sample a parameter value from a prior. And then given the parameter value, you sample fake data from the likelihood, right, at a given parameter value. And if you sample And if your sample fake data are close to the observed data, that means that perhaps the parameter value you sampled from was close to the true value. And then it's worthwhile to keep that sample, right? So the decision to keep or reject ABC sample is typically based on some sort of a selection threshold. And we just keep sampling from priors, keep sampling these fake data. And then some small fraction of those who will be close enough, right, to the observed data, we keep those. And we get an ABC posterior approximation. We get an ABC posterior approximation. It's a beautiful idea, but it does have some practical, you know, obstacles. One of them is that typically the decision to keep or reject is based on some summary statistics. And those may not be always available. It's not entirely obvious what kind of summary statistics will be good for, you know, for each particular problem. And also, you know, there have been research showing that perhaps for model comparisons, it's not. Comparisons, it's not going to be the best strategy. And also, it may take away very many trials, right? Sampling from the prior and the likelihood, very many, many to be able to get a few samples that are kind of close enough. But it can be parallelized, so that's a good thing. So there are many good things about ABC, but we particularly were inspired actually by ABC in a sense that the ability to sample from the likelihood and comparing real and fake data is kind of. And comparing real and fake data is kind of compelling. So, we wanted to kind of transfer that idea into more metropolis Hastings type of setting. And we have decided to do it in the following way. So, if we have a likelihood that we cannot evaluate, what we can do, and here you can see this like IID product, right, of those likelihoods for each observation. What we can do is we can replace it by an estimator, right? By an estimator, right, of the likelihood, or we could deploy some sort of a synthetic likelihood method, which compresses the data into summary statistics and assumes likelihood through the summary statistics, typical Gaussian, right? So both of these are kind of like approximations to the likelihood. I like to group these into, I like to call them pseudo-marginal, but technically not all the metapolystics methods that use approximate likelihood are pseudo-marginal. Likelihood are pseudo-marginal because not all of them will yield the marginal, you know, the correct stationary distribution. So it's pseudo-marginal and others. Okay. So what we basically do is very similar. This is very helpful. Thank you very much. What we basically do is we replace likelihood with an estimator obtained from classification. And by doing that, we are kind of not relying on summary statistics. Not relying on summary statistics, which would be needed for the synthetic likelihood. And it turns out that also oftentimes it could be computationally more advantageous, at least in some of the examples that I'll show you. The sort of margin will take, you know, much, much longer than our classification approach. And so that's kind of like the motivation. So basically, metropolis A stinks with an estimator of the likelihood ratio obtained from classification. So let's just So let's just see how that works. So, obviously, the acceptance probability for metripolis Hastings relies on likelihood ratio, right? But if we cannot evaluate the likelihood, then we are in trouble. And what we basically suggest is estimating the likelihood ratio using a classification approach. And I like to think that what we I like to think that what we are doing here is kind of inspired by generative adversarial networks. I was always kind of compelled by the idea of turning Gantts into posterior simulators, or they're likelihood simulators, right? So like there must be a way of doing it. This is not exactly the solution to that problem. There are multiple ways of thinking about it. But at least, you know, this is like one kind of a way of turning the parts of GAN, so at least classification into like turning optimization into C. Turning optimization into simulation, right? And indeed, inspiration came from GANS, right? Which, as you may know, is a kind of a construct in machine learning for training a machine which can, or okay, training an algorithm or a machine that can generate likelihood samples. And the way it's done is by having two kinds of players playing a game. Players playing a game, right? We have two players. I like to think of them as a good guy and a bad guy. There's a generator and discriminator, whereas a discriminator is a bad guy, right? And they're playing this minimax game where the generator is trying to fool the discriminator by generating fake data, right? And then the discriminator is trying to set them apart. They kind of iterate back and forth until some equilibrium. It turns out that equilibrium. Happens to, I'll show you on a few slides. Back happens when if you have flexible enough discriminator and generator, then the equilibrium actually happens at the correct like a generative model. So here is the kind of loss function for the GAN problem, right? D is the discriminator which assigns score 0 and 1, 1 for the real, 0 for the fake, right? And the G. Fake, right? And the G is the generator guy. So G generates the fake data. Extended will be the fake data throughout the talk. And, you know, as you may know, if you're ever attending any machine learning conferences, you will have seen a lot of kind of pictures like this, where you have an output from Gantz. Gantz can generate pretty faces of celebrities and it's really, really useful. None of these are real faces. These are outputs from Gantz. You know, outputs from GANs. But it's kind of quite remarkable how having these kind of deep learning architectures as you know, playing a game can kind of result in a algorithm that can generate really realistic images. But obviously, that depends on the quality of your discriminatory generator. If you don't have a good enough quality, you may end up having something like this. It's just kind of like one of my jokes I like to use. I like to laugh at my own jokes, even though you guys don't like it. At my own jokes, even though you guys don't like it, like, if you don't laugh, I'll also have a really good time, so that's okay. Some of these are real, some of these are fake. Um, you know, just a little you know, pandemic, pandemic humor here. I also, you know, some time ago, I attended some Gantt talk and somebody used this slide showing actually the failure of GAN. So, if you cannot discriminate between muffins and dogs. So, obviously, you know, not everything is without problems. The quality of the generator and the discriminator really. Of the generator and the discriminator, really, really does affect how things turn out. And that's kind of the case here as well. So, I'll show you actually some kind of theoretical underpinnings showing that you need to have a really good kind of like discriminator to be able to get a good approximation to the posterior. So, let's talk about the two aspects: the generator and then the classification. So, the generator here will generate fake data x tilde, right? And the way it's going to work. And the way it's going to work is that we will have some like IID samples, say, from uniform, and we will pass them through a deterministic mapping, which depends on the parameter theta, and it will give me distribution from the model. So, this is not a very limiting assumption. Many distributions can be generated by the inverse transform sampling and stuff like that. So, we will have the underlying x tilde, which is kind of like over here, some samples which we can. Some samples which we can pass through mapping to generate data which depends on the underlying value of the parameter of interest, theta. So this will be the x-tildas, this will be the actual fake data. And depending where the fake data are, the discriminator will have easy or not so easy time to discriminate between the fake and real. So in this cartoon here, the black is the real data and the green is the fake data. And the blue line is kind of like the score of the discriminator, ranging from one for real data and zero for fake. So, if the two densities don't overlap much, right, the discriminator can kind of set them apart. But if they're kind of overlapping, the discriminator will assign score like half, right? So, this is where the kind of equilibrium happens, right? If the discriminator cannot tell them apart anymore. Yeah. Is it specific to univariate distributions? And if not, do you have? And if not, do you have to have the same dimensionality for Ptla and T theta? So, yes, it is. Yes, so we can have different sample sizes, but it's coming from the same model, right? The only difference is that the real data are kind of assumed to be generated from P of theta naught, and the fake data are generated from P of theta, right? For a given theta. But the sample sizes could be different. So, but the sample sizes could be different. Okay, so what we have here is basically classification, right? So, before I showed you the minimax problem, that was the GAN problem, but this is just the max problem. That's the classification. And here is the same loss function as what you saw before, but it's like the empirical version of it, where n is the sample size of the observed data, m is the sample size of the fake data, and d is the discriminator, again, assigning a score between 0 and 1, right? One for real, 0 for fake. One for real, zero for fake. And actually, what's kind of nice is that if you look at the oracle discriminator, if we knew theta not, this would be the like solution to that problem. So the optimal discriminator would be this one. Right. And now if you just turn things around a little bit, you can actually write the likelihood ratio between P theta and P0 in terms of the oracle discriminator. So this is just simple mathematics where you're just, you know, basically writing the likelihood ratio in terms of the discrimination. Likelihood ratio in terms of the discriminator. And obviously, we don't know the Oracle discriminator, but we can learn it, right? So, the whole idea here is actually to generate real and fake data and train actually a discriminator, like logistic regressional deep learning, right, and plug the d-hat, the estimator, into this equation to get an estimator of the likelihood ratio. So, we call this a likelihood estimator, but it's really like the likelihood ratio estimator because it depends on the not, right? On the truth, but in Truth, but in metropolis hastings, we are looking at ratios of likelihoods, right? So the p-naught cancels out, right? So all we have to do is basically just compute this quantity for the metropolis hastings for two, like a new proposed data or old theta, right? And we are done. So this p-notice actually kind of cancels out. So that's kind of cool. And you may like wonder, is this the only possible way of doing this kind of classification? Of doing this kind of classification-based metropolis hastings. Well, you could imagine that instead of using the real data for comparisons, you could also use other fake data. So, you could even generate two fake data sets for different value of data, right, and get a different reference. You could also use a marginal reference and stuff like that. But here we use the observed data for contrasting. Yes. Okay. Okay. So, just some more, some more just. So, just some more, some more simple mathematics here. This is the estimator, right? Written in terms of the estimated discriminator. And it turns out you can, you know, if you, if you, if you just shuffle things around a little bit, you will see that actually the estimated likelihood is kind of proportional to the true likelihood times an exponential tilt. And the exponential tilt is kind of like a measuring the discrepancy between the uh, it's kind of a It's kind of measuring the discrepancy between the estimated discriminator and the oracle. You want this guy to be as small as possible, right? So the more, like, you know, the better the D, right? This will be close to zero, and then this guy cancels out. But, you know, we do have the exponential tilt, so we can't pretend it's not there. And what we have kind of done is we try to understand theoretically how does that gonna affect actually the stationary distribution and Distribution and under which conditions you can kind of assume that the metropolis is actually sampling from the core extractional distribution. So, I'm going to spend some time just kind of like describing the behavior of this guy and then classifying, talking about the like properties of the stationary distribution. But before doing so, let's just summarize things. So, we have metropolis things, which uses estimator of the likelihood. Of the likelihood in its acceptance probability. So, again, these guys can be computed from the formula on the previous slide. And basically, there are two versions that I would like to talk about. So this is just a typical MetroPlace hasting. There's nothing unusual, just this hat over here. But what's kind of like worth mentioning is this aspect here. Is this this aspect here? So I told you that the fake data is kind of generated by passing these samples through deterministic mapping. So we consider two versions. In the first version, we just kind of use the same underlying latent variables throughout the entire algorithm. And in the second version, we refresh these latent variables. So it may appear like. So it may appear like, okay, what's the big deal? It turns out that actually one of them will have, hopefully, you know, less of a bias. So I'll try to explain that by focusing on the station distribution. So in the fixed case, if I just don't refresh those latent variables, one can show that the station distribution is the actual posterior, the correct posterior, times an exponential tilt. Exponential tilt, which depends on that kind of discrepancy between the oracle and estimated classifier. This exponential tilt is not going anywhere, right? It creates some sort of a bias. But what's kind of cool about a random generated variant is that we refresh the latent variables every step. The stationary distribution still is proportional to the correct posterior of two exponential tilt, but the tilting factor. But the tilting factor will be likely smaller. So, this guy even has a chance of being averaged out towards zero. So, we like to think that the random generated variant may produce less biased, actually, posterior approximations. Here on the right-hand side. It's it's I I think this is c correct, but I I might be wrong. But I think this is good. Yeah, yeah, yeah. Okay. So what we have here is kind of like two versions of the algorithm, the fixed and random. We like to think that the fixed generator variant, this is like, think of this as a true posterior. Actually, the red is the true posterior, right? Actually, the red is the true posterior, right? And then, because of the exponential field, we'll have a little bit of a shift. Okay, so unfortunately, you know, that's kind of how it works. In the random generator variant, we like to think that because of that averaging aspect, the location, you know, there will not be much of a bias in terms of location, but perhaps increased variance. So we thought of a way of kind of, you know, improving on the procedure by combining the two. By combining the two. So, if one wanted to get, say, less biased samples, one could just run the two algorithms and then de-bias it. So, take the actual fixed generator and then kind of rescale it or move the Markov chain towards the mean of the random generator. So, this would be a way how to kind of improve. But we have actually seen in practice that sometimes these two kind of perform pretty similarly, and that the biasing in And the Debay is saying in some examples we saw improvements, in some examples, it was not needed. Okay, so let's think, let's talk a little bit more about some theoretical aspects. So in the paper, we have actually characterized situations under which the algorithm one will lead to correct, actually will lead to station distribution, which has the correct limiting location. So the limiting station distribution will have the correct. The limiting station distribution will have the correct location. And in this second algorithm, we also found conditions under which we'll have the correct location and shape. So those are kind of theoretical conditions. I don't want to really talk about necessarily all those assumptions, but I want to tell you what happens when these are not satisfied. So when we indeed have the exponential tilt, then I want to just kind of tell you. Kind of tell you that you know it can have kind of the station distribution can still have some reasonable properties. So what do we do with the exponential tilt? There are two ways of maybe coping with it. One of them is having it absorbed inside of the prior, and the second one is having it absorbed inside of the likelihood. So the first one is kind of interesting because we have our prior. This exponential tilt kind of depends on the This exponential tilt kind of depends on the data, right? So you could think of this as kind of like a new prior, right? Like empirical base type of a prior. And, you know, then you can just look up in a Bayesian literature already kind of theory for empirical base type of procedures, and we can come up with some sufficient conditions for the statistical distribution to concentrate around the truth. So we've done that. But now the question is: if we know that there is exponential tilt, which is not If we know that there is exponential tilt which is not going anywhere, does it even make sense to quantify a concentration around theta naught? Because maybe there is this kind of bias, which may be non-vanishing even. So maybe it makes more sense to look at concentration around a different place than the truth. And the natural kind of place would be KL projection when we think about the posterior as coming from a misspecified likelihood and a correct prior. So we have these kind of two sets of results, two sets of sufficient. Of results, two sets of sufficient conditions. One set of results focuses on concentration around the truth, and the second one, concentration around the KL projection. We were kind of thinking that maybe the speed of contraction would be faster here, right? Because we are not kind of taking into account the bias here. The speed of concentration will inherently have to incorporate the bias as well. Okay, so let's just take a look at those sufficient conditions. We have this. We have this exponential tilt, right? So, if we, if the exponential tilt kind of diminishes fast enough relative to epsilon n, which is the convergence rate of the actual posterior, then we can show, and this is the typical kind of like Hellinger type of metric, that the posterior will concentrate around the truth at this rate. And the inflation factor actually depends on the speed of convergence of the discriminator. So, if the discriminator is So, if the discriminator is good enough, then we might be able to attain actually nearly the same concentration speed as the actual posterior. So that's kind of a first conclusion. It's kind of reassuring. It's cool. And this condition actually, when we think about a random generator design, it's more likely to be satisfied because this u of theta will be smaller. So it's also kind of motivation for using the random generator. For using the random generator. In terms of the other perspective, the specification lens where we are looking at concentration and radical projection, again, there is an existing theory that we can kind of just like, you know, incorporate here. So, you know, we can funify concentration around PKL projection, get some asymptotic normality. But here, you know, in the specified case, it's known that the, you know, the limiting like You know, the limiting like the centering and the shape of the limiting posterior may depend on the Kel projection, so we may not be able to get like valid, valid Bayesian inference in that case. Okay, so this is just a theory, right? But I want to tell you about those four new slides that I added. Okay, so that's kind of like a variant of what we are doing, but for ABC. So ABC compares fake and real data, right? Compresses them into summary statistics and then accepts the samples when the distance between the summary statistics is small. But people have thought about ways of kind of avoiding the need for summary statistics by comparing directly the empirical distributions of the real and fake data using, say, KL divergence of our system. Using, say, KL divergence of our system distance or something like that. So it's kind of like naturally, if you are using this classification approach, right, if you have real and fake data, why not just run the classifier? And if the classifier cannot tell them apart, maybe that's a good indication that the parameter value we sampled is going to be close to the truth. So am I not to use the classifier for deciding whether or not to keep the Deciding whether or not to keep the ABC sample. And in order to formalize this idea, you know, it's kind of easy to focus on the KL divergence, right? We can actually estimate the KL divergence using classification approach, right? So a few slides ago, I showed you the oracle discriminator being kind of like we can write a like good ratio in terms of the oracle discriminator. Ratio in terms of the oracle discriminator. And because the empirical KL divergence is nothing else than like expectation of the local likelihood ratio, since we have the ratio expressed in terms of the discriminator, you know, this will be just a empirical average of the discriminator. So when we do the classification, we can actually obtain the estimator total KL divergence, and then we can actually use it for ABC, decide whether or not to keep the sample based on the KL divergence. Divergence. So the estimator is obtained basically as follows. So this is going to be like the empirical average, and we replace the oracle with the estimated discriminator, similar as before. So this is going to be very different type of sampling, right? Because it's not Markovian, right? And we run the ABC, the classification at each step. But I think it may have some kind of benefits over the penal post-hastings, which may be a very important thing. Over-dependent plus hastings, which may get stuck. So, the initialization, you know, in metropolis, um, as we will see later in the example, might be important. So, this is just another way of getting posterior approximation using the ABC sampling, right? So, what we've done in the paper, we've kind of shown that the this KL estimator will be converging around the empirical KL at a certain rate. So, this is the rate of the convergence of the actual discriminator and And that's going to actually reason why I'm mentioning that is that because this delta n will come up later. Okay, so I just wanted to have you remember that it's actually the rate of the estimation of the KL divergence. Okay, so I have actually to, I have just summarized the ABC sampling here. So classical ABC, right? We sample from a prior, we generate figures. We sample from a prior, we generate fake data, we estimate a KLV, except when the KL is small enough. This all gives us this object as a posterior approximation. And we've kind of analyzed this object as well to kind of understand how small epsilon n should be relative to delta n, right? Obviously, delta n should be faster. Okay. And then we also thought about And then we also thought about a way of using all the ABC samples, not only the ones that pass the acceptance threshold, using some sort of re-weighting scheme. So if we were to assign a weight for each sampled value from the prior, the weight is proportional to this quantity, you obtain this object. You may wonder why we want to do that. Well, it's kind of interesting that actually this object is exactly the same as the stationary distribution. The same as the station distribution from the metropolis Hastings algorithm that I showed you earlier. So, if one were to run this ABC, we would be actually kind of like computing the same object as the station distribution from the metropolis hastings that we analyzed theoretically before. So there's just a little connection there. Okay, so in terms of some theoretical understanding, so ABC theory has been developed, you know, Judith and Christian have a series of papers. Have a series of papers on that. So, you know, the rate will ultimately depend on epsilon, which is the acceptance threshold. Delta n is the rate of the KL estimator, which has to be kind of faster. And the concentration ends up being a combination of those delta n's. And kappa will be here, the prior concentration condition and stuff like that. condition and stuff like that. So that's just a little theoretical note here. But let's take a look at some examples. So the first example I want to show you is an example where we found that the pseudo-marginal techniques are not as maybe practical. So this model is actually prescribed by SDE. Described by SDE, we have three parameters. And we are actually observing a discretized, like a discretization of this process at some discrete time points. And we want to do Bayesian inference, right, about these parameters. And what's kind of cool about this model is part of the Haston model I showed you earlier, but it's kind of interesting test case because we actually know, we can actually compute the transition. Compute the transition function which leads to the likelihood function, right? So we kind of know we can implement the exact metropolis and then we will implement our classification metropolis and also the pseudo-marginal like approximate metropolis using I call them pseudomarginal, but it's not necessarily true because they don't lead to the correct stationary distribution, but it's the metropolis Monte Carlo-bidden metropolis algorithm. Algorithm. Okay, so it's actually, I close to the margin, but it's the MCWM, if you ever, you know, wonder about that. So the way it works is that we can estimate the likelihood by estimated transition function, right, using some sort of Monte Carlo integration, which depends on n, which is the number of samples. And here the idea is that if you are observing like a discretization, right, what happens in between is kind of missing data. What happens in between is kind of missing data. So we can generate those. We can treat those as kind of like latent variables. So we can partition each gap between two observations into M sub intervals and treat those observations as latent data, right? So M is going to be important and N is also important. M defines the granularity, right? And N is the number of kind of samples we use for the Monte Carlo integration. Monte Carlo integration. And there has been a paper showing that there should be some sort of a relationship like that between those two to have good properties. We wanted to respect that. But increasing NNM, obviously it's going to increase the computational requirements because here we have to sample from modified Brownian bridge to be able to get the missing data in between. And then this integration takes a long time. So I'm actually not going to explain this. I just want to show you that. Not going to explain this. I just want to show you that implementation of these kinds of other approximate metropolis algorithms can be quite challenging and also time consuming. On the other hand, so we tried our classification approach and what we have here is the likelihood estimators. So this would be the truth, the black, right? And here in the top panel, I'm showing you the approach. Approach over here. So, this would be the estimated likelihood using this whole integration technique. And as we increase the granularity, the likelihood is going to kind of improve, but there will be still some sort of a bias. So, that's just how it is. Also, this algorithm is known not to yield the correct statue distribution, so we'd expect there to be some sort of So we would expect there to be some sort of discrepancy like that. And the bottom panel is our classification-based likelihood estimation. So you will see that the likelihood is kind of shaping around the true location. So we are able to learn about these unknown parameters using this approach. We seem to be getting some sort of information accumulation around the truth, right? That's kind of reassuring. So let's see how it is going to work. So let's see how it is going to work inside of the metropolis. So here we have the two, our approach, and then the MCWM with different granularities. So I just want to point out here that actually increasing the granularity to like five, the computation took over 250 hours. And what we got was this blue curve for the sigma. And it turns out it actually did 95. And it turns out that actually the 95% credible interval excludes the true value. So we're not really learning as much, or well, at least for this last parameter. The classification approach, we also have some tuning parameter. One of them is kind of making the approximation better. So the blue curve will be kind of the improved approximation. But what's kind of cool is that this one was obtained within like, I think, 13 hours, and it does seem And it does seem to be, you know, at least accumulating better information around the truth. So that was kind of comparison, which was quite surprising because we were hoping that this procedure would do kind of comparably well, but it does seem to have some sort of bias issue here. Okay, so the second example I have is comparing the classification approach with traditional ABC and the Malawi. And the model we have is Wodka Valtera model, which is kind of one of the typical examples if you know the ABC literature. And here we have four parameters. The data is actually sets of observations of like size of populations of predators and prey. And the predators can, you know, be warned at a certain rate. Be born at a certain rate, and they can die at a certain rate, and prey can be born, and so on. So, we have this kind of changing population sizes according to these different rates, which depend on these four parameters. And there's basically this is one of those generative models where we can sample from these models well, but you know, writing down the likelihood would be kind of cumbersome. So, I like to think that this is an. To think that this is a natural candidate for ABC because the simulation is a simulation of the fake data is quite actually quite simple using the GLSP algorithm. So let's see what we can do. This is just an illustration of kind of the data sets, right? So this would be the true data that we used for the simulation. So we use these parameters, which describe the rate of kind of like the renewal cycle in the population. What's kind of cool about this model. What's kind of cool about this model is that if we tweak the parameters a little bit, we might get different evolutions. So the prey is the red, and the predators are black. So if you are to feed these two data into a classification algorithm, right, like it should be pretty easy to tell these apart. So that's why we believe that the classification approach will be beneficial for the kind of data like here, because the classification should pick up on these differences, right? Differences, right? Okay, so we have simulated data. These are the true parameters. So this is our actually true data, right, that we use. And then we'll be running our classification approach. So proposing a new value, right, simulate fake data, classify, and stuff like that. Before we get to the results of the metropolis, I'm going to show you the results with ABC as well. So ABC, obviously, there are multiple ways of doing it. So, there are multiple ways of doing it. We have taken the kind of simple approach that people have used in the literature: so, taking the mean variance and cross-correlations of the series and stuff like that. When you look at the ABC discrepancy metric for varying parameter value, so if I'm varying just a parameter theta two, keeping everything else fixed at the truth, we are learning something, right? So, the ABC discrepancy will be smallest around the true value. Will be smallest around the true value, which gives us hope that we can learn about some of these parameters, which was nice. And contrary to this, we have the metropolising's likelihood estimator. So we are also learning something because the likelihood seems to be peaking around the true value as well. So that's good. What's maybe not so good? It's maybe you're learning too much, right? Because the likelihood is kind of spiky. And I observed that this actually happens if we use different classifiers. You may have slightly different shape. Classifier, so you may have slightly different shapes, but for this GLM net classifier, it appeared to be kind of like a really spiky likelihood. So there will be just a small range of parameter values which will give to fake data, which have this oscillatory behavior, which results in really spiky, spiky likelihoods. So the theta two and theta three, they're kind of like, you know, not as spiky, but theta and theta four, look at the resolution here. So it's just a tiny spike. So for metal. Just a tiny spike. So, for Metropolis Hastings, you know, if you initialize too far, you know, chances are that you will have a really slow, you know, slow convergence. So, we kind of are aware of that, but when we tried random forests, for instance, desensitivity to initialization wasn't as severe. Okay, so this was just particularly the case for the logistic regression. Okay, so let's take a look at ABC. So, having seen actually these plots, I thought that it would be kind of unfair to run a uniform prior. Unfair to run a uniform prior on like a unit cube for ABC, so we kind of narrowed it down a little bit. So, you know, to these kind of like prior domains. And then we just use the summary statistics. Again, the summary statistics may not have been the best. And ABC kind of learns about theta one and theta four. These two are kind of like the distribution seem to be kind of uninformative. And that did not improve after simulating many more samples, ABC. Samples, ABC samples. This is out to 100,000, and these distributions are taken from the first thousand best samples. So we are not really learning a lot. Again, this could be just because the summary statistics weren't good enough. For the metropolitan hastings, it was actually quite nice to see that if it's initialized in a reasonable neighborhood for the logistic regression, it will be able to kind of It will be able to kind of get to the right spot and give us reasonable kind of estimates of the posterior, which were obtained within like fraction of the time needed for ABC. So this was like ABC1 was the worst one. This was just the 10,000 samples, like four hours. You know, this was 100,000 samples. So multiply that by, you know, 10. So it's 10. So that was like 40 hours, and this was two and a half hours. I mean, you look at those credible intervals, they're much more narrower. And also, the estimates of the mean seem to be closer to the truth. So I like to think that this actually turned out to be a pretty successful implementation of this classification approach on this Lotka Bolterra model. In the revised version of the paper, we also have comparisons with other versions of the metropolis using random forests and using different. Using random forests and using different like reference distributions. But the conclusion was kind of the same: that if you use these summary statistics, it won't be necessarily for ABC, it won't be necessarily good. Actually, in the second paper, we do have the KLABC, but it's in a different table, and I kind of forgot to include a fifth slide. So I only have, yes, I'm actually, yeah. So yeah, I have to just kind of remember exactly what those could be. Yeah, I have to just kind of remember exactly what those comparisons were because we didn't directly compare the MHC against the ABC with classification. But yes, I should have probably included that here. Okay, so I will skip the last example and just conclude. So here we've kind of revised Metropolis Hastings and incorporated some ideas from machine learning and described some properties of the station distribution. Some properties of the station distributions. And if you're interested in reading more, the paper is on archive, currently under revision. And then this follow-up paper was actually uploaded last week. So you can take a look at that one as well. With that, thank you for your attention. Thank you. Any questions for Veronica? Any questions from the floor here? From the floor here, comments should be so for this ABC classification approach, there was a paper, at least I saw the talk by Kyle Cranmer, who used the classification approach as well for ABC. Does it compare? Is it? Well, for ABC, does it compare? Is it similar? Is it completely different? Do so, uh, they use a different um metric for comparison. So, uh, we use the KL. So, we are directly, you know, estimated likelihood ratio means that we can estimate the KL divergence. So, we were really focusing on the KL divergence. They have a different discrimination metric, which does not have necessarily this interpretation, but they have indeed, yeah, they've used the classification approach, but using different metrics. Classification approach, but using different metrics, and they also have not quantified the properties of the distribution. I'm not an expert on ABC, but I once tried to implement it and didn't succeed. It was probably because of the choice of the summer statistic. But I saw in your previous slide. But I saw in your previous slide that the results for playing ABC didn't work well. So, what is the reason there? Is the choice of the summary statistic or is there something else there? So, why it doesn't work? So, I think in this case, it's probably the choice of the summary statistics because we tried many samples. I initially thought that maybe you didn't have enough samples. It could be also the prior domain if you restricted the prior domain. If you restricted the prior domain around a spike, right, then we would maybe get some better results. But I think in this case, it was the choice of the summary statistics. Anyone else? Any questions from, oh, Christian? Just a precision for the metropolis version, you only train the discriminator once, right? That's your position with the Is that the opposition with the other approach where you need a discriminatory each time you change theta? Yeah. So actually, so we do, so there are different possible versions, right? So in our version, we are retraining the classifier at every step, which may not be always beneficial. There is a way of training the classifier ahead of time, but we found that to be more computationally intense. More computationally intense because one needs to have. So, the way to do that would be to use not the true distribution as a reference, but marginal distribution as a reference. And one would need very many samples of the fake data to be able to approximate the likelihood surface well. So, we found it to be it may take much longer to actually learn the likelihood surface than running the metropolis hastings simulation. Running the metropolis hastings simulation. So if the data is not large enough, then running the classification may not be as on the smaller data will be, I think, can be even faster than like running the classifier ahead of time on a huge amount of data. And that turned out to be, we made those comparisons and that was actually the performance was not necessarily better and it took a longer time. So I like to think that if the classification problem is not as involved, like if the data is not very big. Is involved, like if the data is not very big, right? We can run the classification at every step, comparing against the fixed observed data, then it's better. But we could also compare it against fake data and then do all kinds of other things. But I think that using the observed data as a reference is good because we are learning, I guess, more than if you were contrasting against two fake data sets. So it's kind of, I'm not probably explaining it well, but there are ways of doing the classification ahead of time. The classification ahead of time, but it won't be as reliable, I think. Any other comments from the floor here? Any questions online? Or if there's any questions, just unmute yourself and speak up. I just have a quick, I have a quick question. Does it, and this may be silly, but does it make sense to think about other criteria rather than relative entropy for the classification step? Classification step. Yeah. Yeah. So we are kind of like focusing on the entropy because it gives us the nice kind of like, you know, KL divergence and stuff like that. But if you were to modify it, we would be able to maybe approximate other distances, maybe Vassestine and things like that. But those are harder to compute. Right. So the KL seems to be kind of like a natural kind of like byproduct of this, of this. Pro byproduct of this of this cross-entropy loss function. So we kind of like that. But there are definitely ways of extending it. One can solve the classification problem easily. Thank you. That was exactly my intuition. Thank you. Anyone else online? If not, then let's thank Veronica again for a wonderful talk. And we'll have a half an hour. And we'll have a half an hour