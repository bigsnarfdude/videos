Someone needs to leave. Okay, no, no, it's just me. Okay, it's over now? Okay. Okay, great. Alright, uh, welcome everyone to day two. Today we'll be talking about certifiable sub-Gaussianity and clustering. Okay, so right now it's mysterious what it means, but within the hour you'll know. But first I wanted to say that yesterday I actually got a little bit carried away not citing things, so I didn't mean to give this citation for the This citation for the theorem that, or the first works that showed how to robustly estimate the mean using sum of squares, at the very least. So, okay, so that was from Hopkins and Lee, and also from Qatari and Swear. So these were yesterday's citations. But conveniently, they also carry into today. So also today we'll be mostly covering material from Hopkins and Lee, this paper, and then another paper by Kathari and Steinhardt. All from the same year, 2017. Okay, so this is uh this is just doing a little bit of housekeeping. Uh okay, so today uh we'll talk about something called certifiable sub-Gaussianity, which is a property that has been used in many of the works that apply sum of squares to estimation problems. Um but again, just as yesterday, I want to focus on one Focus on one poster problem. Okay, and that problem will be clustering a mixture of Gaussians with identity covariance. So just giving myself some room. Okay, so the problem that we're interested in is clustering mixture. Okay Gaussians in high dimensions. And in this problem, what we're given is we're given samples x1 through xm independently sampled from a mixture. Of K Gaussian distributions, let's say with equal mixing weights, even though you can generalize this to pick not equal mixing weights, with different means. And I get any covariance over D. Okay, one can one can have uh several goals here, but uh But our goals will be twofold. So, goal one will be to estimate the means. Okay, so we want to produce vectors, you know, mu1 through mu k hat that approximate these in L2. And goal two would be to cluster the Samples. So for each I in k or in M decide if X I from X I make sense? Okay, and uh in general this is not always In general, this is not always a possible task, so we'll take an assumption, and the assumption will be that the means are well separated. Okay, so we'll assume that for all i not equal j, mu i minus mu j is at least delta. Okay, so what's known about this problem? So a theorem due to Regev and Vijay Raghavan, from 2017, states that if delta is at least of order square root log k Where k is an M are mixtures, then you can identify the means with polynomial main samples. Okay, so as long as this separation condition holds, then with polynomial and many samples, there's an algorithm which is not efficient that can identify the means. Okay, and this algorithm is some variation on the EM algorithm. And conversely, they showed that when delta is much smaller than square root log k, you need exponentially many samples. Or maybe Or maybe super bottom line many samples. So, what does it mean that you can identify the right so so? I can I can estimate mu1 through mu k. With some error. With some error. I guess, okay, if you want to estimate them up to error, epsilon, you incur a polynomial in epsilon. Uh, dependence on the sample capacity. Okay. Okay, so this is a pretty surprising statement maybe because if you think, or I mean okay, maybe to this audience it won't be surprising. But the picture we shot for ourselves is, you know, if I have a Gaussian in D dimensions, okay, the distance from every, from the typical point, so the mean will be something like square root d. Okay, so here So here, this distance is something like root d. Okay, and we can, sort of without loss of generality, think of d and k as equivalent, because always you can project down to d dimensions. So let's say that this is something like square root of k. Okay, but what we're saying here is that as long as the separation between two clusters is at least something like root log k. Then it's possible to find these means. Okay, so the separation between the means is much, much smaller than the distance from the typical mean to a sample. All right, and imagine now that we have, you know, k, I mean, okay, two dimensions is a little bit smaller for me to draw it, but imagine that we now have k of p is also. And do they understand precisely the transition from polynomial to exponential? The transition from polynomial to exponential? You mean like the sharp constant? I don't think that they understand that at least not in this paper. But it's too, it's literally two constants? Yeah, it's literally two constants. 99% truly, it's literally two constants. Okay, great. So, okay, so now we know that this is possible with an efficient algorithm. Now, what we want to do is we want to be able to get efficient algorithms. And to do that, we'll use the previous algorithm framework. Is it known that EM is inefficient in this case? Yeah, I guess that the... Oh, you mean whether it's known that the threatening EM? I'm not sure. There are some positive disasters for EM for last separated cases. I don't know about this case, but. This place. Yeah, I guess let's talk about it offline. I'm not totally confident in what I remember. I thought we only know if they're mixtures of two Gaussians, like very, very relatively simple settings to my knowledge, but I could be wrong. Right, if the number of mixtures is, if it's two clusters and then results are known, and I guess there's a paper by Costis, right, for the two mixture case. But I think it doesn't generalize to the K-mixture. I think it doesn't generalize to the K-mixture case. I don't think so, yeah. I don't think that case is open in general. All right. Okay, anyway, let's figure it out. I'll find and talk about it later. Okay, so here is the theorem that we will be proving today. Let's put it here. Okay, and this theorem is due to Hopkins-Lee and also to Kathari and Steinhardt. And also in parallel, there's a different work which doesn't use sum of squares, which proved the same result, in my opinion, more complicated way due to diaconicolus's all. Okay, so the theorem is as follows. So if So if delta is equal to k to the alpha, then given d to the one over alpha squared samples there it is There is a time d to the order 1 over alpha squared algorithm, which clusters slash estimates the means with error. Little oh of 1 over k. Okay, and then the these two works are actually using degree one over alpha SOS to get the algorithm. Are you hiding some dependence on K here? I guess, okay, think of D and K as the same from now on. K is the same from now on, because we can sort of without generality take that. So, yeah, so let me just actually explicitly do this. Let's make everything okay, just so that we don't have to keep track of so many parameters. Because I mean, you can sometimes get things when you're in very low dimensions, things become easier, et cetera. But let's just assume that D and K are the same, so we're in the high-dimensional setting. Yeah, sorry about that. So, so what is L? Yeah, so alpha is L. Yeah, so alpha is uh alpha is any constant uh bigger than, sorry, bigger than zero. So, uh and actually in particular if you if you're if you're letting alpha be a function of k that's close to zero, then at separation squared log k in quasi-polynomial time, these algorithms give something. But if you think of alpha as being something like 1/8, okay, then this. Like 1 eighth, okay, then this is saying when the separation is k to the 1 eighth, there's a time, like, you know, k to the, I think, 36 or something to solve it. Okay, so, and just so that you have a comparison, so previously algorithms were known for the case when k When delta was at least k in the one-quarter. Okay, so this was the previous benchmark for polynomial time algorithms. So here you can see that, you know, actually for any polynomial separation, you can get a polynomial time algorithm using S of S. Okay, so uh so let's see how we prove it. So let's see how we prove this. So just as yesterday the key was to identify some property that identifies the mixtures and is also certifiable by SOS. That will be, you know, the theme will occur again today. And today our property will be certifiable subgaussianity. Okay, so let's take some definitions. So So we say that a random variable x is sub Gaussian. Here x is in on t if for all t greater than or equal to 1 and for all unit vectors in d dimensions. In d dimensions, the expectation of x minus its mean inner product with v raised to the tth power is at most k some constant times t to the t over two. So this is the this is the basic definition of sub-Gaussianity for a high-dimensional random variable. High-dimensional random variable. And in particular, Gaussian distributions with identity covariance satisfy this with C equals 1. And what we will need is we will need something called certifiable subgassianity. So we say that. So we say that random variable x is, okay, t certifiably so Gaussian if there exists a degree terminal. T or order T and so as proof that this is the case. Meaning we want that if so we can think about it in the following way. So if v is a vector vector of uh variables. Expectation x minus expectation x v d to t less than or equal to c times t over two norm of v. Norm of V, or sorry, norm of V to the T as a degree or the T. So that's proof. So So T certifiably, the certifiably part refers to the fact that we have a sum of squares certificate for this inequality. T is for the fact that it's T. I guess I can also comment that actually here you could take a more general definition. You could ask for a T certifiable Gaussian in degree K SOS, and then you would relax this to be a degree K proof. But okay, here for Fress, let's. Proof, but okay, here for us, let's keep things consistent. Oh, and also, I guess it mostly makes sense if t is an integer. Because otherwise, these aren't polynomials. What does it mean to have a proof? Like, starting from what? Maybe you're going to get into this? Like, mean by looking at the PDF of it or something? Uh I guess I literally mean that, you know, I can write that you know I can write this inequality as a sum of squares. I see, I see. And like expanding out the expectation and whatever properties you know about the expectation. Yeah, that'll just give you some coefficients for this polynomial in the variable v. Sure, okay, okay. Yeah, yeah. No, really good to clarify. Okay, so this is what T 0 to fabulies of Gaussian means. And yeah, so the whole game will be for us to give, to show Will be for us to give, to show that, I mean, okay, so the whole idea for this theorem is based on the fact that if I have samples from Gaussian with identity covariance, if I have enough samples, then I can give a degree T SOS proof of its inequality. Okay, so let's just see what that will. Okay, so let's just see what that will look like. Okay, so So suppose that I have now x1 through xn, and these are all sampled from the same Gaussian. So we've been mu coherence identity. Now I want an SOS proof of the fact that The fact that 1 over n, sum over i x i minus mu v to the, okay, here let's have an even power, 2t is at most v to the 2t t to the t. Times the constant. Okay, so here is the form that the SOS proof will take. It will use the positive semi-definite ordering of matrices with the empirical tensor of the sample. So it's like 1 over m. Right, so we'll take 1 over n, so here's a fact. If n is at least e to the order t times, okay, times some polylogs which are going to be hidden inside this order t. Then with high probability, The moment tensor flattened to a matrix is bounded by, let's put here, 1.1 times the expectation of Of the standard normal Okay, so if you take the ordered t-centered moment tensor and you write it as a you know d to the t by d to the t max. You know, d to the t by d to the t matrix. This matrix will, with high probability, be bounded by the same matrix for a standard normal random variable. This is just a matrix concentration fact. Assuming you have enough samples. And then this automatically will give an SOS proof of sub-Gaussianity, right? So I guess the one The one fact that maybe I should have said earlier is that if I look at the expectation of z from n zero infinity, okay, some points of t, transpose, and I look at this, the quadratic form of v to the t with this. Then, if I write out the coefficients that I get here, just as Gauden was saying, this will literally be equal to some constant times the norm of v to the 2t times t to the 2. So, I guess, okay, I guess the way that this is an SOS inequality is that now, if I want to get this polynomial. This polynomial, or I guess this polynomial, that's equivalent to conjugating this by V tensor T on both sides. Maybe this also I should have said it's a terminal. On the right-hand side, before the expectation of it. Oh, this is just a constant 1.1. Yeah, let's... 1.1. Yeah, let's turn it to 10. You know, it just controls how many samples we need. Let's put 10 there to be generous to ourselves. Okay, great, so right, so the way that we go from this to this is an SOS inequality is we just would take, you know, this is a matrix A, B, and then we just take the quadratic form of V tensor over T on the both sides. Britton, as we discussed yesterday, this kind of inequality when A and B are positive sine definitely ordered is an SOS inequality. And this fact tells us that this would just be, you know, some other constant times d to the t to the t to the t because you can write. Instead of of degree d to the t version. Of degree d to the t version. Yes, exactly. SOS of degree that's the same as the degree of this polynomial. Right, so SOS of degree d to the t, but also notice that we needed d to the t samples. Right, and that's where this sample complexity comes in. We need a lot of samples. And okay, this theorem that I erased of Regev and Viger-Regman did not need that many samples, right? So in order to get even separation. In order to get even separation square root log k, you could do it with polynomial many samples. Here, to get separation squared log k, you need quasi-polynomial many samples. It's because you need this certifiable sub-Gaussianity. We currently don't know anything for getting polynomial time and sample of Plato, square root log, k or random. Yeah, that's still open. I I don't so yeah, so they have it at polylog Sorry, poly what? Jerry and Alan, they have this theorem with delta the separation being polylog. Nice. Cool. Do you know if it's an SOS? Yeah, I think they have a way to do the SOS more efficiently. Oh, cool. Oh, cool. That's really cool. Okay, I don't know what this was, but I mean, it's yeah, okay. Probably it will be public soon, I guess. Okay, so cool. Alright, so let's see how this kind of sub-Gaussianity effect, certifiable sub-Gaussianity effect, can help us get efficient algorithms. Okay, so let's now imagine two scenarios. Rather than formally proving things, I'll just ask you to imagine two different scenarios. Ask you to imagine two different scenarios. Okay, and hopefully that will be enough. So the first scenario: imagine that we have samples x1 through xn, and they're all coming from cluster 1, okay, with mean mu1. All right, so in this case, these are all standard, I mean they're not standard normal, but after recentering by mu1, they're standard. Normal, but after recentering by new under standard normal. And so it should be the case that sum 1 over n, this font is too small, sorry. So in this case, 1 over n, sum xi minus mu 1 A to T is at most something like constant tt to the t. On the other hand, suppose that we have a bunch of samples, but they're actually coming from two clusters, right? So almost all the samples are coming from cluster one, but there's a one over k fraction contamination coming from cluster two. Okay, mu two uh has distance delta away and if I look at x one through xn then one minus one over k come from c one and one over k from c two. From C2. What we want to argue is that in this situation, if I look at the sub-Gaussianity score at degree t, it will be noticeable. So let's see that. So let me look at one over n. 1 over n sum j equals 1 to n. Okay. Xj minus. Let's take x bar to be the empirical mean of the sample. Okay, so x bar is going to be like 1 over n. Okay, anything. Okay, and now this will concentrate really well, as long as the number of samples is large enough, around 1 minus 1 over k mu1 plus 1 over k mu2. Right, 1 minus 1 over k fraction comes from mu1, from the first cluster, and 1 over k comes from the second cluster. Okay, now we get to choose any unit vector, any unit direction. any unit vector, any unit direction to test here. Right, so what we're gonna do is we're going to take the vector mu 2 minus mu 1, right? And let's take the unit vector in this direction. So let's call that u. So u is like the unit vector in the direction you can see that. And that's the direction we'll test in. Okay, so here, something that I can do is I can just drop all but the terms that participate in the second cluster, right? All the terms contributed. Right, all the terms contribute non-negatively, so I'll just take a second cluster terms. So taking only J and C2, this is at least here I'm doing some renormalization. Okay, now since each of these xj are coming from cluster 2, this quantity I can actually write as zj for zj as standard normal plus mu 2 minus x bar. Minus x bar. Okay, and then this in turn, just doing a little bit of algebra, is zj minus. Sorry, let me make sure that I it's correctly okay. Sorry, let me edit something a little bit just to make the notation cleaner. So let's just let u be the actual vector in this direction, not normalized to be a unit vector. And then I'll put a little arrow on top to say that it's the unit version. Okay, so then here, what I want to say is this is actually zj minus one minus one over k, okay, times u the unnormalized vector in in that direction. The unnormalized vector in that direction. Okay, so then this whole thing is going to be equal to one over k over n sum j c two C J minus one minus one K. Okay, and now my situation is pretty good. Right, cluster 2 has size n over k. So here, and these are normal, so this will concentrate pretty well. This will be something like 1 over k. One over K expectation over Z for normal and then now uh what I want to do is I want to argue that this will have a a large value proportional to the norm of u. Value proportional to the norm of u raised to the 2th power. Now, okay, if I was really being pedantic, I would like to take care of this z cross term. But let's just ignore the z cross term for now and pretend this isn't here and see what would happen if we just had the 2t power. Actually, like, not just for now, let's just do it forever. And you can check later if you really care. Okay, so this is going to be something like 1 over k, okay, norm of u to the u to the 2t, 1 minus 1 over k to the 2t plus some other terms. But basically, we can see that if this term is large compared to this term, what we got in the pure cluster case, then we can distinguish these two situations from each other. From each other. So let's see when that will happen. Maybe I'll do it up here. Sorry for the disorganization. Okay, so we want to compare the kind of the clean case, which is equal to 2t to the t times some constant. And we want to say that this should be much less. Want to say that this should be much less than the dirty case, which is something like 1 over k norm u to the 2t. Okay, this one nice 1 over k to the 2t, I guess I'm just going to be sloppy and put like a half to the 2t. Just a constant. Now, by our separation assumption, the norm of the vector The norm of the vector u should be delta. So actually this is delta. Okay, and then simplifying this, essentially what we get is we require that k be less than delta over t times some constant to the t. To the t. So you can see that if you take t, like k to the 1 over t equal to delta, then you'll have this condition met. And that's exactly the dependence that we needed here, right? If we had delta was k to the alpha, then we needed degree 1 over alpha S of S. Okay, so that's exactly like degree T, where T is like a square with 2G over. Oh yeah, yeah, two T, yeah, yeah. Okay, so so this is the this is the sort of the idea that underlies it all. Now of course, okay, we have to turn this into an SOS proof. Turn it into an SOS proof because it's a little bit tedious. But I am going to at least set up the system of polynomial equations for you that you would solve and discuss a little bit. Solve and discuss a little bit how you turn it into an algorithm. Why is it different from before? Why is what different from before? Turning this part, like this right-hand side of the board, into an SOS proof. Why is it different from the left-hand side? You want to turn this into an SOS proof? Yeah. Well I guess I don't I mean even if this was an SOS proof I guess it doesn't I guess it doesn't yet. I guess it's not yet an algorithm, right? Because I only consider these two extreme situations. So really what I would need to do is I would need to argue for you that I can set up some system of polynomial equations and its answer would give me the cluster somehow. Right, no, of course, you have to talk about the other direction also. Right. So hopefully, like, my scheme was to get it to feel to you guys like this was kind of SOS-sizable, right? Like, you know, if you were creative enough. That if you were creative enough, you could sort of turn this into an SOS proof using this ingredient, right? And then I wasn't going to do the SOS proof for you at all. I was just going to set up a system of polynomial equations and discuss at a high level what one has to do.  Okay, so let me describe to you the polynomial system that we'll have. So here's a polynomial system. So just as yesterday, we'll have variables w1 through wm Where the ideal situation is that Wi is like the indicator that sample Xi was drawn from the mean one, let's say, the cluster one. Okay, I mean, by symmetry, obviously, you could put any cluster here, right? But the Wi's are supposed to be the membership variables for a certain cluster. All right, so then. Alright, so then we want them to be like Boolean valued, right? So we'll add in the constraints wi squared equals wi for all i and n. This should be, this is the same as what we did yesterday. Now we also want that the sum of the wi will be m over k, right? Because every cluster Right? Because every cluster has about a 1 over k fraction of the samples. Now, of course, this won't hold exactly, so really you can add some slack here, right? But okay, let's ignore that issue for now and pretend that exactly 1 over k fraction comes from every cluster. Okay, and then our last constraint will be the certifiable subgai energy constraint, right? So we'll have some matrix B of variables and Of variables in R d to the t by d to the t. And we'll ask that this is the second minutes. K over M times the sum Wi Xi minus U bar. minus u bar transfer t times mu bar transparent transpose is equal to some constant times the matrix that you'd get if you just had standard normal moment tensor what is m here m M. M? Oh. Oh, M is the number of samples. That's not M. Okay. Alright. Yeah, sorry. Over here I had N because I wasn't talking about all the samples that you see. I was only talking about the mini cluster, but yeah, it's M now. Okay, sorry. Okay, minus B. minus B transpose. Right, and I forgot to say that just as before you would set mu bar to be so this is also a programmed variable. It's like the mean of the sample according to W. Okay, so we have these, this constraint. We have this constraint so that they act like indicators for being in the set. And then we have this to ensure sub-Gaussian behavior of the vectors. And what this is saying is it's essentially like a program that's searching for a subset of m over k points that behaves in this sub-Gaussian way. Okay, now of course there's some issue here of symmetry breaking, right? So like, you know, really, so a feasible solution to this would just be to choose the WIs to be, you know, the members of, you know, one for the members of a single cluster. But actually, you could have it be the members of any of the clusters. So, okay, so we're going to break symmetry in the following. So we're going to break symmetry in the following way. So in order to break symmetry, we'll ask for the pseudo-expectation, which minimizes the following convex quantity. The Frobenius norm. The Frobenius norm of the matrix, which is the pseudo-expectation of deli deli transpose. Okay, so this is a convex constraint. So I can add it to my semi-definite program that I was using to solve for pseudo expectation in the first place. Okay, and in this way, I'm asking for w, I'm sort of asking for w to be uniform, like to get To be uniform, like to get the moments of the uniform distribution over clusters. So in the end, like here's a claim. If I look at pseudo expectation, WW transpose, this matrix, it's just going to be a block matrix where each block represents Where each block represents a cluster. Okay, so it'll be like, in other words, it will be like the expectation over i and k of the indicator for cluster i. Indicator for cluster i transpose. Why do we need to break symmetry? I guess we could not break symmetry, and then we would, I guess, maybe run this algorithm k different times to recover each cluster separately. That's also fine. I guess this is a one-shot thing. So maybe it's better, and also the analysis is nice if you do this. So to expand on that one-way running, this would be first you run it, and then you get a set of points, and that gives you one Gaussian, you take it out, and then you continue with K minus one, and so on, and so on. But you can do it this way. But you can do it this way, so why not? What is this suit expectation moment? Why do you call it suit? Oh, yeah, yeah, sorry. Okay, this was from yesterday. Yeah, maybe I should remind S. Right, so like recall from yesterday that the pseudo x, so I have some system of polynomial equations, like pi x equals zero for my. Right, so here is my system of polynomial equalities. And the pseudo-expectation was an operator from monomials of degree up to k, where k is something that, okay, maybe k isn't a good choice for today. L, I think we didn't use it today. So monomials of degree up to L, two reals. Okay, and it satisfied a couple of properties, right? There was like scaling. There was like scaling, and there was non-negativity of lotary squares and there was being zero on our constraint set. Okay, and I promised you that I could give you a polynomial time algorithm to solve for such a pseudo-expectation. And actually, that polynomial time algorithm was a semi-definite program algorithm, right? So I have some semi-definite program whose variables are the value of the pseudo-expectation on these monomials. This I didn't say yesterday, but I am saying it today. Okay, so now let's. Okay, so now basically what happens is I have my system S. I take S, I feed it into some SDP solver. It will spit out for me some positive semi-definite matrix. Let's call it P. Okay, and then from the entries of P, I'll be able to Of P, I'll be able to infer the values of, or, you know, to just write down the values of the pseudo-expectations of x plus most L. The coefficients. Yeah. And now all I'm saying is that I can, okay, so this, this matrix, is some linear function of the entries of this p. So I'm saying that I can add to my SCP solver that I wanted to also minimize this. And now it's not strictly speaking a semi-definite program. And now it's not strictly speaking a semi-definite program, but it is a convex program, right? I want to find the p, which satisfies all of these properties and also minimizes this quantity, which is a convex quantity. Yeah. Good. Sorry, maybe they get the symmetry breaking part though. Isn't W a binary vector, or are you now making it like K way? Or are you now making it like kway vector? Like, does that one? No, no, it's always binary, it's always still binary. Okay, I guess that, so, so here, you know, like the, yeah, this notation, I guess it's a little bit suggestive, right, that I'm not actually getting a single solution, right? I'm getting like a distribution. It's like a relaxation for moments of a distribution over solutions. And you can think of this as me asking for, okay, now I want the moments of the uniform distribution over the clusters. Distribution over the clusters, right? W is supposed to be the indicator for one of the clusters, and now I'm asking it to be the uniform, like the uniform distribution cluster. Sorry? Random introducing cluster. Yeah, random introduction cluster. Let me just say, yeah, let me try to say one thing about why you would do the symmetry breaking. Maybe that will make it sink in just a little bit better. Okay, so the analysis is going to go like this now. Go like this now? Okay, so let's let Ai or A1 through AK be vectors in R M, right? So the number of dimensions is like the number of clusters. And I'll make A sub j of Sub j of i equal to 1 if and only if xi was sampled from cluster j and 0 otherwise. Okay, so these are like the indicator vectors for cluster membership. Alright, so the proof goes by showing goes by showing first that sum of uh W A I squared is at least one minus one over k squared. Okay, so we want an SOS proof of this. So, we want an SOS proof of this. Why will this be enough for us? Well, if you look at this quantity here on the left-hand side, this is like the pseudo-expectation of W, W transpose. Inner product with sum over I, AI, AI transpose. Okay, so this is this very same block matrix that I had here, where every block is the members of the cluster. Block is the number to the cluster. Okay, and via a kind of dull computation that I'm not going to do for you, this is saying that this is at least one minus little o1 times the Frobenius norm of Taylor W transpose times some AI AI transpose. Okay, as long as I chose W. Okay, as long as I chose W to be the minimizing for B storm. So this is or like of the this is a this is for the minimum. Okay, now if I just think about these as vectors in M dimensions or M squared dimensions, right, I'm saying that they're very close to each other. Okay, so that means that I can basically look at this and I can read all I can basically look at this and I can read off cluster membership. I can formalize the statement by doing like some arguments, but let's not do it. Okay, so effectively all I need to do is I need to give you an SOS proof of this. And the SOS proof of this will first, okay, it's like five minutes. Let me just say like one sentence about it. Okay, so there's first like part zero, which is a pretty trivial. Like a part zero, which is a pretty trivial observation, which is just the sum of wa squared. Okay, so if I take the sum over i of w dot ai, I just get the sum over all the w's, right? Because the ai partition the space of m. So this, and I had this constraint in the program that the sum of the wi is m over k. Right, so this is literally equal to m over k. M over k squared. So now this is, and this is where all of the work is. I need to show that W A I W A J is a little o of 1 over k squared, I guess, times 1 over k squared. times 1 over k squared is not equal to j, right? Because if I sum over all the pairs where they're not equal, I'll get there'll be k squared of n. Okay, and then in order to do this, I'll do something that's kind of similar. I would have done something that was kind of similar to this like dirty cluster case. So the analysis will be similar to that, making use of this. But I won't go through it. Yeah, here in my notes, I have like a whole Here in my notes, I have like a whole abbreviated computation for that. I'm glad we didn't get to it. Okay, maybe I have like two or three minutes. So if there aren't any questions, maybe I'll wrap up by making some comments about the use of this certifiable sub-Gaussianity property in other situations. So you you assume Gaussianity, but is sub-Gaussianity enough for for for because I think the only thing you use I think the only thing you used was the symmetrix. Right. So uh so yeah, so so so I guess what what maybe I can clarify the question, like is sub-Gaussian enough in this in this situation? For the clustering problem, right? For the clustering problem. Yeah yeah for clustering mixtures of Gaussians it's it's enough. But mixtures of sub-Gaussians would that make enough? Also enough as long as you have a concentration- spectral concentration of this. Concentration spectral concentration of this. Yeah, all you needed was that this spectral certificate concentrates. Yeah. That's all you need. Yeah, and so in fact, even if you are willing to relax sub-Gaussianity and all you have is some kind of hypercontractivity, then you can do a very similar thing here, right? As long as you have a similar spectral certificate of hypercontractivity, you can take advantage of that for clustering too. This is basically all I wanted to say, right? This is basically all I wanted to say, right? So, this turned out to be a powerful idea, and it lets you, in situations where you know something about how hyper-contractive your distribution is or how well its moments behave, to bake it in to a sum of squares program and get better robustness guarantees. So, like, for example, for mean estimation, yesterday we saw how to get error squared epsilon. But in fact, if you're working in a sub-Gaussian situation, you can get situation you can get error epsilon to the 1 minus 1 over t if you do degree order t sum of squares using this kind of thing. And similar with like robust linear regression, in most of these situations if you're willing to take a hyper contractive assumption on your distribution you can get something out of it, right? It's kind of technique. So coffee