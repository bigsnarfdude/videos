Thanks a lot. I realize it's the first three talk slot that we had, and it's after lunch. But let's start with something light. I thought we're coming to an arts center. I should probably say that this is based on joint work with Ian Monroe from Waterloo, Pat Nicholson, and Musa Sirbach Bekna. The first thing I'll show, that is That is an automatically generated picture. I thought if you come to an art center, I have to show some nice pictures, but I'm just not talented enough to do it on my own, so I played a bit with Dali. It's very fun to do, and almost a bit creepy how well it works. Apart from the fact that this is supposed to be a small tree and I'm compressing trees, the connection to the talk is a bit superficial. Right. Right. I'll talk a little bit about something that I called hyper-succinct trees, only a mildly silly name, and I'll show you on a few examples how that works and briefly talk about beyond trees what you can do for graphs. And I'll not talk about the bonus part. But I had those slides prepared anyway, so I thought I can upload them if you're interested. Or you catch me if you want to learn about this after the talk. After the talk. For context, I want to talk a little bit about where this comes from. The paper that's behind this really had three fields that it took as inspiration. There's succinct data structures where people say to make it concrete, we have binary trees. So there's the set of binary trees of size n, and we'd like to represent one of such. Represent one of such one binary tree with the fewest number of bits we can for identifying any such, and that works out as two n bits as we discussed yesterday in the open problem session already. Now to make that a data structure, we also want to support some operations. So on top of these two n bits, I'm allowing myself a lower order term of extra space to support certain operations. Like in a tree, maybe you want to go to Like in a tree, maybe you want to go to the children of a certain node. And the standard representation, you would have pointers, an object that points to its left child, its right child, maybe its parent. But we can't afford to store those if we're using so little space. That's one branch. Usually here we think about the worst case space or a uniform distribution over these objects. The other branch is information theory, classically about strings or random sources. Random sources for objects, and you'd like to compress those, store them with the fewest number of bits. But in this case, depending on the probability. So, if we have an object, we'd like to store it with essentially log of one over the probability for that object many bits. All these logs are base two, in case it matters. And we brought these two things together using techniques from, I called it analysis algorithms, but put But put whatever name you like. So, what came out is essentially a way to compress, say, binary trees. I'll only talk about binary trees, but you can generalize some things to other classes. A way to represent a tree using often that many bits, but you can also support a lot of operations on them at the same time. The setup is somewhat clear. Somewhat clear. What's known about these data structures for specifically binary trees, say, is storing them in 2n plus little low of n bits and support a huge number of operations all in constant time if you allow yourself to play on bitwise tricks. There's different approaches how to get that. That's just for background. It's not really what I want to talk about. But it motivates. About, but it motivates a thing I'll define on one or two slides in the future. One approach to get these data structures to work decomposes a tree twofold. So there's the actual vertices at the bottom. They are grouped into micro-trees, which have a matching size. I'll come back to that. And those are combined into mini trees, and that's again combined into an overall tree. Into an overall tree. The sizes are chosen so that you can add support for these operations. That's not important for today. But I want to comment on this number again. It's important that the micro trees, the smallest level, is really small. And log n divided 4 is the bound that I chose here. The reason for that is if you have that few many nodes, you can encode that tree with 2 times. Encode that tree with 2 times that many bits, so log n divided by 2. And 2 to the power of that is still something small, root n. What does that mean? It means if you just look at these magenta little trees, there's only root n different local shapes. They can only look like root and different types locally. But of course, we have a lot of them. We have roughly n divided log n, little micro trend. And login, little micro trees overall, so there would be many duplicates. Also, for the data structure, in terms of the space usage that you get at the very end, the dominant part comes from storing how all these little micro trees look like. So I said we have n over log n of them. For each of those, I have to store what's the local shape, what do they look like internally. They look like internally, and that gives you the dominant contribution. Everything else, the thing I didn't even talk about, is a little low off n. That's the part you can just believe for now or ignore for now. The relevant bit is the size of the micro trees has to be chosen so that we can do this trick. There's not too many different of them. And the dominant contribution comes from storing what they look like. Now, Now, you can't just chop up your tree arbitrarily and hope that the data structure will still work. There's a few things that you need to get that going. And I'll just list that as basically our definition, what we would like to have. There's a magic parameter B, which will guarantee that the micro trees have this magic size, so there's only root and different ones at the end, because the micro trees can have up to two b, two b minus one vertices. 2b minus 1 vertices. We'd like that the number of microtrees is roughly n divided by b, so not too many microtrees, and all not too big. There are a partition of the vertices, so every vertex ends up in one of those little colored components. And um if you contract a component into one vertex, it only has three edges out going, one parent and one left and one right child, maybe some missing. One white child, maybe some missing. So, what that means is we can contract the micro trees into individual nodes and obtain a binary tree again. That's what I've shown here in shaded. There's two more properties that we don't need for now. Here's that example in Dicker again. B is 6 in this case. And so, let me briefly explain how this algorithm. Briefly explain how this algorithm works that computes these micro-trees. Because I think one goal for me for this talk is to show you that this is an interesting metric on trees that has a motivation from data structures and adds a few really interesting twists to analyzing it. So maybe this piques some people's interest. The procedure to compute these microtrees is bottom-up. Microtrees is bottom-up. We start at the leaves and we try to make them as big as we can. If we have collected B nodes in one component, we're happy and we close that component off if we can. That's what happens, for example, in this lit yellow one. We keep adding more nodes to the yellow component until we hit this point where the subtree has six nodes. And then we say, fine, I'm big enough, I can seal off that component. That component? It can happen that when you do that, so at this point, was 5 still too small? This one still too small. At this point, both together are actually 2B in the worst case. That's all right. We allow ourselves to go a factor two over the size that we target. There's also components like this one, which is a single vertex. How does that happen? We allow that to happen if both of the sub- If both of the subtrees are heavy, and heavy means at least B nodes in the subtree. The reason why we can afford that is to have two big children, there can't be too many of such vertices. They're basically forming a binary tree over the large parts at the bottom, so they can only be n divided by many such vertices. So the only part that's left is Is vertices like this one, where one sub-tree is heavy and the other is not heavy. And for those, we have to be a bit smart. We have to combine them. Because our tree could look like this. There could be a long chain of nodes with very little hanging down. And then, say, a big sub-tree here. So maybe this is one component, but we can't afford to make these individually components because there could be a linear number. Components because there could be a linear number of those. So we have to lump a few of them together. And the way this works here is I keep combining things if I only have a single heavy child, I keep combining until that component is big enough by itself. That red vertex over there is of the same type as a green vertex? Oh, the colors are just. I mean, the type type. The type is. You should think of each of the colored components as one micro-tree, but the colors have no meaning, they're just to show where the boundaries are. I just used as few colors as easily distinguishable. That red tree that she asked about? Yeah, that has two heavy subtrees. That's why it is fine to be a component of its own. Okay, you can contract that, and you see that's still a binary tree. Here's the algorithm to do that. It's a recursive procedure. The only tricky bit is, so you recursively compute the component from your two children, and there's two options. They're either already sealed as a finished component, so they're marked as permanent, or they're not yet marked as permanent. Or they're not yet marked as permanent, which means you can potentially add more things to them. But that is exactly what I explained. There's these different cases. If both children are small or light, you put them into your current component, you keep that open, only when it reaches the size B, you seal it off. If both are heavy, you make that a single component of its own. And if one is heavy, then you have to distinguish if the component that's passed up from the heavy child That's passed up from the heavy child is not big enough, you have to continue growing it. All right? Yeah. Previous slide, just for visual aid. Previously. In here, when you start at the bottom, right? It's possible that you could have options, different options where to start. I guess yeah, formally I'm it's a recursive procedure that starts at the root and At the root, and the way the algorithm is formulated. You start at the root with the recursion, but then things only start to happen when you come back up. So you return from the child with the two components, and then you build up from there. Yeah, you build up from the bottom up. Yeah, so the construction goes bottom up, but if you want the logic, it's a recursive procedure that starts at the root. Does that help? That help? So as a follow-up, so when we actually start making the components, it's when we've hit a a particular leaf. Is it clear which leaf, but just by looking at a tree you'll end up starting at? Yeah, I think so. Imagine you start at the root, you recurse on the two sub-trees, you keep recursing, and you have a return value. So it's at any point in time you come back with the return value of the two children. Return value of the two children. I think there's no confusion really possible in the algorithm. Okay, I'll try to rephrase this question. Is it clear if I just give you a tree and ask you to perform the algorithm, where is the first micro tree going to be computed? Oh, I guess add the left post. Yeah, left post. Okay, definite search. Yeah, yeah. That research. Yeah, yeah. Yeah, also, if the tree is fixed, then B is fixed, it's deterministic, and so on. Okay. So we know how to cut a tree into micro trees. I can use that to design a compression algorithm for trees as follows. Cut the tree up into micro trees. We somehow store how the trees are. We somehow store how the micro trees connect into the big tree, and I'll ignore how that is done in detail, but I hope you believe me that it's a little order n term, little O of n term. And the second thing we do is we have to store how the microtrees look like. Now, remember, there's only root n different ones, but we have n divided by log n many micro trees in the tree. So there will be many duplicates, and of course, they might And of course, they might not be equally likely. So, a smart way to encode them using little space would be something that exploits these duplicates. So, we can use a Huffman code, which is a best possible way to assign a binary string to each micro-tree so that no two code words are a prefix of each other. And it minimizes the overall length of the code if you write down all the codes for the micro trees. Micro trees. So the size of this encoding, I'm running this algorithm on a tree. The size it comes up with is the low of n plus for each micro tree, how long is the code word of its Huffman code, this encoding. And you can upper bound that in terms of the entropy, the empirical entropy of all these micro trees. So pretend each shape is a different character. You can just count how often each character occurs. How often each character occurs, those frequencies convert to probabilities, and you take the entropy of that. That's this calligraphic age. Yeah. So, sorry, if it's any question, I'm not so familiar with the paradigm, but is there some kind of static lookup table that you have all retreats of some size in? And you can use this code because you can always refer to your lookup table? Yeah, so that should be part of this. This, point C would effectively be such a lookup table. You get charged for the lookup table. The thing is, you know, the lookup table has only one entry for each possible shape. And so it's root n entries. And I don't know how big the entry will be, not too big, but it's this small O of, small little O of n term again. Yep. Now the question I'm asking is for what distributions over trees, binary trees in this case, is this compression best possible? So can I show that this micro tree encoding achieves log of one over the probability for the tree? And it turns out for a large variety of tree shapes or distributions over trees. Or distributions over trees, we were able to show this. In a way, for all distributions where some code is known that compresses that tree well, we could show it. It ended up to be a paper with 99 pages. So we didn't find a nice way to unify the analysis. It was more not an exhaustive case distinction, but an exhausting case distinction. So is it were you s saying the statement was that that Halfman code achieves the asymptotically the best possible value? Right, so I mean halfman code in general if I give you some alphabet letters with probabilities, Halfman codes give you the best prefix-free code. And that's good enough in this case. I don't think I have time to actually. I don't think I have time to actually go through all of the two examples. I had two distributions of trees prepared. One is what crept up yesterday already, the random binary search trees. You start with the random permutation, insert one by one into a binary search tree, and then what's the resulting shape of that? The other is a uniform distribution, but always. A uniform distribution, but over a very small subclass of all binary trees, namely those that are weight balanced. Again, this is something that takes motivation from data structures, but what it means is just at every node, the left size and the right size, the size of the left subtree and the size of the right subtree is an alpha fraction of the total size of that node subtree. They're nicely balanced in the sense that at every node you split the weight roughly into equal. The weight roughly into equal parts, or equal up to alpha. I think I might just show the first one. The ideas tended to be general in the sense that similar tricks worked for other distributions, but not in a way that we could unify it nicely. We talked about this yesterday. So in a random binary search tree, the rank of the root, well, how many elements go left and how many go right. Well, how many elements go left and how many go right? That's the rank is the number of elements left for me, plus one. It is uniform because it's a random permutation. Every element is equally likely to be the first to be inserted. And so we can write the probability for such a tree shape as this, product over all the sub-trees in the tree. It was convenient for us to formalize the generation of a random tree as follows. And this is maybe a model. It's maybe a model. I don't think it's known under a different name. So we coined the term fixed size tree source. You give me a number n of what's the final size of the tree. How many nodes would you like? And you fix for each n a probability distribution for the split. So I start at the root, you say I want a tree of size n. I randomly choose how many go left and how many go in the right subtree. Left and how many go in the right subtree. So for every n, I have to have a probability distribution over these left subtree sizes. That's what we denote by this p. So p of l, l goes left, n minus 1 minus l go right. For binary search tree, those probabilities are uniform, 1 over n for each choice of l if they sum to n minus 1. The analysis then always uses the following. Always uses the following trick. We first craft an encoding for the tree distribution that uses to the maximal extent knowing what the distribution looks like. So I design a way to encode a map binary tree to a binary code word using exactly how it is constructed. But then I show, okay, this is one way to encode the micro trees. I might as well replace that with my Huffman code, and I know that's an option. With my Huffman code, and I know that's an optimal code, so it's only better. So that stores the little micro trees in a good way. The second step one has to show is that you can go from the probabilities of the micro trees to the probability of the entire tree. And that's the step where you have to, depending on what the tree distribution is like, you have to work in different ways. The last step is just piecing inequalities together. Well, the binary search trees. So, this is again, I want to show a bit of this just because I think there are some nice ideas from how you design encoding. So, this is more information theory than anything else, and it's fairly basic stuff. But I'm not sure if people are familiar with arithmetic coding, for example. Even among computer scientists is not something well taught, but it's a beautiful concept and very simple, I think. Simple, I think. Let's see if you agree. The first thing we store is the size of our micro tree. And there's a way to store an integer so that it's a prefix-free code that you can start reading and you know when it ends. That's one thing we do. And the second thing now, for every node, we have to store somehow how big is its left sub-tree size. So we store the tree in the way it would be randomly generated. We start at the root. We start at the root, how many go left? And then we follow a depth-first traversal through the tree. So here's an example: tree. We start at the root. And arithmetic coding means you maintain a sub-interval of the unit interval, an interval of real numbers. And the second step will be how to translate that to a binary code, but bear with me for a minute. We start at the root. We know how big the sub-tree. We know how big the subtree size is inductively because we stored it. What we know from that is the left subtree size is somewhere between nothing and everything but the root. So there's five options in our case. And for the binary search tree model, they're all equally likely. So what we do is we split our unit interval into five intervals with sizes proportional to the probability. Proportional to the probability. In this case, they're all equally big, but if you have different distribution, you can immediately imagine that you can just use different sizes for these sub-intervals. And then we identify from the actual sub-tree size we find in this case, three, which of the intervals did we take? And here it's the fourth. And that is this sub-interval. So I'll zoom into that sub-interval and continue. Now we're at node VT. Now we add node v2, which has subtree size 3. Why? Well, we just encoded that. So we just know, and because we go that first traversal now is always the last size we store. There's three options, what this left sub-tree of that node could be. And in this case, we see it's the middle one. And each of these choices again corresponds to a sub-interval of length proportional to the probabilities of these outcomes. And so, again, here we have. And so again, here we zoom into that little sub-interval. For the remaining three nodes, we inductively know they all have sub-tree size one, and that is nothing to choose, so we don't actually store anything about those. They're encoded with zero bits. So we're left with this interval. That's just a sub-interval of the unit interval. How do we make that into a binary code? The simple trick is to find The simple trick is to find what I would call a dyadic interval. So an interval of this shape. It's an integer divided by an integer power of 2, and then the integer plus 1. So you can always find the largest such that's entirely contained within your interval i, and then use the representation, the binary representation of m as your code. How good is that? Well, if you have an arbitrary interval, if you're unlucky, you may have. Interval, if you're unlucky, you may have to just cut that in half twice before you find such a dyadic interval that's entirely within. But that's the worst case. So you can store the length of this encoding is the length of the log of one over the length of the interval plus two in the worst case. Are you adding that stress switch instead of with? Because you're always storing the left. Yeah, it's important that I'm following. That I'm following the way the tree is generated. I start at the root and I generate the split. And because I now know the split, I know how to continue here. If you don't do depth research, maybe slightly different order also works, but this is the easiest to keep track of. So you can also decode this in the same way. Okay, I think what I already argued is we Argue is we shrink the interval by a factor that's proportional to the probability of the outcome that we found. We always zoom into one sub-interval, and we chose the intervals to be as long as the probability of the event. So that means the final interval we get has length exactly the probability of the entire tree. And so we can store that with log one over the probability plus two. With the probability plus two. That gives us a code for the micro trees. So I score the shape of each micro tree. Yep. So this final interval, okay, it looked in the picture like you keep going down and you keep getting these sub-intervals. But then are you relaying this back up? Is there a you said there's a final interval that somewhere codes the whole size of the whole G. What where does that what I mean by final interval is just Final interval is just this i that you end up with at the end of the process. At each node, you go from i to a sub-interval of that. Go smaller, smaller, smaller. Down the tree. You follow a depth-first traversal. You can also jump back up. You go from one, two, three, four, five. Uh in our case, three to five didn't change the interval, but in in general you you can't jump around the tree. You can't jump around the tree as a traversal would. Okay. I don't completely follow how the journey goes, but I'll ask you later. Okay, because I'm running short on time. We have a code for the micro trees. We can uply bound the Huffman code by whatever we found for this specific code. Code. The other step is to go from probabilities of the micro trees to the probability of the entire tree. And for that, there's a problem that when we encoded this yellow micro tree, we should have used, at this point, the whole subtree size. What we did use is just what's within the yellow subtree. We didn't see this red one. So we used the wrong numbers and we kind of screwed everything up, you see. Kind of screwed everything up, it seems. And what saves us here is that for some of these probabilities for splits, you have a monotonicity property. That means, yes, you use the wrong numbers, but they are working in your favor. They make the inequality in the right way. So you can start with the product of all the microchip probabilities, split that up as a product over all the nodes, and then have the probability from the random. Have the probability from the random generation, what's the number of nodes that go left and go right? Here with the sub-tree sizes relative to the micro-tree, and because of monotonicity, you can go to the sub-tree sizes in the big tree and you get an inequality in the right direction. And then the last step is just piecing this all together. So it gives you the number of bits to store the entire tree is upper bounded by this. Is upper bounded by this plus a lower order term. And what I said yesterday, for random binary search trees, we know that what that size is. That's the series I promised yesterday. In many other cases, we can prove that the encoding is best possible, but we don't know how much space it takes. So it often gives you a way to prove that the encoding is optimal without knowing what that means. And I think I'll leave it here. And I think I'll leave it here. I have a super quick question. So did I see the area? So we assume that M is a bit below M? M was the number of, sorry, yeah, one correction. M is the number of microtrees. M is the number of micro trees. So is it a little log n? Yes. N divided B, so N divided log N. Yeah, sorry. There's a lot of parameters flying around there. So I guess the rest of the discussion here can be taken offline because now it's time for the afternoon. You can have my break. Thanks again, Sebastian. 