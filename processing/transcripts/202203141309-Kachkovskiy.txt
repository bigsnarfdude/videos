Welcome back. So we're happy to have my colleague Ilya Kuchkovsky for the final talk of today. And he's going to tell us something about quasi-periodic operators and Anderson localization. Thank you, Ilya. All right. Thank you, Jeff, for the nice introduction. And thanks to all organizers for the opportunity to speak here. So far, I've been enjoying the talks a lot, and I hope to attend as many as I can. So I'm a mathematician. I'm working in mathematical physics. Working in mathematical physics, in particular, spectral theory of Schroesinger operators. And I'd like to give you some kind of overview of the area of spectral theory of quasi-product operators. I'll try to be as self-contained as possible, but I also give you a flavor of some proofs. And related to the topic of this conference, so right now we don't have anything related to quantum circuits, but there will be some speculation between Between what's random system, what's non-random, and what can happen in between. Okay, so what the object that we are working with is called a discrete shading reparator. So it's a Hamiltonian on an integer lattice described by this formula. So it describes a single electron that can hop on the sides of it. That can hop on the sides of an integer lattice. The hopping term is described by the discrete Laplace operator. And there is a potential at each side of the lattice. So far, just some real valid function on the point of the lattice. So in textbooks and quantum mechanics, it's called a tight binding approximation. Roughly, it means that the interaction between the electron and the lattice sites is so strong that the electron spends most of the time Strong that the electron spends most of the time around one of these points, and we can kind of neglect the movement between the sites. So, in some effective models, you'll have a discrete operator, which is a discrete operator, which you can compare with the operator in the continuum. Okay, and we'll be working on infinite volume. So, and the general question that we're interested in in spectral theory is. In spectral theory is basically how to diagonalize the separators and what the properties of this diagonalization. So informally speaking, we want to know where is the spectrum of the separators. So what are the generalized eigenvalues and which correspond to the energies that correspond to the eigenstates? And so the reason specific that comes with minute volume is that you can have Council with metrolem is that you can have different types of spectrum. You'll see from the examples that you can have discrete point, absolute continuous singular continuous spectrum, and all of them have some kind of physical meaning. Okay, so just as an illustration, why do we want to diagonalize the separators? Well, because we want to solve the Schrodinger equation. So this is the simplest case of the Schrodinger equation for simplest case of stationary. Station simple case of stationary Schrödinger equation where Hamiltonian does not depend on time and we assume that the Planck's constant is equal to one and you can formally write the solution of this equation as e to the ith times the original data treated as the OD where e to the ith is the exponent of the operator and if h was an operator in finite dimensional space then you can just you could just say that the exponent of the operator You could just say that the exponent of the iterator is the iterator with the same eigenvectors and eigenvalues, which are exponents of the eigenvalues. In fact, you can do it with any function. So this formula works in the finite dimensional setting or in infinite dimensional setting when we have actual eigenfunctions that belong to L2. And in general, there is something called the spectral theorem. So to each operator, you can So, to each operator, you can associate some kind of projection-valid measure such that the function of the operator is the integral of the function against the measure. And in the case where the spectrum of the operator consists of eigenvalues, in the usual sense, this meta just becomes a point measure supported in the eigenvalues. So, I will not give more specific details about this because, in the end, while the spectral In the end, while the spectral theory of the operators that we're talking about is very rich, mostly we'll focus on the situations that corresponds to localization where we actually can have eigenfunctions. So other cases will be more or less for comparison. Okay, so let me give you two examples of kind of on the opposite ends of the spectrum, what can happen. Examples of Schrödinger operators. So the first example is so-called Anderson model. So again, we're considering. Model. So again, we're considering a system with this Hamiltonian and the valids of the potential are independent, identical, distributed random variables. And in this case, there are many results. Yeah, so I'm because there is a huge amount of results, sometimes I don't cite particular names, but I will have a list of several review papers in the end. So, in Anderson model, it's known that in dimension one, you basically always have localization, which means the spectrum is purely point, and the eigenfunctions decay exponentially. And in higher dimensions, you have it if the Hopping epsilon is small. And what happens if it's not small? It's a big open question. Okay, so this is one extreme case. So in this case, there are several different characterizations of position. You can talk about it in the dynamic setting. So in a way, you can also characterize it as that there is no transport. So if you have some initial data that's localized in some region of space, and then you start quantum evolution, it will sort of. Position, it will sort of approximately remain in the same area. Yeah, and by the way, I probably didn't mention it, but these are random variables, and this theorem holds the probability one. It cannot hold for all samples because of the second example. And the other end of the spectrum is the periodic case. So, this case is kind of very random, and the periodic case is very deterministic, a deterministic. Deterministic as deterministic as it gets, and in this case, we have some opposite situation. If you look at the eigenstates, so there are no eigenfunctions in the strict sense, but there are generalized eigenfunctions, which look like block waves, block waves or flat waves. And these functions are basically delocalized. So you can look if you want to do some calculations. So the simplest case would be. Calculation: So, the simplest case would be the constant potential, and if it's constant, you can just replace it by zero potential, and then the eigenfunction will basically look like psi n e to the two pi i n theta. So, where theta is the momentum and is the argument of the eigenfunction. Function. So this function do not belong to L2, so they are delocalized. And in terms of transport, we have something opposite to no transport, which is called ballistic motion. But I won't go into a lot of details. Okay, so very roughly, we are interested in different cases of what can happen in between, between these two extreme situations. And again, there are several different answers. And so, one of the answers would be that we can consider potentials which are so-called ergodic. So, I'll give you an example on the next slide. Basically, ergodic potentials are potentials which are generated by some process which is not random but which shares. But which shares some properties with random processes, some, but not all protices. And then you can expect that depending on the properties of this process, it will be either close to this case or closer to this case. So it's either localization or delocalization, AC spectrum, or there are several different situations in between. And again, I won't give you. And again, I won't give you a general theory of ergodic potentials, but I'll give you the class of examples which demonstrates all possible properties, which is called quasi-periodic operators. So we'll start from analytic quasi-periodic operators. Okay, so I mentioned that we are taking some prototypes which shares some properties with random prototypes. Properties with random processes. And this process will, in this case, will be a rational rotation of the circle. Suppose you have a circle and you're starting from some point in the circle and rotating it by an angle, which is not the rational multiple of two pi. It looks like there's a question, if you don't mind in interrupting. Oh, yeah, sorry. Yeah, please. Can I ask you a quick question? You talked about this periodic potential, and you said there's a block waves, and you have these extended solutions. And you have these extended solutions, that's irrespective of dimension. Okay, so this particular solution happens in any dimension for the case of zero potential. Yeah, sure, sure. But like the periodic case, if you said something. Yeah, so okay, so periodic, it has to be periodic with respect to all directions, if that's your question. And then what is the statement that it's delocalized? Yeah, the spectrum is absolutely continuous. The spectrum is absolutely continuous. So the eigenfunctions will look basically a little bit more complicated versions of this. I see. So you could have a very large period A and within it have randomness, fully IIDness, and then repeat that. Oh, no, no, we don't have randomness. So the potential is completely fixed. Can you go up a little bit? Maybe I have a very simple confusion. Right here, yeah, right here. Very simple confusion right here, yeah, right here. So, if I take a to be 10, yes, and so that gives me like 10 values of v I can choose. Can it repeat? And yes, so regardless of which values you choose, the spectrum will definitely be random IID Gaussians, for example. Yeah, if you want to take them, you can still take them random, but for each sample of random variable, there will be some kind of block waves, just have. Some kind of block waves just have will have slightly different shape. So, in particular, as I make A larger and larger, I can introduce more and more randomness, but they still will get a continuous spectrum. Yes. For any A that is, I guess, finite. I guess in some limits, as A goes to infinity and in dimension one, for example, there would be something. Well, if you if A goes to infinity, then you can get anything. You can get anything, but they can the sense of yeah, so the operators you can you can approximate operator, let's say, in a strong topology by periodic operators, but the spectral properties do not survive very well such approximations. All right, so I can take away this way. If A is finite, you know, you always have this continuous spectrum and below. Yes, that's correct. Yes, very cool. Thank you. Yes, so I just show it as a little. Yes, so I just show it as an illustration that if so, this is kind of the periodic sequence is something completely opposite to random to random sequence. So in random, you cannot predict the next value and in periodic you can predict it perfectly from the previous values and it repeats itself. So, okay, so yeah, so let me get back to the quasi-tragic case, quasi-tragic cases where Cases, quasi-project cases where we take a process which is generated by irrational rotation on the circle, then you can see that if you iterate this rotation, then the trajectory is going to feel the circle in a dense way eventually. And the distribution will kind of approximate a random distribution in some sense. So, for example, if you count the average number of points. The average number of points in a given interval, if you take a very large number of iterations, it will approximate it will be proportional to the size of the interval. So some of the things with this operator are similar to random ones, but some of them are obviously not similar to random ones because whatever picture you have here is going to be a translation of the picture you have here. So you can have different domains of space which are very far away, but they're kind of Which are very far away, but they're kind of perfectly correlated. So there is no, if you have a random system in take two boxes in space, which are separated, whatever happens in them is going to be independent from each other. And in quasi-periodic case, they're going to be translations of each other. So, but translation in the sense of the parameter. Okay, so this is kind of the process we are going to use: rotation of the circle. Rotation of the circle, and the operator associated with this process will formally look like this. So, let's take some function, some real valid function on the circle, and let's construct a potential on the lattice by sampling this function over the trajectory of rational rotation that starts at some points that we have chosen in both. So, here I'm talking about. So here I'm talking about the latest version of this thing. So I have to, I have several frequencies and I have to write it like this. But in one dimensional case, it's just going to be f of x plus nm. Okay, so for technical reasons, sometimes we assume, well, we always assume that the components of the frequency are independent over rational numbers, because otherwise separator will be periodic in some directions, and this is not what we want to discuss. This is not what we want to discuss. And sometimes we don't want these numbers to be too close to rational numbers. So we assume some kind of deferencing condition. So if I get to that, I will explain where this condition comes from. But if this condition is not satisfied, then certain things can behave very differently. So a general quasi-project operator is an operator that looks like this. So usually So, usually we consider it as a family of operators that depends on the point x, that's the initial point of the trajectory. So, for each x, we have an operator with the Hopping term, that's just the usual discrete Laplacian, and the potential, which is obtained by the process that I described. It's natural to consider the whole family, not just a particular sample of the separators, because of the nice relation with translation. Of the nice relation with translation. If you translate x by a multiple of omega, then it's equivalent to translating the whole lattice by whatever integer vector we have as a multiple. So this sort of covariance property is very convenient if you study restrictions of the operator to boxes, if you ever get to that. Okay, so now let me summarize. Now, let me summarize a little bit what is known about the spectrum of such operators. So, and remember that the random case we had the regime of small epsilon and not much is known for large epsilon. Here, well, so the first class of periods that we consider is the class where f is analytic, so this trajectory, so this depends. Dependence x is kind of very nice in this function, and turns out that in this case, you can study it in two asymptotic regimes. You can study it in the regime of small epsilon and large epsilon. And there are a lot of results known. So basically, if the epsilon is small, then you have localization. So you have point spectrum. And if epsilon is large, you have in some cases. Is large, you have in some cases, in many cases, AC spectrum. Some intuition that you can observe is that, uh, okay, so if epsilon is small, then you can at least try for a second to forget about this term. Then it's just a diagonal operator which has perfectly localized states. And if you epsilon is large, you can rescale and forget about this term. Well, you can't really forget, but you could pretend to forget. Forget, but you could pretend to forget. Then you get the free Laplacian, and then we get the block waves that I mentioned before. So the actual way of forgetting one of these terms, it's a pretty complicated thing with perturbation theory, which involves small denominators and requires this type of conditions, which is something I might get to. Okay, so. Okay, so we can see there is a little bit of a difference that in random case we have kind of one regime where everything is known. And here we have two regimes. And the reason is that analytic quasi-paradic operators, there is some kind of duality between the regimes of small epsilon and large epsilon. So yes, let me first say about duality. It's a little bit okay. So let's look at this. Let's take our function f that generates our potential. It's an analytic function in the circle. And let's consider its Fourier series. So then you get a sequence. Then it turns out that you can do some kind of a Fourier, some kind of a twisted version of a Fourier transform and relate this appearance. Relate this operator with so-called dual operator. So the formula looks a little bit scary, but basically you can look at it the following way, that here you multiply psi n essentially by f or something that's associated to f. And if you have multiplication, do Fourier transform, you get convolution. So it's not exactly multiplication, but this is kind of intuition behind it. So if you do this transform, you get This transform, you get the operator that looks like a convolution-type operator. And if you transform the Laplacian, so discrete Laplacian in the Fourier series representation has looks like a multiplication by cosine. So, and then basically the roles of kinetic and potential energy change places, and you can kind of transform. And you can kind of transform results in one regime into another regime. But then, because of some kind of Fourier transform, localized states actually transformed into extended states. So there are a lot of papers about it, and it's what I said is very non-rigorous, but this is kind of the idea behind the fact that there is a regime of small epsilon, large epsilon. And an important example here is so-called almost matriarchator. Almost made to operator where f is this cosine function, and you can see that this model is actually self-dual. If you apply this transformation that they described below and dimension is equal to one, then these two terms basically are going to change places. And this is one of the reasons why this model has been studied very well. So it's not just in the asymptotic regimes, but basically there is a almost complete spectral description. Almost complete spectral description for all values of epsilon and for a lot of values of the parameters. So, Zeta Merskaya will give an ICM talk about it, probably. She already gave ICM talk about it before. Okay, so this is kind of a summary of analytic operators, and you can see that depending on the parameters, they can be localized or they can have continuous spec. There are also some intermediate regimes. There are also some intermediate regimes where sometimes you can have singular continuous spectrum, but I don't want to discuss it right now. Maybe if I have time, I can mention it later. There are also a lot of interesting things going on. Okay, so this is analytic operators. And the goal of this talk is to discuss a different class of operators, also quasi-periodic, but of a different type, which we call monotone quasi-periodic operators. So again, it's the formal notation is the same as here. So, well, same class of operators, but we're looking at different types of function f. Instead of the analytic function on the circle, like a cosine, we will look at functions that are non-analytic and the original. The original example from which it came from is so-called Maryland model. So, Maryland model is the quasi-project operator with the potential generated by the tangent function. So, do I have a graph? Yeah, I have a graph. I'll show you a graph in a moment. So, and yeah, so Maryland model, I think it. I think it was studied since the 1980s. There is a chapter about it in the book by Psycho Friedrich Christian Simon. So it was one of the first examples of quasi-project operators where people could prove localization. So the chapter is called Point Spectrum for Maryland Model. So, and it's known that this model has a localization for all Japanese frequencies and for all couplings. Consistency and for all couplings. So there is no small and big coupling regime. So it has a causation for all coupling constants. And yes, so in dimension one, it has been studied by Jitemuskaya Levo in basically full generality. Yes, and I think it's called Maryland model because it was invented originally by some physicists somehow associated with the University of Maryland and sort of And sort of the point of it was that, in some sense, the special form of this function f made it somehow integrable, and you could calculate certain quantities explicitly. But the calculation relied on the particular shape of the tangent function. And what we are interested in is generalization of this. Yes, so first it's an interesting phenomenon that if you change the shape of the function, If you change the shape of the function, suddenly instead of two regimes of large and small coupling, you have localization for all couplings. And we wanted to know how far you can extend it if you replace tangent by general function. So if you replace the general function, then this integrability thing does not work. So you have to do some kind of qualitative proof rather than direct calculation. So, and this is So, and this is the class of models that we want to consider. We want to consider quasi-project operators with monotone sampling functions f. So, we consider two types. Merilland type is when they go like this. And there also some work that we consider functions go like this. Okay, so and our question is: what is a spectra? In the spectra, what are the spectra of such models? And there is some general theory that says that in none of these cases, well, basically, you cannot have absolutely continuous spectrum. So, for this, in this case, the reasons are different, but it's some general results which do not rely on much of the quasi-product structure. So, still, it does not imply localization because there is also a possibility. Because there is also a possibility of seed workers in the spectrum, which is something in many ways really ugly. Okay. So, and there are basically three approaches to these operators. So, Maryland type operator were approached in the 1980s by Belista, Lehman, Scoppola, by KM-type methods for small epsilon. For small epsilon. In dimension one, there are very complete results. So, by myself and our joint work with Jutomirskaya, basically in both of these cases, you can have localization at all couplings under some mild restrictions. And the more new stuff that we've been working recently together with Parnovsky. Working recently together with Parnovsky and Sternberg is a different approach to Maryland-type models, which is a little bit in the spirit of KM, but is more direct. You will see. So basically, as a result of this approach, we can also prove localization in this regime of small epsilon under some regularity assumptions on this function. But the advantage of this approach But the advantage of this approach is that the method of proof is very direct. So basically, we try to construct eigenfunctions using some kind of a naive procedure, and then it turns out that that procedure converges. Okay, so now I'll talk about that procedure. So there will be some calculations, but But again, they will be relatively self-contained. And I hope you'll get a little bit of a flavor, even if these calculations are too technical. Maybe someone has any questions before I go to the next section. Yeah, so the motivation for these results is that unlike analytic case, Unlike analytic case, can I ask you a very quick question? Yeah, since it's a naive question. I mean, this may be really off, but is at the end of it the reason that the Maryland type and sawtooth have very different behavior, that the tangent function is unbounded? Okay, so I think if I get sufficiently far in the talk, it will be very clear what's different between. Will be very clear what's different between these two behaviors. So, I mean, I can try to answer no, or I can postpone it to the page when it actually shows up in the proof. It's up to you. But there is no short answer, yes or no? Or is it like a big part of the story? Well, part of the story, the answer is the following. I can say it. So can I do this? So can I do this? Yeah, so basically it has something to do with the fact that, yes, this is a cosine. This is a, let's say, sort of. So when you try to do perturbation theory, any kind of KM argument, so you sort of start from some point and And you kind of get in trouble when you translate by multiples of omega, and when you get to a point which is on the same level, so then you get something that's called a small denominator. Again, you'll see it from calculation, but that's the idea. And so in the soul-tooth type, say again? We're talking about like vicinity of the asymptotes of the tangent. Like vicinity of the asymptotes of the tangent. Would you say you get small denominators? Well, so you'll see what exactly I mean by small denominator, you basically you get the difference between height of this point and height of this point in the denominator in some calculation. You don't want it to be zero, you don't want it to be small. So in the case of saw-tooth potential, the only way to reach a point on the same A point on the same level is when an omega is close to integer, to an integer. So if you translate this point by multiple of omega, omega, I don't know, two omega. So if you end up on the same height, it means that you are close to the original point model of the period. In case of the cosine, there are two possibilities. So you can get to point Point in the period, or you can get on different branches. So, in the second case, we're getting different branches called the resonance. And dealing with resonances is much harder than dealing with small denominators of regular type. So, that's kind of the difference, but you're probably going to get a better picture after the proof. Thank you. Yeah, thank you. Okay, so let me describe some ideas behind the calculation that proves the collision in some classes of model and type models. So it's actually the calculation itself does not necessarily, it does not use any quasi-predict structure, it starts from a completely in a completely deterministic setting. So it's so-called relay sharing or perturbation series. And it's commonly And it's commonly used in quantum mechanics, quantum chemistry, and other things, although maybe not in the generalities that we are considering. Okay, so suppose you have some operator on L2 of a lattice. For supplements, let's just consider lattice to be one-dimensional. It's going to simplify some constructions, but it's not really important for diagrams. So the operator is a diagonal operator. So the operator is a diagonal operator, multiplication by potential plus epsilon times the Hopkin operator. So, and let's assume the following condition that all the values of the potential are different from each other. So, this condition is satisfied, for example, the monotone quasi-periodic case because we are shifting by irrational. Omega is not satisfied for the cosine, but if you choose carefully the initial point, it will be satisfied. This is something we call algebraic non-risk non-condition. So, and then let's try to diagonalize this operator. And to do it, let's use the following intuition. So, suppose that epsilon is equal to zero, then I'm just a diagonal operator, right? So, I can value this otherwise. So, eigenvalue is otherwise the potential, eigenvector the standard basis vector. And now, so if you have an operator with isolated, if one of the VNs is isolated from the remaining ones, then there is then the corresponding eigenvalue will be analytic in epsilon, and we can write down the power series for the seigen value. In our case, because the potential is kind of quasi-periodic, none of them was going to be isolated. But we can pretend that they're isolated and try to write down a perturbation series. So, and with the outpost regionality, let's assume that we are perturbing the eigenvalue associated to the zero largest point. And let's shift everything so that. Shift everything so that eigenvalue is equal to zero. Then we are looking, basically, we are solving the following equations. So we want to find the eigenfunction of the operator H, which is a perturbation of the standard basis vector support at the origin. And we look for it in terms of a power series. So this is our operator, H. This is our operator, H. This is the eigenvector, which is a power series. This is the eigenvalue, which is also a power series, applied to the same eigenvector. So, and to make the solution unique, we need to impose some kind of normalization condition. So, in this case, we assume that all corrections will be orthogonal to the zeroth component of the eigenvector. Okay, so then we can try to solve these equations. So, the way we do the same thing. These equations, and so the way we do it is just we've opened the parenthesis. So, in the left and right-hand side, you have a formal series epsilon with finitely many terms at each power of epsilon. And we'll just try to make them equal to each other, to force them to be equal. So we'll get an infinite system of equations, and we'll solve them one by one. And turns out that this algebraic non-resonance condition will basically guarantee you that there is a unique solution for the coefficient. That there is a unique solution for the coefficients. So, let me illustrate it by some calculations. This is where it starts to be a little bit technical, but it's sure that for any of you, it's actually easier to do it as an exercise, but I still want to show them to get some kind of flavor. So, if you get, yeah, this is our Egan Mola equation, which generates the system of equations. So, if you look at the So, if you look at the zeroth order, then there is nothing interesting. We just get the initial approximation where epsilon equals zero. So, if you look at the order one, then we get two terms on the left-hand side, two terms on the right-hand side. We know that zero energy is zero because of our change of normalization. So, this term goes away here. And then, what do we do? So, we project Project the equation onto the zeroth basis vector and project it on the orthogonal component. So, for example, here let's project it onto the zeroth basis vector. What do we get? So what do we get here? Psi zero is equal to E0. If you apply a Laplace operator, it's going to propagate from the origin to all neighbors, and there is nothing going to be left of the origin. So if you project this. The origin. So, if you project this into the origin basis vector, you get zero. Here, you also get zero because by assumption, psi one is orthogonal to the all corrections are orthogonal to the initial perturbation. And this is just a multiplication operator. It's still going to be orthogonal. So, the left-hand side is going to be zero. The right-hand side only has one term, which gives us the conclusion that the first correction to the energy is zero. Which is actually something, if you've seen this before, you might remember that in many models, the correction to the energy starts from a quadratic term. This is an illustration of that. So, we found the first correction to the eigenvalue. To find the eigenvector, we need to project onto the orthogonal component to the basis vector. So, and then the potential V is a multiplication operator on the which Which is zero at the origin, but it's not zero everywhere else. So, on the range of this projection, we can invert it. So, we get this equation, we divide by v, and then we get in one dimension two terms. So, it's going to propagate into e minus one and e one. Okay, so this formulas with negative d negative. Formulas with negative v, negative one, negative v1 look a little bit weird because there is negative in front. But remember that we normalize the potential to be zero at the origin. And then in general, you'll get these expressions of the denominators, which is something which might be more familiar. Okay, let's look at the second order now. Look at second order, we get three terms again. So let me scroll real quick. Again, so let me scroll real quick. We are still solving this eigenvalue equation in the third, in the second order of perturbation. So we get this. Again, there are several terms that vanish automatically. This one vanishes because it is 0, E0. This one vanishes because of our previous calculation. So if we project it onto the whatever is at the origin, we get that the second correction to the energy looks like this. The energy looks like this, and we also get the correction to the second correction to the eigenvector. So the eigenvector will propagate into the points two and negative two. So there will be two terms. Okay. So now let's talk about what happens in the general case. Okay, maybe are there any questions about this calculation? I mean, I think it's relatively straightforward, but maybe it's just too fast. I don't know. No, it's great. And that's how it's great. Just a quick question. Do you sometimes perturbation theory, you know, things work up to certain order? But the fact that you have, well, when you make calculations of the Well, when you make calculations of the power series, that's polynomial in epsilon, but the terms can grow combinatorially. So there's like sometimes there's a divergence beyond the point. Do you have similar problems here? We have a lot of problems like this. Yes. So if you look at the individual terms that we get, for example, if you treat these things as individual terms, they will hopelessly grow by several. Grow by several reasons. So, first, the number of them will grow, and second, eventually, these denominators will become small. This is the small denominator I mentioned. And so, but yes, just looking a little bit ahead, if we gather all the terms at given power of epsilon, which is a natural thing to do, then we will actually get some kind of cancellations which I'll try to describe. Which I'll try to describe that allow to prove the series converge. So that will be the main point. So, but I need a couple of steps to get there. But I hope I will actually describe the flavor of it. Okay, so now let's look at the anth term. Yeah, so just this little bit of illustration that we sequence in which we solve the equation. We start from E0, psi 0, then E1, psi 1, E2, Psi2, and so on. Psi one, e two, psi two, and so on. So this is again the equation that we are solving. And if you open the parenthesis and calculate the end term of perturbation theory, you'll get this. So I just opened the parenthesis here, and that happened. And I can observe immediately that all the odd corrections to energy. All the odd corrections to energy will be zero. The reason is that this discrete Laplacian operator, kind of in a certain sense, is even. So to get back to the origin, you need to make even number of steps. It's kind of related. Okay, but then let's try to solve these equations. And again, we solve this in two parts. So we project it onto the original side, the origin. Site the origin to get the eigenvalue, and then they project it on the orthogonal component to get the correction to the eigenvector. So, an eigenvalue is nothing much to look at. Basically, everything here, except for this, will be zero, because all these things involve corrections to tigen vector, which are talking about to the origin. This is going to be non-zero, this is going to be zero. Is going to be non-zero, this is going to be zero. So basically, looking at the origin just gives us that the correction to the eigenvalue comes from the correction to the eigenvector on the previous step, but we need to make one extra step to get to the origin. So, but this is kind of just saying that the actual calculation was done on the previous step. So, the non-trivial stuff is there. Then the non-trivial stuff is the correction to the eigenvector. So, if you look at the correction to die in vector, again, we what do we do? We move this Laplace to the right-hand side. In the left-hand side, we get V psi m. And because psi n are talking about to the origin, we can divide by v, apply v inverse. It's an invertible operator on that subspace. So then we get psi n equal negative v inverse delta psi n minus one plus a bunch of. Plus a bunch of other terms. Okay, so then let's introduce some language. So, this term I call the simple loop term and these terms I call attachment terms. So, the notation will be clear once I describe the calculation a little bit more. So, basically, what happens is that to get sand, we get some kind of recursion. Same, we get some kind of recursion. So q expressed to the sum of these terms, and each of these terms itself we apply recursion and expand it further. So eventually, if we go back to the original psi zero, we'll get a lot of different terms. So we get some kind of complicated chimaterial structure where each term corresponds to the sequence in which we apply in these recursions. So to the choice of the term. So, to the choice of the term at recursion step that we are making. And to start this combinatorially, let's first look at what happens if we only use this type of recursion every time. So if you use simple loop term, meaning that in the recursion, we always choose the first term. Then what happens? Well, I actually kind of wrote it here. Of wrote it here. So psi n is negative inverse delta psi n minus one. So if we expand, I mean, assuming that we only we ignore the remaining terms. So then if we expand it further, we get negative inverse delta, negative inverse psi n minus 2, delta psi n minus 2, and so on. So then you can see the structure. So eventually we'll have to. So eventually we'll have to go back to psi zero, right? And to get to the eigenvalue, once we calculate psi n, we have to sort of return to the origin. So we need to calculate what contribution what this function has when we come back to the origin. So and if you look at this expansion, you can see that it actually corresponds to all possible Corresponds to all possible paths that start from origin and from origin. So each time they make one step, the number of steps is equal to the order of the perturbation theory. And each time we make a step, we multiply, we apply the inverse, which means that we are multiplying by the, we are dividing by the value of the potential measured to that particular point. So graphically, it can be described like this. So this is the example of a term at epsilon to the eight. So it takes eight steps. In this case, we start from the origin, we travel to the point four, and then travel back to the origin. And yes, so each time we visit a non-zero vertex, we get what we can call vertex factor. And each time And each time we have an edge corresponds to applying Laplacian, essentially, it means that we are getting a factor of epsilon. So you can't see it here because this is a coefficient of epsilon to the power eight. But it just to illustrate that the number of steps is the order of perturbation theorem. And this diagram corresponds to this term eventually. So the structures that you visit before once and all other points you visit twice. And all other points you visit twice. In higher dimensions, it can get a little bit more complicated because the path we can return to the origin using a different path, but the general kind of trail structure is the same. We just need, there are just more paths. Okay, so this is already quite complicated, but it's not all because if you remember, we considered only the recursion of the step. Of this type. So, what happens if we are using this type of percussion? So, basically, it means that we allow a little bit more complicated configurations. So, I want kind of to keep the firm here. Well, so what's important here? So, basically, first, instead of minus, we get a plus here. Second, we are having an x. Second, we are having an extra factor of inverse without making an additional step. Yeah, and then we get basically correction to the energy times correction to the eigenfunction of smaller order. So, in the end graphical, it's convenient to represent it using attached loops. So, basically, this is the illustration where in the recursion. Where in the recursion we chose a non-simple loop term once, and then all other steps we chose simple loop terms. So it means that at some point we had, I guess, here we had E4. Yeah, we chose basically this term at one step. So we get E4, which means that we have a factor from a loop of length four. A loop of length four, and the original loop decreased its size because the total number of steps should be still the same. So, this diagram corresponds to this term. So, you can see that this structure is this loop, this structure is this loop, and that extra V factor comes from this part, so there is kind of attachment factor. So, this kind of attachment factor produces extra V inverse without negative sign. And if you look at the whole picture, you can do this multiple times. So, in the end, you get some kind of a tree of loops. So, you can to this loop, you can attach another loop, and so on. And this basically, you can repeat this indefinitely, but well, not indefinitely, because at each order of epsilon, the total number of steps. The epsilon, the total number of steps is still the order of perturbation. But the comatorics is that basically you want to look at all possible trees of loops of the step. Okay, so that was the structure of the series. And this is a general thing which works, which has been known, maybe not in this form, but has been known, and that works in a lot of different settings. So now what happens with the quasi-periodic situation? So, now what happens with the quasi-periodic situation? So, am I right to have approximately 10 minutes? Yes. Okay. So, let's apply this scheme to a quasi-periodic operator. So, now the n is obtained by f of x plus an mm. So, well, first, let me let's ask a general question about convergence. So, if So, if Vn is an isolated tag in Valley, or in our case, we're. So, this graphical form that you organized the perturbation series, were you the first to notice that you can do it in this way? No, so it's, I think there are several different papers in physics. There is a paper by Arnold, but every paper uses kind of their own form, but they are equivalent. But they're equivalent. So, so no, it's not us. So, we kind of used a little bit different form than other authors, which is convenient to illustrate our constellations. But in general, this is something that's been known for probably for many years. It just people are not that much interested in what happens, I don't know, in 100th order of perturbation theory. No one really does hundredths order. Usually, people do like second or fourth order. Okay, so then what about conversion? So, if Vn was an isolate technical, then you can say that, okay, the number of this configuration is absolutely very large, but it's bounded by bounded exponentially n. I know, because each step you can make, I know, three choices. You either step right, step left, or you do a step left or you do attachment so there is some there are some elementary bounds and then epsilon to the n will gonna be this convergence but if the n is not an isolated technical net if you just look at the individual terms they will diverge basically divergence structure you can see it like this so suppose that you have vn is not isolated then there exists some different valid potential which is Different value of the potential which is close to the n. Suppose that it's less than, let's say, epsilon to the 100, then we can construct the following term. So we start from m or from the origin. We go to a different n prime with this property. Then we just step back and forth, do back and forth one step. Each time we do this, we get epsilon square in the numerator because we do two steps, but we gain. But we gain small denominator, epsilon to the hundred. So this thing explodes. So for individual terms, there is no hope to get conversions. So and the same thing happens in the quasi-periodic case. Okay, so if the n is f of x plus n omega, then yeah, like I described in the answer to Ramis' questions. Described in the answer to Remy's questions that we need to be careful about situations where the values of the potential are close to the value at another point. So, and this is where you get small denominator. Basically, the cases where this thing is small. And this is the difference between cosine and Maryland type. So, the cosine, this difference can be small. This difference can be small in two cases where the multiple of omega is close to an integer or when it's not close to an integer, but you get a different branch of cosine. And in the Maryland type case, we can only get on different branches, there's only one branch. So we only have this thing. And this allows us to set up some cancellations. So I'll just show you some pictures of how the cancellations look like. How the constellations look like. So suppose that we have this bad situation. So we have some point, so we have some part that's kind of nice. Then we arrive to point M. We made, let's say, I don't know, two steps to the point n plus one twice. And suppose this diagram, it's kind of bad. Well, two steps, we can make any number of steps, basically. Can make any number of steps basically. This diagram is bad because n can be n omega can be very small. So, but it turns out that the attachment actually helps us. So, if we consider set of this diagram, the family of four diagrams. So, for example, here we go from n to n plus one and back. Instead of that, we just stop at n and make an attachment. Stop at n and make an attachment. So these four diagrams have some kind of nice constellations. So basically, because the contribution from this part and this part, they are translations of each other by n omega. And n omega is small. So mathematically, you can write it like this. If h of x is the contribution of the original part with the one visit of the point. With the one visit of the point M. Then the rest, you can write it like this. So these four diagrams, you can write it like this. So this two, because these two diagrams give the same formula. So and eventually if you cancel everything, you get small, you still get small denominator. You still get small denominator, but you also get small numerator. So, because this contribution minus this contribution, so this is obtained from this by translation by a small number. So, then you can say that because this function is derivative is not very large, then Then this difference of the values of the function will be proportional to the translation. So basically, if you combine these two things together, we have small numerator, which gets basically smaller and smaller, which kind of comparable with the small denominator. So even if the denominator are super small, the numerator is also going to be super small. And this is where we get this cancellation. So the general scheme of cancellation is Scheme of cancellation is much more complicated because we have to do it on the level of the tree of loops. So, this is kind of the simplest possible case, but you have to iterate it. You have to develop some kind of a system which tells you which loops are going to be grouped with which, so that you can cancel them. It's kind of a multi-level scheme, but it's possible to do. And so the result is that for small epsilon, Is that for small epsilon for Maryland type potentials, the series convergence and gives calculation of the operators? Okay, so I had a few remarks about the one-dimensional case indirect methods, but maybe I'll skip them. And yeah, so this is kind of a half of the physics conference, so I will end with some conclusions. In math, we usually don't do it. The conclusion is usually like theorem is proved. Theorem is proved, but I'll make some kind of a summary which is a little bit speculative. So that sometimes you can associate localization with more random behavior and exist vector extended states with more deterministic behavior. There are some rigorous ways of saying this, which I will not specify, but it's kind of some intuition. And if you look at analytic quasi-pursuits, then depend on the parameters like Depend on the parameters like frequency and capture constant. You can get both regimes. You can play with the constants and get either localization or a C spectrum or something in between. And monotone, quasi-priced compressor don't have a C spectra. And I didn't mention that, but basically the localization we get is also stronger than what we get for analytic keys. So basically, monotone potentials. Demonstrate strong collocalization, and in some cases, they do it both in small and large coupling. And in some cases, like Maryland-type models at small epsilon, you can get proofs which are more direct than the usual proofs of localization. So, these are kind of the conclusions. And let me also switch to portrait mode because I wanted to show a couple of things. To show a couple of things. So, I want to have some references, as I promised. So, the first thing is actually something I found just two days ago. The Maryland problem originally was considered as a kind of a toy model that's sort of integrable and model in which you can prove localization easier than other models. And it was very theoretical, but apparently. Stefano Longhi proposed some experimental parallelization with nautical waveguides. Although I don't think, I don't know if I don't think this experiment has been done yet. And so, and in general, I have some kind of literature, some of the basic stuff that I cited. I guess I can go over. I guess I can go over it if there are any questions, but I think I'll stop here. All right. Thank you, Ilya. I think if you upload your notes, if you're willing to do that to the first site, then people can get these references easily that way. Yeah, it looks like there's a question from Ramiz. Thanks, Elijah. Very nice talk. It's kind of a comment/slash question. So a few years ago, Slashy question. So, a few years ago, at certain conferences, I would hear David Hughes giving talks and talking about how Anderson localization, you know, you don't need randomness for it. All you need is the so-called detuning. So, what you meant by detuning? Detuning. Oh, tuning. D-D-tuning. D-tuning. Like, not tuning, but detuning. And I remember the gist of it was that. The gist of it was that you need some potential that's incommensurate with respect to the lattice spacing. So it's yeah, probably it's a quasi-periodic model. I think Hughes was working mostly on many budget localization, which is in quasi-periodic case is very hard. Yeah, so I think it's your results are interesting because at the time it wasn't clear to what extent. At the time, it wasn't clear to what extent, you know, the I mean, the conjecture and the physics claim was that you know, once you have detuning, get localization period. This is, I think it's very cool that you've kind of come and separated these different notions. So, you might want to, you know, but maybe you can just like get in touch with them or yeah, I need to emphasize that these results are for a single particle case. Are for a single particle case, and many the many body cases much harder. So, I guess maybe if you look at the monotone setting, it's a little bit more promising because of direct methods. I think Master Prietory is doing something. I'm also talking about single body as well. It's true that so David Tuesday has worked a lot on many body localization. But if I'm not mistaken, I think some of the claims on localization. Some of the claims of localization that this is a separate set of work he did, which was, you know, the so-called Anderson localization, as we know it doesn't require randomness, all you need is detuning. That's really interesting if you probably will look into that. Yeah, it'd be cool because I think some of us were confused, and I'm sure they would appreciate it. Are there any other questions? Maybe I can ask one more question. Yeah, okay, good. Oh, actually, more you should go first. So I apologize. I am not sure I understood that. So is it localization of the ground state or lower energy states or the it's so-called spectral localization. So basically it's a single particle model. I can just let me can you see this? Yeah. Let me see this. Yeah. So it's a single particle model. Basically, what we prove is that for this operator, there's a complete system of eigenfunctions such that each eigenfunction is localized around its localization center. So yeah, you can look at it like, for example, if you ignore completely this term and look only at the potential, it's going to be a diagonal light operator that's. Going to be a diagonal light operator that's perfectly localized. And basically, we show that for Maryland type, if epsilon is small, then it's kind of a perturbation of a diagonal system in a natural sense. So all eigen, so all functions. Let me add to this because there's a confusion which probabilists sometimes have. Which, when they talk about showing intermittence and looking at the properties of the semi-group and not actually of the time evolution, what they prove is indeed what you said, namely that basically the ground state is localized. And that's just much more what is proved here, these settings. These settings. Yes, probably. My guess is that this is a stronger result. Yeah, it's a much, much stronger result because all eigenfunctions are localized and not just some semi-group or we have some localization property in the long time limit, which is basically sort of a localization of the ground state. But what struck me is that when it's diagonal, means that when it's diagonal, then the local, so then basically each eigenfunction corresponds to a locale. So you have as many eigenfunctions as locales and each locale holds an eigenfunction. So in this case as well? Yes, pretty much. So basically, let's see if I can get to that page. Can get to that page. Yeah, this is kind of an easy situation that, yes, so you can start this perturbation series from any lattice site and obtain an expansion which will have which will be centered around that lattice site and will decay exponentially away from that site. So you'll have a complete system of. You'll have a complete system of eigenfunctions. So there will be one eigenfunction parallelized site. So it will be basically most of it is going to be supported on that site if you normalize node two. And the remaining tail can be estimated with powers of epsilon. So yeah. So if I may have another just to say sort of maybe this because Because this is not really kind of a naive picture. I mean, typically in these random systems, it's not true that it's just a perturbation of the original Vs. Because, you know, in an infinite system, shit happens and there will be resonances. And actually, the wave function behind. You know, the wave function behind the moon will resonate with something which is behind Mars if you look far enough. Yes, so I kind of didn't mention it, but yes, both in random systems and in analytic quasi-project systems, there is some stuff that happens, like you mentioned. And in many cases, you could actually prove that there is no uniform localization. But in this case, we do get uniform localization because we have. Modernization because we have essential no resonances. Yes, but that's special somehow about these quasi-about your particular quasi-periodic systems. And that's special about monotony. Absolutely non-generic, about random systems. Yes. So yeah, so I wanted to ask, I apologize for actually taking it a little away from this, that if your potential