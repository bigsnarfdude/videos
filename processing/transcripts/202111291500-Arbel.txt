I decided to add a first part to my talk that regards more objective-based things and that will be a sort of well aligned with what Isadora presented to us earlier today. So it's related to extreme value theory for which we seek non-informative priors. Okay, so it's joint work with Teo, who is With Teo, who is so Theo is doing a PhD supervised with St√©phane and myself at Tinria Grano Government. Okay, so the first part of my talk will be about uninformative priors for extreme value models. Isadora already presented a lot about what I'm going to lot about what I'm going to talk about. She gave a lot of context. So that will be easier for me. So extreme value in France aims at understanding risks of some events and most of the time of natural events. So for instance, earlier this year we had the highest flood for 40 years in France. In France. And that's good. That gives an extra data point for extreme value people. But most of the time, in extreme value, you get interested into, you want to estimate things that are beyond what you have actually seen in historical data. So that's probably what is a sort of defining point of extreme value theory going beyond what you have actually seen in. Beyond what you have actually seen historical data. Okay, so to introduce some notations, we consider data. Let's assume they are IID for simplicity, let's say, and we denote the maximum by n. And there are essentially two problems in extreme value theory. The first one is to estimate probabilities associated with some events. With some events. Second one is the sort of dual where you are interested in evaluating or estimating a quantile associated to a given probability. And I'm going to be concerned with the second sort of problem. So in the set of extreme value theory, you often assume that the probability you are considering is getting smaller and smaller. Is getting smaller and smaller, so you are going further away in the tail. But it has to be at such a rate so that the sample size times this probability is going to a constant. Otherwise, you cannot really do inference. And so, we are interested in the associated quantile with respect to this probability group. So, in terms of vocabulary, In terms of vocabulary, we want to estimate the return level, which corresponds to an event that will occur, the maximal event that will occur in a period of time t. And the quantile associated to it is one minus one over t. For instance, this millennial return level has a quantile one minus A quantile one minus one over one thousand. Okay, so if you follow this Adora's talk, you remember that there are essentially two main classes of models in extreme value theory. The first one is called block maxima. So the idea of the block maxima model is that instead of dealing with just one maximum in your sample, that would be just one data point, it's not enough, you cut the observation. You cut the observations into n blocks and you take the maximum of each of the blocks. And you assume that you have some independence among the blocks. And the first theorem in the field tells you that once properly renormalized, the density of this maximum goes to a generalized extreme value distribution. Distribution. So, importantly for us, this distribution is parametrized by the three parameters that are given over there. There is a location mu, scale sigma, and shape parameter xi. Okay, and so depending on the value of the shape with respect to zero, you have heavy tails, light tails or short what short tail means that the endpoint is actually finite. You don't have tail on the end. Final to have detail on the right part of the domain. So, this is an illustration. And the second approach that is underrepresented is the peak of the threshold. So, instead of cutting the data into bins, you take a threshold and you consider the data above the threshold. And there is a second theorem that tells you that once You that once you consider the probability of an event above the threshold, you also get a limit distribution that is parametrized by the same three parameters. So there are those two classes, and actually we are not considering one of these, we are considering a sort of a third class that is called the point process class that encompasses the two. And I cannot really spend time on explaining how it is built, but it's built by viewing the sample, X1, XN, as a point process. And what is nice is that it's connected to both previous approaches, the peak of the threshold and the block maximum, like block maximum model. So when we do inference in the point process, we kind of get inference also down here. Get inference also done in the two other setups. Okay, so we've been interested in obtaining priors in the point process model. So when we deal with non-informative priors, most of the time we get to deal with improper priors. So something to check is the posterior proprietary. So I've written here the likelihood that we have for the The likelihood that we have for the three models, and something we are that we will be using is that one of the GPD model is very similar to, well, they are similar, they share this part in blue. And having propriety of the posterior for one, for this one, for the GPD, implies propriety for the point process due to the fact that this quantity here is rounded, one. Is bounded by one. Okay, so we first considered a uniform prior, and the uniform prior on the on the set of parameters is actually defined as a flat prior on the mu, log sigma, and psi set of parameters. So this corresponds to a prior proportional to this one over sigma, which is improper. And it was shown in the literature that the posterior gets proper as soon. The posterior gets proper as soon as we have enough data. So, starting from four data points or three data points, depending on the model that we consider. And a more general prior is considered with the proper density on xi, which is shown to be more often, the posterior is more often prior because it's more often proper. Okay, so we can easily get from these results results. From these results, results on the point process due to the observation that I did. So, as soon as something is proper for the GPD, it's going to be proper for the Poisson process. And that's what we state here. So, with the uniform, it's going to be proper as soon as n is more than three. And for this more specific prior, where we have a prior marginal density chunks. Marginal density on Xi, it's always proper. The second prior that is often considered in objective base is the Jeffres prior, which is defined as proportional to the square root of the determinant of the Fischer information matrix. So, in the literature, it was already obtained for the first two models, and we Two models, and we've worked on the case of the Poisson process. So we call the likelihood of the Poisson process. And so Teo was good enough in computing all these six entries of the information matrix. And then actually, when you get the information matrix, you still have to compute the determinants. We were kind of lucky. We were kind of lucky to be able to actually compute this in a closed form, and actually, the result was unexpectedly simple and a lot of simplification occurs. So, this is the form of the Jeffrey's prior. And what is nice is that it's always proper, whatever the sample size. And the only thing that we note is that it's defined. thing that we note is that it's defined only for some values of xi if xi is less than one than minus one half we can't define it so it actually restricts the class of extreme value models that can be considered there is no prior mass before okay and actually only after computing it we realized we got the intuition of why it was such a closed form and symbol expression is because From a simple expression is because there exists an orthogonal parametrization that comes from the work by Chavez de Moulin and Anthony Davison. And so with this reparametrization, even here, the official information matrix is diagonal. So you can really expect that the Jeffrey square associated to this parametrization is going to be simple. And then by simple and then by doing the change of variable it's also simple for the for the original parametrization okay so we have some experiments so simulated experiments require sampling from the person process and sampling from that is done in three steps the first step consider consists in sampling a number of observations so So, conditional on the parameters that we consider, Mu and the three extreme value parameters, we first sample the number of observations and then conditional on that number, we can sample the actual observations and the times of occurrence of the observations. So, they look like this. Every blue bar is one observation, so you can think of it as a flood that occurred at this. That occurred at these specific times. And here is a quantile quantile of QQ plot that shows that the sampling went right. And so the goal now is to use those data with our models to recover the true parameters that we assume. So they are given here from usigma and xi and UMA. Okay, Isadore. Okay, Isadora told us that from the new sigma xi parameters, we can also recover quantiles in an explicit formula that is given here. So the quantile of probability p has this nice formula given by the parameters. Okay, so we've done some simulations. I had this slide to explain the hyperparameter city. The hyperparameter settings. So, we've considered several priors that are the uniform and JFRAs. We've considered several parameterizations as well. And we used visual policy testing MCMC with four chains and those numbers of iterations. And so, once you have MCMC chains, MCMC chains, one of the well, you want to check that they have mixed well, they have converged. So the typical quantities are autocorrelation plots, effective sample size, and this so-called potential scale reduction factor that is also known as R hat. And in the literature, you find things like that. When R hat is small enough, so less than this threshold of 1.01, then the chains are deemed to have Then the chains are deemed to have mixed well and also an ESS of some 400 sometimes is pointed as enough data for estimation. Okay, so we've implemented the model in Python, in PyMC3. So these autocorrelation plots are automatic outputs, so they seem to be decreasing fast enough. Those plots represent the effective. Represent the effective sample size as the number of observations of iterations increase. And there are two versions of the ESS. I will be back to it in the second part of my talk, but there's a bulk ESS that evaluates the ESS in the bulk of the distribution and a tail one. So for both sets of parameters, mu sigma xi on top and the quant types at the bottom. And the quant types at the bottom. The ESS is growing okay, let's say. And here are the posterior densities estimated with density plots. So we recover more or less the assumed starting parameter values. That's slightly better for the quantites that are pretty well recovered. That are pretty well recorded. We have trace plots on the right for the four chains. Okay, so to compare the different trials, we have repeated these experiments a number of times. So we have 100 experiments where we resample the data every time. And we compare in blue, orange, and green the three priors. But it's this uniform prior with the Uniform prior with the classical parametrization, then in orange, the uniform is orthogonal, and the green is Jeffrey's. So this is for the quantites. We essentially see that the uniform with orthogonal is the one that gives the slowest variability among the different draws of data. Okay, so that's it. So the main thing that we want to The main thing that we want to implement now is to compare those objective trials with the subjective ones and apply this to real world data. So I could take questions right now before moving to the second part, in case there are any questions from the audience. I'm happy to move to the second part. So I originally wanted to speak about MCMC convergence and that chronologically in the thesis of Theo actually came right after what I presented. So he was having all these MCMC outputs and he wanted to understand how, well, basically. Well, basically, how the those bulk are hats and a tail RHAT work, because it's a fairly recent proposal in the literature. It was published actually this year in a Bayesian analysis paper by Vetari and colleagues. And it was actually a discussed paper at Bayesian analysis. So, we started to work from this discussed paper. And we came up with And we came up with what we call a local version of our hats that comes with a number of benefits. But before describing our contribution, let me explain what the R hat is. So people familiar with diagnosing MCMC convergence know that it's usually done by making use of a number of chains, not only one. A number of chains, not only one. So I think that historically there was this important debate whether what's best between the super long single chain or a number of smaller chains. But probably right now that we can have for not much more expansive a number of chains by parallel computation, I guess that we really need to use a number of chains and not just. To use a number of chains and not a single one. And so, when you have several chains, you can see that, well, visually at least, those two chains seem to have mixed because they behave the same, but the two other haven't. And so, what is this potential scale reduction factor? It's called RHAT. It was proposed by Gelman and Robin. That should be just Gelman and Robin, yes. It's a super psychic paper because every application. Psychic paper because every application more or less uses it, and it works like this: so it's comparison of variances of the chains. So you can define the between variance chain that is here the large variance because those two chains are very far away, far apart. Essentially, one has seen one mode and the other one has seen another mode. mode and the other one i seen as another mode another component and the within variance compared with the between here is pretty small and so you define the r hat as the square root of the of this ratio of w hat plus v hat by w hat so when um when b hat is large with respect to w r hat is much larger than one so you get to So you get to that's an indication that the chains haven't mixed. And on the other way around, our hat close to one is an indication of mixing. And recently the threshold were proposed to be 1.01 by Vetari and colleagues. In the past, it used to be less, but people were using also chains that were smaller and less chains. And so as we're agreeing to. And so as we as we are going to see the the number of chains uh has an impact on the on the value that we should uh that we should consider on the threshold okay so there are two uh big classes of uh of uh a failure for our hats the first is when there is infinite mean and different locations so for instance consider For instance, consider chains that would be Pareto distributed with so Pareto has no mean and well it has infinite mean and the locations are different so we will have like four different paradox distributions for four different chains on the right the second class of failure is when there is the same Failure is when there is the same mean, but different variances. So our hatch is not able to tell whether to see a convergence problem in that case where visually we could probably see here that the variances are not the same, but our hats can as soon as the mean are the same. So the ranker hat, this is a new version in the 2020. New version in the 2021 Bayesian analysis paper of Vetali is more robust to these two problems and it works like this. It's made of two things. The bulk error starts by computing the wrongs. So maybe I did not introduce the notation. My chains are theta mn. So m is for a chain m and n is the And n is the iteration of the chain. And the thetas of the thetas are the outputs. And actually, the bulk carrot is not computed on the thetas, but on ranks, on normally transformed rounds. So you take the ranks of each of the chain and you normalize them. And you compute our at on it, and that's called the bulk art. Okay, so that's one index that is supposed to be. That is supposed to be computing the R hat more for the bulk of the distribution. And the second one is the tail that does the same, but not directly on the Z, but on the deviation from the median. And so since you consider deviations, it's going to be more looking at convergence problems in the takes. And so it's a super strong hat because it's a double power. It takes the maximum of the two. It takes the maximum of the two. And the recommendation in the paper is to use this threshold 1.01. So as soon as the ranker rat is less, you're good to go and you have convergence. Okay, so we played with this ranker rat and tried to fool it. So one way to fool ranker rat since it's done on two, it's made of two. It's made of two sub-indices. One way to fool it is to find chains that share the same mean and the same mean above the medium. So it's super easy to construct such chains. I consider uniform, for instance, with the same, well, uniform and a Gaussian, same mean, so center, for instance, and same mean above the median. And so with this, the rank rat to. The rank rat won't really see the difference, and we aim at building something that is going to see the difference. So, we do what we call a local RHAT. The definition is embarrassingly simple, or the idea is simple, and the idea is to construct a hat, so not on some girls. Some Gaussian transformations of ranks or things like that, but on indicator variables. So for any x on the real line, if we consider univariate parameter, for any x, we consider the indicator that the parameter theta is less than x, and we compute r hat on this for any x. So we're going to have not only one r hat, but a Not only one R hat, but a continuous R hat. And let's go back to this example. So it's a slightly different one. It seems to be two quotients with different same mean, but different variances. And in this case, computing air hat as I just defined it provides this red curve, which is very, very low. Well, very, very low at the center, very low at the extremes, but that gets maximum at places where actually the distributions are pretty distant. So here around minus two and plus two. So there are a number of benefits of working like this. It gives a local version of a hat. So in chains that are sampled from the green and from the blue, it's Clue. It's essentially around those modes that we can find the worst convergence issues. And also they are computed in value indicators, variable. So there is no need for rank. There will be always a good definition, no problem of moment. And as we will see, they detect a number of false negatives that rank carrot does not do not. rank error does not do not and if you are interested in a scalar summary so it's important to to be able to summarize the the information in our hat of x we can we can take the soup so for instance here the soup of the of the blue curve is something that is way above the threshold so we can see a convergence issue okay so that was the definition and that's probably the most important slide of this second part so Slide of this second part. So just keep in mind the local version, we compute perhaps on indicator variables. Okay, more question on that? You try to fool? I've tried to fool Ron Kerra for the moment. We can always fool, I guess we can fool it as well. But it's pretty, yeah, it has the power of the continuity. Power of the continuity of R. So let's take the previous example where we have a uniform and a Gaussian and we've repeated 200 data sampling. And the data array on PRAD is always less than 0.01, so it doesn't detect any failure, any convergence issue. Issue while ours magically is always a problem. So that's a good starting point. So empirically, it's going to, it is giving, it is satisfactory. And what is good with this is that since we work with indicator variables, we can actually do a number of theoretical developments that I think is almost. That I think is almost impossible to do with the rank errat. The rank errat has all these rank normalizations and things like that that make it makes it very hard to do any theory. Let's have a look at the theory. So we assume that we consider chains where we denote the stationary distribution of the chains as Fm. Of the chains as Fm, so each of them can be different. We have capital M chains, and the expectation of the binary variable that we consider is simply the fm of x, while the variance is fm of x times one minus fm of x, as the usual banner leading. And we can also compute the within and the between variances. And the between variances. So they are population versions of the within and between variances that we consider in the definition. So we have no hat on them. It's just their true values. So we can compute them. And so we can also obtain a population version of our r hat of x, just combining in the same way and obtaining this. The same way, and obtaining this expression that is not very nice, but that we can actually evaluate. And so we can compare what we get on the simulated experiments where we know all the FI, so all the true distributions and compare with the experiments. So most of the times, what we see is that the experiments The experiments overestimate the R hat, it's going to be a conservative conclusion. We have a number of properties of this population version. Having a value of the index equal to one is equivalent to having all the distributions equal. So it goes in both directions. That's a good point. That's a good point. It's always larger than one. And the limits in minus and plus infinity, depending on the distributions we consider, is always one. And a good point that is important for a multivariate generalization of this. So everything that I present here is univariate. At the end, I spend some time on multivariate setting. It is invariant to reparametrization, the R and The R infinity, so the soup of the R base. And so this invariance is super interesting. It has super interesting properties, implications. Okay, that's something that I find really good because this proposition provides the asymptotic distribution of our hats, which in turn can tell us how to. Tell us how to decide for a value of the thresholds with respect to some probability of exceeding the threshold that we are ready to pay. In the end, we are just doing tests. So, the threshold comes also with a probability of being wrong. Being doing wrong, but here we with this asymptotic distribution, we can actually understand how this works. So if we compute the effective sample size at the x value just by computing it on the binary variables, then we get this limit. So the ESS at x times r. times r hat square minus one converges in distribution to a chi square. And interestingly, the number of chains enters in the in the chi square. So the more chains we have, the bigger will be the enter. Okay, so with different choices of things, we can we can get We can get different conclusions. So, all the times here for the two tables we've considered a fixed probability of the test being wrong. So we fixed this first type one error. Alpha is 0.05. And then this table on the left considers left considers what is the ESS of X associated to a fixed level of R hat and this is the data r value and so we see that that when R hat is fixed increasing the number of chains well produces of course a larger ESS we get to to have more data equipment not more data but more difficult sample size but the size but the one re-normalized by m is actually decreasing and so the intuition behind this is that as soon as as you get more chains one that converges that diverges has a has a lower impact when there are more chains and on the right we fix an ESS of 400 and Of 400, and we look at the corresponding value for the R hat, the functional scale reduction factor. And so actually that ESS of 400 that is proposed by the Tari and colleagues corresponds very precisely to R hat of 1.01 in the case of four chains. So they say that it's a very common choice, four chains that that's probably the default in That's probably the default in PyMC3. And it corresponds somehow magically to the weights of R hats. But having more chains actually leads to R hat values that could be chosen much larger than this 1.01. So keeping 1.01 when we have more than four chains might be really, really too much conservative. I hope this makes sense. I hope this makes sense. So, if you have more chains, you should actually adapt your R hat value. Yeah. Okay, and the rest of my talk, I'd like to spend it on a multivariate extension to this. So, for the moment, everything is by essence very univariate because we consider binary variables and having it multivariate is. Having it multi-variate is maybe not straightforward. This is a straightforward way to get it multi-variate, but it comes with a bit of drawback. So here, instead of computing our hat on actual values, we compute it on binary variables that are like this, where that's for each and every dimension that we require, that this is less than some value. This is less than some value of x. And why do I say that it comes with a bit of drawback? It's because having this implies a very strong asymmetry in terms of dependence. I don't know if I can give the intuition of why in a small amount of time, but essentially having all of these conditions fulfilled means Fulfilled means that we consider essentially a positive dependence because we only look at all of the variables being less than something together. And for instance, in dimension two, another index that we could consider, so with just d equals to two, we could consider the one with the larger instead of smaller, and that would be like. Smaller, and that would be like a sort of different index that would take into account dependencies in a different way, more like the negative dependencies. Anyway, that's a proposal. It has good properties like before that we can compute the local population version. It enjoys the same properties that I've already said for the univariate case. But interestingly, the fact that it's invariant to reproduce. Interestingly, the fact that it's invariant to we parametrization means that we are able to simplify the question of working on the convergence to working only on the dependence. So why do I say this? We can proceed in two steps. So the first step is to compute our hat on each. R hat on each and every margin. So it's just that we compute. And if each of them is actually indicating convergence, then we can move to test the convergence issue of the dependent structure by actually uniformizing the margins. And so we are back to a set. So we are back to a setup that is simple enough, that is just made of a copula with uni well yeah is a CDF with uniform margins and so the second step is to work on the on the multivariate R head that we have but with uniform margins so we have a simple illustration here in dimension two so we assume we sample Assume we sampled data according to independent Gaussian copula for the first and copula with a Gaussian copula with correlation rho for the second. And we denote and compute the R hat with the R infinity for the C1, C2 for varying value of the correlation, minus one and one. And we compare our R hat with the books R hat. With the Brooks hat. That's also a proposal in a multivariate setup. And we were a bit kind of shocked that the Brooks version actually does not detect convergence issues, even in the extreme cases of, well, of total. So what is this one? The case here is really different. Is really different, is a chain that is very different from the independent Gaussian and this one as well. So it's pretty flat, it gives always something that is below the threshold, while the R-hat that we compute is most of the time above, except in the cases where rho equals zero. That is the case where the chains are exactly the same. There are independent Gaussian the two of them. The two of them. Okay, so that's one illustration, and interestingly, we can also compute an upper bound to this r hat infinity. So I present it here with two in dimension two, but we have so that's no, that's sorry, that's not in dimension two, that's for m equals to two. So we have just For m equals to two, so we have just two chains in dimension d and the r hats, the r infinity, we show that it can be bounded by r infinity of different copulas. So as soon as the c1 and c2 are below and upper bounded by other copulas, then we have this property. And the The Crechet of Ding bounds are two copulas that are actually always satisfying this. This W is the let's start with the M. M is the total dependence, the commonotony and W is the anti-comonotony. So negative dependence. And we can compute this bound here in plus four. So it's like square root of g plus one over two. And we have also extended this result to more than two chips. So the good point of being able to compute such a bound is that it gives also some value to the threshold that we choose. We know it cannot be more than that. Okay, so for the moment. Okay, so for the moment the only document we have is the two-page discussion of the Veltari paper that we are preparing at preprint. It's available on AL and many points remain to be explored, understanding the relationship with the ESS, getting some more theoretical results and most of all working the satisfactory setup for the multivariate. Set up for the multivariate setting. But that's essentially it. I can take questions. Thank you. Thank you very much. Are there any questions? I have two questions on the two parts. The first one is still the the the primary uh this difference price in prior change. Difference priorities implied, yeah. I understand the question. So I'm not sure it's a change of prior. It's actually that if you want to be able to define your first prior, you have to assume that psi is more than a minus r. It's not defined before. You cannot compute the fisher information matches or exile this in depth. I would draw the copy deposit completely. That you cannot choose with it. That's so strong that you remove a chunk of the strong assumption that usually biases are told not to make such a strong private assumption. To make such a strong prior assumptions, but maybe you can be happy with it in some mistake. So, xi less than 0.5 is uh it means that your distribution has no as a finite any point is one. Okay, well negative. So having you still can have a negative one, one just no more no less than this. So you really have some some distribution. Distribution with what I would like to say is that I think and the extreme value variable is not such a strong assumption. As these are the most of the solutions we use with sine pi equals zero. A heart is directly higher than could you work directly on the lighting plugin? Sorry, Julian. Could you repeat the question for the people and yeah. Okay, so I should repeat the question. Maybe I should repeat the first question. What is the first question? What was the first question? So, on the first part, we limit ourselves to xi, using the Jeffrey's prior limits to xi that is more than a minus 0.5. And what I said is that I think it corresponds to assumptions that are not so strict for extreme value. The second question is about I didn't get properly the second question. Could you okay, but that was a good question. Thank you, Christian. So I can read it. So I can read it. You can ask your question. Andresh. Thank you. Yes. Thank you for the presentation. I would just have a question to the second part on MCMC. Do you hear me? Can I hear you? Can you repeat your question? Okay, do you hear me? I will try to speak louder. So my question is, is your method on MCNC applicable also to different Applicable also to different types of say MCMC algorithms people research today random walk, metropolis, Mala, and similar. Is this applicable also in general sense to those algorithms? Okay. Yeah, I had the question, I can repeat. So the question by Henriji. The question by Henrij is: Can I use what we developed in the setup of metropolis testing to any other setup? The answer is yes, actually, yeah, yeah. As soon as we have changed the thing that we compare is just between and within variances, so there is nothing actually very specific. Nothing actually very specific to metropolis testing. We've used metropolis testing in the first part of an extreme value project, but that would apply essentially the same for HMC or NETS, I guess. I think it's time to move on. So we thank Julian again for his talk. Our next speaker is Okay, our next speaker is Trevor Campbell from the University of British Columbia in Canada. He's going to talk about parallel temperature for optimized paths. So Trevor, can you share your presentation? Can you guys see that and hear me?