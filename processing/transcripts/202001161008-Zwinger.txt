So first of all I have to apologize for my voice. I hope I don't shut down in the middle of my talk. Else it was nice talking to you. I do my best. I also can't hear anything so I can easily ignore your questions later. Okay, but this is about code optimization and also new programming. And also, new programming paradigms, and naturally biased to the code we are developing, ELMA ICE. It's a very similar talk than Irina's. So there might be a few deja vous in my presentation. So yeah, the first one is HPC is moving to accelerated machines, surprise. In our case, on the other side of the Atlantic, we The Atlantic. We actually will host quite soonish a pre-EXA machine at CSC at my institution. We still don't know what it will be, but for sure, in order to get the flops, you have to invest heavily into accelerators. And yeah, so this is the consortium, and the machines will actually be quite far north due to Far north due to certain cooling reasons. Okay, we stopped working now. So I think the batteries are. These batteries don't hold a charge on one of the five minutes. Yeah, but apart from that, even newer CPUs actually demand something to do. So as Irina correctly pointed out, if you have an MPI code, this simply won't deliver. So you have three strategies. Ignore it and get denied to board on future HPC platforms. That's actually a valid strategy. A lot of good science is done on a laptop. No problem with that. No no problem with that. Uh or abandon your current model code and uh enroll to a CUDA course and uh write the stuff for accelerators uh with all the uh problems you will face on changing the programming paradigms or change and adapt your current code which we also choose to do and Vero will hopefully also uh follow this uh method. Um you have to bite quite a few bullets here. Have to bite quite a few bullets here. The pro, of course, is you can bring along the existing framework and don't have to do that stuff from scratch. The con is actually it's a real pain to optimize existing code to port it to new platforms. Yeah, it works again. So, to shortly introduce, ELMA ICE is a very similar code, but I mainly focus here on I mainly focus here on the Stokes solvers, despite the fact that ELMA ICE also includes CR and SSA solver. I don't focus on the mixed models which Josefin shortly mentioned. And it's basically just about making a forward model faster. ELMA is written mainly in Fortune 95, so this is a slide. So, this is a slide of big confessions. So, I'm a process modeler and I still programming Fortran 90. So, yeah, and good to mention is also that the multi-physics code actually, so ELMA ICE is not a code per se, it is based on a multi-physics code which is called ELMA, which Which is called ELMA, which is beyond, contains modules beyond ISO mechanics, so like electromechanics, solid mechanics, etc. ELMA ICE itself is not a standalone thing, so what we do is we couple to other stuff. Actually this here, both of them are implemented also in Elma. One is a GIU model. Uh one is a GIU model uh that uh uses a uh layered viscoelastic uh flat earth underneath uh the ice sheet. And the other one is a very versatile currently developed rock permafrost model. This is in the context of nuclear waste repository safety assessment. We also have couplers. One has been presented by Xailer, it's the Pfizer coupler, which couples ELMA to ROMs. Couples Elma to ROMS. This is work done by Rupert Gladstone. And then we have the coupler to EDEM, which is a discrete element model in order to compute dedicated cases of Calvin events linked to Elma. Sorry. So coming to the new programming, actually, did I press the button? Did I press the button? I did. Okay, yeah, thanks. I would have forgotten. Coming to the new programming paradigms. New CPU architectures use vector units, which basically take a data pool, shuffle it to processing units, and apply the same instructions on every cycle on it. They are very fast, but of course it's not so easy to program. It's not so easy to program them. And if you have basically a large of this unit, you can think of having something like an accelerator. So basically stuff you do for that can in some way also benefit for porting to accelerators. Until recently, ELMA did not utilize SIMD, which stands for single instruction, multiple data. And one thing is also important: you have to You have to outline your memory in the code such that it easily can fetch multiple data from one cache line. So like the CPUs work, basically they have several cache around the course and the more often you have a full hit on the cache and not have to go to the main data, you are much faster. And if you ignore that, you usually on an You usually on a traditional CPU as it is done now, you easily use three-quarters of your performance. So, by just making a good data outline in your code, you can immediately boost your performance by a factor of four, which is quite relevant. So, all of these things that have not been there have now been developed to be included in the new Stokes solver. New Stokes solver. It also has an interface to a blockbury conditioner, which I would like to present later. I don't know if I can come there time-wise and also by my voice. But this enables us to use Krillov methods for the convergence of the full systems. And as I already mentioned, SIMD is the first step to enable code to run on accelerators, because there are a lot of similar things you have to take care of. You have to take care of. If we take the finite element method, in particular here for the Stokes problem, so you have an assembly loop first, loops over all the elements of a generally non-structured mesh. These are quite large numbers. Usually we now go even to larger numbers here. And then you focus on the assembly inside one element, so these are the nodes of the elements. So, these are the nodes of the elements, and these are the integration points. So, you loop over the integration points, which can be from below 10 up to 100, depending on how you define your functions on the element. And then you evaluate your basis function at the integration point and fill them in into the billionaire forms to create a local stiffness matrix. To create a local stiffness matrix, and then what you do in the end is take the local stiffer matrix and put it to the global stiffness matrix. So that's the assembly part. And then, as Irina already pointed out, the solution is equally important, if not more important. ELMA uses also interfaces to existing libraries because here we really Here we really don't have the manpower to reinvent the wheel. We have some Elma internal Quillov solvers, but basically you rely on linking to external libraries with all the pros and cons that already have been mentioned here. Right, so for the vectorized solver, in the traditional way, you make it like that. You first loop over the elements, so that would be the natural way to program it, to nicely read the code. Loop over the IPs. Code, loop over the IPs, so the integration points, these are these guys here. Get the contributions from each local nodes here and fill in the b linear forms. And then in the end of that all throw the local matrix to the big one, to the complete system matrix. The problem is if you do it like that, old school book, the code is very nicely readable, but it involves a lot of scalar operations. A lot of scalar operations, and you really suck in performance. What we do now in the optimizing code, which contains also OpenMP, so threaded type of loops in there, we first colour the mesh. So we make sure that when you go over the points of several meshes at the same time, that you don't hit meshes that could lead to a race condition. Lead to a race condition. That means, for instance, if we take this orange and this grey one, the point here, you cannot guarantee when this will be addressed from both sides. So you would step on your own foot. So you have to make sure that, for instance, at the same time, only orange triangles are processed and be sure that they are not neighbor and don't create any race conditions. Then we loop over the elements of the same colors already. Same colors already pre-evaluate the test functions therein, so put them into a vector also for the gradients, and then make a vectorized composition of the local matrix, which can be done in the vector unit. And add using DGM the local to the global matrix. And this is quite an effective way. One thing to say. Way. One thing to say: the code gets, of course, way less readable, so you don't immediately intentionally see that or it doesn't become clear that this is an assembly of a finite element. So we call just the linear forms that then basically shuffle that in the vector units to the whole matrix. And this is the local code then of this bilinear form. And in the end, you see that. From and in the end, you see the DJ and glue to the global matrix being done. So, what does this bring us? If I do, in this case on a very small PC running a Skylake CPU, I do a VTune, which analysis. So, VTune is some tool that gives you an idea how your code performs and also is able to point out where the weak Able to point out where the weak parts of the codes are. A comparison between the vectorized version and the legacy solver, you see that it about takes only one-third of the solution for the ISMECOM C problem here with ELMA. And further, you see that in the memory bound, so that's basically the transfer of memory from to the cache, you have Cash, you have almost twice the performance, or actually, you have twice the performance, so you have way fewer cache misses. And the FPU utilization basically reflects the utilization of the vector unit. So here you really win a lot compared to the legacy so on the vectors mean you hand the scalar operations for all in one color at a time. Yes. Yeah. Yes. Yeah. To the to the vector unit. So uh the whole assembly is then done really. And that those vectors are what the GPUs work on? Um no, in this case it's a CPU. But basically the concept, don't beat me, but basically the concept of a GPU is a really huge also CMD concept. So once you have implemented that for your code, you are almost good to go for GPUs. For GPUs. And watching the graph here, so I have to talk you through this one. So this is the new solver. You see, it goes to 61 seconds. This is the old solver. Actually, time-wise, the solution steps, which are these ones where it really starts to burn, the CPU, are pretty much the same. So if I scale Much the same. So if I scale this one down, basically you see that these bugs basically are pretty much the same, but the part between the solution steps is much smaller. And that's where we win. And that's exactly the effect of A, the better utilization of the memory bandwidth by the better data layout, and also the better utilization of the CPU by using vectorization. By using vectorization, so SIMD instructions. So the narrowing of the gaps just means your work on a shorter amount of time. So your assembly is that much more effective, basically. The solution procedure here is the same. It has been done with C Partiso, which is a threaded solver problem. A threaded solver provided by MKL and included in every Intel suite. Okay, that's for that. How am I doing time-wise? A few minutes, okay. I did not read something. Yes. Yeah, so something almost completely different. No, not really, but to solve the Stokes equation, you need a good Equation: You need a good preconditioner because the viscosity variations are really bad, and already it has been discussed here, the aspect ratio of the ice may be even of order 10 to the power 3. And as I learned today, I'm using the wrong law to evaluate the conversion. But I'm in a good club, yeah. It's okay. I'm here to learn. I'm here to learn. So, the general concept of block preconditioning is if you have your system matrix and certain blocks of unknowns, and of course the right-hand side, so aka the forcing. You can divide that into different blocks and then simply use a block Gaussian procedure to iterate through. To iterate through and solve all these sub-problems. Sorry. And the preconditioner is then basically the operator which produces new search direction in order to converge the global system. For that one, we use GCR because it's a very robust method. GCR has just one problem if you have too much iterations before you converge, as it always adds. It always adds new search directions to the whole space. It starts to be a memory hawk at some stage, and you get out of memory. But as the method is very robust and fastly converging with this preconditioner, usually this is not a problem. So if we then basically interpret this concept in terms of the Stokes equation, we have a matrix that looks like that. That looks like that. So you have here these symmetric parts for the velocity block here and the pressure block. So the gradient and the divergence. And C usually is from the stabilization and can also be zero depending on what type of elements you are using. And a very good choice of a preconditioner is then this this one just taking the A and the B. This one just taking the A and the B transpose block, inserting here a zero and q should correspond to the ship complement, but we are approximating it by a mass matrix scaled with the inverse viscosity, so the fluidity of the problem. Um if you want to read that up, and I'm also not in my comfort zone with this here. Um Zone with this here. This is the Bible, but this was all explained by Mr. Ellman. Sorry. For robustness, we had tests, and usually we test on very small systems. In this case, it's a small glacier in Svalbard amid the Loewembren. So we made some tests, some robustness with respect to the aspect ratio of the mesh. Of the mesh, and you see that it not really has a wide span of needed loops of outer iterations of the system. And we also had a check on n is the number of basal elements. They are then extruded, so this doesn't correspond, I think, to the whole problem size, it's a little bit larger. The robustness with respect to the problem size. With respect to the problem size, and you also see that there is not a wide span of needed iterations. So, robustness of that method using the preconditioner, as I just explained it, seems to be very good. And the speed of your computation then basically is determined by the strategy how you are going to solve the separate blocks that I just showed in the previous slide. And I did some. And I did some test run here. This is based on Fabshide's SSA setup actually for the Greenland case, but I made a Stokes problem out of it. And the legacy Navier Stokes solver, where I just used Hilo with the fill-in degree zero plus GCR for the iteration. It wasn't ready after 4200 or a long time basically and the vectorized Navier store. Uh and uh the vectorized Navier-Stokes solver uh where I used uh a a block for each velocity component plus the pressure uh with uh EDRS method for the uh solution of the block problems and GCR for the outer iteration is done on a new node, a node of a new AMD epic in 469 seconds and using uh two nodes of Intel Guxium Gold in 770 seconds. In 770 seconds. And these are quite encouraging results. So I think this is quite a nice performance boost. We see a similar performance boost as for the small ESME pump problems, also for the real-world problems here. Yeah, some last thoughts, if I may, so. So messages is even on pure modern CPU systems, SIMD is. CPU systems, SIMD is inevitable. You have to do it. MPI alone won't deliver. We have to get our heads around how we deal with the challenge of porting stuff to GPUs or using new methods like Ludovic does. And one thing I want to know, maybe there are people here in this room that can give me an answer. The climate community is. The climate community is very, very picky about bit reproducibility. So I was talking to them. I don't know how they want to do it with GPUs. If you have some ideas on that, it would be nice because as I understand it, a GPU is really difficult to get bit reproducible results. You can, of course, but I guess You can, of course, but I guess you have to do you have to bite some bullets here, right? And I think, I mean, it's reproducible. What's different is that you have, there's some, speaking only for NVIDIA GPUs because I have not much expertise with Arrow vendors. I think with the NVIDIA GPUs, the only thing you have to care is like that some of the um arithmetic functions they're done in different ways than like Done in different ways, and like regarding compared versus like a C exponent, like the exponential function or double precision number in C versus how it's done. And it goes through that to one NPR compiler. It's just because they use a different implementation. But if on that level you care like to kind of for both architecture to use one legacy or something, but you know that the algorithm behind it actually is done the same way, then you get one to one reverse. One to one. Okay, lots of things. There was some research done that for the Swiss Leaders Survey, like Metro Swiss, this person related to the Cosmo project, maybe the Perthica. They did kind of for making this, they were interested in feasibility for making these restarts so you would make a large step, save the three kind of restart files and then make the lo high resolution in between the steps to High resolution between those types of rules. But it's such a good question. But there are, I mean, there are efforts in climate models to get beyond big reproducibility. Like, we have some test cases where you sort of build up an envelope of variability and then you look to see if whatever your new test case goes outside of that envelope of variability. But if you try to run with single precision, then you can stay within that. I mean, it's sort of new. So it's not a dogma any longer, basically. Well, it is still a little bit, but people are trying to figure out how to get past it. Are you talking about all the nursing work at PNML? Maybe Xylo Morgan, there's a slightly separate whole effort. DCWF to radically. radically look at scales of counterized physiques that's a certain one and at a half percent at the randomness kind of the hardware in order to be able to simulate this, use the GPUs for scales that you don't trust anyways. It's pretty valuable. Half half precision. Half half precision, so yeah, but that's a power in that itself. Yeah, there's a lot of people looking at putting a single precision in and whether it makes a huge difference or not. I I like how you focus on the assembly because as I saw before it is angry. I'm not sure if it's really relevant for what you are doing, but I'm I do it from the inside out. So I do the assembly and the equation also with elements inside of that. And I found out at least my XP spell of the code or spell of the assembly to manage the memory management as your dealing with and you're following the elements. So maybe you were running the same issue live now by just doing everything inside out, taking the integration. The integration loop outside, and that's the not the outside loop, but everything is inside, and I'll do a vector operation of all the elements for each in every integration function. Not sure about that in your case, but it seems to be now by just changing how we do these things in our practice more expensive. Yes, I was really surprised to see that. It might be, apparently, it is one way of using utilizing. one way of uh using utilizing a good data layout if you do it like that and utilize you're also using SIMD instruction for that or using also SIMD you're vectorizing the whole thing yeah so maybe markup is also a machinery then that automatically can deal with that I don't know are you yes Yes. I'm finished. So before there is a question, maybe we can more questions or so everybody wants coffee coffee for may we come in at eleven o'clock. May we come in at eleven o'clock? Eleven o'clock. Uh then we switched it off so it actually doesn't work on the side. So now uh so what's the one TR and the chat is right.