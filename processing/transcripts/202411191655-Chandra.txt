Costa, can you hear us? Yes, I can. Can you hear me? Now we can hear you, yes. Awesome. Yes, please take it away. Thank you, Siong, and thank you, Amites, for giving me so much time that I can speak for 35 minutes. I think I won't do that because it's almost quite late in the day. I also feel guilty and jealous at the same time that I couldn't be present in person. Hopefully, I will get to meet at least some of you. Uh, hopefully, I will get to meet at least some of you sometime in the near future. But I will try to make the most of this virtual appearance. Uh, there has been a lot of machine learning-based talks for the last two days, and for the next few minutes, skipping up with this session, I will revert back to Stone Edge, where we use the good old-fashioned Bayesian and PSA analysis. However, I will try towards the end of the talk, try to give you some material where you can actually go ahead and test your ML algorithms. I'll be very happy to chat about this offline. Okay, so. Offline. Okay, so I'm going to share with you a couple of things I worked on in the recent past and also try to highlight a couple of things that I think this will be worried about pertaining to next generation gravitational wave data analysis. But before doing all of that, let us take a step back and remind you about what we have achieved in the last nine years or so. So we have reported approximately 100 gravitational wave detections, and I know most of them are consistent with quasi-spherical binary black hole mergers, but there has been a few. Merges, but there has been a few black hole Newton stress candidates and a couple of binary Newton stress events as well. And using these detections, we have been able to study the properties of the compact objects, we have been able to study the demography, aka their population properties in the nearby universe, and we have been able to validate general relativity in strong field dynamical regions. And with just one multi-messenger observation, we have been able to convert the Hubble tension to a Hubble headache. But as we are approaching towards the end of this decade, our detectors are approaching. Decade our detectors are approaching their scientific potential, and moreover, if you stare at this plot, which I actually stole from Wikipedia, you will pretty much realize that most of our detections are from what I like to call as the nearby universe, Z less than equals to one. And no matter what improvements we do to our current detectors, we will struggle to reach the star formation peak, which is roughly at z equals to two. And even if we reach there, we will barely have any enough signal-to-noise SEO to do any kind of. Signal-to-noise ratio to do any kind of precision science. The future is bright, however, as we have planned ground-based gravitational wave experiments such as the cosmic explored and the Einstein telescope and space-based mission like LISA, Taiji, Tian, Quinn, etc. And all of these detectors will hopefully deliver us much more exciting events than the boring binary black hole events that we have been observing so far. I'll focus on the ground-based gravitational wave experiments and just. Gravitational wave experiments, and just to remind you about all of them, uh, here is a one-slide summary for you. So, the dream is to have three XG detectors, two cosmic explorers, both in the continental US and the triangular Einstein telescope, probably somewhere in Sardinia, Italy, but anywhere in Europe into. The cosmic explorers will be LSET and will have 20 and 40 kilometer arms, while the Einstein telescope will be underground with 10 kilometer arms. The detectors are envisioned to be at least 10 times. Are envisioned to be at least 10 times more sensitive than current gravitational wave experiments, such as LIGO-Livingstone, which is shown in yellow out here. And they will also have a larger frequency bandwidth, bettering both at lower as well as higher frequencies. As a consequence of that, we might end up observing more than 1000 gravitational wave signals per day with durations ranging from minutes to hours. Of course, of course, all these numbers are contingent on the binary populations. But this means that if you observe so This means that if you observe so many signals per day, we will be encountering new challenges that we haven't yet faced, both in the data analysis front as well as in science front. Now, these detectors will possibly start observing towards the end of the next week. But even before that, just to get a feel of how challenging or unchallenging the scenario might be, we here at Penn State decided to simulate the data of these XG detectors. And to do that, we use And to do that, we use GWForge. So, well, in all honesty, it is a Python wrapper script that combines the necessary features of a bunch of amazing gravitational wave packages such as PyCVC, Billby, GWPopulation, and GWP. I wrote this script just to abstract myself away from all the gory details that go behind data simulations. So, how does this package work? Well, for starters, you need to specify as input the network configuration. Namely, you need to specify the network. Configuration. Namely, you need to specify what are the detectors in the network, what are their shape, what are their sizes, where they are located, and how sensitive they are. You also need to specify the universe you believe you live in. So, for example, you need to specify the cosmological parameters, you need to specify the kind and distribution of sources you believe you will observe, and the waveform models, of course. And within a matter of a couple of days, one would be able to generate months' worth of data, which are essentially stored as HDF5 file, similar to whatever. File similar to whatever the current LIGO Virgo collaboration data looks like. It is still very much in development, and I would really appreciate any kind of help that you can provide improving it. Do check out the documentation page. I know it is very crappy at the moment. And also, let me know if you find out any possible. By the way, just a fun fact: the name of the package, GWForge, has nothing to do with forging gravitational wave data. It has to do with Tobias Forge, who is the lead singer, of course. Who is the lead singer of coach, and do also check out his songs. So now that we have the data, we can really go ahead and start looking at some of the potential problems that we might face. And this is exactly what East, Satya, and myself decided to do in this paper. Namely, how does foreground noise caused by overlapping signals impacts inference of high-mass binary black hole signals? So, what did we actually do? Well, we simulated, broadly speaking, three different types of data. Broadly speaking, three different types of data. The first data is simply Gaussian noise of 3XG detectors colored by their design sensitivity curve. We have data set 2, which we call as population A, where in addition to the Gaussian noise, we also add binary signals, namely binary Newton star, binary black holes, and black hole Newton source signals, where the corresponding source population has a binary black hole, binary Newton star, and black hole Newton star population that is given out here in blue at the bottom of the slide. I think you can. Slide. I think you can see my cursor. So let me know if you are not able to follow me. We also have a data 3, which we call as population B, where we have Gaussian noise plus again binary signals, but now the source has local merger rate densities that is given there in red. And these numbers, if I recall correctly, are from the GWTC3 catalog. One is the median merger rate estimate, which is the blue one, and the red one corresponds to the upper limit of the merger rate. And then we went ahead and tried to use the And then we went ahead and tried to use the Welch method to estimate the noise power spectral density. And this plot here shows the result. So, if you stare at this plot really, really carefully, you can see a black dotted line, which shows the design sensitivity curve of C40. You can also see a red curve, which indicates the estimate when the noise includes signals from population B. And you can also see a color, a blue color curve, which corresponds to population A. Now, both the red and the blue curves in Now, both the red and the blue curves increase the 90% confidence band. The comparison sort of demonstrates that, depending on the number of signals or more precisely, the local majority density, the data power spectrum can deviate from the expected at lower frequencies, particularly beyond 40 hertz. The question is, why is this the case? Well, the primary assumption for Welsh PSD estimation is white sense stationary of the data, not stationary, white sense stationary. And because we have perpetual overlapping, gradually evolving signal, the data. Gradually evolving signal, the data is not quite sensitive, and hence the PhD estimate is messed up. I almost crushed. Sorry. Additionally, it is important to point out that the data is also not Gaussian as the number of signals per time frequency bin is also not large enough. Okay, so why this particular frequency? Why does the PSD bump happen at somewhere around here? Why is this so much deviated at this particular frequency range and not so much at the lower frequency value or at a higher frequency value? Value or at a higher frequency value. To answer that, we made this plot. It shows the normalized differential SNR squared as a function of frequency. We chose some randomly thousand gravitational wave event from graphical neutron step frequency. And to make this plot, we use GW Bench, which Saurob developed sometime back. As you can see, that the dominant contribution for both the binary Newton star and the black hole Newton star system arises in between 10 to 20 hertz for the C40 detector. The black hole Newton's Detector. The black hole-Newton star system shows a broader sort of distribution, and this is simply because the black hole-neutron star has a broader mass distribution. And the WEFA model that we use to simulate the Black Hole-Newton star signals also contains higher-order model. Namely, we just simply use IMFNM expression. For the Einstein tensor, it moves toward lower frequencies as expected, given its greater sensitivity in the sub-tenhouse region, where the signal tends to spend more time. And as the gravitational wave amplitude decreases, as something Wave amplitude decreases as something like the frequency to the power minus seven by six, and the power spectrum remains more or less flat beyond 20 hertz, the differential SNR squared increases for both type of systems and thereby have lesser impact on the Welsh PhD estimate. Awesome. Now that we have the noise, the PhD, and understand what is going on, we decided to inject two different types of binary black hole signals. We explored how Foger noise affects binary P, namely GDP. Binary P, namely GW150914 like signals and GW190521 like signals. What I mean by that is that the binary source parameters are same as the maximum likelihood value of this system, but their detector frame masses are different. The reason we selected this system specifically is because that the majority of the signal to noise ratio for these kind of systems are within the problematic band of less than 40 hertz. Anyways, let's start with GW190521 like signals, which is the 0521 like signals, which is the plot in the right. We added these signals at a red shift of 1 to 8, if I recall correctly, but the result that we are trying to show out here shows only 1, 2, 4, 6, and 8. As you can see, that the posteriors across all noise realization is more or less consistent. In fact, if you calculate the Jensen's annual divergence, if I recall the values correctly, it is below 0.03 net, which basically means that the population B as well as population and the B as well as population, and the Gaussian distribution is more or less consistent. And if you try to do it between Gaussian and population A, it is even less than that. You can also see that if we go to higher and higher distance or higher and higher red field, there is an a decreases and the posterior sort of becomes broader and broader and tends to become bimodal for all noise realizations. Okay, so now let's move to GW150914-like signals. This time we inject the signals at five. Uh, this time we inject the signals at 5, 10, 15, 20, and 25, and we do that so as to mimic population three-star remnants manual. Again, broadly speaking, the posteriors are more or less consistent. And if you look into the median GSD value, it is something like 0.03. However, if you look at redshift of 10, there is a bias in measurement of M2, where the true value actually lies outside the 90% credible interval. The reason behind that has to do with Gaussian noise simply because if we moved or if we inject the signal in another data segment, which is again colored by the design sensitive curve, but it's just Gaussian noise plus some population A plus or some population B. We don't see this deviation. Again, if we look into redshift of 15, the deviation for population B is again more pronounced, indicating a larger impact. Pronouns indicating a larger impact. And it again has to do with the Gaussian contribution, Gaussian noise contribution, not really has to do anything with the overlapping gravitational wave signals. If you go to higher and higher distances, the breadset, the posterior tends to broaden due to lower signal-to-noise ratio. And the true value for all data realization falls within the 90% critical interval. So the model of the story is: Ogram noise has minimal impact. And with O4, I think we can start worrying even less. I think we can start worrying even less because we are not really observing any new binary neutral standard black hole neutron stand events that can actually impact our measurement. So we can actually start believing more on the population side or even lower than that. I think I have more than 10 minutes. So I will try to show you again another set of results which actually appeared in archive yesterday. So it is pretty fresh. Do check this paper out. You can actually scan this. Can actually scan this keyword code and follow the paper as well. Okay, so what did we do? We consider a bunch of gravitational wave detector networks at different sensitivities as tabulated in this plot. We have a situation where we have no next generation gravitational wave experiments, basically three LIGO detectors operating at a sharp sensitivity. We have a case where we have the dream, namely we have all three next-generation gravitational wave detectors, and then we have other cases which are And then we have other cases which are in between, namely with LIGO India labeled as I, either operating at A sensitivity or A star sensivity. The first thing to understand is what will be the detection rate of all these networks. So here is a plot for you. If you look in this plot, you can see a solid black line which represents the cosmic binary Newton's stomach merger rate, which is basically the theoretical upper limit on the number of events that could be detected. That could be detected, given that there is no limitations in detector sensitivity. If you look, you can also see there is a gray shaded region which indicates the uncertainty in the local modulated density. And then you can see there are basically different colors, which shows the detection rate for different kinds of detector network. If you don't know, the detection rate is basically the cumulative number of observable events up to a given rate shape. Anyways, as Anyways, as you can see, most detector networks can actually detect the entire binary Newton star population up to a dead shift of 0.1 with signal-to-noise ratio greater than 10. The number is more than 100, even if one XG detector is there. And inclusive network will be able to detect sources with signal-to-noise ratio greater than 10 up to a dead shift of equals to 1. What is amazing is that you cannot distinguish the detection rate between. Cannot distinguish the detection rate between two XG detectors and LIGO India operating at a sharp sensitivity and three XG detectors. You can see they're basically overlapping with one another. So detection wise, I think there is no problem even without XG detectors, like three XG detectors. Of course, we need two XG detectors. So what about the measurement precision of the multi-messenger observation parameters like sky radia, luminosity distance, inclination, etc. Well, for status across all Well, for status, across all metrics, the three XG detectors is the best, and SNAP. But closely following it is a network consisting of C40, LIGO India at A-sharp sensitivity, and the AT detector. And again, if you look, if you replace the A-sharp sensitivity, LIGO India detector with A plus detector, you more or less get similar sort of results. However, again, God forbids, Tokofero, if we have just one XG detector, then we are doomed. Then we are doomed. We will end up detecting an order of magnitude lesser events, significantly limiting our capabilities. Okay. So, what about early warning? Well, again, as expected, a network of three XG detectors, of course, performs the best. It excels at localizing the events, but a network of two XG detectors, namely one with CE40 I plus and ET, or CE40 ISARP and I SARP and ET, shows comparable. And ET shows comparable sort of performance. You can also see in this plot basically that these networks can detect around 100 events with 10 square degrees at around 60 seconds before the merger. And you can also see that for few events around 10, if I can read the plots accurately, that the sky area can be less than one square degree even 10 minutes before the merger. Of course, this results underscore the. Of course, these results underscore the improvements in detection and localization achievable by XG detectors, but it also shows a LIGO India detector will be handy in case we don't achieve our dream of 3XG detectors. So at least from the point of view of a multi-messenger astronomy, we may be able to do without 3XG detectors. Before wrapping up, let me share with you my worry, things that will be a huge problem if we don't fix them. It doesn't fix them. Waveform systematics. So let's start with the obvious problem. I'm no waveform expert, by the way. I will attempt to answer this issue from a data analysis point of view. Simply speaking, I will assume that waveform models are black boxes, where if you plug in some parameters, they give me polarizations as functions of time of frequency. I promise I will try to do a better job in future. I'm trying to learn from the very best, actually. So these plots compared the current best waveform model, namely SEO Binard. Model namely SEO binar V5PHM against IMF NMXO4A. And in this plot, I've tried to compare SEO binar V5PHM with inner surrogate. Just not to disappoint people, I have also compared TOB, DSMS waveforms with the NR surrogate model, and they more or less tells the same story. So I have hid it from you. The plot compares the match between these waveform models in C40's design sensory curve for these kind of populations, which as you can see is more or less Taylor binary. As you can see, it's more or less Taylor binary black hole populism, where the inner surrogate waveforms are valid. Let's start with the bad plot. Sorry, Strovana. The right one, as you can see, the M of N of XO4 has a large mismatch or low match, depending on what you want across the entire population. If you look in this plot, you can see that the values drop below 0.6 in some cases. Now, if you don't want waveform systematics to creep in our posterior. To creep in our posterior binary parameter estimation, then depending on our signal-to-noise ratio, the match must be pretty low. For example, if I try to do some calculation on top of my head, if the SNL is something like 10, then the threshold value is roughly 0.95 for waveform systematics to not mess up my things. And if I'm talking about the golden events, the golden dark silence, then the SNR is roughly of the order of 500 to greater than 1000, even in such cases, then the mismatch value should be less than. Then the mismatch value should be less than 10 to the minus 7. So, how does the SEOB waveform compares to NR surrogate? Well, it's better, a little bit better, but not great. Especially when the system is precessing or have large mass ratio, you can see that the match actually drops to something like 0.8. And remember, we need values in the range of 0.99 Es. Of course, we have ample time to sort these things out, but it is a serious issue. Keep in mind, as Shrovana pointed out, And as Shravana points out, the energy surrogate waveform models can give you, can actually work if the total mass is greater than 60 and the starting frequency is something like 20 hertz. So if you want to generate a system with 60 total mass, then the starting frequency must be 20 hertz. Of course, if you go to higher and higher total mass, you can actually start generating from even lower frequencies. And C40ET actually will be operating at 5 hertz or 3 hertz. So it is much more of a worry. So, it is much more of a worry because the match depends on the phase evolution and missed modeling actually creeps up cumulatively. And this can mess up things pretty badly if I push down the frequency. Now, since I started getting my hands dirty with some numerical relative simulations, I have come to the realization that math doesn't tell you the entire story. It is a sort of a summary statistic. What is really important for us is not only the phase evolution, but the amplitude. Not only the phase evolution, but the amplitude evolution is also very, very important. Also, you want comparison between the SXS waveform against the GR Athena waveform. And there are two different numerical diluted catalog which use two different ways to simulate a binary black hole merger. So the plots out here, as you can see, shows the dominant 2-2 mode of the GR Athena plus plus waveform and SXS waveform for four different masses, one, two, three, and four. And if you stare at this plot, you can see. And if you stare at this plot, you can see there is a sort of a visual, great, amazing visual agreement between these waveforms. And if you also see the bottom plot, you can see that the phase evolution and the amplitude evolutions are also in pretty much a good agreement. But if you look into Q plus 24 system, you see that the waveforms and the phase difference as well as the amplitude difference is greater than 1%. And this is worse if I show you the higher order mode for the Show you the higher-order mode for this particular system where the amplitude, relative amplitude difference for the 3-2 mode, if I recall correctly, is something like 20%. I won't delve much more into details of these calculations because the paper will be out tomorrow morning and it is pretty much late in the day. So, if you ask me, I'll be worried because even non-spinning quasi-circular binary black hole simulations of two different NR catalogs don't agree. So, of course, wafer models like TOB, SEOB, For models like TOB, SEOB, and IMR, and even NR interpolants like the NR circuit, which relies on this simulation, will carry forward this sort of disagreement, not only in the phase, but also in the amplitude. So, what are the things that waveform systematics will impact the most? Short answer, any measurement that are made with high enough signal-to-noise ratio. Here, these are not sound-ending lists, I would say. So, let me start with golden dark samples. So, these are the events. Channels. So these are the events which will be detected with signal-to-noise ratio greater than 500, and hence, any inaccurate waveform modeling will mess up my measurement. It can lead to wrongful host galaxy association due to incorrect 3D dimensional sky localization, and hence it can flow into inaccurate Hubble constant measurement. Right silence, same sense. Let me stop and correct myself a bit. I love my wafer modeler, and I really appreciate their work, and I don't want to offend her. And I don't want to offend her. So instead of saying inaccurate waveform modeling, I will rephrase and say insufficient waveform modeling. Why? Because even if we include all the missing physics, we might have insufficient waveform model because we have not included enough PN orders or haven't calibrated the Mojer and ring down with enough inner waveforms. I think apart from worrying about missing physics, we should also focus on issues stemming from this sort of insufficiency. Test of January 2. Anurvada spoke a lot about this, so I will skip. Incorrect astrophysical implications. We have DW190521, and even when restricting to quasi-spherical binary black holes, and of course, no head-on collision, no crocus star, no dynamical environment, we just assume that this is a quasi-spherical binary black hole. This event is either a highly mass-asymmetric merger. If you use IMFRM XPHM for the runs, you find this to be a highly mass-asymmetric merger. A mass asymmetric merger, and if you use NR surrogate, you finally end up find that this is a very symmetric merger with super large BCM. We'll encounter such events in the future, so better be prepared for it. Now, wrong equation of state constraints due to inaccurate tidal deformation measurement, we can also have inaccurate constraints on equation of state. And the list is actually never ending. So, to summarize, yes, XD. So to summarize, yes, XG detector is still 15 years in the future, but we need to start from today. And to help to do so, we have generated the mock data of these detectors. You can access the first warm-up exercise, as I like to call it, because we have basically told shared all the information that is there. We have told you what kind of binaries are present, what are the wafer models, what is the population models, everything in this data set. So you can actually go ahead and test out all of your algorithms. Ahead and test out all of your algorithms, be it machine learning, be it non-machine learning, because it will be impossible for me to actually go ahead and try to test them all along. If you want to get in touch, please, please, please. I'm very happy to chat with you all. And we'll stop here and we'll be very happy to take any questions if you have. Thanks, Christoph. Questions? Hi Ghostal, thanks for the very nice talk. Can you go to the slide that showed the matches for all the three waveforms? Oops. Can you see my screen still? Yes. We just can't see slides anymore. Okay, just give me a moment. I will try to reshave my screen. Can you see it now? Yeah. Yeah, I can't see yeah, okay. I wanted to know what the 3C plot on both the panels are. Which plot? I mean the bottom right on both the bottom right on both the panels shows Q and theta J and this shows Chi P versus Chi effective. Okay, and what kind of mass ratios or systems are these? Because the two. Or systems like this because so the mass ratios actually range from one by uh one by six to one uh because that is the valid legium for inner circuit. We restricted to inner circuit waveform models. I'm extremely sorry. I didn't realize that this won't be visible. I mean uh so where you when you mentioned that uh exo4a or d5 pH m does slightly better than exo4a, you are comparing d5 hm field. You are comparing DeFi HM PHM with target. Yes. Look at the DeFi PHM versus exoplanet. So I can't tell why you're saying that it's better. I didn't show the other plot. I also got a plot with NO surrogate versus IMF exoperate. And I find a similar set of results like SOBNA V5 PHM with IMF NMXOFORE was actually a slightly bit better results because in that particular case I was. Results because in that particular case, I was getting values low as 0.2. So I sort of didn't show you that plot. Sorry. Yeah, I can get in touch with you later. Yeah, sure. Any other questions? One's a bit burnt up today. If not, then let's just thank you for stopping here. Thank you. And I think that's it for today. Yes. Any coming announcements? No. 8:45 tomorrow. 8:45 tomorrow, yes. Thanks. Thank you.