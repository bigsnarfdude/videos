It's a pleasure to introduce Mike, who is going to talk about discussed solutions for me. Thank you. Thank you, Lisa, and thank you, organizers, for a wonderful meeting. I mean, at my age, I've been to any one of these places many, many times. And each time I come to them, it's really a lot of fun. Again and again, it's like an extended family that I meet. And also, I see that the science is moving in very In a very dynamic and healthy way. I mean, 30 years ago, we would not be talking about any of this stuff. The community has developed the theory in very different ways. And from this, I also came to realize that this type of meetings are extremely important for this purpose as well. I mean, this is where we get together, this is where we discuss on the hiking trails or wherever when we're drinking beer. So it is extremely. Here. So it is extremely important that this happens, and for that reason, I'm very grateful to those who take the time and organize this. It is extremely important. I enjoy coming to these science every time. And it's also a very stark reminder to every one of us that science is a communal thing. It is done by the community here. And individuals, however talented and unique they are, they need the feedback and the And the collaboration from this state. So, for that reason, I'm really grateful that this is being done. So, having said that, so I'm going to work on Viscasti solutions. This is something going back to 40 years for me, maybe. So, I said that things go, but at least Nimfield wasn't there when I was doing that. So, what I want to do is slightly change. So, I put this part over here. Whoops, this is supposed to be, well, okay, this is the Swiss version. It should work well. It should work well. So, the chromota one I just put there over there. This is work that I have done with a bunch of people at Telugu. But this, I want to put everything into context. I mean, midfield game, midfield control. It's been around. I just want to give you an example, at least to have a caricature in your mind, what kind of things that we are talking about. I'm going to give a very general theory in the second part right here. This is what I have done with Jin Chen, but I want to give you an example. So, before Example. So before so, let me just tell you what we've done. Yeah, okay. Before, this is joint work with René Carmona and Content Cornier. This is the Kromoto part. Everybody knows René, so his picture is not there. He's not invited either. I didn't want to cross the organizer. So this is Quentin, obviously. And Quentin is now, he used to be a postdoc at Princeton with René. Now he's in India, somewhere in the code called Technique. Somewhere in a quote called Technique, right? He's sitting with you guys, right? I mean, and the other, and this is Jin Shin. Jin Shin is going, he's a graduate student at Lightmath at Princeton, and in fact, he'll be at the Hard next year. This is the work that I have done on Viscast solutions. So, the main goal was, so there is an example, but the main goal of the second part is to give you a comparison result for Viscast solutions. And there are a lot of them. And there are a lot of them. I mean, Juyan is very much involved in this thing for many years right now. And I first saw his results. I just said, wonderful results. I wanted to use the linear derivative just to push the theory in different directions. So that was also comparison results, but I want to use the linear derivative. So this is really the interest. It's a mathematical interest, really. Many people work. This is a very dangerous slide. I'm missing many, many people there. I don't know the theory. People there. I don't know the theory. It's a huge field. But this very first paper is the one that Jinxin had to read so that I would accept her as a graduate student. It is a really beautiful paper, tough paper, and she came back in 10 days and tell me all about it, so she made it. So many other things. John Frank is going to talk about his results this Friday, I think, right? Or yeah. So there are many others over here that I don't want to begin. So let me start with Chromoto. Chromota probably is not known to this community. It's a chromota model for synchronization. It's a paper written by Kromota, obviously, in 1975 or so. It's a very dense paper, written on a typewriter. And I tried to read it, I couldn't understand too much physics for me. But then there is. Oh, sorry, what the yeah. So. Yeah, so this is from I Tell You from which there is a survey beautiful. So this is a model for oscillators, coupled oscillators. So theta over here is a function of time. There are many of them indexed by superscripts over here. And theta is going around the circle, right? I mean, real value, but think of it as 0, 2π valued. And it has its intrinsic frequency over here. Without this term over here, it would just look good. Here it was just like good. And in the jet lag recovery model that Rene did a few years back, omega k is something like 24 hours, or slightly more than 24 hours, and then everything is done in 24 hour cycle. But this one over here is trying to think about sine over here as a linear function, right, essentially. So it is trying to align each other. It is trying to make this term equal to zero. So it's trying to align each one with the other ones. And depending on the ones and depending on the size of the kappa this this interaction term is maybe more important or if it is small this term is important so there is a there is a balance going on over there and the beautiful thing is that the physicists define this order parameter over here this is complex order order parameter right here so it's a complex function you just define it to be this mean field type of a thing and r over here is the length of it and if you Is the length of it, and if theta i's or theta j's over here are very much close to each other, then r will be very close to one, right? Because each one of them is length one object. If they're all similar, r would be very close to one. So r being close to one means that they are synchronized, r not equal to close to one means, or close to zero means they are incoherent. And then you can write that equation in this way. This is not a talk on chromoto, so I'm going to go fast on this one. This one, an equation can be written in the following form. So I'm going to rewrite that equation, and this is exactly the paper that I was referring to: Soragat's paper. It's written beautifully. If you want to understand Kromoto, that's the place you should start from. It's a beautiful paper, a long paper, but it has a lot of information. And these are his words I'm stealing from him. So I will just let you read it by yourself, but there's a feedback mechanism over here. If they are synchronized, Over here. If they are synchronized, they become even more synchronized. So, Kappa helps them to do that. So, there is a feedback mechanism and they become more and more synchronized. But for that to happen, Kappa has to be large. And the result of this one, and this is why the model is interesting, there is a phase transition at a critical interaction parameter, which they call kappa hat C over here. We can give a formula for that one, but sharp. For that one, that's sharp. Below which, there is no coherence, and I'm sorry, there is no synchronation. They call it an incoherent state, and slightly above that, there is synchronization, and they start becoming behaving like each other. So, there is, of course, and can go to infinity, they look at finite antics and all that. There is a lot of literature. There's a huge literature on this one. So, this is Chromotor, why is it relevant? Chrome motor, why is it relevant? At some point, Sean Main and his students decided to have a look at it through the mean field pop again. And so let me just tell you what they were trying to do in the words of Red and Carmona, I believe. This is another thing that I have solved. So instead of saying that the dynamics of theta is equal to something, you just forget about that. And okay, if it is coming from, instead of the Okay, if it is coming from I say that it's coming from physics, but the Klomoto model is a phenomenology. There is no really bio, in fact, it's applied mostly to biology and some physics problem as well. It is a phenomenological model. They like it because it has a nice outcome. That is the reason. So if you buy that one, I'm just going to try to do the same thing, not through dynamical systems. So I'm going to forget about the dynamical system. And then I'm going to allow. And then I'm going to allow the particles, or the oscillators in this case, to choose their dynamics by minimizing a certain function. But they interact with each other too, and they find a Nash equilibrium. That's exactly what they want to do. So therefore, we have a backward-forward system. One is a Hamilton-Jacobi equation coming from the optimal control problem they solve. And another one is the Fokker-Planck equation because they interact with each other. Of course, this is a usual. Of course, this is the usual thing that those who know me feel games, this is that you know a lot more than that. And of course, this is due to all these people. But I mean, I think that what I'm doing is more in the project or the program of John Lastly and Lyons. I mean, they do many things, and that evolves more to that. So, in this particular case, the chromatomyfield game, motivated by the chromatodynamical system, is this one. System is this one. The important thing over here is this: first of all, let me go through this interaction parameter over here. Lx mu. Mu is going to be the distribution of the particle systems over here. So L over here is like 2 times sine squared, blah, blah, here. Why do I choose this one? This is, in fact, exactly the chromotor system in the finite ant system is exactly the gradient flow for this function. That's why. This function. That's why we put that one over there. That makes a connection to chromoto. That's the only connection. I mean, we are motivated by chromota as a gradient flow, and I take its potential and use it in my optimal control problem. And then, because it's a Nash equilibrium, and I'm defining this one directly on the limit, what you do is that you start with a mu, so mu is the initial condition, so this is supposed to be both phrase. So you start with a So you start with a flow of probability measures. Given that probability measures, you compute this thing, which is a function of x. You solve this simple one-dimensional optimal control problem. It has a clearly minimizer. And you look at the distribution of the optimal solution, and you want that to be the original one that you started, this flow that you have started. So that is mean field game problem. So that is mean field gain problem. And with René and Quentin, we studied this one. And the remarkable thing about this one is: this was observed again by other people before, but we proved it rigorously that there is a critical kappa parameter right here. There is a kappa over here, above which there is synchronization, below which there is no synchronization. Again, it is a phase field type of attack. We work, and that is the critical thing that for small Kappa not having a synchronization. Kappa not having a synchronization is obvious for large companies having this clear, but we want to show the phase translation, we were able to show it. So it's remarkable that minfield games can deliver the same things that the dynamical systems could deliver. So that is the first message. So this could be a modeling tool that we can use in many cases, I mean, people in these communities apply it mainly to a We apply it mainly to economics, financial economics, or things of that type. But it can be also applied to biology and physics as well. This one is code. So that is the first message. And I don't want to dwell it too much. But the thing over here, those who know the mean field problems, it's a mean field game. So I look for it, I don't optimize, I mean, I could easily change mu over here to the law of x alpha. Over here to the law of x alpha, and then I didn't have to do the fixed point argument, right? That would be the mean field control problem. And they are not the same, they are different, we know it why. So, if I change this mu t over here by the law of x alpha, that would be the control problem. And we say that that control problem is, yeah, is it not even a large problem? Yeah, If I change, where is my control? Control is here, so you want to change this one? I don't know what would happen. It would be, it's just like we were trying to do exactly. I mean, in fact, this was done by Sean Main and his studios, so he took their model directly. That would be interesting to do as well. I don't know what. There's not much known around here. I have a paper, a few other papers. That would be interesting. And yeah, so let's. And yeah, so let me go back to Meerfield control problem. Meanfield control problem is generally motivated with this community over here as the central planner problem. But let me show you one other reason why it is important. And this is the beautiful paper by, it's hard to read, by the way, but by Pierre and Diriani, relatively recent. Chromoto Minfield Game is a potential game. I don't know who, I think in the original Who, I think, in the original paper of Last Lie and Lyons, this was mentioned as well. So, I don't know the exact literature of potential games, but if this running cost that I have in my Minfield game problem, if it is the gradient of a potential, then I call it a potential game. And gradient on the other end, this is the linear derivative, usual derivative that you should use in the set of in the space. Set of in the space of probability measures. In this case, if you look at this one, this is linear in μ, so it's going to have a quadratic function whose derivative is exactly equal to all. That's easy to see. But then in this paper again, it is motivated and shown. It's hard to find exactly where it is, but it is shown that Nephiel control, there is a Nephiel control problem, which uses this as running cost, just like this one over here. Just like this one over here. But now I'm not doing game but control because I'm using L alpha over here, which is just the law of X alpha. It's a control problem, no fixed point involved. It's an optimal control. And this one, if you solve this one, get a solution, optimal control, that is a Nash equilibrium of the original problem. So that is another way of motivating Ni-field control problems. So that's what I want to show with this. That's what I want to show with this example. I want to do it in 10 minutes, it's 15 minutes. Let me know if there are any questions. So, by this time, what I want to have achieved is the following. Ni field games are natural in many, many fields, not only in economics, but also in biology and physics. It can be used as phenomenologist. And Nephil control appears not only as a central planner problem, but Is a central planner problem, but also in the potential games they are relevant, and also there is a subtle difference if you haven't seen these things before. The game and control are different. And I want to do the control because I know control, not game. So that's a personal choice. So this part is what I have done with Jinxia. So it's a good point if you have any questions to ask questions. Okay. I'll let you. I'll let you have a look at this busy slide. This is just a notation that everybody uses. I'm going to work on the torus, D-dimensional torus. I'll tell you about the excession later, the usual filteration. We need a non-trivial F0 because we're going to put non-trivial initial conditions. L, the script L is always going to refer to the law of a random variable. P is going to be set of probability major. P is going to be a set of probability measures. I always use weak star topology, but different metrics. And this is the linear derivative. Of course, the derivative is defined up to a constant, but it is for me in all the equations that constant is available. So that's not going to make the difference. And then I say that things are smooth and all that. I mean, okay, well, I just want to say that the derivative of a function, derivative with respect to mu is a function on the underlying. Is a function on the underlying space. So that's a little bit, it gets used to, you have to get used to it. So formally, what I want to do is this following problem in the Macromoto problem. I have an infinite horizon problem. For technical reasons, I want to do this one, finite horizon. One, but we all know in control that if you can't do one, you can do most of the others as well. So I have a running cost over here, which Running cost over here, which depends on the X process, it's low, and the control. I just take the control to be as a feedback form like that, and there is a final cost as well. So, and I want to minimize this thing subject to this equation over here. This is a standard diffusion equation controlled. Right? And B is, of course, I'm going to use B to be my primary motion. That is my notation of choice. And obviously, B and C. And are these. The B and sigma, L and phi are given functions. And alpha is my choice. So the state process over here, so I want to start making one point is that I don't want to, I may even refer to this one state myself as well, but it's not a state. I'm interested in controlling the law of this thing. So this is why this is auxiliary to me. The important thing is the law of that one. It's a control process. And now, what is the controls? Controls is Controls. Controls is generally you have a control set A, and then you look at things which take values in that. But I don't want to do that. I want to take functions of time which takes values in here. And they don't have to be, these are really deterministic functions. Because my state is the probability measure or the law of the X process. X process. That one is control. That makes it a deterministic control problem because the law is a deterministic object. It's a deterministic problem, but it's in infinite dimensions. So it's not a second-order problem, first-order problem in infinite dimensions. And in first-order problems, we take generally the deterministic controls. It is taking values in a set, weird set, which is that one. It's a subset of that one. One. It's a subset of that one. So now the feedback controls. This is not a feedback form. It's an open loop control if you know the jargon in ultimate control. It's an open loop control because I have a function, general function of time going into the control set. The feedback control would be if I make this function to be a function of the law that I see at a given time, which would be a deterministic thing. Okay, so that is a subtle thing, but it's So, that is a subtle thing, but it itself looks like a control. So, a little bit of notational jargon over here. The only thing I want to introduce is that I'm going to look at running costs and all the functions. There is going to be a superscript above it, which is going to say that that is a controlled object, a function of x and v, but there's a controlled dependence over there. These are given functions, b sigma, b is the drift, sigma is the coefficient of volatility, and this is the terminal condition. And this is the terminal condition, and L is the running cost. So, now formally, the problem is this one: the control process is the law of this stochastic process over there, which is solving a Machine velocity, not type, this Martin's loss of STE. If I make enough regularity assumptions on alpha, there will be solutions, and I will make a lot of regularity. Solutions and I will make a lot of regularity assumptions, so there's no problem of that. So, L over here, the state process, and it is the law of this one. And now, there is a subtle thing in this busyness. You want to make sure that your value function depends on the law of your initial condition, but not on the initial condition itself. That is an important technical point. If you look at this process over If you look at this process over here, it clearly depends on the choice of Xi. In fact, C, I want to fix the law of C to be equal to mu, given the same. But there are many x that have that property, right? And they may have corrections to many other things. So x over there is going to depend on c, but its law is going to be independent of x, but it depends. Going to be independent of x, but it depends on mu itself. Because I can write down the Fokker-Planck equation, it's a deterministic thing, it only uses the control. This is why I chose the control the way I choose it. That is one thing, and now you believe me that this guy over here depends on mu and then when you take the expected values over here, the dependence on c goes away, and this thing is a function of mu only. In fact, you can prove it in many other ways. You can prove it in many other ways. I mean, in your work, if Rian's work is proved, these are a different way of looking at it. I mean, everybody has makes sure that this depends only on view, and this one depends on view only, not C in choice. Okay, and the reason for that is that, I mean, there is a little thing, I'll let you read that one if you're into the technical stuff. Now, j is a function of mu, then I take the infimum, and then I have a function of time and the initial distribution. And the initial distribution, and this is my value function. Now, the point over here is that at this point, I want to describe V as the unique discussed solution of some. That was from the very beginning. Any questions? Why do you say that only control still your control depends on only on the state of the state? The state of the current state. But my state is not x. It's not x. My state is mu. I'm sorry. So it is not. You write alpha pixels. Yeah, in the previous one. Yes. Alpha pixels. Next one. Next one. Next slide. This one. Next one in the other one. Alpha pixels in the blue box. Yeah, it's alpha x, but I mean, alpha x is. This is the thing to get used to. So in fact, the one that I don't want is I want. In fact, the one that I don't want is I want this one. Alpha over here is a function of time taking values in something. This is my control set. And then the way you choose a control, forget about how I define this thing, is that you give me an alpha, there is a function that goes with it. Alpha is a, it happens to be in my control set, which is a function of x. My control set is by my choice is a function of x. It's not a pernuminum sense. It doesn't depend on. Does it depend on the full pattern that the name machine? No, no, it yeah, I mean, but but this is a control set the way I choose it. So it's a control set, and the control itself takes the function. The open uh feedback over here would be alpha looks at the law of x and decides where to go. And I'm not doing that, I'm just saying that I look at time and I just started training. But the law of x is hidden in the pattern. It is hidden, but I mean I don't, I mean. Interesting, but I mean, I don't, I mean, but still, this is this is the way that I want to see it. Okay, we can talk about it, but this is, I think, in a formal optimal control setting, this is caution. No, I mean, all these things will not change the value function. It's just like, but I want to see it in this formula. Yes, absolutely, that's an extremely important point. In fact, it's all in his papers, and it's all proved that nothing is changing. So, let me, alternative description is that I do not need to go to the X process at all. I could just simply write down the law. So I don't just have there is in here, there is no X at all. I just say that, okay, L is the solution to a martingale problem or whatever the name you choose if it satisfies this red equality for every test function. If it does, then Function. If it does, then it shows the Kolmogorov equation. In fact, I can even write down the Kolmogorov equation. It does it. And then I just use L as my running push, and there's no X so if I can't, there's an alternative description I can give it this way. And this way it becomes very clear that it's a one first order infinite-dimensional problem. Okay, what is dynamic programming? I didn't write the dynamic programming principle. In fact, this is easy here. And there is a paper by D. Here. And there is a paper by Dylan and Charles Lutin and somebody else. Fabrice as well. It's much more general. I don't need that at all. I mean, this is a simpler one. But if you want a more general one, well, at least it exists. I'm really happy because I don't like to do dynamic programming. But I like the dynamic programming equation. And the equation is just simply a first-order equation. So the time derivative of e is equal to a non-linear function of the derivative of the value function. And that Hamiltonian looks like. And that Hamiltonian looks like this, the usual thing, if you moment control, and then you put the dynamics over here. The dynamics shows up like this, mu acting on these functions of x. That is what I want to see. So this is an integral with respect to mu, and you have the running cost and also the infinitesimal generator of your process acting on your derivative gamma. I think probably stop really visible. This is the derivative of the linear derivative. Of the linear derivative. So, gamma over here, this guy over here is a function of x. So, for this to be defined, to be able to define this thing, I need this object right here sitting there to be a C2 function. I mean, that is a C2 function. So, in discuss the solutions, generally, the problem is that the value function is not differentiable with respect to time or the state which is muted there. But I have a second. Mu in there. But I have a second difficulty here in infinite difficulty, this is an infinite dimensional difficulty. Even if V is differentiable, I want that derivative to be twice differentiable. So that is the part. And now, so this is the equation I want to work with. Any equations? So the definition I give it like this one over here. So I say that some function of over here is really. Here is really time crossed set of probability measures. So it's a compact set. I say that it's a test function if it is continuously differentiable with respect to time and mu. I didn't know that I would get a reaction like that. Okay. Okay. So when you can explain it to the Zoom, hello? Is someone speaking on the Zoom? Hello? That is the moral of the story. Okay, I think it's too task. Welcome to hybrid work. Maybe I go down to it. And also find the dice in Zamsa. So it's not a good idea. Okay, she's gone. Last time I gave this talk in Chicago, we had a problem also. There's something about this talk that somebody is really doesn't like to talk. It's your problem. I don't know. Maybe you didn't. So let me go back to. So I want to define the solutions, viscous solutions of the equation I have shown. So I need test functions, it's usually viscous solutions. Test functions, you just say that it's different. Test functions, you just say that it's differentiable, then that would be it. But in this case, I only have I also have to say that the derivative not only it exists, but it is a twice differentiable function. Normally, this is just a function of x, a function on the torus, but I want it to be twice differentiable. And there's a little another technical stuff, it's not so important. So I have the test functions, and I say that the function, a continuous function on this space. A continuous function on this space of time and probability measures is a viscosity sub-solution. If, if you look at the difference between a test function and u, you look at the maximizer, this exists because of compactness and continuity, you want this inequality at that point. Right, I mean, and super solution, you want the other inequality, and the solution is what's up as super solution. So, this is exactly just like discuss the solutions in infinite dimensions, that's how you define it. Infinite dimensions, that's how you define it. There's a beautiful paper, book by Gotzi and Sliech, which does a lot of this stuff, anyways. So this is infinite dimensional stuff, it's hidden like that. So what are our results? The first thing that we get a continuity result, which is a Bipsy's continuity result, in time it's kind of easy. And in the mu variable, in the probability this distribution variable, I have Lipschitz-Carter. Variable, I have Lipschitz continuity with respect to which metric. I mean, I have to choose the metric here. In Euclidean spaces, you just have to, it doesn't matter. But here, I can choose many, many metrics, and I haven't told you what it is, and it's not Wasserstein. One metric. In Wasserstein, one, this is rather an easy result. In fact, it is related to your talk, Sanjay's talk about independence of the initial condition of the solutions to the. Of the solutions to the controlled equation. So there is some connection there. But roth star, I will tell you, this is a hard metric, so the result is not trivial. And it is important. And the viscosity property that the value function that I have defined is a viscosity solution that's almost trivial. The comparison result says that if I have some supersolutions, they don't have to be value functions, sub-and-super solutions, continuous. Continuous, and I want one of them to be Lipschitz, like the value function. If that is the case, then I have subsolution less than or equal to supersolution. This is the comparison result. There is the metric lying around here. That is important. So, I want Lipschitz continuity with respect to a metric rho star. In fact, over here, I got a Lipschitz continuity with respect to rho star hat. With respect to rho star hat, but we're going to see that this implies Lucchis continuity with respect to rho star. What is rho star will come in a second. So these are what we have proved, and that is really the results that you want in this thing. Of course, Lipschitz regularity is not really... I want to get rid of it if I can, but yeah. So these are the metrics. And I'll tell you the row start is going to be somewhere here. So I'm going to tell you. Going to be somewhere here. So I'm going to tell you what roads are here. So it is, in fact, I don't know why I call the Sobolev-Wasajstein. This is really the dual of regular Sobolev spaces with fractional derivatives. There's nothing fancy about it. And this is how it is defined. It's a dual norm so that you look at the H lambda norm over here, and H lambda norm is this guy over here, and I define it through the Fourier coefficient. I define it through the Fourier coefficients, but hλ is really the classical sub-leg space with fraction of derivatives. It's just simply that. In the case of Taurus, I have this representation. That's the important thing. So, rho lambda without the hat is the dual of the Soviet spaces. Rho sub n with the hat over here is the dual of the C n. So, if n is equal to 1, this is really by the Kantorovich space. This is really by the counter, which duality is W1. Rho hat one is really W1. And what you have over there, these guys are over here. FK is the Fourier coefficients, which shows a certain notation for that. But nevertheless, this is Fourier, and this is how we define fractional megabytes in the Fourier Fourier space. Now, the couple of classical results that we would use. So, if you take the integer, then the space of continuous functions with that many derivatives is well defined, and it would be in the Sobolev space with that many derivatives in L2. This is called W L2, and therefore, the dual norm that goes with this one would be less than or equal to the dual norm given by that. So, in the integer case, I have the integer. So in the integer case, I have the inequality between the hat and the other one. But without that, maybe it's true even without, well, I don't know how to define this thing without that. And for Lucius' stuff, it was important. And the other thing is that if lambda is large enough over here, you have embedding of these summon spaces into continuous functions with the derivatives. And this is, in this case, is the lambda needs to be this many. And I'm interested in making. And I'm interested in making things C2, so I'm going to make this sure that this lambda is bigger than 2 plus the oath. And then the Fourier, the good thing is that once I have this norm in Fourier representation, I have a Fourier representation of the dual norms as well. I mean, it is very, very classical. So the the row lambda norm uh this is a norm, but I just write it as a metric over here. metric over here. Rho lambda in fact has this form and now if mu and mu are probability measures this is bounded by one and if this is summable this thing is finite. And also the other way of saying is that probability measures in the dual of the h lambda if lambda is bigger is sufficiently large. So that you need that you want this thing to be finite and for that what I need This thing to be finite, and for that one, I need lambda to be sufficiently large. And this one, I have mentioned that, that is already obvious. So I choose rho star to be, I choose an n integer which is 3 plus d over 2. It has these two properties. That's the smallest number that I, smallest integer I can choose has these properties. So h l star is embedded into C2, and then you have. C2, and then you have this inequality because I chose it to be an integer. Integer is important for Lipschitz calculator. So, this is my norm. So, how do I prove? So, the important thing is the following. So, the differentiability of this object over here is very important. So, let me go ahead and why is it important? Because when you do viscousity. Do discuss the proof, comparison proof, you always go like this: you double the variables, and then you penalize the difference between the variables. So I'm going to penalize the difference between the things by the square of the minus rho star naught. But this would be my test function. In T, of course, it is quadratic. In in mu and nu, I want this thing to be differentiable. And not only that, I want the derivative to be C two. I want the derivative to be C2. So that is, this slide is doing that. It says if you take lambda large, this is for the general lambda, the derivative with respect to, okay, this is the square of the norm, and that has an explicit form that I can compute through Fourier coefficients. That's really trivial. And the other thing is that if lambda is equal to n star, this guy over here, This guy over here, the dairy over here, is in C2 because of the embedding I have. Because of this embedding that I have mentioned before. This embedding allows me to say that, because this guy over here is in the H and star. Therefore, it is embedded into C. But that is the crucial construction. And the proof, I'm not going to go through the proof really, that it is not from this point on, everything will follow. I set everything up in such a way. Follow, I set everything up in such a way that everything will go through using the regular proof assertion. Except, and this is also regular. Let me show you one thing. This estimate over here, if you have a Lipschitz function, this guy over here, think about it that mu minus nu over here, that is bounded by epsilon. That's a huge estimate for the proof. So Lipschitz continuity gives me that, and that's. Continue to use me that, and that's a huge. This is something that we knew in the finite dimensional case. It's a trick from 30 years ago. You just surfacing it. So, I will not go through the proof, but it is doable. Lipschitz continuity, as I said, is an estimate. If you look at the law of your solution with a control alpha and then mu. Alpha and the mu. And then you keep the control the same, but you just change the initial condition. So I'm looking at the dependence of the stochastic process with respect to its initial distribution. That point is dominated by the initial distance in this way. Again, when this is W1, it is very easy. I'm not doing as fancy as I mean. I'm not doing as fancy as I mean, this one would be exponential time and everything like that. I mean, Sanger has a much better estimate, but it's in W1. What I improve over here is that I agree in this solo, which is not entirely trivial. And I will not go through that because it is. But it is, yeah, I mean, it uses the definitions and some properties of the stochastic processes. I can differentiate. Processes, I can differentiate. If I see this stochastic process as a function of its initial point, I can differentiate it as many times as I want. This is the classical SDE theory, and I get regularity. I'm sorry, I get derivatives of this one with respect to x, and that is really useful. I mean, many derivatives as I want. So, that is the part really useful. So, if you can come up with a proof without So, if you can come up with a proof without the Lichis continuity, it would be nice. And I'm about to finish. And we don't have it. So, let me just tell you with one slide what we are doing in this. Yes, this thing, I just showed you the results in the Taurus. It is important because Taurus is compact. I mean, that was used. If you go to Rd, what happens? Well, I could do this one as in my earlier paper with Earlier paper with Matteo, Vincenzo, and Max, we just look at invariant sets which are compact and then work with that. So, that is a trick that we developed there. It works kind of exactly that way. I could put jump processes, jumps over here. If you use a linear derivative, the equation is easy to write and that is possible. We use just simply the Fourier transform, not the Fourier series. So, that's the same thing. So that's the same thing. So the paper is about ready. We're waiting for this one to finish and then it will be. But what is more exciting to either that we have just done, if you look at the separable case where the L over here is a function of A and a function of mu separated, then the Hamiltonian has a simpler form. And in this case, if you if you assume that this function H is C two and Lipschitz, uh then you have uh we have existence like comparison as well, but I'm not assuming Lipschitz here. As well, but I'm not assuming which is yet. So that is really the more the result that we really want. And let me just leave you with that and remind you that this is joint work with Jin Shin and there's another paper that I should have written it with René Quentin on Kuramoto. Thank you very much for your introduction. Maybe we can take one question before the next one. Anybody here? I think I have asked you many questions about this audio. So let's move to Alejandro's talk. Thank you very much. Is it getting recorded, or we should push it again? I'm worried that the noise will come back. Would come back. And then this lady comes back. Okay, that's good. She's good.