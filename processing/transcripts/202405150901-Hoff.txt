Variance estimation for matrix variant data and by extension tensor variant data. And this project came about as follows. So I had done some work on covariance models and models for tensor value data back in the early 2010s. But I sort of stepped away from that area for a little while. And when Anru joined Duke University, we met and we talked about research. He said, oh, you worked on this. Researching, you said, Oh, you worked on this tensor-variant stuff, you know, but then you sort of stopped. What happened? Why did you stop working in the area? I said, Well, I thought all these models were wrong. When I analyzed the data, like, there's like with separable covariance, I can tell that the true covariance really is not separable. So the model is wrong. And so he said, Well, Peter, you know, the model might be wrong, but it might be useful. And so I thought that was a very wise expression that he told me. Expression that he told me, and so this talk is sort of trying to use the utility of separable covariance models, but sort of address the issue that, in fact, they are wrong. And so, what can we do to fix the situation? So, this is work with Anru and also my former PhD Pizza, Andrew McCormick, who will be joining the University of Alberta in the fall. Okay, so I'm going to start with an application to motivate. Application to motivate matrix variant covariance estimation. And then we'll go into some theory and methods. And then at the end, we will return to this application. So this application is sort of a data hobby of mine, the analysis of sound and audio data. So what you see up here is a WAV file of somebody saying the word up. So it's a one-second long WAV file, and this is what WAVE file, and this is what that looks like, a WAV file looks like. And the bottom is the same person saying the word down. And these data come from an open source data set that was collected. It's not a corporate data set. It's an open source data set that anybody can use to develop audio speech recognition tasks. So, this doesn't look like a matrix at all, so it looks like a vector time series, but the way that people analyze But the way that people analyze audio data is to basically look at the short-term Fourier transform or some version thereof. So they take this time series and they compute a spectrum. They make a bunch of chunks of time points, break it up into 100 time points here, so actually 99, I think. And then we do a short-term Fourier transform in each one. These are the amplitudes of the different frequencies. And so that's what that looks like. So that's what that looks like. Now, human audio perception, we're able to distinguish lower frequencies. We're able to distinguish lower frequencies better than we can distinguish high frequencies. So there's a transformation that people make called the MEL transformation that warps the stretches out the low frequencies. And so these are on some scale that is based on human audio perception. And so that's the frequency spectrum on the MEL. That's the frequency spectrum on the MEL scale. And then finally, to do speech recognition, people do a discrete cosine transformation along this way, and they get something called the Mel frequency substral coefficients. Yes? Vertical axes correspond to the frequency. Yeah, this is frequency, and this is time. Yeah, so this is the same horizontal axis going across all of these figures. And so this is the matrix that we get. Matrix that we get that people will use to analyze speech technically. And not just speech, other types of communication, such as animal communication. Okay, so let's say we want to use this open source data set to construct our own speech recognition. So this data set has a bunch of a sample for each of 10 words. People saying the word up. Here's nine of these. Up. Here's nine of these matrices for different people saying the word up, and here's nine matrices of people saying the word down. So the amount of data we have in this open source data set is not huge. It's sort of on the order of the dimension of these matrices. And so we're going to use a very simple, you might want to use this very simple type of classifier such as quadratic discriminant analysis. So quadratic discriminant analysis says let's assign, let's get a, suppose we have 10 words. Get a, suppose we have 10 words, we're going to find the mean of these matrices for each of the words, and we're going to find a covariance for each of the words. And then we will, so we'll compute a mean mu k, mu hat k for each word k, a covariance matrix sigma hat k for each word as well. And then if we get a new audio sample, we'll assign it to category k if this score is maximized among all the different scores. So that's quadratic. All the different scores. So that's quadratic discriminant analysis. And of course, to do a good job here, you need to get a good estimate of this covariance. Okay, so that's my motivation for matrix variant covariance estimation. Okay, so let's take a look at how I might do this. So what is the covariance matrix of a matrix, right? So typically we will define the covariance of a matrix as the covariance of its Of a matrix as the covariance of its vectorization. So we have the male frequency sectional coefficients that are used for audio tasks, typically we just look at about 12 or 13 of these coefficients. And we'll look at 12 or 13 coefficients across these 99 time points. And so our data matrix for each observation, each WAV file, our data matrix is a 13 by 99 matrix. And so we want Matrix. And so we will vectorize each one of these matrices. And then across our sample, we'll compute the sample covariance matrix. So this is the sample covariance matrix of the word up. And so what does this look like? So the number of rows here is really 13 times 99, but I block them into groups of 99. And so this is the covariance matrix of the first substrate coefficient, sample covariance. Coefficient, sample covariance matrix of the first separate coefficient, covariance matrix of the second separate coefficient. Here is the covariance between the first separate coefficient and the second coefficient over the 99 time frames. So each one of these submatrices is a 99 by 99 matrix. Yes? Backing up a little bit, why QDA? Why QDA? Because I wanted to motivate covariance estimation. But I will also defend QDA here in the sense that we don't have, the number of observations we have is sort of on the same scale as the dimension of these matrices. So we're going to need to use something pretty simple. We're not going to be using a neural network or something like that, which would require a lot more data. So if we're using sort of open source non-industry data sets, we're going to need some simpler method. Method. There's other things one could do. There's other ways you could add on things to make a classifier a little bit more sophisticated. I can mention that at the end if people are interested. But right now I'm trying to show that maybe we might, like if we're trying to estimate the covariance matrix here, then maybe we want it to maybe look separable, maybe not. So let's take a look. So if Look. So, if we look back at these matrices here, we notice that the matrices, the submatrices, look sort of similar in each one of these blocks. So the variance covariance of the first separate coefficient does not look that different from the variance covariance matrix of the second sepsal coefficient over time. And so, maybe the sort of population-level covariance matrix, maybe the variance of the case. Maybe the variance of the k-seps for coefficient is proportional to the variance of the coefficients for word k prime. And maybe that variance is sort of proportional to the variance covariance, the covariance between two substrate coefficients. Okay, if that's true, so we sort of saw that from the image. These blocks look sort of similar. So if that was true of the population covariance matrix, then Population covariance matrix, then this structure would hold for the variance of the 13 by 99 vector of observations. Here, y1 is actually a vector of length 99. y13 is a vector of length 99 for all the 99 type points. Okay, so if this assumption holds, then the covariance matrix for the whole vector would be this, would have this form here, and this is just the Kronecker product. And this is just a chronecker product of two covariance matrices. One being a time-specific covariance matrix, and the other one being specific to the substrate coefficients. So this would be a 99 by 99 matrix, this would be a 13 by 13 covariance matrix, omega. And you can flip it around if the same assumptions held for, like, for each time point, if the covariance matrix, between two time points, if the variances of the subsequent covariance. Time points, if the variances of the subsidial coefficients were proportional, then we'd have the same thing. And so if this sort of structure holds, we say that the variance of this long vectorized version of a matrix, you say that that variance is separable. So that's a separable covariance matrix or a chronicer structured covariance. Okay. So here are the covariance matrices for two different substrum coefficients and Substitute coefficients. And if the separability assumption were to hold, then these should be proportional to each other. And so here I've just simply plotted these coefficients versus one another, made a scatter plot, and on the scale of the correlation, I think. And it's, I mean, it's close in some sense, but obviously not true. Okay, so there's like some separable structure there. And. There. But it's definitely not truly separable. And so we would like to make use of the fact that the covariance is sort of close to being separable, but is not exactly separable. Okay. So like I said, each covariance, the covariance matrix of one of these vectors is a, you know, each vectorized matrix is 99 times 13 or about 13. Is 99 times 13 or about 130 length 1300, and so the covariance matrix is about 130 times 1300. We have about 2,000 observations for each of these categories. And so we say, well, the separable model is wrong. We can't use a separable model. Let's just use a completely unstructured model covariance estimate. We could use the MLE under normality or distance sample covariance matrix. And so if we do that, we're making no assumptions about the structure. Assumptions about the structure of the covariance matrix, but of course, it's going to be poorly estimated because of the limited amount of data relative to the dimension. So, on the other hand, we could use a separable covariance assumption. We could estimate separable covariance matrix. So, the coefficients will be well estimated in the sense that they'll be statistically stable, but it's not going to actually match the true population covariance matrix. And it wouldn't be consistent either. Wouldn't be consistent either. Okay, so which estimator to use? Well, in this talk, the rest of the talk is going to be talking about our motivating some ideas for the following. We're going to come up with this very obvious compromise. We're just going to estimate the covariance matrix as a weighted average of the unstructured MLE and the MLE in the chronic or structured case. And we're going to have to estimate this optimal amount of shape. This optimal amount of shrinkage from the data. So that's going to be the method that is at the end, that we'll talk more about at the end of this talk. So that's the motivation. Now we're going to do something different. So as we developed this sort of shrinkage estimation procedure, we sort of came up with some things we thought were quite interesting. We came up with sort of a decomposition. Sort of a decomposition of the set of all covariance matrices into two parts. So we've sort of developed this new kind of matrix decomposition for covariance matrices, which we call the Kronecker chord decomposition. So we're going to define what it means for, we're going to define the Kronecker part of a non-separable covariance matrix. And the part that's left over, we're going to call the core covariance matrix. And together, Matrix. And together, these form a parametrization of the set of all covariance matrices. And this will lead to a chronic record decomposition. And then finally, we'll come back to the application and we'll show how this chronic record decomposition relates to shrinkage estimation. So that's the plan for the remainder talk. Okay, so let's review what it means for a Review what it means for a covariance matrix to be separable. Let me do this quickly, and then we'll talk about how you would describe the row and column covariance for a non-separable matrix. Okay, so if we have y as a random mean zero matrix, and it has some covariance, so we're going to define the variance of a matrix as the variance of its vectorization. So, in this mean zero case, that's going to be the expectation of yy transpose. Of yy transpose. And we say that sigma is separable if it can be written as the Kronecker product of two lower-dimensional covariance matrices. So if we have a P1 by P2 random matrix, the covariance is separable if the covariance can be written as K2 prime for K1 for some row covariance matrix and some K1 and some column covariance matrix K2. Okay. And And okay. And so if the variance of y has this structure, then if we take the expectation of yy transpose, that's the row covariance, then that'll be proportional to k1. And similarly, the expectation of y transpose y would be k2. So we think of k1 and k2 as being the row and column covariance matrices, respectively. It'll be useful to think about a stochastic representation of such random matrices. So suppose you wanted to generate. Random matrices. So, suppose you wanted to generate a random matrix Y that had a separable covariance. How would we do that? So, here's: so, if we wanted to generate a random Y with this particular covariance matrix K2 product for K1, or for example, K2 written as BD transpose, what we could do is we'd start with a random matrix that has zero correlation across the centuries and is homoscedastic. So, the variance of Z is just the identity. So, I know that the Just the identity. So I know that the product or product of these two identities is also just an identity, but I've written it this way to sort of highlight that we're thinking that Z is a matrix. So if you have this covariant, this random matrix Z, this uncorrelated matrix Z, we multiply on the left by A, multiply on the right by B, and then Y is, if we take the vectorization of Y then by the product or identity, then little Y is equal to this matrix here product. Matrix here, product or product here, times C. Okay, and so then just computing the variance of y leads us to this formula, and you've done that. So the way you want to think about generating random matrices with some row and column covariance is you take an uncorrelated matrix C, multiply on the left by some matrix A, then your row covariance will be A A transpose. Multiply on the right by a matrix B, then it will be B transpose. Do you want to consider like multiple terms of the parameter product? Yeah, so that would extend to tensors, yeah. So if you had like a three-way array or something like that, then you can think about starting with a three-way array Z that's three dimensions, and then you multiply on the left by some matrix A, multiply the right. Some matrix A, multiply the right by some matrix B, multiply on the top or something by a matrix C, and then the covariance would be C C transpose, chronic or B V transpose, chronic for A transpose. Actually, I'm thinking two, the sum of two chronicle products or the sum of two chronic products. Then you would not, then the, then the covariance would not be chronic products, or would not be separable. Yeah, but. Yeah, but any covariance matrix can be represented as some of the. Yes, that's right. So that's a different kind of representation of a non-separable covariance matrix. I can mention a little bit of research that people have done on that, so we're not doing that kind of decomposition. Okay. So just a comment. So if the mapping from Y to Y times matrix C on the left and a matrix E on the right, matrix C on the left and a matrix E on the right. This induces a transitive group action on the set of covariance matrices. So if we think about Y is having some separable covariance matrix, and then we multiply on the left and the right by two matrices, the new covariance matrix of Y, of this transformed Y is also separable. And by multiplying by different C's and D's on the left and the right, you actually move around the whole space of the separable covariance. The separable covariance matrices, which I indicate here by S plus P1, P3. Okay, so like I said, what if your covariance matrix is not separable? Okay, what if you look at your data, say this definitely looks sort of separable, but it's not really separable? Can we still define a row covariance for a non-separable covariance matrix? Can we still define a notion of a Can we still define a notion of a column covariance? So there's a couple of possibilities that you might think of. You could define the row covariance of an arbitrary covariance matrix sigma as, so if y has some covariance matrix sigma that's not separable, you could define the row covariance as the proportional to the expectation of yy transpose. You could define the column covariance this way. Okay, so I would call that the partial trace. I would call that the partial trace definition. And there's been a few articles on estimating separable covariance using partial traces. I can say a bit more about that if there's interest. Instead, we recommend this definition. It's just the chroniker. It would be the, well, let me explain. So we're going to define the row covariance of a non-separable matrix and the column covariance. Non-separable matrix and the column covariance matrix as K1 and K2, where K1 and K2 satisfy this pair of equations. And so you can sort of see what are the features of K1 and K2 if they satisfy these equations. We're defining K1 as sort of the expectation of YY transpose, but after whitening by the column covariance matrix. So we're sort of whitening, doing a partial whitening on the matrix. Doing a partial whitening on the matrix, that should get rid of the column covariance, and then we're left with a row covariance. And similarly, you define the column covariance. Interestingly, if the true covariance matrix is separable, then these two definitions are the same. But if sigma is not separable, then they are different. And how are they different? So the partial trace estimator is really just, if we think about our covariance. If we think about our covariance matrix sigma and write it in terms of these blocks, partial trace estimator, or like example the row covariance, is just a straight average of these sub-covariance matrices and does not use any information on the covariances between columns. Okay, the chroniker covariance, on the other hand, is much more complicated, but it does use all the information. It does use all the information in the full covariance matrix. It's a weighted average of the subcovariances and covariances of the full matrix. Another way to think about the definition we're using here of the chronecker covariance of a non-separable matrix, it's the unique minimizer of this divergence function. So it's really the k. The k. So big K with no index on it, that's K2, Kronecker, K1. So the thing you get is the minimizer of this divergence. It's the divergence between the true covariance and the closest Kronecker covariance. Okay. And we can think of this as really the pseudo-true parameter in a normal model as well. So it has a couple of interpretations. Okay, so we define the chronic covariance. So, we define the Kronika covariance this way. So, this is an alternative definition of this. It's the same as the other one. It's just a different way to write it. So, it's the minimizer of this divergence function. Okay. So, the Karoneker covariance has a very nice property, and that's that it is equivariant. And let me explain what that means. So, let's say we're going to take our random data matrix. Going to take our random data matrix Y and we're going to multiply on the, and it has some non-structured, non-code-separable covariance matrix, sigma. And we're going to multiply Y on the left by a matrix A1 and on the right by a matrix A2. How does that change the Kronecker covariance? So we can show that, so the new covariance matrix of Y after transforming by A1 and A2 is going to be this covariance matrix. So again, it's not necessarily. So again, it's not necessarily separable because sigma is not separable. But the Kronecker, if we apply this Kronecker function to it, then by its properties, this comes out here, and then we get basically the, so it transforms this way. And so what this means is if the original row covariance matrix of this random matrix was K1, and we multiply on the left by A1, then the new column covariance Then the new column covariance, or sorry, row covariance matrix is this. And the column covariance matrix is not affected by multiplication on the left. That's not true of the partial trace estimator. If I, or the partial trace definition, if I were to multiply on the left by some matrix A1, then that could change the partial trace column covariance. So that's a very nice feature of this way of defining what it means, what. What it means, what the row covariance of a non-separable matrix is. And so, again, we have this nice result. So, this action, this group action of multiplying on the left and the right of a random matrix by A1 and A2, this induces this group action here. And K is equivariant with respect to this action. So, intuitively, this means that K is capturing the covariance that's induced by. Covariance that's induced by left and right transformations of the random data matrix. We have some nice results. So not too surprisingly, the Kronecker part of an identity matrix is the identity matrix. Okay, sort of obvious. More generally, if sigma is truly separable, if sigma is in S plus P1 and P2, then the Kronecker part of sigma Part of sigma is sigma itself. So that should be sort of obvious. There's some other properties of this covariance, of this, of this operation k, as follows, as are less interesting. Okay. So that's a way of defining the row and column covariance of a non-separable covariance matrix. And so we have this function k. We plug in a covariance matrix and outpost like some ideas. An outpost, like some idea of the left and right for the row and column covariance. Okay, now if sigma is not separable, then what's left over? Okay, what part of the covariance are we not representing with the Kronecker part? So what's the non-separable part? So the idea we had was that if the variance-covariance matrix of this y is sigma. Of this y is sigma, and we decorrelate or whiten y the Kronecker part, that should be whitening out all of the separable part of the covariance. It should be removing any sort of row covariance and column covariance. So we will define C, a new covariance matrix, as the row and column whitened version of the non-separable covariance matrix sigma. So we're going to call C. So we're going to call C a core covariance matrix. And yes, there is an analogy to the core and tensor decompositions. So does the core covariance matrix have any rho or colon heteroscedasticity or correlation? So if we say what's the chronic, so remember k is our function that identifies the row and column covariance matrix of an R. The row and column covariance matrix of an arbitrary variance matrix. So, what is the Kronecker part of a core covariance matrix? Well, you go through these simple calculations and you find it's the identity. So, the separable part of any core covariance matrix is the identity. And so, and we so, in some sense, that means there's no separable heteroscedasticity, there's no separable correlation. Separable correlation in a core covariance matrix. We're going to define the set of matrices whose Kronecker part is the identity. We're going to call these the core covariance matrices. And we can show that the intersection of the separable covariance matrices and the core covariance matrices, their only intersection is the identity matrix. And as we will see in a moment, these Will see in a moment these two sets, the core and the separable covariance matrices, these together parametrize the set of all covariance matrices. You can think of these as indexing the space of all covariance matrices. Okay. So, how do we define a unique core for sigma? So, you may have noticed that in order So, you may have noticed that in order to form the core of a covariance matrix, you have to whiten by some k to the minus one-half. And there's different matrix square roots that one could use. So, this is the kind of slide that is really, really interesting to a very, very small number of people. So, if you're really, really interested in matrix square roots like I am, then this is fascinating. But I understand that it's not. I understand that it's not fascinating for everyone. Okay, so I'm just, well, but I find it fascinating, so I will just say. So we have to define a matrix square root to define our core covariance matrix. And so separable square root function, what is that? That's just any, you plug in a sigma, and then out pops a matrix H of sigma. H stands for half. Okay, so half of sigma. So half of sigma, if we multiply it by its transpose, must be sigma. And there's different kinds of matrix square root functions you could use. You could use the symmetric square root, that's the most popular. There's some interesting, to a small number of people, issues with the symmetric square root. In some ways, the Cholesky factorization is a better choice for some things. For some things, for what we're doing today, it's not going to matter which one we use. You just have to pick a symmetric square root function, and that gives you a unique definition of the core covariance matrix of any arbitrary covariance matrix signal. So once you choose that, we define the core this way, and that uniquely defines the core. Okay. So we're trying to think of this. We have these two functions, we have the k function. We have the k function, little k function, which maps a sigma to the chronicer covariance matrices, and we have the core function, which maps sigma to the cores. And we're thinking of these two functions as describing the separable part and the non-separable part of any arbitrary covariance matrix. And so we've already discussed how if we have the Kradeker covariance of A sigma A transpose, where A itself Where A itself is separable, then that A just pops out like this. And so the way of thinking about this is that K is capturing all the separable transformations that you might make on sigma. So how does the core change as we multiply our matrix on the left and the right? So what we would hope for is that the core of A sigma A transpose is the core of sigma. So in other words, Core of sigma. So in other words, we would hope that the core covariance matrix function is invariant to multiplications of our random matrix on the left and the right. Because we want to think of the core as capturing the non-separable part. And if we multiply on the left and the right by some matrices, that's just changing the separable part covariance matrix. Okay, so that's what we hope for intuitively. And it's not, whether that occurs depends on the matrix square root function. depends on the matrix square root function you choose. So the core covariance matrix of A sigma A transpose, it's going to be equal to the core of the original matrix, but then multiplied by some orthogonal matrices. Unless the matrix square root function you use, the transformations you use are in a group. So if you use, so the set of symmetric square, well, if you take the symmetric Symmetric square, well, if you take the symmetric square root, all the symmetric square roots of all covariance matrices, that does not form a group. But if you take the lower triangular Cholesky factorization, then that is a group, or the set of lower triangular matrices is a group. And so if you use the lower triangular square root to define your core function, then you have this property: the core of A sigma A transpose is the core of sigma. Is the core of the sigma. Alright, so that's the idea. K is an equivariant function of the multiplication on the left and the right. So it changes and captures the left and right transformations of your data matrix. C is an invariant function, depending on your choice of square root. So as you multiply the left and the right by, as you move around the space of separable, or as you make separable transformations of your data. Separable transformations of your data matrix that doesn't change the core. So, with that, we define the chronic record decomposition. So, the chronic record decomposition of any covariance matrix doesn't have to be separable, could be separable, doesn't have to be. Any covariance matrix is the expression sigma is H C H transpose, where H is a A is the sort of chroniker part of your covariance matrix, and C is the core of your covariance matrix. So this gives a unique identifiable parametrization of the set all positive definite covariance matrices. So we can define a function mapping a set of all covariance matrices to the product space of the separable covariance matrix. Of the separable covariance matrices and the cores, so we can define it this way, and it's a bijection. So that's pretty nice. Okay. Here's a picture of what's going on. So, the set of all positive definite matrices is a cone. This cone here, okay? So, that's the cone of all positive definite matrices. Instead of all chronicer separable covariance matrices, is a submanifold here, and it's non-compact, and it's this. Okay, so this is the space of all covariance matrices. The set of all core covariance matrices is a compact subset of Guthode, and I'm drawing it here like this. And remember, as we said, the only Said, the only point in the intersection of the cores and the separable covariance matrices is the identity. So the idea is that for any covariance matrix sigma, there's a core and a chronecker part which indexes the covariance matrix. Okay. Any questions there? All right. So that's a new matrix decomposition. I feel like I'm a big fan of matrix decomposition. I'm a big fan of matrix decompositions. This is matrix decomposition for covariance matrices. There's no idea of rank involved or anything like that. So it's just based on this sort of partial whitening of the original covariance matrix. All right. So now we're going to tie this back to the problem that we discussed at the beginning, which was covert. Beginning, which was covariance estimation when you think that your covariance matrix might be close to being separable but is not truly separable. So remember, so we've defined for any covariance matrix, we've now separated out the prodiker part and the core part. Actually, maybe just go back to this picture. Remember, we said if the core, actually, if the core ends up being the identity, okay, then the covariance matrix is. Okay, then the covariance matrix is truly separable. So if the core is the identity, right, the core is here, then we must be on that sub-manifold environment of covariance matrices. So imagine we get a sample covariance matrix somewhere, and we think that maybe the truth is sort of separable. What should we do? Maybe we should move the core towards the idea. Move the core towards the identity. That, in fact, is what we end up doing. Okay, so now I'm going to, we'll do this in the context of a normal model. Almost none of our results depend on normality. Just makes it easier to talk about. So we have some population quantities. We have a true covariance matrix, sigma. We have a true chroniker part of that covariance matrix, K, and a true core matrix, C. Core matrix C, okay, and we get some sample quantities, and we get the sample covariance matrix S, and K hat is going to be K of S, C hat is going to be C of S. So under the normal model, the MLE is just S, and of course that has this decomposition, K to the one-half, C hat, K 1 half to the 1 half. Okay, so what we're going to do is we're, so based on the Based on the discussion of the core and quantitative parts of a covariance matrix, what we're going to do is we're going to shrink the core. We're going to come up with a new estimator, sigma hats of w, different from the MLE, that's going to just keep the Kronecker part of the covariance matrices, of the sample covariance matrices. But we're going to shrink the core. And so we'll just do linear shrinkage of the core. We're going to have a weight on the identity. A weight on the identity, which is really a weight on being separable, and a weight, one minus that weight, on the core. And then, just because this is linear shrinkage, that ends up having this expression here. And in fact, this is just exactly the same as 1 minus the weight on the unrestricted MLE plus the weight on the MLE under the Kronecker separable assumption. So linear shrinkage of the. Assumption. So linear shrinkage of the core is exactly the same as linear shrinkage of the sample covariance matrix towards the MLE under the separability assumption. Okay, so then you might say, well, what was the point of all of that work that we did, right, to come up with this decomposition? Well, it's to come up with this neat decomposition. That's what. Alright, so we're going to get a sample covariance matrix here. It's going to have a sample covariance. A sample core as a sample chroniker part. We're going to shrink that sample core towards the identity, which is the same thing as shrinking sigma hat towards the space of separable matrices. How are we going to do this shrinkage? There's different ways you can choose this weight on the space of separable covariance matrices. So we ended up doing an empirical base. Ended up doing an empirical Bayes estimate of the appropriate amount of shrinkage. So if you use an inverse Wisher prior distribution for the true covariance matrix, then the posterior expectation or the posterior mean estimate of sigma given S is a weighted average of S and this separable part. So in other words, if you're prior for sigma, If your prior for sigma is that it's inverse Fisher and it's centered around some separable covariance matrix, not equal to a separable covariance matrix, but like centered around. So this is a distribution that's near some separable covariance matrix. Then the posterior mean estimate is a weighted average of the MLE and the prior expectation. That's just your usual feature of a Bayes estimate. Feature of a Bayes estimator under an exponential family. All right, so we're going to do an empirical Bayes approach. We're just going to replace the sigma 2, Kronecker, Sigma 1, and our prior with our estimate from the data, or the MLE, or the sample Kronecker covariance of our sample covariance matrix. Then we'll estimate this hyperparameter and this prior. In this prior from the marginal distribution. So this is just standard empirical Bayes. And it's a big mess, but it ends up just really depending mostly on the eigenvalues of this sample covariance matrix, sample core covariance matrix, C hat. And so the closer C hat is to the identity, the more shrinkage we're going to do. So it's really kind of this adaptive, it ends up being this adaptive estimation of Up being this adaptive estimation of the amount of weight you want to put on the Kronecker part by looking at the sample quark of Aries matrix and seeing how close that is to the identity. Closer C hat is to the identity, the more you want to shrink the whole thing in that direction. The more evidence there is that the truth is separable. Yes? So if you look at the marginal distribution of S, or if you look at the expectation of S, does it have sigma 1 and sigma 2? Sorry, if you take the prior prediction. Sorry, if you take the prior predictive expectation of S? No, I'm thinking about the marginal distribution of S. If you take the expectation of S, uh does it have sigma one, sigma two as the So marginalizing over what? So the expectation of S given sigma is just sigma. So S is the sample covariance matrix. So expectation of S is just sigma. I see Okay. All right. So then after identifying this base optimal weight, we construct the shrinkage estimator outward. So we're able to show that this estimator is consistent. So of course the MLE is consistent no matter what the structure of the true covariance matrix is. The separable MLE is not consistent unless Is not consistent unless the truth truly is consistent. So we showed that this shrinkage estimator is consistent. What does that require? All it actually requires is that the weight on separability goes to zero if the true covariance matrix is not separable. Why is that? So if sigma is not separable, well, our shrinkage estimator is a weight on the unrestricted estimate. The unrestricted estimate plus a weight on the Chronicle MLE here. And if the weight, if sigma is not separable and the weight goes to zero in that case, then this goes to one and this goes to zero and it just gets sigma. So if it's not separable, then the weight eventually just puts all, you just put all the weight on the sample covariance matrix, which is consistent. If sigma is separable, then both S Then both S and K hat are consistent. So we get consistency that way, regardless of how the behavior of W hat. Yeah. So when you say S is consistent, is it assuming the dimension is fixed? Yes. So this is all in N. So yeah, so this all holds in a fixed P growing N scenario. Otherwise I have to just explain what sigma is as P can ask my question one second later. Question one second later. So then we see this magic. Yeah, sometimes these like hiding the results is irritating, isn't it? And your empirical base estimator for the weight goes to zero? If the yeah, so if the true covariance matrix is not separable, then the the weight W half goes to zero and we end up putting all the mass on the sample covariance matrix, which is consistent. Sample covariance matrix, which is consistent. We have some conjectures that this estimator is in some sense super efficient, but we have not been able to show that. We need to, we have some initial results in a p-growing with n scenario in this case. So, but we're not quite there. To do this, we need to study the properties of the sample covariance matrix and obtain some sort of Marchenko-Pasteur law for the core covariance matrix. And we are working on that now. They're probably quite tiny, but let me just highlight this and then I'll show you the final data analysis and we'll be done. Okay, so this is just some results from a simulation study and Study and we looked at a few different estimators. There's the unrestricted MLE in black, and there's the core shrinkage estimator in blue, and there's the chroniker MLE in red. And there's an oracle estimate, which is hard to explain in green. So, this plot over here, this is a scenario where the true covariance matrix. Where the true covariance matrix is separable. So the true population covariance matrix is separable. And what we're seeing on the y-axis is some estimate of the risk or a loss sort of over different simulations. And as our sample size increases, the loss, the average loss or risk of the MLE decreases, as you'd expect, because it's consistent. But if the truth is really separable or chronic or structured, then the Separable or chronic or structured, then the chronic or MLE is really good. That's as good as we could do. And our estimator, our course rankage estimator, matches that. Blue, we match the performance of the Kronecker MLE in the case that it's truly separable. On the other hand, way down here, this is when the true covariance matrix is not separable really at all. And in that case, the black is the MLE. The black is the MLE, and as N increases, we get better and better performance, and our estimator matches the MLE in that case. And of course, the Kronecker MLE, the Kronecker restricted MLE is converging to something, but it's not the truth. And so you never improve beyond some threshold as you increase the sample size. So our estimator sort of matches the MLE. MLE, the unstructured sample covariance matrix in this case here, matches the performance of the restricted Kronecker MLE when it's appropriate. And matches sort of the oracle estimate when we're in between. Okay, so we have this conjecture about some kind of super efficiency, but we haven't put that, we haven't wrapped that up. Okay. All right, so if. That up. Okay. All right. So finally, let's just look at these data. So are there real data sets where the truth is sort of separable, but not quite truly separable? Well, so that's what this data analysis was meant to illustrate. We look at this. We want to know, is the shrinkage estimator going to do better than either the truly the restricted Kronecker MLE? Is it going to do better than the unrestricted? Is it going to do better than the unrestricted sample covariance matrix? Okay, so remember we're going to do this classification task, and to do this, we need to estimate this sample covariance matrix for each one of these 10 words that we have. So we did that, and here are the confusion matrices. These words are what they call command words, yes, no, up, down, left, right, on, off, stop, go. And so here's the confusion matrix. If you assume separate. Assume separability, it does sort of okay. Here's the, using the sample covariance matrix, it does terribly. And so we do better than either of those. So we take this as evidence that the true covariance matrix, the covariance matrix that is going to give you the best performance is one that is sort of separable or is towards the space of separable covariance matrices, but is not exactly. Covariance matrices, but it's not exactly separable. So that's the idea here. I have some error rates there, but that's actually less informative than the fusion matrices. So let me just summarize. So what have we done? We've developed a matrix decomposition for covariance matrices. For any covariance matrix, we can decompose that matrix into parts, a separable part, and a core. Part and a core. That's a unique parametrization, identifiable parametrization. We propose an empirical-based shrinkage estimator, sort of an obvious estimator, shrinking an unrestricted covariance matrix towards a restricted estimator. So that seems sort of obvious, but we've related that to this, we've indicated how that is related to this notion of shrinkage of the core. And so we've evaluated some of the properties of the core. Some of the properties of the shrinkage estimator. Well, okay, so some future work. I don't know, I put this here. You could extend it to tensors. I will say that everything extends to tensors almost immediately. There's nothing in here that doesn't really isn't very straightforward. What we really want to do is understand the properties of the sample covariance matrix. So we have this unstructured covariance matrix. unstructured covariance matrix, the dimensions are P1 P2 times P1 P2. So that's too big. P1 times P2 is too big. And the separable part is P1 squared on the order of P1 squared plus P2 squared. Okay, so that's not so big, and that's manageable. The core covariance matrix is still P1P2 by P1P2. So that's still like a high-dimensional object. And so right now we're working on how. So right now we're working on how to study the properties of that so perhaps we can do some sort of dimension reduction on the sample core itself rather than just shrink towards the identity. Can we do some sort of dimension reduction? So that's what we're trying to do and also evaluate whether or not there's some sort of super efficiency property going on. Okay, and please try out matrix decompositions if you're a fan of matrix decompositions, as am I. SMI. So there's an R package which will compute the chronic core decomposition and do this shrinkage estimate. So it's fun to do matrix decomposition. So please try it out. Okay, that's all. Thanks very much. Okay, also a few minutes, more questions. Really interesting stuff. One question, one of the beauties of the One of the beauties of the Kroniker product is that it preserves kind of the eigenstructure of the separable matrices. Do you have any results? Does it preserve the eigenstructure of the separable part and a separate one of the core part? Or does it all get scrambled? Have you looked at this? So maybe I'm not quite understanding, but we have this function which defines a separable part of an arbitrary covariance matrix. Covariance matrix. So, what are you asking if it's preserved? So, if sigma is separable and has eigenstructure, like so, if sigma is separable, then the function maps exactly to the separable. It doesn't change anything. But the eigenvectors and eigenvalues are separable for any separable distance. And so, my question is, if the eigen, if you can relate these. The eigen, if you can relate the eigenstruct of sigma itself to the separable part and the non-separable part. So it's basically the eigenstate. So if sigma is truly separable, then there is no, then the separability, then it just maps to itself. And so there's no issue there. So if it's not separable, then I don't think that happens. But I think we looked at that, but I've forgotten. So I don't think it, I don't see. So, I don't think it, I don't see why it would in that case. So, but the eigenstructure of the separable part will be preserved under transformations because the Kronecker function is equivariant. So, okay. So that was a mouthful. But so. Doesn't that contradict what you just said? No, so maybe. I don't know. We'll find out. So this, okay, so the Kronecker part of any covariance matrix is Kronecker's structure. And then if we multiply on the left and the right by, say, A sigma A transpose, where A is A2 prime for A1, then this is going to be A2A transpose prime for A1. A transpose. You can't necessarily relate those A's to the eigenvectors of sigma itself, unless sigma is separate. So A is a transformation of the original sigma. So what is preserved, so the eigenvectors of the separable part will mount to the eigenvectors of this. I think the question maybe is if sigma has the big structure like u lambda intranspose, with large structure Lambda U transpose with large lambda plus u orthogonal, another lambda orthogonal, u orthogonal transpose where lambda orthogonal is much smaller. Will you get your big lambda in the first part and small lambda in the second part? I don't know. I don't think that was my part. Well, I can agree this part. Other question. Back to the data. Back to the data space, can we support us separating the data signal plus noise or some sort of, can the signal has this separable structure and the noise is operating? That's an interesting question. Well, it wouldn't be signal plus noise. So it's not an additive decomposition. So, but that would be. That would be, I'm not sure how that would work. So, I have a student right now. We are looking at sort of a spiked covariance model or a partial isotropy model, in which case there might be the core would be like separable plus noise. No, the core wouldn't be, but the covariance matrix would be separability plus some other isotropic noise. And so, it's a good question. Can we separate out the, do we get an The do we get an improved estimate of the chroniker part in that case? I don't know, but it stands to reason, but it's not going to pop out exactly because the decomposition itself is sort of this multiplicative decomposition and not an additive decomposition. Is there any way to check sigma ma is it's separable? Yeah, well, I mean, if you like hypothesis tests, that's one way to do it. So there's a bunch of hypothesis tests. Hypothesis tests, frequentist hypothesis tests for checking separability. So we've shown here that testing for separability is equivalent to testing whether or not the core is equal to the identity. And so in a different article, in the tensor case, actually, we use this idea to come up with improved equivariant tests, or invariant tests, I should say, for the separability of a tensor or a covariant or a matrix. But yeah, so testing that is equivalent to testing whether the core is the identity. So the shrinkage you have done is linear shrinkage when the elevation of the core and identity behavior. So we both thought about shrinkage non-linear fashion, like hard thresholding. Does it degenerate to some sum of the product term in SS? So what we have looked at is so every So, every the sample covariance matrix S can be written as K hat C hat K hat. And the closer that this is to the identity, the closer the resulting shrunken thing will be to separability. And so we've done, as you said, we've done linear shrinkage here, but you could alternatively start thinking about looking at the eigenstructure of C hat and then doing shrinkage on those eigenvalues there. Eigenvalues there. And to do that appropriately, we need to understand what the behavior of C hat is under separability. So, right now we're trying to come up with a Marcheco-Pasture type of limit for Cat for the distribution of C hat in that case. So, we haven't done it yet, but we're looking at that because we think that would probably be a little bit more sophisticated than linear shrinkage. Yeah. Thanks for the talk. Yeah. Thanks for the hot talk. So you mentioned the QDA as a motivating example, and then you just go through this general problem. I'm just wondering if we go back to the QDA problem itself and if there is a chronicle separability structure like you mentioned there, is there any way of improving the QDA itself rather than plugging your covariance estimate? Well, so the analysis we showed was that assuming separability for those covariates. Separability for those covariance matrices gives you worse predictions, gives you worse classification. So we wanted to show these data because we go through this. Maybe you think, oh, separability, it's great. It's always true. And then another person will say, no, you never use separability. You should just use the MLE because you're not making any assumptions. So the whole, the idea is really, can we use an estimator that's moved towards separability? Moved towards separability, but not restricted to be separable. And so we showed that that gives a better prediction performance. I think this is from the real data perspective. I'm just wondering theoretically if we believe the separability is true, then it's a better way of doing that. If you're right, then yeah, that would be better. But I don't believe separability is true. So I've done likelihood ratio tests and tests of whether or not a sample covariance matrix is separable. Sample covariance matrix is separable, and I've always rejected. But in QDA itself, you don't have to plug in your covariance estimator and then use the QDA follower. You may just provide another algorithm called QDA that takes advantage of the separability structure. If you believe it's true. Well, I'm not sure I'm understanding, but if you believe it's true, you would just use the Kronecker MLE, the restricted MLE. I think maybe. I think maybe it's not important whether it's true or not. If you can show in some real data the QDA with systemic for these data. Okay. Yes. So it did better than the restricted and totally unrestricted. So shrinkage does better. Kind of an old theme, yes. Yeah. I'm just kind of reiterating on my questions. Of reiterating on my question. So basically, when majority write it out, that would help me focus. Yeah, majority of the estimators, they assume that they basically shrink, take covariance matrix and take the eigenvalue decomposition and take few eigenvalues, so making it separable, which is spike covariance model, but this is pretty much what people do. So according to you, you need to add. According to you, you need to add something else to it, and that will be better. So, the intuition is the following. So, the chroniker part of any arbitrary covariance matrix is lower dimensional than the sample covariance matrix and is much better estimated than the population, full population covariance matrix. So, we decided let's focus on the Let's focus on the remaining part, which is much higher dimensional. That seems like that would be the part that needs the shrinkage. We don't need to apply shrinkage to the chroniker part because it's lower dimensional. What I'm trying to say is that in majority of cases, they put zero weight on the core, and basically the message is that put non-zero weight on the core when you're estimating a variance matrix. And you may do better than with the traditional estimator, which just takes. Estimate, it just takes K. Yes. You've crystallized my thoughts exactly. Excellent. So, this is what I was telling you. That's right. Thank you. So, in the beginning, 25 minutes break, is that right? Okay. We'll come back half. So basically, when you have a huge element here, then you of course have set only because you really have a spike variance. But in material pieces, you don't have a two spike variance, but when it goes like this, have it click this, and then add the rest. And so the The rest. And so the related question is where to cut and then how to send away. I mean, as a practical question, right? Yes, that's right. So this kind of shrinkage that we did, there's only one weight, but we would like to do something maybe more specific. With the sample core, C parent, we would like to look at its eigenvalues and shrink those maybe in some non-uniform way, non-linear way. Because we think that would probably. You almost have to do some decomposition and basically make it the standard. You cannot take the policy if you do. No, but it is it is not No, but it is it is not different. It is actually different. Sure. Because you have different dialogue matters, like this coming. No, but usually it is like this. People cut it somewhere and there is infinite number of papers of where to cut. So your message is cut somewhere and then deal with the rest. And then she can be using the rest. Well, when you cut, it's to Well when say you cut it's so this is why I so I've done a lot a lot of work with where we have to sort of choose and then she choose how many things to keep. But I always you know, you always get the question of how do you decide whether that's a kind of tree? How do you do that in real life? So I like this because there's no choice of like, oh, is it three-dimensional? But it was you know, it was because the it was using the conveyance event just Well, there is not a bad issue. But when you estimate K1 and K2, imagine that if my P1 E2 is 4. Even when you have to do something, you are not taking it. It depends. Right. So you could also do some sort of strength adjustment on the case. So, in that case, there's several articles on the way to hide a bunch of stuff. Because from practical point of view, I think we are talking about data which is not like thing about the product of covariance is like estimating like k1 and k2 in some sense my sample size for estimating k1 is n times p2 and my sample size for estimating k2 is n half p4 so it's not really nice yeah so usually those are sort of yeah and gets one for each of them you have for each of these one like i think if you do that's what you do like but m can never separate Well, it's never growing data because it's always something. Yeah, I know, but what I mean is when people started doing the science and they felt like they have ten and hundreds. Yeah, I feel like it's all my show. So it was almost everywhere. Now, I mean, you know, it's weird because I'm not saying I have the same. I'm just like, it's like you have there's like a two thickness. I mean it was DNA very conceptual.