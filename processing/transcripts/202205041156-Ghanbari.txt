So that's interesting as her passions to the box. Thanks for the introduction and so And so as we heard in this workshop so far, we heard about the importance of the interpretability and also to accounting for the bias in the data and also uncertainty of the model. And I'm going to show you an application in genomics where we use deep learning models to gain more insight about regulatory genomics and we account for this kind. And we account for these kind of aspects. So, a little bit of background in biology. As you might know, genes only make up 2% of our human genomes and they encode for the synthesis of the old proteins. But the on and off switches or these regulatory elements that control these genes and regulate when and where these genes should be activated are encrypted in the remaining. Encrypted the remaining 98% of the genome. And these sutures could be on the DNA level, like as a transcription, or it can be also on the RNA level as a post-transcriptional part. And these regulatory elements contain a short sequence pattern called motif that function as a binding. That function as a binding site for some specific protein called regulatory proteins that bind to this site and just regulate genes in a positive or negative aspect. And I'm going to show you an application or like one application in this post-transcriptional regulation where RNA-binding proteins are involved. Proteins are involved. So these proteins are proteins that bind to RNA, as the name suggested, and they regulate almost all aspects of post-transcriptional regulation. And thanks to the new technology that we have called CLIPSEC, we now can be identify the binding site of this protein's genome bi or transcriptome bi. Or transcript on white. So the technology I'm just going to explain briefly. So first the protein of interest is cross-linked with the RNA and then you extract this RNA and protein complex and then you remove like the protein and just sequence the RNA part and then you map this fragment of sequence back. Sequence back to the genome, and then you find this part that is like enriched for these mapped sequences, which is we called peak. And there are some algorithms called peak colors that are designed to find these high accretions binding cycle peaks. And actually, we use these peaks as input for our deep learning models. Models. So we use different kinds of protocols for the CLIP data and also from different sublimes to also explore a broader spec of this RNA-binding protein. But similar to any other biological data, there are some challenges. The first thing is that the data is super noisy, as we might expect. And here, in the case of RNA-by-made proteins, the data is noisy. Of RNA binding proteins, the data is super imbalanced. So, for example, for some RNPs we have only a few hundreds binding sites, and for some RNPs we have millions. And having these challenges in the mind, we developed a method to use the sequence information and also the transcript region information to predict the binding side of RNA-binding proteins. Inside of RNA binding proteins at the same time. So the module consists of two modules. The sequence module extracts information using convolution layers from the sequence and the regional module that extracts information from the transcript regions. And then we merge these features together and fit it to a model. And fit it to a multitask module that predicts the binding site of different RVPs at the same time. So, in this way, we actually define the negative set of one RVP as a binding site of all other RVPs. And I will show you why this is important. And here you can see the performance of the model in the terms of rub curves and precision recall curve. Rock curves and precision recall curve. And as you can see, the performance can vary a lot. So we have like some proteins, like MLP1, that is almost perfect, and we have also some proteins that is really not really. And we look further into this, and I'm not going to the detail, but we could say that this could be due to the poor quality of the data. And therefore, we were not really interested in optimization. Really interested in optimizing the performance score and were more interested in the interpretation. So, when we started this project, there were not so many interpretable methods available, so we were like integrated gradients, deep leaf and LRP. And we decided to use integrated gradients because at the time we were also working with the recurrent DRAM network and the only option was integrated gradients. Integrated varieties. But as you can see here, each of these boxes contains attribution maps for different inputs for different RPPs. And as you can see, this method can highlight the motifs here. And this is actually in agreement with the known motifs for these proclamations. And it's also very interestingly find different kinds of patterns. Of pattern, so it can just find multiple occurrences of one, it can just find like a motif or a repeated pattern like this one, and also like the extended pattern, like for example, some RVPs don't bind to the actual motifs, they just bind to a good element like here. So, and we also find complex patterns, and when there are more than one. More than one segment spatter. And one advantage of like, okay, there's only maybe this is the only advantage compared to the image analysis data because the data is 1D. So we could also obtain this population level interpretation by finding these motifs in the input or the attribution maps. The input or the attribution maps, and then cluster them to obtain a consensus motive for the whole population. And I think this is very nice because, for example, image analysis is not really straightforward, and I think this is one of the advantages that we could use to also compare different interpretation methods. And here I explain the I explained late before that why it is important to have like a multitask approach. And here you can see that we use different negative sets. So we define multitask as a, so the negative set would be the binding side of all other RPPs and we also have that single task. And for the single task we define two different negative set. One is just random check. One is just random sequences from the whole genome, and the other one is random sequences from the band region or the binding side of other RVPs. And as you can see, the performance of the single multi-test method is better. However, we were not really interested in this, but as you can see in the interpretation, this is the interesting part. Because as you can see, in the multitask model, you can In the multitask model, we can really see the motive, the actual motive, or the real motive, that is the unit element here. But this is not really like abuse in the two other single task models. And especially when we are just using random sequences from the genome. And this is really important because sometimes I think this is a common practice in genomics that people just randomly use. Use generate negative data, and I think this is really important to carefully design this negative set because this also affects what the model will learn. And as you can see here, for example, in single-fast models, you can see this G pops out. And actually, it was a funny story because I was working with this and I didn't know what's happening, I didn't know why this G pops out, and at the end, I just G pops out, and at the end, I just went to my mentor and I just told him, Okay, I'm just seeing this G and I have no idea why what this means. And he was like super excited about this. Wow, you find this, and this is actually known bias in the clip data. So, in this way, when we define this problem as a multitask method, because this bias is shared between many experiments. Sure between many experiments, then the model cannot really just distinguish between the positive and negative just by using this biases. So it has to learn the actual motive. And I think this is a very important issue in biological application. And one of the challenges in the biology is to understand the impact of this non-coding segment. Impact of this non-coding sequence variant. So the variants are not in the coding part and it is in the regulatory parts. And we thought that maybe we could use this explainable AI as a potential to study the impact of these variants on the binding side of these proteins. And we collected some data from We collected some data from a cancer database for the variants for the cancer and we tried to find like variants that impact this binding cycle. Here you can see that some examples for this MBNL1 protein that as you can see one variance could actually destroy the the binding site. Destroy the binding side. As you can see in here, in two examples, it can also weaken the binding side, but the binding side is not found because there are multiple appearances of the same motif in the input, and therefore, if one is not available, the other one is available. And it can also create a new binding site. And it was really nice to see that we can see this kind of impact by using explainable AI. AI. However, these predictions are based on only a model that is trained on a very noisy and imbalanced data. So we wanted to also allow the model to say that I don't know. And that's why we were interested in the uncertainty quantifications. And actually, there are different sorts of uncertainty in this past. Uncertainty in this task and also in biological data set. The first one is the data set size. If you are training in the multi-task approach, it's even more pronounced and the noise is also like in every biological data. There are many mislabeled data in biology because there is no ground truth and you just have to like trust the experiments and this is yeah. This is, yeah, most of the time you will get the mislabel data. And there is also differences between single and multi-label settings because of the loss and also the data. And we also have out-of-distribution samples. I think the natural way to obtain the uncertainty score was Bayesian neural networks. However, as we know, it's As you know, it's hard to train these kinds of networks and also they need more data compared to the deterministic approaches. So we came across a new method, not really a new method anymore, this montecarlo dropout or emissive dropout. So the idea is very simple and they claim that it can approximate the Bayesian approach. And so we are all familiar with Robout. All familiar with dropout. So, the only thing is that in the small MC dropout approach, they use this dropout also in the inference time. So, in this way, we also obtain very like slightly different networks. So, it's like ensemble networks, and then you can obtain a distribution by running, for example, thousand times. running for example thousand time inference and then you can use the the the mean of this uh uh distribution as a prediction and the the variance as an uncertainty this is actually an ongoing project but we wanted to make sure that this mc dropout is actually uh doing uh what it uh promised because uh recently there is like a very There is like a very controversial discussion that people are not really sure about the behavior of this MC draft and what kind of uncertainty it can measure. So we decided to empirically compare these two methods in this data and also very small RNA-binding proteins data sets. And here you can see And here you can see the correlation between the predictions and uncertainty. As you can see, the correlation is not really great. So we decided to explore this in more detail. So we look at different aspects of uncertainty. For example, here we look at the effect of imbalance on the uncertainty as you can see on the left part. And the left part is the BNN and the right part MCD. And as you can see here, like this protein has the lowest number of binding sites. And as you can see here, both methods obtain larger uncertainty, but is more pronounced in the Bayesian approach. We also look at the incorrectly classified samples, and as you can see here, both can See here, both can like the for both approaches, the uncertainty for uncorrectly classified is higher, but again, there is a difference in the range, and we are investigating this. So, this is an ongoing project, and we decided to investigate this using a simulated data because we have a more controlled way to actually make claim that they are. Actually, make claims that they are different or they are not different. And here I'm just showing some results that samples that were uncertain, for example, in MNIST data. And as you can see, actually by visual inspection, it makes sense. So we want to just make sure that we are getting if we want to use a MCIT graph out, we don't really miss anything. We don't really miss anything. And here I will just give you some outlook and what we plan to do and what we are doing actually. As I mentioned before, we use integrated gradients, but there are so many other methods available now, and we decided to systematically evaluate these methods. And for this, we need to this we need to also again similar data and we also have to find a way to like an evaluation methods and we heard many talks about how to evaluate these methods and the image setting so we did yeah I have to think about the method that we can also use for the segments data for the base method and we also want to improve the integrity of the model by Integrity of the model by studying the different aspects of model, for example, the loss function, the multilegal or single cell, multilabel or single legal setting, and how this affects the interpretation. And is there a different way of post-processing of these explanation methods to obtain the To obtain less noisier attribution maps, because this is the case for at least for the genomics, that we also have very noisy attribution maps. So we want to actually just find the actual signal and not the actual signal, but actually the complete signal. Because when you are using, we were studying about the effect of this sparsity regularization and we observed that sometimes it's the model that does a The model doesn't learn the complete features, so it just learns the part that is maybe the most important part. And then we are also exploring the way of combination of the interpretation and uncertainty quantification. So we were able to identify the source of uncertainty by using interpretation, and we also want to use also uncertainty to aggregate and analyze the And analyze the explanation. And since there are many applications in genomics that use unsupervised setting, we are also exploring ways to obtain interpretability and also uncertainty quantification in this setting. And I would be very interested in discussing this with you in the break times. And with this, I would like to thank my. this I would like to thank my mentor, Ube Ola, and Sethi, the Talette, the official student that is doing the unsynthetic quantification part and also the funding and the whole group the lab. So any questions about your show that's Uh so you showed at some point a plot with the correlation the prediction. So we were like okay if the uncertainty were the same so that sort of the kind of uncertainty they are measuring is the same the the correlation should be high this is for This is for different models. So we also this is like for different initialization, different parameters. So each dot is actually a one model. But we couldn't really see a very high correlation between them. model uncertainty and the model error. Is it not m uh model error? Okay. Yeah, no, it's just a question if you have thought of having a look at that, because I guess your correlation should be higher if your model is more uncertain. So this is what we see here when we like look into incorrect class for example. So this is I think what you so I think both method could like like Like, like, have higher uncertainty for this incorrectly classified sample, but as you can see, there are not really similar. Yeah, I have a more conceptual question about this drop-out method, because what you're basically doing is just randomly like rolls in the inner layers. But that basically measures to what extent the prediction is kept. Extent the prediction is captured by globally by the network as a whole, or very locally. So I would expect that if a signal relies on very local activation of the network, then it would be more sensitive to dropout, whereas if the signals were captured globally, then it might matter less. But that's conceptually not the same as uncertainty, because even if it's depending on a very local activation signal in your network, you might still be very certain that that local They'll be very certain that that local region should be active. So, isn't there like a conceptual problem with using this is what we are investigating, so yeah, because we were also not really sure because they claim in the paper that it's actually approximating very like the Bayesian approach very nicely, but it was just for, I think, MX data, and that's all. And not really like a different set. Like a different setting, so that's why we were also very not sure about this. But I think in general, it makes sense because it's also like it's averaging, so it's like 100,000 time inference. So the chance that you hit a very important node is very low. So at the end, it makes sense because it's like ensemble methods, because there are also some ensemble methods to quantify uncertainty. methods to quantify uncertainty. This is also, this could actually be considered as ensemble methods because you just have a very slightly different network. Yes, but with an assemble method you have slightly different networks that were still the same as the network contrain. Whereas with dropout you were model and then you're removing parts of the network. model and then you're removing parts of it. So if the model had basically you're going to encode specific cases in specific parts of the network but but okay when you use dropout in the training you also like make sure that the the prediction doesn't really depend on single nodes and it's distributed through the networks. Through the networks. Are you using the training? Yeah, of course. So the only difference is that it's also used during the inference. Normally, you don't really use the dropout in that inference, but you use it only for the training. The only difference here is that they also use it in the inference. Just to add on to this conversation, and maybe another avenue to explore. We did an analysis of dropout again. Of dropout. I think it was ASS faster. But basically, we can see dropout as a regularizer of interaction effects. So it preferentially targets high-order interaction effects. And so if your function has no interaction effects, then dropout doesn't do very much to it. But the more complicated your function is, the more dropout destroys that signal. And so that actually changes the that biases the uncertainty quantification that comes from dropout. That comes from dropout. You might be getting different results and different experiments depending on the function that's actually being learned. Yeah, that's true. Yeah. Yes, well, I have some technical questions about the the of the data, but maybe we cannot talk later. But like I was curious to to include, like, for instance, uh Do we include like for instance structure information or even just like analysing those things? Actually we did like I don't know years of working on RNA structure. It's super complicated. It's like super super complicated and what we so we are using different kind of data and different kind of like prediction methods and we like At the end, we concluded that all these predictions are random. And there was one method, I think in last year, or I don't know, that also used this secondary structure. And they were very, very excited that we find this the best way, and we were. Way and we were also very excited. And one of our collaborators they retrained the network, and this is like the problem that I explained here. So the performance depends a lot on the negative data. And they just use this random negative data. And they got a very good performance. But if you retrain the network with a more reasonable negative set, the information. It's the information from the RNA culture doesn't help, and maybe it's event-like make it work. So, yeah, it's really nice to include this RNA structure, but I think we don't have the good data that we can use for the good prediction method that we can use. That's also another thing that we decided not to go into like things because we decided to go with the genome. So we obtain the pins from the genome and not bothered with finding the isochromosome. But yeah. Thanks again. Thanks again. Shall you make that place enough for trying to put on your shoes and have your stuff? So that's it. Like yesterday was at the log hole. You remember doing one point? Somebody gave One point. One of them is water. Uh shoes. Should I be editing? I'm only media.