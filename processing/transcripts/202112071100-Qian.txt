Okay, yeah, thank you, Noami, for the introduction. And I want to thank the organizers for the invitation to be part of this workshop. I'm really excited to present the work that I have for you today. And it's more like a methodological work, so not tied to any specific application. Like I have an engineering background and then moved into computational methods. And so that's sort of the perspective I'm coming from. And this is joint work with a number of co-authors who are all listed here. Number of co-authors who are all listed here, and we all met at the IACERM semester program on model and dimension reduction back in spring of 2020. All right, so I'm going to dive right into the inverse problem formulation that we consider. So in our case, we're going to consider a linear dynamical system. So x is going to be the state. Okay, here's my mouse. So x is the state. Its time derivatives given by this equation. So x dot equals ax. And we have some unknown initial state, which I'm going to denote. Known initial state, which I'm going to denote p, because that's the unknown parameter that we're going to try to infer. And we endow this with a Gaussian prior. So p is distributed according to this normal distribution. We assume mean zero, and the prior covariance is denoted with this gamma prior. And then we assume that we have measurements taken at times after the initial time. So for some n measurement times T1 through Tn, we have a linear measurement operator C plus some additive Gaussian noise. Plus some additive Gaussian noise. So the Gaussian noise is assumed to be independent across measurement times and has this different Gaussian noise covariance. And A is this D by D operator, so our state has dimension D and our observations have dimension D out. And so this measurement operator C is also linear. And so when we take a Bayesian approach for this, we're going to sort of concatenate all the measurement operators. To sort of concatenate all the measurement operators at different times, we get a forward model that I denote g. So we then get a Gaussian likelihood, so that we say that the probability of the measurements conditioned on the parameter is given by this normal distribution with mean GP. So the forward operator applied to the unknown parameter. And we have this sort of block diagonal observation matrix since things, since the noise, since the noise covariances are, or since the noise is independent across observation times. Observation times. And because this is a linear Gaussian problem, we have a Gaussian posterior. So we have a posterior distribution of the unknown parameter condition on the measurements given by a normal distribution where the mean is given by this equation here, and the covariance is given by this expression here, where h is going to be a quantity that comes back to haunt us. So this is the Fischer information matrix, and it's given by G transpose gamma observation inverse G. Observation inverse G. And this formula will be important later. Okay. So one challenge in doing Bayesian inference is that the computing the posterior is often expensive, especially when the state that we're trying to infer is large. So if D is large. And also another challenge that can come up is if G is only implicitly available through some dynamical systems model. So instead of just being able to systems model. So instead of just being able to compute this like sort of all these exponential, these matrix exponentials, we have to actually like time step a high dimensional system. So the solution I want to talk about today is a dimension reduction solution. So we're going to try to reduce the dimension of both the unknown parameter P as well as of the forward model G via a method called balance truncation. And this is a system theoretic method that we're going to adapt to this balance truncation setting. So like I said, balance truncation. So, like I said, balance truncation is a system theoretic for model reduction. There's this natural way to adapt it to Bayesian inference for linear dynamics, which I hope to show you today. And we view this work as maybe the first work in an exciting connection between this rich area of system theory and control theory and Bayesian inference. And we hope that by establishing these connections, we can sort of borrow techniques from both fields to lead to advances in the other ones. So, we specifically established connections. So, we specifically established connections between some established system theoretical model reduction analysis, as well as some theoretical results on linear inference from this 2015 CISC paper. And by using those connections, we're able to show in certain settings that the reduced model that we get has some desirable properties, including stability and a computable error bound, and in certain settings can also recover an optimal posterior covariance approximation. Okay, so, like I said, we're trying to make connections between two fields. So, I'd like to now present some of the background between these two fields. And I'm going to spend a little bit more time on balance truncation since this is an inverse problems workshop. And I'm not sure how familiar everyone is with system theory. So, the usual setting that we consider that we the user the usual setting for system theory is as follows. So, ignore the bottom line for now. We consider a stable system. For now, we consider a stable system with an input u. So, this is similar to the system we looked at before, but there's no measurement noise. And now we've added this input u that enters the system through this matrix B. And then we have a linear output Y. When I say stable, this means that all the eigenvalues of A are in the left open left half plane. So, this system has what's called an infinite reachability and infinite observability gramian given by P and Q. So, P is a reachability Gramian, Q is an observability. Is a reachability gramian, Q is an observability gramian. If you look at P, what this is, is the input-to-state map squared, kind of integrated over all time. And if you look at Q, this is the state-to-output map squared integrated over all time. So these measure the sort of system theoretic quantities of reachability and observability in the sense that they define these energies. So P defines a reachability energy defined this way. So we've got like X transpose P inverse X. And so a B inverse x. And so a state x that has a low reachability energy is considered easy to reach from the origin, meaning you only need small controls to steer the system from the origin to this state x. And then q defines this observability Gramian, x transpose qx, or this observability energy. And so a state with high observability energy is considered easy to observe because it has a large contribution to the output. So remember, we're integrating like the state to output map over all time positive. State to output map over all time positive time. So, something with a high observability energy really contributes a lot to that output. And so, the goal of balance truncation is to truncate, well, is to reduce the dimension of this high-dimensional system with high-dimensional state X and reduce it to a low-dimensional state of rank or of size R. And so, to do that, what we want to do is retain a rank R subspace of directions that are both. Our subspace of directions that are both easy to observe, so they're contributing a lot to the outputs and easy to reach. So that you know, it doesn't take really large controls to get there. We just, in some sense, we expect that it's more likely that we would be in those easy to reach states. So stated more precisely, the state directions that are retained by balance truncation maximize the following Rayleigh quotient. So on top, I have the observability energy. On the bottom, I have the reachability energy. On the bottom, I have the reachability energy. So, by maximizing this, I have things that are very observable and things that are very easy to reach. And the directions that maximize this Rayleigh quotient are the generalized eigenvectors of the pencil Q and P inverse. So, here's the eigenvalue problem, where V are the generalized eigenvectors and delta squared are the generalized eigenvalues. And so, we're going to be retaining the To be retaining the r directions, satisfying this problem with like the largest delta squared values. And just as a system theory note, like system theorists call the delta the Henkel singular values of the linear time invariant system. So balance truncation obtains a reduced model of size R, where so you have these, instead of the D-dimensional A, it's like the D by D A and like the D by D in B R. The d by d in vr and the d out by d CR, we now have these reduced operators. So we have an r that's much less than d. And we get this model by first transforming to the balanced basis. So that's the basis of generalized eigenvectors, and then we truncate to the leading r states. So that's what balanced truncation does. And balanced truncation models have certain desirable properties in the system theoretic and control theoretic system. So if we're assuming that the original LTI system is linear. That the original LTI system is linearly stable, so all the eigenvalues of A are in the open left half plane, and minimal, which just means that the reachability and observability gradients both have to be full rank. Then the reduced model is balanced, so this means that the reachability and observability gradients of the new reduced model are both diagonal and equal. The reduced model inherits stability from the full model, so it's linearly stable. And the reduced output error has this computable error bound. Has this computable error bound. So we basically have the difference between the true output and the reduced model output in this norm over all time can be bounded by the tail sum of the Henkel singular values. So these are those generalized eigenvalues of that pencil. And this is like the norm of the input. So basically, balance truncation exploits this low-dimensional structure in an input-output map in an LTI system in order to reduce. In an LTI system, in order to reduce its state dimension in a way that allows it to approximate, continue to approximate the input-output map well. And a different type of low-dimensional structure arises in many Bayesian inference problems because oftentimes measured data are only informative in a low-rank subspace of the parameter space. So, this can happen either if you have very few measurements relative to the size of your parameter, or if you have lots of measurements, but perhaps they're like sort of redundant in some sense. Redundant in some senses that they're not informing all of the directions of your high-dimensional parameter. So, what I want to talk about next is a way, like a sort of recent result, or a 2015 result that exploits that low-dimensional structure of the Bayesian inference problem. So I'm going to come back to our Bayesian prior and the posterior, and we're going to focus on covariance. So, like I said, this formula is going to come back. So, to haunt us, so the So, to haunt us. So, the posterior covariance is given by the Fisher information matrix plus the inverse prior covariance and that whole thing inverse. So if we were to sort of apply a Woodbury matrix identity, we would sort of be able to shuffle things around and note that this means that the posterior covariance is going to be strictly less than the prior covariance in the sense of the like the prior. Like the prior covariance minus the posterior covariance has to be positive definite. So we can think of the data as informing our, like improving on our prior knowledge to shrink our uncertainty or to shrink the variance in the posterior. In particular, the posterior covariance shrinks relative to the prior covariance only in the directions where the data has been informative. So if the data doesn't tell us anything about a certain direction state space, it'll be the same as the prior. Be the same as the prior. And so this motivates approximating the posterior covariance with this formula. So I've got a posterior covariance hat here as a negative semi-definite update to the prior covariance. And we like will enforce that negative semi-definite update this low rank because we only have data that are informative in certain directions. So the result of this or the this 2015 system. Or the this 2015 SISC paper, you know, considers this approximation. So they try to find the approximation within this class of low-rank negative 70 definite updates to the prior covariance. They measure the approximation quality in the Forstner metric for symmetric positive definite matrices. And they prove this main result that says the optimal approximation is determined by this generalized eigenvalue problem of the pencil H, where H is the Fisher information matrix, and the prior. Information matrix and the prior precision. And so, again, we have a generalized eigenvalue problem. And the main result of this 2015 paper is that the optimal covariance update lies in the directions that are spanned by the dominant eigendirections of this pencil here. So, the key connections that we have identified and now want to exploit is that both of these low-dimensional approximations rely on a generalized eigenvalue problem and the balanced truncation. Generalized eigenvalue problem in the balanced truncation setting. It's the generalized eigenvalue problem is the observability and the reachability. And in the optimal posterior covariance approximation approach in this linear Bayesian inference setting, it's defined by the Fisher information and the prior precision. So what we are able to identify are some natural analogies between this reachability Gramian P and the prior covariance gamma prior. Gamma prior, as well as between the observability gradient Q and the Fisher information matrix H. And that enables us to propose a balanced truncation approach for Bayesian inverse problems for LTI systems. So that is what I'm going to present next. All right, so let's start with the reachability connection and with the prior covariance. So if we recall our inference setting, we have this linear dynamical system. Have this linear dynamical system, this unknown initial parameter p. So suppose we spin up this system from negative infinity time with white noise. So we have like prior to time zero, there is some white noise input through this matrix B, and then after time zero, it's allowed to evolve, you know, input free. If under this spin-up model, a natural prior distribution is the stationary distribution at time t equals. Distribution at time t equals zero, which has mean zero and it has covariance given by this expression here, which is indeed exactly the reachability Gramian of the system with input matrix P. So this leads to a prior where we have mean zero and gamma prior is the reachability gramian p. So we can choose any arbitrary b to spin something up and define a prior. A natural question to ask is whether any prior covariance To ask is whether any prior covariance can be interpreted this way, and the answer is no. So, we define what we call prior covariance compatibility. So, we say an independently specified prior covariance is compatible with a linear system dynamics if this relationship is satisfied. And what that means basically is that if something is, if a prior covariance is compatible, it lets us interpret the prior covariance as a reachability premium without explicitly defining v. Explicitly defining B. Okay. So now I want to talk about the connection between observability and Fisher information. So I'm going to go back to our measurement model. So the measurements M are given by this Ford model G times the initial parameter P plus some additive measurement noise. So the Fisher information matrix is given as follows by this sum under this measurement model. So we have this sum over finite, like over n observation times of this. Over n observation times of this sum and if we compare that to the LTI observability grain in Q, which has been floating here, you know, something that jumps out to us is there is sort of a clear structural similarity here, right? That like the integrand and the sum and look very similar. There's this difference of a inverse noise covariance in the middle here, but otherwise you can sort of think of H as attempting to approximate Q, right? With a finite sum, you're approximating it. With a finite sum, you're approximating an integral. So, this leads us to propose the use of balance truncation based on not the traditional system theoretic gramians q and p, but rather on the system theoretic grammarian q and the prior precision, which with a compatible, like with a compatible prior, which allows us to interpret it as a reachability gramian. In particular, we use this C bar. Use this C bar, which is equal to the inverse square root of the noise covariance times C, to define the infinite observability gramium, which, if I just go back one slide, lets us sort of match these two up in terms of this sum approximating this integral. And we will assume that the prior covariance is compatible, which lets us interpret it as a reachability gramian. And so when we do this, so when we use this C bar to define Q and when the prior covariance is compatible. Prior covariance is compatible, then the resulting balance truncation reduced model that's based on this pencil has certain desirable properties. So, this is in more detail in the preprint that I linked in the chat, but basically we are able to inherit the stability, the balancedness, and the error bound of the system theoretic setting. And in certain observation limits, the reduced model leads to an optimal posterior covariance approximation. So, when I say certain observation limits, this sort of relies how Observation limits: this sort of relies heavily on how well H approximates Q. So, this, you know, some man being a good approximation for Q. And so, you can kind of imagine this really requires like lots of observations into time, like until the dynamics sort of die out. So, infinite and close together is kind of what we're looking at for those limits. All right, so I would just like to show you guys how this method performs in practice before I take questions. Performance and practice before I take questions. And I'm going to demonstrate this method on two model reduction benchmarks. And, like I said, so this is more of a methods work. We haven't really been driven by any particular application, but I would really be interested in hearing from anyone at this workshop who works on inverse problems in a specific application that sort of fits this dynamical systems model because it's a work I'm interested in developing further. So, the two examples are downloadable from this website. It's like a heat equation example, and it's like a model of the ISS. model of the ISS like structural structures for one of the service modules. For both models, I'm going to compute the mean and covariance approximations for the spontaneity method of 2015, as well as our proposed balance truncation approach, as well as a variant of our proposed balance truncation approach. What's different between these two is just the use of like Q versus the use of H in the definition of the pencil that defines. Definition of the pencil that defines the projection directions. So, all right, so first for the heat equation, under these idealized measurements, like I was saying, how well this does sort of depends on how close H becomes to Q. So, like basically how weather observations getting to be near continuous and forever. So in this case, we have really closely spaced measurements and they go up to t equals 50. And this leads to delta t times h. So, delta t being the measurement spatium being like very close to q. Being like very close to Q. And what we see in this case is that the generalized eigenvalues of these two pencils are basically the same. And that leads to our BTQ approach. So that's kind of the main method being basically the same as this optimal spontaneous result in the posterior covariance. As we get to higher dimensions of the approximation, things like sort of stop matching as well, but they still go down. And also very close results in the posterior means. And then we get the same thing. means and then we get the same thing for the bth result um so with a dimension reduction from the original dimension of about 200 to r equals 20 we get these near optimal posterior approximations um if i go to a sort of worse measurement setting um so i have things that are measurements that are spaced further apart in time and only 10 like only like not going as far out into the future this leads to something with like a 15 um frobenius norm error relative to q so we're Frobenius norm error relative to q, so we're not approximating q as well with the fisher information, and so now we see that these generalized eigenvalues are not the same except for the leading ones, and so we see that the approximation does not do as well in the distance measure that spontaneous optimal in. So we're like definitely not doing as well as the optimal approximation, but this is how we do on the posterior mean. So again, not as optimal as the spontaneous optimal one, but if we look at the relative L2 error, still like a fairly low relative L2 error, so 10 to the Low of relative L2 error, so 10 to the minus 5, which we think is still a good approximation, just not the optimal one. So that's what that says. I'm low on time, so I'm going to just skip through the second example, but the conclusions are similar. So if I have idealized measurements, things are really optimal. If I have really limited measurements, not optimal, but still decent, like look, this is like a 10 to the minus 3 relative error. And I'll just jump to conclusions. So for Bayesian inference of a linear time invariant. inference of linear time invariant initial states, we use system theory to first define a notion of compatibility for the prior covariance with system state dynamics. We define an infinite observability gradient Q that accounts for the noisy measurement process. And together these lead to a balanced truncation model reduction method based on Q and the prior precision that is stable, balanced, and subject to computable error bound and recovers the Spontini optimal posterior covariance approximation in certain limits. And our numerical And our numerical experiments demonstrate near-optimal performance when measurements are finely spaced over long observation periods, so when H gets close to Q, and relative posterior mean errors that are less than 1%, even in suboptimal performance settings when measurements are limited and H is far from Q. So practically, the proposed balance truncation approach yields an efficient forward model for linear Gaussian Bayesian inference from LTI systems. And there's like lots of directions for future work. So we are interested in building a balanced truncation. Works. So we are interested in building a balance truncation for stochastic systems, balance truncation for quadratic, bilinear, and nonlinear systems, analyzing the posterior means, and like I just mentioned, you know, sort of going closer to application settings that are, you know, where you have a specific dynamical system that you're inferring for. So with that, I'd like to thank you for your attention. I can be reached for questions at this email. I have a website, the preprint I put in the chat, and also you can search it with that. Code for these examples is also on my GitHub. For these examples is also on my GitHub, and finally, I just want to acknowledge again that this work started at ISM during the spring 2020 semester program. So, happy to take questions.