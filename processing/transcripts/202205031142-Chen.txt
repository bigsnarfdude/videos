At the end of the talk, I think we're supposed to take a group picture. So I guess I will try to take somehow a screenshot of this movement and try to do my best. And then probably we could also take a picture for all of those that are here. So they will do it. Okay. I feel better if I don't have to do it. Okay. So yeah, so we're ready for your talk now. Yeah, thanks a lot for the introduction. And let me share my screen. And let me share my screen. Okay, can I see the screen? Yes, you can see it. Cool. Then I can start now, right? So, hello, everyone. My name is Ju Chen and I'm a PhD student at Duke University. And today I'm going to talk about our work on concept whitening for interpretable image recognition. And this is joint work with EJ Bay and my advisor, Cinder Rudy. So first, we know like, what is this? So, first, we know like what is this? This is a deep neural network, right? And we all know that deep neural network has achieved great success in so many different tasks, like image recognition, image generation, and so on. And specifically for image recognition, we know that if we train the model with a lot of labeled data, and it can predict the class of the image very accurately. For example, if we feed the image on the left into the neural network, the neural network will predict that this is The neural network will predict that this is an image about sunset. But what else do we want to know about the neural network? We might want to know what has been learned by the neural network, like for example, what has been learned by the specific layer or what has been learned by a specific neuron in this neural network. So this actually raised the question of the interpretability. And it's very important because we want to understand the decision process of the Decision process of the deep learning model because, first, it can help users build trust in the model. It can also help practitioner debug the model when it goes wrong. It can also help domain experts try to gain new knowledge from the model or compare their own domain knowledge with the knowledge learned by the model. So, it's very important in practice. And some earlier attempts on this direction are some post-hoc analysis that try to analyze it after the model is trained. It after the model is trained. And a representative way of doing post-hog analysis is called concept-based one. So it's trying to analyze what kind of abstract concepts are learned in this neural network and use these abstract concepts to explain the behavior of the neural network. And it actually makes a lot of sense because humans also reason based on abstract concepts or symbols and do not just And do not just you know basically reason from directly from the pixels, right? So, this actually makes a lot of sense. And the very early work on this reaction tried to expand what has been learned by a single neuron. And although people discovered that some concepts like objects might emerge in the neurons of a central CNN, this kind of concepts might be impure. Of concepts might be impure. So, if we look at the top-activated image on this neuron in your CNN, we can find that both human face and mountains are highly activated on that neuron. And the second problem is if you plot those top activated images for different neurons, you will see that human faces are activated on both neuron one and neuron two of this neural network. So, this actually means a single neuron of standard CNN. A single neuron of standard CNN doesn't represent a single concept. And to improve this, people propose that maybe we can use a linear combination of neurons, which is equivalent to a vector in a latent space to represent a concept that is learned by the neural network. And this actually achieves a much better performance than just a single neuron. And one of the representative methods is called TCAF. And one potential problem here is because this is a post-hoc. Because this is a post-hoc analysis method, so it has a lot of assumptions for the structure of the latent space. So, one important assumption is it assumes that all concepts are well separated in the image, in the latent space of the neural network, just as the image shown on the right, that all those cluster of concepts are well separated. But this might not be true in reality. So, for example, suppose a distribution is not mean-centered, and suppose it's very far away. Suppose it's very far away from the origin, you will know that all the points are pointing to the same direction, right? And if even if you mean center the distribution, like the one shown in figure B, suppose it is an elongated distribution, you might also have two clusters pointing to the same direction, but representing totally different concepts. So, in reality, concept vectors, you know. Concept vectors, you know, they might be very similar, even though they're representing different concepts. And this is not what we want. And what we want is we hope the concepts to be well separated. And our idea is, you know, why not just do it by ourselves? We can, maybe we can not just doing post-ARC analysis, maybe we can directly build an inherently integral model whose latent space is disentangled so that its neurons. So, that its neurons are representing known concepts, and those concepts are well separated. So, with this idea, we propose our module that can be applied into a neural network called concept whitening. So, it actually consists of two steps. The first step is a whitening transformation that try to decorrelate the latent space. And after this whitening transformation, all the directions are kind of decorrelated. Kind of decorrelated, and we will show that this will help separating the concepts. And the second step is called rotation transformation, where you use this rotation matrix to align the predefined concepts to the corresponding axis. For example, suppose we assign the first neuron to be an airplane axis, and this is actually achieved by applying this rotation transformation. Applying this rotation transformation. And a good property of this rotation transformation is it doesn't really change the structure of the distribution. So it will still maintain the decorrelation property even after applying this rotation matrix. So let me show a visual illustration of the forward path of our module. So the first thing we need to do is first, we want to mean center the distribution. And this is the first step. And this is the first step because we have to do this in order to do the whitening. And the second thing is we calculate the whitening matrix. And the whitening matrix is defined as this. So after applying this matrix W, the data matrix, the covariance matrix is identity. So we can directly calculate this whitening matrix. And after this Whitening transformation, we can see that the We can see that the thin direction on this distribution is actually stretched, and the entire distribution now looks more like a spherical distribution, and all the directions are almost homogeneous. And you can see that the concepts denoted as the red dots and the green dots, they're actually more separated than before. And the next step is applying this orthogonal machine. step is applying this orthogonal matrix or you know equivalent to a rotation transformation and this will help uh you know align um the red dots to the first neuron and the green dots to the second neuron and after applying this we will have all of our predefined concepts to be aligned with the axis we pre-select for them and one thing And one thing we need to notice is after applying this Q matrix, this Q transverse W matrix is still a whitening matrix because Q is orthogonal. So here are some learnable parameters in this module. The first group of parameters is the sample mean mu and the whiteening matrix W. And in the training phase, we can directly compute them on the fly. We can directly compute them on the fly. And past work has suggested that, you know, those two things, you can just calculate them and they just support bad propagation. And in the testing phase, we can just calculate the exponential moving average of this mu parameter and w parameter over all the mini batches. And it's almost the same process as what people did in batch normalization. And this is how we get the parameter mu and w. Parameter Î¼ and W. And the next parameter we need to optimize is the orthogonal matrix Q. And the objective is to maximize the concept's activation under orthogonality constraint. And in other words, for the data points we pre-select to represent concept one, we want them to be as highly activated on axis one as possible. So basically, we just calculate their projection onto that axis and Onto that axis and optimizing that value. And this is also true for the other axis. And this is done by assuming Q is an orthogonal matrix. And this kind of optimization under orthogonality constraint can be directly trained using gradient descent on Steifold manifold. And I know that now we can actually directly use PyTorch to implement this gradient descent, which is quite simple. Which is quite simple. And after getting this model, we can apply it to CNN and to see how it works. So, the first question we want to answer is what is the cost of interpretability? And people are always talking about this accuracy interpretability trade-off. But actually, we found that if we use our module, this kind of thing doesn't really exist. So, the accuracy of CN. Accuracy of CNN plus concept vitamin module is on par with standard CNNs. And we try it for different data sets, different backbone architecture, and different layers, and also different number of concepts we learn in the CW module. And it's always true. And the second thing about the performance is the speed. So although we found that constant bitening is a little bit slower than batteryization, Than batchmalization, it's still quite convenient because we can directly warm star from the pre-trained model. And if we just replace one layer of batch malization with concept whitening, we can get this good model just within one additional epoch of further training, which is almost ignorable in the entire training process. So this means using concept lighting to improve interpretability wouldn't sacrifice accuracy and speed. Sacrifice accuracy and speed. So, the second thing we want to evaluate is how good are the learned concepts. So, we want to know what do the learned concepts look like. So, here, in order to show how well the concepts are learned, we directly visualize the concept axis and we plot the top activated images on the airplane axis, for example. On the airplane axis, for example. And we can see if we apply concept whitening to some later layers, it can learn very pure concepts that the top activated images on the airplane axis are all airplane, and the images on the bad axis are all bad, and the images on the person axis are all images of the person, which is great. And we also found that this kind of concept is not just restricted. Of concept is not just restricted to visual objects, it can also be some more abstract concepts like weather, like the one shown here, and also materials of objects inside the image. And we can see here it can learn, for example, the objects are made of metal, plastic, and wood. So this kind of concepts can be actually very general. Of concepts can be actually very general visual concepts. And if we apply concept widgeting to some earlier layers, the result is also quite interesting. So because we know that suppose we apply it to the second layer and two layers of neural network can definitely not learn some complex concepts like airplane, bed, and person. Although it cannot do that, it can still learn some abstraction of those complex concepts. For example, on the airplane axis, For example, on the airplane axis, you can see that those images are all blue image with some white or gray object in it. And it's very similar to those airplane in a blue sky, for example. And for the bad axis, you can see those are images contained of warm color. And this might be true because the The bed images are often in a bedroom with wooden furniture. So that's why the color looks like wood. And for the person axis, you can see some very dense textures, which also occurs in an image with person. So this is what would happen if we apply concept writing to some early layers. So, next thing is, you know, before we just qualitatively evaluate the performance of the concepts, and now we want to quantitatively measure the quality. So we propose first the first metric is called concept separation. So as I said before, that we hope the concepts to be well separated in the latent space. And that's not achievable by post-hoc analysis. Post-hoc analysis. And in order to measure this, we measure this inter-intra-concept ratio, where we calculate the average cosine similarity between data points belonging to different concepts divided by the square root of cosine similarity of the data points within one concept. So we hope the cosine similarity of different concepts to be as small as possible, while the cosine similarity Small as possible while the cosine similarity of the same concept to be as large as possible. So we hope this ratio to be small. And we measure it for both biologization layer, which is the standard neural network, and also inherently interpretable baseline, and also compare that with concept buttoning. So for this graph, you need to look at the off-diagonal elements of this matrix. So this measures the So this measures the inter-intra concept ratio, and we hope that to be as small as possible and as dark as possible as shown in this image. And we can see that first batchmalization layer, it's pretty bad that the concepts are not well separated, while in our concept wetting module, it's really well separated. And interestingly, if we look at the figure B, which is the result we got when we directly built a concept. When we directly build a concept classifier in the latent space. So it's more like another word called concept bottleneck. And you will see that this, because it didn't apply whitening transformation, so the concepts here are still not well separated compared with concept whitening. And the next metric we measure for the quality of concept is called concept purity. So what we hope is we hope What we hope is, we hope the concept access we got is as pure as possible, right? So, in order to do this, we collect a bunch of images representing the concepts and not belonging to the concepts, and we get their projection onto the concept axis. And after getting the projection, we can calculate the AUC of the activation. And this AUC as close as one actually indicates the concept is more pure. The concept is more pure, and if the AOC is closer to zero, it suggests that the concept is not very pure. So we measure the concept purity of concept whitening module versus the post-hoc analysis competitors that we mentioned before. So as is shown here, we found that the concept purity of concept writing module is always better than the other post-hoc competitors. Competitors. So now we show that the concepts learned by concept mitigating module is actually better than the competitors. And another thing we want to know is what can we use this model for, right? So first, we show that we can actually use those axis as a reference of the reasoning process of the neural network. So if we apply Network. So if we apply concept lighting to different layers, we will for an image, we will get the projection of that image onto the airplane bed axis, for example. And if we apply it to different layers, we will get different points. And then we'll get some kind of trajectory of this data in this concept basis. And as we can see here, And as we can see here, this image of sunset is at the very beginning very highly activated on the bad axis and very low activated on the airplane axis. And we guess this is because this image contains a lot of warm color, and that is very close to the abstract concepts learned for the bed in layer two. Bad in layer two. And when the layer gets deeper, we will see that the activation on the bad axis is decreasing while the activation on the airplane axis is increasing. And this is because this sunset image contains sky, which is closely related to airplane. And at layer 12 to layer 14, we can see that the activation on the airplane axis start to drop because there is no airplane inside this image. No airplane inside this image. So, this shows the reasoning process of the neural network of how it predicts this sunset image as a sunset. So another application is we can actually calculate the concept importance of different concepts in doing the classification. For example, we can calculate how important a bed is. How important a bed is for classifying bedroom. And the way we did this is we just permitted the output of the image on one neuron, and we will get a different loss function. And we divide that loss value called e-switch with the original loss value. And we can use this as a variable importance for the axis chain. And for example, And for example, if we apply this method to a scene classification data set, and we can see that this knowledge learned by the neural network is very similar to human knowledge, that airplane is very important in classifying airfield, and bed is very important in classifying bedroom, and so on. And we also apply this for a skin lesion malignancy classification data set called ISIC. And we found that And we found that so the concepts we chose is first is age less than 20, and the second axis is size of the lesion greater than 10 millimeter. And we found that the age is very close to white, which means it's not important at all. And size is actually more important than age, and it's also more important than the mean importance of all different axes. But still, Axis, but still it's not the most important axis. And the most important axis is axis 76. So we are very curious and we visualize the top activated image with its receptive field on axis 76. And we found that this axis seems to be focusing on some boundary information of the lesion. And to validate this, we took a look at To validate this, we took a look at some medical paper, and we also found that doctors also agree that the boundary of the lesion is more important than size and more important than age in classifying the malignancy of skin lesion. And another thing we can do with this kind of model is called model intervention and model editing. Note that this is not our work, but some related work called concept bottleneck model. Bottleneck model, and their difference is they didn't disentangle the space, but they still built an inherently interpretable deep learning model that based on concepts. And they suggest that we can use this kind of model to do test time intervention. For example, if the doctor thinks that the model goes wrong in learning in, for example, the activation on different concepts, they can directly just modify it. Directly just modify it and modify that will just correct the prediction of the model. So, this is also what our model can do. So, in summary, concept wetting module is something that can improve the interpretability of deep learning because concepts now are disentangled in the latent space and aligned with the neurons in the layer of that neural network. And it has no sense. Network and it has no sacrificing accuracy, which means their accuracy is on par with standard CNN. And it's also quite easy to use because a warm star form pre-trained model requires only one additional epoch of further training to get a very accurate model with constant biting. So here are the links to our paper and our code. Thanks. Thank you very much. I was really happy to listen to this presentation. Do you guys have any questions? Otherwise, I have like a moment to put one. Can you hear me? Yeah. Okay. So wonderful work. And I was wondering where you apply usually your concept wiping. I mean, if I could get you like a restnet, you apply That you apply on one layer, like the penuculate, or you apply in all of them, and yeah, this is the question. So, your question is, you know, for example, where do we, like which layer do we apply concept mining for? Right. So, basically, we can apply it for, so we actually try different architecture like, you know, Like ResNet 18 and ResNet 50, and also VGG. And we found that it worked for a different architecture. And we also found that it worked for different layers. And if you want to get very pure concepts, you'd better just apply it to some later layers. But the results of applying it to some earlier layers is also quite interesting because it tells you how the network will. Like, how the network will learn the abstraction of those more complex concepts in those earlier layers. Does that answer your question? And there is no like if I choose to put the concepts as the labels and I do it and I force the concepts in early layers, do I ever like lose a chorus in some way? Because I'm forcing argumentation that is not good for the network? Good for the network? Sorry, I think the signal is not. I cannot hear your question very clearly. Okay, so the question is, there is a configuration in which you like use your some regularization, let's say, in an early layer that worsen the performance of the network because the regularization doesn't help the Decision doesn't guide the training? Oh, so if we apply it to early layer, it will not hurt the performance of the main task performance is still okay. But if you look at the concept performance, it will be worse because it cannot learn those concepts. Let me try to take a look at that. Yeah, if you take a look at this, you can. if you if you take a look at this you can see like the AUC is you know smaller uh in earlier layers why very large in the you know final layers um but the not like this AUC is just for concept purity this is worse but but the but the main task performance is still as good as before okay okay thank you I have a follow-up question on this so like if the performance of So if the performance of concepts are lower on the previous layers, then doesn't this compromise the reasoning process that you showed before? Because then like the earlier layer, as far as I understood, then you would have a confounding of concepts that you're reasonable. So that kind of impacts your reasoning process that you showed on the early. Yeah, so I think you cannot expect like very early layer to completely learn the concept. And that's the reason why the concept purity is lower here. Because two layer of neural network cannot learn what is an airplane and what is a person. So that's why the performance is worse. But still, it's trying to maximize the activation so that. Activation so that it's already kind of maximizing this AUC under the constraint that this is a very shallow layer. So yes, it might make the reasoning process not as accurate as the layers, for example, as a 16th layer, but it can still. Layer, but it can still show some a lot of information, for example, how the network will think the concept is in some earlier layer. So let me just show you the image here. So here you can see that it cannot learn the concepts completely, but it tries to learn some abstraction of the concept. And this abstraction of concept is also informative in telling you. Is also informative in telling you what kind of knowledge has been learned in the neural network. Thank you. Hi, so thank you. Very nice talk. So I was wondering how your approach to disentangling the latency space compares with other approaches, like for instance, Vita parish router holders. Oh, you mean the, for example, beta VAE? Yeah, so I think that. Yeah, so I think that the application. So, first thing is for beta VAE, like the application is different. They are generating the image, right? But our is we are trying to build a classification network and the goal is first different. And the second thing is, I think beta-bAE, what they are trying to do is they want to kind of discover the concepts. While in this concept writing in neural network, our goal is. Network, our goal is first we just predefine the concepts by selecting images that represent the concepts, and then want to place those concepts to be aligned with the neurons. So like the concepts here is not discovered, but they are actually directly learned from label data. Okay, thank you, David. And related to this, what is the maximum number of concepts that you managed to disentangle well? To this entanglement work well? I think we tried 20 and it's pretty good. And we didn't try more because collecting those data and like the training process can be a bit slower. But the maximum number of concepts that we try, I think it's 20 and it's still pretty good. Do you think is it possible with these results to create a kind of like hierarchy of Create a kind of a hierarchy of concepts, let's say how they are related, like starting from the early layers, like how they like here, for instance, when you say blue related to airplane, and so these like you know an association that you are discovering. So, like at the end, you can have a kind of a hierarchy of concepts in a way. Yeah, I think if you So, if you increase the size of the concepts to be very, very large, I think that might have some problem because it leaves no information almost for the main classification task. And here, I think the trick is you basically still, so for example, the number of axes in this layer is 500. This layer is 500, and you just use, for example, 100 of them to represent concepts, and you still have 400 to get the residual information related to the final classification task. And if you just replace all of them with the concept access, I think this might not actually work. Okay, okay, great. Thank you. Yeah, I I I think that that was one of my questions actually. What happens when you oversaturate your space? And I mean I guess that's the kind of answer. Yeah, I think it also depends on what kind of concepts you choose. If the concept you choose is very close to like your final classification task, I guess it's also fine. But if it's not covering the entire information, I think it will make the final classification task. Make the final classification task, the performance worse. Yeah, and about this, I also had a question because one of the things that actually puzzled me the most when I read this paper was what happens if you try to decorrelate concepts that simply are not possible to be decorrelated. So, here in this image here that you're showing, you have the airplane and there is the very basic first-order statistics of color, right? That is mostly also shining. Right, that is mostly associated and is very frequently associated with planes. So, the planes are in blue sky, so I would expect to have blue, but it's a concept that is very hard to decorrelate from the concept of plane itself. So it works. I was wondering if you looked into that or if you have a comment for that, because I feel that in our feel like in biology or medical imaging, what often happens is that concepts are so close to each other and the boundary so is so Is so narrow that it's very difficult to say whether it would be possible to actually fully decorrelate some concepts. Yeah, that's a very good question. So I think, so in terms of just decorrelation, it can still decorrelate, but the correlation is like the Pearson correlation. It's still kind of decorrelated, but it doesn't mean they have no mutual information. Have no mutual information. Does that make sense? So it will still have mutual information. And I think this is something that cannot be achieved by using our method. And I think it also make a lot of sense to not just decorrelate those concepts because they are naturally correlated. So instead of maybe instead of forcing the covariance matrix to be an identity, you can probably just force it to be some kind of block. force it to be some kind of block diagonal matrix where the block diagonal in the covariance matrix are representing a group of concepts that that are you know naturally correlated while different groups are actually decorrelated in that matrix then I had just a couple of very technical questions so it's like one of the things is this One of the things is this steeflow uh conifone. So is it integrated from the start of training or you use it only for the concept widening part? Oh, so so so yeah, you mean this gradient descent from stifle manfold? Yeah. So I think this optimization process is more like an alternating optimization approach. So the first step is, so it's like two steps. The first step is you optimizing the whole neural network. Optimizing the whole neural network to maximizing the accuracy, for example, final classification accuracy. And then every like every 10 steps, you update this rotation matrix by maximizing this objective. So it's like an alternating way of doing optimization. And just for this styleful manifold gradient descent, I think in PyTorch, there's some model. In PyTorch, there's some module called orthogonal parametrization, and you can directly use that, and it will directly calculate the update for the gradient. And it will force those matrix to be orthogonal. But Sven, when you are learning this rotation, because in the end it's some kind of rotation that you're doing, you know. But if you do this at an intermediate layer, I would expect that. Layer, I would expect that then you have to kind of de-rotate your space again to match it again to what the weights of your model were learning, right? If you have your embedding space that is being learned in one go for optimizing your training objective, then you rotate this embedding space. I guess you have to pre-rotate it back to make it match somehow. Or is my understanding wrong? Yeah, so the so the yeah so so this so the like i think the important thing here is to learn the rotation and uh if the model of course the model can can for example learn the inverse transformation of that rotation matrix but it can still but it's also possible that it didn't learn that uh kind of transformation to still maintain the accuracy so it's it's quite flexible for the second half of the neural network to do what it actually want but the important thing here But the important thing here is it needs to learn this rotation matrix so that those axes are representing the concepts you actually want them to represent. So then just as a follow-up question, and I'll let you go, I guess I can always write to you if I have more questions. As a follow-up, just really last, I was wondering then if your model were not to learn this inverse, so if the model, you're assuming that the model If the model, you're assuming that the model would learn this rotation, right? Because it's powerful enough. But then I was wondering if you had a look at actually whether these dimensions are really used then for the final prediction by looking at the waves or somehow how the dynamics the flow goes. Yeah, I think in our experience, I think the model will be able to learn the We'll be able to learn the projection after this layer to the final classification layer. And if the yeah, you're right, like if the dimension is very high, it's likely that it will not, but like, I think the parameter of learning that inverse transformation is not a lot because it's the same amount as this Q matrix, right? This Q matrix, right? So, a basic linear transformation can just recover the inverse transformation. So, I think the answer is: I think, so suppose we can learn this Q matrix, then learning the inverse transformation can be pretty easy. This is my thought on this question. Okay, thanks. Thank you very much. Thank you again. Thank you again. So I guess I will take a screenshot now of if all the people online could just open their cameras for a second.