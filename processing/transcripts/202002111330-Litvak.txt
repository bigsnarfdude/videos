So mostly we consider zero, one Vernouli random matrices. But I will start just to give some intuition and motivation and to describe previously known results. I will start with plus minus one matrices. So we have end by end matrices. We have end by n matrix, each variable's uh entry sub i d, uh which takes value uh plus and minus one with probability half each. And uh question, quite old problem, what is probability that such matrix is singular? So uh geometric language it uh means uh that we are we are looking for probabilities that if you take uh n vector is uniformly distributed on the q vertical. uniformly distributed on the vertices of the cube, what is probability that they are linearly dependent. And well, so of course you have a very simple low bound that matrix will be singular if two rows or two columns are equal, or one is equal to minus uh another, and uh this probability is uh bounded below by bounded below by this number. So essentially we have two to the minus n, the probability that two rows are equal and then we compute how many rows and pairs of rows and pairs of columns here. And these two counts that you can take one row equals to minus another. Of course I'm cheating a bit, but uh this is essentially uh the overbound. the lower bound. And there were natural well natural conjectures that this is the only reason for singularity. I mean not only the but main reason. And first conjecture is that essentially this term is most important but we just slightly increase this power n. So we have one plus four small of one which means that some function which goes to means that some function which goes to zero when n goes to infinity. And second conjecture is more precise, so we don't lose, we don't want to lose in power anything, so we stay with this 2 to the minus n, but we are losing something in front of this term. So second of course is a bit stronger than the first one. And so there are there are more many results. So there are more many results in this direction, so at least some of them. So first Commerce proved that this probability, which I denote by Pn, that matrix plus minus one is singular. This probability goes to zero, then of course question how fast it goes to zero. And then come Comos and Shremitzi proved that probability is exponentially small, but then of course you want to improve this brace for probability. want to improve this base for probability because conjecture is essentially that this should be half. And uh then uh tau wu improved uh this bound uh uh to three quarters. Then uh Bridgien, Wu and Wood improved to essentially one square root of two. And uh very recently Kosty Chikamirov uh solved uh conjecture one, so he proved that uh this probability is half plus something to the n. Plus something to the n. This corresponds to the first conjecture when you have essentially this. So 1 gives goes to power, so it's half to the probability 1 plus or small of 1 to the n. So this is the history about plus minus 1 matrices. And now let us discuss Bernoulli random matrices, which means that we take also n by n matrix, but now we also But now variables will be 0, 1. 1 appears with probability p, which is between 0 and half. So what is well, the question will be the same, but this is just natural connection to has natural connection to random graphs. So you're the Schneider graph and you have n vertices, and you put H with probability P between two vertices. Of course, if I don't ask uh formatics to be symmetric, formatics to be symmetric, it should be directed graph. And then this is just a regension semantics of such a graph. And question is the same, like I said, that what is the probability that this is singular. And conjecture is that now reason is similar but slightly different essentially. That main reason that we will have zero row or zero column. Not that two rows are equal, but that we have zero row. equal but uh that we have zero row or zero column. And uh of course it's uh even easier to compute uh what is probability. So zero row will give uh probability one to the minus p to the n. This uh to n comes from n rows and n columns. And uh conjecture now I have only I state only one conjecture very precise that we want to have this term times uh one plus something which goes to zero with n. And geometrically of course And geometrically, of course, what does it mean that we have 0 or column 1? Either out of n columns 1 is 0, or they all live in some coordinate subspace. So, well, this is a conjecture, and of course, this is one of examples of sparse matrices, and sparse matrices are very well studied, and with different. And well, with different models, probably most standard model is to take some matrix with ID entries and to multiply by such Bernoulli variables. Also, everything is dependent. And there are many, many works. Gersov, Alexander Techemir, this is Dijk Techemir, Castellovo, Mark with Basak, with Kostauvo, of course, and many others. So what is known about this? So first I would known about this. So first I would like to mention uh Mark Silver and Passak. So they proved uh uh the following. So if you have P which is larger than uh C L Gary form n over n then this probability is well it's what is written here. What we want to get essentially probably I will put previous slide what we want to get we want to get 1 minus p to the n which means get one minus p to the n, which means e to the minus p n, essentially. Uh so here we will have uh additional constant which we don't like, but uh it's already almost optimal result. And moreover, they have uh uh they have estimate for smallest singular number uh so it's not uh just that matrix is singular, it's how far it uh what is distance to say To a set of singular matrices. Well, equivalent definition of singular numbers here. So they essentially proved that this is almost sharp estimate. Square root of p over n, that's what we expect. And there is some error, which I wrote here. If p is not very close to one over n, but say polynomial one over n to power say alpha. n to power say alpha, this will be absolute constant. Depends well not absolute, depending on alpha. So this term will disappear and essentially will have sharp estimate. And another result is I wanted to say why we have this restriction. We should have some restriction here because uh if P is below uh threshold, which is lan n over n essentially, then uh the probability that you have zero O is half, at least. Always help at least. So this is, in a sense, this is less interesting regime. And well, like I said, that it would be nice to prove that this we have just absolute constant independent of p and n, which we have if p is larger than, say, one of the square root of n, but we want to be closer here. And um And next result in this direction that for in the same paper of Kosta, Tikhamirov it is proved that for any P, but now we have P which is constant, which is independent of N. And then we take, well, this result is for N large enough, meaning that larger than some constant dependent on P. But P is fixed here. So it's not like marked. So it's not like uh Mark Pasak exam when they have P dependent on N. Then he proved uh this estimate once again. It's almost sharp to just we have this O of one in the base and we want to have it in front. But this is good for any field uh which is independent of n, meaning essentially constant p. And uh he managed to get uh shark bound on the singular variables. So Singular variables. So exactly as we expected. So this is recent result. And now what is now I want to be closer to the conjecture, to the Shark conjecture. First, in another paper of Basak Woodenson, they essentially proved essentially they proved conjecture when P is close to threshold. So if P of n i is less than 1, Is less than ln over n plus some term which goes to zero with respect to lanland n. Then they got what we want, but remember that if p is smaller than this, then here we just have constant. And they also managed to get the same estimate as before, this is the same constant as before, and times uh two on the that we expect to get. And uh our And our theorem essentially well resolves the case when P is above threshold with absolute constant P and below some absolute constant. Then we get sharp estimate for the probability. For singular value we get not so sharp result. It's uh like n to the power uh log n minus log n, which is uh which is far from what you expect. Far from what you expect, but at least this probability is exactly as we want. And in the case when of constant P, like in Christendom's previous work, so if P is between two absolute constants, then we can improve this singular value, we can get some polynomial. Can get some polynomial decay. But still, it's not what we expect because we expect square root of p over n and not n to the minus 3. Well, but this we have with right probability, I mean this probability. I probably say a bit more about this result. It means that uh whenever well, this uh upper bound uh you need some small absolute constant. But low uh But law uh for low restriction on the for a low uh for interval for P, we can put here some parameter, say q, and then all constants here will depend on q. So if q goes to z zero, then constants behave badly. Well and uh before I say a few words about uh well uh about proof uh and just some probably discussion related to the proof I will Related to the proof, I will say and remind some results about very much related model. So, in our previous works with Panelito, Ostrich Khamirov, Nikolito, Sukregerman, and Pierreosev, we considered related but different model essentially the same sparsity. It was also 0, 1 matrices. But we considered a set of 0, 1 matrices such that in each row and Such that in each row and in each column, sum of ones equals to D. So it's a double stochastic matrix. So we take this set of such matrices and introduce uniform probability and we ask similar question. So of course this matrix is adjusted a matrix of the regular graph which can be defined as this matrix. In once again graphs you can see that the Once again, graphs you consider directed meaning that matrix is not symmetric. If graph is undirected, matrix will be symmetric, but this is more difficult case. And for D we have this restriction, I will explain why in a moment. And conjecture, which was which appears in Castello Wu and Wu papers in 2008, and then in ICM talks of Frieza and Wu in 2008. Of Fries and Wu in 2014, that such matrix is non-singular with high probability. High probability means that probability grows to one, it's non-singular, and then grows to infinity. And they of course formulated for uh directed n no no undirected graphs, it's more natural in graph theory. Uh and uh Cook reformulated this conjecture uh reformulated this conjecture uh for uh for undire for directed graphs or osmatic matrices uh and uh I will mention a few results but first uh I want to mention why we're actually free because if d equals to one so we have exactly one one in each row and in each column then it's just permutation matrix so of course it's unvertible it's not interesting uh in uh uh in uh In if d equals 2, then matrix is singular. It's also possible to show. So the interesting case starts with 3, and you don't need to go above n over 2 because you can interchange ones and 0, and then you essentially will get the same problem. And another remark that two models, I want to say a bit more about this, they are similar, but of course up to some level. But of course, up to some level, because in this model we don't have the main reason which I mentioned for the previous model, we don't have zero rows or zero columns. In particular, if P or corresponds to P here parameter is below 1 n over n, then of course the models behave completely differently. And like I said, why they are similar because on average and of course there is In average, and uh how there is concentration. In average, in Bernoulli model, we have in every row and in every column the same amount of ones. It's Pn. So if you have Z equals Pn, then models should, well, have some similarities and intuitively they should behave the same. Probably it's of course wrong intuition, but still. And at least for the case when D is larger, how much larger we don't know, but How much larger we don't know, but larger than local. And many works were done to study this conjecture. I will mention some of them. First, Cook, when he formulated this conjecture, he solved the case when z is larger than log square of n. I put two years because first is when the paper appeared on a hive, and second is when publication. Essentially, it's important just for first two results. Because Because one year after to prove this, we proved this also the case when G is between a constant and log square. So, of course, our method works above this level, but this level already was solved by Cook. And next well, just one and half year ago, Joan Juan solved the conjecture for The conjecture for non-symmetric matrices for directed graphs. Then Mesar solves the symmetric case assuming that n is 0. And then Gwyn and Wood they produce some different methods which also solve conjecture for non-symmetric matrices and solve symmetric case for This is unsolved for symmetric case for even n. So this is about history. And my third conjecture was that probability goes to one. But then you want to get some estimates for singular numbers and also some estimates for probability. So in another paper Cook proves that if Uh Cook proves that if D is uh larger than uh than one to eleven, then uh singular number behaves like this. And eleven essentially comes from from this. We want this to be less than one. So uh this was his estimate for probability. And uh the same year, but after him uh we proved the better bound, polynomial bound, which works up to n over log square n. Over log square n. And probability also, unfortunately, it goes to 1 when d goes to infinity. So if d is a function of m such that dm goes to infinity, then it's also conjecture. Otherwise, if this becomes close to constant, then this probability depends on d, not on n. So then, of course, problem to first to improve singular value. First, to improve singular value and second singular value, like I said, we expect square root of p over n or square root of g divided by n. And this is just by comparison, say, Gaussian matrix. So we expect that to have the same, but here we need to probably rescale, because for Gaussian it's 1 on the square root of n, but corresponding rescaling here will give you square root of p i t square. So this is So this is uh results for uh this was results about uh about uh the regular uh graphs or the regular matrices. Now I will say a few words but not too many about the proof. And uh of course we will split the sphere uh into several parts and this is uh very common uh way nowadays uh for random matrix theory, especially for singular uh values, because uh singular uh values because uh uh because uh well I will explain because um because we have vectors uh when you apply matrix to vectors it behaves completely differently for different uh uh reactors. And this idea goes back to Cassian when he proved that uh L1 has a proportional uh uh section which is equivalent to L1, and of course he compared L1 and L2 norths. Compared P1 and Tel2 North for his splitting. And the same idea was used by Schachman by 2004. We use slightly different splitting because our reason it says a serial but it's different. So what we want to do, we want to estimate Euclidean norm of matrix applied to vector and we want to show Vector and we want to show that it's not very small. So, of course, we cannot work with all vectors on the sphere. We work with some discretization. So, we work with nets. So, first we want to prove that for each fixed vector you have bound, so a max is larger than something. And second, we want to say that this is true for every vector in the net. So, you need to balance somehow permeability, which should be small. Small enough to beat the size of the net. So, this is a general idea. And now I will give some simple example. It's simpler to give you radomacher values plus minus one to show, to try to convince you that they behave differently. So, if we have just vector of radomacher random variables, so plus minus one. Random variable, so plus minus one, the probability half. And we try to apply, well, this vector will correspond to, say, rho of our matrix. So you apply matrix to fixed vector, which I will take just sum of two first two vectors from the canonical places. So then you get that it is epsilon one, epsilon two, so some of it micro random variables, so it equals zero with probability half. You don't want to have probability help. You don't want to have zero because you want the matrix applied to vertex is separated from zero. But if you have direction of the vertice of the cube, so sum of P r i's, then it's very well known that probability is much better. It's already depends on n and goes to zero with n. One square root of n by little root of at level one. So this also gives this gives us This gives some intuition, and for very simple splitting, you need to split the sphere at least in two parts. One of part which corresponds to the first example, where we have very small support. So say we have small support of vectors, so we call such vectors sparse. And what's going on here? Here we will not get good probability for the events that. For the even that mx is larger than something that we want. But what is good is that in fact here we work with much lower dimension. So size of the net is much lower. And if you think that we want to get bound, probability bound, which depends on dimension, it's more or less clear that you cannot get here very good probability bound if you work in lower dimension, because bounds depend on dimension. You stick yourself to smaller. Restrict uh self to smaller dimensions. And second part uh is uh vectors which are called uh incompany. Well uh I would like to say that uh of course then we will take not only sparse vectors but all vectors which are close to sparse, so uh which will not have be confused. And such vectors we will call compressible. And uh incompressible is uh all other vectors. And for all other vectors we will uh have in We will have in sense in such a situation when we have very good probability, so we can pit the net. And this splitting works very well for rectangular matrices and also for square crease. So first such splitting was used in our paper with Alian and Mark and Nicole. And then this was developed very much in a series. Very much in a series of work for this and the same for square matrices, for rectangular matrices which are close to square. They also gave this name because, well, now papering, there was no name, we just use properties that we want and everything else comes from Buddhist and machine works. And uh for zero one what is this I outline some difficulties. So uh uh first uh First uh first uh one of the difficulties is in approximation because uh when uh in the scheme that I told that uh we approximate vectors by nets and uh we apply uh matrix to it. Or we say that some vector is close to sparse vector. Once again we apply matrix to the difference. So we need to control matrix uh norm of the matrix from as operator from L2 to L2. With some Gaussian same matrices, so in previously considered cases, usually we had that with high probability norm of matrix square root of n. But for Ramo matrix, for Bernoulli matrix, norm is essentially behaves like p times n. And this square root of n is very important. But what is good, good that in fact this matrix behaves, normally this matrix behaves like this only in one direction. We have like this only in one direction. It's in the direction of well, of one vertice of the cube. So if you take this direction out and consider orthogonal subspace, then norm will be already much better. And well, I'm cheating a bit, but essentially not this, but everything leads to our split. Everything leads to our split. And first class will be sparse vectors which are shifted by some constant vectors. And here we need to use norm, because if you shift by a vector which is in this direction, we need to control how much we split. How much we lose when we apply matrix, so we need normal of the matrix. The matrix. And for this class, this class is in fact split in many, many subclasses. But standard transition technique will work. We just use one class, it's not enough. So we need really many, many subclasses and to understand how small it is, how small sub-work, and well, how sparse it is. How sparse it is, and there are a few places, but then more or less standard methods work. And additionally, methods that we develop for the regular graphs. And for second class, we show that the complement of these vectors that we considered in the first class. They are contained in another class which we call gradient non-constant vectors. So, non-constant means that there is no big part of coordinates which are close to each other. Otherwise, it would be in first class. Gradual means that if we renorm coordinates in decrease in quarter, then we will essentially we will not have jumps. Essentially, it will not have jumps. So it will decay more or less smoothly. It will not happen that have some xi and some, say, x2i. If I take ratios, then it's very big. Essentially, what I'm saying now is definition of how we split the first class in many subclasses. And Tau normalization was like a I wrote here. So Fiat is corresponding. I wrote here. So the rat is uh corresponds to to the fact that they are not constant. So if we uh rearrange them but without absolute values and decrease in quarter and we cut uh some proportional amount, say delta n, from the beginning and from the rest, then difference uh on this level will be controlled by some some constant. And uh gradual means that if you normalize vector in such a way that uh foul range increasing constant Fourier range in the case of coordinate. Then on some level, which is our parameter Rn, we have one. Then we control decay of coordinates. Then stop very soon. So then for this class of vectors we use essentially we use Release and Version scheme in ideas. Problem let me say so. Let me say so uh first uh you probably don't have too much time so so what I say that general scheme is uh the same but to to deal with some anti-concentration properties uh introduce uh least common denominator which is uh not enough it doesn't give uh good probability for our purposes. So we introduce another parameter which we investigated and uh We investigated and reported this another parameter which is completely different, it works. So, we'll stop here. Are there any questions for us? So, the singularity of the adjacency matrix does it what does it tell you about the graph? Well for symmetric parts of the regular part is not the vertical and for the erected one it means that there is a vertex with no outgoing agent for it. Rainy it means also not connected with high probability but it is not yet rigid. It's not what? Yatter. At least the function. Any further questions, remarks?