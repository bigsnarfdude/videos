Okay, thank you. Yeah, thanks for having me today. Yeah, my name is Renje Leo. So I'm an assistant professor at the Electrical Computer Engineering Department and the Social and Computer Science Department at University of British Columbia. And I'm also a visiting faculty at Google Bridge. So yeah, I'm kind of a deep learning guy here. So please bear with me with my sloppiness in terms of mathematical rigorousness. So today I'm going to talk about the generalization. The generalization of graph neural networks, which is one of our work last year in iClear. And it's actually a very related work to Ron's talk this morning. So these days we know that genes are as powerful as Whisper-Lehman graph isomorphism testing algorithm in terms of its expressiveness. And we know a lot about expressiveness in this. Expressively these days. And we also know that genes suffer from over-smoothing and maybe over-scorching. But not much is known with respect to their generalization ability in the statistical learning theory sense. That's why like this area is kind of not have much attention in the past. And I think Roan's paper is probably the latest breakthrough in this area for some time yet. Sometime, yeah. And yeah, I want to give you a quick recap on the statistical learning theory in name and language. So the statistical learning theory typically assumes that our data, which consists of input, in our case, image, and output, in our case, a class label, in the IID fashion, that training and test follow the same distribution, and we independently draw a sample from this unknown distribution. And then we typically have And then we typically have a so-called hypothesis class or model class. In the deep learning case, we basically specify all the neural networks that under, for example, certain restriction of architecture and the width. That's our hypothesis class. And then we perform machine learning by using some learning algorithm, like for example, stochastic gradient descent to find the best model. By best, I mean the one with the best training performance. Performance. And then the majority of the results in terms of generalization bound in statistical theory is in terms of uniform convergence. And that just means like for Alley hypothesis in this class H and Alley data distribution and the Alley delta between zero and one with probability one mass delta, we want to have something that as follows like true error is no larger than the training error plus some function. Larger than the training error plus some function of complexity measure of the hypothesis class, sample size, and so on. So the true error is actually in the classification sense, you can think about true error as the probability of making a mistake for your model. And the training error is just the, for example, in the classification case, is the cross-sample B loss. And then you hope to bound the gap using some function that depends on some. That depends on some complexity measure of the hypothesis class. For example, the typical one people use in the history, like VC dimension, read marker complexity, and so on and so forth. And also, like, you hope as the sample size increases, the function value decreases, for example, as the rate of one over square root of m. And then I'm going to describe a technique, one of the tactics in the statistical memory theory. Tactic in the statistical memory theory called the PAC-Bay theory, which is a little bit different from the tactic you see in Ron's talk this morning. And the idea of Pak-Bay theory still builds upon the basic setup where you have IID data. And then the major difference here is based on your previous hypothesis class, now you introduce a so-called prior distribution over the models in your hypothesis class. And then you And then you still have a learning process, like SCD algorithm, for example. And then it gives you a posterior distribution Q over the same hypothesis class H. And the classical results given by David and McAllister back in 2003, something as follows. So the true error, now the error is taken expectation with respect to the posterior distribution over the models. It's no larger than the training. Is no larger than the training error. Again, the training error now should take the expectation with respect to the posterior. Should be no larger than this training error plus some function. And the classic result depends on the KL divergence between the posterior and the prior, and also the sample size M and the delta parameter. And you might wonder, this is kind of very far away from practice because in practice, Away from practice because, in practice, we don't have the luxuries of training very many models at the same time and then gather the samples from this posterior. But in practice, we do have some priority distribution over weights. For example, when we initialize the weights of a neural network, we typically, for example, draw random weights from a Gaussian distribution, right? So that could be treated as one sample from the prior defined on the weights. Final weights. And that's kind of falls into this paradigm. But the tricky part is we don't know the analytical form of the posterior Q and sometimes P we also don't know. So that KL divergence here is intractable in practice. So that's why this kind of theory is not so relevant to practice for a long time, deep learning. But in 2017, the work 17. The work done by Neishabo et al. actually relates the pack-based theory to practice by having this result. And let me try to explain what this result implies in general. So it relates the true error. Now, if you recall that previously in the PACBS, we need to have the expectation with respect to the posterior Q. But here, their result doesn't have this. Doesn't have this expectation with respect to Q, it removes the dependency on Q. And the training error is our just like training, for example, cross-entropy loss. So they kind of remove the expectation with respect to Q in their result. And that's that kind of brings the theory closer to the practice. Because in practice, we don't have the expected true error and expected training error, and we can't compute KR divergence. So in highlight. Divergence. So, in high level, how do you achieve this is as follows? So, you first train a neural network, you know, draw some random weights from the Gaussian distribution as your initial weights and run SGD. You get some learned model with some learned weights. And then you apply some perturbation to the weights. For example, you can add a Gaussian perturbation. By perturbed weights, now you have a posterior distribution defined over the same weight space. The same weight space. It will be centered on the nernum model, but it will have support cover all possible weights in the space. Now you have the Q distribution as a Gaussian and the P distribution, if you treat it as a Gaussian as well, then you can actually have analytical form of the KR divergence and then you can actually work out this whole bound and actually compute it. So that's kind of the idea. So you basically perturb. So you basically perturb the model you learn, and then you get a you construct a Gaussian posterior Q, which is centered on the learned model, and you have a prior distribution which you draw weights from. Then you have the analytical form of the KL divergence. And that brings this whole Pack-Bay theory closer to practice. And in our case, we try to kind of leverage this technique, but working. Tactic, but work it out in the graph problem. For example, here we consider the graph classification problem. So, for example, here are some input graphs and the output is some labels like wheel and the barbell. We want to do the classification and we have some assumptions. So, the first assumption is the IID assumption. So, here the data is not input-output prior, like image and the class label. It's the genesis matrix A, load feature X. Matrix A, load feature X, where each row corresponding to a feature of a node, and then graph label Y. So we assume that this triplet follows an unknown distribution and are drawn identically independently from this unknown distribution. And then we assume the maximum hidden dimension across all the G and layers is this small h. And then the third assumption is all the load features should be contained in the L2 ball. Should be contained in the L2 ball with radius b. So you can get an empirical estimation of this b by compute the norm of your load feature in the data set. And then the last the last assumption is we only consider the simple graphs, which means undirected, no self-loops and no multiple edges. And the maximum load degree we denote it as d minus one. Okay, so this is the setup. And then Setup and then the models we considers are the classic like convolutional graph convolutional networks. Here we use a matrix notation to compactly write it. So the L2 dot here is the normalized Laplacian people use in GCN. And then for the graph classification, you need a global radar layer, which is just the average over the whole node features at the last layer. And then we also consider the mass. We also consider the message passing GN, where, as you can see here, we write the message passing in terms of like three procedures. The first is a computation of message, which based on the load feature, and then you have this C out matrix. It's basically the incidence matrix of the graph. And then you get messages. And then you have another transpose of the incidence matrix that aggregates messages. And then you have the And then you have the per load update nonlinearity. Like the phi is a could be MLP, and the row could be another MLP. Like MLP is a multinear perceptron. And then you do this iteratively, and then in the end, you still do global pooling. So these are two classes of models we consider. And then the first result we derive is to understanding the kind of stability or The kind of stability or the perturbation result. So, here the idea is you apply some perturbation to the weights of the GGN, and you want to know how much the node representation will be deviated from the unperturbed version. And you can see that this result depends on the maximum load degree, which is d minus one, this small d exponentially, and L here is the depth. And L here is the depth of the light work. And Wi is the weight matrix of the GCM, for example. And it depends on the product of the spectral norm of all the weights. And U here is the amount of perturbation you apply to the weights. So you have this kind of dependency. And capital B here is the radius of the input load feature. The radius of the outer board, input feature, load feature. Input feature, no feature lies in. And yeah, so you still see an exponential dependency on the degree and also the product, which will be most likely will be increased as the depth L increases. So that's part of the reason why the bond is kind of loose in that sense. And then for the massive path engine, you can similarly derive a result, but now you didn't see the Result, but now you didn't see the product of all the spectral norms because in the massive passing gene, here we analyze the case, you reuse the weights at every massive passing step. So that's why you only see the width of the spectrum of the width show up only once. But you still see the dependency on the maximum node degree, like in the form of expansion, depending on the depths of the network. And also you have some like Lipschitz. And also you have some like Lipschitz constant C phi zero C G all these things inside your inside your like bond okay with with the perturbation bound now we can kind of get the final generalization bound and it's a little bit complicated but I'll show you the sketch of the proof. So the idea is relying on the perturbation bound we know that if we add a certain amount of perturbation to the weights we're gonna have Weights, we're going to have a certain deviation on the load feature, and then we can use the measured concentration inequality to get the generalization bound, which holds for a certain amount of perturbation. And then we take a union bound to consider all possible weights and the perturbations. And the surprising thing is actually the union bond, you might think there's an infinite many possible. might think there's an infinite many possibilities and actually there's a finite carbon set so that you don't need to take the union bonds of infinite many uh kind of weights so that's how the overall proof goes and then in the in the final bound you can still see that uh you have a dependency of the uh degree maximal degree uh exponentially with respect to the depths of the network and then you have the product of And then you have the product of spectral norms. The good thing of this bond is actually you can compute it exactly given the empirical data set. Because once you learn the model, the spectral norm of the widths, you can compute it, the probability norm you can compute it, and all these hyperparameters are specified by yourself. So you know everything you can compute it. We'll see that how large the value is later on. Yeah. And another kind of And another kind of side finding we have is like if we treat the MLPs or CNs as a GCN applied to a data set like image classification data set, you can actually see that they are the special case of GCN. The reason is because if you treat the image classification problem as a graph problem in the sense that this graph only has a single node. only has a single node and only has a self loop. And the reason why we add a self loop is because in GCN, people typically have this identity matrix plus the normalized genes matrix and then construct LaPassion. So that's why we add a self loop here. And if you do that, then the image classification problem will be exactly the same as the graph classification problem. And you can show and also you can You can show, and also you can show that convolution is a matrix modification, so that actually CNs, MLPs are special cases of GCNs. And that's an interesting finding in the sense that because people already derive the generalization bond for MLPs and CNs, and we here derive the generalization bond for GCN. But since MLP and CN are special pieces of GCN, what's the ratio between these two bonds? And it's actually. And it's actually interesting that they only differ in one term: that is the maximum load degree to the power of L minus one, which is this part. And this is the only part that the whole bound depends on graph. Because if you think about the previous graph, like here, we have this graph with a single load, one self-loop. The maximum load degree is actually. Is actually one and d minus one is sorry, we should have the maximum degree is d minus one. And in this case, d minus one is one. So no degree is two. And that will make the term here to be equal to one. And then the two bounds coincide with each other, which actually verify that you know, kind of our bound. Kind of our bond is a or the pack-based bond for previous MLP or seeing the special case of our bond where one where the graph is kind of degenerates. That's a kind of interesting collection with existing results on the MLPs or CNs. Of course, here we use value activation because in TCM people use that as well. And similarly for message passing neural actor, we can also derive a generalization bond. It's a more complicated. It's a more convoluted, but the tickle message is similar: that you have the exponential dependency with respect to depth for the maximum node degree. And then we summarize the bond at that time before Rhone's work. So you didn't see Rhone's row here, but previous work. So you see that our bond is tighter in terms of like dependence. Is tighter in terms of like dependency on the maximum low degree and also better in terms of the maximum hidden dimension H. But in terms of the spectral norm, it's hard to say from the form. So that's why we later do the numerical experiments to verify. But I want to mention that, oh, yeah, so this is the numerical results we did on real-world data sets like IMDB and proteins, and also on some SBM models. On some SBM models that generally synthetic stochastic block model graphs. As you can see, the bound value is super large, which makes me quite frustrating because it can't explain why GN generalized. Because if you recall that the true error is the probability of making a mistake, and then the training error is the empirical probability of making a mistake. So these are probabilities. Take. So these are probabilities, but the difference, the gap, is much larger than one. So, which means the bound is kind of meaningless in that sense. Yeah, but the good thing is it reveals the importance of this maximum node degree. And recently, we have ongoing work with Professor Lee Carwe at UBC and his PhD student, Emmanuel Seuss. We managed to improve this dependency of maximum degree. Of maximum degree to the power of L minus one to OD directly, which is a kind of significant improvement. But still, when we do the calculation, the spectral norms of the learned model weights dominates so that the numerical values makes the whole bound like kind of meaningless in some sense. But we are still like working on it. Okay, so the trick-home message here is like the in the bond, we show that actually spectral lump of the neural network. That actually spectralum of the nerve weights of GN and the maximum degree are important in explaining the generalization. And this has some implication to practice that if your graph has a large node degree, a trick that can help you maybe generate better is adding a super node in your graph so that the maximum node degree will be reduced. Will be reduced, and then in practice, people found this trick also helps in many cases. So, that could be the case to improve here. And then, another thing is we found that the bond, the generalization bond for the MLPs or CNs with RALU activation are kind of special case of the bonds for GCNs, which reconciled with the observation that those models are special cases of GCNs as well. And then the last one is our pipe-based bond is empirically titled and read the marker complexity-based bound, which at that time is the steady art. And yeah, another thing I want to mention is the difference between our work and Ron's work is that Rhone assumes the generative model of the graph is assumed, right? You have a graph model, but here we don't assume anything about the graph generating process. So it applies to So it applies to any kind of graph distribution. But that's why, in that sense, our result is kind of looser. Okay, so there are still some open questions. For example, is the maximum load degree the only graph statistic that has an impact on generalization ability of genes? And as Rong showed, that in his case, the expected number of nodes and the Minkowski dimension also play an important role. Also, play an important role. So that's good progress along this line. And then would the pack-based analysis still work for other interesting problems like auto-distribution generalization or link prediction, so on and so forth. And then what is the impact of optimization algorithm like SGD on the generalization of GN? So this is the last one is important because like for uniform Because, like, for uniform convergent type of bonds, we are dealing with the worst case. We hope the bond holds for alley model in our hypothesis class. But in practice, when we train the model, we actually just use SGD. So, SGD may have a bias on what kind of models we can find through learning. So, that fact that we ignore SGD on the generalization may be one of the reasons that why. One of the reasons that why our bond is so loose at this moment. Okay, I would like to introduce a bit on other ongoing projects shortly happening in our lab. For example, the first direction we are working on recently is the neural algorithmic reasoning led by one of my students, Sadek, who is here today. Yeah, so in this kind of problem, we actually try to learn a gen on algorithms. On algorithmic problems, like for example, sorting problem or BFS problem, so that given enough input/output pairs, we want to use the gen to learn the underlying algorithm. So, in this case, for example, in the BFS case, you give a graph, you give the starting load, and the problem of learning a output of BFS is just to learn those red arrows, which points to the predecessor. Predecessor in the BFS traversal. So it becomes a kind of a pointer prediction problem. And then we can train lots of different gen architectures on this problem. And we can see actually some of them generalize. I mean, most of them general fit in the in distribution very well. By in distribution, here I mean you can sample graphs with certain sizes for your training data set. Sizes for your training data set, like five to 10 nodes. And then you can test your model on the graphs with five to 10 nodes as well. But you can also test it on very large graphs, like a thousand nodes, which is kind of out of distribution compared to your training. And interestingly, we found that many of the genes do very well on in-distribution generalization, but most of them fail on the out-of-distribution generalization. Distribution generalization. For BFS, actually, some messages passing neural networks do very well on auto-distribution generalization, but on DFS, like depth-first search, all kinds of gene sucks in that sense. So we are still trying to figure out why, why like different architecture has grammatically different performance on different algorithmic reasoning tasks. Another interesting project we are doing right now is the Deloisian diffusion models for graph generation. Loisian diffusion models for graph generation. This work is led by one of another student of mine, like Chi, who is here as well. And in this work, we try to transfer the recent successful generation model for images to graphs. As you can see here in the Deloisian diffusion generating model, they have two markup chains. So the first markup chain is the Q. The Q is the transition kernel is the QXT given that T mass one, which maps clean images to loisy images and RTB maps to pure Gaussian noise. And that Q distribution is typically hand-specified. But we want to learn the P distribution, which is another mark of trend. Starting from pure Gaussian loads, we want to deloyise so that we gradually improve the samples of generating. Samples of generated images. So, this is the model that is kind of supporting the recent success of big generated models like you might heard DALI E2 from OpenAI, which use Deloitte diffusion models as well. But it cannot directly apply to image graphs in the sense that if you trigger the adjacency matrix as an image, you can directly apply. But the problem is, you know, graph models, you want to. Graph models: you want to build a density which is invariant to permutations, so that will not hold for those Deloisian diffusion models directly because those are CN-based models. If you shuffle the rows and columns of the images, the output will be will not be equivariant and invariant, have an invariant density. So that's why we are kind of working on building an equivalent transition kernel for the markup chain so that. The markup chain so that we can still generate graphs as a form of the GSA matrix. Maybe it's hard to see here clearly, but you can see from the GIF that in the beginning, the adjacency matrix is kind of full of Gaussian noise, but as the diffusion goes on, the banding structure appears because here we want to generate a grid graph with random height and width. And if it's a grade graph, if you arrange them properly in terms of rows and columns, you should see. In terms of rows and columns, you should see the kind of band structure in the diagonal. And we also show the final samples generated from our model in the bottom row. You can see that it's basically a gray graph without violation on the topological constraints. So that's another ongoing project. Okay, that's pretty much of my talk. Thank you.