Hey, our next speaker is Tsujoshi Joneda. He's going to talk about mathematical structures of perfect predictive reservoir computing for autoregressive types of time series data. Please. Thank you. Thank you very much for giving me this opportunity to talk. Actually, I have been working on the fluid dynamics for a long time, but recently I could get very, for me, very interesting results in machine learning. So I decided to talk about my fresh results. And believe me, this result is based on the mathematical fluid analysis, not rather machine learning itself. And my message is that many people say. That uh many people say yesterday talk also uh that uh machine learning is black box, but but after my talk, I believe, I hope you can change, you change your mind. So this is my starting point. As I'm going to spawn, maybe everybody knows, and uh this is, I think, crucial insight. And always when we apply the Fourier analysis, always we need The Fourier analysis, always we need to be in mind in that is in Fourier space and the real space, these two things are totally different world. In Fourier space, we use amplitude, frequency, phase. And in real space, we use some kind of pattern. And I think there is nothing to do with each other. Let's be more precise. This is the substrutin principle. So let Ïˆ be a function in a Schwarz class with normalized condition. And for any some local point x in real space and C0 in Fourier space, then we have this lower bound. Well, this is just, you know, undergrad calculus. So this means that scientists That psi cannot be localized in both Fourier and real spaces. This tells us more than what we are thinking. To specify this uncertainty principle, let us consider more extreme case, which is quadricriodic functions already appeared in the yesterday talk. But this is only finite frequency. Frequency with just factor L elements with this one. The point is that, okay, after that, we consider machine learning in mathematics. We need to discretize in T. We take T in integer. And yeah, so point is that we take Fourier transform, this one, then you see, this is an image of the Fourier transform. We have direct delta function like this. We have a direct delta function like this, extremely local. But if we take in through the inverse transform, it becomes totally non-local, something like this. But the insight, okay, first I give an insight. Good time series data should have such kind of Kaji periodic structure. This is also working. This is also work in progress with my collaborator in implementer implementer in my study group. So the crucial question naturally pops up. Can you find a minimum pattern memory and uh can we reconstruct uh function y from uh such kind of pattern memory? From such kind of pattern memory without any full rate information, such as I explained before, amplitude AJ and phase is just a translation Bj and frequency lambda j. And the answer is yes. For me, answer is yes. And uh the answer is was just simply machine learning, which is uh we call recurrent neural network and uh reserved computing. Yesterday talked Computing. Yesterday talked that he used the digital neural network, but here we use recurrent neural network. So could you clarify a bit what kind of information you have at the moment and what you don't? And I understand that you don't have information like amplitude, phase frequency, and what do you have for this reconstruction? I explain detail later. Yes, so that's the point, yes. So first, okay. So, first, okay. Anyway, first we split the data into training and reference data. Okay, so we we have anyway we have okay we have this function given function and okay for range we normalize range into minus one and one okay we we have this data and then we separate into training data and reference. Separate into training data and reference data, and also initial data. So something like this picture. We have a function y here, and minus l minus one we split here, and zero here. And minus l minus infinity, minus infinity, minus l minus one, we use this part as training, and minus l to minus one as the initial. As the initial data. I explain about the meaning of initial data later. And after that, we use this data as the reference data. We split into three parts, the data. And this is, we need to discretize the range. Second, we need to discretize the range. So we choose two k elements such as that. Elements such as that we just distribute in between minus 1 and 1 in the range. And y of range, of course, tends to 0 if we take capital K tend take to infinity. And we discretize the data, something like this. We take argument minimum. Well, I I just use the picture to just okay, discretize in the range, something like this. Something like this. And if okay, this is a function. If the value is the function is something like this, then which is the value a k in this function. And if the function is something like this, which is the value as the a k plus 1. And if the value is totally half Totally half half of these two guys. In this case, we just take here minus 0. So in this case, minus 0, so we take here a k plus 1. If function here, totally middle, then we take this k plus 1. Then we can totally discretize the data. And now I give a remark. Very old machine learning model, which is autoregressive model versus Kazi PODC. The classical this autoregressive model is based on the fluid information. This is a maybe many people in this room doesn't know this AR model, but you are mathematicians, you can understand. Mathematicians, you can understand immediately because this is it. We take P1, Y, just time delay and we put here option and just optimize this option, P1, P2, and PL, up to PL. And so this AR model crucially includes the structure of quasi-periodic functions. Quasi-periodic functions. And this model, well, this is very simple and linear. So extensively being used for the economy data. And so just plug this polynomial into here, here, here, here, here, here, and then we Here, here, here, here, and then we get a characteristic equation. Well, I feel, okay, to be honest, I feel very strange because I realized this AR model just last year and I realized that economic guys doesn't use this kind of classic equation. But for me, this is very important because this is linear. Because uh this is linear. So at first first glance last year then oh we we easily get the characteristic equation but uh in economy paper nobody nobody nobody nobody point point out about this kind of characteristic equation. But uh I insist that this is crucial for this AR model. So we plug this one then we we get the also the polynomial then Polynomial, then whereby the factorization, then if the modulus of all eigenvalues are exactly one, so factorization, and this is all eigenvalues are exactly one, modulus of eigenvalues are exactly one case, then we can get that the solution of this error model is a quasi-periodic function. And the point is that otherwise, otherwise, it is explained. Otherwise, it is exponential or polynomial, growing by the game. You know, polynomial means some Jordan normal form case, we get some polynomial case with other ODE courses you may teach in your course. Okay, so that's why I'm feeling that recurrent neural network or reservoir computing, or machine learning. Machine learning is superior to the classical model, AR model. Because I explained later that RNN and RC is trying to capture the pattern of the function, not capture the public information. So, from now on I explain the machine learning and Machine learning. And I think this is good to learn for beginner. I I still have 20 minutes, so I I believe you can understand. If even if you are a beginner, you can understand machine learning from my talk. Okay, from training data, we make reserva state vector R bar. Please remember R bar, we say reservoir state vector. And after that, we make a W, which is Which is a square matrix, and W in, which is a vector. And the point is that dimension is much larger than capital L. And this pattern memory, if we take random distribution to this W and Wing, we call reservoir computing. And if we optimize this W and Wing, we call recurrent neural network. And the third one, this is our result. We impose a complete structure which we are trying to capture some pattern of the function. Then we can get our result. And I will explain later about the concrete structure. And anyway, first I Anyway, first we need to construct reservoir r bar and function h. Function h is a tongue h as the activate activate function as yesterday talk. And okay, first we okay first we fix time t and put this trend. This training data and the W in, and with the activated function, then we get this R tilde T function vector. And this is induction argument. So we put this R tilde T function into here, next step. And we put here the next. And we put here the next time uh training data. Then we get next time uh this tentative R tilde. And we put again here next time and uh we put next time training data, then we get next time this R tilde. We repeat this one up to chapter L times, then we get R tilde Tt. R tilde tt and we define this r tilde tt as the visible vector alpha t. Then after that, by using alpha t, we use mean square error optimize. Using this reservoir state vector alpha, we can determine the following w out. Okay, just alpha t Alpha T, a large dimensional vector with W out, optimize W out. And while T is just data, training data, then optimize, then we get W out. Okay, now we have we already have now, we have W, W in, and W out. We have three things. Things okay, next step, inference phase. We need to check that the W, W in, W out are indeed the pattern memory. We need to check that. So, from initial data, y var from minus L to minus 1, as I explained before, we need to generate the prediction data. Okay, we write R U by T. Write R u by D from the positive time. The same thing we do. This is just also the induction argument. So we have initial data minus L, y bar minus L, we put here, cloud here, and W in it's already known, and H is a activate function. Then we get next time reservoir data. Then we plug this next time reservoir data into here, and then we put next. And then we put next step initial data. Then we create the next time reservoir and again and again, repeating this argument up to capital L times. After that, we got the R bar 0. Then we impose this W out. Then we finally get U bar 0 prediction data. From time zero, we also repeat again, but we put this prediction data into here, put here, and alpha t also put here, then we generate alba t and again for the next time we put this alpha t and the predicted prediction data plug here, then we get next time. We get next time album, also W out we put here, then we get prediction data, and we plug these two guys into here again, and then we make next time prediction data and reservoir data. And yeah, we okay, the point, yeah, here we echo we don't, okay, maybe somebody knows echo-stated property in recurrent neural networks. Uh, I we don't, we Work. We don't know whether or not this W and W satisfy the equal state property. So we need to subtract the past inference here. Then, anyway, even if we subtract past data, then it goes well. And for the real assignment, we need. We need to deal with this nearly equal, but we omit each detail because this is just technical reasons. Then we can get a main theorem, reference data versus prediction data. That theorem is as follows. There exists well-organized pattern memory, capital W and W in, such that we have this estimate. This estimate is uh great. This estimate is uh great because uh you see u bar is uh uh prediction data and y is uh reference data. And uh of course this exponential growth is very natural because uh error accumul error is accumulating uh forever. But point is that we have this capital K here, so you see this capital K comes from a this K comes from a disc discretized discretizing range. So if we take this chapter K to infinity, then we can get this u by t is coincide with this y t. So this is what I say, perfect prediction. So HL proof, only one slide. H of proof. Only one slide. The point is construction of uh w and w in and uh with permutation. Okay? So from here I can ask we can I can answer your question. So pattern we extract pattern. So So you have one to L. So something like this. So this pattern J here we have some A. And uh and uh we define adjoint here so the adjoint is uh just uh take here sigma just uh sigma j over so we call this adjoint and uh we have And uh we have uh capital L capital N element here. Parameter N appears here. And the N means uh number of all buttons. Okay, so you remember that we have a function piece. So we discretize here, then we extract every buttons. For each time t, we have a pattern, capital L length pattern. So we extract this pattern by using a sigma j and we define adjoint sigma star i and uh we can estimate this capital N. This capital N is less than two k minus one with the power L. This is due to the sequence of sequence with repetition. So With repetition. So, but I believe this value chapter n is much less than this sequence with repetition. And I think this chapter n also deeply comes from uncontrolling principle. I don't know how to estimate this chapter n mathematically, but yeah. And anyway, now we can define w in w in we define by using uh this adjoint. Using this adjoint device. Then, RNN, recurrent neural network, can be rewritten as the following linear algebra. Okay, capital X, this is a square matrix. Y is also square matrix. And this comes from, as I said, just the induction argument. Just induction argument. We apply the induction argument here. So, from such kind of induction argument, we get this formula, capital X and capital Y. And I think this observation is related to the Kuckman operator, but our study is more much more close to the practical sense. And the lemma construction of the unit matrix W, we get here. If we have inverse matrix and we can have inverse matrix, then we get this W. We can construct this W unit matrix. So, conclusion. My message: recurrent neural network is composed of the Composed of the pattern memory with investment recognition. This is my message. So I think now you feel that, oh, machine learning is not black box. It's pattern memory with industries business. And my future work is analyze chat GPT. I have a funny story. I asked ChatGPT directly that, are you using recurrent neural network? I asked he or she, I don't know, said, well, I don't use a recurrent neural network. He said, I use a transform model. He said, he or she said. And I asked again, well, origin. Origin, but origin of the transformer model should be recurrent neural network. I asked again. Then he or she said, well, well, yeah, you are right. Yeah, but still, different. Still transform model and recurrent meteorology. Different, he said. So I realize that he is more than engineering. He he has an engineering mind. Has an engineering mind because I'm an analysis guy, so always I feel recurrent neural network is close to the transformer model, but he said it's not. So yeah, it's funny story. So thank you. Thank you very much. Do we have some questions? Thanks, very nice talk. Could you go back to the slide or out of anything is better than you said like that is factualization you could say that like the solution of like a L model is either as periodic or As periodic or exponentially growing or like decaying, yes. I have no almost no mechanic machine learning, so I don't I cannot really sense why it is not. Give me some intuition of why it is but a machine learning. Oh, you mean why this is if wh why this RNN is created AR? Yeah. Oh yeah, so Yeah, so yeah, that's a good point. Because AR model is based on the Fourier information. So you see here, I wrote here lambda 1, lambda 1, and lambda here. Lambda is frequency. Lambda tells us the frequency. And frequency is the most crucial in, I believe, AR model. Model and uh but uh okay okay this means like uh you know like if this thing is like has periodic or like exponentially growing or dying you have very little information very little information yes yes uh yeah very little information on yeah that's uh It's a deep question. Yeah. But yeah, just going back to the Ansar 20 piece file. So time series data is we need okay. We need a real space data to how space data to have uh future prediction, I believe. But uh in my feeling is that using uh such kind of frequency or something is not good for the such kind of uh prediction, future prediction, because it has a it has a past and a future, which means that uh it should be we have to treat we have to deal with the real space. Something like that. Something like that. Then like you get very little certainty for the real space, which is very. Yeah, yeah. Are there any other questions? No, let's uh thank the speaker. And uh yes that concludes our uh workshop. We have a still a coffee break on the schedule, but uh so thanks thanks to everyone. Very nice question. Yeah, just thanks to the organizers again.