All the organizers for inviting me to this very nice workshop. Yeah, I think I can just share the screen, right? Yes. Yes, and I would ask Igor to record the. Yes, I already started. Ah, thank you. So, can you see the screen now? Okay, it's okay. Perfect. Thank you. Okay, yeah, so writing verse of the divergence. Okay, this is well, maybe some people know it more under the name of the Bugovsky operator. So this is actually a lockdown work. So I really wish to acknowledge here to my co-author, Oli Sari. So all this research went in a Zoom collaboration, essentially. Collaboration essentially, and it was like: I don't know how it was for you, but for me, the second corona winter was even more frustrating than the first one. And somehow, this is essentially what came out. I mean, let's say this is sort of from my part, maybe the most interesting thing that I did in winter. And yeah, so many credits to Olisari from the University of Bonn, who somehow kept me doing science over this period. Over this period. So, let me introduce to this subject. So, what and why is this? So, we are looking for this operator BF, which is also known as the Bugovsky operator. So, this actually takes a scalar function f and produces a vector in Rn. So, what is important here is that we are considering non-cylindrical space-time domains. So, this means that by So, this means that by this omega t, I mean a time slice here of a domain which could be anything essentially in space-time. So, yeah, so this one dimension is the time variable, and these n dimensions are the space variables. And the interesting thing is that the Bogovsky operator is a space operator. So, essentially, it only acts on these n variables. So, the question is, why do we care about time at all? And the reason is that we look for a very special right inverse. A right inverse of the divergence. So we look, okay, we want some operator which inverts the divergence. So the divergence of this vector here should be the function f again. And somehow what makes it a time-dependent operator is that we also want that this operator has zero boundary values. And yeah, okay, this is what makes somehow this inverse a Bogowski operator. And this is also different to, I don't know, just different to uh i don't know just solving a laplace equation or something yeah so that you have like full zero boundary values for the for the uh for the for the operator here uh due to this boundary conditions here uh so you see this is slice-wise so yeah so for for all of these slices uh of omega time slices yeah we want that the the zero boundary values are satisfied so we also need a compatibility condition for each time slice so For each time slice. So, here this means essentially that the integral over the function f, which is a scalar function, is zero over each time slice. This is what this means. And so for each time slice, we want to sort of have an operator acting on it such that it inverts the divergence and that we have the zero boundary values. And so this is not the only aim. So this is like these are the first two things we want. So we want an inverse of the divergence, we want the zero boundary values. We want the zero boundary values, but we are also interested in estimates. So, further aims are the estimates for, this is the most classical one for the gradient of Bf. So, this is something like a Carlton von Sigmund operator, taking the gradient of the Bogowski. But we're also interested, in particular, in the time derivative of this Bogovsky operator. Even so, the operator is inverting the space divergence. There is like this time dependence. There is like this time dependence due to the zero boundary value constraints. So essentially, we want all kinds of estimates. This is what we wanted to get. Okay, and here's like the first observation. So, and I'm now going to introduce somehow my point of view on this Bogowski operator or somehow what we found together with Ollie, like our heuristics on this operator. And okay, the key observation is maybe this very simple thing here. This very simple thing here that okay, essentially, because I can put the gradient to the left-hand side. If this is a test function here, this is an identity here. And so, by definition, if I like take the adjoint of this Bogovsky operator, I get something which is like reverting the gradient. So, essentially, any Bogovsky operator, so any inverse of the divergence, clearly. Clearly, it is a dual of minus the gradient. And this is, of course, known and was used even, I think, by Pokovsky by many people. But this point of view is sort of important for the next, for the heuristics. They come in a few slides. First, I have another motivation. So this is a motivation from something which I did a lot recently. This was like working on fluid structure interactions, so-called. And the motivation. And the motivation for Bogovsky operators is mainly in fluid dynamics. There are many other applications, but this was sort of my focus here. And in particular, I'd like to study this unsteady Bogovsky operator is motivated by these non-cylindrical domains, which appear, for instance, if you imagine like this solid here to move down or to fall down here in a fluid. So essentially, In a fluid, so essentially the domain here, the fluid is the blue part here, and this is a solid. So essentially, the fluid is only situated here in this blue part. And when this ball here is moving, then the domain is changing. And in order to estimate this, for instance, the pressure in this time-changing domain, we need an operator that is able to sort of adapt to the time-changing situations. Time-changing situation. So you see, these are like velocity profiles. So you see the velocity somehow turning around here. It doesn't have so many singularity. And here, what is important now, sort of when the board is very close to the bottom, then essentially you don't even have a Lipschitz domain here. So this is like here, it becomes very thin and it's not uniformly Lipschitz eventually. And what is more important that precisely in this region here? Precisely in this region here, this is like the graph, the pressure. Precisely in this region, the pressure builds its singularity. And I will explain in the next slide that the Bogovsky operator can be used very well to get pressure estimates and to construct the pressure and to get estimates for it, to know what it looks like. And somehow, this is here my motivation: this picture, where you see that essentially exactly where we want to know how the pressure looks like, or where we have to expect the singularity. Or where we have to expect the singularity. This is sort of where a Lipschitz Bogowski operator cannot be used. So, this is like the problem. So, we have a time-changing domain. This is the first thing. And we have a domain which is not uniformly Lipschitz. And if we think about, let's say, other applications, maybe some of Tormo's favorite applications like moving particles or homogeneization flows. Organization flows, approximations of porous media flows, particle flows, then the domain can look much, much more complicated and much, much more non-Lipschitz. And successfully, Bogovsky operators have been used in this framework. Okay, so this is like maybe the most important motivations for building such an operator, which I'm going to show you how to do it. And so here's like this Navier-Stokes equation. This Navier-Stokes equations, just again, still, this is motivations. So here's the pressure, this is the pi, then we have a velocity, and it obeys this set of equations, the Navier-Stokes equations. And this is now studied in some time-space domain, which can like look what it wants to. We have an outer fort, we have a force, we have a positive viscosity, and then eventually one can also. Eventually, one can also consider so-called very weak solutions. This means the moment we know that we have a velocity which is an L2 function, locally, maybe even, and we have a pressure which is like the weakest possible space ever, in the weakest possible space ever. We can essentially write down these equations here for divergence-free test functions. So, here this is, of course, Here, this is, of course, a scalar function, so this is not divergence-free. I'm sorry for this, but like the momentum equation here, this is integration by parts of the first line here, and this produces some concept of very big solution. And maybe this is the only thing that you can expect to have. In many situations, this is what you can expect to have, in particular when the domain is rough. So this is. So, this is essentially what you can expect. Okay, weak solutions you can expect, the Larais solutions, yes. So, essentially, the gradient maybe can be kept here. But why not considering the weakest possible situation here? And the question is now, if you have such a velocity, which you maybe can produce, what do we know about the pressure? And so essentially, now on the next slide, I'm going to show you. Essentially, now on the next slide, I'm going to show you how to use the Bogovsky to get some information for the pressure. So, this is usual. Yeah, I mean, for Stokes, you just minimize over divergence-free functions the Dirichlet energy, and you get a velocity. And in the unsteady setup, it's very similar. You can usually produce a velocity without considering the pressure. And the pressure is then this Lagrange multiplier, or yeah, sort of this leftover of the. Left over of the construction. And so, this was the question which I was asking already before. So, what kind of pressure can exist? And okay, somehow you want to use this guy and somehow make it a gradient, right? So, this is the heuristics. And so, you want to put the left inverse of the gradient to this left-hand side of the momentum equation. And how to do this rigorously. And how to do this rigorously is the following way. Well, you take a right inverse of the divergence and you look at this functional here. So, yeah, so you just take a test function phi and don't care about the regularity at this moment, which is mean value free. So, you can apply an Apogovsky operator to it. And then you look at these functionalisms. Uh, look at these functionals, and you can split it up. You can look at each of these functionals independently, you can like solve them each. However, it's important that you always have the same operator here. You can look at them independently, but you should always take the same operator for that in order to construct the pressure. So this means we need a very flexible Ogovsky operator here in order to apply it. And this function actually is a pressure or more precisely. More precisely, the gradient of this functional. So, the gradient of pressure acting on the psi, you can then write it as the solution to this. Yeah, I mean, you can use this formulation. You can write it here. You put the divergence of the psi's into this Bogowski here. And then you see that because you know that for divergence-free functions, this momentum equation is already satisfied. This was the condition. So, essentially, you can replace this Bogovski. So, essentially, you can replace this Bogowski of the divergence by the test function itself, and you get the equation you want. And as I said, in order now to know what the pressure does, we need to know like regularity of these parts here. So now the aim is actually a bit more. We don't only want estimates on the gradient and the time derivative, but maybe also on the Laplacian here or other things. We need the zero trace. This we need for the in order to. Zero trace, this we need for the in order to justify the partial integration here. Yeah, here are some more applications for which you can use the Bugovsky operators. This is like the homogenization part where you have this like the homogenization for fluids, this work from Tata and Alaire. You need like restrictions operators in order to make like cutoff functions divergence-free. For this, you can use the Bogovsky operator very well. Operator very well. You can build solenoidal extensions. You can do for the compressible Navier-Stokes some higher integrability estimates for the densities. So there are many, many applications of the Mokowski operator. Right. Okay. So this is like the motivation. And now let's look at the background. And the background here is the time-independent domains, of course. And so this is like the famous Bogovsky's. This is like the famous Bogovsky's theorem. Somebody told me this he did not invent this, but I don't know. I mean, somehow, this is like the original, let's say. It's a Kaideron-Sigmund operator. And essentially, it's some kind of integration, which I will show on the next slide how it works. And it is, yeah, it's well defined for all functions. It produces a zero trace here on, let's say, we take a star-shaped domain. Shaped domain with respect to some ball q or some reference domain q and we have the we have we have an inverse of the divergence and we have like these natural bounds and they hold essentially for the full fractional regime of Sobolev spaces. So you can take it in negative order and positive order. Yeah, so this is you have it like the full story if you wish. The full story, if you wish. Of course, it depends on how many zero boundary values you have here, how much it will be translating to the Bogovsky operator, but essentially you can always like apply this kernel here and get the right inverse of the divergence. And somehow this is like how we call it. We have like the right inverse of the divergence in this form is a Bogovsky integral, and the left inverse of the gradient, which is the dual, is called a Poincare. It's called the Poincaré integral. And so this is somehow important to keep in mind that there is this duality. And yeah, here's some more references for steady domains, for stationary domains. So you can like, because I mean, it's a local operator, so you can cut it into pieces with a partition of unity, and you can produce Lipschitz domains as a union of stars. As a union of star-shaped domains. This is known for some time. You can check it, for instance, in Gaudi's book on Navier-Stokes equations. What is more recent is like the construction for so-called John domains, which are, let's say, similar to Lipschitz, but not really. I mean, they can have fractionals on the boundary, but essentially you still can reach. I will talk more about this, but then. I will talk more about this, but for John Domains, there's a very, very nice theory. In particular, I would like to mention this seminar paper by Deening Rushischke and Schumacher, where they produced a very, very nice way of getting John domains by covering arguments. And somehow, our techniques will need this approach also here. We will actually need ideas from all of the papers here. And it was important. And it was important somehow for us to study everything before we could start with this with this time-changing domains. And okay, so then here there is what is very nice is also this paper, which is a construction. And yeah, somehow these are like always the gradient bounds, which are here mentioned for arbitrary bounded domains. And somehow here, here now the interesting stuff begins with the rough bound. Stuff begins with the rough boundaries, so to say, that you get a weight which depends on the distance to the boundary. So, essentially, when you leave the Lipschitz or the John regime, then close to the boundary, you need either more information on the right-hand side or you will lose on the left-hand side. So at the boundary, singularities can occur for this operator, and this somehow can be quantified in this very natural weight of the distance to the boundary. So, I think, yeah, in this. So, I think, yeah, and this is also known from this distance weight, they also appear in a natural way in regularity theory, also. Yes, and so when you look at, I don't know, Laplace operator with Dirichlet boundary values. I mean, it's just somehow a very natural thing that appears whenever you leave Lipschitz, essentially. And sort of what we are interested in is now to generalize the theory to Herder domains because of this motivation which I gave at the beginning. Which I gave at the beginning. And something which is related to Hölder domains are the S-John domains, which are sort of the Holder, the extension of what is the John domain to the Lipschitz domain. The S-John domain is to the Hölder domain. So it's like sort of a fractional continuation. Yeah. And so this is like the general message. Beyond Lipschitz, there's a loss of regularity, but it's near the boundary, because it's a local operator. Okay. Okay, so this is what we have, and let's see what we can do now for the time space setting. So, here is like the Poincare integral. And so, okay, the idea is really you invert the gradient. I mean, so you dualize the gradient. So, essentially, what you do is, okay, if you take a value x and you look at the function value x, then you can write down this value fx, and maybe you have to subtract the f of z here. The f of z here. So if you take f of x minus f of z, you can write it as an integral. So essentially, when you map the gradient to the function, you have many ways to do this because you can essentially choose how to integrate, how to integrate the gradient in order to reconstruct the function from it. And there you see how actually the flexibility of the Borgovsky operator, or how weak the divergence equation is, if you wish. You have like, if you have a gradient. Like, if you have a gradient, you can deduce from it its function by many, many ways because the gradient essentially has a lot of information of the function in it. This is the idea. And so this is maybe the central idea of Bogowsky. So you can do this. So you go from z to x using the gradient. And now you can sort of smooth this. So you can actually like do this integration by doing it over a large set. By doing it over a large set. So you always get the function value at x, but you get it with lots of reference points z, and this is where the modification is happening. So this is where the kernel is integrating. So this, if this is like, has a has a measure mean, if the ball has like, let's say, mass one, then you would get here f of x minus the mean value of f of z dz. So you would really get here that if the mean value was three, then so if the Was free, then so if the integral of the function f over b was zero, then you would get f of x is equal to this modified integral. And now you can take this Poincaré integral, plug next to it the function, and calculate the dual kernel. And if you calculate the dual kernel, then you reach with this integral here. And you see that sort of this is now the Bogovsky at the point x turns then out to be the integral over this code. Then out to be the integral over these cones, but it should be really these lines. Yes, this is more how to see it. So, this area is what we will integrate, but essentially, we are really integrating over these lines coming from the boundary. This is what the Bugovsky does. And then you see sort of the closer the x is getting to the boundary, the smaller this cone is getting. And this means at the boundary, we will be zero. Or, and this is the better point of view, integrating away from the boundary. Integrating away from the boundary, you will like always go away in some kind of Lipschitz manner. And this is what the Bugovsky is essentially. It took us really months to find this. And yeah, so this is sort of my summary here. Okay, here are the comments. So this is the construction of the Bogovsky. And here are my comments. So it's a kind of unsequent kernel. The support is nicely. Is nicely, it's in the context HAL of the support of the function and the reference ball over which you integrate. And the operators work for all domains that are star-shaped with respect to a fixed ball. This means if you take a ball and take star-shaped time-changing domains, you can always use the same operator. This means the time derivative would not attack it. So, if you have a domain which is uniformly star-shaped with respect to one. Uniformly star-shaped with respect to one fixed ball, you have a uniform in time Bogowski operator, and you always produce zero boundary values. So this is an important observation, I think. So what can we do now with the John domain? So the John domain is rather similar somehow, but the difference is now that, and this is like the Acostas-Doran et al. approach, you essentially replace taking line integrals by curves. Is taking line integrals by curves. And this sort of produces the definition of the John domain. And then you can again modify over these curves. And this is possible because you still have the cone condition and the cone condition is the John domain condition. And so you can produce a Poincaré integral and then you can dualize it and produce a Bogowski operator, a Bogowski integral. So the dual might maybe. So, the dual might maybe look a bit funny. So, here's the reference board here, like the curves. And then, but essentially, you still get something which will produce the zero boundary values in the end. So, this is somehow the constructive way how to construct the Bogowski here. And now let's see the John domains. So, here are the pictures again. So, this is a John domain. This is a John domain. These fractals are John. These fractals are John domains, however, like this outward cusp, in particular, if it's bending, it's not a John domain anymore. This means, in particular, like this example of the ball falling down is precisely not covered by John domains here. So we have to think about something else here. Okay. So it's not that general. And so we have to, and we want to get some more general theory here. So. So, yeah, and here is where my internet problems or my started. So, this is now let's see. Somehow I had problems producing my lecture just before the talk. And so, I hope that the slides are still okay, but there might be some irregularities. Okay, so this is like the summary of the stationary constructions. Yeah, but essentially, I was talking at Yeah, but essentially, I was talking, I think, about everything here. So, let's look now at the non-cylindrical result. And for this, so this is our setting. We really said, okay, let's start with something very general. And the general setting we are having is like that. We really just take an open connected and bounded set, which is alpha-holder continuous in time and beta-holder continuous in space. Maybe just continuous, maybe Lipschitz. So, this one means here that it's Lipschitz. One means here that it's Lipschitz. And then we assume that we can cover, I mean, what this means essentially is that we can cover it locally by graphs of functions that are alpha-herdocontinuous in time and beta-herdocontinuous in space. So we need this graph setting here. And moreover, we refined it so by just saying, okay, somehow it plays a role. What is the Hausdorff dimension of the boundary, if you wish? So how thick the boundary can be. And so here's like this theta number. So, here's like this theta number, and this says essentially how thick the boundary can be. So, yeah, so essentially, if you know that it's a surface, then this theta will be one, even so it might only be Hurdle continuous. So, there are many non-trivial cases where theta is equal to one, but beta is not equal to one, smaller than one. And these are interesting cases. These are essentially when you have some solar effort. Essentially, when you have some solvolific regularity for the gradient of the map. And then this is like the natural space somehow. Observe that we don't talk about the zero boundary values here because like in the book for the Bogovsky operator, you can always apply the operator, even so you might not produce zero boundary values on high order, but you can always produce something. But what is important is that you have the compatibility condition, and this compatibility condition, you need it on all slices. Condition you need it on all slices, all time slices. Okay, so this is the setting, and this is the result now. Okay, this looks like a okay, so let me explain this. So we have an operator, that's the nice thing. We have an operator which is defined on this space, and it is a right inverse of the divergence, that's the first thing, and it has zero boundary values. Okay, so. Uh, okay, so this is actually wrong. So, this is what I was saying. So, essentially, the zero boundary values is just of the first order. Yeah, so the BF is zero on the boundary. And if you had here Z infinity, mean value zero and zero boundary values, then you would get also the C infinity zero here. And here is the estimate now. Okay, so we have like now the full story. Yes, so we have k plus. k plus one derivatives in space, we have kappa derivatives in time, and now we have like a loss on the boundary which naturally relates to alpha and beta. So this means here we have a loss with regard to the Lipschitz regularity sorry, with regard to the Hölder regularity of the domain in space. So this is the beta, this is the space regularity. The beta, this is the space regularity. So you see, this measure vanishes the moment beta is equal to one. So the moment we're in the Lipschitz regime, yeah. But if it's holder, then you have lost. This means you can maybe have a little singularity at some outer cusp. This is sort of what can be here. That's why you have this weight. And on the right-hand side, this now looks a bit annoying, but essentially you get like this horrible object. These horrible objects here, this means you get a distance weight, which somehow depends on the lower order terms. So, what does it mean? So, here, this gamma is now the derivative in space. So, if this is like highest order, if you're on the k level, then you don't see this weight at all. But if you have a horrible domain, then this good information on the gradient in a higher order does not mean that you have good information. Does not mean that you have good information on the decay, how you reach the boundary for lower order terms. So, this is why somehow this non-trivial weights are appearing and they become more serious even in the case you look at a time derivative. So, for instance, if you look at the time derivative here, kappa being equal to one, then on highest order, you don't have any loss. But when sort of you look at the zero order, you get this horrible weight which explodes quite heavily. Explodes quite heavily if near the boundary if the alpha is small. And nevertheless, this is not too bad because you see, okay, in particular, if you like have, let's say, beta equals to one, then this weight vanishes. So you have the Lipschitz bound again. And in particular, here, you get the distance weight with some scale. Weight with some scaling, which you can remove again using Hardy's inequality. So essentially, this theorem reproduces back the Lipschitz estimates as they are known, and you sort of have a loss if you're in the Hölder domain. And also for the time derivative, it's similar. So now I wanted to show you some very nice implications of this of our operator. It also works for negative norms, but I think this is somehow lost. I think this is somehow lost. Let me see what I have here. Ah, yeah. So, here is a nice theorem. So, if we have like a function which has a time derivative in a negative spatial Sobolev space, and it is somehow integrable for some large number, maybe large number, so this depends here on this continuity, then you still can produce a time derivative which is in the Lebesgue space. So, essentially, rising the negative differentiability. Uh, the negative differentiability to a Lebesgue category is possible, however, you have a loss with respect to the singularities, the singularities of the boundary. And yeah, so this is this is here somehow, I mean, you can compare. Now, this theta was the thickness. So, if the theta is one, you see that if the beta is getting very close to Lipschitz, then essentially you get the same Q here. And if the alpha is now going. And if the alpha is now going to one, then you don't have a loss also on the s. So, in the limit, you can choose the p equal to the s equal to the q, which is what you know from the cylindrical domains. And you get it back for, let's say, domains which are Lipschitz in time, Lipschitz in space. And somehow you have the full scale. And what does this mean now? This means for the pressure, for instance. For the pressure, for instance, so you recall that somehow the maybe the most critical part was this guy with respect to the time derivative. Yes, so essentially we can now construct these pressures here, P1 and P2, in such a way that one part is like in the dual of a of a. So if we have a velocity in this very weak space, LQ Lp, then we can construct the respective. Can construct the respective pressure related to the time derivative with respect to these regularities. So we have one which is in a Lebesgue space and one which is in a negative sovereign space in time, but has some differentiability in space. And again, somehow in the cylindrical domain, you know that this is true for put when you put here everywhere the same exponent p. So here p and here also p. And yeah, so. And yeah, so the P1 and P2 are related to this Q here and this Q there. So this is sort of what is missing on this slide. Okay, let me explain. How much more time do I have? Two minutes, right? I think you're very close because it's 30 minutes and then it's two minutes missing. Missing you. Ah, so two minutes, that's perfect. So let me just show you how we solve this. Okay, let me just mention that there is space to play with these weights. So I think this is a very nice result, and especially the operator is very nice. But using this operator, I think you can get much, much better estimates if you have a more concrete situation. So I think maybe it's worth to tell you a bit how the operator looks like. And somehow here's the picture. So, this is the space-time. So, here is space, here is time. Here we have the omega, and here's the complement of omega. So, essentially, what we do is, okay, this is like now partition of unity. You split it up in graphs, whatever. And now, this is the Bogowski operator. So, the trick is that you integrate just down with respect to these straight lines which you have from the graph. You have from the graph. And then you multiply it with the direction. So here it is maybe the en direction. So you just integrate. And if you take the divergence of this operator, then you get the function f back. So this is really like the story. And of course, this is an operator which is nice in which is regular in this n direction, but it's not very regular in the other directions. So you modify this, and this is sort of the horrible option. This and this is sort of the horrible object here. You modify this, and this is where we use the idea of Diening Rushischke-Schumacher by a Wittney covering in space-time. And with these two ingredients, if you combine them in a good way, you can produce this operator. And then you need to be very careful doing the right estimates, but this is essentially the idea. Okay, so yeah, thank you very much for your attention. And And I stop here. Thank you. So, thanks a lot, Sebastian. It was a very comprehensive and informative talk. Are there questions, remarks, complaints? Appreciations. No complaint. Oh, no, Dominique. Dominique has a complaint. No, yes, I. Yes, I actually do have a complaint. No, let me see how he answers my question. So, if I look at, I'm now interested in the case that gamma is between zero and one, right? In your estimate, this means I have a loss in integrability, right? For the time derivative. You know what I want to do with this estimate, right? So, you want you're interested in the higher integrability of the density, I guess. Yes, of course. Yeah, we need to check it. No, because I see from this estimate that. I see from this estimate there is a loss in integrability in this case, right? Well, near the boundary, of course, yes. Yeah, but this is exactly the point. Yeah, yeah, of course. Yeah. Okay, then one has to check if it improves the. Yeah, okay. Okay, somehow, okay, this is a we have a nice project on compressible flows in fluid structure interaction. And one of the motivation was also maybe to improve these density estimates there. It's not straightforward. This is what I can say. Not straightforward. This is what I can say. Somehow, our estimates before were quite good, I would say. I don't think they can be improved. Okay, now I can say I believe you this result, but I'm not sure if it helps to improve our estimate. This one has to check. I agree. I agree. Okay, thank you. We did a good job. Maybe this is related to this question. Can you prove any optimality of this estimate that you have? Of this estimate that you have for special domains, or yeah, this is a very, very good question. And okay, and okay, so to produce examples. So somehow, okay, the problem is that the Bukovsky operator is very, very flexible. And okay, there are optimality results by, I think, Koskela and co-authors, which somehow can close the gap. If a Poincare inequality exists, then a Pogovsky option. Then a Bogovsky operator exists. This is equivalent to if you have like a good Bogovsky, the shot the Bogowski operator without loss, and then you have a weighted Poincare inequality, and this is equivalent of it being a John domain, I think. So these kinds of results are very nice and available. And it could be that something similar is also true for Hölder domains. However, the difference between Lipschitz and Hörder is very, very subtle. So there is a lot Is very, very subtle. So there is a lot happening in between. One size is already this theta size. So I think there are so many spaces in between Hölder and Lipschitz that I would not know. I mean, for some cases, you could probably get something, some kind of under some restrictions, but the restrictions would have to be more subtle and More subtle, and the results would maybe not look too nice, I think. What about the case of the standard Budowski operator, not the time-dependent one as you have here? So, are there sharp results concerning this kind of inequalities near boundaries like early continuity or even more general models of continuity? Or well, I mean, okay, there are counterexamples for the for the for the for the for the for the for the okay for the gradient estimate yes like just gradient of the pkovsky of f being estimated by f yeah so l2 maps to w12 but if you have some weight as you have here depending on the distance from the boundary in the spirit of your results but just for the x dependence and i think what you know is that what you know is that if that what you what i mean okay i don't think it's written but i would conjecture that if you have a weighted pump That if you have a weighted Poincaré, then this implies that you have a Bogovsky operator. So if the weighted Poincaré fails, maybe you can disprove this in certain cases, then you cannot expect to have a weighted Bogowski. Okay, but I think that our operator is a very good one. And I think that sort of for the generality which we have here, so this really also works for a domain where you have, I don't know, some kind of Weierstrass boundary. Boundary, yeah. I'm pretty sure that you cannot beat this because essentially you more or less have a very good choice of integration that you base your operator on. Yeah, so, but as I said, I mean, the difference between lipsticks and non-lips is very, very tough one. And I think there's a lot of space to improve, to use this operator, maybe to improve it at a few points and to get for specific situations. And to get for specific situations better estimates. I think there's lots of space, and but one should then probably tailor to the problem. Thanks, Sarah. Yeah, let me also add that this connection with the weighted Poincaré. I mean, if a weighted Poincaré works, then this works. I mean, these are very theoretical because then the validity of Poincaré is very, very close to what you're proving, because Poincaré inequality tells Because Poincaré inequality tells you, I mean, it's a discretized version of the fundamental theorem of calculus. That's the thing. And since what you do is to take the fundamental theorem of calculus and put it into action into small cubes, then this is completely equivalent. Then, of course, you can make a lot of philosophy if a point works, but what you're doing is you're translating what you need in itself. Very close, very, very. Itself very close, very, very close. It's like when you prove regularity of a function using x's and decay estimates on small cubes using Campanatus characterization. It's exactly the same thing. At the end, what you're doing, you're transferring the same information directly at the integral level, but you're always there. So it's, I mean, that's the point. Yeah, I mean, it is an integration. That's what it is. Yeah. It is an integration. Yeah, it is an integration. So you get the fundamental theorem of Cauchy's, you make an integration, and then you come up with an inequality. That's exactly what you need. So many people say if this holds, then this. It's a lot of philosophy behind terminology, but the essence doesn't move so much. Yeah, somehow our objective here was to do this integration in such a way that it works fine with the time derivative, that you can have also no troubles differentiating. Like no travel is differentiated. I mean, this equivalence with the point might have some sense in some contexts, in some, let's say, abstract-down context, like metric spaces, where you do not have a pointwise gradient. And then you look for a replacement of the property of connecting two points via something that can be summed up, essentially, right, which is an integral. But outside that, I mean, you're speaking to the same problem and translating into a different language, it doesn't move so much. Don't move so much. I mean, that's my feeling over there. Yeah, no, I agree. But somehow, what we produced somehow is doing a nice job. So it's concentrating to the boundary. You have an. No, no, that's exactly what you need because these are the... I think that this is a version of the well-known Hilder norms that you also find in classical books like Gilberg and Trudinger's and blah blah. So you have everything is okay, and then you put a weight to correct the boundary. To correct the boundary, this is the old, the old style, the old good style. I mean, I mean, the boundary is failing, and then you correct by killing, adding an extra term that kills everything over there. That's a distance to the boundary. It rectifies the thing. Yeah, but I think, I mean, I agree. There are many estimates, but maybe they're not even enough. So even in, I think, even in regularity theory, up to the boundary. No, this is very non-trivial. Very non-trivial because this is time-dependent. This is time-dependent. So, I mean, this is uh, this is not a joke. I mean, yeah, but also, sorry, I didn't want to interrupt. No, no, no, you're not interrupting anything, but maybe it's time that Tomo is approaching. Yeah, very good.