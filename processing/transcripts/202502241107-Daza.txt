It was experimental. So, whatever, I'm going to talk fast. This is some people here have seen part of this talk. I think, like, Irina, you've seen this like twice, and both times you've been like, I don't know, man, I'm not sure if I believe you. So, I'm going to try to convince you. And, Julia, you've skipped this talk at least twice. So, I'm glad you're here for this. I'm glad you're here for this one. It's going to be great. So, this is a project that I did with Alvaro, who's a postdoc of mine gameway in the Columbia Biostat Department, does great stuff with quantile regression. Keith Diaz is a long-term collaborator working in cardiovascular and behavioral health. And I'm just going to show generally sort of generic accelerometer data. I'm not going to talk too much about what it is, but here's accelerometer data that should look fairly familiar. Data that should look fairly familiar for people who work with this kind of thing for three participants. There's less activity at night, more activity in the day, and sort of a bunch of spread going around. And if I told you outright, like, look, here's a mean curve for each of these participants, you'd probably be like, okay, yeah, great, we can see mean curves for each of these participants, right? And nobody's like surprised by that. And I'm emphasizing that because, like, you might think there are a bunch of ways that you can generate mean curves for each of these participants, and that's absolutely true. And that's absolutely true. So keep that in mind when we get to the next part of what we actually did this time. In addition to mean curves, there are a lot of other things, though, that you might want to be interested in. My co-Keith is very interested in sedentary or low-intensity physical activity behaviors, basically what happens in sort of the very low part of the physical activity spectrum. Some people care an awful lot about like the opposite side, the high-intensity version of things. And neither of those is really well captured by an expected value curve. You buy an expected value curve, you're looking at sort of different properties at the subject level. Long story short, though, there's a lot of interest in sort of understanding the joint effects of like the timing of physical activity and also the intensity, which are sort of two different things that we want to be interested in. And so the way that we've been thinking about this is at any particular moment, there's sort of an underlying distribution of possible physical activity values for any particular person. So right now, I could be more or less accurate. Now, I could be more or less active, but I have a person-specific time and intensity distribution. It might be different for me than it is for Fadima or for Julia or for somebody else. But basically, that's what we're interested in understanding. It's like, right now, I have a complete distribution of physical activity, and that's what we want to try to understand. And fine, we get one observation, one data point from that full distribution, but we're going to try to reconstruct that entire distribution in some way from that. And the reason I'm sort of emphasizing that piece is even though. Of emphasizing that piece is even though, like, I tell you, I've only got one observation in this minute, you can get expected values, and that's never very surprising. So, now I'm just telling you, like, yeah, you can get the mean from that sort of one data point, but you could also get other things that we might be interested in as well. And what I mean by that is this sort of a thing. So, the punchline at the end of this is that what we've been interested in doing is trying to model person-specific distributions or person-specific quantile functions. So, you see in the left panel. So you see in the left panel participant A and the red line has that person's 10th percentile curve, and the blue line is that person's median, and then the green line is that person's sort of 90th percentile curve, 90th quantile curve. We get different versions of these for each of the different participants. And our underlying idea is that if you're interested in understanding the effect of low-intensity fiscal activity, well, you're probably a lot more interested in the tenth percentile curve than you are in the overall average or the expected value. And if you care about... Average or the expected value. And if you care about high-intensity physical activity, you probably care more about the green line for each of these participants as well. And again, the basic idea, the thing that we're going to leverage is sort of the same technology that you might use to estimate a person-specific average. We're just going to do that to estimate person-specific quantiles instead. One of the things that's sort of interesting about this, you look at participants A and B, they're actually similar at the 10th percentile and quite different at the 90th percentile. So it's not like it's completely redundant information. It's completely redundant information. You actually do get different things for different people, and sort of gives you an opportunity to understand the individual effects of those things. We'll see more about that later on. So in the first figure that I showed where we had expected values, we were using FPCA. And hopefully this is sort of like a familiar thing. It's dimension reduction. You get a population level mean curve. And then you get population level principal components and subject specific scores. And you just need to estimate all of those things. And you just need to estimate all of those things. But if you can estimate all of those things, population level, principal components, and subject-specific scores, then you can use those to get your reconstructed expected values. And we did exactly the same thing, but we put taus on everything to be like we're looking at the quantile instead of the expected value. So we took away expected value and we said 10th percentile, 90th percentile, but otherwise don't touch a thing. Just do the same thing that you had done before, right? And it's like fine. Before, right, and it's like fine. So then we said, we're gonna estimate all the parameters in that model basically directly. So, like, for folks who spend a lot of time thinking about FPCA, sometimes you do a covariance decomposition that gets weird when you're doing quantiles. So, we ignored that. And we just said, we want to estimate scores and basis functions. And so, we'll just estimate scores and basis functions using a quantile regression check loss function. We have an estimation strategy that just like alternates. Strategy that just like alternates between given scores, estimate basis functions, and given basis functions, estimate scores. And you just sort of iterate between those two things until you get to the sort of converged solution and everything works out. And again, this is not like it's new in the context of quantiles, but we've been doing this for a very long time in functional principal components analysis. So we really just sort of swapped out what is our loss function and focused on the check loss instead. I say here, both of these steps are easy. Easy in the sense that if you Easy. Easy in the sense that if you want, you can throw them into the quantum write package and it will give you answers. It gets a little bit more complicated if you want things to be like faster or efficient or other things like this. But in some sense, this is a relatively easy problem where you've just got like estimating scores given basis functions, that's just font right. And estimating basis expansions given scores, also font right. You can just throw everything into standard software if you want to. You have to choose smoothness. You have to choose smoothness and tuning parameters, and I'm not going to talk about that. The computation, sometimes it takes a while, but there are faster ways to do it, and I'm not going to talk so much about that either. Adding to the list of things, and I'm not going to talk too much about, we need a bunch of simulations. Works great. We feel really happy about this. We tried some reasonable competitors, and this worked better than those. And we thought, fantastic. All right. All right, so I am going to talk at least a little bit more about the application. Again, these were in. Again, these were in Haynes Accelerometer 2005-2006 wave. We looked at participants who had at least three valid days. We did, for this, average across those three days. So that like 10th percentile curve is in some sense like an average 10th percentile curve for a participant across multiple days. And I'm not sure I love that, but like I'll talk more about that later, maybe. And so we took this, we had a 3-ish thousand participants who met these criteria, we applied our algorithm to it, and we saw what came out. It and we saw what came out. And this is a version of the results that we saw previously, right? So you can, for each participant, come up with 10th percentiles and 90th percentiles at the individual level that just eyeballing it looks to be about right, right? Like if it seems like this sort of works. We did a variety of cross-validation to see whether or not it seems like it works. It does. It's pretty good. Some of the other things that are interesting, though, you can look at the principal components at each of these different directions. These different directions. And so the top row, I'm looking at the first principal component for the 10th, 50th percentile, and 90th percentile. What's interesting is like the shape stays basically the same, right? Like the first principal component at the 10th percentile is like higher or lower. The 90th percentile, it's higher or lower. Sort of going from bottom to top, these are individual participants or sort of like points on the score distribution. We were actually struck by with this. So this person's. This. So, this person's 10th percentile curve is about the same as the lower version of this person's 90th percentile curve. So, there are some people whose like 10th percentile is at or above other people's 90th percentile. And like, that's the sort of thing I think you wouldn't really be able to capture if you were looking only at expected values. There are some other sort of nice things about this. 10th percentile curves, like it's basically flat all the way through the nighttime, but you start to see stuff that actually happens at the 90th percentile. Stuff that actually happens at the 90th percentile, like it gets a little bit away from zero, indicating some amounts of like sleep fragmentation and other things. The second principal components are like shifts from morning to afternoon, and that's also sort of true all the way across the line. But you can do some of these other kinds of exploratory plots. Do we ask questions now? Yeah, yeah, I don't know. Ask Fadim if you ask questions. Do we ask questions now? Yeah, I think that that would make it more. Six, and hence they weren't wearing the sensor during the night. So if you have a bam at night, it probably means there is a shift in the urinal outer question at all. No, no, no, no. That's allowed. Yeah. I like, honestly, I will have to go back and look at that because I yeah, I'll go back and double check what what happened there. Good point. I have a question. I have a question. Can you map these lower or higher percentages to any specific clinical concepts or in general concepts that are explainable? When you say clinical concepts or concepts that are explainable, you mean like do we look at outcomes? Yeah, low physical function, children, like that. Yeah, so let me go here next, and then I'll talk a little bit more. So when I say Little bit, a little bit more. So, when I say like next, like this is principal components as a function of age. So, this is like median, you see, sort of a general decline and also lower variability. This is the second principal component over time. And what's interesting about it is it like sort of starts wider distribution and also a little bit negative, and ends very narrow distribution and also a little bit positive. And that second principal component was this one, which is sort of saying like younger person. Sort of saying, like, younger participants tend to be a little bit more active, I want to say later in the day here in the evening. As you get older, it becomes more active earlier in the morning and less. This is actually sort of consistent with other things that we've seen here. But we can look at this at the 10th percentile, at the median, and at the 90th percentile as well. And the first principal component, like there's a general decline in the amount of physical activity over time. Yeah. Some of the other, and I'll come back to this in just a second. I'll come back to this in just a second as well. Some of the other things that we look at are like: take a look at the median and principal component one versus principal component two. Just is there any sort of association between those? But you could look at first principal component for the 10th percentile and the 90th percentile. Anybody, and in sort of you pick a vertical row here, anybody in that vertical row has basically the same 10th percentile curve. And any of the variation from top to bottom means that some people have higher 90th percentiles and some people have a lower 90th. 90th percentiles, and some people have lower 90th percentiles. So, you do actually see, especially sort of like in this region, quite a bit of variability at like low levels of physical activity, sedentary behaviors are pretty similar, but the upper limit becomes quite a bit different. So, you can start to sort of interrogate both levels of timing and intensity in this way. I'm going to talk about this future direction first, just because we did sort of take a Just because we did sort of take a quick, let's do a scalar on function regression in this data set looking at all-cause mortality predicted by the 10th percentile curve. And we compared it to doing something like FPCA and using those results. And we found like there is a coefficient function. It shows you some amount of information that might be going on, an impact of like increased physical activity, reduces risk of all color mortality in the middle part of the day. And when we threw in some other predictors like the Threw in some other predictors like the overall activity, this sort of persisted, stayed significant. So it seemed like it was adding some additional information there. The other future direction that I want to mention is you can fit this at every percentile you want, 10th through 90th, but you get results that are sort of crossing, you know, the 40th and the 50th and the 60th will sometimes cross. And so we've started developing works that are going to be like more of a surface. So this is, you know, it's like non-negative. This is, you know, it's like non-negative everywhere and monotonically increasing in tau, and also sort of parameterized in a nice way. We're sort of fleshing that out a little bit now. Last things that I'm going to say, we spent a lot of time by we, I mean primarily Fabian Schaipo and one of his students, but also Julia and I occasionally as well, developing a framework for doing tidy functional data analysis. I think it's great if you code in tidyverse and you're always like, oh man, my functions, this is really hard, try tidy fun instead. Try tidy fun instead. It's actually pretty nice. So, this code on the left produces this figure on the right, and there's like group by and summarize and ggplot and geons and other things, and just sort of makes everything we need. And Chibrin would be really mad if I didn't tell everybody that we wrote a book. We wrote a book, it's really great. That's all I have. Happy to take any questions. Thanks. Everybody gets to ask all the questions, right? We should check online with these guys. Guys, those who are online, do you have any questions? And if you don't, please unmute yourself and ask. All right, it looks like there are no questions online. Then it's time to open the floor. So let's start here. Thank you, John. This is great. I have a question number. I have a question about you said that in your study you got data from this follow for three days. I just wanted to get a sense, you know, whether instead you had 100 days or like thousands of days or how would this work? Like is the computational, would you think it would be important computational issues or it just would be better? Oh well so the the I guess maybe I'll be clear. What we wanted to have was at least three days of data for each participant. Days of data for each participant so that we could aggregate and get like validated. Yeah. So if we, oh, it's fine. We're on Vadim time now. Just feel like that. So we've been working on like a multi-level extension of this where rather than taking days and aggregating, we actually just try to build in both within subject effects and across time effects. And in that one, we're using an example where we have, I want to say, like 30 to 60, I think maybe 60 participants, but with follow-up. Maybe 60 participants, but with follow-up to 60 to 100 days per participant. And, like, yeah, computation is an issue, but it still works. Or at least the extremely preliminary version of it seems to sort of work. Right, next question. I think Julie has one. Yeah, I'm wondering if there's a framework for testing at the population level. So, let's say I'm interested. One of the things we've been looking at a lot is sex differences and physical activity. So, like, is To be. So, like, is there an extension from this, or maybe it's already there where, like, I can test whether there's a difference in the 90th percentile? Yeah, so actually, that was one of the things that we were sort of one of our original motivations, right? Was we were doing like a function on scalar regression where you wanted to look for differences between men and women at different quantiles. But, like, the biggest challenge in function on scalar regression is the residual correlation, right? Residual correlation, right? And there just wasn't a good way to account for residual correlation doing function on scalar regression. And so we sort of said FPCA works at the expected values. Maybe we need to develop a similar thing so that we can try this in that same sort of framework. So yeah, again, like fingers crossed, sort of working on it, but haven't solved that problem yet. Great, next question. So you have multiple principal components. Because we're component systems, I wonder if you did any standardization those eigenfunctions. How do you interpret and compare 0.5 weight for the first PC, first set of PC, and 0.3 for the second PC? So I think the ways that we have tried to do the visualization is effectively borrowing from usual FPCA. So we looked at clocks like this. This is to try to see what is the sort of the effect at any particular level. We've made screen plots and other things like that, but I think one of the challenges that we're still working on are things like this one, which is what do you do if you want to understand what's happening at the 10th percentile versus the 90th percentile? I mean, is there sort of good way to aggregate information across those levels? And like so far, this is kind of what we've come up with. And as a second follow-up question, so I have comparisons. So I hope you could parasitize around the Malta Mario PC, where they do have Malta Mario to check sources, which is different from these one. I wonder whether there's any connection because you do have to make a module. Yeah, no, that's a really good idea. I hadn't thought of that, but good idea. Thank you. Next question. Hi, real quick. So here, I believe you're analyzing the raw accelerometer data. What if I, or maybe I'm evaluating? I think, like, I'm looking at John because I think he'd be like, well, it depends on what. John, because I think he'd be like, well, it depends on what you mean by medium rare exception. Yes, what if I work on a well-done snip counts data? Would all these methods apply directly? Is there any tweets or special dissipations? Like minute-level step counts. Yeah, no, I think like minute-level step counts, minute-level mems, minute-level Euclidean norm minus one. Level Euclidean norm minus one. If you're getting like minute level data, I think you could just sort of apply this more or less out of the box. So here, if you condition to an age, so let's say somebody, you know, very young, like, you know, five. Do you have additional value prediction from like different percentiles? Like is it more activity than I saw some personal contrast between like morning and afternoon? Yeah, you know, I mean like I think unclear, something we're working on. I feel like that's a question that is generally applicable for a lot of the things that we're talking about, which is like, suppose you already have AIDS, what good does any amount of physical activity? Does any amount of physical activity information do you? And I think, like, it seems like not me, but others in this room have published things that are like: if you get objective physical activity measurements, it's one of the best predictors of all cause mortality and other things, and actually beats out age in a variety of cases and contributes independently after you adjust for age and other things. So I think that's like a fair question in general. In this case, we haven't looked at it, we haven't tried to make sure, but I think so. Like, I think in the same way that the motor. Like, I think in the same way that the motivation being like understanding activity in general probably does have some additional benefit after you condition on age. I think that would be true here, but we haven't tried it yet. Thank you. What was your definition for validating? We used, I think, whatever the definition that was coming out of the NHAN survey was. So we I don't I would have to go back and double tell you I don't think that was one that we came up with. We just sort of used what was there. At least an hour was if we got it. At least an hour and a twenty morning. At least an hour and a twenty four. At least half yeah. Okay. Next question. So for defining this q by tau t at H T, so how many kind of like minimum number of measurements do you need to estimate it reliably at the higher or very lower point? Yeah, no, that's a good question. Actually, one of the things that we found was if you had like 100 or 200 time points on your grid, this was mostly in simulation. On your grid. This was mostly in simulation. If you had like 100 time points and you're trying to estimate a 90th percentile, that gets extremely challenging. So, in doing something like this, we thought, you know, we didn't do like an extremely rigorous study of this, but like ballpark, I would guess it helps to have about a thousand time points if you try and guess maybe the 90th percentile and protect percentile within it pretty fast. Or any other national release continuity. No. We should. No, I mean, like, that makes a ton of sense. Let me explain, because I would hypothesize that that's going to capture functional spectrum students, right? Yes. You notice yourself that sometimes these lower percentiles are the same, but highest are the best. It's interesting what that means, right? Again, I would expect with age that's gonna shine, right? That's the machine, right? What else? Yeah, yeah, yeah, no. I like the reason I was sort of able to say no pretty quickly is like that makes a ton of sense. I think we sort of got to this point because we have a lot of reason to believe based on like preliminary work that we've done and that others have done that trying to understand both high and low levels, intensity levels are important, but we haven't figured out exactly what the right way of summarizing that difference is or trying to leverage that information and understanding other health outcomes. Other health outcomes. We're sort of trying to do this first, or at least have got this first thing done. But yeah, looking at the difference between the 90th and the 10th, I think that would be a great way to try to understand what is the sort of health impact of this distribution. I have a question regarding the interpretation of the result of Promptile. My understanding of person-specific Promtel is like across different days, this person's physical activity at a certain time could be varied, right? At a certain time, it could be varied, right? Investors are thinking: if you have multiple days for a person and field is multiple days data, like each person needs to compute one data point, one trajectory maybe contributes to multiple trajectories. That would be better to estimate this function for each person. So I think the way that I would interpret this is more of like a cross-sectional sense, meaning like at least in this case, like we think at this particular time point, there's a full distribution. Particular time point, there's a full distribution, and this is the 10th percentile, this is the median, and this is the 90th, and this is the observation that we actually saw. In the multi-level extension, like our hope is to do something more like what you're saying, which is to try to understand when you look across multiple days, can you establish thresholds that tell you, like, here's the 10th percentile for this person, aggregating across two weeks, and then follow them up over time to see if that like 10th percentile changes, shifts up or down. But this I would sort of think of as more like cross-sectional. sort of think of as more like cross-sectional. We're just sort of saying like this is vertically looking, this is like a distribution at a person for one person at a particular time. All right. So it looks like I hope I will have so much energy at the end of the conference Thursday and Friday. So let's take a second. And our next speaker for today Eric Taza, let's see. Eric, can you hear us? Can you start sharing your screen? Yeah, sure. Give me one second. Okay, y'all should be able to see that. Yep. And let me see. One sec. I'm just pushing random.