Thanks very much for having me today. It's a shame we can't be there in beautiful Banff together in person, but c'est la vie. Okay, so indeed I'm going to be talking to you today about how the functional specialization of visual cortex can actually emerge from training a neural network with parallel pathways with self-supervised predictive learning. Predictive learning. So let me start by clarifying what I mean with respect to functional specialization in visual cortex. In particular, what I'm interested in here with this work is the distinction between the what versus where pathways of the brain, or alternatively, the dorsal versus ventral pathways. So most of you are probably familiar with this, but just a brief review. Brief review. When we look at the visual cortex of mammals, we see a fairly clear functional division between the dorsal and ventral pathways, in particular in the primate brain, where, of course, the anatomical terms come from here. And what we see with regards to this specialization is that in the dorsal pathway, neurons are more concerned with, you know, Neurons are more concerned with, you know, where are things and where are they moving to, and not so concerned with static object features or identities. And in contrast, in the ventral pathway, neurons are generally more concerned with what things are, i.e. those static object features and their identities and what they mean, and they're not as concerned with their location or their direction of movement. And lesion studies also back up this functional specialization, and we can. Back up this functional specialization, and we can say roughly that we have these two distinct pathways: one for processing moving things and one for processing static object features. So why does the brain have these specialized pathways? That's the question that we're interested in trying to get at in this work. And one of the things that we thought about when we first approached this question is this question of visual This question of visual prediction. Now, we got on to prediction because more broadly, we in the lab are interested in predictive learning and the idea that the brain, and in particular, the neocortex might be engaged in some form of predictive learning. And one of the things that struck us when we were considering this problem of prediction is that visual prediction requires two different and competing potentially forms of invariance. So, let's imagine that you're watching. So let's imagine that you're watching a car move to the left, and you're trying to predict what you're going to see in the near future. Well, there are two different things that you can base your prediction off of here. One is simply the fact that you'll probably still see an orange car. Unless the car is moving very, very fast in the next few hundred milliseconds. If you see an orange car right now, you're still going to see an orange car in the future. Right now, you're still going to see an orange car in the future. And importantly, it doesn't matter which direction the car is moving in. You can make that prediction regardless of its direction of movement, or even regardless of whether or not it's moving. So you have a movement invariant prediction that you will see an orange car in the near future. At the same time, if the car is moving to the left, then you can make another prediction, which is that you're going to see some leftward movement in the next few frames still because movement. Still, because movement tends to be consistent, or alternatively, that you're going to see an object appear towards the left of the visual field that wasn't there previously. And here, as in the previous case, this is invariant to the other property. That is to say, in this case, it doesn't matter what the object is, whether it's a car or a horse or a person. If there's something moving to the left, you're probably still going to see something moving to the left, and you're going to see. Moving to the left, and you're going to see something more to the left in the near future. So, you have an object invariant prediction as well. And you then have these two different forms of invariants that are ultimately slightly intention with one another, precisely because they're invariant to each other. So you have movement invariant predictions and object-invariant predictions that can potentially compete for how you generate your prediction. And this brings us to our hypothesis, which is that the what versus where specifying the prediction is. Which is that the what versus where specializations in the mammalian visual cortex ultimately emerges from a process of optimization for visual predictions. And the way that we're going to test this hypothesis is by training ANNs with a self-supervised predictive loss and then see whether or not they develop representations similar to those in the what and where pathways in the brain. Now, of course, that can't tell us that this is definitely why the brain. That this is definitely why the brain has these different pathways, but it provides a nice normative model that gives us a proof of concept. If, in fact, ANs trained to do this sort of visual prediction task develop these specializations, then it suggests that, yes, in principle, these specializations could have emerged from some process of optimization for visual prediction, whether that be optimization over the course of evolution or optimization in an animal's lifetime. In an animal's lifetime. Now, as a preview to what we find, we find that indeed self-supervised learning will induce separate what and where pathways in a neural network, but only if you also provide some anatomical separation for the pathways, which is interesting also with respect to the brain. If you're interested in this work, I'll just mention that it was published at NERUPS this year as a spotlight paper. You can find it online. Find it online. That's Bactieri et al. Okay, so how are we also going to like actually assess the functional specialization? Because what we'd like to do is really compare it to brains as well. To really make this concretely relevant to brains, we want not only to show that such functional specialization can emerge, but to make the claim that it maps onto something that we see in mammalian cortex as well. And so to do that comparison, we're going to use, we needed a data set. Going to use, we needed a data set that included recordings from a bunch of different visual areas, including areas that are in the dorsal or ventral pathways. Now, that can be hard to find in primate studies that are available online, but the Allen Institute provides these wonderful data sets for mice that include recordings from many different visual areas. These recordings are to photon imaging sessions, so calcium recordings, I should say. Calcium recordings, I should say. So they, you know, do a surgery to head-fix the animal, some intrinsic imaging to detect which region they're recording from. And then they do two-photon calcium imaging in these mice as they're freely moving on a wheel while they present them with videos. And what they found in this paper that they published back in 2019 was that the thing that elicited the most robust responses in most visual cortex. Responses in most visual cortex were natural movies, by which we just mean movies that are film of actual occurrences in the real world, like this example clip shown here. I think they actually used clips from movies in their data set. And so the mice were presented with a bunch of different images and movies and stuff, but the data that we're going to use here is all from those instances. All from those instances where the mice were presented with these natural movies. And they recorded from six different areas in this data set: V1, and then five different higher order regions in the mouse visual cortex, LMAL, AL, RL, AM, and PM. For the record, we're not going to include RL because it's a more multimodal area and didn't show very robust responses even to the videos. And so we're just going to have these five ones with four higher order regions. ones with four higher order regions. Okay, and in order to compare our neural networks to the responses in the Allen data set, we're going to use representational similarity analysis. So the basic idea is very simple. You've got your neurons. They're responding to a movie over time. So for each frame of the movie, we get some neural response in the high-dimensional space of neural activity. That then gives us a matrix where That then gives us a matrix where we have a number of neurons and the number of frames of the movie. So each of these rows represents a response of the neural population to a frame of the movie. We can take the square of that matrix, giving us basically an estimate of the dot product between the responses of all of these different frames in this neural circuit. You can also do some other things like normalize this, whatever you want. Whatever you want, there are ways, but this is the basic analysis tool, and that gives us something we call a representational similarity matrix. So, how similar are the representations for each pair of frames of the movie? And so, this matrix is frames by frames. We're looking at how similar the responses are to pairs of frames. And critically, because this is in pairs of frames, you can then do this analysis both for the real neural network and Real neural network and for the artificial neural network, and you can then compare these representational similarity matrices between them, and you can then assess the extent to which the representational geometry is the same in both of the networks. Okay, the other thing I just need to do here to kind of prep the analysis that I'm going to show is what we also do. What we also did is we looked at both the existing literature and the responses in the brains to develop what we called a ventral versus dorsal score. Apologies for abusing the anatomy here a little bit because mice, in mice, the, though mice, I should say, so mice do possess a functional specialization with some higher order regions that are more concerned with movement and some higher order regions that are more concerned with static object features. More concerned with static object features. It doesn't map perfectly onto the dorsal versus ventral anatomy of the primate visual cortex, but nonetheless, that's the lingo I'm going to use here. So my apologies for that abuse of anatomical terms. But ultimately, what we can do is we can see based upon the anatomy and the responses of the neurons in the mouse's brain that there are some regions, such as vis-al-M, that are clearly more ventral. That are clearly more ventral, that is, they're more a what pathway or a static feature pathway for the mice. And then a region like Vis-AM, which is very clearly more dorsal and is more concerned with movement and is really more of a wear pathway. So we get this basically for our five regions, an index for how ventral or dorsal it is. And then as a first And then, as a first pass and sanity check, what we did is we did that representational similarity matrix analysis between animals, so not between neural networks, artificial neural networks and animals, but instead between animals, and between animals with recordings in different brain regions. And a nice thing to see was that if we look at the representational similarity matrix between animals, we can see that when you compare recordings. When you compare recordings from the same brain region, for example, primary visual cortex, which region does its representational geometry best match? Well, the primary visual cortex of other mice. Likewise, if you compare visel-m of an animal, which regions in other mice does it best match the representational geometry of? Visel-M in other mice. So it's a nice sanity check that this representational geometry is an actual helpful indication of. Helpful indication of the nature of the representations in the circuit. So now, what is the neural network we're going to train up? So in this case, we trained up our neural networks using an approach called contrastive predictive coding. So this is a self-supervised learning approach, and the basic way it works in our hands is illustrated here. It was taken from this paper by Han et al. We feed segments. We feed segments of videos into a convolutional neural network, which then feeds into a recurrent neural network, and this is rolled out over time. So, you feed video segments over time into your ConvNet, you get these tensors out, you feed those into your recurrent neural network, and then your recurrent neural network is used to generate what we call a context tensor. That context tensor is then run through another neural network that generates our prediction. Now, in this case, Prediction. Now, in this case, the prediction is happening in latent space. So, we're not predicting the video frames. What we're predicting is the output of the convnet at each time step. This is what distinguishes contrastive predictive coding to, for example, more auto-encoder style approaches. So anyway, we generate our prediction of what the next output from the COMVNET is going to be, Z hat, that's our prediction. And then we can compare that to the actual data that we receive based. Data that we receive based on the next video frames in the future. And we train the neural network to make its predictions more similar to the actual data that it received. Now, this is the loss function that we use. And so we not only train it to be more similar, which is what that numerator is here. So we're just taking the dot product of our prediction with the output from the conv net, and because of the negative sign here, minimum. Because of the negative sign here, minimizing this loss will mean maximizing that dot product, so maximizing the similarity. We also have this denominator, which is where the contrastive term for this learning rule comes from. And what this denominator says is that if you consider the other video frames from the same movie, but not the correct one for the future, make your representation, your prediction that is, less similar to those frames. Less similar to those frames. And this is an important little inclusion in this learning algorithm because it's what prevents representational collapse. So it prevents the system from just guessing the same thing every time. Okay, so that's how we train this system up. And so it's all self-supervised. It just learns to predict future outputs from the ConvNet. Everything's trained end-to-end with gradient descent. And then we train it on this data set, UCF 101. This is a UCF 101. This is a big data set of videos that has different activities. Now, the reason we use this data set is because we also wanted to compare to a supervised training approach. So though we're doing self-supervised training here, this data set actually contains labels of the actions that are shown in each video. So baby crawling, blowing candles, body weight squats, etc. And thus, if you want to, you can also do a supervised task where you try to. A supervised task where you try to categorize the actions in the video. And we will use that as a downstream control here. So that's our training data. And so we train up our CPC network on this ECF 101 data set. And then we compare the representational similarity matrices to the different regions of the mouse brain. And that's what I'm plotting here. That is the correlation between the representational similarity. Between the representational similarity matrix of the artificial neural network versus the representational similarity matrix from the mice in the recordings in those brain regions. And we're also comparing across the layers of the neural network in this particular case. And what you can see is that when we train a single pathway network, we get representations that are a much better fit to the more ventral-like regions of the mouse's brain. So here in the most ventral-like regions, So, here in the most ventral-like region, Vis-LM, we can see that as you move through the layers of the network, it gets more and more similar to Vis-LM. And this is an important control because, so here the gray dot I should mention is just if you do representational similarity matrix analysis between the frames of the videos themselves, so the pixel-level representations of the frames and the mouse's brain. And that's sort of a good control because that, if you get this level of RSM analysis, that's basically just. RSM analysis, that's basically just that you know you have that degree of similarity because that is the inherent information in the movie frames themselves, and so it might be a relatively trivial result. So when we see this RSM go, similarity go well beyond that initial pixel level thing, it tells you that the network is developing representations that are kind of non-trivially more similar to the mouse's brain. And we see similar things in primary visual cortex. Things in primary visual cortex, which is more ventral, and vis PM, which is another slightly more ventral region. But as we move more dorsal, this RSM similarity gets worse and worse. And by the time we're here at Vis-AM, the most dorsal region, we're actually doing worse than the pixel level similarity across the layers of the neural network, which tells you that the representations that have been learned are actually much less like the dorsal region representations than you would get. Region representations than you would get if you just considered the pixel level correlations themselves in the video frames. So, this is a summary of that data. Here we're looking at the max RSM similarity across all the layers of the neural network. And these are for four different models. In this case, we're comparing our CPC network here with one pathway to a network that was trained on object categorization. Trained on object categorization with ImageNet, an untrained version of our network. I should say all three of these networks have the same backbone convolutional neural network architecture. So an untrained convolutional neural network, and then a 3D Gabor model, which is kind of a sort of classic model of visual processing in mammalian visual cortex. But what you can see is that the CPC network does a much better The CPC network does a much better job of matching the representational geometry of the ventral regions in the mouse's brain than do the object categorization network or the untrained network or the Gabora model. That is not the case though in the more dorsal regions where we can see that actually our CPC network is doing worse than an untrained neural network in matching the mouse's brain. But here's where things get interesting. Where things get interesting. If we take our neural network and we split the convolutional component into two pathways, so right after the first conv, we then have a series of convolutions that are anatomically segregated from one another. So they don't interact here until they get up to the top where they are then combined again before being fed into the recurrent neural network. When we do this anatomical separation, now we see that actually one of We see that actually one of the pathways becomes more like the ventral regions, but the other pathway becomes more like the dorsal regions. So we've color-coded it here with blue and red. So in the blue pathway, you see the same better fit to vis-LM and visP. But in the red pathway, you don't see a good fit to the ventral regions. And instead, what you get is that you get a better and better fit throughout the network to the most dorsal region, viz. AM. This is a summary of that data. So, here we're comparing the CPC network with one pathway and with two pathways to the untrained convolutional neural network. And what you can see is that, and so here we're considering across the entire network, what is our best RSM similarity. Whereas we have layers in the two pathway network that are as good as the one pathway network in the dorsal, sorry, in the ventral regions in the sort of what. In the sort of what pathway. When we move to the dorsal regions, the one pathway network, which as I noted, does worse, slightly worse than the untrained network, is overshadowed by the two pathway network, which has a very nice fit to this more dorsal region AM, much better than the untrained network. So, based upon the representational geometry, it would look like this two-pathway network has developed two different specialized pathways, a sort of ventral or what pathway. A ventral or what pathway and a dorsal or where pathway. Now, to further verify this, we wanted to actually test the functions that these pathways can support. So we looked at two different tasks, object categorization with CFAR 10 and motion discrimination with random dot kinetograms. So this is fairly simple. There are 10 different categories in CIFAR 10. The network has to categorize the images. I'm sure you're all familiar with this. You're probably also familiar with This. You're probably also familiar with these random dot kinetograms, but for those who aren't, briefly, what happens here is you present the experimental subject or your neural network with dots that are moving across the screen where some percentage of the dots are moving in a coherent direction and the rest of them are moving randomly. And the task is then to identify what the direction of movement is. This is, of course, harder the fewer dots you have moving in a coherent direction. So you can modify task difficulty. So, you can modify task difficulty with this. And the reason we pick these tasks is because these are the sorts of tasks that historically have been shown to depend on the ventral or what pathway for object categorization and the dorsal pathway or where pathway for motion discrimination. If you get a lesion to your dorsal pathway, you'll have trouble with random dog kinetograms. If you get a lesion to your ventral pathway, you'll have trouble with object categorization. So what do we find when we then use these two different specialized pathways? Then use these two different specialized pathways in our neural networks to complete these tasks. So, what we do here, I should say, is we train it up with our contrastive predictive coding. We then freeze the representations, and then we use a linear regression to do the categorization. So it's just a linear readout here to accomplish the final image categorization or motion categorization. Final image categorization or motion categorization based on the frozen representations in the neural network. And what we find is that if we use the representations that were developed in the ventral-like pathway, we get our best performance on CIFAR 10. And if we use the representations from the dorsal-like pathway, we get our best results on motion discrimination. And so that verifies that indeed the representations that have been developed in these two distinct pathways are better at supporting. Are better at supporting either object categorization in the ventral leg pathway or motion discrimination in the dorsal leg pathway. And what's interesting also is that by using two pathways, we were able to do better at the prediction task with fewer parameters. So even if we have the number of parameters roughly, I guess it's a bit less than having it, but if we significantly reduce the number of parameters from 400,000 Of parameters from 435,000 to 285,000 in our two-pathway resNet, we get the same level of accuracy as we do with our one-pathway network. And so that shows us that it does seem to help this prediction task to have these two specializations. And of course, then the natural question that emerges is: well, but are there not maybe some dorsal? Are there not maybe some dorsal-like units in the one pathway model? And indeed, there are, but what we find is that in general, in the one pathway model, the dorsal-like units are a minority of units, and so they're sort of being out-competed by the ventral-like representations. Whereas in the two-pathway model, the system is able to split the difference and so have a one-of-the-pathways where there's a majority of dorsal-like neurons. Majority of dorsal-like neurons. Here, where dorsal-like units are measured by their tendency to respond to motion. Now, another interesting control here that I briefly alluded to earlier is we can then also train in a supervised fashion on the same data set. So we can ask the neural network simply to categorize the actions into the different types of actions that exist in the data set. The data side. And what we find is that when we do this supervised training to categorize actions, we do not get that specialization into dorsal-like pathways and ventral-like pathways. Instead, what we find is that generally we just get two ventral-like pathways. And you can see that by the fact that there's no clear distinction between the blue and the red pathways here in these representational similarity plots, and the fact that we just don't have a good fit anywhere to the dorsal region. To the dorsal regions in this network. And so that tells us that it's not the data set that's doing this by itself. It's not the architecture of the neural network that's doing it by itself. It's the interaction of these movies and this architecture with the predictive loss. This is just a summary of that result. So here we've got our action categorization network with one path. Network with one pathway versus two pathways. And what we can see is that even with two pathways, we don't get a good fit to the more dorsal-like regions with this supervised training. Okay, so wrapping up now. So these results demonstrate that optimizing to predict upcoming visual inputs is sufficient to induce kind of what versus where specialization. Specialization. Now, there are two notes that I want to make on this. The first is that we can remain agnostic as to whether or not this optimization was, you know, if the brain truly has these specializations because of this, the way in which it helps prediction, we can remain agnostic as to whether or not that was evolution optimizing the brain to do prediction or learning in life. It's not really important for this specific study. I have my own. Study. I have my own ideas about that, but that's another question. The other is, and I think this is important to note, is that we cannot rule out the possibility that there are other losses, including potentially supervised ones, that would also produce the same results. So that's why I highlighted sufficient here. We can't say that predictive learning is optimizing on prediction is necessary. In fact, it probably isn't, but it appears to be a sufficient condition for developing. A sufficient condition for developing this if you also have the requisite anatomical segregation. Now, two additional questions that then follow from this work, I think. I mean, there's many questions that could follow, including some that are already in the chat, which I will respond to in a moment. But the questions that ultimately came up for us after doing this study was, first, can we find inductive biases to ensure that specific pathways take on specific That specific pathways take on specific functions. Because one of the things I don't think I said yet, but I think I should note, is that it was random which pathway became which in our neural network. So it depended on the seed that we used at the start of training. Generally, it was very robust that we saw these two pathways emerge. There were a couple of runs. We did like, I forget, like a dozen different runs with different seeds. There were maybe one or two where we didn't see. Were maybe one or two where we didn't see really clear functional specialization, but in the vast majority of runs, we see functional specialization. However, it's basically random as to which pathway becomes which. And of course, that's not the case in the brain. You will not develop a what pathway in the dorsal regions of your anatomy. In fact, I don't, yeah, unless something very seriously goes wrong. So there must be some additional inductive biases or other things. Biases or other things in the brain that ensures that something is the dorsal pathway and something is the ventral pathway. I suppose that only counts if it's learning, but that's another question. Anyway, and then the other question is, could we do a better job of matching the neural representations by adding some additional losses? We're not explaining all of the variance in the representational geometry here, and it's possible that we could do a much better job by adding some additional losses because surely the brain is not just concerned with prediction, but it's also concerned with other things. But it's also concerned with other things. There are all sorts of things that animals have to do, which prediction might help with, but is just probably one part of the puzzle. And then as a final thought, which is interesting for me as someone who works at an AI institute, I think that there would be an argument to be made that using parallel pathways with self-supervised learning might be a good strategy for control in AI systems. Because what we see in the brain, of course, is that the dorsal pathway. Brain, of course, is that the dorsal pathway hooks up to our motor systems pretty directly. And that makes sense because if you're going to try to control things and do movement, you probably want to pay a lot of attention to dynamics and movement, irrespective of the specific identity of objects. And thus, one possibility that I think is an interesting one is that AI systems need a where pathway. And arguably, they don't have that yet because everyone keeps. That yet, because everyone keeps doing prediction tasks for representation learning, but without the requisite anatomical segregation to allow for the emergence of a distinct WHERE pathway. Okay, so thanks to Shahad Bakchari, the postdoc who did all this work, our collaborators on this, Timothy Lillikrap, Patrick McNill, and Christopher Pack, and of course the Alan Brain Observatory for making this data freely available to everyone and our funders. Thanks very much for listening and Thanks very much for listening, and I'll take a couple of questions now.