Yeah, so yeah, first of all, you know, I want to thank the organizers, you know, Ling Long and Yao in particular. Unfortunately, she's not here on the Ping Shong and thank Jordan for organizing this workshop and for the invitation. It's really a pleasure to be here. It's not my first time to Mexico. My first time was to Cancung, but people told me that's not real Mexico. You don't see real Mexico like in Cancun. Mexico again can kung. But I hope this is a trip to Rio Mexico, and it's very nice to be here. So, my talk is about networks. And to begin, I want to acknowledge a few collaborators. And today's talk is based on joint work with Jiang Zhou, Wang Bing Huilio, and Jian Hua Guo. These are three faculty from the Northeast Normal University. The Northeast Normal University in China. And Jingfeijang, she's, I think, an associate professor at Emory University. And Yuan is a former student, co-advised by Lisa Lovina and me. Lisa is also a long-term friend and collaborator. Jin Ming is a current PhD student, co-advised by me and my colleague Gong Junxi. So, again, you know, today's talk is based on joint works with these people. And I do want to acknowledge, you know, People and I do want to acknowledge their help, their friendship, and their collaboration. And also, as mentioned, I will talk about network data. And here, what I want to emphasize is that network data is a little bit different from the usual multi-variant data that we typically see in statistics. Network data is about the relationship. It's hard to control this, so I'll just use my hand. It's about the relationships. Relationships. And in the network data, there are two basic elements: nodes and also links. So nodes represent individuals and links represent relationships among the individuals. And in different contexts, the nodes and the links may have different meanings. They may correspond to different things. So, for example, in a social network, each node might be a person. If two people are friends with each other, then there's a People are friends with each other, then there's a link connecting the two people. Or, like, in, say, biological network, for example, you know, in a brain network, each node might be a neuron. If two neurons connect, you know, communicate information with each other, then there might be a link connecting these two neurons. Or like in an information network, each node might be a web page. If there's a hyperlink connecting two web pages, then there's a connection. So again, you know, network data or data with network structure is ubiquitous. Data with network structure is ubiquitous. It's just like in different contexts, the nodes and links may correspond to different things. And to fix the notation here, we'll use that N to denote the number of nodes. And we'll use this so-called adjacency matrix A to represent the network. So each element in this adjacency matrix Aij is equal to one if there's a link from node I to node J. From node I to node J, and it's zero otherwise. And in this talk, we'll focus on undirected and unweighted networks. So this A will be a symmetric matrix, and also each element is either one or zero. So we don't consider weights on the edge. For example, like in a friendship network, you may consider two people are friends, but the friendship can be different depending on how close they are, but we don't consider that. They are right, but we don't consider that. We only consider whether there is a relationship or not. We don't consider how close two brands are. And here's an illustration: this is a network. It's undirected and also just one or zero. And here's a corresponding adjacency matrix. So one, the two are not connected. So A1, two, and A21 both are equal to 0. 1, the 3 are connected. So both A1, 3, and A31 are equal to 1. Think and here, what I want to emphasize is that the data we are going to deal with is a network. The network itself is a data that we're going to deal with. Again, you know, this is a symmetric binary matrix. And because this, you know, Aij is either one or zero, so very naturally we'll treat it as a Bernoulli random variable with the probability Pij. And also, we'll make another assumption. We'll assume that the different. another assumption we'll assume that the different node pairs the different node pairs the different ij pairs these aij i mean it's equal to aji so these different node pairs they are independent and so this is a uh you may argue this is a like very strong assumption uh but this is the assumption you know most people assume in the literature and we will also see later this assumption can be relaxed but for now we'll assume different node pairs are independent and the fundamental problem in network analysis is the In network analysis, is the following. So basically, we observe this adjacency matrix. So that's the network we observed. That's the data we are going to deal with. And the goal is to estimate this link probability matrix. We want to estimate all these different PIJs. So we have the data, these are the parameters, and we want to estimate them. And here I'm just rewriting this problem here again. And there are several challenges associated with this problem. One is that if you Problem. One is that if you think about it, the sample size is small. For each parameter Pij, we only observe one observation, Aij. So the sample size is very small. But at the same time, the size of the data is not small, it's large. Because if you consider this by n-adjacency matrix, there are n-square elements. So say, for example, in terms of computation, we have to deal with unsquare number of unsquared like numbers. So the computation can be a challenge. So the size of the data is actually. So the size of the data is actually now small, although the sample size from the statistical perspective is. And also, like the number of parameters increases when n increases, when we have more and more nodes, when it increases, the number of parameters we need to estimate also increases. So the number of parameters increases as increases. So that's also a challenge. Another challenge, this is what in the network literature it's called sparsity. Literature called sparsity. This is different from the sparsity in the usual high-dimensional regression, where we have a linear model, we have these p parameters, we assume many of them are equal to zero. This is different from that. This sparsity refers to sort of like the so-called network density that will go to zero when n increases. So roughly speaking, this is by thinking of suppose we have a social network. Why consider a person and how many friends does that person have? So naturally, when So naturally, when it increases, when the number of people increases, the more friends this person will have. But the number of friends this person will have does not increase as fast as linearly. So although it increases, the number of friends does not increase as fast as linearly. Or like in other words, the link probability decreases to zero when it increases. And this is the kind of far state we refer from here. So given all these characters, especially for the first Even all these chapters, especially for the first one, because for each parameter Pij, we only have one observation Aij. So, in order for this problem to be meaningful, to be durable, to be interesting, we have to assume some structure on that link probability matrix P. And here are three commonly assumed structures on this link probability P. People often assume in the literature. One is a so-called community structure. And I will elaborate. And I will elaborate on that. Another kind of structure is so-called smooth structure, assuming this link probability matrix to be smooth. So, what does it mean for a link probability matrix to be smooth? Again, I will also elaborate on that. And the third one, this is the one sort of I like the most recently. This is the latent space model. Again, I will also elaborate on that. So, for the first structure, the community structure, and the most commonly stacked. And the most commonly studied or widely used model accommodating a community structure is probably like this so-called stochastic block model. And here, we'll use this I to denote the index for a node. And then we'll use this big k to denote the number of communities. And the ci denotes the community label for node i. So c i can take k possible values. Can take k possible values, one of the k communities. And so, naturally, we can assume Ci is a categorical variable with probability pi one to pi k. So the sum of pi one to pi k is equal to one. And here's the key component of this stochastic block model. So what this says is that if we know the community labels for all the nodes, if we know C1 to CN, we know like which community each node belongs to. We have that information. Then the linked probability. information then the linked probability between two nodes between nodes i and the j only depends on which communities they belong to so this link probability between nodes i and the j this only depends on c i and c j only depends on which community they belong to and we denote this probability as b c i c j and so roughly speaking this is a by saying you know we ignore all the individual characteristics of the node they you know Of the node, you know, everything only depends on which communities they belong to. For example, suppose, you know, in this, we have, you know, people in this room might be like two types of people, like a faculty and friends, sorry, faculty and students. And these are the two communities, whether two people are friends of each other only depends on whether they are faculty or they are students. And they don't depend on any other individual characteristics of the people. So you can argue this is a strong. So, you can argue this is a strong assumption, but this is the model we'll start with. And this is a generative model in the sense that given all these, you know, the value of all these parameters, et cetera, here, we can use this to generate a network. So, for example, here, we have a whole bunch of nodes. Suppose there are two communities, for k is equal to two. So, we can use pi one and pi two to assign each node to a community. So, now we have the community labels for each node. And then, based on the color of the community labels, Based on the color or the community labels of these nodes, then we can generate a newly random variable for each node pair. Then we can get a network. So in that sense, it's a generative model. And in practice, the inferential task is the reverse. So in practice, is that we observe a network, we observe the adjacency matrix, that's our data, but we don't see the colors of these nodes. We don't know which community each node belongs to. And the goal is that we. node belongs to. And the goal is that we try to recover which community each node belongs to. We try to recover the colors for these nodes. And of course, we also want to estimate these linked probabilities B. But once we have the colors, estimating the link probabilities is straightforward. So that's the inferential task. And then very naturally, we may use this block model. We can fit the model to the data and then try to estimate those community labels. Those community labels. So here's the likelihood function. This is the product across different node pairs. So depending on whether Aij is equal to one or zero, this is either B or one minus B. And in this likelihood function, we observe these A's, but we don't observe these Ci, CJs, and we don't know the B's. So we try to estimate these CI CJs and also B. So in principle, you can directly maximize this quantity. Maximize this quantity by finding the C1 to C and such that this is maximized. So we can do that. But computationally, it's very challenging. It's impossible. This is a LMP-hard problem. It's a combinatorial problem. So people have developed many methods to fit this SPM model. For example, you know, the NCMC variational method, the profile likelihood method of moment, et cetera, et cetera. So these methods, they are fine, they work well, they can be justified theoretically, et cetera. Justified theoretically, et cetera. But one problem with all these methods is that they're not scalable. So you can use them to a network with maybe like 100 or 1,000 nodes, but beyond that, the computation cost is just unbearable. And the work that motivated our work is this one by Arash Amini and Peter Biko and their collaborators. What they did is that they lifted the symmetry. They lifted the symmetry in a sorry they treated AIJ, AJI and AIJ, they are the same, they are the same quantity. So, what they did is that they treated AIJ and AJI as two different quantities, so they are not the same quantity anymore. So, different rows of that adjacency matrix become independent. They're doing that, and they also use some approximation, et cetera, et cetera, and they propose the method what they call like a pseudonym likelihood. Actually, we'll see. It's not really like the kind of pseudo-likelihood we wish to have, but they call it as a pseudo-lihood. They call it as a student likelihood. But one very important advantage or feature about their algorithm is that it's scalable, it's very, very fast. And but one issue, a very important issue with this algorithm, they also pointed out this issue in their paper, is that the algorithm is not guaranteed to converge. In fact, most of the time it doesn't converge. So what they suggested in their paper is that you run their algorithm for three or four steps, and then you stop. Three or four steps, and then you stop. So that's their work, and that's the work which motivated our work. Another kind of method for fitting the SBM is to use the so-called spectral classing. This is again scalable, very fast, but in practice, people have empirically found that the performance in terms of the accuracy, find the accuracy for labeling this community, for this, the accuracy for labeling this. Accuracy for label in these nodes is often not, it's not as accurate as the likelihood-based method that would give a empirical studies. So here's our idea. Again, this is the likelihood function. And so there are two key points in our idea. One is the same as what I mean, you know, Vitco and their You know, Vico and their what they proposed is that we would lift the symmetry between AIT and AI. So we treat them as two different quantities. We don't, they're not that, we don't just, you know, we just ignore like you know the fact that they are the same. And this will sort of decouple the dependence between different rows of that adjacency matrix. And the second idea, this is very important, is that when we look at this link probability, we will decouple. We will decouple the row labels and the column labels as well. And we'll treat them differently. We'll treat the row labels as latent variables. So it's a random variable. It can take a K possible values. And we'll treat the column labels as unknown parameters. So they are fixed. They are not random variables. And to emphasize that, we'll also use two different notations for them. We use C to denote the row labels. We'll use E to denote the column labels. And based on that, we can write out what we call like the pseudo-likelihood function. So, here, this is the pseudo-likelihood function. This product is across different rows. So now, you know, we lift the symmetry between AIJ and AJI. So, we consider different rows as independent. So, that's the product across different rows. And this summation is due to treating each row. To treating each row label as a latent random variable, which can take k possible values. So, this is a summation over across all k possible values of a CI, each row label. You can consider each row follows a mixture distribution. And this mixture distribution has k components. And this is the summation across those k components of that mixture distribution. And the pi L is just the, you know, the. just the you know the the probability for each node ci you know belonging to the community uh l right and this product this is a sort of just like from the distribution from uh from the each component of the mixture and this product it's just across different columns because the different columns are independent and this is just a you know product across different columns and so that's the pseudo-likelihood function and the so so here like we have two sets of parameters Here, like we have two sets of parameters. One is what we call omega, so that's the pi and the b, right? The and the link probability and the sort of like the prior probability of each node belonging to each community. And also these east, these column labels, these are also the parameters. So these are the two sets of parameters in this pseudo-likelihood function. And then we can use an alternating algorithm to maximize the pseudo-likelih function. For example, given E, we can estimate omega, and given omega, we can. Estimate omega, and given omega, we can estimate e, and we iterate between these two steps. And it turns out the nice thing about that pseudo-lactor function is that in both steps, in this iteration, both can be computed very, very fast. So, for example, given E updating omega, we can use the EM algorithm because it's a mixture model. So we can use the EM algorithm. And both the E step and M step have closed form formula. So we can compute very quickly. And then given omega, when we try to update the column labels E, then we can use the. The column labels E, then we can use this formula. Again, this is the key thing is, but we can update this for each J for each column separately. So, this can be also done very, very quickly. And then we can iterate between these two steps until convergence. We'll see like it's guaranteed to converge. And the final output for the community labels will be the final output for E. So, as I just mentioned, one important feature about this algorithm is that it's guaranteed to converge. Algorithm is that it's guaranteed to converge. And here's an illustration of comparison between our method and Amini and Bickel and their method. So the horizontal axis is the iteration number. Vertical axis is the pseudo-likelihood that we wish to maximize. So for our method, as we can see, that you can show this is guaranteed to increase. And then after a few steps, it converts. And this is what happens to Amini's approach. To Amini's approach is that so if the first you know, this there sort of like a you know student likelihood of function criteria first increases and then like you know has a very sharp decrease and then starts to oscillate and it's never converged and you can this is not like atypical turns out this is pretty typical for their approach so that's why we they suggest in their paper that you run their algorithm for a few steps and then you stop and use that output as the That output at your estimate for the community label. Yeah. Are you using the same provider? No, no, it's different. So they didn't have this. The key is that they didn't really treat the row labels and column labels like separately. They just used some like approximation. They used some approximation like inside here, not using like a mixture model or whatever. Yeah. Right, right, so right, right. So like the two criterions are different. It's not like a different algorithm for the same criterion. The two criteria are different. So that's the convergence. And in terms of other theoretical properties, so like in the Kubernetes detection literature, people often care about something. People often care about something so-called consistency. And there are two kinds of consistency. One is the so-called strong consistency, the other is the weak consistency. So the strong consistency is to compare the C-hat, this is a n by one vector, C1 to Cn, comparing the estimated community labels with the true community labels. If the two vectors are exactly the same, like the probability for these two vectors to be exactly the same, goes to one, then this is referred to as a strong consistent. This is referred to as a strong consistency. And the weak consistency is sort of by looking at a, you can consider this is sort of like the misclustering error, right? This is the proportion of nodes that are misclustered, that are mislabeled. And this fraction or this rate goes to zero in probability, then this is referred to as weak consistency. So strong consistency implies weak consistency, and that's often what people prefer in the later. In the literature. And with our method, we can show that under certain regularity conditions, especially basically, this is saying if the signal is strong enough. So this is the K equal to 2 scenario. These ABBA are the link probabilities within community or between communities. And this is by looking at the difference between the within community link probability and the between. Probability and the between community link probability, the difference is large enough, then we can show that our algorithm can achieve strong consistency. This is for the general case, one case greater than two. Yeah. Oh, yeah, this one. Yeah, I could do like the light. Right, so so y y you do that, then like uh then it's just a you it's hard to get like, you know, sort of a um it's hard to to just so that's that's basically like this. Then you can you can you know yeah see, do you still treat like the row labels and columns differently? So that would be something like this. This is just by looking at different notes. This is just by looking at different node pairs. And directly optimizing this is very challenging. Like this one. It's just like, you know, for each CI, there are k possibilities. So in principle, there are, you know, k to the power of n possibilities for these C's. And maybe the lattice idea is to, let's say, suppose we work with this, but when we, yeah, no, but I don't think right out. But I don't think they can write out, it's hard to write like a generic formula for that, I guess. Another more relevant idea is that we tried also is that we treat both the row labels and the column labels. We treat the row labels and column labels separately. We don't consider row labels and column labels the same anymore. And we treat both of them as latent variables. You can also write out sort of like, you know, a sort of like you know a pseudo-likelihood function and turns out for that one you cannot get a very you cannot get a close form at least you cannot get a close form uh updating formulas in your em algorithms so you can that does not work either so somehow like you know we experimented a bit and just this treating the the row labels and column labels separately but one as the random variables the other as the parameter will allow us to have fast l Fast Les pressure like in the EM step. Yeah. Right, right, right. Right, right. Right. Good point. So for the one case equal to two, actually like you can show that you know, as long as your initial value is better than random guessing, then as long as not random guessing, it's guaranteed. You can achieve. Right. Right. But in general, when case is greater than two, it's not guaranteed. We have to make a strong, stronger assumption on the initial community labels. Right. We have to, because in our algorithm, we have to. We have to, because in our algorithm, we have to start with some initial E. So, for the, so, for the, so, in general, when case is greater than two, we need some, you know, we have some, we need the initial e to satisfy certain, you know, conditions so that you can have that consistent. Basically, we require, we need this actually to be weak consistent. And so, usually, we use the spectral clustering to initialize our output. And we can show actually that leads to our numerical studies. So we right, right, right, yeah, yeah, yeah. Because once you have the community labels, estimating pi is the we didn't really study the, but the thing is, like, once if we if we if we have strong consistency for of a C hat. But for C hat, then estimating the, you know, the like the pi height and the b height, they will also, they will, they, they will, they will be consistent, and you can also, yeah, they can be consistent. That's three for, and for b d height, you can also show like asymptotic like normality as it so yeah, so we can. When k is equal to two, that's not difficult. You can just do something. For example, you run spectral cluster. So basically, like what we do is that we use the spectral cluster to initialize our algorithm. And for spectral classing, you can show actually spectral classing guarantees weak consistency. So, yeah, so I will now go. I'm not going to show all the simulation studies we did, which is to show you. We did just show you a couple examples to illustrate the point. So, here case equals three, these are the pi's, and these are the link, you know, link probabilities, and we use a lambda to control the difficulty of the problem. So, lambda, you know, rubber sign corresponds to the mean node degree. And so, here, as I said, you know, we use spectral clustering to initialize our algorithm. And this is the comparison in terms of. Comparison in terms of computational cost and also clustering accuracy. So there's three methods. SCP is the spectral clustering, Amini's method, and PL is our method, due to the likelihood approach. And we can look at this first. This is to compare the running time, the computational time. So when we increase, here we have different number of nodes, increasing from 10,000 to a million. So this is like a very large network with a million nodes. Like a very large network with a million nodes. And here, what we can see is that for our algorithm, like the partial, the pseudo-likelihood approach, it takes about maybe 30 seconds to fit a network with a million nodes. So that's very, very fast. So with this kind of size, those MCMC or variational approach won't be able to apply. And this is a comparison in terms of the accuracy. This is the The accuracy. This is the NMI, the normalize the mutual information, the higher, the better. So, here we can see that in general, so all these two methods both use spectral clustering as the initial, like to initialize the algorithm. So, both perform better than spectral clustering, but our method achieves a higher accuracy than a mini's approach. And here's another setting, you know, sorry, similar, so I'll skip that. And just to So I'll skip that. And just to sorry, spectral SCP is spectral clustering. Yeah. So we use the spectral clustering to initialize ODF, to initialize both the Amini's approach and our approach. So just to quickly summarize, so here we have developed algorithm based on pseudo-likelihood to fit the SDM. With the SVM, and the key word is that this is a very fast algorithm. With a million nodes, you can fit the model within 30 seconds. And we guarantee the convergence, so that's an advantage over Amini's approach. And you can show some other theoretical properties. Another thing I want to emphasize is that this approach is not restricted to the SDM, and it can also be extended to, say, degree-corrected so-called block. To say degree corrected the sto-cast block mode, as we mentioned, the SBM has some limitation, like it ignored the individual characteristics of different nodes, right? Each whether two nodes are linked or not only depends on which communities they belong to. So ignored all the individual characteristics of these nodes. So there have been many network models which try to address these limitations for the SDM, for example, the degree corrected block model, et cetera. And this algorithm, this idea. And this idea can be extended to these other different models, like equity corrected SDM, bipartite-directed network models, et cetera, et cetera. Which I did talk about here. And so that's the first structure. The second one is what people often refer to as a smooth, so a smooth structure. So, what does this mean? So, it turns out that this SDM, it's just an instant. It's just an instance of a much larger network model. And this is referred to as exchangeable network models. And roughly speaking, this exchangeable means is that if you consider the distribution of this A, and if the distribution of A is invariant with respect to the permutation of the node labels, then we say this is an exchangeable network. So the distribution of A does not depend on how I label these. How I label these nodes, how I, you know, I label this node as one, this as two, this three, or I label this node as two, this as three, this as well. So, this if the distribution of a is invariant with this kind of permutation, then this is an exchangeable network. And David Aldos and Hoover probabilities, they showed a representative theorem for exchangeable network. So, what they say is that if So what they say is that if a network is exchangeable, then there exists a function f. So this f is a bivariate function. It maps this unit interval to this interval for probabilities. And also a set of IID random variables, Ci's, which follow a uniform, standard uniform distribution. And then such that the Pij is equal to F cos Ci of Kj. So that's what this, so this is like a representer theorem. It gives you existence. It gives you the existence of this f and casi, such that the p i is equal to this. And this f in the literature is often referred as graph fun. And these cases can be considered as a latent node prediction. So these c's are from the unit interval. So this unit interval corresponds to this unit interval. And this is this interval corresponds to this probability. But the key is that both this f and c, they are unknown. And one remark I want to make here is that the structure of this link propagated matrix, remember, that's the quantity we are interested in. The structure of p comes from the structure of f. So for example, if this f is the block-wise constant function, then it will lead to the stochastic block model in terms of the link probability matrix. So for example, here, suppose this is the unit square. This is how the F looks. This is how the F looks like. It's a block-wise constant. So there are these blocks. And F is a constant here, a constant here, and a constant here and here, but it's symmetric. And then we have these cos C's. And then if we look at these F, cos C I, cos C J at these intersecting points, and we pull those values out, we'll get a link probability matrix P. And then because this F is the block-wise constant, so this link probability matrix P will also be blockwise constant. Will also be blockwise constant, and that corresponds to the link probability matrix for the SDM. So that's what we mean by the structure of P comes from the structure of F. And the very natural question is that then, given A, can we estimate this F and can we estimate this, right? So especially can we estimate this F. It turns out that this problem is in general not doable because F is not identifiable. Again, shown by two probabilities. Firstly, Diacon is. Probabilities, Christy Diakanis, and his former student. But then other people have also shown that under certain conditions or restrictions, this f can be identifiable. For example, if we assume that the expected node degree, if we assume this function is the strictly nomotone, and this function is referred as the expected node degrees, because if you look at this function, this is an integral. So I fixed u is f u v, you know, roughly speaking. Because FUV, you know, roughly speaking, this is a link probability between two nodes u and v. So when I fix u, I integrate out v. This is sort of like how on average, how many nodes are connected with node u, right? Because each is an interpropability and this is the integral. So this is often referred as the connecting node degree. And people have shown that if this function is strictly monotone, then f is identifiable, and they have also proposed an algorithm to estimate f under this. Estimate F under this restriction. But what we try to argue here is that in practice, often our goal is to estimate this link probability matrix P rather than estimating this graph on function F. So maybe we don't have to set our goals that high. We can lower our goal. Instead of estimating F, we are just interested in estimating P. And estimate and P is always identifiable, but F is not. So that's It's not so that that's our goal, and also, uh, with this, you know, lowering goal, we don't need to make a very strong structural assumptions, like here, assuming that the no degree function is strictly monotone. We want to make strong assumptions like that. And also, we want our algorithm to be computational efficient, and also hopefully achieving competitive error rate. So, here's the idea or the format that motivated our idea. And then here we And here we rewrite this Aij as the Pij platform arrow, and the P is equal to F of the Ci Pj. And this format looks very much like the kind of format we see in non-parametric regression. So f is the underlying function. This is our observation. There's an error. But there's a key difference here. The key difference is that here we don't observe cassette. Unlike in the usual non-parametric regression, we observe x. Regression: We observe x. We wish we have y equals the f of x plus epsilon. We observe x, we observe y, we try to estimate f. But here we don't observe to see, because that's the key difference. But there's one idea we can borrow from the EU non-parametric. Yeah, it's still like a 001, yeah. And this is just a format. We're not really like study this. And there's an idea. Uh, but the idea, but there's an idea we can borrow from the non-parametric regression. That's the neighborhood smoothing idea we can borrow from the non-parametric regression here. The thing is that if given a node i, right, if we can find all the neighbors to a node i, we can find the neighbors of node i. But these neighbors of node i, they are not in the usual sense of neighbors of node i in terms of the Cassie, but in terms of how these nodes are. Of how these nodes are connected with other nodes, whether that behavior is similar to node I. So, how node I is connected with other nodes is described by this p i dot, the i throw of this link property matrix. And if another node I prime, and if how i prime is connected with other nodes, it's similar to how node i is connected with other nodes. So, in the sense that if a p i prime dot is similar to p i dot, then we consider i prime is the neighbor of of i. Is the neighbor of i. And if we can, so given a node i, if we can identify all the neighbors of node i in terms of this, and then we get a neighborhood of a node i, and then we can use neighborhood smoothing. We can borrow information from these i prime to take a neighborhood average, for example, to help us estimate the link probability, pij. So that's the basic idea. And here again, And here again, I want to emphasize that the neighbors are not in the sense of C, as in the usual non-parametric expression, but they are in the sense of how these nodes are connected with other nodes. Now, here, then, this is an example where f is the blockwise constant function, and we have three nodes, one, two, three. So, in terms of K C, we see that nodes one and two, they are very close to each other. In terms of the C, they are next to each other. But in terms of how these But in terms of how these two nodes are connected with other nodes, which are described by the F value on this dashed line and the F value on this dashed line, and they are very different. They are very different. They belong to different communities. So basically, the C1 and C2 are node one, node two, although they are very close in terms of the C, but they are not neighbors of each other. They are not neighbors of each other in this sense. On the other hand, this node three. On the other hand, this node 3, although node 3 is far from node 2 in terms of the CAC, but in terms of how these two nodes are connected with other nodes, they are identical. They are from the same community and they are identical. So these two should be considered as neighbors of each other. So that's the difference from the usual non-parametric regression. And then the question now is that, you know, how do we, given the data, how do we define like, you know, two nodes are really neighbors of each other, et cetera. Are really neighbors of each other, et cetera. So we use this distance. So this is just by looking at. So this tells us how node I is connected with other nodes. This tells us how node I prime is connected with other nodes. So we look at the difference between these two. And we use that to quantify the distance between two nodes, I and I prime. But this is, you know, as a definition, it's fine. But in practice, we cannot compute it because we don't know F, we don't know the C, we cannot really compute this quantity. To really compute this quantity. So, how do we find? So, given a node, how do we find the neighbors of node I? That's the question. And it turns out that although we cannot compute this because we don't know the C and F, but we can bound this quantity using A. So it turns out we can bound this quantity, which depends on C and F, which we don't know, by another quantity, which only depends on A, where A is observed. That's the network, that's the data we have. So we can use this upper bound. This upper bound to help us find the neighbors of node i. So, give basically given a node i, we can get all the i plan such that this d tilde, which only depends on a, is smaller than some threshold. And once we have the neighborhood of node i, then we can use the neighborhood smoothing to help us estimate the link probability. And then we can also show like that and. And we can also show that under certain regularity conditions, our estimated p-hat can achieve certain error rate. So the regularity conditions is rather weak. We only need to assume that f is p by the leap sheet function. So we don't need, it doesn't have to be like a super smooth function. And also, we did a lot of simulation studies. lot of you know simulation studies we tried on different structures of the graphon for example whether the graphon could be you know low rank or with a strictly monotone actually this not a strictly monotone but low rank but with monotone degree or low rank but not a monotone degree and also whether the the graphon has the local structure or not having local structure and there are several benchmark methods we compared with one is the so-called network histogram method this is the by sort of This is by sort of approximating this graph on f by a step function, but with many, many steps. So you have a graph on function f, I can approximate that using a step function, but with many steps. So this is similar to fitting an STM model with a very large number of communities. Because we saw that a step function, a block-wise content function corresponds to an SDM. So if we fit an SDM with many, many With many, many communities, that's like a function with many, many steps. So we can use that to approximate f, or you know, use this short and smooth method, but this relies on the assumption that the expected node degree is a monotone. And also just like the low-rank approximation. This is a generic method. This is not a network estimation. This is just a generic matrix approximation. So we which one? This is a generic, not for SBM. This is a generic. If the graph bound is the piece by the lip sheet, then we have this thing. And here's the, you know, this is one example where this is an SBM example, that's a true link probability matrix. It has this block structure. Each block corresponds to a community. And this is the observed A. This is our estimated p-hat. And these are from different other methods. And of course, in practice, we don't observe this A matrix with this nice structure where the nodes are sort. Structure where the nodes are sorted, right? This matrix, the rows and the columns will be permutated. So you don't observe this structure. So over here, different methods all work similarly well, but this is another structure. This is where, again, the P is low rank, but the no degree is not a monotone anymore. It's a homogeneous. And then our method still works well. But then, like, you know, the short and smooth method, which relies on the assumption that the node. The assumption that the noded degree is the monotone stopped working. And there are other structures where this is the full rank, then, and this is the full rank for the homogeneous degree and with the global smoothness, et cetera. Our method still works reasonably well. So, under different settings, basically, you know, under all these different settings, our method is probably like the most robust one across these different other benchmark methods. The other benchmark method, which rely on different assumptions. So, just to quickly summarize, this is our contribution is that we propose an algorithm for estimating the link probability matrix P. So we set our goal lower rather than estimating the graph on F, our target is to estimate the link of probability matrix P. So we have an algorithm which is computational, and it's very easy to implement. And it's very easy to implement. Based on neighborhood most thing, which is very easy to implement. And I didn't talk about that, is that the tuning is very also easy. And the key feature is that it does not impose a strong prior structure. So it does not require the F to be block-wise constant. It does not require a community structure. It does not require a low-rank structure. And it does not require a monotone expected degree assumption. The degree assumption. So the assumption is rather weak, and also it achieves a competitive error rate. So that's the second structure. And for the last one, this is the latent phase model. And as I mentioned, this is the kind of model that I like the most recently. The reason is if we think about the block model, the stochastic block model, that's a very restrictive parametric model. Very restrictive parametric model. And the grapha model, on the other hand, is very flexible. It's sort of like a non-parametric, very flexible non-parametric model. And this latent space model is sort of like somewhere in between. I feel it's not as restrictive as the block model, but also not as flexible as the graph model. But still, it's very powerful and it still allows you, gives you good interpretation and allows you to. And allows you to do a lot of downstream tasks. So that's the kind of model that I like the most recently. And so the idea is that for each node i, we will represent each node i by a z i, which is the d-dimensional vector zi. And then the link probability between two nodes, i and j, will only depend on z and zj, the two latent representations for these two nodes, okay, z and z. For these two nodes, Zn and Zj. And you can consider this as kind of like embedding. You embed each node into some dimensional space. And this function g, and also the corresponding parameter, we say that g can be treated as either unknown or known. If g is unknown, this is very much like the graph amount. But then that will be too flexible. So usually what people do with latent space model is that we would treat this g model is that we will treat this g as a pre-specified we will specify the format of g but there will be some parameters theta which are unknown in this in this model and these latent positions these z i z j's they can also be either treated as random variables or fixed parameters and here in this talk we'll treat them as fixed parameters and but I think you can treat them as random variables and usually you have to use some Bayesian computational methods to estimate the model. Computational methods to estimate the model, which is usually computationally computational costs will be higher. So we treat them as a fixed parameters. And this model includes many models as special cases. For example, the stochastic block model and different variants of the stochastic block model can all be considered as special cases of this model. And in this model, there's also one particular Also, one particular family of models that we will consider here. This is the so-called inner product latent space model. And the inner product in space model, here we'll consider Zi. Again, that's the latent position or latent embedding for node I. That will be a d-dimensional vector in an Euclidean space. Euclidean space RD. And you also usually will associate each node I with a degree parameter alpha i. That's a real number. And then again, Real number. And then again, the link probability between nodes i and the j, t i j that depends on this alpha i, alpha j, and the inner product. So that's where the name inner product label space model comes from. So that depends on the inner product between z i and z j. And this alpha i and alpha j, they are referred as degree parameters in the sense that if you consider the alpha, usually if alpha i value is larger, that means you know node i is more likely to be connected with other nodes. With other nodes, and vice versa. And here are two examples of this inner product latent space model. One is the so-called RDPG model. So in this model, this G is just the link function. It's just the identity matrix, identity function. And then these alpha values are equal to zero. So the link probability, dij is just equal to zi transpose zj. And so that's the RDPG model. So that's the RDPG model. But obviously, like that model has some limitation in the sense that just the link of probability Pij is between 0 and 1, this inner product Zi transpose Zj, you know, you can restrict them to be, you can restrict the inner product to be between 0 and 1, but it's not very natural. So here's a more natural model. Instead of using the identity link function, you can use the logit link function. So the G will be the low j. So the g will be the loaded function. So that this is guaranteed to be between 0 and 1. And also these alpha are not necessarily like 0. And this is also like a generative model in the sense, suppose you have, you know, the corresponding z, then you can get the link probability matrix, and based on that, you can generate your network. And in terms of estimation, so you know, observing a network A, how do we estimate these So we estimate these, we estimate these least and alpha, et cetera, in terms of estimation. People have developed different methods for different models. So for the RDPG, for example, you can use the spectral, you can use spectral method. And for the logistic link function, you can use the project gradient descent to estimate these. But in terms of uncertainty quantification, sort of quantifying the uncertainty of your estimate of these, there is a gap between the linear and nonlinear link functions. link function. So for the RDPG, which uses the linear or the identity link function, people have developed the consistency, average consistency, also individual consistency, and also developed the syntactical distribution for these estimated Z hat. But for the non-linear link function, for this logistic link function, it turns out that the technique people use to develop these People use to develop these things cannot be extended to the non-linear link function. So these are all open, sort of like you know, question marks. So that's the these are the problems that we try to address here. So here we'll consider the maximum likelihood estimate. And this is the, so if we will consider the maximum likelihood estimate. likelihood estimate and the the the key observation is that these zi's they are not identifiable okay so this from from this inner product latent space model these zi's are not in the sense for example if the z's if you rotate these z's and this inner product will not change okay and also if you shift these z you can absorb the the shift into these alphas so the alpha so these these they are not identifiable up to a rotation also a shift To a rotation, also a shift. And this makes the log likelihood function corresponding to this inner product latent space model, this makes the Haitian of the negative log likelihood function corresponding to this inner product latent space model to be asymptotically non-positive definite. So that this allows you to, but this allows you to but allow you to but this allows you to sort of further develop the asymptotic distribution and also estimate the z hat so to address that we instead of using the the the negative log likelihood function or the log likelihood function we consider a penalized version of this log likelihood function and this penalty is different from the usual penalty we considered in high-dimensional regression where we try to shrink the uh you know the estimated parameters to shrink them Estimated the parameters to shrink them towards zero, over you know, etc. So to achieve sparse data, etc. So, this penalty is really to address this non-identifiability. So, this is the negative, not negative, this is a likelihood function based based on the inner product data space model. And this is the penalty term. So, the penalty term, this penalty term penalizes the off-diagonal element, but V transpose Z. Transpose Z. And this pinolite is the center of Z. So basically, like the effect of these two penalties, and this constant, the C2 doesn't really matter. It doesn't really matter. The key is that in order to penalize the off-diagonal elements of the transpose Z, so that, and also to penalize the center of Z, the effect is that in the end, in terms of the estimated Z hat, all the off-diagonal elements will be estimated as zero, and the center of Z hat will also be estimated as zero. Center of the Z hat will also be estimated as zero. So, this will fix the rotation and fix the shift of Z hat. And this addresses the identifiability issue. Basically, it gives you a unique solution. And it automatically imposes identifiability constraint on our model. And also, these two terms allows the Hesitation of this quantity to be symptomatically positive, definite, and that will allow us to develop the Allow us to develop the individual consistency and also asymptotic normality for the estimated rehab. So that's the key idea. And also that's why the exact quantity of this C doesn't really matter. As long as this is positive, this will be estimated as zero, this will be estimated as zero. And then so based on that, we were able to develop, especially like, you know, individual You know, individual uniform consistency and also asymptotic normality. So the estimated phi here contains both z and alpha. Here are some numerical results. Just take that just to show like the consistency and the syntactic normality, et cetera. And also this idea, you know, I showed that for this specific. Because this specific inner product latent space model where the link function is the loaded link, but this can be extended to other link functions. So, not just for the loaded link function. And also this can accommodate the sparse setting. So again, we need to do some work to add some additional parameter to control the network density, which will allow the link probability to go to zero and as increases. And also, we can allow for dependent edge. Allow for dependent x with again with you know more assumptions so so this approach can also be applied for the linear for the linear link function but the approach the proof the uh the proof the data for the linear case does not extend to the non-linear case because they use the the the approof the Because they used the, the proof, they used based on spectral decomposition. They used the spectral, they didn't use the likelihood-based method to estimate those z hats. They used the spectral method to estimate the z-hat. And the proof is based on that. And with nonlinear think function, we use the likelihood-based approach to estimate the z. And that's why I give the proof, but this one does not apply. Yeah, this is a data example. This is like a statistician classroom network. So each node is a statistician. If two statistics have co-authored more than two papers, then there's a link. So this is usually, this is not atypical. This is very common. So you have a network A and you fit a latent space model. You can embed each node into a two-dimensional space. You can get a picture which looks like this. You can get a picture which looks like this. So each node has a two-dimensional representation, et cetera. And basically, what we did is that we are able to associate each estimate with some uncertainty quantification. So we're able to estimate the covariance for each estimate z. So we can associate each estimate with some uncertainty quantification. And again, just to quickly summarize, so basically what we did here is to establish a symptomatic distribution for the A symptomatic distribution for the estimated Z hat, the latent predictions for these nodes. And also, this strategy, so based on this penalized likelihood, again, the penalty is different from the usual penalty in high-dimensional regression. The penalty is really used to address the identifiability issue for the model. And that strategy can be also extended to accommodate a sparse network and also a network where the edge depends. Acts are dependent. And yeah, this work is on archive. Sorry. Right, yeah, this is a like basically people usually use just the gradient descent to estimate the z. So in terms of the number of parameters, it's on the order of. Parameters, it's on the order of n times d. So n is the number of nodes, d is the dimension of the latent prediction, z, for the n times of d parameters, and you can use the gradient descent to estimate those z. It's not a convex because of the, you know, z, because of the, it's not, because of the each z transpose z, so it's not a convex, right? So, actually, yeah, that's pretty much what I wanted to say. Yeah, thank you. Oh, sorry, I guess I didn't make it. No, but in the second part, the key is that we want to deviate from the piecewise concept. To deviate from the piecewise constant function, we want to make a this f is just a this f is just a no no no p f is just a like a piecewise leaf sheet function. So suppose you can ignore, suppose ignore the piecewise thing, just can be like a leap sheet function. So f doesn't have to be a piecewise constant function. So the So P will be, so depending on what depending on how F looks like, so P will be a matrix with the entries corresponding to the F values at these intersecting points. So this is a particular case where F is a piecewise constant. Yeah, piecewise constant function. But in general, f doesn't have to be a piecewise constant function. Suppose f is just like a small. Function. Suppose f is just like a smooth function, then your p will be smooth with respect to a particular permutation of the nodes in this sense. So when f is not a piecewise constant, there's no concept of community anymore. So this community concept only applies when f has this piecewise constant structure. But if f is in general, But if f is in general not a piecewise constant function, if it's just like a smooth function, and then your p does not correspond to a spokesperson block model anymore. Right, so yeah, second part is not to determine the community structure anymore. The second part is just to, yeah, sorry, I didn't make it clear. So the second part is really just to estimate the peak so I observe our, you know, everywhere like Everywhere, like throughout the talk, the goal is that I observe A, I want to estimate the P. And the first part is that if I assume a community structure, then I can estimate P. It's a, you know, I wouldn't say it's easy, but it's sort of straightforward. The second part is that I don't want to make any assumption on P, on the structure. I just want to assume that this underlying F is, say, piecewise smooth. say piecewise smooth. Smooth, right. Okay. Could be just one piece. That's also smooth. So that could just f is piecewise smooth. And then how do I estimate p right? And the third part, I'm trying to propose something, not propose, I'm trying to do something for a model which is somewhere in between. It's not like a, it doesn't have to have like a community structure. It doesn't have to be just a non-parametric smooth function. It just has this inner It just has this inner product latent space structure. Right, I mean, I guess the yeah, you can. Yeah, you can probably, I know we didn't consider we didn't do that, but you know, presumably you can develop some like goodness of fit thing, goodness of fit test thing to check. And you can, you know, if, but, you know, within this, within the inner product latency-based model, you can develop a test to, for example, testing the dimension of D. I think just in general, it's a I think just in general, it's probably like a difficult property in general to test whether this is the right model. But within the family of this model, you probably can do some testing thing to test D. Because I'm sure, I want to say I'm sure. Yeah, you started slightly. Oh, because I feel this is still like a powerful model. It's a powerful model in comparison to the SBM. It's more flexible because in SBM, it's very restrictive. These have to be discrete. And another, in comparison to the graphal model, this is not that flexible because in the graphical model, we can only estimate p. And then what do we do with that estimated p? With that estimated p. But with this model, we can get this estimated z. And with this estimate z, we can do a lot of downstream things. So that's more useful. And also, you can still interpret this model in the sense that each node has a representation. So it has a better interpretation than the graphon model. In the graphon model, in general, we cannot estimate f, we cannot estimate c, although we can estimate. Although we can estimate those p-hat, but we don't get much interpretation out of them.