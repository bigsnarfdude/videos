With many, many different kinds of objects. So, we had 10 different objects. The objects, the images of the objects were either synthetic images that we generated in the lab because we had full control of where we can place the image, what kind of rotation and position and size of the objects to choose from. We also used natural photographs from Microsoft Cocoa database. So, there were two kinds of images that were used. Okay, so here is the first plot that I want to show you comparing performance of at that time AlexNet versus. At that time, AlexNet versus pulled monkey performances. So, monkey, so each dot is how well each dot corresponds to an image, and the magnitude means how well could the monkey or the model detect what object was there in this particular image across average across all possible distractor. So, just for illustration purposes, I would divide this into two groups, control images. So, you can think of these images where the models and the monkeys are aligned. So, if the models did well, monkeys also did well. Did well, monkeys also did well. If the models could not solve it, monkeys also couldn't solve it. There are also these challenge images where the monkeys were much better than the models. So we hypothesize that maybe within the space of challenge images, there are some images where the monkey is able to solve the task because it can take advantage of this recurrence system, which is dynamic in nature. And all of these things together make the monkey able to solve the task. Whereas the models with just one pass of the feed forward. One pass of the feed-forward hierarchy through the feed-forward hierarchy, they're unable to complete the solution for these particular images. So, that was one of the hypotheses. So, to test whether there is anything similar that we can observe or anything that is kind of evidence of what I just said in the brain, I started recording in V4 and IT of these monkeys when they were actually doing the task. So, we implanted UTA arrays across IT and V4 in two monkeys, and we started recording in these areas while the monkeys. In these areas, while the monkey was doing the task. So, just to illustrate the next steps, I want to focus on a very specific band of behavior and on these two image types, so control and challenge images. So, let me take you through the process of the analysis that we did. So, let's say we show an image of a face to the monkey, and this is picked from the control image group. So, let's say we are recording from three arrays. These are three arrays, each could be 100 electrodes. Each could be 100 electrodes, so we can be having 300 neurons recorded simultaneously. So we can show an image to the monkey and then we can start recording from this arrays. And there is some neural activity. So what for the analysis, what we did is we binn the neural activity into different time bins. And so this is the first 10 millisecond, let's say, of the response. And we can count the number of spikes in each of these neurons. So each row is a separate neuron. Rho is a separate neuron. So then we can make what is called a population vector, IT population vector. We can pass it through a decoder that is trained and tested in a cross-validated way, and we can predict how what the object is in that image. And from those predictions, we can come up with a D prime score or percent correct score, for example, of like how well you are doing. So right now, this is the first 10 milliseconds of the response. This information about the image hasn't even reached IT. So the decoding accuracy. So, the decoding accuracy is very low, as you see here. So, as time progresses, the neuron starts to respond. You see here in green, and then the decoding accuracy becomes better and better of the system. So, you can kind of see this is a probe sort of into the dynamics of the evolution of object identity information in the brain, in the inferior temporal cortex of the monkey. And we call this point the object solution time, which is the time at which The time at which the blue curve, which is the decoded performance from the neuron, hits this gray kind of orangish band. So this is what the monkey's actual accuracy was in the task. So for every image, the monkey has a, it's kind of like the dots that I was showing you in the previous plot. So every dot had a value. So each image has a particular accuracy. And this is the time it takes to reach the monkey's original accuracy in the behavioral task internally. Role task internally. So, also, some other things remember is that the task is done kind of around here. The images are shown for 100 milliseconds, then the choice screen comes up at 200. The monkey probably provides a response around 700 milliseconds or so. So, all of this is happening even before the choice screen has come up. So, you can kind of decode what the monkey is going to predict internally, even before the monkey has actually made a decision. So, I can pick another image from the control image set, and you can. image from the control image set and you can I can plot the same plot I can pick images from the challenge image and this is where we start seeing some differences here is another image of a dog what the main difference that we saw was that although the neurons start to fire quite early similar to what we were observing in the control image set the decoding accuracies were only becoming sort of high at the later time bins it's almost like the linear separability of the solution took a while to sort of Solution took a while to sort of unfold, whereas the responses in the IT was already there. So, this is kind of, it suggests that there might be some information coming into IT later on that is able to linearly separate out the objects into its different categories, whereas in the beginning, that sort of secret sauce is not there. And we could assign, we could hypothesize that that might be some sort of problem. Some sort of product of a recurrent computation that is happening at the level of IT. As I was saying, there was no difference in the latencies of the control and challenge images in terms of the firing rate. Most of the differences were in the firing rates later on in the responses. So in a similar way, we actually got the images. We ran around 1,320 images, and for each image, we could reliably measure the time of solution completion. Measure the time of solution completion in IT. And what we found is that on average, there was a 30-millisecond delay between the challenge images and the control images. And this is the distribution that shows that effect. So one of the main hypothesis from this study was that maybe this is evidence for, this additional compute time is evidence for recurrent processing. We could not, this is not direct causal evidence of recurrent processing, but all of the analysis that we did sort of together. Analysis that we did sort of together provided converging kind of evidence that that is a very strong hypothesis that this additional compute time might be due to recurrent processing. And in fact, instead of thinking of this as challenge and control images, we could probably say that recurrence might be more critical for images that have longer solution times. Another interesting observation was that, so these neural network models, the feed-forward neural network models, initially. Neural network models initially got a lot of hype because they could explain neural activity that you can record from monkey IT. What I found in this study was that that high explained variance was mostly limited to explaining the early parts of the response of the neural responses to the images. So, this is also consistent with the idea that maybe later part of the neural response. Maybe later part of the neural responses are contaminated with recurrent signals. So, these models are actually struggling to account for that part of the signal. So, again, this is another explanatory gap between the early and late response that might be attributed to recurrent processing. Now, all of this data as it is coming out now becomes the target for recurrent neural networks, which will have its own dynamics to kind of account for. So, imagine we started with this. So, imagine we started with this plot, you know, the scatter plot where there were challenge images, control images. If you build a better network, you would expect that all the challenge images will now fall in the unity line because the monkeys and models should be performing exactly in the same way. That's kind of a success. But if you're making a recurrent neural network model, which the promise of which is that it's actually sort of absorbing the recurrence within the system, they should not only do that at the behavioral level, they should also start bridging this gap between, you know. Bridging this gap between the early and late responses, and they should also start producing the solution exactly at the times that we had recorded in the previous plot. So I could kind of say that, okay, dynamics and recurrent computations might be necessary for core object recognition. But one question or one of the sort of follow-up question was that which recurrence circuit motives? Because, like, recurrence could be anything. So, we were using recurrence kind of loosely as both. Using recurrence kind of loosely as both lateral feedback and like corticocortical feedback. So, one of the follow-up questions that I asked was: What kind of recurrence circuit motive are relevant in this particular behavior? So, if we draw out kind of the neural circuitry a little bit more elaborately, we can kind of see that, okay, there are many other factors and players here. The ventral stream projects to itself, it has bypass connections, it projects to PFC, projects to paranormal cortex, and many other parts of the brain. To parhenial cortex and many other parts of the brain, they all project back to IT. So it's like whatever you can choose your favorite, you know, circuit motive and ask: Is this circuit motive important for core object recognition? Now, not all experiments are quite feasible yet. For example, it will be very difficult to just specifically perturb recurrence from V2 to V2 or V4 into V4. But there are slightly easier experiments like you can probably inactivate. Sorry, you can probably inactivate. Sorry, you can probably inactivate PFC and ask what happens to this system. So, for multiple reasons, I chose this particular circuitry to probe. The other reason was also that there has been a lot of work done in PFC with object recognition, yet the current models don't really talk about the role of prefrontal cortex in terms of core object recognition. So, I thought it would be an interesting next step to take to kind of see whether that circuitry is involved in this process. So, I started with this experiment. I started with this experiment. It's a very simple sort of setup. The idea is that the main question is: are top-down feedback from ventrilateral PFC critical to this behavior? So, the main idea is that you see the monkey sees an image, it goes through the ventral stream, reaches the ventrilateral PFC. We know that these anatomical connections exist, and they kind of feed back to the ventral stream, maybe IT, maybe V4, multiple areas. And so, the goal is to then go in and deactivate, so using musimol injections, sort of inactivate PFC. Injections, sort of inactivate PFC for temporarily, and ask like what happens to whatever I was doing before. So I injected, implanted arrays in IT and the monkey was doing a behavior. This was exactly the setup I used previously. So now I do the same exact experiment, but with two conditions, one where the monkey is intact and once where the monkey doesn't have a PFC working as before. So, and the main hypothesis is a recap from before is that we kind of determine. That we kind of determined these images that were early solved and late solved. And the main hypothesis is that maybe late solved images will have slightly more deficits than early solved images if when you perturb PFC, for example, in this case. So here are the results. I'll quickly go over it. So both the neural and the behavioral results were consistent with this hypothesis. So we found that late solved images on average showed around On average, showed around 2% higher, lower accuracy. So these numbers are really low, and we can go into why these numbers are so low. And there might be actually very interesting applications of these kind of numbers in terms of model building. But regardless, there were significantly different between early and late SALT images. And also behaviorally, we observed that the late SALT images showed a higher behavioral deficit compared to the early SALT. Behavioral deficit compared to the early solved images. And another kind of interesting observation was that, remember, I showed you this plot where early responses were explained better by the current feedforward networks compared to the late responses. So, this was an intact monkey where there was this feedback going on. So, now we kind of like this is the explanatory gap. So, now we sort of perturb the feedback. Perturb the feedback projections potentially from PFC to IT or PFC to the ventral stream, kind of you can think of this as making the monkey maybe more like the feed forward models in some sense. So what we observed is that the feed, the forward response predictivity, the early response predictivity remained almost the same where there was a slight improvement, but it was significant of the late part of the response once you perturb PFC. So it's almost like PFC. So it's almost like you have made the monkey more like the feed forward neural network. One thing that this kind of result tells me, I think, is that the feed forward neural network that we had been building and was prevalent in the field, we're not too far off in approximating the feed forward responses because it almost tells me that we are in the right sort of space in terms of just explaining the feed forward responses in the system. Responses in the system. All right, so I think at this point, I can probably claim that I did test whether brain dynamics, specifically tied to recurrence, is critical to model this behavior. So I think we have proven the point that, okay, dynamics in IT potentially at least is an important and critical component of the behavior that we are interested in modeling and studying. So now is the time to add these dynamics to models and test whether these models can be falsified. Test whether these models can be falsified. So, this work I have been mostly doing with in collaboration with Jonas Kibulias, Martin Schrimm, Dan Baer, Aran Naibi, and Dan Yamin's group. So these groups have been building these kind of feedback networks, record neural networks, and the goal is that these networks are supposed to be explaining the data that previously feed-forward networks could not explain. So there are some. Some successes, for example, you know, Martin's paper showed that there was correlation was higher than zero potentially. So it looks like a high correlation, but these were just averaged across different dots. So the actual reported correlation is image by image. These are averaged across image. So it kind of looks like a smooth line. But this was the plot they made, which showed that object solution times predicted from the models were correlated with the object solution times. With the object solution times that we got from the actual neural responses. So we might be in the right direction, but this is still, I think, I consider this work in progress where these models are now coming out and their goal is to sort of try and explain the neural data that I have collected. As I showed you that now I have added another burden onto the modelers, which is like they have to actually now accommodate this VLPFC node into their model. So this is again, this work has not. Model. So, this is again, this work has not been done. This is kind of a proposal that maybe in future models need to also incorporate some form of a PFC node in their system. And I have been trying to like see what I can do in order to help them. And one of the things that I've been doing is actually recording now in PFC in the same areas that I had previously inactivated so that that data can also start to constrain the kind of efforts that the modelers are making in terms of coming up with these models. So we started with this idea that, okay, there was a feed-forward system, and I kind of like, you know, challenged whether dynamics or recurrence are important for this behavior. And we sort of, based on different experiments, shifted this model into a recurrent model that has sort of connections within the recurrent connections all across the ventral stream. Now, one question that I think sort of came up while we were doing all of this. Came up while we were doing all of this analysis. And I think it's a very important question. And also, sort of the it is kind of empirically challenged. So I think the answers have to come from, you know, labs that are doing experimental work, which is what is this a model of? Is this a model of one specific monkey? Is it a model of a superset of all monkey behavior and neural activity? Or is it like just the are you trying to just model the shared variance across different monkeys? And depending on Across different monkeys. And depending on what you set this up as, you might expect different goals or different ways of falsifying the model. So I'll quickly go through this. So if you consider the model to be just one monkey, so model is just any random monkey that you can pick up from the wilderness. So if that is the case, then the experiment goes like this. So you have a pool of monkey and you pick a monkey and you ask, like, how similar is this monkey to the pool? You have some number. You pick another. You have some number. You pick another monkey, you get another number. You pick different monkeys, you get different numbers. And you have a distribution of monkey to pool similarity, right? So you can get this distribution. And now you can ask, okay, I'm going to treat a model just like a monkey. And how similar is it to this distribution? So how similar is to the pool of monkeys? So maybe it's not, or maybe it is. So this model will, we will say that we cannot falsify the green model, but we can falsify the red model. So based on what Falsify the red model. So, based on what we have defined as kind of the target or target model, this sort of way of falsification will proceed. The second one is: what if you like want to treat the monkey as model as just another sort of random pool of monkeys? So then you split the monkey pool and then you do the exact same thing. You compare these two pools and you get kind of get a similarity score across different splits. And then you can do the exact same thing with models comparing with the pool. Now, I have shown you this. Now, I have shown you this data where I collected data from one monkey and I got this kind of neural decoding accuracy over time. What I wanted to show you was that, so I got different numbers for different images. What I wanted to show you that I can use the same framework to analyze this particular data as well, if I had many, many monkeys in the pipeline. So I can take monkey one and I can take X number of neurons. This is kind of a critical concept. I can go into it later in the question answer because, like, Go into it later in the question answer because, like, how many neurons we take and how we take them is an important factor. But you can, for each, you know, image set a threshold of like when does the neural decoding accuracy based on X number of neuron hit a certain level? So let's say it's two, you can get a number, you can get a number for another image. And you can do this for monkey one. You can get a vector of for n images, you can get an n-dimensional vector for decoder latency for a set particular threshold. And you can also get that for Can also get that for the pool. And now you have these two vectors that you can compare and set up the expectation for what a model should produce in terms of dynamics if it is a model of like one individual random monkey compared to the pool. So I've done some of this analysis based on three monkeys. We have now seven monkeys in the pipeline. So we can do more of this analysis. And, you know, based on different thresholds, you get different consistencies between neural decodes. And potentially, you can claim what is a better benchmark. Can claim what is a better benchmark to use in terms of falsifying models using this technique. So, the question is now: like, where do dynamic models stand with respect to these kinds of benchmarks? And we are updating this in BrainScore, which is a platform that we have developed in the lab. And so, if you are interested in this kind of analysis and the results, so stay tuned. This is going to come up in BrainScore very soon. All right. So, I think this is ongoing work, and I've said that BrainScore is going to be the answers that we are providing. So, please keep checking this website if you have. Keep checking this website if you haven't, and that's where all of this data will come in some format, one or the other. So, finally, I want to thank my lab for being a great place to conduct all of this research. I've had great colleagues for the last six to seven years working with me. And I've learned from all of them a lot, and also especially Jim, who has been super supportive of my research, and all my funding agencies for keeping the lights on. And I'm also excited to start my own group at York University. A group at York University, and I'm going to start this July 2022. So I'm kind of very excited to take this forward in all possible ways. Thank you. All right. Thanks for a very cool talk, Ko. And it looks like we have a few questions already in the chat. So I'll just go through them. So Hannah Choi asks, what are the key Asks, what are the key differences in terms of image properties that separate early versus late solved images? For example, noise level? Yeah, so I think we tried to answer that. So we took a lot of these image properties and we asked, like, can some image property, you know, how well can the image property classify images into control and challenge? That was one way we did this analysis. So what we found is that, okay, there was. So, what we found is that, okay, there were some properties, for example, eccentricity of the object or whether the object is occluded or not, that could significantly predict whether the images were going to be classified as high object solution time and low object solution time. But what we found was that the predictor that had the highest predictivity was basically the difference between the actual behavior of the model and the human behavior. So, I think. Behavior. So I think at the end of the day, some image property or a combination of some image property, you know, that is what makes the image. So that will predict these differences. But what we found is that regardless of that, the best predictor was the difference between models and behavior. The other important thing was that even if we controlled for that particular image factor, so let's say it's like clutter or blur, even if that produced high or low object solution time. Reduced high or low object solution time, if you control for it, or you took all images where it's the same clutter level or same blur level, you could still find the solution time differences between the challenge and control image sets within that group. So these were the two ways in which I kind of like dealt with this control analysis. Thanks, Ko. And then Sarah had a question. Do you just want to unmute? Yep. So So, um, thanks. Very, very interesting. The main point of the first part of your talk was to convince us that dynamics need to be incorporated, in this case by incorporating feedback. But if I think of your deep network as a feed-forward network representing different areas of the visual pathway, anatomically we know of the existence of massive feedback. Massive feedback, just even from V1 to LGN. So, why does one need to evoke these performance arguments in order to indicate that this feedback, which of course results in recurrence and results in dynamics, is going to be an important component of the architecture? Would any visual science neuroscientists argue that feed forward would be Feed forward would be the way to model this system? Yeah, so I think there's a key difference here, which I tried to make, which is that I was not trying to make a point that feed forward or feedback is like required for visual processing. It was mostly for this very specific behavior, which happens within like milliseconds. So even within 100 millisecond presentation, even the first pass of activity from retina to IT, maybe for this very specific behavior. This very specific behavior, it's probably the dynamics that is happening from V2 to V1 or from V1 to LG. And it is possible that all of those dynamics only play a critical role if you think of visual behavior that is slightly more long-term, slightly more extended in time. I think that was the initial point that I was trying to make. And I think the reason is that these models, these feed forward models, they are potentially only trying to kind of To kind of account for this forward path of activity. So, if we have to build the next model based on the failures of this model, then we have to try to constrain our experimental paradigm within those sort of parameters. And I think that's the reason why it was important to verify that even for this very fast timing stimuli and task, feedback is actually important. It can play a critical role. I think that's, but I don't think any. I think that's, but I don't think anybody, as you said, nobody would argue that at the end to completely explain all visual behaviors, you have to evoke feedback. And I don't need, and also I want to mention another point in this regard is that I think that, again, I totally agree with you that I could have basically made the point of, hey, feedback is important by kind of reading various other papers who have also kind of, in one way or the other, showed exactly the same thing. Exactly the same thing. But I think the added value of our paper that we published was that now, instead of just saying that feedback is important and there exists feedback between layer A and layer B or area A and area B, we actually have reliable measurements of image by image dynamics in IT that A are tied to feedback potentially, B are potentially the targets for the next generation model. So I think, but I take your point that it's not a very Point that it's not a very, very, you know, incredibly surprising finding that feedback is important, which kind of was a driving force for me, because I thought, why the heck are modelers not even paying any attention to these kind of processes? Because clearly, the first thing you see in anatomy is feedback, which is even more prevalent than feed-forward connections to some degree. Okay, thank you. All right, thanks, Sarah. And then Christina has a question. Christina has a question. Do you want to? Yeah, it goes very nicely after Sarah's question. So I was wondering about like local circuit recurrence because when you say recurrent, that's what my brain automatically reads. And then it's like, no, it's actually top-down feedback. So what's the role of recurring connections, which is by far the largest source of input to cortical circuits in this picture? Do you have any thoughts about that? So, when you say, so can you please like maybe can you expand a little bit on the like right? So, so feed forward input, top-down input, local circuit recurrence input. So, is there a place for that in the context of these kind of tasks or this kind of computation? Yeah, so well, first of all, I think right now we were just trying to see if there's anything beyond feed forward. So, local records, as you said, local and top-down, they're all. Local and top-down, they're all part of recurrence. And as I said, like some are easier experiments to do than others. So we just went with the top-down feedback, like causal testing of that to begin with. But I don't think that at the end, the answer will be like only top-down or only local recurrence is going to be important. Probably everything together is kind of giving rise to this particular behavior and neural dynamics in IT. Great. And the other thing that I was sort of wondering is whether. The other thing that I was sort of wondering is whether these top-down signals that we're looking at have to do with some form of attention. So, in these sort of ambiguous kind of contexts where the image is not really very identifiable as a class, you're focusing your attention to some features that are more distinguishable. So, it's like depending on the definition of attention in this kind of tasks, because these are like 100-millisecond presentations. And first of all, like the same kind of dynamics exist if the monkey is doing the task versus. The monkey is doing the task versus if they're not doing the task. So that's the first point. The second point is that, so because the images are drawn from like a set of like thousands of images, every day the monkey probably sees one image only once. So at priority, there is no expectation of like this particular image is going to come up or that. So in that sense, the monkey doesn't really, but once the image is there, there is a mechanism that you can call attention, which is like the feed forward drive could trigger certain kinds of information. Trigger certain kinds of information flow back into IT or v4 that might be trying to bias the representation in certain parts of the space. I was just wondering if sort of if you have some understanding of the tuning selectivity of the cells that you're recording from, whether the later responses that you're recording look in some way as a game modulation of this kind of neural correlates of what people occasionally call attention. Yeah, so I think as I showed you in the showed in one of the figures, like The it showed in one of the figures, like overall, there is an increase in the firing rate of neurons in the late part of the signal, which I think is kind of consistent with this sort of gain change idea. There's no feature selectivity in that pool, so it's just kind of uniform for everybody. I don't think it's uniform for everybody because the actual response pattern changes from the feed forward to the feedback. So there is definitely a change in the selectivity as well with time. With time. And the decoder that you're extracting rotates or something. Yeah, exactly. Yeah. So I think, but then the question is that, like, if I did see something going up or down with respect to certain selectivity, how does that influence building the next model? That's where I think we kind of get stuck with sort of this individual cell level questions, because I think it's interesting to look at, but then. Interesting to look at, but then to formulate the next step, sometimes it's difficult. But I think some of these analyses are useful because they can provide some insight into like maybe we need to train the network with certain kind of images more than others or something like that. But I'm also recording in PFC. So the other kind of analysis that I'm doing is like, if you regress PFC activity out of IT activity, what kind of features go missing or what type of signals or what type of Or, what type of signals, or what type of image attributes can you not decode for across time, things like that? So, that might also start providing some of these kind of insights. But yeah, thank you. Thanks. All right. Tatiana. Yeah, what you were saying raised this question in my head. So, if you train, Had so if you took a standard fit-forward architecture, but during training you overexposed it to difficult images, yeah, likely it would learn a different representation. So, in that case, would you get a good performance on these images? And can you explain neural activity in purely feed-forward fashion with that network? Do you need to invoke recurrence? I think that's a very good question. So, you're basically saying that if you train up the images so that they already perform very well for the challenge image. Perform very well for the challenge images, then will they perform better for the neural representation? I think I have not done that analysis, but I think my guess is that yes, they will. And that creates a problem in interpreting the first half of the results because without then the direct causal perturbation, then we don't really have evidence for recurrence anymore. I mean, we kind of already saw that in an interesting way in the networks, which is that we started observing that the deeper networks that were already so. That were already. So it started from AlexNet, but then, you know, ResNet came up, and other networks came up that were far more accurate than AlexNet. And those networks and their penultimate layers were able to explain the late half of the responses better than the AlexNet. So that already kind of suggested that if you can improve performance, you can kind of get to the neural response explainability. And it's just a matter of like, where does it stay more consistent with? Where does it stay more consistent with neuroscience and where does it just become, you know, how to best make the networks better? But I guess it maybe sounds like when you say you can add more layers to better explain your awake neural responses, then maybe these extra layers serve like it's kind of unrolled recurrently. Exactly. That's the point that we made, which is that the deeper networks can be thought of as just an approximation of an unrolled recurrent neural network. It's like the brain solves it more efficiently. Work. It's like the brain solves it more efficiently than maybe computer vision folks. And so I think that was kind of the argument. And then another interesting thing is that if you're trying, so there are issues with training recurrent neural networks. And if you know that there is a unrolled, like very deep network that can actually solve the problem, you can use interesting like model distillation techniques and other stuff to actually train recurrent neural networks more efficiently rather than relying on something else. Rather than relying on something else. So, yeah, there are other benefits of that too. But I think I totally agree with your intuition. That I think if you train on challenge images, but maybe there will be like, you know, some problems with explaining the feed forward pass at that time, because I think what we also observed is that the deeper layers, which have higher predictivity for later responses, were lower predictive of the earlier responses. So that's where I think the So that's where I think the connection with biology falls apart. So then we falsify the models anyways. And I don't know. But yeah, great question. Thanks. Looks like we got one more from Sarah, and then maybe we'll go to break. Go ahead. I think you're unmuted, Sarah. Sorry. If I can follow up on Christina and Tatiana's comment. Tatiana is mostly. You can always think of any recurrent network as a deep network, right? You just map the network onto itself and every layer corresponds to the next time step. But the problem is then how deep does a network have to be, which is related to the question of how long is the memory of the dynamics in the recurrent network. And as you make that. Longer and longer, then you get into these training problems of vanishing gradients. You get into the same training problems with the very deep networks that you do with a recurrent network that is not sufficiently attenuated in the memory of its dynamics. So, what is the depth of the Markov process that you're trying to represent? So, I don't see that thinking of it as a deep, as a layer network solves any of the Network solves any of the technical problems of recurrence. And I also don't see how you get rid of the temporal problem because you insist that you're going to model a very fast process, which is essentially feedforward, yet you add recurrence, but you still argue that you stay within the time scale of 100 of milliseconds, while any kind of recurrence, even if it was just a local recurrence. Just a local recurrent, like Christina was saying, not feedback and just local, just lateral connectivity within a given area. It already uses one more cycle of neural processing. So how do you argue that this cycle reflects only feed forward, but stays as a very rapid process? I don't think I was arguing that it stays like completely feed forward, but the question is that within the type of data that was collected. Within the type of data that was collected, within the type of behavior that was done, it is possible that that little bit of recurrence that was being added to the feed forward pass is not the main thing that we are after. I think that was kind of the rationale. It's like, so should we spend all of our time building recurrent neural networks? Whereas if the recurrence only adds like two percent change in behavior, for example, I think that's kind of like was the argument. I don't think. Like, what was the argument? I don't think realistically, an argument can be made that even for I think like 70 millisecond recurrence cannot play a role because I think even within IT, things are going back and forth. And it's very hard to make that argument. I think you're right. But it was mostly even now, like when we think of like training the monkey versus not training the monkey, like if you train a monkey, your IT features change. If you don't train the monkey, your IT features change. If you don't train the monkey, your ID features are slightly different. But what we find is that 80% of the variance of the trained and untrained monkey are exact, they explain 80% of the variance between trained and untrained monkey. So now the question is, if you're trying to build a first pass model of the system, would you rather, you know, rely on that 20% idiosyncrasy or you want to first get that big chunk of meat of the 80%? I think that was kind of our thought process. You know, part brusques. Right. Well, thanks, everybody, for the discussion, and thanks, Ko, for fielding everyone's questions and for a nice talk. Thanks. So I'll just say briefly, and then we'll go to a short break. So the QA session.