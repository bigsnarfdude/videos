Of single-cell unsupervised multimodal methods. I'll be monitoring the chat. I forgot to ask this to Susan earlier, otherwise, I would have known her preference. But Mike and I, I confirmed with Mike that he would prefer me interrupt him. So if you post questions in the chat box, I will monitor that closely and try to ask him in real time as he's discussing the individual topics. All right, the floor is yours, Mike. Great. Thanks, Stephanie. And thanks to the organizers. This has been a And thanks to the organizers, this has been a really amazing session, and I'm sure we'll do it again in Banff soon. So I'm much newer to this field of multimodal methods or multi-omics methods than the majority of the people here. And I'm also newer to single cell than the majority of the people here. So that's why one reason why I'm happy to take points of discussion. Points of discussion as we go because I think people might have interesting feedback. So we've been interested in my lab at unsupervised multimodal methods for, I'd say, maybe three years now is when we kind of really started focusing. So we're interested in unsupervised methods where we're looking at common variation over samples from two or more assays or data modalities. I will say, I will say, I've given this talk, something like this talk, about three times now to groups that are biologists or computational biologists or geneticists. And I'll point out that it's very rare that the non-methods developers people are aware of these methods. And I ask, typically ask, it doesn't work for this audience, but I typically ask, raise your hand if you've used a multimodal or multi-omics method before for data. Method before for data integration, and usually one or no people raise their hands. So people are familiar with overlapping peaks and genes, but not with these unsupervised methods I'll talk about today. So mostly we've been running existing methods. We haven't created a new method in our lab yet. So we're just kind of looking at what is out there because there's a lot. Because there's a lot. And we've, you know, when I first got into this, I didn't know much. And so the methods that I first looked at and we were able to run and evaluate easily were the sparse MCCA method from in the PMA package and the MOFA methods. The kind of data sets we've been looking at, we started looking at the PICREL data, so that's accessibility and RNA, and I believe a couple more. And I believe a couple more. We've been looking at a data set from intestine of Crohn's disease patients at UNC. We looked at the TCGA, obviously, and we focused on the breast cancer data set. That's a great public resource. We've also been running these in collaboration with Yun Li's lab on multi-omics data sets from TopMed, including Mesa. And now, so in the past year, So, in the past year, looking at some single-cell data sets. So, I'll talk about the single-cell data sets today. And typically, we're kind of focused on RNA, so, gene expression, chromatin accessibility, maybe also methylation, microRNA. These are kind of examples of the assays. And we did publish last year an evaluation paper, kind of our first look at how we might think about evaluating these methods. So, that's in Briefings in Bioinformatics. So, first, I'll talk about Bioinformatics. So, first, I'll talk about that, although that was on bulk data, just give a framework for how we thought about it. And then I'll show the MOFA plus results that we've been looking at on SNARESeq and on the data set, one of the three data sets for this meeting. So, you know, when I motivate, why are we interested in multimodal methods? For me, one of the really motivating reasons is it comes back to like, Comes back to like this issue of there being unwanted or technical variation in data. So we have a single assay, we have like the primary signal we're interested in, and there's also other variability that is not desirable. And one very interesting thing is that that may to some degree drop out of a multimodal analysis because to the degree that you can To the degree that you can randomize things like library preparation, they won't be shared. It's not true across the board. So, like tissue extraction, if you think about G-TEX, so if you're performing multiple assays on a sample, the issues with the extraction, how long the sample, degradation, that's gonna be confounded across all the assays. And also, something like study phase. And also, something like study phase. If a study goes on for a decade, that's obviously going to be confounded across the assays. But things like library prep, you could ideally randomize. Okay, so one thing we did was we modeled on this GitHub repository called Awesome Single Cell, which is from Sean Davis and others have contributed. We made something called Awesome Multi-Omics. So it's just a list where anyone can contribute. Where anyone can contribute of many, but not all, multi-omics methods. So, a couple things. This is before I started chatting with Susan Holmes. Maybe we should have called this multimodal, but now it's too late. People have like linked to this, even in a, there's a paper that links to this GitHub repo. But if you see something missing, you're free to add it. So you can take a pull request, and if you really want to work on it, I could. And if you really want to work on it, I could easily add you as an owner. So we have software packages. There are a lot. There are an ever-growing number of software packages. So if it's blue, that's a link to the software package. If it's bolded, it means that there's not like a software package that was associated with the paper. Yeah, so it keeps going on and on. We've tried to kind of link to literature in the field, so like ecology and So, like ecology and chemometrics, I've been talking about this for a long time. And then we also have some like review papers, and then there's a bit at the end about single cell, but it's this is actually not single cell focused. Okay, so go ahead and check that out and add something. If I missed your paper, I'm sorry. So, you know, this was two years ago. We thought, how can we evaluate some of these multimodal methods? And as everyone here knows, we have As everyone here knows, we have potentially multiple data types and multiple sets of factors or weights that are estimated by an unsupervised method, which is trying to find some latent space. And for this first paper, we just looked at the first set of weights. And I think we'll get into that in the second part. About it's difficult to actually go further down the list. So there comes a registration problem like Susan Holmes. Problem like Susan Holmes was illustrating quite well. We compared three methods. Had I known what I know now, we probably should have added a lot more. So these were the three that we looked at was the CCA extension from Witten and Tipshirani. AGIVE is basically a double SVD, and that's from a group at UNC. So I more knowledge about Jive and Ajive because I talked to Steve Marin and then MOFA. And then MOFA was kind of a different class than CCA. So we were interested in these different classes. So definitely missing a lot of great methods. And maybe we should go back and readdress this. So one thing I want to mention, throughout every project, I found that pre-processing is very important. This has already been discussed. So I have this simulation where I spike in an outlier. In an outlier for three out of a thousand features, and it's in two of the data matrices, and it's just one sample. And that will drive the top latent factor. So this is CCA, sparse CCA. But I think many methods are, especially linear methods, imagine PCA, will be sensitive to there being just a single. To there being just a single sample that differs from the others. And outliers, so one, like you will immediately just detect outliers as your top factors. And the other thing is scaling and transformation really matter. So just be very careful. Yep. Quick question. Do you know why our jive was removed from CRAN? I don't. There's a lot of development there. There's a ton of development on jives. Steve Marin. On Jives. Steve Marin is constantly renewing this method. And they have even a new method called Divas, which is, I think, very interesting and probably more interesting than the previous methods. But I don't think that's pre-printed yet. But that's from Jan Honig and Marin, also. Okay. So So, we tried to come up with some like distinctions between different methods. So, there are methods which are classifying into discrete groups. We didn't focus on that. We focused on methods that kind of find common variations. So, more like factor analysis, more like CCA. And we came up with this way of plotting the solution. The solution to these methods where we plot the contribution from multiple assays. So I'll give a picture of that. But it's basically especially appropriate for CCA, maybe less appropriate for other methods. But with CCA, we have weights for each feature. We multiply the feature by the weight, and then we get a contribution from that assay to the common space. And so a picture of that would be: let's say assay one is RNA, assay two is attack. Assay two is attack. We multiply the original data matrix by this weight vector, like thinking like a CCA weight vector, and we get what we call this contribution plot. We hadn't seen this kind of formally described. Maybe someone had already described it and we missed it. But basically, many of the methods are seeking to maximize this correlation or likelihood that the data falls, you know, that there's some shared information. And ideally, you would be. And ideally, you could be capturing some biological variation that exists in this reduced dimension across the two assays. One thing it's also very good for is finding sample swaps. So, for example, if you, one of the latent factors, if you have a big study would be sex. So like you have samples with XX and XY for humans potentially. And one thing you can, we found is sample swaps where RNA says XY and says XY and attack says XX. And it's typically, if you dive deep, it's a mislabeled sample. So one thing we noticed was that overfitting was definitely possible. So if you here we create null data and we ran CCA on piece on top PCs. And as we increase the number of top PCs from two assays, we get this arbitrarily high correlation, although the two assays Correlation, although the two assays are totally independent. So you can produce high correlations despite there not being a relationship between data modalities. So if we do find a high correlation in this plot, which is the objective for many of the methods, is that real? Will it generalize? And we used a technique that was already existing in the literature for a long time. I've actually now compiled a much longer list of cases where people. Of cases where people assessed generalization of the solution, but for example, this is just a subset of three. You can do leave one-out cross-validation, you can do training and validation sets or just K-fold cross-validation. And yeah, so I've actually started compiling a little bit more systematic literature on this for this meeting. And it seems like I think 1993 was the first. Like, I think 1993 was the first example I could see for a leave-one-out cross-validation for CCA. If you go further back and you don't talk about integration but just single matrices, there was a paper from Svante-Vold from 1978, cross-validation, the number of components. So like what's the like the rank of in a PCA? And he suggested to perform cross-validation and try to. And try to predict the held-out samples. So we did just that. We tried to perform cross-validation. And we just did these, you know, like I mentioned, we just did these three methods. Obviously, there's many more that we could have tried to put in here, but the idea is we're learning which features contribute to the common space, and we're projecting test samples, so held out samples into that space, and then assessing if the correlation. If the correlation is realized in the samples that were not used for training the model. So, for example, if we permute one of the assays, so let's say we permute attack and we look at whether we recapitulate a high correlation, with this setup where we had done PCs followed by CCA, we had this spurious high correlation. You can see in the cross-validated analysis that there is no relationship. And we expect that because we'd And that we expect that because we permuted the sample. So, when there's a high correlation and it drops down in the cross-validation, we worry about overfitting. Other methods which included internally a regularization, so sparse MCCA has a regularization embedded in it to try to avoid this, they were doing much better. So, they sparse MCCA here on permuted data was not getting a really big correlation, nothing like what we saw with this technology. What we saw with this technique that we kind of engineered to be overfitting. We kind of tried to create an overfitting example. We then ran this on TCGA. We also ran on the PICREL data. And on the left side is this contribution plot on the full data set. On the right side is in the cross-validated. We see there's generally it's holding up. So we're holding up the high correlation and the held up. We're holding up the high correlation in the held-out sample. So that's good for these three methods. What is it? What was it actually finding? This is the breast cancer data from TCGA. You plot estrogen receptor on the y-axis. It's basically finding the driving factor that most influences expression level in breast tumor, which is estrogen receptor status. So this is the ER positive and the ER negative classes. But it's not trying to find distinct classes, but it's just looking for. Distinct classes, but it's just looking for covariance. So that was this multi-omics assessment performed by Sean McCabe. So now thinking about looking at more than just one. So that was just looking at the top factor. So what do we do? We have to think about how to try to look further down the list. So from the MOFA paper, or one of the MOFA papers, they mention Papers. They mention you can train multiple model instances and then look at the Pearson correlation across multiple randomized initializations and try to assess which of the factors that are discovered are kind of consistent, that you see them time and time again, because you might not want to focus on a factor that popped up one time under one initialization, but it doesn't recur. So I think. So, I think that's a very good check, like a self-consistency check. But we also like cross-validation. And I think maybe it's not obvious how to extend it to different methods, but basically it's assessing some objective, whether it's a correlation or a likelihood, data likelihood, of a train model on held-out samples. I think you may have to be clever about how you construct that, but I think it's valuable. But the tricky task, or what's been the most tricky for us. Or, what's been the most tricky for us is this registration problem that Susan just mentioned. So, how do we match factor labels across runs or across folds? And one idea would be to run CCA, like vanilla CCA, on the factors across different folds to try to say factor one here is factor seven here. Of course, and then as Susan was just discussing, there's maybe other even more. Maybe other even more interesting ideas on how to do this registration across different solutions. So, okay, so I'm going to now show our single cell results. So we took the SNARESeeq data and the SC NMT data from the MOFA vignette. So this is, you know, I really want to thank the MOFA and MOFA plus authors for making this really easy to work with. So starting with So starting with the snare seek, that's a data set which has very clear covariation. So we know there are these four cell lines and they're very different. So we're just trying to recover basically there's the dimensions that will separate these four cell lines. And in this case, we followed the recommendation to use the cis topic reduced dimensions for attack. So I think that's 25 dimensions for attack here. For attack here. And this analysis was all performed by Wen Sen Mu, who's a PhD candidate at UNC. All these single-cell analyses I'll show were performed by Wan Sen. And also I'll mention this is a link here, and I've posted these slides. If you click on the link, there's a report which gives a lot more detail about all of these single-cell analyses that we did. So you can, that'll be at the end too. So with this snare-seq data, we get extremely Seek data, we get extremely high consistency here on the y-axis is the cross-validated contribution for RNA, and on the x-axis is what we see in the full data for RNA. And then this is across four different latent factors that we ran. So we get very high consistency, but this is kind of a the two, the four different cell lines are very distinct. So each basically we get one factor which identifies. You'll get one factor which identifies a cell line. However, to get that consistency, we do have to do factor matching. So, in this case, it's very trivial, but it's like, you know, factor one in the full data is factor two for fold number four. So, like, we have to do this matching to make to make the plots make sense. And one thing just I want to mention, in some of our analyses, we found that sometimes the fact We found that sometimes the factors are not necessarily orthogonal, which isn't necessarily indicating a problem depending on the method, but just that's something to note. So if you see an off-diagonal large or large negative or positive value, that means that the factors were not orthogonal to each other. This is what the contribution plots look like when we plot, for example, RNA on the x-axis and attack on the y-axis. And attack on the y-axis. On the left side of this slide is the full data set. On the right side is the cross-validated. And basically, the correlations that we find on the full data set hold up in the cross-validated data set. Okay, so yeah, one thing to notice, there is a bit of discreteness in when we multiply the data by a weight, when we project it, there is a bit of discreteness. And we'll see that again with the SC and MT. Again, with the SC and MT. And this is despite in this case the dimension reduction with LDA. So now moving to the SCNMT. So here are correlations of the full factors. So the factors that we found on the full data set across the cross-validated factors, in this case, for the RNA contribution. So we project the RNA data using the weights learned by most. RNA data using the weights learned by MOFA and then repeat that for fourfold cross-validation in this case. So we can do this, it is basically after matching up the labels. So we match up the labels, we get these pretty consistent factors which are identified. And again, there are some kind of cross-factor correlations here. So, like, this is a negative 0.7 correlation between factor one and factor three. Factor one and factor three. So we see it's this consistency is very good for MOFA plus. So this is for sc RNA for the five latent factors that we fit. And then there's a little bit of less consistency for other of the data modalities. So here the promoter attack was mostly consistent, but then you can see, like, so compare. See, like, so compare. Here's the consistency of the cross-validation in full for RNA. So it's very high, like 0.98, 0.99. And then here it gets a little less for the promoter attack. So this would be the fourth latent factor. And the contribution from promoter attack to the fourth latent factor is a little bit different after doing our best job matching the labels of the factors. The labels of the factors. And then, yeah, so I see a question from Kim on. Absolutely, I think this is, and in the report, Wanson has kind of spelled that out in more detail. I think this is a good way to assess the number of factors. So here you can see one fold actually gets zeros. And we have some thought about why that might happen. But maybe it's best to I'll leave that for the to read over in the Leave that to read over in the report. Yeah, and this is an example from factor five in enhancer methylation. So I'm kind of highlighting the problem cases. If the other cases were actually holding up really well. So then another plot that Wencent produced that I thought was really a good way to look at this was the correlations that are found in the full data set as a heat map and then in the cross-validation. And then in the cross-validated data set. So when we see a deep red color, that indicates that there's a very strong correlation. It's showing up in the full data set. So here, this is the promoter methylation and enhancer methylation. And we find the exact same structure or covariance in the cross-validated analysis. Sometimes, so this is an example of promoter attack and promoter methylation. ATTACK and promoter methylation, we see a correlation of 0.68 in the training in the full data set, but then it looks like it's missing the contribution from attack. So the x-axis here is kind of missing in the cross-validated data. So this is just kind of showing that as you go down the factors from the factor one to the other. The factor one to the others, some of the strongest correlations do hold up in the cross-validated analysis, but some of the other factors, which maybe aren't major contributors to that factor, are kind of, they drop away a little bit. So we see some of the factors hold up very well, and some of the factors are kind of more defined by certain subsets of the data modalities, and the other subsets contribute less, and the correlations go down a little bit. And the correlations go down a little bit when we cross-validate. So, the five minutes left. Yeah, and this is my last slide. So, we find in our hands, these methods that we've tried do not overfit. I do think if you're creating a new method, multimodal data provides a very big search space, and there's lots of opportunities to overfit, especially as you go down the list. So, maybe the top axis of covariance that you find or latent space. That you find, or latent space is very reproducible, but as you go down the list, it's easier to overfit. I like to see trying to find correlations or likelihoods or whatever objective you're seeking, try to see if you get the same objective in held out data. Correlations with linear projections are easy and fairly fast, but it's not necessarily appropriate for all methods. For all methods. And I think it may not be easy to apply this cross-validation framework to other nonlinear methods or methods that aren't kind of training, getting a weight for each feature. Here we have lots of samples and lots of cells in the data sets we've looked at. And I think what may be happening, so with discussion with Wanson, is that lower factors may be driven by pairs of factors or other modalities. Of factors or other modalities, and there may be a sparseness or a rarity that inhibits the way we've tried to evaluate. So maybe our evaluation is not appropriate. And I think how we match or register is a very interesting problem and important. So I just want to, I'll stop here. I want to acknowledge the bulk assessment was by Sean. There's a link here to that paper. The single cell assessment we did for this meeting is here, and there's a link to a report that Wan sent. And there's a link to a report that Wanson has written up. And I'll stop there and take questions. Yay! Question from Dean. Correlation. Do you want to ask Adine or do you want me to just read your comment? You can just read it. It's fine. Correlation can be skewed if data have outliers towards higher values, but high gene expression can represent constitutively expressed genes and can miss impactful genes that have low. Can miss impactful genes that have low expression? How do we develop better statistical metrics, biological importance? Absolutely. Especially on, so like we've been working with this top med data for a while, and we've spent many months dealing with outliers and looking at high variance genes that skew all of the solutions. It doesn't really necessarily matter which method you use. Necessarily matter which method you use. I think some of the things that Susan mentioned are very appropriate. So, like rank transformations maybe are a good approach. Then we get into a problem when we have truly bimodal data. So, like if we have different sexes and we have sex differentially expressed genes by sex, then rank transformation will collapse the bimodality. And so you're losing something real. So, you're losing something real. This is just a really hard problem, and don't have a solution for it yet. Let's see. So, Kim had asked about: could you choose, use this to choose a number of factors as well? You discussed that. Another comment from Kim and Another comment from Kimen. I'm trying to, she says she's trying to implement a similar approach to tune the lasso parameters and mix-o-mix approaches such as SPLS. Yeah, I think so, you know, one thought and I want to emphasize, like also to the MOFA plus developers that are here is I think one problem with our thoughts on evaluation is that it may not be always appropriate. And it may be. And it may be so, just because we don't see correlation or high likelihood in held-out data, it could be a problem with the evaluation framework that it may be hard to, it may be that there's a true relationship that it's detecting, but that it's very hard to match. And it may be one thing that is difficult for us is what if it's split? So, what if on the full data factor five is split across multiple? Is split across multiple latent factors when we do cross-fold or cross-validation. And that's something that it might appear in our evaluation that we lost a high correlation, but it may just be a problem with how we're trying to evaluate. Consistency. Okay, so I'm reading the. Yeah, there's a comment from Debbie D. Yeah. So consistency between dimension reduction approaches doesn't always predict the best method. Um, sparse, uh, sparse approaches which use the same lasso, sparsity will generate similar results. Yeah, so I would like, I don't think that the highest, so what we found is consistency, and we tried to we tried to make that clear in the 2019 paper that we're looking at basically self-consistency. I think it doesn't mean that you've that it's the best method, but I think self-consistency is like one of the tick boxes that we would want for in a method that if it says, In a method, that if it says there's a really strong relationship here, that you can also see that relationship and held out data, it doesn't mean it's the best method. I totally agree with that. It's just like one of the many, one of the many things that you would want for in a good stable method. But we can be consistently wrong. Oh, yes, yeah, yeah. So it could be consistently non-interesting. Yeah, one thing Wolfgang Huber always told me is you can generate a very consistent method. You can generate a very consistent method just by ranking on the feature name. So, like gene one, gene two, gene three. This will be very consistent across folds. And if we reduce it to just the really highly expressed genes, it will be really consistent as well. So there's a trade-off that's so important. It's like, yeah, it's maybe akin to sensitivity specificity. You want both. All right. Well, let's increase consistency a requirement. But isn't consistency a requirement for correctness? I don't know, maybe not. We can kick it to Slack. Let's continue on Slack. All right, let's thank Mike again for a wonderful talk on benchmarking. And next, we are going to move.