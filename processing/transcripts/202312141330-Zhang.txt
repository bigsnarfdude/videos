So, today my topic is called distributed learning on finite Gaussian mixtures. So, this is a joint work with my advisor, Java, which has been published in Journal of Machine Learning Research in 2022. So, let me give you a little bit of background of this project. So, we have talked about finite mixture models a lot, like in this workshop. So, just to introduce a bit of notations that we used. Of notations that we used. So, finite mixture models that are actually a family of distributions. So, we use this curly F to denote a parametric distribution family, such as Gaussian or exponential. And the finite mixture model of this curly F, has its density function being a convex combination of distinct density functions from this parametric distribution family. So, one thing that is unique about the finite Gaussian mixtures is that its parameter is Parameter is a discrete distribution which we call the mixing distribution that put mixing weight WK on its subpopulation parameter theta k. And here this capital K is called the order of the mixture. And for the rest of my talk, we are going to just assume that this order is known. And specifically, by meaning finite Gaussian mixtures, we mean that the distribution We mean that the distribution family F is actually the Gaussian density function, the Gaussian distribution family. And we're going to respectly using this small phi and capital phi to denote the probability that CDF and PDF and CDF of a finite Gaussian mixture. So I wanted to specifically explain why we want to parameterize this model by a discrete distribution rather than a vector. So in this So, in this two-component mixture here, in this particular example, so one may propose to use a vector that put the parameters of the first component followed by the parameters of the second component as vector and then use that vector to parameterize this mixture model. However, this kind of parameterization may lead to unidentifiability issues because if you switch the order of these two components, you ended up Two components, you ended up with two different vectors, but these two vectors leading to the same mixture identity, which means that the model is non-identifiable. But if you use the mixing distribution G, we do not have this issue anymore. So that is one of the major reasons that we want to parameterize this model by discrete distribution rather than the vector. So where does this finite mixture model be used in machine learning? So one of the So, one of the most fundamental applications is in the clustering. So, the density function of a finite mixture model corresponds to the marginal density of this variable x in this latent variable representation. So, then after we fit in a mixture model based on a set of data set, then we can use this maximize the posterior probability rule to assign these observations into one of the capital K groups. Capital K groups. So here in this example, we can see that we divided these observations into one female group and the other one is the male group. And another application is that finite Gaussian mixtures are widely used to approximate any smooth density functions. So regardless of these applications, one of the most fundamental goals for us is to learn or in statistical language is to estimate the mixing distribution G given the set of data. The set of data. So, the specific setting we considered in this paper is called the distributed data storage, where we consider the case that the data sets are too large to be stored on a single facility or they're collected by different agencies and cannot be shared due to privacy considerations. So, in that case, the data sets are stored in different places. And one widely adopted approach in this approach in this setting is called display and conquer where we where the machines that stores this data set they perform standard statistical inference at the first place which is called the local inference procedure and then these inference results will be sent to a central machine for aggregation so by doing this approach we kind of protect the the privacy of the users in a certain way Of the users in a certain way by only sharing the summary statistics, not the raw data. And we also have a low transmission cost by not sharing the raw data. So to be more specifically, we consider that we have capital M data set, local data set, x1, x2, up to xm, and the sample proportion on these machines are denoted as lambda m. And these observations, and these data sets, they are ID observations from the same model as. From the same model, F X theta star. So, in the first step, where we perform the local inference, we will get the parameter estimate theta one hat up to theta m hat. So, if your parameter is in a Euclidean space where it's a vector, so what people would usually do is to just aggregate this local estimate by their like weighted average. That's what people normally do. But under final. Do. But under finite author mixture models, however, our parameter space is formed by discrete distributions with a fixed number of support points. So here in my notation, we have capital K number of support points. So if we still follow what we do in the Euclidean space, we ended up with this mixing distribution G bar. So each one of them has capital K support point and I have M machines in total. That means this G bar has M times K. G bar has m times k is a bulk point, and no longer in the parameter space that I'm considering here. So this g bar would lead to an average mixture, which we denoted as phi x g bar. So from density function point of view, this density function is actually indeed a good estimate for the true mixture, because each one of them is a good estimate for the true density. So is their weighted average. However, this mixing distribution G bar, it is unstable. Distribution g bar, it is unsatisfactory for revealing the latent structures that I should, as I just mentioned, has m times k support point rather than the k groups that I wanted. So this naturally leads to our research problem of how to aggregate these local estimates under finite Gaussian mixture models. So we kind of think of two potential aggregation approaches. So I'm going to use this row as the divergence function. As the divergence function that measures some kind of similarity between two distributions. So, the first approach that we think of is called the barycenter approach. So, this barycenter approach is actually motivated by the average in the Euclidean space. So, geometrically speaking, the mean of a set of points x1 up to xn is a geometric very center of this observation. Of these observations. So, since we're not in a Euclidean space and we are in a distribution space, we can propose an analogy of those very center in the space of the mixing distributions. So, that is our first proposed method, which is just the very center of this local estimate. And then the second approach is what we call the reduction approach. As I just mentioned earlier, this averaging, this average This average mixing distribution, it has the corresponding mixture has actually a good estimate for the true mixture. The only bad thing about this distribution is that it is no longer in the parameter space. So for the second approach, what we propose is that I find a mixing distribution in the desired space to approximate this average mixing distribution g bar. And then this one is. And then this one is the so-called reduction approach. So, what are the connections of these two potential aggregation approaches? We found that when the divergence row is chosen to be the KL divergence of the corresponding mixtures, then these two approaches are essentially the same. However, finding the exact solution, like solving that optimization problem under KL divergence, is computationally intractable. Is computationally intractable. So that's why we did not adopt that approach in our case. And another observation that we have is that the barycenter approach may not be ideal. So I'm going to give you an example here. So let's consider the case where we have two local machines and each one have a two-component mixture. So these two mixtures have exactly the identical subpopulations but opposite mixing weight. Opposite mixing weight. So intuitively, we may expect our aggregated result would have, would keep the same subpopulation parameters by having their corresponding mixing weight being averaged. However, we found that for the very center approach, for some choice of divergence, we may end up with this kind of aggregated result. So where this result keeps the mixing weight on the first machine by having it sub-population. Machine by having its subpopulation parameters changed. So that is only for a particular choice of the row, but one may argue that you may choose a different divergence row to avoid this case. But for the reduction approach, regardless of the choice of the divergence, we will not have this kind of issue. So based on these observations, we ended up with proposing ended up with proposing aggregating using this reduction approach. That is, we propose to aggregate this local estimate by finding a mixing distribution in this mixing in this mixing distribution with k support point to minimize the divergence to this averaging mixing distribution g bar. Then a natural question to ask is that which divergence row should we pick? Divergence row, should we pick? So, some key observations that we had is that divergences are usually hard to compute between mixtures, but they are actually quite easy to compute between Gaussian distributions. So, for example, the widely used KL divergence. Between two Gaussian distributions, it has a closed form, but the KL divergence between two mixtures, they are computationally interactive. So, based on these kind of So, based on these kinds of observations, the divergence that we use is called the composite transportation divergence, which is a byproduct of optimal transport. I'm going to introduce that divergence now. So the composite transportation divergence between two Gaussian mixtures is defined as follows. So we have two mixtures, one with the capital N number of component, and the other one has capital M number. The other one has capital M number of components. And we have this cost function defined on the space of this probability distribution family curly F. So this C is usually chosen to be a divergence on this distribution family F. Then the composite transportation divergence is defined as this optimization problem here. So how do we interpret this divergence? So as I mentioned, it is a byproduct of the optimal transportation. The byproduct of the optimal transportation theory. So the question essentially is saying that so this cost function matters the unit cost of moving mass from a location phi theta n to phi theta m tilde. That is the unit cost. And this pi here matters how much amount of mass that I move from one location to another. And here this constraint, not coupling constraint, basically saying that the Basically, saying that the mass that I moved from location phi theta n must equal to the total mass it has over there. And the other constraint is basically saying that the mass that transported to this location here must equals to the location, must equals to the mass it needs, which is Wm tilde. And then this optimization and this summation term here is that the total cost under this transportation. Under this transportation plan pi, and this divergence corresponding to the minimum transportation cost over all possible transportation plans here. And with this composite transportation divergence, we propose to do the aggregation by minimizing the composite transportation divergence to the average mixture. So since this G bar is given, so I'm going to abuse the notation slightly and then Notation slightly and then denote this whole divergence as T C G. Okay, so we define this estimator as a solution to an optimization problem, and the question is, how do we compute it numerically in practice? So, let's take a look at our numerical optimization problem. So, at the first glance, it seems like our aggregated estimator is a solution to a bi-level. Is a solution to a bi-level optimization problem, meaning that our objective function itself involves another optimization problem. So that's what we mean by a bi-level optimization problem. However, we find that we actually can find a simplified equivalent objective and optimization problem that has a closed form objective function that allows us to use the MM algorithm to minimize this simplified objective function. Objective function. So, how do we do that? So, earlier, our objective function is TCG, right? So, TCG, which is the minimum transportation cost, satisfying two marginal constraints for this transportation plan pi. So, our new objective function here removes the second set of constraints. So, we only have one marginal constraint on this transportation plan pi. So, with So with only one set of marginal constraints, this JCG actually has a closed form. So its optimal transportation plan has its specific form. So if the cost function of if phi n bar is closest to the m's location, then all the mass at that location will be moved over the m's location. So that's the meaning of this divergence. Then we have shown theoretically in our paper that these two optimization problems are essentially the same. And then we can update the mixing distributions by summing up over one marginals of this optimal transportation plan pi star. So what are the advantages of this optimization, new optimization objective? So the first thing is that the star population parameters and the mixing weights. Parameters and the mixing weights can be updated separately. Because in this objective function, the only thing that is involved is the subpopulation parameters of this mixture. So none of the mixing weight is involved. So I can update these theta m's first and then to obtain this optimal transportation plan pi, and then some summarize it over the first margin to obtain the corresponding mixing distribution. Then this gives us the Then this gives us the following efficient MM algorithm. So, before I jump into the details of the MM algorithm for our objective, let me just give a quick overview of what is MM algorithm. So, suppose that you want to minimize this non-convex objective function G. So, if you look at the picture that we know, the minimum is over here, right? The global minimum is here. So, the MM algorithm is essentially. The MM algorithm is essentially an iterative algorithm, so it starts from some initial value. So suppose this algorithm at location t, its candidate position is at xt. So what MM algorithm do is that at this location XT, it finds a surrogate function that satisfies these two conditions. So basically saying that the surrogate function is no smaller than the original objective. Smaller than the original objective, and they coincide at the current location. And at the next iteration, instead of directly minimizing the original objective function g, so it minimizes the surrogate function h, which is this dashed line function here. So then next step, I've got my candidate position x t plus 1 over here. And then in the next iteration, I'm going to construct another surrogate function. Another surrogate function and then minimize this surrogate function again until the algorithm converges. So the advantage of MM or the success of MM algorithm relies heavily on finding this surrogate function, which is usually convex and can be solved very efficiently. So going back to our case where our objective function is given by this here, right? As I mentioned earlier, this objective function has a closed form. has a closed form. So the surrogate function that we found at G at the T step is given by this form here. So the difference of this the surrogate function and the original objective function is that so the transportation plan is evaluated at GT and the locations of this the aggregate make sure they are three parameters that we want to minimize. Then in the next step, we're going to minimize over this measurization function to get the next step estimate of this mixing distribution G. And we claim that for some choices of the cost function, this step can be solved in a closed form. Okay, let me just using some cartoons to realize the concrete MM step that we use. So consider here. So, consider here in a case where we have three machines and each one is going to fit a two-component extra. So, we ended up with these six different sub-populations, estimated sub-populations. So, the MM algorithm would propose some candidate locations. So, this red star and the blue stars are the candidate locations of the aggregated mixture, which has two components. So, in the majorization step for this proposed location, we're going to find Proposed location, we're going to find the optimal transportation plan, which is given in a closed form by this pi star here. So, what does this pi star say? If one component is closest to the m's component, I'm going to move all the mass, right? So, that is essentially doing a clustering step on the space of Gaussian distributions. So, here, since these three components are very close to the red star and those three components are close to the blue star. Components are close to the blue star, so we're going to divide them into two different groups. So the red groups and the blue groups. And then in the next minimization step, for a given transportation plan pi, we are going to update the subpopulation parameters. So each one of them, if you still remember the definition of barycenter, so they are going to look for a barycenter in the space of Gaussian distributions. So if this cost function is chosen to be, for example, the K-L divergence between two Gaussian distributions, then this barycenter has an elliptical form. So what do we, if we translate back into this cartoon here, so in the minimization step, we are going to update the centers of these observations that are assigned to the same group by using what we call the barycenter approach. What we call the barycenter approach. Then we'll have this new candidate locations here. So then, then in the next iteration, we're going to perform the majorization step, which is essentially clustering them into two groups. Because since in this example here, the group assignment does not change, right? So also the update step would not change and the algorithm converges. So that is how the M. So, that is how the MM algorithm works. So, we have in the paper we have also established the algorithm convergence. So, if our cost function is continuous in both argument and satisfies certain conditions, we can actually show that our algorithm converges. So, just to summarize our full recipe for the distributed learning of finite Gaussian mixtures. Distributed learning of finite Gaussian mixtures. So these are the concrete steps if you have you wanted to fit a mixture model in the distributed learning fashion on your own. So the first step is to use some traditional inference method to obtain the local estimate GM hat. And then in the second step, you're going to form a plane average. And in the third step, you're going to choose a composite transportation divergence, which is essentially saying that you have to pay. That saying that you have to pick the divergence here, the cost function C, and then we propose to use the KL divergence. And then later on, for the last step, you're going to use the M algorithm to find the aggregated estimator, which we call the G bar R. So essentially, you're going to do a clustering on the space of Gaussian distributions. So, what is the statistical performance of this aggregated estimator G bar R? Estimator cheap R. So we have under some general conditions, if the data are ID observations from the same model, and the sample proportion has a non-zero limit, and the cost function satisfy this triangular inequality locally, only locally, then we're able to show that this aggregate estimator is root and consistent. So let me just show you some numerical results. So in our simulation, So in our simulation, to avoid human bias, we actually generate the parameter values of the mixtures where we generate samples from. So we generate the parameter values of 100 random Gaussian mixtures in a 50-dimensional space with five components. And then we pick the different degree of overlapping, which is basically saying how close the two subpopulations are in the mixture. Are in the mixture. So if the degree of overlapping is large, meaning that the mixture model is more and more difficult to learn. So we pick this value to be ranging from 1% to 10%. And the total sample size that we pick here is about a million. And then we also set different number of machines to be all the way up to 64. And what are the estimators for comparison in our work? For comparison in our work, the first one is global estimator. So, this estimator is based on the full data set if you're able to get access to this full data set on a single machine. And then the second one is what we call the median, which is the best local estimator in practice. So this estimator actually essentially only based on one portion of this data set. And the third one is our proposed reduction approach. And then, third, Approach. And then, third, this fourth one is called the KL averaging, which is choosing that divergence to be the KL divergence, the computationally intractable one that I mentioned earlier. And the last one is some related work in the reference. So these last two are existing methods in the literature. So what kind of result that we get? So the performance criteria which we pick here is the wet system. Here is the what's the distance between the mixing distributions from the estimated mixing distribution to the true mixing distribution. So, in this figure here, the second box is our proposed method. So, let's take a look at these graphs. From left to right, the maximum degree for overlapping increases, and then leading to a larger value of this west system distance for all maximum. Western distance for all methods, which validates our intuition that as the degree of overlapping becomes larger, all the methods become, the mixed model becomes more and more difficult to learn. And then the performance of all estimators degrades. And then within each picture here on x-axis, we have different number of local machines. So as you can see, regardless of the number of local machines, our proposal. Number of local machines, our proposed method always have almost identical performance to the global estimator, which is the golden standard. And it can beat the rest of the method in the literature. So that is the West and distance between the estimated mixing distribution to the true mixture. And another performance criteria we pick is the adjusted rent index. So this performance criteria is used to essentially compare the The agreedness of two sets of clustering results. So, for us, we clustering the training data set using the two mixing distribution and the aggregated mixing distribution, and then compare the result of the clustering using the adjusted random index. So, the larger of the value, the better the performance of the estimator. And the similar story holds as the previous page where our proposed map. Previous page, where our proposed method has a very comparable performance to the global ST. And for the distributed learning setting, we also compare the computational time to see whether this kind of split and conquer approach can indeed save the computational time compared to the global method. So as you can see, all of the split and conquer based approaches, they can save computational time compared to the global estimator. And from a computational perspective, From a computational perspective, this sub-sampling-based core set approach has the best, the slowest time, but since its statistical performance is not as good as ours, so regardless of the computational time, it's not a very good choice. And we have a bunch of real data analysis in our paper, but for the sake of time, I'm going to only I'm going to only present the result that we had on this NICE handwriting data set. So, what we do in this example is that we have these handwritten digits, these are images, we pass it into a pre-trained convolutional neural network to extract the lower dimensional representation of these images, which is in a 50-dimensional vector space. And we're going to use this burn-conquer approach where we have 10. We have 10 local machines, and on each one, we choose the number of components to k because we conjecture that each digit will have its own subpopulation. So these are the results that we had. So on the left panel here, we have the adjusted random index. So this adjusted random index is computed between the clustering results and the true label of those images. Label of those images. And on the left panel here, we have the adjusted render index on the training data set. And on the right-hand side, here we have the adjusted render index on the test data set. So one interesting message that we want to say here is that, so in this real data set where the Gaussian mixture assumption might not be true, in this case, all the splan-conquer-based method, so these three span-conquer. So these three then learning-based methods, they can actually beat the performance of the global estimator. Okay, so just a summary of our contribution in this work. So in this work, we have developed novel aggregation methods for span-conquer learning of finite mixture models. So it does not necessarily be a finite Gaussian mixture and can be mixture on the parametric distribution family in general. So we have theoretically shown that. So we have theoretically showed that the aggregated estimator is computationally efficient and it is root and consistent when the order of the mixture is known. And we have also empirically demonstrated the superior performance of our proposed method. So this QR code here is actually a link to our paper. So just scan it if you're interested. So that's all for my talk. Thank you.