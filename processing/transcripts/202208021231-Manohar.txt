Great. So, first, I'd like to thank you for the introduction and the invitation to come and speak at this workshop. I've seen several very cool talks on turbulence modeling and many different applications, including the previous one. So, I will be talking about data-driven prediction of partially observed multi-scale Partially observed multi-scale dynamics. This is actually part of my postdoc work at Caltech, but since then I am now in the mechanical engineering department at University of Washington as assistant professor. I thought I would start with the prediction problem, and then I will talk about some of my thrusts in optimal sensing and optimal sensor placement. But this first part nicely motivates the need for optimizing the measurements you are taking of your system for data-driven modeling. Okay. Oh, sorry for the text. But that says the goal that we're looking for is a real-time interpretable machine learning with guarantees, with performance guarantees. With performance guarantees, which is necessary in a lot of engineering tasks, including modeling, data-driven control, and sensing. So the idea of offline learning to use for online decision making is often limited by the observations we have at hand. So, one of the thrusts of my work is optimizing sensor placements and Sensor placements and sensing for these different decision-making tasks. And it's often a problem that is not usually considered while designing these algorithms. And the way my work approaches that is to use data from high-fidelity simulation or experiments to learn relevant features that are specifically relevant to the task at hand, including POD, dynamic modal decomposition. Dynamic modal decompositions and autoencoders. So these features will differ based on the task at hand. And I would like to, the goal of my work is to bring in the sensing component into these different decision-making tasks. So I will start with the problem of forecasting these very high-dimensional complex systems. So we assume we're given data from We're given data from this multi-scale dynamical system where we only observe the x, and the x variable is coupled in some unknown way to the fast dynamics given by y. And for now, we assume that there is scale separation in the system, meaning that the characteristic time scale epsilon is very small. But we'll also look at the case where that's not the case. We also assume we have data. We also assume we have data of this response variable f, scalar-valued response variable or observable, alongside the observations of X. The challenge with forecasting here is that, of course, this is partially observed, meaning that subsequent snapshots of X are not necessarily Markovian, and the interactions with the Y variables are unknown. Y variables are unknown. So, the approach that we use is called kernel analog forecasting, which is an established data-driven technique that uses features of a kernel operator. The nice thing about this algorithm is that it has an operator-theoretic interpretation with respect to the underlying dynamical system by means of the Koopman operator, despite not knowing what those dynamics are. Dynamics are. It also gives us a way of having guarantees on your model. It gives us a way of predicting the uncertainty associated with your forecast. And what we do is analyze this KAF method, which was introduced by Alexander and Dinakis with respect to many canonical classes of multi-scale dynamics that are prevalent in fluids, climate, and biology. So, an example would be trying Would be trying to predict the El Nino index, which is a very important indicator in global climate patterns, based on satellite measurements of the sea surface temperature. So this is a very chaotic system, and tackling it is one of the most important problems in the climate modeling community. So we use this as a So, we use this as a motivating example and consider classes of multi-scale test problems that kind of resemble these dynamics and analyze the KAF forecast and interpretation with respect to these multi-scale dynamics. So, the way we get this operator theoretic interpretation is by optimizing over. Is by optimizing over a function space, which is actually a Hilbert subspace of an RKHS, a reproducing kernel Hilbert space. But the main idea is that we're approximating this Koopman operator, which gives us some notion of a ground truth forecast. It acts on these responses or observables F of a system, such as, for example, the Ni√±o index, by composing it with the flow map of the underlying dynamical system. Of the underlying dynamical system, where omega is the state space, but remember, omega consists of both x and the fast dynamics y. However, we're only observing x via some mild assumptions on this observation map, namely that it is smooth. So the action of this Koopman operator on L2 observables, square and integrable observables, advances the observables forward in time. And this method has a statistical consistency result that it converges to the conditional expectation of the Koopman operator acting on F, the ground truth forecast, in other words, given the observations in the infinite data limit as the number of features we use goes to infinity. So this So, this result is actually in line with a lot of classical time series analysis techniques, except this one applies to nonlinear dynamics, whereas things like ARIMA and vector autoregressive models are often linear by construction. And the nice thing about this is that it gives you this interpretation, even if the flow map phi. map phi tau is non-linear okay please interrupt me with questions if there are any so here are the details of the data-driven computation so the phi j and lambda j which are embedded within this coefficient term are eigenpairs of a kernel operator that we construct from the data using a variable band Using a variable bandwidth RBF kernel. So this is a variant of the squared exponential kernel acting on the X data points. The only difference is that there's this data dependent bandwidth scaling that helps us account for variations in the sampling density of the data. And the goal is to construct a compact diffusion type operator G that basically gives us Basically, it gives us the machinery of an RKHS to work with, namely that we get a basis of continuous functions in a subspace of an RKHS. What we need to know is that once we compute these kernel eigenfunctions, the RKHS basis functions are computed using Niestrom extensions of these phijs so that we can evaluate them pointwise at any new initial condition x. Initial condition x. These coefficients, which are this projection coefficients onto utau f, are approximated entirely from the data using this inner product. So you just take the inner product of the eigenfunctions with the time shifted time shifted observable corresponding to how far ahead you want to forecast this observable. Using this computation, you can also get the uncertainty prediction for basically for free by predicting the conditional bearings of the forecast. And this can be done without recomputing the features is the main idea. So you basically treat the main idea is to treat U tau F, which is the ground truth data, minus the KAF forecast, which is a conditional mean, and that quantity squared as the And that quantity squared as the new observable. And again, you follow the same data-driven approximation where you calculate the inner product with these kernel eigenfunctions to evaluate the forecast at a new initial condition x. And remember, the x are these partially observed, partial observations of a larger underlying dynamical system. So this can be useful for a lot of things such as close. Useful for a lot of things such as closure modeling and given partial observations or a coarse grain model of a dynamical system. So we consider several different classes of dynamical systems, but I will highlight multi-scale averaging. We also have an example where the X variables homogenize to a stochastic process and we can still predict. Stochastic process, and we can still predict for the stochastic dynamical system. So, we use this canonical chaotic model, multi-dimensional model, which is, I believe, 81-dimensional. And it has these k slow variables that are coupled in this way to each other and also interact with the y variables, which are the fast variables, using this averaging term. An illustration of the trajectories are shown here, where this is the first, this is the first, the gray is the first fast variable, and the green is the first slow variable for the periodic and quasi-periodic regimes controlled by this parameter here. In the small epsilon limit, this system exhibits this closure model in the x variable. The X variable, where this closure term is unknown. And typically, what is done is to use this data-driven model, this reduced order model, and take samples of this term in order to build a data-driven closure and simulate this as the reduced order model. But what we have is a purely data-driven model based only on observations of X with no knowledge of the dynamics. So we'll compare those two. So we'll compare those two for these three different regimes of the chaotic Lorenz 96 or the Lorenz 96. So here shown on the left is the KAF forecast with increasing tau for a long-term forecast, long lead time, long time horizon forecasts. And you can see that for the periodic regime, For the periodic regime, the KAF forecast remains in phase for both periodic and quasi-periodic regimes. And the uncertainty prediction occurs at the extrema of the trajectory. In the chaotic regimes, it very quickly predicts uncertainty, growing uncertainty, and this is because this, which roughly occurs at the Lyapunov timescale of this system. And eventually, And eventually it converges to a constant, meaning we have complete loss of predictability at around five or six Lyapunov timescales of the system. In comparison, a reduced order model constructed with the GPR closure very quickly goes out of phase in the simpler periodic and quasi-periodic regimes. And it also accesses more information. And you can see. And you can see that it is roughly the same performance as the KAF forecast in the chaotic regime. However, it does not give any indication of the uncertainty associated with the model or the trustworthiness of the forecast. And so, this is in the so this class of systems which exhibit averaging the conditional. The conditional expectation, the expectation goes away because there's a deterministic closure in terms of X. However, for multi-scale homogenized system where the X dynamics homogenize to a stochastic process, then the forecast converges to the mean of the stochastic process and the moments and the higher order moments of the stochastic process. We also consider the case where we have non-Markovian observations, namely, we observe too little information to highlight the difference between classical time series analysis methods. We compared to, we only observe X1 in the periodic Lorenz model. This is kind of like observing only one component of a simple harmonic oscillator and trying to predict. Simple harmonic oscillator and trying to predict the other. The Lorentz forecasting method simply reports the time-shifted response value closest to the initial condition. And for chaotic systems, this method is out of favor because it was discontinuous with respect to the initial condition and is non-physical. However, this method, KEF, is continuous by the construction of the features, and this is illustrated. Of the features, and this is illustrated in these two very close together initial conditions. KAF reports high uncertainty because this is near the midpoint of the attractor and you're not seeing the other coordinates. However, Lorenz's method is clearly the analog forecasting method clearly diverges for these two very close together initial conditions. So, in a sense, this gives you. In a sense, this gives you an idea of the robustness of this model and the interpretability of these types of models. I also want to point out that the observations must be carefully chosen to extract the most information out of the model. For example, in the Lorentz 96 model, you have to observe the first nine, the nine slow variables in order to predict one of the slow variables. slow variables. So this motivates some of this is connected to this problem is connected to some of my work on optimal sensor placement. So going back to this high level overview, in a sense, these observations must be carefully chosen for the task at hand, including forecasting. So we will look at optimal sensor placement for high-dimensional High-dimensional reconstruction problems that use a method related to linear dimensionality reduction techniques like Gappy POD. So we look at a simple linear reconstruction, linear inverse problem where we want to reconstruct a high-dimensional state from p sparse measurements stored in y. For now, we'll consider just point measurements with zero. Point measurements with zero mean IID noise on the measurements. So, this corresponds to taking spatial point measurements of a fluid flow, for example. We assume that X is low dimensional in some basis of features. In this case, we choose the basis to be the POD modes of the flow. And even we use this dimensionality reduction step to optimize. Dimensionality reduction step to optimize sensor placements for this reconstruction task. And we find out that this task is very sensitive to the choice of measurements. The design criteria we choose to optimize to design the measurement matrix C is the inverse error covariance. So we want to minimize the error covariance of the POD coefficient estimate. And in order to do that, we And in order to do that, we maximize the log determinant of this product of the measurement and the modes. So this is an NP-hard brute force search with N choose P possible placements to search over. So for a state space of 25 variables, which is very small, already the brute force search consists of nearly 500. Consists of nearly 500,000 different possibilities for C. What we want to do is get the C to approximate the C, C matrix that lies at the tail end of this distribution of all possible placements. So the maximizer of this distribution. And we formulate a greedy solution of this problem by reducing it to a matrix operation. Reducing it to a matrix optimization problem. And this is actually work with Bing Brunton and Steve Brunton from earlier, where we use the pivoted QR factorization of the POD modes in order to get these optimal sensors. So here's the application of this method to many high-dimensional reconstruction problems in imaging. Problems in imaging, fluid flow, and manufacturing data sets. So, already with the state space of 1000, this is the brute force optimization is something like 10 to the 200 different possibilities to search for. So, computationally intractable. But with the QR greedy method, the optimal place near optimal placements can be obtained with order nr squared observations, where r is the Where R is the number of POD modes retained. So it's typically small. And we can see that the reconstruction with optimal sensors, which are clustered around important features, is much more accurate than with a random choice of sensors. And here, this manufacturing data set, we would like to predict the shim sizes required for a new. Shim sizes required for a new aircraft within some machining tolerance. And we're able to predict 99% of these gaps using just 5% of these optimized measurements. Whereas with random measurements, we would need approximately three to four times that many measurements for this process. So this method is actually quite flexible to different choices of bases. Choices of bases, different choices of constraints on the sensors, and it's extremely efficient because of the dimensionality reduction step involved. And there are also results showing that these sensors are good for use with, these sensor measurements are optimal in a sense for use with nonlinear embeddings like autoencoders and variational autoencoders that people mentioned earlier in the workshop. Um, in the workshop. Um, so it's oftentimes uh very practical to impose spatial constraints on where your sensors can be placed. For example, here we can incorporate spatial constraints in our model and place sensors closer to the coastlines for easy maintenance and monitoring of these sensors. We can also generalize to optimal accuracy. To optimal actuator placement for optimal control. And in this case, the choices of bases are the balanced POD direct and adjoint modes of the flow. And we can see that the choices are very close to the optimal sensor and actuator placements are identical to the optimal placements for this low. So I will. So, I will end on the note of our ongoing directions. So, we're adapting this technique for adaptive sensor placement and sensor failure detection events. We have ongoing directions on optimizing sensor placements for Bayesian inverse problems such as atmospheric source diffusion. Source diffusion, sensing for non-linear embeddings, such as autoencoders, multimodal measurements where you're measuring more than one modality at a given sensor. So the idea is that we want to optimize the sensors for each of these tasks. And also optimal sensing for digital twins, which are these virtual representations of a product lifecycle. And we have a collaboration with And we have a collaboration with Idaho National Lab on optimizing sensor placements in a nuclear field rod prototype. And there are actually very complex constraints on where you can place the sensors, how far a sensor should be from other sensors nearby. So these require adaptations of these methods and the method is quite flexible to the choice of bases. Bases or features that we learn from our model and constraints. So, this is ongoing work. I will end with a really cool applied project where all of these challenges combine, which is we are studying a manual composite layup and we are integrating. And we are integrating position capture and pressure sensing data to evaluate the level of safety involved and ergonomic risk involved when workers perform these complex layup tasks in manufacturing. So this problem is challenging even at the data collection phase because oftentimes sensors occlude each other. For example, these pressure sensing gloves occlude the Loves occlude the motion sensoring, motion tracking of the cameras. And so, when these sensor failure events occur, we want to know how to limit or optimize sensors to only have high fidelity measurements at a certain few certain locations. And this is also, there's also a classification problem involved, and where we're trying to classify the level. Where we're trying to classify the level of ergonomic risk. And we also want to study the features involved to hopefully eventually automate these tasks or passively automate these tasks. So far, due to the geometries and the scales involved of the parts that these workers are laying up, this process has resisted automation. So it would be really interesting to use things like reinforcement learning and these time series analysis. And these time series analysis techniques to this kind of very complex, noisy, multimodal data set. So I'll stop here and ask if there are any questions. Well, thank you, Professor Manohar. So floor is now open for questions. Let's see if there is anybody from Zoom or from the okay. One question from the audience here. Thank you for a very nice presentation. I think that the work on optimal I think the work on optimal sensor placement is very interesting and relevant. We have been trying in that context to use explainability methods of deep learning, like Chapley values, for example. And it seems that in certain cases, it actually can show potential. Have you considered this approach? Can you repeat the last part? Yeah, if you have considered an approach based on explainability, for example, Shapley values or something in that direction of the deep learning methods. Deep learning methods for the sensor placement. Okay, so this is ongoing work. So I'm not sure what you mean by explainability, but usually we use different information theoretic criteria, like even this error covariance or de-optimal criteria, as it's known in the literature. So, in a way, we are choosing certain concrete measures of Concrete measures of how much each sensor explains the dynamics. For deep learning methods, what we have thought about doing is constructing a linear operator that maps the inputs and the outputs, output layer of the neural network, and trying to use that to optimize the sensor placements instead of directly. Instead of directly optimizing, for example, the sparsity level of the input layers, for example. So that's one way we are hoping to tackle it is by using linear approximations of the input-output map of the neural network weights. That sounds very interesting, and I think it can be helpful. Maybe it could be interesting to look also at this experimentability because then you would leverage the non-linearity. You wouldn't have to. You wouldn't have to, but that's something maybe that we can discuss. I think it could be an interesting approach. Very nice work. Yeah, thank you for your comment. Yeah, I'm sure that would be an interesting direction. All right. If there are no questions, let me thank Professor Manor Van Murtan. Excellent, excellent work. We'll be following your work and hopefully we'll see you in person in the future workshops. Thank you. Workshops. Thank you. And okay, so our next speaker is Professor David Weidwitt.