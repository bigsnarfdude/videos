The last prepending report was in fact pretty much the same audience. It was biotech. So that's a nice book end. And there's a lot I want to get through. But first I want to say this is work that I've done together with my student Nitaj, who basically is going to defend, I think, within a month or so. And a lot of this was actually conceived of during the pandemic, where you basically sit at home, and in my case, Where you basically sit at home, and in my case, you just scribble and scribble. And it's actually, I was reviewing a paper, and I was looking at and squinted and like, ah, but you know, this can be done better. And then this entire work came out of that. So in this talk, there's a little part of it which has a lot of math in it. And for those where this is not the thing that they kind of like to do, you don't have to understand it. But for those who Have to understand it, but for those who like it, you know, there's sort of a payoff. I mean, I was really, you know, pleased when I saw this particular thing drop out, and maybe some of you will too, and that's why I put some of the math in here. I want to get through the first few slides very quickly where it says predicting the function of a biomolecule based on sequence alone is the holy grail of bioinformatics. Yeah, I mean, you can imagine that predicting function is going to be something that is going to be worth money, right? Be worth money, right? It's a license to print money. And I don't really have to convince you, you know, that this is going to be important in many, many different fields. For example, predicting binding affinity of the protein, you know, does the receptor binding domain of this coronavirus bind this particular antibody? You know, that's the kind of predictions that you would like to do. Or predict whether a target is resistant to drugs. Is this patient's HIV 140 resistant to innovative? There's a company called Medallion Biosciences who made a billion dollars just with that little thing. Dollars just with that little thing. Predict the effect of a mutation on the molecule's function, like for example, is this mutation Q61K observed in this tyrosine kinase function without actually going into the lab and doing the mutant and so on? Being able to do that, like I said, is something that would be great. Now, of course, there are tools to do this, and I'm going to give you sort of a brief overview of how this is done today generally. One is, of course, molecular dynamics simulations. So you try Dynamics simulations. So you try to predict the structure of a protein using molecular dynamics simulations, meaning you know, you simulate the atomic force and so on and try to really understand how this thing wiggles and binds. And it is expensive. And for each protein, you're going to have to work really hard. You can also calculate binding infidelity by molecular docking simulations. There's whole industries, whole labs that are devoted to that. Why? Because, of course, this is important for pharmaceutical design and so on. It's expensive, also. But instead of simulating the chemistry of the molecules, you can also attempt to understand the map from sequence to function. So basically you'd say, okay, we have a set of sequences of fixed length, and we have sort of it's a vector S out of some space, and you have a genotype-phenotype map that maps from that space to a real-valued function. That's the genotype-phenotype map. And there's two main strategies in trying to find out. In trying to find out what this map is. Because if you had this map, you just plug in one of the sequences and get the value, yes? But you need to define a phenotype to get this mark. What is the number? Well, anything you want. This real value function could be the replication rate of the organisms. It could be the binding affinity of this protein to a particular molecule. It could be the fold resistance of the protein to a drug. You know, protein to a drug. That's a real value function. Any such function that characterizes how good that molecule is. It's a function you normally want to maximize. The two main tools I'm going to discuss here are regression and deep learning. There's more actually. In fact, Olivier is going to talk about a different approach right after me. But I'm sort of focusing on these two: regression and deep learning. Focusing on these two, regression and deep learning, as an introduction. So, regression is basically the following: there's this thing called a molecular POTS model, which is designed by making a maximum entropy assumption to an energy function. The energy function is supposed to be like this. If your energy is zero, then you're the ground state and you have the highest fitness, the highest function. And if the energy is higher, that means in fact you're inferior. It's obviously directly inspired from physics. So, if you define From physics. So if you define P sub K as the likelihood to find this sequence S in a functional ensemble, and this little E here just supposed to mean that this is an ensemble that is functional in an environment E, as opposed to if I take away the E, then it's basically all sequences of that. And then basically this maximum likelihood approach tells you that, well, the probability distribution is a Boltzmann function. E to the minus Function, e to the minus e. You don't see a temperature here because, in fact, the temperature in the Boltzmann function is just a Lagrange parameter, and the Lagrange parameters actually appear in the energy functional, as you will see now. Basically, the energy function that maximizes this entropy has two terms. One, which is basically a function at each side. These are all the Lagrange parameters here. This L times the dimensionality of the alphabet, and here L squared divided by 2 roughly. Divided by two, roughly, times the dimension of the alpha squared. There's many of these things, but they are basically contributing to this energy. And they're obtained by, well, here, you know, they're free parameters, but then basically what this approach is, you're going to be fitting these parameters to a multiple sequence alignment. This model is successful in using these pairwise correlations, these JIJ terms, to increase the prediction accuracy. To increase the prediction accuracy beyond basically doing a monomer-based prediction. But it comes at a huge cost in fitting thousands of Lagrange parameters, basically doing Gibbs sampling on GPUs. So it's very expensive. And very often, in fact, they can't even use the full alphabet, let's say, of 20 residues and proteins and have restricted themselves to about four of them. Because the number of parameters is much, much larger than the number of sequences that you have in the multiple sequence alignment. That you have in the multiple sequence alignment, yes? So, as you're talking about regression, so how are you white noise? What is the white noise? Well, so the noise is the fact that there is a substantive amount of fluctuation in the energy that is not actually reflected in the function. So, basically, what these approaches are trying to do is they're trying to extract the function from the multiple sequence alignment. But of course, they don't know what is it. But of course, they don't know what is information and what is entropy. They're trying to fit basically, you know, what they're trying to do is come up with a set of parameters that will reproduce the monomer probability distributions and the pairwise correlations that are apparent from there. And that's it. However, it's a lot of parameters. And there's going to be a lot of noise. So I call it regression because of the fitting. There are other methods which actually directly use a regression method, but this Directly use a regression method, but this one is different because it uses this maximum entropy approach. And correlations between three or more monomes simply cannot be taken into account. Because if you put a third term there, with jijk, you know, you come into the millions of parameters. All right. Another approach is to use neural networks. Yeah. Does a method like this work when I have a library of antibodies which don't exist in nature? Of anti-values which don't exist in nature, but I have a function, like a real function called the correlations between so if so so let me say something very general which I'm hoping is going to answer your question. None of this work, all the work I'm going to be talking about, will work without a multiple sequence alignment that you have obtained under certain conditions. You could, for example, have, you know, here's a set of sequence that binds this particular antibody, okay, at level between At level between x and y. So you have to have this kind of data, or else I will show you just simply an alignment that you're taking from P5. But if I don't have any particular data because I don't have the experiments with that particular binding factor, then I can't do it. So the neural networks, of course, that I use famously for detecting cats and pictures and so on. For detecting cats and pictures and so on, they have a deep learning network. They will just see the entire pattern of mutations in the multiple sequence alignment and try to sort of figure out how to predict sequences function that are not in the training example, so tests. By the way, while they're actually very good, in fact, they're currently leading until today, or next week, approach. Approach does very well, but they have a known vulnerability, namely that it's difficult to control overfitting of the training day. This is true for any deep learning approach by trying to teach a computer to recognize cats. Or, in fact, any of Google's approaches, which are also based on the same approaches, they spent 90% of their time trying to understand overfitting. These methods cannot tell the difference between information and entropy in the data. Between information and entropy in the data. Because everything in the data looks like, you know, what looks like it's important. Okay? That's what I just said. They can't tell that this difference. And so they have very sophisticated methods to avoid the overfitting. That's what they all work on. Now, where we can tell the difference between information and intro. So, in biomolecules, Later. So, in biomolecules, of course, function is encoded within the sequence in the form of information. So, can we locate the functional information and separate it from entropy? And I'll try to convince you that indeed we can do this. And as you can imagine, if you can do this, then you'll be better at predicting function from sequence. In fact, what you should be able to do is even create sequences that you predict will be highly functional. So, I have to give you a little bit of background. So, I have to give you a little bit of background before I can show you how this works. And so, I go all the way back, in this case, to a paper from 2003, where Jack Shostak shows us how to calculate the information content of a biomolecular sequence of length L from an ensemble of functional sequences. Basically, he shows you this formula, which he calls functional information, which is minus the logarithm of the size of the ensemble, the number of What is that activity to achieve? And n is the total number of sequences in the ensemble. And so this here is the fraction of functional sequences. And minus log of that will be the infographic. He has also a little picture here. Basically, think of it this way. So, of course, back to this. Think of this way. To this, think of this way: like this is all of the functional, all of the sequences of length L, you know, n, and then as you increase your threshold parameter, and you know, you're getting to sequences that have a little bit of function, more function, more function. So basically, you have this, here is a particular level, and the ratio of the number of sequences in here and that minus log of that, that's this function I'm in from. Okay? So far, so good. Turns out that it shows us like. Turns out that characterizes the information content of an ensemble of sequences. I think there was a question. Yeah, can you ask? For the NE of theta, is that the area of the size? Well, it's the total number in that group. So it's on the slice number. Yeah, yeah, on the slide is, you know, oh and actually, um it is everything from here uh to the top. Oh, I see. Yeah, from here to the top. A minimum, right? At least as functional as theta. At least as functional as theta. So this functional information, it turns out, is a coarse-grained version of the information content I had introduced earlier in this paper down there. Now, it turns out that Shosak knew that. He just didn't know, I mean, he knew this paper, he cites this paper. He just didn't know the relationship between the two, which I'm going to show you here. So, what I define is this information here. Is this information here? Conditioned on a variable. So let me actually define the variables here. S is the sequence random variable, and E is an environment random variable that can take on states E sub I. And this one is one particular of those. So that means that this difference of entropy is, well, the H of S is the entropy of sequences not constrained for a function. And H of S given E is the entropy of sequences functioning at this particular. We have sequences functioning at this particular level that you're interested in. It's obviously smaller than H. And as a consequence, this difference is positive. And is that at level theta or at least? You could say at level theta and better. Okay. Because what's on top, actually, is in a sense a much, much smaller number. So in fact, whether you're doing that or not, it's just a small color. So H of S is, of course, the log N, you know, basically if you assume that each of the numbers Know basically if you assume that each of the non-functional sequences are roughly the same frequency, yes? Yeah, I have a question about this. So, basically, you spent some time talking about the maximum entropy model and how it was modeling the probability distribution of the frequency. That's what cube the h of s? Yes, and it's I think the next slide. Okay, so it's not, but they don't want to be born again, it would be something. So, the h of s is the unfunctional one. So, basically, there is the probability distribution is one over p is. Is one over p is one over n. Okay, so you're basically assuming if you're non-functional, then the abundance of non-functional is roughly the same. That's why it's log n. This one here, the probability distribution is essentially, you know, like you could plug in the Boltzmann distribution, you know, from the Poltzmann. And I'll do that later. But these are unfunctional. And for unfunctional modules, you have each unfunctional module. Each unfunctional module molecule is unfunctional in the same way. It's almost paraphrased, also. So its probability is, you know, is expected to be 1 over n. So that means, and here, this entropy, of course, is just p log p, and that p is, of course, the probability to find this particular molecule k in that muscle. Now, in s case, of course, from that ensemble. So, what is the number of non-functional sequences? Well, log n, if you take Sequences well, log n, if you're taking sequences from an alphabet of size d, there's d to the l possible one of those. So this log n is just the length of sequence. Information is the length of sequence minus its conditional entropy. So you have this formula over there. Now, can the pK be calculated from an ensemble? And the POTS model says yes, by learning this precise bias and site-to-site correlations, which are these two different functions, you know. Two different functions, you know, it is basically basically by learning these Lagrange parameters. And my model says, yes, by reading off per-site bias and site-to-cycle directly from other sequences, I understand this sentence at this point. The rest of the talk is basically to teach you why you could actually do this and how you could actually do this. So, but before I can do any of this, I'm going to have to teach you a theorem. If you haven't seen this theorem before, you are Seen this theorem before, you are going to want to know about it. Okay, it's called the entropy decomposition theorem, and it's actually due to Fano in this book from 1961. He himself quotes, and I looked at that case, so Fano is the one who wrote it down first. What we want to calculate, sorry, let me actually go back. What we want to calculate is this sequence entropy, okay? And it's an entropy of L monomers, okay? Monomics. Okay, so what does Spano say? It basically says the following. He says, okay, it's this long sum, where the first term is the sum of percite entropies. I'll show you later how you calculate per site entropies. But, you know, there's no correlation. The next term is, in fact, the sum of all pairwise informations, which is like, you know, can you predict what's at j from knowing what's at i? Okay? And the next term is the sum of all tributes. Term is the sum of all triplet informations and so on and so on with alternating signs until the last term you know is basically the most higher order correlations and there won't be a sum. It's just this multi-information which I'm going to show you in a second how it's defined. I mean you probably already know how, sorry, let me go back. You probably already know how this pairwise channel information is. Shannon information is calculated. Often people use actually a semicolon here. I just like the colon, but I'm probably going to transition to semicolon because every reviewer goes, Why are you doing this? It turns out in physics they're more likely to use the colon, and I didn't even know that. I mean, I used to do quantum information theory. And then when I went to classical physics, and realizing, oh, they're all using semicolon. Anyway. Sorry, could you explain why there is an alternating sign here? Yes. Yes. This is a special case of an inclusion-exclusion theorem that has been known since 200 years. In the paper, I have a quotation from a French paper that basically says, you know, it's almost like the ancient Greeks already knew this theorem. And if you know the inclusion-exclusion theorem, which you can think of in terms of Venn diagrams, then you will understand where this comes from. You might also understand it from a slightly later expression that I will give you, but understanding the But understanding the alternating signs of that is almost the most crucial part of this. It is like the most important takeaway from these energy and information decomposition theories. So I give you an example of three variables. So here's the first one. It has a particular entropy, which we have here in yellow. Then, of course, I might have a second variable, and these are entropy Venn diagrams. And here's the entropy of the second variable, and then this. Second variable, and then this here is the entropy of the joint set of the two variables. And here, that thing in the middle is the shared entropy between the two ensembles, or the information that one monomere has about the other monomer. If I introduce a third variable, a third monomere for a trimer, then I have, of course, here the entropy of the third variable. Here is the joint entropy of all three variables, which in a sense would be the sequence entropy, right? Because S1, S2, S3 would be here the full sequence. To S3, would be here the full sequence. And that thing in the middle, that's the triplet information. And don't be fooled, you know, all the things that I wrote down before can be positive. That thing can be negative. And I don't have an hour to tell you why, and it's really very interesting in terms of cryptography, but we'll skip this from now. You can imagine what the fourth order term looks like. It's a bit more difficult to do, but you'll find that there is a thing, an intersection, an informational intersection between all four. Between all four, and then you know, in five variables, there'll be intersection from all five and so on. And that's these things. These, you know, they play the roles. So, this is the entropy decomposition theorem. You've seen this now before. There is a dual theorem to this one, which is an information decomposition theorem. Now, Fano didn't really write it down in his book. We wrote it down in this paper here, but frankly, Fano already knew about this. This is the same kind of structure. This is the same kind of structure, it's just written in terms of combinatorics here. So, what this looks like here is that instead of the H here, you have the I here, this thing, which appears here. But in this here, in this theorem, this H of all appears actually here. And instead of this one, I would have the joint entropy of all three. Here I have the joint entropy of all two, and here I have the same thing. Okay, so this is why this theorem. So, this is why this theory is in a sense completely dual. So, here you decompose the information in terms of pairwise, triplet, and so on, joint entropies. And here you are decomposing a joint entropy in terms of correlations. They're not difficult to prove, actually, but you might spend the night to know. So, how can you now calculate information from this multiple sequence alignment? Well, the multiple sequence alignment aligns align Multiple sequence alignment. Well, the multiple sequence alignment can be used to calculate the monomer entropies and pairwise information entropies, which reveals how information is composed of multiple variable correlations. And all you have to do, for example, to calculate the monomer entropy is you basically look in one of the columns, let's say the first column of your multiple sequence alignment, and look how often do you have, let's say we're doing nucleotides, how often do I find C? How often do I find G? A, T, and you basically do your maximum likelihood probabilities, P A, P, C, P, G, P, T, and then you do minus. P C P G P T and then you do minus P lot P of those. And they, of course, do not appear at equal frequency. Some appear at high frequency, that means very important. And some appear at low frequency, like you better not have that. So that gives you H of S across all the sites. And then summing up, that's the first term. And then you can look at the pairwise correlations. But this calculation refers to sets of sequences. But what I would like to do is predict the information. To do is predict the information content of a signal sequence, right? Because if I can do the information in a signal sequence, I know the function of the sequence. I'm not interested in predicting the function of an ensemble of sequences, because that's not what the people want. They want the people, it's like this one, what is it worth? So, can we do this? And the answer is no and yes. It's no, because by definition, information is a property of an ensemble of sequences. It can't Of sequences. It can't be a property of a single sequence. And it is yes, because given a particular ensemble, I can basically tell you about, you know, how does this sequence relate to that ensemble? It's possible to define an information score for a single sequence in such a way that the average score over the ensemble is in fact the information contained in the ensemble. That's not my idea. People have figured this out, you know. My idea. People have figured this out a while ago. We can see this, by the way, from the formula for the joint shared probabilities that are at the root of the proof of the entropy and information decomposition theorem. And you probably haven't seen this before, but in Fano's book, that's like, you know, the central equation. I'm only giving an example for three, because when you see the example of three, you can kind of see how it is. This is this probability that he writes down. It's a correlation probability between I, J, K, and it's constructed. Between IJK, and it's constructed from pairwise, triple, and mono probabilities. Okay? And you see, your pairwise are all on top, the signals are all on the bottom, and the triples is on the bottom. And you see the sign, you know, if you take the log of this, you can see that there's a sign change coming from the different orders. And if you build the fourth order one, then you're going to have the fourth order on top, the third order below, the The third order below, the ones single order here, and I think the pairwise over anyway, so you can construct those yourself. Now, if I take the average of this with the joint probability, then indeed what I get, well, I'm going to get this term, this term, this term, which are this, this, and this. Then I get three terms from here, which is here with minus signs, and then I get one term from this, which has the same sign as that. So that's how you see. Understood. So that's how you see that you get this triplet information. And this is the information decomposition theorem, but at order 3. But it turns out that you can do a decomposition of the probabilities. And these things are pertaining to single sequences. Is it easy or hard to show that this construct is parallelized? PRJK or if you're not sure. Yeah, it's the that's easy to show. I mean, ten minutes. Ten minutes. You just have to sum over all I and J, and you'll see, for example, if I... I mean, I'm not going to do it on board. May I ask? Is this similar to the trajectory probability in stochastic thermodynamics where people define or observe a Markov dynamics for a few time points, then they can calculate a trajectory probability? Is this formula similar to? Is this formula similar to that? I don't think so, but I really should say I don't know. Because I don't think I've seen this. I'm not sure about this formula, but the previous thing you're describing essentially, you know, defining information for a single sequence, and then you have the average over all the sequences. I think that has a direct analogy. That's right. I mean, people like supplies over the past and you know, it's the same way you can define like you have entropy of an ensemble, you also define entropy of a trajectory. Right, so indeed, the log of things like that will be called a surprise. The log of that doesn't really have a name, but of course, once you stare at the formula, for example, for the pairwise, you know, you'll see it's an average over the log of pi doc, i, you know, and j. Anyway, let's move on. So I'm going to show you now how to calculate information scores directly from multiple sequence alignment. So the first thing I have to do. Sequence alignment. So the first thing I have to do is I have to define a score, a monomere score, R of S sub i. So at a site I in a sequence of length L, there might be, you know, think of this as a vector. Okay, so it's a function from a vector. And to do that, I will define for you the position weight matrix R I A. So if S is a vector that has an index A, then you can define a matrix R sub. A matrix R sub I A. And it's just the logarithm. You know, it's a ratio of two probabilities. Let me actually show you that. So Pi of S A is the probability to find the eighth monomer at position I, just like I said for the sort of CJA and T, but you can do it for proteins and randomly one. It's the maximum likelihood estimator coming from the frequencies that you find in a alignment. The one over D, I could have just written, oh, this is just one plus. Written, oh, this is just one plus log, because if I take logarithms to the base D, which I usually am going to be doing, and if I'm not doing it, I'll tell you. But it's important because, in fact, it's the unbiased predictor. So this is really a ratio between a prior and a posterior. So if you do Bayesian inference, you've seen things like that, the log ratio of the posterior and the prior before. And you can play around. And you can play around with that. Sometimes there's better priors than 1 over D, and that's why I'm not just writing this formula like that. So say the sequence actually has a monomere B at position I. Then I will say that the score of that sequence is this matrix with a delta function where I say, oh, just set this one to that. So it's just a particular matrix element. I'll show you in detail with an example how this. I'll show you in detail as an example how this is done. Then, of course, the monomer score is just the sum over the single size scores. You'll understand this better from an example. Let's do an example. We'll do a DNA binding site, and it's the MYV transcription factor, never mind what that is. And there's, let's say, 70 sequences in my alignment, and it's a site with length 9. So it's simple enough. And let's say this is what I measure. So I find. Okay, so I find A 19 times at position 1, I find it 64 times at position 2, and so on. And of course, there are some cases where I don't see anything. Now I'm going to get maximum likelihood estimators from that, but I need pseudo counts because I'm going to take the log, and if there's a zero in there, my computer won't like it. So basically, my probability estimate is the count plus some pseudo count divided by the total number plus, obviously in this case, because I've nucleotide 4. Obviously, in this case, because I have nucleotide four times the pseudo count, so that these things are normalized. Okay? So now I can do that. I'm going to show you. I'm going to, you know, make this matrix just by a click. I actually had to do this the hard way, but here, you know, just a click. So you see all these probabilities, and you see where there was a pseudo counts, there are very small probabilities that come from that. Now I have to take the log of that divided by a quarter, right? This probability divided by a quarter, and then the log of that. Quarter and then the log of that, and that gives me this matrix. And there's, of course, negatives in there because there are some, you know, numbers that are smaller than one in the log that I'm taking. And in fact, these scores, these information scores I'm going to show you, unlike information in Shannon theory, which is positive, you know, semi-definite, these scores can be negative. And obviously, you know, as you can see there, in particular, sequences that are very far away from a functional That are very far away from a functional sequence would have very, very negative scales. So let's calculate. Oops, I think I went one too far. Maybe not. Oh yeah. So this is actually a sequence logo. Tom Schneider basically pioneered thes. And in fact, this method of calculating information scores is also Tom Schneider. He did a different way of doing the prior and so on. Way of doing the prior and so on, but never mind. That's why I said that I did not invent this. But you see, for example, here, this A has 1.81 score. You know, you could call them bits if you take logarithms to base 2. And this one has 1.79. That's the height of these things. And this is mostly C with a tiny bit of A, and so on. So that's how these sequence programs go. And you see, there's nothing here. There's a good functional reason why there's nothing there. But let me show you how to calculate the score. How to calculate the score. So here's your matrix, position weight matrix. And then let's say somebody gives you this sequence and says, give me the score. It's like, okay, all right. This first order score. Okay, you have an A in the first position. I can see a 0.11 there, so that's what I'm going to add. The next position is an A, okay, that adds a 1.81. The next position is a T, well, that subtracts minus 1.62. It basically tells you that that's not the preferred. The preferred nucleotide for a good binding site. It would have been this one. That's why you're getting a minus there. And you just go through this thing and add up. And then you have the sum of the score, which is actually a pretty good score. It's positive and so on. And this was actually shown how to do this by Berk and van Hippel in 1988. If you're interested in this kind of stuff, read this paper. Kind of stuff, read this paper. It's just a blog post. So he actually created a somewhat different position weight matrix, but it's related, and I'll talk about this later. But in spirit, they're the same. But we can create a second-order position-weight matrix from the multiple sequence alignment. And that's just simply, instead of the probability of one nucleotide, I have here the probability of, you know, finding a particular pair, you know, a C at twenty-one and an A at forty-eight, right? A at 48, right? And I'll divide by 1 over d squared. I can do this, just as a straight extension. We can create a third order position weight matrix, which is just this thing. You're going to need really big alignments in order to get decent statistics out of it, but you can define them. You can use the full score of a sequence, the full exact score, can actually be decomposed into a sum of first order, second order, etc. terms, just like Terms, just like the entropy decomposition theorem has taught you. And that Berkman vin Hippo did not know. If you need to calculate for the more higher orders, would that require more data? Yeah. We will see this. So, first order approximation to an information score is just the sum of monomer scores, and that's Berkman-Hibble theory. You can also call You could also call it Tom Schneider's theory, but in fact, I don't know who came first. I believe Berkman Vernon came first, but I'm not 100% sure. But we call it RI, RIS. And, you know, I'm interested to see whether I'm about to add it. Second-order approximation. Basically, you are summing up those and then you're subtracting the pairwise. Yeah. Is there a guarantee that the later term? Guarantee that the later terms, which are more numerous, will contribute less and less. There is an expectation of that. There is no proof of that. For a well-behaved ensemble? It is generally the case. But I believe there are exceptions. I'm going to show you a protein where this might not be the case. But the important thing, of course, is that the number of terms explodes. And so once your ensemble isn't very big, then because of Very big, then because of the estimation noise, you're going to get problems. So, this is beyond Burpo-Nibble theory. And by the way, this pairwise score, of course, is just calculated from the single scores and the joint pairwise scores, like Shannon with TELIS, like this formula from Farno with TELIS. The third order approximation then would, of course, look like this, and where this triplet score is, of course, in combination of singlets. Is of course in combination of singlet scores, pairwise scores, and the full triplet score for any particular triplet. So now what's the relationship to the POTS model? And now comes in the math that if you just don't want to worry about it, I'll tell you what you can pick up again. The theory, in fact, will reveal that the Potts model construction is unnecessarily complicated. The Potts model relies on a maximum entropy assumption for the entire sequence S. The entire sequence S, as you remember, where E is this energy functional that is basically approximated by estimating these Lagrange parameters, which are designed so that they're going to reproduce the single substitution probabilities and the pairwise substitution probability. And it's very expensive. Now, let's actually define instead of an information score, an energy score that is dual. An energy score that is dual to the information score using an alternative position weight matrix, which I wrote here. There's no second index because here, think of this as a vector. It has an index. But instead of the Pi on top divided by the prior, a quarter, what you actually have here is this P0, which is a likelihood to find the consensus monomer at position I. And the consensus monomer is the one that has the highest frequency of all. Okay? So if A. Okay, so if A is like you know 80% of the time there, that's the consensus. So it's very different estimator, right? It's basically the biggest one divided by this one, and the other one was the one divided by the random one, okay. So yeah, Pi is likely to find monomer SI at position I. P0 was likely to find the consensus monomer, and a reminder that, you know, the That the R matrix was different. The second order, of course, it's the same thing. I have the consensus pair in the numerator and the pairwise probability in the denominator. This energy score, so the way you've constructed it, it's the ground state energy will be, it's the consensus sequence, and everything else is an excitation. Yeah, so the consensus sequence will have energy zero by definition. And the consensus multiple times. And the consensus monomer has a contribution of zero. And the consensus pair has a contribution of zero. Any deviation from that will have a higher, and because the ground state is the preferred state, you know, so on. Good question, thank you. The energy score I will now show you is in fact the energy function of the Potsman. And I'll show you how you can see that. So I've written now the energy score, you get the On the energy score, you get the logarithm of the entire consensus sequence divided by the probability to find this particular sequence. Yeah. That's a good question for in case that, for example, if you have two consensus sequences, how do you defer it to the same probability? So, you know, in a sense, you're asking me how we implemented that in the computer. First of all, it's very rare to find that if you have large enough ensembles, but I would probably just for Some of those, but I would probably just throw a die and say one of the two is going to be, yeah. It's maybe more sophisticated ways of doing it. And I didn't program that. Nitash will probably answer this better than I. I was going to ask for the opposite end of the spectrum. Like for an entire sequence, probably you wouldn't find a sequence that occurs multiple times. Yeah, so we never actually deal with the score of the entire sequence because indeed the ensemble size in order to estimate those is insane. Estimate those is insane. It's essentially the entire universe. Okay, so we do not go to order L, we stop at some point. So I meant the formula there. Yeah, yeah, sure. But this is the one that they are writing down. Instead of the one at R. Oh, yeah. Oh, for the consensus. Yeah, yeah. So you'll see in a second how important that is. But what I want to say is I can rewrite this where the log is now base E. So I write this as P is P0 times E to the minus the energy. E to the minus the energy. And you start seeing that, hey, that looks just like this formula that they had, except that there's a partition function here, which is a sum, obviously normalizing these things as probabilities. But it turns out you can show that this partition function is, in fact, one over the frequency of the consensus sequence. That is the key. I mean, once I've realized that, and the proof takes a little while, it's true. Takes a little while, but it's true. You realize, oh my god, they're actually one and the same. And when they're actually one and the same, well, then actually, let's calculate the entropy of that sequence, because just like you actually suggested, you know, that you have this thing, right? And they are doing an assumption, a maximum entropy assumption with the energy of the entire sequence. You know, what is the P that maximizes the entropy of the entire sequence, which kind of would assume that you have an infinite ensemble that is a thermodynamic equilibrium. That is a thermodynamic equilibrium, which is nonsense. What you should be doing is assuming that each site is an equilibrium or a pair of sites, entripless of sites, and that's what's actually going to be happening. So, I'm plugging now in this pK into my entropy formula. And of course, you get the log turb and so on. And you get this by plugging in this energy there. Okay? So I did nothing but calculate the entropy, which is this thing that they're maximizing, plus the constraint. Plus, the constraints that they put in. So, now what I'm going to do is I'm going to expand this entropy to second order using the entropy decomposition theorem. Okay? So, that's this. You've seen this before. Now, I put it on the left-hand side, and I'm going to write the log z on the left-hand side and put all the terms to the right-hand side. And it looks like this. You see? There is, that term comes from entropy, that comes. Is that term comes from entropy? That comes from the Lagrange parameters, from the entropy from the Lagrange parameters. And so now I'm going to define the average of Lagrange parameters. And then suddenly this right-hand side starts to look really simple. You have entropies minus average Lagrange parameters and pairwise information plus, and they probably are wondering why do our second order Lagrange parameters You know, Lagrange parameters always end up being negative. Yeah, because you did not know about the changing in signs. Now, what I'm going to do, because Z is just, sorry, Z is of course just, you know, 1 over P0, I can now expand this to second order. Like, this is a second order expansion. So I'm going to have to expand this. By the way, when they're doing a maximization of this, they're actually saying this is a constant. They're actually thinking this is a constant, and they don't even have to worry about it. And that's a huge mistake. It's very important, obviously. So I expand this now to second order, and you see the expansion there. It's basically you have a monomer term and you have a pairwise term. And now I'm going to be plugging those in here. What I find is that this equality holds only if every for every order basically that For every order, basically, that this average of Lagrange problem is equal to the entropy at that site plus the logarithm of the frequency of, in other words, this is fixed. All this comes from the alignment. This is not a variable quantity. Same thing to second order. You see, everything's negative here, which is what they find from optimization. These relations that I showed you just are in fact. These relations that I showed you just are in fact equivalent to the Helmholtz free energy relations of thermodynamics. Here it is. Usually you write H there, but we already have an H, so the reviewer said you cannot do that because some readers will think the H is an entropy and they're like, fine. So we call this omega. It's the free energy. E is the energy. S is the entropy. And T is the Lagrange parameters. The Lagrange parameters that you are using, you know, in the workplace. In the Berkman-Hippel theory, I've set to one. There's also a Lagrange Prime Minister in Berkman-Hibble theory because Berkman-Hippel realized that they got their matrix from a maximum entropy assumption on the monomer. And so they were smart. So this omega, of course, is minus log of the partition function, as we remember from the first semester of thermodynamics, which we now know is log P sub 0. So this is basically So, this is basically Helmholtz's theorem to first order approximation. This is Helmholtz's theorem to the second order, and so on. So these are the ones which allow us to replace this, do no search, but just simply read this off of the alignment. That's the sentence that you couldn't understand at the beginning, but which I hope you are understanding now. So, I have a question about this matching of order to order. In which sense are you actually defining? In which sense are you actually defining first and second order, et cetera, on both the left and right-hand side? I think it's obvious because you always have single probabilities or pairwise probabilities or triplet probabilities. So basically it's under the assumption that larger multiples of probabilities, even if they're different probabilities, will be smaller. Yeah, it's not an expansion in a small term. It's the final expansion. Right? So in principle, they could all be blown. So, in principle, they could all be blowing up, but I'm showing it order by order. And yes, if you're going to be cutting things off, you better hope that they become smaller. We have another paper where we've actually shown this to be the case, but only for the averages. But I can't talk about that now because that just, you know, not going to fit. But yeah, we worried about that too. We saw that the estimate for the average of the scores is wrong in first order. And second order is wrong on the other side. Third order, basically, it oscillates. The third order basically oscillates, but it kind of approaches. So that when you're cutting it off, you're actually very close to the. So it's not a proof, but at least for that ensemble, it seemed that indeed this decomposition is almost like a perturbative decomposition, even though there is no small parameter. I have a question. So just curious, why you're calling that T as the Leibniz Leibniz parameter? Because technically, like Landing parameters are the coefficients for the constants. Yeah, but you know, like. Yeah, but you know, like in thermodynamics, you know, if you are in equilibrium at a fixed temperature, then temperature is constant. And temperature is just a derivative of the entropy with respect to NA. I mean, if you go through the derivation of the Boltzmann distribution via an optimization of the entropy, maximization of the entropy, you'll see that you have to introduce a parameter, and it'll be e to the minus e over lambda. And then you'll realize this lambda has to have the same. And then you'll realize this lambda has to have the same dimension as E, and E is in kilocalorie joules or whatever, then you have to introduce a conversion parameter which you call k, and then a dimensionless parameter you call t, and then kt is the energy element. But I want to show you that not just the average of the Lagrange parameters are given by averages of the PWMs. In fact, this is true for the scores themselves. I mean, you kind of hinted at that before because I showed you that there was a decomposition. That there was a decomposition theorem from the scores. But the beauty is that this parameter is just essentially, you know, because at each position, you know, I have A, you know, D different, you know, so this is why the dimensionality is so bad. If I have 20 amino acids and I have lengths 100, I have 20 times 100, just first order Lagrange parameter. But they're given by the elements of the PWM. This is in fact Birkenville Hippels. Like Berkeley and Will Hibbles, you know, position weight matrix. The pairwise ones, this is the energy PWM as opposed to the information PWM, which are related to each other by the Helmholtz relation in the end. This Jijs, of which there are so many, you know, for each pair, 20 by 20 parameters, right? That's the second order of position weight mixture, which goes beyond work on the Herbal theory. And so, like I said. And so, like I said, the Lagrange priorities can be read off the molecular sequence alignment and they do not need to be fitted. It's a complete waste of CPU time. So, let's apply the theory. So, the new theory should be able to make predictions about the functionality of sequences by giving you a score function. You know, it's either high or low. Without any need to fit, taking into account higher order correlations beyond pairwise. Which the only one that could do this was deep learning, but they didn't know it's an am. That they didn't know it's an amalgam of something, right? There is no knowledge about signals pairwise and so on. There's no way to control the order of correlations that's taken into account when you're doing deep learning. So we're going to test it on a synthetic data set first that has perfect experimental function data because it's coming out of a computer. So we use that because we just happen to have that, not because it's the greatest example to use. And then we're going to test it on protein sequences with imperfect protein. On protein sequences with imperfect functional data because they were taken in real-life experiments. So, the data set that we're going to get is from Avidian self-replicators. Avida is a way of doing evolution inside of the computer. We have computer programs that evolve. They are self-replicating computer programs. It was developed in my lab over 25 years ago, and some of the students are still playing around with this. And so, these sequences are taken from an alphabet of 26 symbols, which are the components. 26 symbols, which are the computer instructions. And these self-replicating programs are extremely rare. You basically cannot find them by random search. Okay, right. If you wait long enough, then perhaps. So here's like typical sequences. They don't mean anything for you, but they have motifs, replicative motifs and functional motifs and so on. They're just like biomolecules, except in the computer. So the minimum length of an Avidian self-replicator is actually a length 8 sequence. And I believe. And I believe that's length A sequences. To tell you that they're rare is, you know, there are 26 to the 8ths, it's 209 billion possible sequences, out of which we found that exactly 941 of them can self-replicate. Now that we have functional sequences and then all sequences, I can calculate Schost information content. And that's 5.91 merg. So what's a merv? It's a measure of information where I'm taking the logarithm to the base of the alphabet. So that in Of the alphabet. So that an L mirror has L mers of entropy. Get it? Right? And that's first of all, potential information. And so here it's six, almost six out of the eight mers are information. So about three quarter, right? But they're the smallest replicators in this set. So, but we are also able to analyze the complete length 9 landscape, and we just took that in order to do our little test. There's 26. To do our little test. There's 26 to the 9 sequences at about 5.4 trillion. Probably the world's biggest genotype phenotype now. It took quite a bit of compute time to do that, three months actually. You know, massively parallel search. I mean, 5.4 trillion that you have to execute and see whether it can make RSFI. So we found exactly 36,000 and some replicators, which means that the information content is actually slightly less. And there's many reasons we can talk about that. And I'm going to show you a cluster. You a cluster, the largest cluster, because these sequences, of course, some of them are mutationally related. Just to show you, they look like biological sequence clusters. Okay, so these are clustered by mutational distance. You even can do the edge distributions. These are really complex, complicated things. But these are all the self-replicators in that landscape. Now, what I want to do is I want to make a classifier that will tell, give me any sequence, and I'm going to tell you whether it's a replicator or not. Without running it. Replicate or not. Without running it through the assay that we usually run it through in order to test whether it can make offspring or not. All right, so to classify randomly generated sequences which are known to be non-functional, because if by chance we didn't find a replicator, we would have known because we checked against our data set. But we'll also have non-functional, single and double mutants of functional sequences. And you can imagine that if you just do a single mutant of a functional sequence where they differ only in one out of nine monomers, they look almost Monomers, you know, they look almost exactly the same. But this tool should be able to say that they're in fact different. Now, a classifier is a function that generates a score from a sequence and given a particular threshold classifies the sequence as function or not. Then, by varying this threshold, we can generate a receiver operating characteristics curve. And the area under the curve is the quality of the classifier. And many of you know exactly what that is. Many of you know exactly what that is, but basically, a classifier with quality of score one is perfect, and if it's one-half, basically it just classifies value. So we'll construct classifiers that take into account correlations up to order four in these nine modes. The first candidates for classifier, of course, are the information scores that I showed you, right? But it turns out they're terrible. And the reason they're terrible is because of this alternation of signs. And that's because, of course, Because, of course, the higher the order, there's going to be some cases where the particular set of symbols that is in my test case does not appear in the alignment. And then I have to introduce these pseudocounts. But there's this cancellation between single and pairwise sites, and you can't use the same pseudo count for all of them. So in fact, now the cancellation doesn't happen exactly because of the pseudo count, and it just Because of the pseudo count, and it just messes up everything. So you can't use those. That's what I just said. And it is not possible to prevent the pseudo-count contribution from overwhelming this classifier. You have to find something else. But in fact, you know, Nitash just came up with these, and they work really well. The first order one is actually the same. Just the sum of the, because there's no cancellations. For the second order, he just takes this sum over the pairwise You know, joint scores. Okay? And the joint scores, of course, I already have singlets and doublets in it, but I don't have to do this cancellation because I'm just calculating those directly from the sequence. Yeah? A quick question about this, ignoring the higher order terms. Are you assuming that the R is going to decrease rapidly? I'm not assuming it. It either does or it doesn't. But if you are keeping the pseudo-prones in there, If you are keeping the pseudo counts in there, you can make them as small as you want. It's not going, they're going to explode. Because then you won't get the exact cancellations. And then, you know, because there's so many. They have to cancel exactly, otherwise you can't. I have another concern. Even if when they're decreasing rapidly, as you increase the order until to the half of the size of the length, the summation, the number of terms in the summation would also increase rapidly. Right, it does. So we will never go. So we will never go. We can actually go to half the length for these ones. But we've never gone beyond order 4 up to this point. Remember, the rest of the world goes to order 2, and then say it's over. So in third order, just take the sum of the pairwise joint scores. Or if you do an energy matrix, the triplet energy score, rather than the triplet correlation integral. The fourth order, of course. In fourth order, of course, do this. So, to test the accuracy, what we're going to do is we only take a portion of the functional sequence of our multiple sequence alignment, even though we know all of them, and then try to classify those that are not in it. So, we'll first have a classifier for random sequences, then we'll have a classifier. I'm going to go through this quickly, otherwise I'm not going to get to proteins, for one mutant and then two mutants. And so, what I'm going to show you here on the x-axis is. What I'm going to show you here on the x-axis is the percentage of the set. And this is 0.1%. So this is 36,000 sequences in my set. This is 36. And this will be the quality score of the classifier. I'm starting here at 0.75. And so what you see here for random sequences, it's like a joke. So even the first order classifier, which you see here in red, is almost perfect. And there's almost no improvement going to higher orders. Improvement going to higher orders. That's because classifying random sequences is just child play. Here is classifying two mutants. So it's not quite as simple, but remember, I already started at 0.75. The random classifier is at 0.5. And you see, first order, I mean, you know, obviously this increases with the size of the training set. But the higher the orders, the second order does a lot. Third order still does a lot. The fourth, you know, taking fourth order into account doesn't do that much anymore. Doesn't do that much anymore for this data set. And then, in single, by the way, the way you think of this, this point here at point 9 comes from the fact that these are the functional sequence scores, the distribution, and this is the distribution of the two mutant scores. And so even though they're clearly distinct, there's an overlap, and that's why the quality score is not perfect. So, and then here's the signal ones. And we do get all the way up to perfect, but To perfect, but it does take some work. And here you see something interesting. If you have such a small ensemble here, you know, the singlet does pretty well, but the choir, you know, fourth order does terrible. So does three. Second order does well. And that's because this set is just too small. So the fourth order one, it picks up entropy, that is actually, it picks up something and thinks it's information, but it's not, it's entropy. Essential. So, but the good thing is that we can control if we have a small data set. I'm just not going to go to fourth order. So, you know, just at this size, fourth order already is the highest point. But as you go to smaller and smaller data set, you just simply are going to limit the order that you're going to take into account when constructing scores. All right, now I'm going to do proteins. Now, I have barely five minutes for that. So, the question we're asking here is: what is the effect of a single point mutation on the protein's function? On the protein's function. In fact, the POTS model approach was used to predict the mutational effect based on a multiple sequence alignment from public databases. So basically, you just take a protein, functional protein, and you look at PFARM alignment of that, and you're training your POTS model parameters on that. But then you have a data set of these single mutations and some value associated with it, they're measured. And then they're getting a score for that mutation, and they're doing a correlation process. That mutation, and they're doing a correlation plot. And the goodness of their prediction is the Spearman rank correlation between the scores, which are the functional predictions, and the actual experiments from the single-point mutants, which hopefully there are many. They published data on the mutation effect on 21 different proteins, and I'm going to show you only one of them because otherwise we'll be here forever, and calculate how well their prediction correlates with experiment. We use here the same data to make the same prediction. The same data to make the same prediction, but with you know our tool without any fitting whatsoever. Okay, so here's what that looks like: here's their result, their energy over here, and the actual ratio of the wild-type fitness to the mutant fitness. So these are all the neutral ones at one. These are beneficial mutants, and these are majority of delta. But they're doing pretty well, you see? They have a correlation coefficient of 0.69. So indeed, this method, after training, you know. Method after training forever and doing all kinds of really neat tricks, they're getting a pretty decent result. So, what we're going to do is we're going to show you an example of a WW domain which is binding small molecules. Here's a small molecule and this thing binds that. The fitness here measurement is the strength of binding to a particular signal molecule. And there it is. So, here is our first order score, and here is first. Score, and here is from the experiment this ratio. And like I said, we're using the same PFAM alignment that they graciously have deposited in the supplementary of their paper. And so, yeah, here, these are all the neutral mutants. And so, here is the correlation plot that we're getting. There's no training whatsoever for our first order score. It's got a correlation proportion of 0.594. Isn't it great? The number I showed you before was not this protein, it's a different one. But we can go now to second order, and we get a slight improvement. And we get a slight improvement. Now we go to third order and we get actually a very significant improvement to a correlation coefficient of 0.645. How does this compare? Well, let me show you. So we showed you, first order gives you this correlation coefficient here. Second order, slight improvement. Third order uh sorry. POTS first order is actually worse than ours. That's not necessarily the case, but for this protein it was. But their second order was actually worse than their first order. Was actually worse than their first photo, which tells you something that the information isn't really mostly encoded in the pairwise. Because when we did triplets, we ended up optic. Now we went to the literature and the same group with Deborah Marksburg and Harvard actually also did deep learning on this protein. You know where they ended up? They are. Almost exactly on top of ours. Which tells you that their deep learning approach was basically just slightly below our Slightly below or triplet, and we haven't even done quadruplet. We haven't done any optimization of pseudo-count of bias. The most important one is actually of correcting for common descent in the public database. Because if you have sequences that are common descent, then this will look like that's information when it's just common descent. Yeah. Okay, so honestly, it went a little bit fast for me for the past, you know, however it is. But it seems to me like this is almost like an accident, like a really brave accident. Like an accident, like a really brave accident. Like, you know, in one case, what you're doing is you have this sort of algorithm that's trying to fit a probability distribution over sequences, and you're looking at the sequence. No, there's a very good reason. Okay. Think of it this way. The sequences that are in the public database, for example, right, they are always reflecting functional proteins only. They're not bad. Proteins only. There are no bad proteins that lend you the sequence. So, what we're seeing there is the information is in there. There might be some variation in how well these sequences are doing in the database, but it's slight. So, essentially, it's like the information on how to do this is in there, but how do you read it? So, this method essentially says, oh, I see the monomer information, the pairwise information I'm adding, so I'm extracting the information out of that. And it's information about how to be functional. About how to be functional. So, in the perfect case, you know, I would, in a sense, have exact values of their energy measurement. If, in fact, this sequence that you show by itself is responsible for how to bind this molecule, then we should have perfect correlation. But that's, of course, not true. It's only one gene. This gene probably has more than just this one function of binding this molecule. And on top of that, it interacts with other genes. So, no matter what, in a sense, the What? In a sense, the multiple sequence alignment from PFAN is better than any single measurement that you can give me because these measurements are going to be noisy. So there's very good reason to believe that this spore should, in fact, reflect the function of the model power. Last point, well, actually, we expect Order 4 to actually blow this out of the water. Other applications I'm not going to show you. Applications I'm not going to show you. So I'm just going to go through the conclusions. So it's a new approach of function prediction from sequence alone and outperforms existing approaches and requires no training. And we can clearly do better by doing more things with the multiple sequence alignment. It's useful for predicting the function of any sequence in which information about function is encoded and annotated data is available. And right now, MSU is really interested in commercializing that technology because, like I told you, that is a license to print one. Like I told you, that is a license to print one. So I want to really thank Nitash, who wrote all the code and also helped out with some of the theory, without whom none of this stuff would work. So thank you for your attention. One or two more questions, and we can have the rest of the discussion during popular break. Could you go to the blog on the differences? A couple of slides. This one? Yeah. If I'm somewhat of a cynic, how how important is it to predict a spearman correlation from point six to point six four? To 0.64, this is not presumably a predictive model for the predictance. Well, think of it this way: if you can predict the score, then you can come up with an algorithm to actually generate high functional sequences. And the higher a correlation coefficient is going to be, the better you are going to be at generating sequences that are. Fair, I guess, I'm asking you. Also, to me, it seems like the R1 method is already good enough with. Is already good enough with very little effort, you're getting sequences that are good. Yeah, so keep in mind that this is just one protein, okay? And we obviously have to show that this is a very general method, and I believe we can get to do a lot better than that. But the holy grail, really, is to use this as a generative mode. If you have a deep learning method, you already know that running the network backward, like an auto-encoder, will generate sequences for them. And the Marx group has. Sequences for them. And the Marx group had basically abandoned all prediction stuff. It's just working on generation. So if you can generate a target sequence for, let's say, a drug and say, try this out, we're telling you that this is the best thing that you can get. And you don't have to do anything but put it into your assay. That's worth money. So the better this correlation coefficient, it's what this is, is it's a quality score for your ability to discern information from entropy. Is it possible to include information about the mutants here? You can do maybe half and half to test the model, but so are you asking about heuristics? No, somehow add that information. So in fact, as far as I know, the Marx Group has done this when they're doing deep learning. They are actually introducing other things like polarity and things like that, that they know are important. That they know we are important into the stuff that is being fed into the neural networks. Because I think what you're asking is: can you introduce stuff other than sequence or just the fitness step you would try and predict? You're subtracting from the score, it's a wild-type score, and because there's a ranked correlation coefficient, it doesn't matter what you add or subtract. So, because everything here is relative, but only for the prediction of the single mutant effect. If you want to predict the actual If you want to predict the actual catalytic efficiency, which is a real number, then that's a different thing. And I haven't shown you any of the work that we've done predicting, you know, go back. You know, go back frequency a sequence in the ensemble as a substitute, a proxy for how good it's going to be. And they're literally showing you a correlation plot between the and the frequency of the sequence in the, you know, they call it the prevalence. Because things that humans have thrown stuff into. And so you could have like a clinical trial and they're just dumping. A clinical trial, and they're just dumping 10,000 sequences that are very closely related in there. And that's this kind of thing that we have to carefully curate because now lots of correlations