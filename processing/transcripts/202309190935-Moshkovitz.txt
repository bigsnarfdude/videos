John, Justin and David Zachelmine. Um okay, let's see if this works. Yes. Uh so this talk is about random walks. Uh beautiful idea, beautiful algorithmic idea, an analytic idea, and it comes up in all of science. And we have this big sample space, and it's usually huge. We suggested we call it 2DBM, and we'll try to sample from it. From it, and we do that by setting up some graph on the elements of the space. Usually, this graph has a small degree, right? So the degree would be d, I would think of it as a constant. And this corresponds to some similar elements in the space. And you start with some elements, some node, and then you start walking randomly. Stop walking randomly and independently on this graph, picking a neighbor each time. And the hope is that you do it enough times, as then you do it about m times, and then sufficiently a bit more than m, and then you're close to uniform on the nodes. Okay. And this talk is going to be about the question of what happens. The question of what happens if the steps are not uniformly independent. So the steps are going to be correlated and they're not going to be uniformly distributed anymore. Can we still set up the graph such that there is some sort of mixing in the graph? Okay, so here is the Here is the one important definition, what are those Shawn Codra sources. So this is our definition for what does it mean that the steps are co-related. So here it is. So there are going to be n steps and it will be dedicated to m. Each step is over D. And the definition is that for every step, no matter what happened before, Before, so this is what happened before. Always, there is some entropy in the new step in the sense that the probability here, right, if it was uniform, it should have been 1 over d. But it may not be 1 over d, it can be 1 over d to the delta. And the important thing about this definition This definition. So the steps are extremely correlated, completely dependent on the history, the new step, which new steps are allowable, which are completely correlated. But the important thing is that there are no deterministic steps. So one could wonder, so every step is not determined. And one could wonder what's a problem with steps that are not determination. What's the problem with steps that are deterministic? Okay, you don't want all your steps to be deterministic, but okay, you don't gain anything from them. Is it so terrible to tolerate them? And it is terrible to have deterministic steps. And the issue is that deterministic steps not only don't gain anything for you, but they may actually cause you to lose something. And here is the counter, here is the example. So think of a stupid So, think of a stupid walk on any underwrited graph, and every odd step is just completely uniform, but then every even step just brings you back to where you were. You do nothing with such a walk, right? And the issue is that the deterministic step took back the entropy that you had. So, we don't like deterministic steps. Deterministic steps, every step is going to have some randomness. This is the meaning of this definition. This is a definition that was first suggested in the 80s by Santa and Basilani for t equals 2. Then showing Godrai, consider it larger t and here the degree will be more than 2, so Godrels show Godrai sources. Okay, this means. This means this sort of definition means that the mean entropy in the step condition on the path is entropy is in a logarithmic scale, right? So it's delta log D. And the vision is that, okay, so we have delta log D, so we don't have the full log D entropy in each set, we have this delta fraction. The vision is that we'll take many, many such steps. will take many, many such steps, and each one will gain about this. And overall, you should gain enough to be basically uniform on the graph. Okay? And we'll show indeed the result to this extent. Notice that the walk is no longer on coffee. It's very dependent on the last. Okay. Good. What, what? Delta is less than one, greater than one. Delta is very small. Think 0.001. Any other questions about this definition? Good. So this is a very basic question about A very basic question about random walks. What happens to random walks? If the steps are correlated, and people were wondering about this question. And the reason that no one kind of did what I tell you before is that the fault pro answer was that you can't prove anything. Definitely, there is not going to be mixing. And I'm going to show you the simple example. You also allow delta to depend on height itself and maybe you'll fix you can't have mixing, right? As in no matter which graph you set up, I can always find a short-order source that is going to lead to a vertex that's really not uniform. Not uniform. As in, it's stuck inside the set, that's not everything. So here is the argument. So take any graph and just bisect it. Colour some green, some blue. And bisect it in a way that the neighborhoods are roughly balanced. So for every vertex, some are green, some are about half of green, half of blue. So now, no matter where you start, you can always. You can always find many neighbors that are blue. So just choose only steps that lead to blue. So what happens is that no matter where you started, starting the next step, you just stay blue all the time. So no mixing, right? You're just blue. There is no hope, right, to prove anything. Prove anything, except that, so here is what we prove. We prove that this is essentially the only thing that can happen. What we prove is that for popular graphs, basically good enough expanders, if you do any Schun-Gondreich walk on the graph, then indeed you do accumulate all the entropy, this delta log D times. This delta log D times N is the number of steps. And the only thing is that you're not going to be uniform over everything, but you will be uniform basically over a set of constant fractions. Which is enormous, which is basically as good as uniform for all intents and purposes. And we saw with it it's needed. Okay, so this is the new theorem. Right, so Right, so the exact formulation is that you're close to having mean entropy, which is cool minus a constant, right, which basically means you're close to being convex combination of those uniform of a constant function in sets. Okay? Questions about this theom? What? No, no, the vertex that you get to is closed. The context that you get to is close in statistical distance to something with mean entropy, which is almost full. In the pseudo-ranones lingo, this means that the work is a deterministic condenser. So, right, so there are extractors where you're trying to be close to uniform, and then there are condensers where you say, Oh, I don't have the full entropy, I'm only less than full entropy. Usually, when people Less than full entropy. Usually, when people talk about condensers, they talk about entropy which is maybe m over 2 or something like this. And we really talk about something which is as close to full entropy as possible. It's full entropy minus concept. Can I ask a clarification question? So is this equivalent to saying that I'm uniform over a constant fraction? This is what I said, your consolidation. Your consolidation. Okay, more questions? Okay, so let me just application. So the kind of the motivation that we gave for this theorem is as follows. Suppose that, right, this is the pseudo-randomness motivation. You have any randomized algorithm that has no error, because it's a sub-constant. And suppose that you don't have... And suppose that you don't have access to bits that are fully random, you only have access to shoreboard life sources. So, those definitions of Santo-Pasilanian shoreboard life sources, this is how they try to model in the very early days of pseudo-anomnas. What does it mean that you don't have uniform bits, you have just low-quality source? This is the definition. So, suppose that this. Is the real definition. So, suppose that this is indeed a situation, what this theory implies is that you can set up this random walk. So, use these as the instructions for the walk, walk, walk, walk, get to some vertex, give the vertex that you reached as the randomness to your algorithm. Now get, do the algorithm, get an answer, and with high probability, the answer is correct. Because if your error is subconstant, and you are basically uniform. And you are basically uniform over a constant function set, then you still have lower even then. And the remarkable thing is that we only need one application of the algorithm. So usually in pseudo-randomness, you need to try a bunch of different randomness strings, apply the algorithm, and all of them take some majority. And he'll know just one application of the algorithm works. It means that you don't lose time in the algorithm. Usually, the deterministic algorithm is slower than the original algorithm. No, you don't lose time. Additionally, this could be not just an algorithm. This could be a protocol that interacts with the world. And in such situations, you sometimes cannot just run it many times. Fine, here, you just need to run it one time. You just need to run it one time. So, this was the motivation for this theorem. Let me just say, I think it's a very basic theorem, the expect that you can basically run random walks when the steps are correlated. I think that there should be other reasons to care about correlated steps. And I'm kind of giving this talk in the hope that maybe one of you would have some. Maybe one of you would have some ideas for situations, maybe down the road, where you want to set up correlated random walks. Okay. Okay. What I want to do next is first define the graphs, right? So basically, those suitable expansion properties that allow something like this, and then show you the analysis. Basically, I'll show you. Basically, I'll show you the full analysis, the full, with small shortcuts, I'll show you the analysis of the proof of this theorem. Very ambitious goal. But any questions before I do that? Capital D that you had? D was true to you, or D is like a constant order? No, no, this is a constant. Sorry. The whole thing here. The whole thing here. Even though you do little steps, eventually you'll uniform of a huge step. Okay, so I want to define this lossless expanders. The most extreme definition of expanders. And this is the picture. You have a k epsilon expander if every set of size k expands to nearly fully. Nearly fully, right? If the degree is d, at most, you can have k times d, distinct neighbors, and I want almost all of it, 1 minus epsilon of the k d. And probabilistically, by the probabilistic method, there exists such things, and even the epsilon can be one of the order t. Okay, so you're going to have. So, you're going to have only a constant number, not a constant fraction, constant number of collisions. So, probabilistically, there should be great graphs like that. Explicitly, we don't have such graphs. This is actually a question for the open problem session, a well-known question on expanders: can you come closer to those parameters? The best. The best construction that we do have gets epsilon, which is 1 of L d to the beta for this beta. We think that the beta should be 1. Okay. Good. These are lossless expanders. What we're going to do is set up a walk on a lossless expander that is right-directed and kind of goes from left to right. And kind of goes from left to right in this layered way. So we go. And what I'll show you is the analysis for the case where beta is very close to 1. We don't have explicit graphs like this. So, really, in the actual work that we set up, in the graph that we set up, we actually have this. This two-level structure. Well, you choose the steps in a big graph using a small expander that is optimal, but it's constant size, so we can find it by boot force or something. So we do this sort of structure. So we actually have explicit graphs on which we work. But as I said, in the actual proof, I'm going to assume that we just have access to this optional graph with beta, which is close to 1. But you might like not to say. Good. So, here is the proof strategy. We're going to consider Pi as the distribution over V of the work at step I. Okay, so this. Okay, so this distribution starts at one vertex, so it's like completely spiked. And each time that I walk, the idea is that it disperses the weight all over. And we hope that in the end we get to basically this uniform, almost uniform distribution of weights over the vertices, or at least uniform over vertical exact or something. So we're walking on this lossless expander. We're walking on an amazing lossless expander. Okay, so the first question is, does this follow from the definition of a lossless expander? And of course it doesn't, because the definition of a lossless expander says that we're going to if if we just walk randomly, we're not going to have uh almost any collisions, but there is still a constant probability of collision, maybe a smaller constant probability. Maybe a smaller constant probability. If you do a walk for n steps, and right, so if you have a constant probability to get collisions, you will get a constant function of collisions. So by no means, this just follows from Union Town or something. What people usually do when they analyze random logs on expanders, they just analyze the L2 norm. The L2 norm of this distribution. This allows them to move to eigenvalues and just analyze the eigenvalues. This is very nice when you have something to write the eigenvalues, which we don't have here. And also if you have independence, and it's very easy to understand what happens. And this is another big thing that we don't have here. So what we're going to try to do is just use the combinatorial like expansion property Like expansion probability, and yet bound the norm of this distribution, we're going to show that the norm keeps decreasing, right? And eventually, when it's sufficiently small, it means that we disperse all of the weight all over the graph. And the big idea in the analysis is that we can actually work with any norm that we want. That we want, but the norm that's going to be useful for us here is the one plus alpha norm for alpha, which is very small. Okay, so often when people analyze it, they move to actually norms that are larger, which what the effect of that is to put a lot of weight on outliers. And we want to do the exact opposite. We want to put as little weight on outliers as possible. Possible. And this claim, which I'll state without proof, says that we can actually work with any one plus alpha norm. And as long as we show that the norm becomes smaller than 2 to the minus alpha k, this is enough. This shows that the distribution is close to mean entropy, which is about k. So the thing to remember is that the exponent here gives you the entropy, and the alpha. Here gives you the entropy, and the alpha is the right normalization to put there because you are the one plus alpha norm. And really, the idea of this is just the distribution, the norm, is sufficiently small. It just means that almost always it's small. So close to entropy. Okay, so what we want to show is to show that there is this geometric decrease. Geometric decrease in the norm. So I started with some norm, and then the new Q norm Q is going to be lower by this. And remember, the 1 over D to the delta is exactly the entropy that we're hoping to gain. And the alpha is exactly the right normalization to have by this. So this would exactly show that I gain all the entropy that I'm going to get. That I gain, all the entropy that I need to gain. Okay? And this would work only assuming alpha is sufficiently small. Alpha needs to be smaller, basically smaller than dead enough to know. That is a quality of each step, of the entropy in each step. Okay? Can you tell us the thing in the stress expect? No, no, delta is like each step, instead of being one over d, it has one over d. D. It has one over D to the delta. Oh, so this is the quality of the step, one step. So we are hoping to get one over D to the delta, and alpha we know needs to be delta. So we'll exactly get everything. Okay? Good. I started a bit late, so hopefully I have five more minutes. Okay, so we need one lemma which I'll state. One lemma, which I'll state and I'll ask you to accept that this can be proved. And the lemma is basically that when you work with lossless expanders, you can work with lossless expanders with respect to any wave function. So whatever wave functions you want to give to your k elements. So the whole thing of lossless expanders. thing of lossless expanders is roughly each vertex here has just one vertex it came from. Lossless expanders just tell you that there aren't many collisions. K goes to okay and we want to bound the contribution in weight, in whatever weight we want, from elements that are not the unique neighbor. The unique neighbor. We find UVs just say that the one that maximizes the weight among everything, and we want to bound the contribution of everything else, and this is also bounded. Okay, so so lossless expander basically talks about what on the weights are one, and you can actually it implies a similar property when the weights are anything. Okay. Okay. So so now I'll do the analysis, right? The whole analysis of this COM is on one slide. And it's really this one big idea. Okay, so here is the so one simplifying assumption. I'm going to assume that each step is just uniform over d to the delta neighbors. Of L D to the delta neighbors. So, this is one way to get this property that we had. There would be other ways, we handle all the other ways. But this would make everything a bit slightly easier. Okay. So, what does it mean? It means that if I look at the probability distribution in the i plus 1 staff, this distribution, what I get from every neighbor, right of v. Right of v from every source that it had in what I had before, I get probability, which is 1 over d to the delta, and I get it both from UV, which is kind of we think of it as like the main neighbor, the main source of V, but I also get it from all the other possible sources of V. Okay, so this is just by definition. Definition. And here is the big idea. We're going to use, right? So we want to analyze the Q norm to the Q of this. So we're going to use the convexity of the raising to a power Q. And what we can convexity implies that any convex combination raised to the power Q is at most the complex combination of the. Combination of the Powell Q. And I'm going to apply it with a term that gives this, I'm going to give this coefficient half, right, some large constant, and I want to apply this with, all those will get about one over D constants. Right, which so what I mean is that I just say this is half times two d's, and all of these are these times. L D times 1 over L D, 1 over 2 D times 2d. And then I can get the following inequality up to constants. The Qth norm of this is at most these are the coefficients that I get. Here I had a constant coefficient, but the one over d to the delta was the term. Was the term. So it was raised to power q. And here I had to add, right, so I had basically 1 over d to the times d. So I get d to the q. Okay, so I do this convexity trick. Let me just simplify everything. And I got this bound. Bound. And what I need to do is to sum this over all the possible V's on the right-hand side. So let me do that. I'm going to sum all the V's here. And now the idea is that every UV is counted D to the delta times, right? So this thing is going to be D to the delta times the previous norm. Norm, so that d to the delta and the d to the delta cancel out, and I get exactly the one over d to the alpha delta that I wanted. Whereas the other terms, right, so I can use this lemma that I showed you, right, so the raising to the power q is the sum of weights that I give the elements in the set. And therefore, when I sum over And therefore, when I sum over all the v, I get kind of the epsilon of the lossless expansion times d times the times the norm. And I assume that I work with an extremely good lossless expander. So I have this d, right, which is basically this is like d to something very small. But the point is that this is going to be tiny because I took alpha. Tiny because I took alpha to be much smaller than delta. This is going to be tiny. Definitely tiny compared to what I have here. And I'm done. Okay, so maybe let me stop here. The paper contains all kinds of extensions of this theorem, but really it just proves this very, very basic theorem. Very, very basic theorem that I hope we find lots of applications. Okay, this is it questions?