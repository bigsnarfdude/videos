So first of all, I would like to thank my collaborator for this project. So two groups. One, Migal Byron, Yang Yang Wang, who's in the audience, Shijia Wang and Erin Jang. And a second group, Saf Said, George Lydianides, and Arnold Dusse. Douce. So the setup is that we're going to talk about in this here is Bayesian phylogenetic in France. So the goal is to approximate a posterior distribution over phylogenetic trees. So that's an old problem. And sort of just a background and motivation for looking at the posterior distribution is to quantify the uncertainty over tree reconstruction, among other things. Other things. And a twist here that's a bit less standard is real-time inference. And so, what do I mean by this? Let's talk about first so-called online phylogenetic inference. So, the motivation here is that the reconstruction of a large tree from scratch is computationally expensive. So, for complex models, it can be days or weeks. And that put limits in particular in the application. Limits in particular and the applicability of Bayesian phylodynamics method to monitoring. So, online methods have started around 2016. So, there are methods that allow you to add one more sequence to a tree or posterior distribution over n trees that has already been computed to get a new posterior distribution over trees having n plus one needs. And this is done without starting from scratch. From scratch. So, the current work on Bayesian online methods have a cost for adding one sequence that grows linearly in the size of the tree. And so that means that these methods will help quite a bit because you don't have to start from scratch, but they do still get slower as the tree gets bigger. So that eventually that will mean that you will lag behind real time if you're doing. Behind real-time, if you're doing monitoring, for example. So, what we call real-time phylogenetics is methods that would scale sublinearly in the number of leaves of the tree that you're adding one more sequence to. So today I'm going to talk about early stage work on a method that we conjecture will achieve log n running time for identity. Running time for adding one sequence. So, this method is related to tree placements. So, these methods are well developed and they are fast, but sort of the distinguishing feature here is that they do not update the reference tree and they generally ignore the uncertainty in the reference tree. So, the outline is first I'm going to talk a little bit about distribution continuum and some methodological And some methodological advance on how they can be used to facilitate posterior inference. And then I'll talk about a new type of distribution continuum to support real-time Bayesian phylogenetics. So let's start with a classical example of a distribution continuum. So that sounds complicated, but this simple example is very simple. I'm sure many of you are familiar with it. It's annealing. So it's an interpolation between a prior distribution. interpolation between a prior distribution and a posterior distribution for example a prior distribution over over here non-clock trees to a posterior distribution over non-clock trees where you just do a geometric interpolation between the prior and the likelihood and let's start with talking a little bit about how can we use a continuum to infer a posterior distribution and i'll talk briefly about two main methods Briefly, about two main methods that we're interested in. One is called parallel tempering or the replica exchange. The idea is that we take this continuum of interpolating distribution, we discretize it to these points in the space of distribution. Here's the prior, here's the posterior, and we maintain one copy of tree for each one. We apply MCMC move in parallel, and we use metropolized swaps to propagate. Uh, swaps to propagate information from prior to posterior. Okay, so we've been doing work recently on a flavor of this called non-reversible parallel temperature. So I won't have time to talk in detail, but it gives a fresh perspective on these parallelize algorithms. A second method that's used quite a bit for using these continuum of distribution is annealed SMC, which you could also call sequential change of measure. Sequential change of measure. So, here the idea is that we're not adding sort of a point one at the time. Instead, we're walking on this cretization of that continuum of distribution, starting with a population of particles that are at the prior. And then if we have a at each step here, we have a weight population of weighted particles, so a bunch of trees with weights. And using MCMC moves, re-weighting, and resampling, we can move this population from... move these this population from one uh from one of these discretization points to the next one okay so one thing that you may have noticed is that for these two methods we both discretize so you may ask so why do we care about the continuum if we discretize this continuum of distribution the the answer to that is that to achieve high performance for both of these methods it's critical to use uh what's called an adaptive discretization so basically to uh to So, basically, to do optimization over these points on the continuum of distribution, since the continuum of distribution will often change more quickly, for example, close to the prior, and there might be some phase transition at intermediate points. And so, in the annual SMC method, sort of the state of the art right now is a method called conditional ESS that Yang Yang talked a little bit about on Monday. And a second method that we recently designed for parallel tampering. Designed for parallel tampering is what we call communication barrier-based methods. So, just to give you a rough idea of what that is, so it's a method where it's really, so it's tailored for pyrol tempering. And we estimate a continuous intensity function, lambda, which you can think about. So, it's a function that lives on that continuum of distribution as the x-axis. On the y-axis, it encodes. On the y-axis, it encodes how quickly the distribution are changing along that consumer. So, here's an example for an Ising Mole, for example. So, the distribution is changing faster close to the phase transition. And so we've shown that the optimal discretization is to basically estimate that function and use spacings of points where we achieve constant area under that curve. And people typically basically look some kind of Typically, basically, look at some kind of geometric spacing where it's very packed on the left and gets less packed on the right. But we've seen that, and that would correspond to a monotonic decreasing function lambda. But we've seen in a range of models, and I include a couple of phylogenetic models, that the shape of that function varies quite a bit for different problems. So it's worth estimating it and finding optimal discretizations. So, some results briefly. So, an example on Bayesian mixture model. So, that's a classical problem which can be made hard and is well understood. So, we know by exchangeability that the posterior distribution over the two mixture components should be identical. And so, this is something known sometimes as label switching. And so, we've essentially these two rows, if you Essentially, these two rows, if you capture the posture correctly, should be identical. So, from the method from the last slide, this adaptive discretization for non-reversible parallel tampering, we obtained in a certain time budget the correct posterior. And here we show a bunch of other methods that are taking longer or equal to run. So, you see that a single MCMC chain fails, annual important sampling fails because these two distributions are not the same. So, older discretization. So, older discretization methods fail, and annual SNC works with these with adaptive discretization, but requires a bit more time. So that's the takeaway point is that parallel tampering and anneal-SMC really need a smart discretization of the consumer distributions. Sort of a side product of these methods that I've the two methods, parallel tempering and anneal-sMC, is that. Tempering and an ELSMC is that they both give fast converging normalization constants. So if you're comparing models in a Bayesian framework, we know that these normalization constants can be tricky to compute. So these methods give fast converging estimates. So X is number of iteration, Y is an estimation of the log normalization constant, and they quickly converge. Okay, so. Okay, so this was sort of motivation. So we have methodologies to handle this continuum of distribution. They've improved quite a bit in the last few years. Now, let's talk about a distribution continuum specifically designed to support real-time Bayesian phylogenetic inference. And both of the method I've described in the first part can be applied to that continuum. So that was the old example. So just annealing from prior to posterior. The example I want to talk about. The example I want to talk about now is a continuum that we could call interpolated leaf addition. So we start at one n, which is a prior distribution over two leaves of a tree. So just one branch length, essentially. And as we progress, what we do, we add one leaf at a time. So here at one point in the continuum, we have here a posterior distribution over four leaf tree. And here we add a Here we add essentially one new leaf according to a conditional prior. I'll describe this soon. And then at the beginning here, basically you think about as having no data attached to it. And we slowly anneal the data just for that one leaf. So it's in a sense some sort of local annealing where sort of I think the easiest way to think about it is that imagine that each leaf as an artificial As an artificial sequencing error model. So maybe you really have a sequencing error model, maybe you don't, but imagine that we introduce one as an auxiliary distribution. And basically, a new leaf starts with an error probability equal to one. Essentially, no matter what's the internal state, you put a fixed distribution on the tip nucleotides, and we slowly reduce the error on that pseudo-sequencing error model. Sequencing error model. And so this creates an infinite continuum that adds more and more leaves to that tree. And let me talk briefly of how we could not design an algorithm that used that sequence of distribution to do real-time phylogenetic inference. So let's think about the specific example of sequential Monte Carlo. So at each step, we have a populations of a population. Populations of a population of trees, um, each with n leaves, and we're going to add to each of these particles one more leaf. Okay, so I'm going to focus on only one of these particles to illustrate it, but imagine that this is done in parallel over maybe a thousand particles or so. So, first, we're going to sample the location for the new leaf, and this is done from the conditional prior, which is basically it's not all prior that have this, this, that you can easily. have this this that you can easily do this but for some prior you can say if i've generated n leaves according to the prior find the distribution the prior distribution of n plus one leaf given uh the given that that tree that i've already generated from the prior here the tree is actually from the posterior but things work out essentially the the the the quantity you need is the conditional prior and since it's this is true essentially because the the the data is fully annealed at that point so you can treat it as Anneal at that point. So you can treat it as just one more pseudo observation with no data. Now, once we've added this leaf, then we're going to do MCMC walk on the tree, but a different kind of walk where we just perturb the tree locally. Essentially, we move that one tip that we're adding. We're making small changes to the tree, but just in the neighborhood, so that we can do these changes very quickly. And so this is like a random walk. And so, this is like a random walk on the tree, which is a bit unusual. Typically, with MCMC, we have a walk on the tree space. And this is more like a walk on a single tree, which is simpler to analyze, which is what we're doing right now. And as we move that one additional data point, we slowly anneal it back to remove the pseudo-sequencing error. Sequencing error. So basically, now these leaf walks are combined using the modern SMC toolbox. So we would use at the same time adaptive discretization that I talked about in the first half, adaptive resampling, parallelized implementation. And so we have already all this implemented. So the package that we're creating, it's not ready yet, but it's moving. It's called FiloStream. So it's a real-time Called phylostream. So it's a real-time phylogenetic SMC method. And as I described earlier, so it's a new method to add one newly sequenced leaf to each particle and anneal that to a population of particle over n plus one leaves. And it comes with the standards that's called guarantees, so consistency, unbiased normalization constant, central limit theorem. And in terms of the progress so far, so we have so So we have so far only one class of moles supported. So the standard GTR plus I plus gamma, and moreover, with the fixed rate matrix. So it's still a prototype. It has assumes a non-shock tree prior. So it's based on a Parliamentic programming language that we've been designing from the last four years that enables rapid mode prototyping. And we've done a suite of tests to Of tests to sort of check the correctness of the implementation using formal methods. Just one quickly future directions that we're thinking of how to relax these strong restrictions on the model class. So suppose we're in the context where we're updating a population of phylogenetic tree to approximate a posterior based on model one. Let's call model one this vanilla non-clock tree that I've just described with. three that I've just described with GTR plus I plus gamma. But later on, we want to investigate model two and soon model three, we have a lot of models that we're interested in. So the naive approach would be, okay, so we've done this maybe a real-time analysis up to now and we have model one posterior. And model do now we're interested in model two. So what we would do, we would start from scratch with model two and go all the way to the posterior. Now, a future direction, which Now, a future direction which falls off that sort of continuum of distribution view is model bridging. So we would maybe carry only a small collection of model that are updated in real time. And if we can create continuous small interpolation between a large class of model, this means that we might be able to maybe just look at the population of topologies or something like this and anneal them to or some kind of continuous process. Of continuous process to another model. That seems like another way to go from real time on a single model to real time to many models. So here's some reference. The first one is published on this annual SMC method, not in real time. And we're building on this for the real-time version, which you can spy our progress in follow stream. So hopefully the progress will go a bit faster. Hopefully, the progress will go a bit faster after teaching is over. And this paper on the adaptive discretization for apparel tampering is in the archive under review. So time for, I can take a few questions, hopefully. Okay, thank you very much. We've got a few questions in the chat. So Aaron's asking, can you explain the meaning of the lambda a bit more? And does it have an information theoretic interpretation? Yes. Yes, so lambda, in terms of how you would compute it, it's actually fairly simple. So, if you want, so beta here is a point in the continuum, so it's an anneal parameter between zero and one. So, what you would do is you would look at, imagine that you have two sets of samples. So, that's the naive way to compute it. There are ways to do it with only one set of sample, but imagine you have one set of sample X beta for at temperature beta and another set of samples. Temperature beta, another set of samples at y beta. Maybe you split your sample into two halves or something like that. And if you would compute the expectation of the absolute value of the difference of log likelihood, then that would give you the numerical value for lambda. So as you can see, it's a fairly concrete thing that there's actually better ways to compute this, but that's the simplest way to describe. In terms of In terms of information theoretic interpretation, so we're looking at there are some inequalities with KL divergence, symmetrized KL, and we're looking at sort of deeper thinking in terms of the geometry of these continuum of distribution. But that's more in the making. But that's a really good question. There's definitely information theory connections here. And from Leonid, what's the basis for the hypothesis that it takes O of log N per leaf? Yeah, so there's two components to it, which is one of them is that the, so let me find a slide to support this. One of them is that if we cache all, there's a way to do the Felsenstein pruning where messages are kept on both directions. So that's used in many paths. On both directions. So that's used in many packages already. And that means that if you do local moves, each move costs order one. Now there's the second part, which is we don't want to make too many moves. And that's here the conjecture part. So we want to, since it's a simpler random walk, so it's a random walk on a tree, which is almost fixed. So there's some local perturbation, but these random walk, we're hoping to get fast mixed. We're hoping to get fast mixing results on that tree. That essentially, so current, just a comparison with current online methods, they would compute a likelihood for each possible tree attachment. So you have to look at each edge of the tree. Instead, what we want to rely on is a random walk on that tree that hopefully visits a small set of edge and is guided by the Guided by the likelihood to find its place.