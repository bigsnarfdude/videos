Virtually. So I've put an alarm at 35 minutes and then we go from there. So, okay, so I guess the basic story is really simple here. This started by me thinking that this wasn't even a research problem at all. Try to put the full screen, but I think you tried it already, right? Yes, but I can sorry, I can maybe stop here and start. Maybe stop sharing and start over again. Okay, so share screen. If I share the full desktop, let's see. Oh, this is perfect. This is better. Yeah. Yes? Okay. Good, good. All right. So, so, yeah. So, this started with an economist, a friend of mine a few years ago, telling me that one of the latest rages in high-dimensional In high-dimensional regression, was this literature on estimating treatment effects when you have many, many controls? And there's this method or family of methods that started off being called double lasso and then generated a literature, I think they call it now, double machine learning. And the first time I heard about this being the idiots that I am, I thought, you know, this is just a frequentist thing, they're sort of Frequentistic, they're sort of walking around circles. This put is so easy to solve by applying standard Bayesian model averaging or Bayesian model selection. But then we started looking into this and we realized that some of the issues went deeper. We realized there was a Bayesian literature on this as well. And so this is the story of today: a problem that seemed very simple on the surface, but actually wasn't so easy to fix. It affixes. So the basic setting couldn't be simpler. We want to estimate the effect of T treatments, so T variables of interest, but we want to adjust for the potential effect of J controls. My interest is going to be in situations where T is smallish, so there's just one or a few of them, but J could be very large. You could have many controls. We're going to frame this within GLMs. And so we're going to have that linear predictor. And so we're going to have that the linear predictor eta i looks something like like this. So alphas are the treatment effect that we want to do inference on, and then the betas is the guys whom we want to somehow decide whether they need to be included in the model or not. We could have a dispersion parameter for Gaussian regression, we wouldn't for logistic and Poisson regression, so the usual story. Sort of what's coming up is that, of course, by now there's But of course, by now there's tons of high-dimensional regression methods. But if we apply these methods directly to this problem, there's an issue that one often runs into underselection. I'll explain why in a second. And there's new methods that have been coming up in the past five, ten years that fix this under-selection. The issue is that they often run into over-selection, and this over-selection Overselection and this overselection causes a pretty bad variance inflation. And so there's no satisfactory solution that we could find at least. And so we tried to come up with our own fix to the problem. So somewhat at the heart of the issue of what happens with standard high-dimensional methods, say LASSO, standard Bayesian model selection, is this underlying assumption that one is encouraging solutions. One is encouraging solutions that have two characteristics. One is that it's sparse, so we don't have many variables in. I mean, there's debates about this. There's the recent articles in the latest issue of Econometrica, there's a paper called The Illusion of Sparsity, showing that in the social sciences, sparsity is probably the exception rather than the norm. And a second part of the construction is that often one considers all controls or all variables to be Controls or all variables to be treated equally. So, you know, if we want to encourage sparsity, we don't discriminate between potential controls. So this actually creates some problems, not theoretically, like if you work out the asymptotics of these things, like it all works and you get wonderful minimax estimation rates and model selection recovery and these things. But in real life, so for finite end, this is actually an important issue. Then this is actually an important issue. And it is an issue intuitively, it's very clear why. Because, see, if the treatment of interest is highly correlated with the control, any penalized shrinkage method is going to penalize you for including both of them in the regression equation, unless n is large enough, right? You don't need both of them to predict, only one is enough. But if both of them truly have a non-zero effect, then failing to... Have a non-zero effect, then failing to include one of the other, particularly when they're highly correlated, can severely bias inference. Okay, so that's what's at the heart of the issue, really. So to show this, let me start with a super simple example. Then I'll show you that this that's going to be simulated, but then I'll show you guys that this matters in real life too. So this is not just a statistical construction that we do for fun. We've seen that this matters or can matter in some situations. Or can matter in some situations. So I'll start really simple. I have one treatment. There's 49 controls. Sample size is 100, Gaussian errors. The treatment has an effect, really. That's alpha star equals one. All of the controls, so six of the controls are associated with the outcome. The rest do not have an effect on the outcome. The error variance is one, this phi star. But we designed this such that the But we designed this such that the treatment is correlated with controls one and six. So, see, this is a full confounding situation in the sense that all of the controls that have an effect on the outcome, all of them, they're also correlated with the treatment. Okay, so there's full compounding. Now, if we do LASO with post-selection inference, this is the Li et al. Analysis of Statistics paper 2016. What happens is What happens is that the treatment variable is included, but none of the controls, none of the six controls are included. And standard BMA, well, I used my favorite MOM priors and beta binomial priors on the model space, but the same thing for other priors. So same thing. You include the treatment, but you fail to include the controls. If you look at what the estimate for this parameter of interest, Parameter of interest looks like. For Oracle OLS, the interval looks fine. For LASSO, it's a bit off, but not horrible. But now for the BMA interval, it's totally off. As you can see, the true value is one, the interval is around two. And this isn't even as bad as it gets. It could get arbitrarily worse. You could fail to detect the effect if it's there. You could claim an effect when it's not really there. So all sorts of nasty things can happen. sorts of nasty things can happen. Now I added here two methods, this double machine learning and Bayesian adjustment for confounders. These are methods that are designed for treatment effect difference when the controls are correlated with the treatment. And we can see that these two methods, at least in this particular situation, they do a very good job. They get an interval that's essentially the same as Oracle OLS. Oracle OLS means that I fit OLS on OLS means that I fit OLS only on the variables that truly have an effect. Okay, so these methods seem brutal, right? Well, not quite, not in general. So let me explain what these methods do. And these are just the two options that we chose from a large literature. But essentially, the philosophy of what the literature is about, I think, is captured by these two methods. Is about I think is captured by these two methods. So, so this double LASSO method, what it does is well, it starts by in step one, it fits via LASSO, a regression on the outcome with all the treatments and all of the controls. So just LASSO. Then in step two, it does a regression for the treatment now on all of the controls, again via LASSO. Okay, so you've done two separate regression equations. Okay, so you've done two separate regression equations, one for the treatment, sorry, one for the outcome versus treatment and everything else, the other for the treatment versus all of the controls. Each of these two regressions will have selected a different set of variables. So you take the variables, any control that was selected in either of these two exercises. So you take all of them. And then in step three, you regress the outcome on the treatment and all of these controls that were selected a priori in these two steps. A priori in these two steps. There's a theorem, so the main reason why this became popular, I think, is because there's a theorem that then the three-minute effect estimate that you get is asymptotically normal and unbiased, I think. The variance actually is not very good, but that's not much discussed. And that's part of what we want to point out. But you get normalities, so you can actually get intervals and things like that. Then Bayesian adjustment for compounders is sort of like Adjustment for compounders is sort of like a similar idea. It's just that you also do two regression equations: so, outcome, one for the outcome, one for the treatment, but now you induce prior dependence between the inclusion probabilities, so the inclusion indicators in these two regression equations through some joint prior on the model space. So, again, the main idea here is that what you do is any control that is correlated with the treatment, you give it higher chances for Give it higher chances for being included in the outcome equation. And this is so that when you then do your whatever sparse thing that you do on the outcome, you're no longer being hurt by the treatment being highly correlated with the control because you're forcing its inclusion or encouraging its inclusion. So that sort of is designed to fix the under-selection problem that I showed you guys in this example a moment ago, was going on. Now, the issue with this, and this is a question I ask to students. And this is a question I ask to students in class. And there's normally one or two, the smart ones, I guess, who guess it. Like, what's the problem with taking these approaches? And well, the problem, I think it's very clear for the Davalasso, is that if you have controls that are correlated with the treatment, but have no effect whatsoever on the outcome, you're going to select them in step two. And so you're going to be adding to So, you're going to be adding to the regression equation for the outcome totally irrelevant variables, but worse than that, they're highly correlated with the variable of interest. So there's going to be a huge variable inflation potentially, right? So that's the story, really. That's the heart of the story of why I think we need to do something else. Just to show you guys now the same example that I showed you before. That I showed you before. Everything is the same, but before I had full confounding, and now there's no confounding, meaning that all of the controls that are correlated with the treatment, none of them have truly an effect on the outcome. So I repeat the exact same exercise as before. Now you'll see that here, BMA is doing a wonderful job. It sucked before, but now it does a wonderful job. It recovers essentially the oracle of LS. Whereas now you see that BML and BAC, which were doing a great job before, now they incur a huge. Before, now they incur a huge variance inflation. To put this in perspective, the interval is what, like, six, seven times larger. So that means that you would need six squares, so like somewhere between 36 and 50 times the sample size to get the same estimation accuracy with these methods. Okay, so if you have to spend 50 times the money or the time to collect data, this is something that matters in practice. All right. All right, so that's the story. And if you look exactly at what's the issue with VML and BAC, it's exactly what I said, that it overselects. So I guess the high-level point here, then, let me just drive the basic notion here, is that high-dimensional methods look for a few needles in a haystack, and all straws are equal. These high-dimensional treatment effect methods have been designed to. Methods have been designed to consider that certain straws are different. So, that the controls that are correlated with the treatment are treated non-exchangeably, right? So, some straws are different. What we propose is actually that, well, look, first, should we be assuming sparsity at all in the first place? So, there could be many needles, right? We want to consider that maybe certain straws are different. So, maybe we need to treat the inclusion. Are different, so maybe we need to treat the inclusion of some variables non-exchangeably, or maybe not. Maybe it should be exchangeable, and we want to be able to learn this from data. So, we want to move away. So, this is in an O-base type of meeting. I guess this is okay. We want to move away from fully vision formulations. We want to actually sort of learn these things from the data rather than enforcing it through the prior. So, I want to see whether things should be exchangeable or not, whether the prior should be sparse or not, and learn that. Prior should be sparse or not, and learn that in an empirical base type of way. Part of the contribution will be developing computational algorithms, and I'll explain now how we got about doing that. Essentially, we use expectation propagation, and then you get into the world where you can apply this in real life. So, you know, if I had to put a marketing punchline to this, is that what we're looking is, we're looking, the method is looking for more. Method is looking for more than a few needles in a haystack, potentially, where some straws are possibly more promising than others. Okay, so we're departing from the few needles in a haystack paradigm to sort of allow ourselves a bit more flexibility. Okay, all right, so how do we do this? It's simple, really. We're simple-minded people, so we couldn't do anything that was too hard. Think that was too hard. So, what we're going to do is we're going to put a regression, sort of a modified logistic regression on the prior inclusion probabilities. So, this pi j is the probability, the prior probability that we assign for the inclusion of a control. It is regressed via a form of logistic regression. This row is just sort of as the row is bounding the prior inclusion probability away from zero and one, so that we never totally. From zero and one, so that we never totally exclude or include a variable a priori. But other than that, it's logistic regression. And so, what we do is we regress the prior inclusion probability on these, on features extracted from the data, this F. This F, so F J T is basically measuring the degree of association between a control J and a treatment T. Right, so let's look at this picture over here. So, on the y-axis, I have So on the y-axis, I have the prior inclusion probability. On the x-axis, I have a measure of association between the treatment, say this is treatment one, and each of the controls. To get this measure of association, this is a feature that we extract from the data. For instance, what we do by default is some LASTOR regression. So we do a LASO regression of the treatment on all of the controls. Based on that, we get a regression. Based on that, we get a regression estimate. We get the absolute value of that, and that's the x-axis over here. So, the feature. So, for example, if you look here at the black line, at the solid black line, what this is saying is if there's a control that has zero association with the treatment, its prior inclusion probability is given by, what's this, around 25-30%. For features, For features that have, so for controls that have higher correlation, then this is increasing, this period inclusion probability is increasing according to the solid line. So that's if one were to estimate this logistic regression parameter, theta, to be positive. If we were to estimate it to be zero, then the paradigm probability would be the same for all covariates, and that would correspond to the exchangeable case. Potentially, even we want to allow ourselves for this regression parameter to be negative so that controls that have high association with the treatment are disencouraged to enter from the regression equation. This was not possible from the earlier methods. We want to allow ourselves to have this specificity. And so, what we want to do is we want to learn the theta parameters driving this logistic regression to learn from data whether we. Logistic regression to learn from data whether we should be encouraging or disencouraging variable inclusion when these variables are highly correlated with the treatments of interest. Okay, so that's it really. Now everything, all that's coming up is me going on how we do the computation and showing you some examples. All right, so I guess I'll sort of skip a little the details on the priors. So we used more or less a standard priors, so priors. Standard priors, so priors and the regression coefficients that that spike and slab priors that factor across the included coefficients. We're using these non-local priors, MOM priors, just because I like them, but people have used something else. And I guess that one main feature here is that, see, all we're doing here is learning these hyperparameters, these theta parameters that featured in this logistic regression here on the prior inclusion probabilities. On the prior inclusion probabilities. After we have learned them, we have some theta hats, then all of the inference that proceeds is totally standard. Like this will be standard Bayesian model averaging, Bayesian model selection inference, conditional on theta hat. So really the only issue that comes up is learning, is this data app. To do this, we're going to do e-base. So I'm going to maximize the marginal likelihood of the data, given these theta parameters. Even these theta parameters. This can be easily written as a sum across models of integrated likelihoods across models times prior probabilities, or equivalently, as I have in this equation here, this P U is the Can you pause for one second? Like we just want to change the resolution on the screen, like your indices came through really badly. Okay, I don't need to do anything. Okay. I don't need to do anything, yeah? No, no, just it's the resolution on our projection. Okay. Oh, much better. Now we have a crisp and clean picture of you. I mean, earlier we didn't really see you. So now you can actually. It's about the text, like specialty formulas. Thank you. I hear you. So now you can actually spot all of the mistakes. Splat all of the mistakes in the stuff that I wrote. Okay, so I want to maximize the marginal likelihood. The marginal likelihood can be written as a sum of posterior model probabilities under a uniform model times some prior model probabilities. So, this is what the objective function looks like. I want to sort of drive down the notion that doing this is quintessentially different to Quintessentially different to doing full base. So, in a full evasion framework, you might consider setting a hyper prior on this data parameter and then doing full base. But if you do that, then the posterior model probabilities that you get, they're still, of course, proportional to the integrated likelihood given the model times the model prior probability. The model prior probability is equal to this integral that you have in the last line. So it is expressed as a as a so. As a as a so it depends on the hyper prior that you've set on the hyperparameters, but it's integrated out a priori, right? So this P of delta gamma, it doesn't depend on the data. Like you can see it very clearly from this from this equation. So there's no learning of this hyperparameter from the data. So if you do full base here, you're just changing the prior on the model space, but you're not learning from data whether. Learning from data whether that logistic regression should be pointing upwards or downwards or staying flat. So you still get into an exchangeable type of scenario. So this is not no good. And I want to point this out because I think we payments tend to think that we can just put hyperparators and everything and then things work. Well, I think this is a case where this just doesn't happen. Okay, so we're going to do empirical base. I'm going to plug in point estimates. Base. I'm going to plug in point estimates for the hyperparameters. And I claim that this, that this you're not going to be able to get the kind of inference that we get if you were to do full base. Okay, so I know this can be contentious, but let me move on. Okay, so the problem computationally is if you look at, sorry, let me move back here in this the first line here, you see that maximizing this function with respect to. Maximizing this function with respect to theta requires a costly integral, a costly sum across models. So, gammas and deltas are the inclusion indicators for the treatment and the controls. So, this is a sum across all possible models. So, obviously, this is going to be hard to do. Now, proposition number one is that actually, if you look at the gradient of the objective function, it can be written as a sum of j terms. So, j is the number of controls. terms. So j is the number of controls. So we've gone from two to the power t plus j terms in a sum to now a sum that only has j terms. And each of the terms involves these features, which are pre-computed, these f's and then a difference between the posterior probability and the prior probability associated to this to this value of theta. So this is a massive simplification, but still it's not fully desirable because see to compute this posterior model. Because see to compute these posterior model probabilities given theta, I would typically have to run an MCMC for each possible value of theta that I'm considering. So, even that is a bit problematic. In fact, also, the objective function is multimodal. So, you know, even though you have the gradient, it's not necessarily going to bring you to a global mode. So, instead, what we do is expect. Instead, what we do is expectation propagation. So we approximate. So let me go two slides back. So within this sum, there's this P U. So these are posterior model probabilities under a uniform prior. So we replace it by P U hat, which factors brutally across all of the variable inclusion indicators. We need to figure out, so this approximation, this variational approximation depends on this. Variational approximation depends on these parameters, S and the S T's and the QJ's. So, proposition number two is that the optimal value of these guys to minimize KL diversions is given by taking the posterior inclusion probabilities under the model that has theta equals zero. That's just the model that has a uniform model prior, basically, under our setting. Okay, so the EP approximation can be obtained very easily. Approximation can be obtained very easily. You just obtain you run a single MCMC now, not a different MCMC for each possible theta value that you consider, just a single one. For theta, equals zero, so just a uniform model prior. And from there, you estimate this S and this Q. And once you have this, then the second part of the proposition is that the estimated theta is given by maximizing a sum of Maximizing a sum of J terms, and this sum is now super cheap to evaluate because it just involves these q hats, which are pre-computed, and these pi j's, which are super easy to evaluate. And so now the objective function is something that's really, really cheap to evaluate, and now you can afford even to do grid searches or whatever. You can look at the gradient as well, and Hessians, we have expressions for this, but basically there's now been a brutal simplification. A brutal simplification. And if you look at the actual shape of these posteriors, on the left, you've got the exact objective function for empirical base. So the exact marginal likelihood, the log of the marginal likelihood, because it's multimodal. The dominant mode is actually preserved in the EP approximation. And so if you maximize the EP approximation, you can see that you're going to get a very similar solution. But this is now way, way faster. But this is now way, way faster to do computationally. Okay, so I guess all I'm saying is that we have formulated a model and it's computationally practical to do. Okay, so it's not worse than your standard Bayesian model selection exercise. In the interest of time, I think I'll skip some further details. I'll just mention that, so for instance, one of the things one needs to compute in One needs to compute in Bayesian model selection, of course, are integrated likelihoods under each of the models. For Gaussian regression, that's kind of okay, but for logistic, boson, etc., there's no closed forms. And even Laplace approximations are typically too costly because if n or p are very large, then you have to, the optimization exercise takes long. And so there's this paper that we just got out at Series P on the approximate Laplace approximation. That's sort of like a cheap approach. That's sort of like a cheap approximate inference type strategy to get a very, very fast approximation to integrated likelihoods. And so, yeah, just wanted to advertise that. Okay, so combining all of these tricks, things are doable computationally. So let me show you the sort of like real life example where I try to argue that this is not just. Where I try to argue that this is not just statistical mumbo jumbo. So this is these are data from a survey in the US. We took two years, 2010 and 2019, because we didn't want to have to deal with the effects of COVID. We wanted to look at potential salary discrimination. So the outcome is going to be the log hourly salary that a person gets. We've got huge sample sizes for both years, about 60,000 in both years. 60,000 in both years. We're interested in understanding how salary is associated with gender, black race, Hispanic ethnicity, and being born in Latin America. So those would be potentially discriminatory factors. But we want to control for many things, like the state, the economic sector in which you work, your education, et cetera, et cetera. So there's 278 controls that we want to add. Controls that we want to add in this regression. And so the idea is that, you know, after you control for all of these things, if there's still a negative association with being a female or being black, for instance, with salary, then that's probably due to discrimination, not due to your having a lower education or something like that. And of course, this is a situation where the treatments are super highly associated with the controls, because unfortunately, depending on Because unfortunately, depending on your race, you're very likely to have worse opportunities in terms of education, have maybe less context to get your good job and so on. So, okay, so that's the exercise. Now, let me show you the results. You've got out of the four treatments, the two that have a clear effect is gender and the black race indicators. rays indicators. So let's focus first on the black intervals. So the black intervals are on the those on the real data. The gray intervals are after we add some artificial controls that were not in the original data, but we just simulated them to be correlated to the controls to see what happens as you get more and more controls that are confounded with the treatment, basically. So just looking at the So, just looking at the black guys, one thing that's obvious. So, we're considering four methods, OLS. We can do an OLS, right? Sample size is 60,000. So, it feels like OLS should be good enough. But anyway, we are going to do this dial machine learning, standard patient model averaging, and what we propose, this confounder importance learning. You can see that all methods in 2010 forecast a or infer a Or infer a reduced salary for females. And likewise in 2019, although the gap is not quite as marked in 2019. One thing that you notice is that as you start adding artificial controls, you can see how the intervals get way wider for OLS and DML to the point that in 2019, you would even maybe not even detect that there's a negative association between. That there's a negative association between gender and salary. But you know, here, at least in the original data, the black guys, all methods agree. Now, I guess the more relevant one is for the black race. So you can see that here, at least the squares would have not detected that there's a negative association between the black race and salary in 2010. And most methods, well, would have not detected that even in. Would have not detected that even in 2019. Like BMA, for instance, fails to detect that. Note that DML and BMA, once you start adding artificial controls, these gray lines, they also stop detecting that there's an effect of the black rays. In one case, this is due to over-selection. In the other case, this is due to under-selection. Under selection, okay, but in either case, you end up getting essentially what we argue is not the right inference that you should be getting. And sort of, you know, what we proposed, this CIL guy, you can see that it remains remarkably scalable to adding these artificial controls. Okay, so that's sort of, I guess, my main story on the salary data. We looked at slightly more things. Slightly more things. So, since we have a full Bayesian model, we could look at posterior predictive quantities. So, we could try to obtain an overall measure of salary discrimination. We do that by taking the expected log salary for someone with some given treatment values relative to the marginal quantity marginalized across the possible treatments. And so, yeah, so basically, this is sort of like a standard. Yeah, so basically, this is sort of like a standard posterior predictive type exercise. And so this quantity essentially, if it's one, it means that there's no variation in salary associated to the potential discriminatory factors. And if it's greater than one, it means that there's an association. So this is what this posterior predictive distribution looks like in 2010 and 2019 on the left. So we can see that clearly this. That clearly this has gone down over time. That's good. We can also, since we added effects for each of the states, we can do state level inference and we notice a catch-up effect. So for instance, you can see Ohio down here, which was very, well, it was fairly worse than the other states in 2010, but then in 2019, it's sort of caught up. So you see bigger improvements for the states that were. For the states that were worse off in 2010. Okay, so you know, that's, I guess, good news. Now, of course, I mean, you know, I'm hearing about the statistical implication of things. If this is an applied paper and then we need to discuss the implications of this, of course, there's many things I'm not considering here. For instance, it could easily be that if there's discrimination, it could even happen at the moment. could even happen at the moment of being hired right so maybe if you're female or black you're not hired for a certain position and we would never see this in this data right because this is conditional on people who have been hired in the first place and that have a certain level of education and so on and so forth so so of course the results of this analysis is very different are very different from from a margin analysis where where i would just look at the marginal association between each of these treatments and and salary so here i want to see even if And salary. So, here I want to see: even if you get employed, even if your characteristics or if these controls are the same, is there still some variation due to these discriminatory factors? Anyway, so this is one example. I guess just for the sake of time, what I'll do is I'll just very quickly walk you through a slightly larger simulation study. So, before I show you, So, before I showed you a single simulated data set so that you could see how things played out. But of course, we did a more extensive exercise. We actually spent probably too much time on looking at every possible simulation. So I'll just summarize some of them here so that you can see that what I'm planning works actually works in a fairly varied array of settings. So here I'm just going to report relative mean square. Just going to report relative mean squared error relative to the Oracle list squares estimate. There's going to be a single treatment, 49 controls, and it's 100. So it's an extension of the earlier simulation I showed you guys. There's three settings, whether there's a strong effects, weak effects, or no effects. And then we consider different possibilities depending on whether the controls that have an effect on the outcome, whether they are correlated with the treatment or not. So whether there's a lot of confounding or very little. A lot of confounding or very little confounding. So let me show you the results. So the three panels again correspond to strong, medium, and no treatment effect. If you look at the dashed lines over here, so on the y-axis, it is the relative mean squared error relative to MSE. So one means you get the same as the oracle OLS. On the x-axis, you've got the amount of confounding. So zero means no confounding, six means there's six out of six confounded controls. So you can see that the two dashed guys, which are LASSO and BMA, so standard high-dimensional methods, when there's high confounding, they do horribly. They do very well when there's little confounding. Now, if you then look at DML and BAC, A DML and BAC. So you'll see that what happens for these two is what there's no confounding, they have very high error, but when there's high confounding, they do very well. So this is this over-and-under selection thingy that I've been talking about all along. So if there's high confounding, you run into a problem. If there's low confounding, you run into another problem. Different sets of methods work well in different scenarios. And the guy in black is us, and you can see that we kind of work okay more. to work okay most across across the board. And you can see these on all of the panels. And so we were pretty happy about this. We have more results and things. We have simulations where you have multiple treatments, not just one. The catch of the story, the basic summary of the story is that as you get more treatments, then these issues just become potentially worse. Just become potentially worse. Okay, so the issues that you run into, for instance, in this plot, we're running similar simulations, but the number of treatment is increasing. You have two, three, four, five. This diced line with the lower vertical, the lower triangles, triangles that point downwards. That's BMA. So you can see that BMA does well in this set of simulations up to three treatments. And then when you have more than three, then things start going really bad. Then things will start going really bad. All right, so yeah, I'm not going into the details of this so that we have time for questions, as Peter requested. But I guess, again, to drag down the main notion is that we want to be careful not to impose sparsity across the board. And so we do that through that logistic regression by setting the intercept in that logistic regression that determines the overall degree of sparsity, but also it allows us to treat variables not. It variables non-exchangeably. Okay, so that's it. Didn't work out any theory for this. So, all we have is this empirical evidence and computational algorithms, but that would be something interesting to do. And these are the references. The one on top is our paper, and then the other references are other things that I've cited along the way. All right, so I'll set up and take your complaints. And take your complaints. Thank you, David. Let's start if there's any questions from the live audience here. Any comments, Chris? Thanks, David. It's Chris Holmes here. So I just wanted to go back to the Just wanted to go back to the like the very beginning, the double lasso. So, imagine the case you have one control, one treatment, yeah, and the control is associated with the treatment, but not the outcome. So, the kind of the causal DAG is control links into treatment, treatment. Oh, hey. Hey, treatment links into outcome. So, in the reason why you want to include it, even though it's not associated with the outcome, so the reason why you would want to include the control variable, even though it has no association with the outcome, is because if you don't include it, you'll get a biased estimate of a causal effect. Of a causal effect. So the difference is the interpretation of the coefficient for the treatment effect is that, and it's a kind of a bias-variance trade-off. What you worry about, if you want, or what people worry about rather, is that if we're going to think about this in a counterfactual setting, what would happen if I was to change the treatment? Then you worry about bias and you need to adjust for variables that are. For variables that are, even though they're not associated with the outcome, you need to adjust for them in order to give a causal interpretation on the parameters of the treatment effect. And so, I think that's the motivation for the double lasso is that you kind of accept that you're going to increase the variance, but you're going to get unbiasedness on a causal interpretation. And could you just comment on how that impacts the particular priors you do? Yeah, yeah, yeah, that's it. Yeah, yeah, yeah, that's a great point. No, I think you answered it already. So it's a bias variance trade-off. So yeah, you hope that by including this, let's call it irrelevant treatment in terms of the outcome equation, you may reduce bias. But the variance inflation that you get could be arbitrarily bad, right? Because, well, it's easy to work out in this setting what the variance inflation is. And so, yes, I think it's. And so, yes, I think it's a bias variance trade-off. Based on what we observed in terms of mean squared error, you pay an acceptable price for doing this. So frankly, I would have to think about this more carefully. I think it's a very valid point, and I think we haven't spent enough time thinking about it. But frankly, based on what we have at this point, I don't think we should be caring so much about bias, but maybe we should be caring really about MSE. Should be caring really about MSE, and in terms of MSE, our results seem to suggest that you don't want to include this guy. But yeah, I'll think about it. Yeah, it's a fair point. Chris? Hi, David. Hey, good to see you. I have a vocabulary question, so pardon my friend. What you called... What you called expectation propagation sounded more like a rational base to me. So, why is expectation propagation part occurring? Well, because it's the reverse. It's not KL versus F and G, it's KL between G and F, basically. That's why it's expectation propagation, I think. Unless we translate it poorly from French to Spanish. Let's see where are we? Let's see where where we um yes i think uh-huh no i think i i probably wrote it uh wrong here yes because as as it is yes exactly um yes you're right this would be variational yes yes i think i probably wrote it wrong here yeah yes you're right yeah i think so when we looked at this both ways and the one that was tractable was was was one of the two and i think Was one of the two, and I think it was EP. So I think actually I goofed when writing this, and I wrote it in the reverse order. But you're right, the way it's written here, it would be variation of this. Yes. Thank you. Thanks for correcting me. Thank you. I'll check that. That's very good. Any more comments from here? Any comments from the online audience? David, yeah, I have actually two points. One, One, with respect to what Chris Holmes was saying, which I certainly agree with, you can have situations where, for example, Y is not very correlated with X1, not very correlated with X2, but Y is really correlated with X1 minus X2. So, therefore, right, if you fail to put one in, the effect is completely missed. And that can happen. Missed and that can happen for any number of variables. So that's a reason why an example where a weak variable can really make the other variables very powerful. And so yeah, it's called super adaptivity. And a second point, and I also thought, this has to do with when you said fully Bayes versus empirical Bayes, and you thought that the fully Bayes really didn't learn anything. Really didn't learn anything. And I thought that for a while, too, many years ago. And an example where that was not the case, and I'm pretty sure it's not the case with you that the fully base is bad is would be when you're trying to estimate when you don't know what the probability of inclusion is, let's say in a spike and slab prior setting. So you don't know what that theta is. And if you integrate out the theta, And if you integrate out the theta, you might think, well, you're not learning empirical Bayes, you could learn what the theta is. But if you integrate it out, you get a much more flexible sort of marginal prior component, like a beta, which itself can learn and whatever. So what I found was that, oddly enough, integrating out the theta, you really do, you get more flexibility, and the thing does learn and fully bays indeed, I found to be superior. Indeed, I found to be superior to empirical Bayes, though I still like empirical Bayes. I hear you. I hear you. See, I was so, yeah, I think this would be something worth sitting down and maybe discussing at more length. I was talking to Jim Berger a couple of years ago or three years ago, maybe, and he was actually saying the opposite. He used to think that full base was okay, but now he realized that. But now you realize that you couldn't actually learn these things. So, yeah, I don't know. I think it would be really worth it. Yeah. Can we still quickly move to one more last question, Andrej Sraka? Thank you. Can you unmute yourself? Yes, thanks. Thank you for a very nice presentation. Very short questions. Firstly, is this applicable to both counterfactual and DAG situation? Graphical. So this was pointed. So, this was pointed previously, I think, by Chris Holmes. Secondly, is this applicable to any type of treatments, say continuous or treatment heterogeneity? And also, the situation you have is very close to endogeneity. Is this applicable also if you would have any type of say reverse causality or similar in the treatment model in this regression? Thanks. Yeah, yes, so I guess no. Yes, so I guess no, no and yes and no is the answer. We don't. We have potentially this could be embedded in a causal or DAC type of setting. We haven't done that. So at the moment, we haven't done that. So this is just a standard sort of regression setting. But yes, you can have any type of treatment that you want. Of treatment that you want. It doesn't need to be continuous. You could have a heterogeneous treatment, certainly. You could have interactions of the treatment effect with other variables. That's fine. In fact, in this example, we had interactions of the treatment effect with state. So yes, you can do that. But we stopped at sort of the regression setting, really did not go into the more sort of causal type literature. Okay. Thank you. Okay, thanks. Well, thank you. Thanks for all your comments, and thanks to David for an inspiring talk.