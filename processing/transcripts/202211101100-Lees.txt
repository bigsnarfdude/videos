Yeah, I'm looking out of the sort of dark and wet window in the office and seeing that it's it's clearly quite sunny there. So yes, regretting not being there, but happy to still be able to give the talk anyway. So this, I think, should follow on quite nicely from what Leonid's talk was about and also the discussion that's been happening. So I know the meeting is Matt. The meeting is maths and stats of genomic epidemiology, but this is sort of mostly a talk about their cousin computation, sort of with a focus on data sharing and AMR prediction. And what I wanted to talk about was kind of some experiences I've had in the past, I would say, three or so years about trying to scale up some of the tools we have and really based on. The tools we have, and really based on our genomic epidemiology tools, although this is going to be focused more on statistical genetics, with kind of writing tools that worked for maybe a thousand genomes and then trying to get them to work for maybe 10,000 and then all of a sudden finding they need to work for millions of genomes because there's just loads more available now. And some of these databases have been mentioned actually in the past few talks. Actually, in the past few talks. So, I think the first talk in this session mentioned the Genome Tracker database, which is run by the FDA. This is definitely one of my favourites, sort of very well curated, lots of information there, relatively easy to get at. Fair sequencing, last time I looked, about 500 new isolates a day get added to that database. So, it's growing all the time. And really, the reason these are so big is because of projects like this. Is because of projects like this. So, surveillance being integrated into both public health and research and people kind of sequencing everything they can get their hands on. And that's really increased the number of genomes available. There's sort of efforts at curation. So, Leonid mentioned Sam, Iqbal, and Grace Blackwell's project, which they assembled all of the ENA, a particular snapshot that led to 660. That let to 661,000 genomes. They're talking about doing that again now, which would, so about 18 months later, take another snapshot, that would about double the size of that resource. But even, so the third row here, even kind of research projects have become quite global. So this is one run out of the Sanger Institute that we're involved in, the Global Pneumococcal Sequencing Project. They're sort of getting to 50,000 genomes, and those projects. Genomes. And those projects, so this one in particular, as well as capturing disease data, there's lots of carriage data there. So there's sort of not as many of the biases as you see in some sort of large public data sets. And then I've given up updating the SARS-2 figure, but it's easily tens of millions. There's just huge numbers of those genomes available now. So, what is the kind of like, why do we want to use all of this? So, I mean, To use all of this, so I mean, there's kind of that. I think actually, just mentioned the last data, the last point on this slide here. I think there's just an implicit promise that we kind of think of is that more data is going to be better, more powered, we'll find more things. I think we could probably debate whether that's true later, but in many fields, that is the case. And particularly with machine learning, I think it is a kind of Particularly with machine learning, I think it is a kind of implicit promise that the more data you put in, the more accuracy you get. So we should at least try and use as much of the data as possible. From a kind of so the first point here, sort of from a GWAS perspective, having more samples is going to let us find weaker signals. So sort of think about AMR prediction, we could probably find more of those edge cases or variants which aren't as penetrant, which I think Leonid and Guillaume were just discussing, or rarer variants which you only get sort of. Rarer variants, which you only get sort of enough counts of when you have a large number of subsamples. Also, we want to make our analysis more globally representative. In the past, we've run a lot of studies that have looked at a single country, some countries are oversampled. But in future, we have more countries producing more sequence data, and we want to be able to use all of this. And we know there's diversity out there we want to include. But what are the problems with this? So the With this. So the rest of the talk is kind of going to be broken into three sections based on this. And again, Lena's just been talking about a lot of this. But one is just even before the analysis, it's difficult to get this data. It's difficult to find it. It's difficult to get it. It's difficult to find sort of consistent metadata. Even sort of find out about the studies and where it's from, make all of that consistent. So that's sort of one set of problems. And then once you Sort of one set of problems, and then once you do have the data, there's difficulties with the computation and the analysis. So, going up from a thousand to a hundred thousand genomes is difficult, I think, and a million even more so. So, scaling starts to matter more. Parts of your code that were fine before suddenly start to just stop. As well as having more samples, you start to have more variants. So, if you're sort of thinking about a matrix. Thinking about a matrix of samples by variance, it grows bigger in both directions. Quite a boring point is that the size of the data just gets very big, it's slow to download. Downloading it can become the slowest step. You might not have a computer where you can just deal with those files. You start, your analysis becomes quite redundant. You know that you're probably doing analysis which the authors have already done, or you might be repeating analysis kind of genome by genome, but actually. Analysis kind of genome by genome, but actually you're running a lot of the same steps. And we haven't really, I think, thought too much in the past about reducing that redundancy. And then making it all consistent. So that is very difficult. Being consistent with a published study, being totally consistent, I think is very challenging in most places. But even being internally consistent with your own analysis, and Leonid kind of showed some of the stuff that's available just makes this. Of the stuff that's available just makes this really difficult to do. And so, dealing with that is another challenge. So, those are the kind of three parts I'll talk about in the rest of this talk: how you get the samples, how you work with them, and then how we can evaluate accuracy and consistency in this kind of study. So, to give this talk kind of a bit more structure rather than just sort of telling you about computational problems and solutions, which is a bit dry, I'm going to sort of try and structure it. Dry. I'm going to sort of try and structure it around this imagined summer project. So imagine you're a master's student or PhD student, and your supervisor asks you to do this. Okay, so we know there's all these streptococcus pneumoniae genomes, probably about 50,000 of them. Can you download as many of those as possible, get some metadata, create a model, can be whatever model you want, but we want the model to try and predict drug resistance from the genome. So if I give you a new From the genome. So, if I give you a new pneumogenome, it will give out, it will spit out what we think the resistance is to various drugs. And as well as just giving us that prediction, I would like to know something about how accurate we think that prediction is. So, do we think that prediction is any good or not? So, to sort of go through these steps and how you might do it. First of all, just getting the data. So Um, so this actually was something I tried a few years ago. Um, I had been quite frustrated with getting genomic data and trying to reproduce studies that were published, and I wanted to sort of see if other people had the same issues. Um, and I picked this study, and I should say, this is, I think, um, a good study. I like it a lot. I think the authors did a very good job with data sharing. With data sharing and the data is available. I did end up emailing them. They were very kind of receptive and helpful. But the study basically looked at about 1600 Neisseria gonorino genomes. They were from three separate studies. I don't think they actually sequenced any of them themselves, but they'd collected the genomes from these three different studies and they had paired metadata. So they had antibiotic resistance. Antibiotic resistance, I think, on a few different samples. So, just as a kind of exercise, I was wondering how easy would it be to get the data for that paper and either reproduce what the authors did or use it for your own purposes. And what I wanted was the phenotypic data on the antibiotic resistance in some format, and the genomic data in any format, whether that In any format, whether that, well, so not reads, I wanted it to be able to have some data we could actually run the prediction directly on it. We can't really do it directly on the reads. So they need to either be assemblies or alignments with SNP data. And you can't just email the study authors to help you with that. So I did this myself and I managed to open four other people into doing this. And kind of in summary, it was a bit of a nightmare. Was a bit of a nightmare. People kind of did it in different ways. So I'm pretty sure, I can't remember exactly who was who in this table. I'm pretty sure from what I remember that I was the one that was by far the slowest. Part of the reason for that was that I was refusing to assemble read data. I knew that it had already been assembled and must be somewhere. So I was really trying quite hard to find that rather than rerunning the computation. Whereas other people, I think, tended to rerun the computation and use more resources to do. Run the computation and use more resources to do that. One person I remember in particular was very diligent about making sure all of the data was processed in a very consistent way, which made me quite embarrassed by what I'd done, which was extremely inconsistent. But it was kind of interesting to see how different people approach this. But to kind of summarize the problems, one is that we were definitely all repeating computation that had already been done. And you can see in the second column, it's like a significant amount of computation. Significant amount of computation. And to do this, you needed to be really an expert bioinformatician and you needed to have a big computer to be able to do it. I mean, you can't run any of this on a laptop. Basically, everyone reported that the SRA or the ENA was difficult to use, and that was kind of what the links given to in the paper. Actually, evaluating whether these assemblies were the same as what was in the paper was just not possible with the information there. I mean, almost certainly they were different. Information there. I mean, almost certainly they were different. And then this didn't affect any of the people here. I mean, these are all kind of at sort of, I guess, like big universities, high-income countries, but they all had a subscription to the journal. But the paper was not open access. So to actually get the information that was in the supplementary material, and you needed to be subscribed to the journal to even read that. So for a lot of, I think, people in the world, this just you would fail at the first hurdle. And I think sometimes we fail. First hurdle, and I think sometimes we forget that as well. So, our attempt to sort of fix some of these things and make it easier to get this kind of data is we've tried to make a sort of search engine for bacteria called Bacteria. This was developed by Daniel Anderson. I've got a link here. I'm just going to say now I'm currently in the process of porting this. So, I moved this year between Imperial and I'm now. This year, between Imperial, and I'm now at the EBI. We're sort of moving over where everything is hosted, and unfortunately, half of this is broken at the moment. So, if you try and use it, only the first half will work, but I hope to fix that in the next month or so. So, what I'm showing you live, I have a video. It goes a bit quickly, so I'll play it twice. But what I'm showing is you can just put in kind of free form text as you would a normal search engine. So, I put in my species name, which I have misspelled on Purdue. Which I have misspelled on purpose, I promise. Just to show you, you can get partial matches. You can put in the genotype and you'll get back your results like a normal search engine. I'm just going to go back and show that part again. The results you get in this first view. So, here I'm actually, there's a few kind of handy filters. So, I'm doing it by country, but in this first view, these are ordered by quality of sequence. So, you get references at the top. Sequence, so you get references at the top. So, you tend, we hopefully, we came up with a score. You tend to get the better sequences at the top. You can hover over this, find some information. We've got this button to download all of the assemblies. So, getting the assemblies out is one thing that lots of people found difficult and complained about. So, we wanted to make that easier. So, we've just got a button that downloads them for you. And then you can click onto some of your results. We display lots of information about them that are in public data. That about them that are in public databases. We do a bit of analysis so you can see whether they're contaminated. We also link to the genes in the sample, and you can sort of filter through those. So, this is looking at TET-M, which is the gene that causes tetracycline resistance. So, you can see that this sample does have TETM. Another thing that people sort of more in microbiology labs have complained about is that it's very hard to know with a given gene what's its population frequency. That sort of information just isn't out there unless you've done the analysis. So, we wanted to have. You've done the analysis, so we wanted to have sort of a gene-centric view. So when you click on the gene, it gives you its frequency, it gives you the different names for it, and you also get this sub-sampled alignment. So you can sort of very quickly see, like, is it a gene with loads of variation in or not? And this was kind of based around a project on picking vaccine candidates where this sort of information at the population level is just very hard to access. So that's our attempt. We're going to add to it. To it. But the sort of next plan is to expand it to some of those databases I mentioned at the start. So hopefully, sort of early next year, that's going to be more functional and will help you get some of this data more easily. What we haven't linked up and don't have immediate plans to do is phenotype data. So it'd be nice to get that in at some point. But for the moment, it's mostly a way to find either data about genes or get assemblies for species. Okay, so first part done. We've got So, first part done, we've got our number of genomes. And I'm not actually going to go into too much detail about how one writes a model, but I think what you would find, especially if you're doing this for the first time, is you write your fancy model, you run it, and you press go, and nothing happens. If you do it on a cluster, probably nothing happens, and then the job dies. Dies. And that's just because there's some part of your code that's failing to scale. So, what should you do in this situation? You could just let it run for longer. You could run it on a better computer. Probably both of those things, if it's just a small increase, they might work. You might not have access to a better computer. Something that's probably more common is to start sub-sampling your data, but we said at the start, we're not going to do that. We're going to try and run it on everything. Going to do that, we're going to try and run it on everything, and or you could just give up. I think that would be a reasonable approach. I mean, this is quite hard. Um, but uh, really, I think the first thing you should do is you should find out a bit more information. Um, so first of all, yeah, probably do sub-sample it, um, run it on less data and see if you can get it to complete at all. And then if you just assume it's linear and extrapolate, how long should it take? Or how long do you think it will take? And this is kind of like dealing with scaling empirically. You just want to know: will this analysis ever? You just want to know: will this analysis ever complete, or do I need to change something fundamental about it? So, running analysis on smaller subsets of the data and kind of increasing the data size and seeing how long it takes and plotting that is very effective. The other thing that's effective to do is rather than just linearly increasing resources, if you don't know the size, make them log space. So, double the number of resources you give it each time. That's a more effective way to explore the space. And this is kind of And this is kind of very, you know, basic, but just add progress bars. Something I found incredibly useful in our kind of genomic epidemiology code is just to put progress bars in. This is the example of like the Python package that does it. It's incredibly easy to do. You just need to put one little bit around your loops and you get these nice progress bars, which show you how far through you are, how long it's got left. And you can put this in more bits of your code and you'll sort of find out much more quickly which bit of it is slow. Which bit of it is slow? Will this ever complete? So that's kind of a nice strategy to use as well. Just a kind of brief note on scaling. If you plot your data, it probably looks something like this. Normally there's a period of overheads where the time is constant or increasing slowly. But then when the samples get large, you sort of start to see the overall behavior. And this is kind of increasing linearly the amount of time with the number of samples. Some typical shapes you might see here. Some typical shapes you might see here. The black line there is constant amount of time. So that's great. You can do as many samples as you want if you've got constant time. Linear time is probably what is realistic and what you want to aim for. Then increasing resources is feasible. N-log n also might be feasible. You see that in searching. I should also probably mention constant access. Dictionaries are constant access. But this is the kind Access. But this is the kind of danger zone here. Anything that grows as a polynomial, exponential, or worst of all, a factorial, so combinatorial problems are just not going to be tractable. And you sort of see if you increase the samples by 10 times, if you've got quadratic scaling, it takes 100 times as long. If you've got cubic scaling, it takes 1000 times as long. So you can't just brute force your way through those problems. And that's when you need to start to think about. And that's when you need to start to think about alternative approaches. I've got some other speed tips which are quite specific here. I'm not going to go through them. They're in the slides, and I'll hopefully share the slides later. I guess this is being recorded and they'll be on here long enough. But if you're kind of specifically looking at doing sort of working with large numbers of genomes, these are some things I found useful. So there's page one. Here's page two. So hopefully, if anyone's interested, you can look at that later. If anyone's interested, you can look at that later. I'm happy to share the slides if anyone wants to see it specifically. Just the final note is: some people, and I probably include myself among this group, it can be a bit addictive to do this. You can get a bit too into making something as fast as possible. You've really got to stop and think whether it's actually worth it or not. And you can really go down the kind of rabbit hole of tiny little improvements. Tiny little improvements. I think, think how many people are going to use it. If you get a change in scaling, it's probably worth it because it's going to significantly change the scale of problem you can analyze. And you can probably actually do quite different amounts of analysis yourself. Something like a two-time speed up is nice, but you can probably afford to leave things running for twice as long. If it's something that you use a lot or other people use a lot, then it might be worth it. Chasing like a temp. Worth it. Chasing like a 10% speed up is probably only really worth it if you're writing like the next linear algebra library and it's something that's going to be used by people all over the world, by like millions of people over and over again. Realistically, our research code is probably used by, most of it's probably used by one person. Some of it might be used by like 10 or 100. So think about, I think, how much time it's worth. If you do like this kind of thing, this book is great. It's kind of talking about how you do high performance code. High performance code. It also sort of made me realize lots of things I've tried in the past are completely pointless. So it's probably actually saved me time. But yeah, that's a good book. Okay, so that's the kind of computation part. Talked a bit about data access. Now I'm going to sort of talk about, I guess this is kind of close to what Leonid was talking about and consistency of models. So let's think about how we might evaluate these sort of models. So got my data. So, got my data. I have done some nice things with my code so I can do this. But what I've decided to do is just to start with, I'm just going to fit a really basic model. I'm just going to try with some sort of linear model because I expect this can scale well. Specifically, so this is looking at prediction of penicillin resistance in Streptococcus pneumoniae. This is caused by three penicillin binding proteins in the chromosome, which we can. Proteins in the chromosome, which we combine. This is actually a Manhattan plot. So, this is looking at a marginal model of effect size. Actually, these are the p-values of slopes in a regression. And you can see this kind of picks out the three penicillin binding proteins that are causal for this quite well. Although there's a few other points there that probably aren't causal. And we could do this in two ways. We could try and fit all of these slopes jointly. Maybe we'd use some. Maybe we'd use some sort of penalized model like an elastic net so we don't overfit to the data. So that would be, they're both linear models, but we end up with different predictors, different effect sizes. So that actually running it on this data set, it looks like this. These are the effect sizes of all of the SNPs and the p-values of all the SNPs. Or we could run with a linear mix model, running them one at a time, so marginal model. And this is called. Model. And this is called in human genetics or statistical genetics. This is normally called a polygenic risk score. So this is a marginal model. And we just sum up the SNPs, the effect sizes of the SNPs, which are significant. So how do these do? So this, I'm using heritability here to give an idea of how well we could probably do in the best case. And these plots that I'm going to show on the x-axis, you have the linear predictor. Is you have the linear predictor. The red line is where the decision changes to predict it as resistant or sensitive based on whether it's greater than zero or less than zero. And you can see the LASU model does quite well. So gold should be on the right-hand side of the line, silver should be on the left-hand side of the line. And we do a reasonably good job, sort of 80% correct, sort of expecting we should be able to get 90 to 100% correct. The polygenic risk score also does quite well. Also, it does quite well. If we look in another population, we do less well. And particularly, the polygenic risk scores do worse than the LASU model. So we can evaluate that. But one thing I thought was quite interesting here is that if we try and use the model fitted in one population and apply it to the other population, we basically get an accuracy of zero. It's totally terrible. And sort of looking into this in more detail, the reasons for this is that the variant calling. For this, is that the variant calling is much worse in data set two, and that's overall why it's worse predicted. The big problem is that the variant calling was done separately, so they just don't overlap. So, the big effect variants in one population aren't really called well in the other data set. And then there's this general problem, which is that the genetic background is quite different in the two populations, and there's very little overlap of the strains between the two populations. So it doesn't really work at all. It may be in general that effects. It may be in general that effect sizes are genuinely different between the populations. It may be that phenotype measurements were different as well. But in this case, it's really just down to kind of genomic errors. So Leonard was sort of talking about some of these other effects, but these are just more kind of basic data errors. And you see this in other traits as well. So this is carriage duration, which is a quantitative trait, doesn't have as high a heritability, but when we try and predict it, it looks really predictable. It looks like it's working. Really predictable, it looks like it's working really well. But if we hold out some data, it's suddenly sort of looking less good and more like the kind of predicted heritability. So, if we just fit these models, we kind of tend to overestimate how well they work. One thing we have tried to do is re-weighting the data by the strain makeup. So, that's kind of shown in this column on the right. And we think that maybe gives a more honest accounting of how well you're likely to do in other populations. It doesn't improve the accuracy. It doesn't improve the accuracy, but it does hopefully give you better kind of estimates of how well you're doing. And how long have I got left? A few minutes. I'll just quickly mention this. Previously in these studies, we've used SNPs to do this, but we also know that variation of entire genes might matter. But gene calling is quite hard and probably even harder to make consistent than SNP calling. So we've kind of given up on this and we just use K-Mas. Given up on this, and we just use k-mers, which are really easy to use. They're just sort of sequence words. They're also really easy to make consistent between data sets because you're just doing string searching. And we now sort of recommend using Unitigs, which is a concept from sort of assembly and De Bruyne graphs. There's this nice paper in Postgres from a few years ago that introduced this. And this generally works quite well to represent variation and make it consistent between data sets. Okay. Okay, so these are the sort of parts in the summer project. So, download data. Well, we can get it from the ENA through Zam's project with Genome Tracker. If BlackQueria worked, hopefully, we could use that. Create a model which predicts drug resistance from a genome. Well, we've got packages to count UNITIGs. We fit a linear model. Our original code didn't work. We kind of improved it line by line, got it to work. Evaluate the accuracy. Well, we used an external data set, made sure not to sort of just. Made sure not to sort of just evaluate it on the same data set we ran it on, fitted the model to, but made sure our genotyping calls were consistent. And this was actually a student called Marie, this is actually her summer project, and this is basically what she did. And I think she did a really good job of it. In the end, we did do it on a smaller number of samples that we had sort of good phenotype data from, but from a couple of different cohorts. Used a linear model, found that the UNITIG selected. Found that the Unitig selected were largely in the right place, looked at uncertainty by taking two completely different cohorts. So, actually, these are the two from before, and evaluating this with the balanced accuracy on a few different drugs. But generally, it seemed to do pretty well and pretty comparable to the heritabilities. And then, one thing that was nice about this model is that Marie was able to make it into just a browser tool. So, I've sort of been lazy here and just clicked on the example. Sort of been lazy here and just clicked on the example sequences, but you can just drag and drop genomes into here. Within your browser, without using a server, it'll create indexes of these genomes, call the predicted unitics in them, fit this linear model, and give you a prediction of these resistances super quick. And data doesn't need to be shared. It's all run on the user's computer. Sort of a nice way of doing this. Maria is now extending this to also do kind of To also do kind of clay prediction as well, strain prediction with our tool pop punk. That's coming out soon. That's called Bebop. But this was sort of the first version. So I'm actually going to end there. So those are kind of my three parts. I'm just going to very quickly give a shameless plug. Embel's postdoc program or sort of fellowship program has just opened. We are participating. So this is a call to anyone who's interested in doing a postdoc on statistics or computation in this. Statistics or computation in this area, or anyone who is in Europe that would be interested in collaborating, they're collaborative between different groups, funded for three years. It's your own project proposal. Have a look on our website if you're interested in either collaborating or doing a postdoc as part of this scheme. And I'll just end by pointing to we've got the group website is backpop.org. If you want to find out more about our tools and sort of how we've optimized them, you can find that information there. Optimize them, you can find that information there. And then, thanks to the people involved in this project, particularly Daniel, who did the bulk of the work on Backqueria, and Marie, who did the bulk of the work on the prediction project. So, yeah, thanks for listening. Fantastic. Thanks, John. We are, we started a little bit late. So, there's, I guess, there's a comment in the chat. I was going to say we could do a quick question and then maybe move on and leave discussion for the break. Was it a comment or a question? I can respond to that if it's leaning as one in the chat. So, yeah, paywalls are a big issue. PubMed Central and PubMed are great. PubMed Central and PubMed are great. I've actually, I just heard some people from PubMed Central talk the other day about the kind of general data mining they do and the data mining that's possible with their resources. And apparently this works really well in the protein world. But two things they said, one is the same as you, Leonid, which is if the data is in a figure, it is so much harder to extract. They have some methods specific to proteins because people tend to make their figures in the same way that might work as an automated. Same way that might work as an automated method. But apparently, the big thing, the thing that is a real pain is the supplementary data is just not there and not possible to pass. It's much more manual. And I think that's something they're sort of hoping to work on in the future. But most papers in PubMed Central don't have the supplementary data. And it's also in a much less standard format, PubMed Central passes everything into HTML so you can do your text mining on it. But supplementary data is just whatever, some PDF that someone threw. Just whatever, some PDF that someone threw together. All right. And unless there's a quick, quick question, I think we will move on to Max and save additional questions and discussion for the break. That's okay. Thanks so much, John. Max, do you want to go ahead and try? Max, do you want to go ahead and try sharing your screen? Yeah, hopefully people can see my slides. Yep, looks great. So yeah, take it away. Great. Thanks for the invitation. Thanks for having me here to speak. My name is Max LeBrecht. I'm at the School of Computing Science at SFU in Vancouver, BC. Leaned set up what I'm going to talk about. Leonid set up what I'm going to talk about really nicely, so I'm going to jump right in. As Leonid said, resistance to antibiotic drugs is a public health threat. It's been projected that by 2050, antimicrobial resistance can cause around 10 million deaths and 66 trillion pounds. It's in pounds because this figure is from Public Health England. To put that into context, that's close to the yearly. Close to the yearly world GDP. Sorry to interrupt. We can't see your slides advancing. I don't know if that's a lag issue or if yes. Can we train it? We see those the yeah, okay, yeah.