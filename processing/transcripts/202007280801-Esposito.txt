Inviting me to speak. So, first, I will ask you if you see my mouse, because that's yes, we can see the mouse. Okay, good. So, the second thing I want to do is mention that, of course, I didn't do that work alone. There are many people in the group, so I will not list them all. But whenever you will see references, I put the explicit. See references, I put the explicit name there, and there are many postdocs and PhD who participated in this. And then I think I can now start the presentation per se. So, okay. So, as a motivation, let me say the following: so, thermodynamic. Following. So, thermodynamics, as you all know, is a very old and important science. But remarkably, despite being very old and having been used throughout natural sciences, there's been a lot of progress in recent years, decades actually, two decades, to extend the traditional theory of thermodynamics, which holds close to equilibrium, to Close to equilibrium to a very small system that fluctuates and can be driven far from equilibrium. And that's often called nowadays stochastic thermodynamics. And yesterday you've heard nice talks about stochastic thermodynamics. What I'm gonna try to do is somehow move to the next level and trying to And trying to use what we've learned from stochastic thermodynamics to study larger systems, in particular chemical reaction networks, not necessarily at the level of the stochastic description where species are described by numbers which jump stochastically when reactions happen, but in terms of concentration. But in terms of concentrations, deterministic concentrations, because they follow a rate equation, the ones that we are most familiar with from high school, and the one that very often experimentalists measure also in biological systems. And the hope is that by starting to understand the thermodynamics of larger systems like these, we will start to see a path to some. See a path to somehow go back from the small scales but far from equilibrium back to the large scale but remaining far from equilibrium because what characterizes macroscopic living systems is that they have plenty of layers that remain out of equilibrium, which is not the case in traditional thermodynamics, where everything is based on the fact that bulk perturbation typically we Bulk perturbation typically weakly brings the system weakly out of equilibrium. So, the hope is that we will one day be able to make statements about macroscopic non-equilibrium system like living organism or even entire food webs. Let me also add that you can interrupt me at any time if you want. I don't mind at all. So, the outline of my talk is the following. The outline of my talk is the following. I will start with the deterministic description of open chemical reaction network, tell you how to formulate thermodynamics at that level, then emphasize the role of topology. I will tell you what I mean by topology. It's really the stoichiometric matrix of the network. And I will emphasize the role of information at this deterministic level, which is different from the role of information. Different from the role of information at the stochastic level. Then I will give some applications about dissipative self-assembly, and I hope I will have time to show you how this deterministic description can be seen as the macroscopic limit from a stochastic thermodynamics of a chemical reaction network and say a few words about accuracy in this at that level. Then I will end with Then I will end with conclusions and perspectives. So let's start with the deterministic description of chemical reaction networks. So when I will use as an example this very simple chemical reaction network, which has five species. For a closed, what I'm going to call a closed chemical reaction network, I describe the diagnostic. I describe the dynamics of such a system simply by rate equations, and that's simply a fancy way of rewriting the rate equation for each concentration of each of the five species. So Z is a vector. For the moment, don't care about this separation. I will come to it in a moment, but it's Z is simply the vector containing the five different concentrations. Different concentrations of each species, and its change in time is given by this object, which is called the stoichiometric matrix, acting on the current along each reaction. So, what one does is one chooses an arbitrary direction for every reaction, and then one writes a vector which for each reaction. So, here we have four reactions. So here we have four reactions. That vector contains the currents along each of these reactions. So it's oriented according to the convention that one chooses. And we will assume that mass action law, kinetics, governs this system. And this means that the current is going to be, it's split into forward and backward current. And it's proportional up to a constant, which is the. Up to a constant, which is the rate constant, to the concentration of the reactant or product to the power of the exponent. This is really high school chemical kinetics as we learned it, but simply written in this matrix form. Let's take, for instance, the current along reaction one here. So, in the forward direction, because I chose the positive direction to go from left to right. From left to right. So it's K1 times the concentration of A and E, which is here, minus K minus 1, this rate constant in the backward direction, times the concentration of E star. Okay, and you can repeat this for the four different reactions. Now the stoichiometric matrix is simply taking care of how the current contributes to the change of the concentration of each species. Concentration of each species. And therefore, if we look at the first species here, which is E, there's going to be a minus one because when the reaction happens in the direction from left to right, it's consuming E while it's producing E star. And that's why we have a plus one here. And then you can continue. The reaction three is also producing E. producing E and reaction 4 is sorry reaction 3 is consuming E that's why there's a minus 1 and reaction 4 is producing E that's why there's a plus 1 here. Okay so I hope this is clear. I tried to go into the details to give you the setup but that's simply a way to write the dynamics rate equation of this closed chemical reaction network. Reaction network. Now, what I will call an open chemical reaction network is I will assume that certain species here A and B are controlled from the outside. The simplest way of controlling a species in an autonomous open chemical reaction network is to assume that the concentration of A and B is chemical. And B, it's chemostatt, so it's fixed and it doesn't change in time. This means that whenever reactions are consuming A and B, they are compensated by something coming from the outside. Okay, so whenever a reaction consumes B, from the outside I bring back a B in so that the concentration of B stays constant. One way to model this is to assume that Y are in very large concentration. Are in very large concentration, and then we can neglect the amount by which this concentration of y is being consumed. But we don't need to. We could also consider specific changes in this concentration that might be due to other mechanism. And then we have this balance equation that this is set by the reaction. If this is controlled, this concentration of the chemostat, then of course, as a result, As a result, we know what's the current that needs to be brought from the outside. So that's how we open a chemical reaction network. Now, that means that the stoichiometric matrix can be split into two parts. The part that was the full was acting everywhere on every species, now the one that becomes relevant is the one acting on the subset of dynamical species that are the X species. That are the X species because the Y are set from the outside. This is really the dynamics of the open chemical reaction network. Now, how does the energetics come in? We simply assume that we deal with ideal solutions. So the analog of an energy for a species in a solution is a chemical potential. It's the standard chemical potential plus this typical RT log Z. RT log z, log of the concentration term. And the connection between the dynamics and the energetics is made via this local detail balance condition. This is known in chemistry that for elementary reaction, meaning reactions which do not hide other reactions behind them, so they can really be boiled down to single collisions. Down to single collisions in solution. The logarithm of the forward divided by the backward rate constant is related to the change of standard chemical potential across each reaction. And the role of this local detail balance can be seen as a consistency condition because it ensures that in a closed system, In a closed system, the system will reach a state of equilibrium where the currents, the net currents, vanish. In other words, each forward current is balanced by the backward current. And that's really the this together, this local detail balance together with the mass action law ensures that this is going to happen. So, as we expect from physical and chemical argument, a closed system. Argument: a closed system should always relax to equilibrium. But if we open a system, this may not happen anymore. So let's come to really defining thermodynamics for this open chemical reaction network by mimicking in spirit what we do for in stochastic thermodynamics at the level of probabilistic description. But here we will do the same, but at the level of concentration. Same, but at the level of concentrations. So we define the enthalpy of the system as simply the concentration times the enthalpy of each species. And this is a product, which is a sum over each species. That's, of course, obvious. It's the analog of the energy of each species. Then the entropy is reminiscent of the Shannon entropy, but Shannon entropy. But Shannon entropy, we should remember, is for probabilities. Concentrations are not probabilities. And it's the concentration of each species multiplied by the internal entropy of each molecule. But then there is this minus R log Z term, which is the analog of the log P in the Shannon entropy. And there's an additional term here, which is important because total concentration. Because total concentration, so the total concentration here is the sum of the concentration of each species. The total concentration may change because we don't have a normalization condition. Probability is always normalized, so the sum of all probability is always one. But this can change in a chemical reaction network. Think of an aggregation reaction. In that case, the concentration is the total concentration is shrinking. So that term needs to be So, that term needs to be here. Now, by taking derivatives of the enthalpy and derivatives of the entropy, we can write two balance equations, one which is the enthalpy balance, or analog of an entropy balance, which says that the way in which the enthalpy changes in the chemical reaction network is the enthalpy change due to every reaction, and that's actually what is associated to the Is associated to the thermal, the heat increase in the reaction due to the heat released by each reaction, but also the enthalpy exchanged with the outside world, which are these chemostats. So, if you want, this is the enthalpy exchange with the chemostat, well, this is the enthalpy exchange with the thermal bath surrounding the solution. And this can be rewritten. And this can be rewritten by defining the heat flow in the following way: the heat of reaction and the entropic exchanges with the chemostat. Because in that way, we basically simply added and removed the entropy here to build the quantity that is the chemical work. This is really the chemical potential brought by. Brought by the chemostat into the system. So that's really the chemical energy brought from the outside into the system. And it's actually a matter of convention whether one wants to put this in the formal definition of heat. This way of doing has the advantage that we keep a balance equation for the entropy that is like the Clausius keeps the Clausius form, else I would have an additional term with the entropic contribution. Term with the entropic contribution here. But you see, now I can write my balance of entropy, the change of the entropy in the system, as heat divided by T, which is the entropy change in the thermal and chemical reservoir, so chemostat and thermal bath, because as I said, this is the thermal part, these are the chemostat. And because the sum of the entropy changes in the system plus the entropy changes in the Plus, the entropy changes in the reservoir, we know should always be positive, but here we can prove it explicitly. This quantity, the total entropy change or the entropy production, is a quantity that is always greater or equal to zero. It is what one expects from chemistry, namely the change of chemical potential due to every Potential due to every reaction because that's a change multiplied by the flux of every reaction, and it can be rewritten in thanks to the mass action law in this form, which clearly show why it's always positive, because if this term is positive, then the log is also positive. If this term is negative, this also becomes negative. The log also becomes negative. So, this is by construction always zero, and it is. And it is always greater than zero, and it is equal to zero when we are at equilibrium, namely when g plus equals g minus. So everything seems to be very consistent. And I emphasize the first connection to information, which is what I said that we have a Shannon-like entropy describing our system, although it's, I will come back to that. I will come back to that. It's not a Shannon entropy because we are dealing with concentrations here and deterministic dynamics. So that's really the basics. But now what is very important is to make use of the topology of the network to write this entropy production in a more revealing form. Now, the topology, as I said, I briefly alluded to that, I will Alluded to that, I will call topological property the property that depends only on the stoichiometric matrix. So it has nothing to do with the rate laws, the dynamics itself. It only has to do with how reactions are talking to each other. And these stoichiometric matrix, mathematicians studying chemical reaction network have been studying, defining these different concepts. Defining these different concepts. So, this matrix has left null vectors. So, this is simply the definition of a left-null vector. And these left-null vectors are associated to what we call conservation laws, because when they are multiplied by the vector of concentration, they give us quantities which are conserved by the dynamics. If you act in a closed system here, if you In a closed system here, if you act, you take the derivative of this, you get the derivative of z. But since it's the soiometric matrix acting on the fluxes, L on Z is zero, so that quantity will not evolve. So let's look specifically at this chemical reaction network. We have two left null vectors, which correspond to these two quantities. The first is the total concentration of E. The total concentration of E. You can think of E as a kind of enzyme. E is the free form of the enzyme. E star and E double star are some binded form. So here the total concentration of E is constant and in the closed system it's easy to see because every reaction consumes E but creates an E star. The reaction 2 consumes an E star but creates an E, so this quantity remains. Creates an E, so this quantity remains constant, same for the next two reactions. The second quantity also contains, in addition, to E star and E star star A and B. And you can also see, let's take reaction 3, for instance, reaction 3 consumes B, produces E star star, so minus 1 plus 1, it's conserved. So the reactions always conserve these two quantities. Conserve these two quantities. And this is in the closed chemical reaction network. If we now open the chemical reaction network, the conservation law that are relevant are those of the upper part of the stoichiometric matrix, the one acting on the dynamical variables. And what we can easily see is that one conservation law now breaks. It's not anymore a conservation law. It's not anymore a conservation law while the other remains a conservation law. The one that breaks is, we will call it a broken conservation law. And let's see why it is broken. As we said, when we chemostat A and B, we fix the concentration of A and B when we open the system. So now if we, for instance, go through the first reaction, we create E star, so plus one. Star, so plus one, but although we consume an A, the A has immediately being compensated by the outside, so A is constant, and so that quantity is not constant anymore. Okay, so there are exchanges with the outside, so that quantity is not conserved anymore. Let's now look at the right null vector of this stoichiometric matrix. These are called cycles. Cycles. Why cycles? Because the element of these vectors are reactions. So, in this case, it's very simple. It's acting with the first reaction, the second reaction, the third reaction, and the fourth reaction. So, you see that if you go through the four reactions, you basically come back to the original state of your system in the closed chemical reaction network. So, it's So it's meaningful to call it a cycle. But now, when we open the chemical reaction network, there can be more cycles than in the closed system. And let's see why. Here, the new cycle that emerges, we call it an emergent cycle, is the cycle 1, 1, 0, 0. Because if I go through the first reaction and then the second, you see that because A and B. You see that because A and B are fixed, I started from E and I came back to E. So I'm back to the original state of the system. What is interesting, and that's going to be crucial on the next slide, is that while the X species, the internal species are unchanged, namely E, as I showed you here, or actually all the three E. Actually, all the three E star and E star star. The fact of going through one of these cycles implies that we had to bring from the outside the A that has been consumed and the B that has been produced, has to be removed to keep this concentration fixed. So emergent cycles leave the internal species of the chemical reaction network unchanged, but they transfer things across the reservoir. things across the reservoirs. And that's why they're going to be so important. Now there's a mathematical implication of the rank nullity theorem. I will not go into the details, but one can easily prove that the number of chemostats, so the number of species that are controlled from the outside, here too, is equal to the number of broken conservation laws plus the number of Plus the number of emergent cycles. So let's simply go very quickly through simple examples. So if the system is closed, of course, there's no chemostat, there's nothing, no broken conservation law, no emergent cycle. If we add the first chemostat, for instance, A, because I can argue physically, but also mathematically for why this is the case, but we always Or why is this the case, but we always break a conservation law because we start exchanging things with the outside. So the first chemostat will always break a conservation law. And that's also the reason why necessarily there's not going to be an emergence cycle associated to it. And you can actually see it if only you can check that if you would only consider the part of the stoichiometric matrix without the last row. Matrix without the last row of the closed one, you already see that the conservation law here is broken. Now, if you add a second chemostat in this system, there's no other conservation law that is going to be broken. So, in this model specifically, we will create an emergent cycle, and that's precisely the emergent cycle that I described here. So, that's what is happening in this specific system. Specific system. More in general, and we will now see what this implies at the level of the dissipation in the system. We can have all these various possibility. Whenever we add a chemostat, the first one, as I said, always creates a broken conservation law, breaks a conservation law. So that one is there's no alternative. But the next one, But the next one will either break another conservation law and not create an emergent cycle or create an emergent cycle. So we can go further. Third chemostat could break a third conservation law, still no emergent cycle, or not, then we would get the first emergent cycle or create two emergent cycles and only one broken conservation law and etc. So you see that the conservation law will really tell us how many emergency law Conservation law will really tell us how many emergent cycles there are. Why is this so important? Because we can rewrite the entropy production of the system, the total entropy system plus bath, in term of three contributions. Let me go now through this, the meaning of these three contributions. The first one looked as a parametric time derivative containing the chemical potentials. The chemical potentials of the chemostats that break conservation laws. This means that in an autonomous system, if the concentration of the chemostat are fixed, this term will any way be zero. So that term has really only a non-trivial role if we consider non-autonomous open chemical reaction network where there are temporal changes in the imposed concentration of the chemostats. Concentration of the chemostats. So I will not go into more detail, except mention that they are related to the broken conservation law. So it's related to the work that these external time-dependent forces need to do to bring in broken conservation laws. The second, let me first go to the third one, is a total derivative of a potential, which is sometimes called the semi-grand. Sometimes called the semi-grand Gibbs free energy. You see, H minus Ts is the traditional free energy, but now if you think in a way that is reminiscent of equilibrium thermodynamics, you know that when there are additional quantities that are exchanged with your environment, like number of particles, you typically add a Lagrange multiplier and times the quantity that is exchanged. The quantity that is exchanged. So, μn, for instance, is the most familiar one when we go from canonical to grand canonical. Here you see that we are adding certain combinations of the chemical potential of the broken, those chemostats that break a conservation law, multiplying the broken conservation law. And so, in the case, in our example, we would have, it's quite simple, we would only have mu A times L. Only have μA times L2, which is the broken conservation law. Now, this is a total derivative. So, if we consider stationary states, for instance, that quantity would not play any role because the system would not change and therefore the total derivative of that quantity would be zero. So, we now go to the very important term, which is the non-conservative work. That non-conservative work has to do. Work has to do with the fact that the chemostat are inducing currents across the system. And the form that this chemical non-conservative work takes is the product of what I will call affinities or forces along the emergent cycles times the current along the emergent cycles. The affinities The affinities are combinations of the chemical potentials of the chemostat along the emergence cycles. So one way to picture this, it's only a picture, but I think it gives you a sense of what is happening. I told you that these emergent cycles leave the system unchanged, but they transfer things with the reservoirs. So you can imagine that these outside You can imagine that these outside arrows are exchanges with the reservoir, while the cycle itself acts only on the internal species. And the chemical potentials that are relevant are those of the chemostats that exchange matter with the system while we complete that cycle. And the current along the cycle is simply the current with which one completes that cycle here. Yeah. Sorry, so we've had some questions to help explain the concept of cycles. If you go back a slide, there was a question about linearly independent combinations of cycles. Yes, of course. Right? So maybe if you illustrate that on the previous. So of course, any linear combination of these two cycles, for instance, will also be a cycle. And there's a notion of basis, of course. One can take Of course, one can take any basis will be fine, but there's always a representation that is more convenient to illustrate things. But if I take this minus twice this, I will also have a new cycle. So it's simply the fundamental basis that is going to tell me how many of those vectors I need. So there's a kind of Gauge invariance. I can comment on that here. Comment on that here because here you see this term, as I said, is a well, no, I didn't say it, I was about to say it. So, this will be a sum over all the emergent cycle. In our system, we only have one emergent cycle, so it's going to be mu B times the current of the B species. But in other systems, one may have more cycles, and we will have as many emergent cycles. As many emergent cycles here as what is given by this relation. So you see that the number of chemostat and the number of broken conservation law will determine how many emergent cycles there are. But that the specific decomposition of the sum into each piece is up to a certain freedom. So you can always redefine, take linear combinations. Take linear combinations, the total result will not be changed, but the pieces will be different. So now let me also comment on something that I may use later is the notion of detail balance chemical reaction network. It's not to be confused with the fact that at equilibrium the system is detailed balance. This really has to do with the class of chemical reaction networks. Of chemical reaction networks that such that the chemostat chemical potential here always give rise to zero affinities or zero forces. In other words, there's no non-conservative work in the system. And in our example, this would correspond to the situation where the chemical potential of B is equal to the chemical potential of A. In that case, because that term here, the non-conservative work is zero, Here, the non-conservative work is zero. Because if we don't drive the system with the chemostat, we have that the entropy production is minus the change, the total change of this G curly. And this G curly can only therefore decrease. And it will decrease until it reaches equilibrium. So these detail balanced chemical reaction networks are systems that, if not driven by time-dependent. Not driven by time-dependent forces, which we will almost never consider in what follows, will relax to equilibrium. So that's also an interesting property of this system. But in presence of non-conservative forces, of course, this will not happen because the chemostat are creating fluxes going through the system. Okay, so if there are no So, if there are no further questions at this point, it's a bit technical, I agree, but it's very important to identify these different pieces because each of them play a role in a distinctive situation. I emphasize further, I don't know, I forgot if I said it, that in a stationary state, the third term is going to be zero because it's a total derivative. We don't drive the Derivative, we don't drive the system. So the non-conservative work is really what is going to control the dissipation in a non-equilibrium stationary state. And if you want, this is the generalization of the concept that was introduced by Prigogine that the dissipation in a steady state, in a non-equilibrium steady state, can be expressed as a product of forces time fluxes. And close to equilibrium, the forces are typically. Close to equilibrium, the forces are typically expressed as proportional to the forces, but here we don't do that, this is completely general. Okay, so I will now move to the concept of information beyond only the entropy that I was describing before. So let's look at this semi-grand potential that I introduced. There's a remarkable property. Introduced as a remarkable property: is that if I can rewrite it in terms of the equilibrium, the corresponding equilibrium G curly, which is the equilibrium is defined as that state where all the affinities are equal to zero. There's a specific procedure, but I will not go into the detail. But there's a well-defined equilibrium that we pick as a reference, and now the difference. And now the difference between these two curly G semi-grand potential can be expressed in terms of a relative entropy almost. Again, if we would deal with probabilities, this would be a true relative entropy because these two terms here would be zero because again, this is the total concentration of X and the total concentration of X at equilibrium. Since probabilities are normalized, this term would be zero. Are normalized, this term would be zero. But in chemical reaction network described by concentration, this term may be non-zero. And that relative entropy or Kullbach-Leibler divergence for those who prefer that term can be seen as in information theory as a kind of metric, a distance between two probability distributions. So in this case, it's a distance between the non-equilibrium. Between the non-equilibrium distribution of concentration with respect to the equilibrium distribution of concentration. And as a direct consequence of the previous slide where I used topology to rewrite the second law, I can now prove that the work that may, if there is time-dependent driving, that driving work plus the non-conservative one. Plus, the non-conservative one, the very important one that is present when the chemical reaction entry is not detailed balanced, must be always greater than the change that may be induced between two equilibrium states. That's only relevant when there's a time dependence here. Again, in autonomous systems, this would be zero and this would also be zero. Plus, this change in relative entropy between the non-equilibrium concentration. The non-equilibrium concentration profile and the equilibrium one. So, this really tells you that there's a cost, it's the minimal cost that you need to do from the outside in order to bring your system out of equilibrium into a state X is given by this relative entropy or Kulbach-Leibler distance between the non-equilibrium distribution and the equilibrium one. So, you can really interpret. So you can really interpret this RT times the relative entropy as the minimum work needed to generate a non-equilibrium distribution of concentration, or the opposite is also true, the maximum work that one can extract from a non-equilibrium distribution by bringing it back to equilibrium. So I think this is a second very nice connection with notions of information distinct from Distinct from what we have in stochastic system because we deal with concentration. So, let me now, I will go a bit faster because these are really the basis. And now I simply want to show you some extensions. We can generalize this to reaction diffusion, open chemical reaction network, but also undergoing diffusion. And basically, everything carries on. Everything carries on, and what is nice is that now this relative entropy or Kulber-Leibler distance is not only in the space of species, but also in the real space, because the concentration become fields. And so we really have an access to the cost needed to structure concentrations in space. We can also show that the Also, show that the relative entropy of a pattern compared to an equilibrium distribution is always greater than the corresponding space averaged pattern corresponding to the same equilibrium. So, that really tells you that there's a cost for structuring things in space. And we can use this formalism, and we did this in this paper, to study the cost for generating a Turing pattern. Or generating a Turing pattern, for instance. So that's the first thing I wanted to mention. The second thing is that we can also look at really things locally in space and phenomenon like waves. For instance, in this case, a Fisher wave. That's an example of a detailed balance system because X, there's only one chemostat here, which is Y, and these X species. And this X species is undergoing a reaction along the front, but as the front moves, the system in the back reaches equilibrium. And we can really look at the dissipation occurring along that front. That's what is specific of these detail balance waves: that everything happens inside the front. And we could also prove that the entropy production is proportional to the amplitude and to the speed. Amplitude and to the speed of the wave. And another type of wave that here is appearing in a Brussels later model that was studied some time ago by Nicolis and co-worker. This is an example of a non-detail balanced system where there's a cost for sustaining the system and on top of it producing wave propagation. And in that case, we see that if you want these non-conservative Want this non-conservative work is almost all the dissipation, so that's really the sustaining of the system is very costly, while the wave in itself, the modulation, is only a small fraction of the total dissipation. So we have a tool here to calculate the cost, the thermodynamic cost for creating chemical signals. That's the second message I wanted to get through. Now, very briefly, Very briefly, I can. This is a funny funny application where we could show that if you have, if you put, if you consider a system where you maintain a linear gradient between the left and the right, okay, without anything in between, it's going to be diffusion will set a linear concentration gradient of Z. If you put an object in that gradient, the object will distort the gradient. Distort the gradient. And what we did here is we imagine that one could, that this object could be surrounded by what we call the cloaking device, which contains a chemical reaction where A and B are some chemostats. And we could explicitly show that with a given space profile of this chemostat, the cloaking device can restore the linear The linear profile that was there without the object. Yeah, so it can restore it. And so you can think of it as the object is protecting itself from external, from perturbing the surrounding. And so it's hiding from potential means of detection that would detect the object by measuring how the gradient is distorted. Is distorted. And what was quite interesting is that, from an energetic point of view, which we did the analysis in the paper, we could show that a significant part of the energy that this clocking device is using is actually coming from the linear gradient itself. So it can not fully be powered by the gradient, but a significant portion of it can come from this gradient. So it's also interesting to see. Gradients. So it's also interesting to see how, from a thermodynamic point of view, gradients can be used as a resource to do interesting chemistry. Since my time is essentially over, I'm going to skip the application to dissipative synthesis, which was an illustration of the concept that I described to you. I simply want to make one additional comment to really make it clear. To really make it clear what is the difference between the stochastic and the deterministic description of chemical reaction networks. When we speak of a stochastic description, we really deal with the molecular counts, while at the deterministic level, we deal with concentrations. You can explicitly show the connection between these two levels of description by taking a limit of large volume, where the number of species are large. Number of species are large. But so that's a technical point, but that it's interesting to go through that calculation explicitly. But from the information point of view, what I want to emphasize is that we can do stochastic thermodynamics of these networks. For instance, in this paper, again, emphasizing the importance of conservation laws on the dissipation. But the entropy at the stochastic level contains this famous. contains this famous minus log P term, which we call the entropy and information we assign to this term, plus here simply the internal entropy of each species and the log of the n factorial, which comes from the fact that the molecules of the same kind are indistinguishable. When you take the macroscopic limit of stochastic thermodynamics to go to the deterministic description, the Shannon entropy The Shannon entropy in terms of concentration that I was describing before has nothing to do with this term, which disappears because the probability distribution in the macroscopic limit become very narrow, but it entirely comes from this log n factorial using a Stirling approximation. So it's an emergent notion of entropy at the deterministic level. Level. And comparing in which situation the stochastic description coincides with the deterministic is also very interesting. In linear networks, it's always the same, but in general, it's not, and it's particularly not close to phase transitions or bifurcations and typical non-linear phenomenon like that. Okay, so to conclude, Okay, so to conclude, I will say that what are the fundamental findings is that the topology of the network plays a key role in defining the dissipation and thus the thermodynamics of these networks and that the thermodynamics and information once again, even at the deterministic level, are fundamentally related. It's a different notion of information, but again, it's striking to see. But again, it's striking to see how a non-equilibrium thermodynamics and information really go together. And I didn't spend much time on the application and the stochastic description, but we have now a framework that can really be used to study energy transduction from molecular motors to entire metabolic networks. And we can also study the cost of various cellular information processing. Information processing and even computation tasks. And I finish with two perspectives. One first question, I think it's a very interesting one and nobody really knows the answer yet, is to what extent energy and information constrains biology at higher level. So how far can we go into explaining biology using energetic Using energetic and information considerations, fascinating, open topic. And the second one is a warning: be careful. Udo commented on that also yesterday. The same problem also appears here. When one goes to higher levels, keeping track of the energetics can be very challenging because coarse graining Because coarse graining can lose very meaningful information to keep track of that energetics. And we did various work on that that I didn't present. We know how to coarse-grain enzymatic steps, for instance, but there's really a challenge on how to bring those methods to study large macroscopic biological systems that have all these underlying layers of non-equilibrium. So it's not at all. Non-equilibrium. So it's not at all that we can take any dynamics of a biological system and apply thermodynamics or stochastic thermodynamics. We always need to be very careful in making sure that the description we have keeps track of all the degrees of freedom that are out of equilibrium to predict the right thermodynamics. And with that, I thank you very much for your attention and I'm I open I hope to have interesting questions. Of interesting questions. Thank you. Okay, thank you very much, Massimiliano. While people gather their thoughts for questions, which you could either put into the chat window and I'll pick them up from there, or if you just want to unmute yourself, I think we can do it both ways. I'll kick off with sort of a simple question. I appreciate this message that concentrations and probabilities are not interchangeable concepts. And probabilities are not interchangeable concepts, particularly because I think I have made the mistake of conflating them for my students. When I teach computational neuroscience, the first lecture is about the NERSC potential. And in electrochemistry, if you have two sides of the membrane, you are trying to figure out the concentrations on the two sides. I talk about it as if I was randomly choosing a sodium ion from one, from this enclosed box. And in that case, I think there is a quantum. And in that case, I think there is a quantitative relationship between concentration and probability. So the question is: there are some situations where the two concepts could be very closely related. Would it be fair to say in closed systems that really linear systems? So it's every linear chemical reaction network. So everything that can be written as a sequence of reaction, A. Sequence of reaction A goes to B, goes to C, then can split into D and E, but it's always one reaction, one species. In these systems, which are called linear systems, there's a full equivalence between the two. And whether one uses stochastic thermodynamics or the framework I just described, it's basically identical. But as soon as there are nonlinearities, As there are non-linearities, A plus B gives C, then one faces a fundamental difficulty and there's an explosion in the information needed to describe such a system at the stochastic level, because one really needs to go to this huge space of how many A, how many B, how many C there are compared to simply three. Compared to simply three numbers, which are three concentrations. So, describing at the stochastic level with a chemical master equation non-linear networks is extremely difficult. Luckily, in the past, a lot of very important work by Terrell Hill and these pioneers who studied molecular motors and developed thermodynamics concepts for these systems. They were always describing. Always describing enzymes as bilinear networks because it's always something binding from the outside to the network, unbinding, rebinding. So it's always can be described by linear networks. But nonlinear systems are very important and they are, of course, dynamically infinitely richer because we know that this system can have at the deterministic level by stability, oscillation. By stability, oscillations, and at the stochastic level, it's still quite challenging to calculate the behavior of the fluctuations around those behaviors. Can I also make a small comment on this? Is that from a chemistry point of view, I think what is being assumed here is that the solutions are ideal, that the metabolites are not interacting with each other. So the actual thing that should go into the chemical reactions are the activities or the thing which has fugacity in it. Where which has fugacity in it. So the correspondence between concentration and probability will become much weaker there because it's not only the numbers of individual molecules that affect the reaction rates and free energies, but actually how they associate with each other in the 3D space as well. Yeah, this is true, but we have preliminary results showing that this can be done. So the trick. So the treatment of interactions, we are doing it in the framework of electrochemistry with also charge Coulomb interactions, is not a major problem for the theory. It can be dealt with. Thank you, Parasharam. I've got a couple of questions coming in from the chat. So, Amit wants to ask: in the case of diffusion reaction dynamics, how does your formulation Formulation in terms of deterministic dynamics relates to stochastic PDE models, for example, with active matter? Ah, okay. There are two. Okay, let me split this question in two. Sure. The first is how to do this, to deal with the stochastic version of that. This we haven't done explicitly. We know how to do it. It's simply heavy. One can, of course, write. Heavy, one can, of course, write the chemical master equation with, in addition, space degrees of freedom that would be the stochastic analog of these, which in the macroscopic limit give rise to these deterministic equations. Dynamically, there are various people have worked on that. It's not been done in detail for the thermodynamics, but I The thermodynamics, but I don't think one would encounter major obstacles at that level. However, this would still be in the frame where we assume elementary reactions. So this is the second part of the question. Active matter is part of the warning I was giving here at the very end. Keeping track of the energetics at higher level is a major challenge. You know, if you, if in active matter, typically the dynamics that people write, and Langevin type of dynamics, etc., they are hiding a lot of degrees of freedom that are not at equilibrium. And so trying to build energetic consideration only on those degrees of freedom can be very dangerous and need to complete underestimates of the energetic cost for that. Cost for that. So, various people, including Udo, Gaspar, tried to make some arguments under what condition things can be made thermodynamically consistent, but there's a big warning there. In most situations, you are already at way too much coarse-grained level of description to be able to use these energetic considerations. I'm not saying it's impossible, and I think it's a very interesting. And I think it's a very interesting research direction, but one should certainly be very cautious. Okay, here's another question going back to the cloaking example. So the picture you showed has uniform external flow. What about other flows? And let's see, perhaps you're dealing with Laplace's equation itself. Yeah, so in this paper, The in this paper, it's even 2D, so you might say, it's not very meaningful. Why is it not 3D? The reason is that in that case, the cloaking condition could analyte, we could show analytically that this chemical reaction network could achieve the cloaking conditions. If you go beyond linear profiles, more dimensions. More dimensions. The concept can be used and applied, but I think one would have to go into more numerical methods to try to find the chemical reaction networks that can generate the chloroactin. So this was more a proof of principle than an extensive study, but I think it shows already some interesting concepts like one. Concepts like one can almost completely feed the chemical reaction network using the gradients. And this is also an interesting question. There might be situations where actually the Cloaking device can be fully powered by the gradient in which it is operating. But there's much more to be studied there, but analytically, one gets quite rapidly limited, so there might be some numerical. Some numerical work to be done and optimization there to be done. Okay, zooming out to another level of generality. Does your framework give a handle on, say, quantitative estimates for the overall costs of information processing in a cell? This question is from Ilka Bishops. So, what can we say about okay, overall is the tricky part. tricky part uh certainly specific task uh specific information processing operation yes and and we are we are really Rory Britton who's here is working specifically on on analyzing the energetics of some of these processes where we know that we can guarantee that we have the right thermodynamics but in in general there's always this problem where an entire cell once again is Entire cell, once again, is processing information using mechanisms that themselves are burning a lot of energy in hidden degrees of freedom. So, there's always this fundamental question to what extent can we make sure that we have all the information about those non-equilibrium degrees of freedom to claim that it's really the energetics of the full cell, but at the level of But at the level of pieces of chemical reaction network doing computation, certainly the answer is yes. At the larger network level, the same caution that I mentioned at the end applies. So I have a question that follows up on that. So imagine if I knew all the internal states, let's say enzyme plus substrate and product, and then substrate and product and then in that case even if the kinetics is not mass action uh i'm guessing i can still draw the same conclusions that you have right if i knew all the if i knew all the thermodynamic states of the system basically yeah okay and so the problem in coarse graining is that for many enzymes i do not know the mechanism uh and as a result uh uh there's some hidden uh there's some hidden uh states a substrate plus enzyme plus product that that will contribute to the free energy That will contribute to the free energy, but they're not part of my equations. Yes, and we can even be a bit more general than that because we can say that there are classes of mechanisms that we can coarse grain. This is work that we did with Arto Bashtel. Actually, the reference is somewhere here. So, there are classes of enzymes, and we think most cytozolic enzymes we can cause grain. Enzymes, we can coarse grain. The key point, let's, I will not go into the technicalities, we know precisely how to define that in terms of emergent cycles in the mechanism, but the simple way to say it is you need to make sure that there are no hidden loops into the coarse grain mechanism that are burning energy. That's the qualitative picture. If the enzymatic mechanism has some hidden Hidden futile cycles, you might sometimes call them. But again, this is hand-wavy. The precise statement is in the paper there. Then you can coarse grain. And that's why Michaelis Mentan, substrate inhibition, all these things, we know we can coarse grain. And that's a very good news. We were afraid that this would be a major problem. But in more complicated mechanisms, like transmembrane mechanisms. Like transmembrane mechanism, one is drawn here. You might need to know not only the net flux through this coarse grain mechanism, but you might need more than one flux. You might need two fluxes across this. And then there's a question of how do you get this information experimentally? Can you measure it? This is a bit more tricky. Is it? So it's definitely true when you have these cycles, as Massimoano says. As Massimoliano says, yeah, you definitely, if you're missing cycles, then you're going to be missing dissipation. I wonder if you can apply some of the results like Udo was showing yesterday to still derive bounds in those cases, but they just won't be that tight. Yes, but of course, the bounds that Udo was describing have to do with fluctuations, and most of what I was saying here is deterministic, so there's no fluctuations. Is deterministic, so there's no fluctuation. But of course, one of the advantages of the uncertainty relation is that there you can make statements independently of how many hidden mechanisms there are. Of course, the bound becomes very loose, but the result still holds. Yeah, no, I guess your point. But yeah, then I was thinking about your stochastic level description, but yeah, of course, most of you were talking about the deterministic level there. Yeah. So, I'm going to say that way. Yeah. Very good. So, we have about 10 minutes before the poster session officially starts. We can keep this channel open for discussion. That's fine. If people would like to get to their posters on time and take a short break before, now would be a fine time to segue. But we can also entertain more questions. Entertain more questions if there's more questions for Maus Miliano. So, of course, questions can be pursued through other means as well, indirectly. So, let's thank Massimiliano again for a very interesting talk. And you can hear the hundreds applauding, although they're all muted. And so we will, that's the last formal. That's the last formal talk for today. We'll start again tomorrow morning. But for the next two hours, those presenting posters should be at the Zoom links for their posters, which you can find in the conference webpage schedule. And if you have trouble finding that, you can write to me or write to the Burrs staff, which is help at burrs.ca. 