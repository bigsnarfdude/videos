Okay, so welcome everybody to this afternoon session. We resume with a talk by Simon Rudine Meyerson, who will talk about two-dimensional delta method and pairs of quadratic forms. Thank you so much. Well, so I appreciate very much the invitation to this conference. I'm having a great time. I hope everyone else is too. And yeah, this has been a lovely week so far. I'm looking forward to the rest of it. Looking forward to the rest of it. Okay. Oh, yes. So I would like to begin with this footnote. So important note, right? So you can read my slides online. You can go to tiny.cc forward slash time dash list. Tiny.cc forward slash time.list. So in the in the in the event. In the event that, for example, they switch to the generator and we have to, you know, the projector has to be restarted, then it'll be handy to have these on your phone so you don't lose track. Okay, so recommend downloading these slides at tiny.cc forward slash time-list. Right. So, and everything I'm talking about today is going to work with Jung Sian Li and And with Pankaj Vishi, who are also grateful for co-organizing this week. So this is kind of okay, maybe it's presuming too much to claim to be third in a series, but Collier Talen's talk and Tim's talk were both on closely related topics. So we've got a bit of, all right, that's probably about. It's got a bit of all right, that's probably better. Um, so yeah, this like those talks, this is a this is a talk about um uh some uh some quadratic forms, some systems of quadratic forms. And we're going to prove some some Hasse principles and some asymptotic formulae. Uh, so I'm going to use this notation throughout the whole talk. So, vector f is going to be a system of homogeneous quadratic forms. Quadratic forms. There are R of them. There are S variables. The variables are whole numbers. They're called X. Vector X is the vector of S integers. And okay, I'm also saying here that S is bigger than 2R. And the reason for this is that then we expect there to be lots of solutions to F equals zero, basically. Basically. Okay. So we're looking at solutions to f equals zero. So all of these are quadratic forms equal to zero, where the variables are integers in some box. And I have this notation here to talk about that. So n subscript fw is the sum over non-zero integer zeros of f. Zeros of f, so non-zero vectors of integers such that all of these quadratic forms vanish at x. And we're summing some smooth weight evaluated at x over b. So this is a technical device to get the variables to be between minus b and b. And we, okay, we use a smooth weight here for some analytic reasons, but the important some analytic reasons but the important thing um is that uh the important thing is that this this term w of x over b it forces all of the integer variables to lie between minus b and b so we're trying to solve these equations in integers between minus b and b okay and uh oh no i also like dot product notation so that you're going to see a bit of that so alpha dot f is the this this is a quadratic form with real This is a quadratic form with real coefficients where we take a linear combination of these R quadratic forms with coefficients alpha one up to alpha R. So alpha is going to be a real vector with R entries. And we want this system of R quadratic forms to genuinely be a system of R quadratic forms. And for And for that reason, I'm going to assume that all of these real linear combinations of the quadratic forms are non-zero. If one of the quadratic forms was a real linear combination of the others, then we wouldn't really have a system of R quadratic forms. It would be more like a system of R minus one quadratic forms with one of them repeated twice. So I'm just making that assumption for the rest of the talk. Okay. So I say that this. I say that this counting function, which counts integer solutions between minus b and b, is the main thing that we're going to talk about. So many people here are very familiar with this, but just so that we're all on the same page, I'm going to remind you how big we expect this function n to be. So how many solutions we expect there to be to this system of equations. So what we do. What we do is we think of this vector of integers x as being similar to a random real vector. So we imagine that we model this vector of integers by a vector of random real numbers. And we assume, so since we're interested in integer solutions between minus b and b, we can assume that these real numbers are between minus b and b. Okay, so there's a slight problem here that if the variable That if the variables are real numbers, then the quadratic forms are almost, you know, the probability that the quadratic forms are going to be zero is just zero. I mean, the probability of choosing one of these very rare real vectors, which actually lies on the zero locus, is zero. So we're going to model the variables by a random real vector, and we're going to model the quadratic forms by the integer part of the vector of quadratic forms. Of quadratic forms. Okay, and then this gives you this prediction for the number of zeros to this system of equations. So this is what circle method people call a singular integral, even though you can't see an integral there ever. And so the size is typically some constant times b to the s minus 2r. So this is one reason to require the number of. So this is one reason to require the number of variables to be bigger than 2r so that this heuristic predicts lots of solutions. All right. So this is not completely correct because we could have some p-adic obstructions to solubility. So, okay, here's some quadratic form with a p-adic obstruction to solubility. And so And so this should say zero, not one. There are no non-zero zeros of this quadratic form. Okay, so we do a version of this same heuristic, but over Qp instead of R, and then we multiply all the probabilities together, and we get this expectation for the number of solutions to this system of equations in integers. Equations in integers of size at most b. Okay. So I've been lazy about one thing I've forgotten about the weight so that I could describe this more easily, but let's not worry about that. Okay. So we expect that we have here then a sort of prediction for how many zeros we expect to find in integers between minus b and b. Is between minus b and b. And when the circle method works, this is the prediction it's generally going to verify. And so we're going to go and try and prove this asymptotic formula. So I've just made some room here. So this thing, this asymptotic formula, sometimes people call it like a Hardy-Littlewood asymptotic formula. And I'm going to call it the quantitative Hasse principle. Okay. So here I've used this little O. So here I've used this little O, and okay, now I think probably everybody really does know this, but I've just recalled here this big O and little O and Vinogradov notation. So, okay, and then yes, so the Hasse principle is basically that one side tends to infinity if and only if the other does. And so, in that sense, this is like a more quantitatively precise version of the Hasse principle. Okay. Okay. So this can't always be true. So if the number of variables is really small, then this is certainly not going to be correct. But also, for some subtler reasons, if we have a single quadratic form and a fairly small number of variables, three or four variables, then it turns out that there can be some extra log factors, which is related to the product over P here. Related to the product over P here, not converging. So, this is a technical issue. And so, really, what we're interested in is a very slightly corrected version of this formula. All right. So, College Talen has already told us all about the Hasse principle, what's known about the Hasse principle for systems of quadratic equations like this. So, here are some highlights. Um, so here are some highlights from that. Uh, and for the quantitative Hasse principle, for the asymptotic formula, um, we have uh uh so when we have a single quadratic form, there are lots and lots of ways to prove this asymptotic formula. Um, so for example, the circle method this works for um a non-singular non-singular quadratic form in five or more variables. Or more variables. And some other, okay, here are some references giving some alternative ways to prove this using height zeta functions, using the geometry of numbers. There are lots and lots of ways to do this. All right. So we've also already heard mention of in Tim's talk of Bertley's result for systems of quadratic, in the case of systems of quadratic forms. So Bertley's result is more general. Result is more general, as is the method of Hardy Littlewood and many other workers. But I'm restricting just to the quadratic case here, just to keep all the notation and all the amount of stuff I need to say under control. So in the quadratic case, then this is Bertley's result that we get this asymptotic formula when S is at least this quadratic polynomial in R and the And the system F is smooth. So by smooth here, I mean that there are no singular points over the algebraic closure. So that this condition that I've written explicitly here. Okay. Right. So there's been lots of work, lots of work by the Circle Method since Berkeley. Work by the Circle method since Birch's sort of inaugural result on this kind of problem. One particularly interesting piece of work in terms of having influenced a lot of later results, a lot of later writers, is Heath Brown's so-called new form of the circle method. So, this is also called the delta method, which is what I'm generally going to be calling it. So, this is So, this is a slightly corrected version of this quantitative Hasse principle as soon as the number of variables is three or more. And versions of this delta method were then used, so all except the last result that I've listed here use some. Use some version of the delta method. So we have, so the three results here are a series of, in fact, all four results here are a series of works on pairs of quadratic forms of various shapes. So we have here in the first two results, we have some singular pairs of quadratic forms. So a pair of quadratic forms, which is not necessarily smooth. forms which is not necessarily smooth which is not smooth but which has sort of known and well understood singular locus and so here we're so so the the result of birch here requires 14 or more variables and so here we get and actually in this setting where that where there's a singular locus of dimension a projective dimension one it would require 50 Projective dimension one, it would require 15 or more variables. And here we're getting away with nine variables in this case, 10 variables in this case. And if you assume something like Li Rh, then you can reduce the nine variable result to eight variables. Okay, so this is an interesting two-dimensional version of the delta method introduced by Browning and Munchie. Browning and Munchley. And this is a sort of refined application of the same idea by a student of mine, Nana Aurala, which turns out to be a little more technical than you might expect. And so Munchi was able to build on these ideas and give a two-dimensional Telson method which worked on smooth pairs of quadratic forms in 11 or more vectors. In 11 or more variables. And Vichy was able even to go down to nine variables in a function field setting. So these three use sort of a related two-dimensional version of the delta method. And this is a different and novel two-dimensional delta method, which gets as few as none. Delta method, which gets as few as nine variables for smooth pairs of quadratic forms, but over the function field fqt. Okay, so these are the, so I've put these up in particular because these are all attempts to use the delta method for systems of more than one quadratic form. So it was originally introduced for a single quadratic form. And so we'll explore in a little more detail later in the talk what it might mean. Later in the talk, what it might mean to take this one-dimensional method and try to apply it in two dimensions. But these authors did various versions of that. And in particular, none of these four results quite has the same shape as the results of Birch or Heath Brown. They all have, except for Munchies, they all have slightly more, not quite the same conditions. Not quite the same conditions, and Munchley's result has more variables than the ones with slightly more exotic hypotheses. Okay, so there are, I should mention a little bit some approaches since I'm listed. So I'm really talking about this delta method and I'll tell you in more detail what that means in a moment. And in particular, I'm talking about applying this delta method to systems of two equations. To systems of two equations. But while I'm talking about what's known for pairs of quadratic forms, I should mention a few more results in that vein, which are not obtained by the delta method. So if the system of quadratic forms is split, meaning that it's the sum of a pair of quadratic forms in one vector of variables and a vector of quadratic forms in another disjoint vector of variables. So we have. Variables. So we have a group of at least five variables here and a group of at least five variables here. And the whole vector of quadratic forms is a sum of two vectors of quadratic forms like this. If each vector has at least five variables, then Heath, Brown, and Pierce, by a slightly different version of the circle method, got down to 10 variables. So this is improving on Mundle's result by one variable in this case. In this case. And okay, and Pierce Gendler and Magic Wood were even able to were even able to get a version of this result for systems of three quadratic forms in 20 variables, so reducing the number of variables by seven compared to Birch. But there is a slightly more elaborate condition than just that the system be split and smooth. That the system be split and smooth. And okay, Tim, and then these last two results were mentioned in Tim's talk. So this is my earlier work. And here we have sort of one case of the work that Tim talked about is that you can go down as low as six R variables for certain special systems of quadratic forms. Or quadratic forms. Okay. So, this then, finally arriving at the point, is the result that I'm going to talk about today. So, we have 10 variables and a smooth pair of quadratic forms. So, zeros of a non-singular pair of quadratic forms in 10 integer variables. And we get this quantitative Hasse principle. Principle. So that's 10 variables, and we can reduce it by one variable if one assumes Clearh. So in particular, this is improving on Mundli's result by one variable and is one could also think of it, I guess, as removing the split hypothesis from this special case of Heath-Brown and Peirce's results. It is also a genuine analog of the Analog of the result of Visier over function fields, because of FQT, you know the Riemann hypothesis. So the correct comparison between this result and the other results is one should ask how many variables one needs if one assumes GRH, because in the function field setting, one essentially has this. And so since we have nine variables, this is the correct sort of analog of the FQT result. Sort of analog of the FQT result. Okay. And so the proof is by some version of the delta method, some version of the delta method actually, which is quite similar to what Vichy does. And I've said words like version of the delta method quite a lot of times recently. And actually, my key objective in this talk is to try to explore what it might mean to have different versions of. Might mean to have different versions of the circle method. So I sort of said many times that these results are obtained by different versions of the circle method. And, you know, at a certain point, I noticed that people would say this kind of thing. I mean, maybe inspired in particular by Eve Brown's article introducing the delta method, which calls it a new form of the circle method. And so, yeah, I was kind of thinking, what does it actually mean? Yeah, I was kind of thinking, you know, what does it actually mean to have different forms of the circle method? Like, how when do I know that my form of the circle method is the same as like Anna's form of the circle method over there, you know? So what I did, oh, the text is going to go off the bottom of the slide. That's unfortunate. Oh, well, we'll survive. So, oh, how bad is this going to be? Okay, it's fine. This slide is fine. It's going to be a problem later. Is it? No, okay. It looks like this one is fine, which is why I didn't notice it. Okay, so right. So, versions of the circle method, then, what does that mean? So, we have here our counting function, our way. Counting function, our weighted counting function. And now I'm going to introduce this exponential sum, which depends on this parameter alpha, the real vector from before. And this is a sum over all integer vectors now of e to the 2 pi i alpha dot f of x. So instead of summing a weight over the zeros, we're summing the weight twisted by this character over all values of the. Values of the of all integer vectors x. So the circle method on one level is just this easy identity. So the circle method is just the elementary fact from trigonometry. But if you integrate this exponential sum over all values of alpha in the unit hypercube, you get this counting function. So this is just writing the counting. So, this is just writing the counting function in terms of some kind of Fourier transform of the values of the quadratic system of quadratic forms. And really, to be the circle method, one needs to take this identity together with a nice decomposition of the unit hypercube. So that's what really makes it the circle method, is the modern understanding, I think. Okay. So, buh buh buh. So here's the super classical version. So here's the super classical version of it, the sort of original version of the circle method. What we're going to do is we're going to tell, I mean, it's not quite the original version, it's sort of Vinogradov's presentation of the original version. So the idea is that we're going to decompose the one-dimensional unit hypercube, that is the interval 0, 1, and we're going to decompose it using the fairy dissection. So we're going to take all of the rational numbers with denominator at most q in lowest terms. At most q in lowest terms, denominator at most capital q in lowest terms. We're going to put them all in order. For each one of these rational numbers a over q, we're going to consider the fractions on either side of it once we've put all of these fractions in order. And then we're going to take these end points here. So this interval here is a little interval containing. Here is a little interval containing the rational number a over q. And we're going to take this interval and we're going to think of this interval as being a small interval around this rational number a over q. So we've divided up the unit interval into little short intervals, each one of which is more or less centered on a rational number a over q. And so what I've written here is So, what I've written here is exactly just this trigonometric identity, but breaking up the interval 0, in the case when r is equal to 1, but breaking up the interval 0, 1 into these arcs here. Okay. And it turns out that on an arc like this, one can give some nice sort of estimations, approximations, and formulae for this exponential sum. Exponential sum. And that's what powers the classical circle method: that this exponential sum is well behaved on a short interval around a rational number of small denominator. So that's the superclassical version of the circle method. And in some sense, everything done after that is called the circle method by analogy with this. This is the sort of canonical thing that one means when one says the circle method. So So, what's the delta method, and why is the delta method said to be a form of the circle method? Well, Heath Brown's delta method is essentially a smooth vari dissection. So instead of decomposing the interval 0, 1 into short intervals centered on rational numbers, we're going to do a partition of unity supported on the interval 0, 1. So we're going to decompose the indicator function at the interval 0, 1. function at the interval 0, 1 into smooth functions which are supported or essentially supported on small intervals around rational numbers of small denominator. Okay, that's a lot of words, but it looks like this. So the idea here is that we've got some function. So I guess there's some information cut off at the bottom, which I can just tell you. So we've got some function. Tell you. So we've got some function p theta q. So this is some smooth function. And we've got some set of parameters big p. So big p is some unknown set of parameters. So I basically am not telling you anything about what p looks like. It's just some set. This curly q, this is some set of denominators, little q. Of denominators, little q. So, this is some set of natural numbers which are allowed to be the denominators of this a over q. This lambda, this is some set of numerators, and this should probably be a subgroup of z mod qz, something like that. So, for example, this gamma, this lambda rather, this lambda could be. Lambda rather, this lambda could be z mod qz to the r. Okay, so we've got some set of denominators that are allowed, we've got some set of numerators that are allowed, the numerators are drawn from some subgroup of z mod qz to the r. And then I've got here some co-primality conditions. So, for example, this might just say that this fraction has to be in lowest terms. So, this and then so this thing here, if this smooth function This thing here, if this smooth function p has small support, then this term here is requiring alpha to be close to a over q. So if this p has some very small support, then this term is going to be zero unless alpha is close to a over q. And so this is sort of like requiring alpha to lie in a small interval around a rational number of small denominator. So this in particular looks similar. This in particular looks similar to this, but here we have r equal to one, and instead of having a smooth weight, we just have sharp end points on our integral here. Okay, so all right, so P is, so this is what I said, yes. So P is some set of parameters, lambda is some subgroup of Z mod Q Z to the R, and P is a Schwarz function. And so, the advance, so this has the same advantages as the fairy dissection in the sense that one can get good estimates perhaps for this exponential sum when alpha is close to a rational number with controlled denominator. And also now, and this is sort of the advantage of the delta method, this function p doesn't depend on a. So, I've laid down a requirement for something to be called the delta method. You should have a smooth wall. Delta method, you should have a smooth weight here, which doesn't depend on the numerator A, but only on the denominator Q. And this means that one could perhaps take advantage of some averaging over A. And if the dependence on Q is nice, perhaps one could also take advantage of some averaging over Q. That's the idea here. The idea is that maybe one can estimate not just this exponential sum here, but one can estimate maybe the sum of this whole thing over A. Sum of this whole thing over A. And this should give one some kind of advantage. Okay, and yeah, the standard thing to do is to apply Poisson summation to this exponential sum in the setting. Okay, so in particular, so what I've done here is write down my idea of what an R-dimensional delta method should be. When he introduced it, Heath Brown actually said, Actually, he said that there should be, he gave a one-dimensional delta method, so that is r equal to one. And he didn't have a set of parameters p. You could think of p as being like some singleton or something. So there's no parameter theta. There's just this inner sum. The set of possible denominators is all of the little q's between one. All of the little q's between one and big q. And the coprimality condition is genuinely this q is coprim, this fraction being in lowest terms. And then this is really what Heath Brown said. So one can also write Mundley's version of the circle method in this form. And then one finds that there genuinely is a parameter that you can't average over. So this set P on the outside. um so this set p on the outside um is the natural numbers up to to q q capital q is some parameter right so uh um just like in the fairy dissection you get some parameter where you just you have to decide how big the denominator is allowed to be um uh so here you have some parameter big q the parameter that you don't get to average over is a natural number up to q to the two-thirds um and then the set of possible denominators set of possible denominators is has to be is the natural numbers up to q big q that are divisible by uh by the parameter um so you get to do some averaging over this set but not over the the set uh curly p and um uh so over fqt um uh we can we could actually kind of have the best of both worlds over fqt Both worlds over FQT. So Pancourt showed that one actually has a version of this where this is not just a Schwartz function, it's actually an indicator function of an interval. So you really have all of the advantages of both methods. And so now we come to, in a way, the point of the talk. So after I say this, you can have a nap. So the point is that if Is that if you accept for the moment my premise that this is what we should call a sort of version of the delta method, that this is what if something looks like this, we should call it the delta method. So if you accept that premise, then the reason that one might expect there to be lots of different versions of the delta method is that we have three conflicting demands here. So this set of parameters, this curly P on the outside, this is This is the way in which everything depends on theta is probably going to be a complete mess. So, probably we aren't going to be able to take advantage of any averaging over this curly p. And therefore, we want this set curly p to be as small as possible, because probably we have no averaging over this. Probably the bigger p gets, the worse everything is for us. The more terms we have to estimate. It's bad. We want that set to be small. We also want alpha to be close to a over q. Alpha to be close to a over q because that makes the exponential sum easier to estimate. And we also want the denominator q to be small. That's because somehow this is like a p-adic version of the condition that alpha is near to a over q. If q is very big, then somehow we have some large loss for p-adic reasons. And somehow these three requirements are in tension with each other. So to me, the To me, the existence of these three competing demands is sort of a reason why one might expect there to be lots of different formulae of this form that people have written down and used for some purpose. Okay, so it's all still there. I've just moved it slightly up on the screen. And so for completeness, I should say that Right, so for completeness, I mentioned before that there were also very interesting results which aren't obtained by the delta method but by other forms of the circle method. So if you like, a form of the classical circle method would look like this. So here there's a first term which looks, yeah, which is really the best of both worlds. This is called Of both worlds. This is called the major arcs. We have a sum over some denominator little q. We have a sum over fractions a over q. And then we have an integral over alpha close to a over q. So this main term is super nice. It's this best of both world situation where we have no awkward set. Well, we have no awkward set of parameters, curly p. We have no smooth weight, we just have a nice, nice integral, and a nice interval. On the other hand, we have here an error term, which is sort of the worst of both worlds in some ways. So depending on what exactly one is doing, one might have some set of parameters here. So it's pushing it a bit, but if you want to write Berkeley's method in this form, then you have like a. method in this form, then you have like a set of parameters here, which is a set of dyadic integers. And then you have some sum over A over Q's, and then you have an absolute value on the exponential sum in there, which limits your ability to do averaging. So this is a classical circle method where you have major arcs and minor arcs, and you have an absolute value on the exponential sum on the minor arcs. On the exponential sum on the minor arcs. And yeah, one can see that actually, it's maybe it's a stretch to say that this is what something should look like to call it the classical circle method. But, you know, it does kind of make sense. I mean, all of the examples I've talked about can be sort of shoved into this form. And yeah, it does kind of capture. Kind of capture the idea that there's some possibility for averaging in some things but not in other things. Yeah, okay. So I guess maybe I should pause for a moment to see if we have any questions. So what I have next is I have, for aesthetic purposes, I have written down the form of the circle method that Form of the circle method that we use to prove our theorem. And then I have some discussion of sort of technical aspects. But yeah, this is in a way the main point that I wanted to reach, which is a description of sort of different forms of the circle method and what it might mean to have a different form of the circle method. So, does anybody have any thoughts they would like to share? Please. Um, so I didn't hear every word. So, you're talking about the inhomogeneous problem, like detecting. Ah, I see. You mean like over number fields? So you mean as opposed to like real coefficients? Ah, I see, I see. Yes. Okay, so that's a very good question. Yes. So the question is about Diphantine inequalities. So the question is, what if we had, instead of a system of equations with integer coefficients, what if we have a collection of What if we have a collection of quadratic forms with real coefficients? And maybe we instead of looking for them being equal to zero, we looked for them being between zero and one, something like this. So some Diophantine problem with real coefficients. So, yeah, so there exist versions of the circle method for this also, and it's true that they don't fall into my categorization here. So I think that would. Here. So I think that would, yes, that would require a different formalism. And in fact, I'm not sure that we're at the point where we can say, like, I kind of feel like we're approaching the point where one could maybe systematically write down and sort of categorize the different ways people have applied the circle method to solve problems with integer coefficients or with coefficients and number fields. But maybe the But maybe the study of the Socle method for problems with real coefficients is maybe not developed enough to confidently say, okay, it always looks like this. I mean, there are certainly some, in particular, there are some versions of the Davenport-Holbron method, so the circle method for forms with real coefficients, which are really quite elaborate in the different dissections that they use. And I feel like in particular, And I feel like, in particular, yeah, there's some very advanced work of Friedrich Goethe and collaborators, which recalls, to me, is like reminiscent of Ben Green's work on quadratic forms in eight prime variables in very tantalizing ways. Yeah, so I think there's lots more to be said about the circle math. Uh, quadratic for about the circle method for forms with real coefficients, but it doesn't really fall into this binary, yeah. Uh, further reflections, please. No, no, I have more to say, but uh, I just wondered if anybody wants to ask any questions at this point. Yes, that is a good point. Yes, I guess I hadn't sort of put that carelessly, but I think what you're saying is true. But the idea of this thing being an indicator. That the idea of this thing being an indicator function of an interval and a Schwartz function is probably okay over FQT. Yes. Yeah. I mean, somehow, yeah, I guess it's a slightly weird situation where somehow the best situation of all is that this thing is the indicator function of an interval, and the second best situation is that it's a Schwartz function, even though these are kind of like different things, and you'd think there'd be something in between. Something a bit to it. Yeah. Okay. So what I propose to do now is to show you for decorative purposes. There's a lot of symbols here. So I wouldn't, you know, you can download the slides at tiny.cc forward slash timelist if you want to if you want to get into the weeds. But for aesthetic purposes, I'm going to show you the form of the circle method. To show you the form of the circle method that me, Pancarjo, and John Xian have used. So it looks like this. So here we have two forms. Oh, and some of it's gone off the bottom of the screen. Okay, I'll fix that later and the leg will still work. So yeah, so we have in particular, so this sum outside is the sum over the set of parameters curly p. So our parameters. So our parameter is like a tuple of two natural numbers called d and k and a primitive vector called c in z squared. And then here we have our sum over denominators with some kind of co-primality condition. Our denominators have to be divisible by little q, by little d, pardon me. By little D, pardon me. And then, so here I've written actually what you get after applying Poisson summation. And okay, yeah, so one gets, in particular, one ends up with some kind of integral times some kind of exponential sum. One gets two kinds of exponential sums. These dq of u are classical things that occur. That occur one is familiar with these things showing up in the circle method. We also get these more awkward exponential sums, which are very similar to certain exponential sums that showed up in Pankoj's work over FQT. And therefore, we have some estimates from that source for these more complicated exponential sums, which are similar to the classical exponential. Which are similar to the classical exponential sums, but you have some kind of additional linear condition on the vector of the vector A. Okay, so as I say, this is for aesthetic purposes. So let's see. So I have a few. Let's see. So, I have a few things to talk about. Um, so one is about where I can talk about where this version of the circle method comes from, like how we let me go back to that. Okay, yeah, so this our version of the circle method is this identity, right? It is this identity that the counting function is equal to this big sum here. Um, so I can talk about how we. So, I can talk about how we actually proved that the counting function really is equal to that big sum. Or I could talk about what we do after getting to this point. And I feel like it might be slightly more helpful to talk about what we do after getting to this point. But I'm open to suggestions otherwise. So once So once we've got to the so once we've got to the to the point where we need to right here we go so we've got here ah yeah this is a this is a good this is a good question actually thank you Yeah. Yeah. Right, so this is a good question. So here we have two terms, and this might look like a situation where the first term is the minor arcs, the major arcs, and the second term is the minor arcs. We've got two terms, and the first one looks nice, and the second one looks a bit messy. So, in fact, Messy. So, in fact, this is not quite true. So, the reason we have two terms is to do with the way that we get this partition of unity. And what one actually finds is that when this denominator, little q is small, then one can put the terms with that denominator, little q from both of these terms together. And that's the major arcs, if that makes some sense. So these two. So these two terms are, if you like, yeah, these two terms are just two different circle method shaped terms that come out of our method. And to reconstruct the main term, we really need to use both of these two terms in the case where little Q is small, where the denominator is small. And one can even kind of get some sense of this from the fact that this weight P, it is just visible at the bottom of. It is just visible at the bottom of the screen that when the denominator q is small, and when this parameter w, which is basically the distance between alpha and a over q is small, this p can actually be big. It can be as big as capital q. And that seems strange because while I'm kind of trying to think of this weight p as being just This weight p as being just like a little indicator function of an interval or something. But actually, what happens is that when little q is small, the function p1 and the function p2 kind of almost cancel out and add up to about one. And this is where one gets the major arcs from. So I'm happy to talk about this more later. Okay. So here then is the Here then is the general shape that I claim is the general shape of the delta method again. And so this is our experience of which once you've got the once we have our form of the circle method, this is our experience of what the Keralanges are, then in applying it to a pair of quadratic forms. So what turns out to happen, as is true also in As is true also in other applications of the delta method, is that so we want to save from some kind of averaging over the denominator Q, and it turns out that some bad divisors of Q are a problem. So there are certain primes that are bad, and the part of Q that is made of those bad divisors is a problem. Also, if Q has like a powerful part, high power divisors, that's also a problem. That's also a problem. And so we can only really average over the good part of Q, and we have to take the bad part of Q away and put it in some other sum. And so what we end up with is the formalism that I've put here. So we have the good part of Q is Q primed, and we have some sets Q primed and R. R such that Q factorizes uniquely into a good part in Q' and a bad part in R. And then we take the sum of the bad part R outside and the sum of the good part Q primed inside. And we want to save in the sum over the good part Q primed. And so we end up with a problem of the general shape that I've written here, where we have some sums over many parameters outside. Sums over many parameters outside, u is some vector of variables coming from Poisson summation, and then some average over the good part q primed inside. And I've written down the things that we're summing here. So the general pattern is so this u, as I say, is some vector of integer variables that wasn't in the original problem. It appears when we do Poisson summation. And typically, what one finds is that for this sum in red, which looks horrible, but is actually very nice, generally what happens is we have a good estimate for this as long as you is not somehow bad. And what it means for you to be bad is kind of reminiscent of actually some condition that Yulia asked about in her talk. Julia asked about in her talk: vanishing of some kind of Hessian determinant. This is not the exact condition, but it is reminiscent of the kind of condition that comes up for you here. Someone has some polynomial equations and congruences, and when they're satisfied, then U is bad and it's a problem, and we need to count the bad U. And for this, as is, you know, as has become kind of traditional. You know, as has become kind of traditional in applications of the Delta method, we cite results of Salberger and others that are proved using the determinant method in order to count the number of bad ewes. And it turns out that the so counting the, so even, so estimating this red part turns out to be relatively straightforward. And really, the big difficult part of the argument is once one has the estimate for. Is once one has the estimate for the good part, one has some rather unsatisfactory estimate for the bad part here. And then one has to handle the sum of the bad parts summed over u. And this is somehow where the main action comes when we want to count the bad u sufficiently well that we can handle that. Okay, so maybe I'll draw it to a close here. Draw it to a close here. Thank you for your time. We already had some questions. Any more questions? Okay, yes. So this is a good question. So these are these are related to Related to so I said here that we have these slightly less classical exponential sums. And so one can imagine perhaps that if this vector u so there's this condition here that some part of q divides some linear form in a so this part this So, this part, this term here, one already knows something about what this term does, mod Q. And you can maybe imagine that if some factor of Q also divides U, then this sum could be large. So we sort of already know that this term here is often congruent to zero mod Q. If U were divisible by some large factor of Q, then both of Then both of these terms here are often congruent to zero mod q, and one can kind of imagine that this might create a conspiracy where this exponential sum is very big. And so the congruences are basically saying that some linear form in U is divisible by some bad divisor of Q. So the congruences, in fact, are kind of explicable in the sense that one can really see that maybe some linear congruence in U. Maybe some linear congruence in U might make this sum large since we already know some linear congruence in A. In some ways, it's the equations that U satisfies when it's bad, which to me personally are more surprising. But I think to real experts, it's obvious. So it turns out, and this is still slightly. Turns out, and this is still a magical and wonderful thing to me, that the bad ewes are basically going to be the ewes that are on some dual variety of the variety you start with, which is tremendously helpful because people know so much about dual varieties and have studied them so much. And so it's always been very satisfying to me that the dual variety makes an appearance when you do quite a lot. Variety makes an appearance when you do Poisson summation in this way. Further questions? Um, right, right, yes. So there's always Right, right, yes. So there's always, yes. So in both of these terms, there's a good part of Q. And if we assume GRH, then we can do a very efficient double clusterman refinement. And yeah. And this is basically, I mean, when we say DRH, really what we're assuming is a bound for. Is a bound for for like an exponential sum, like not unlike this one, summed over q co square free q co-prime to some bad primes. So we use TRH in a pretty concrete form that is adapted. Yeah, doing doing that when if you if one assumes TRH, then actually it's all much easier and the double class and refinement comes quite easily. I mean, I say that there's actually some like formulating the bound that one needs under clear or a is delicate. But once Pancert had written it down correctly, then yeah, this is not so hard.