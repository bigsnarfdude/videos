I want it to be very easy. So, multilayer networks, at least in the nomenclature that I'm using, and already Bernardo said something about the difference, the different names that are attached to different structures, but I'm going to define that. So, in the Roman Cladder that I'm using, multilayer networks are very general structures, right? And the idea here is that they encompass a lot of... And they encompass a lot of particular cases which are very studied, like the multiplex networks, or I'm going to call them edge-colored networks, the ones that Bernardo talked us about, or node-colored networks, dynamic networks. These are all encompassed by the general structure of the multilayer networks. The idea here is that we can model this kind of multilayered network. So each kind of multilayer network can be modeled via. Network can be modeled via a matching kind of probabilistic symmetry at the level of the nodes. Okay, so let's see how. This is a depiction of a general multilayer network. So we have basically a bunch of nodes and we also have, oh, sorry, but we also have a bunch of layers and these layers are organized. And these layers are organized in this product space, basically, of different aspects. Here, L1 and L2. Each aspect is telling us something about the characteristic of the nodes or the characteristic of the edges or both. So then we work on the product space of these aspects to have the higher generality possible. So we have, as I said, multiple dimension of layerization, edges can go. Liarization, edges can go wherever. So within the same layer, across layer, across different layerization levels. And we have copies of the same nodes in different layer, like in the multiplex case, for example. So here the goal would be node clustering. And on top of that, using the inference on node clustering for getting some edge prediction. So prediction of edges. So, prediction of edges among nodes that we didn't observe. Okay. How to do so, and how to do so with invariants? Well, the sources of information, those information here are, we can decompose them in these two parts, right? Oh, sorry again. In these two parts, the structure itself of the network, so the multi-layer structure, which is telling us something, and the Telling us something, and the connectivity behavior of the nodes. So, the idea here is just divide these two sources of information in the two tools that we have in vision modeling. So, try to inform the model about the multi-layered structure via the prior that we put on the latent partition that we're going to use to do the node clustering. And then the likely part of the model is going to talk about the observed edges. Okay, so there's this. Okay, so there's this division. How to do so? Well, Bayesian paramedic clustering does this like customarily, I would say. We have seen a lot of examples. What we do is to attach, we take in this case a discrete random array, so one entry per each node in our multilayer structure, and we take it from a discrete distribution so that there's positive, non-negative probability. Positive, non-negative probability of ties. And then we define a random partition in this very intuitive way that two nodes or two copies of nodes are clustered together if they express the same entry in this random array. Okay. What about the distribution of the edges? Well, I'm spoiling everything. So, what about the distribution of the edges? Well, The edges, well, I want to do inference on a latent partition. So, my model is going to be defined conditionally on a latent partition. And a possibility is, of course, stochastic block models, in which the connection, the probability of connection between two nodes is solely dependent on the allocation of these two nodes in some latent partition. Okay. I did it again, I'm sorry. Because. Yeah, maybe I just don't touch it. Sorry about that. Okay, so what about how do we induce the multi-layered structure as prior information? Basically by choosing the distribution of the random probability measure or random probability measures which are going to generate x, so that the distribution of x is invariant with respect to some class. With respect to some class of permutation of its indices. And this class of permutation is gonna reflect the multilayer structure. So this is the general idea. And from now on, I'm giving you basically three examples of that with three famous kinds of multilayer networks. Okay, so the first example, which is actually the beginning of the story. So this is what we did at first. And then we said, okay, this is this. And then we said, okay, this is an idea, this invariance idea can actually be generalized to different kinds of multi-layered networks. Well, not color networks are pretty easy. Example, there's just one aspect, so just one direction. And the layerization, the division in layer is about basically a characteristic, a categorical characteristic of the nodes. Okay, so if you want, is well, the name fits it well, is a node-color network. Fits it well is a node-colored network in the sense that we are looking at a network with categorical covalence, basically. Okay, as you see, connections can go within and also across layers. So we may want to infer a clustering that being informed both by the structure and by the connectivity behavior may go also across layers. Okay, we want something like that. Okay, we want something like that. What kind of invariance at the level of the nodes is matching this kind of structure? So a good idea here, I would say, is partial exchangeability with respect to the division in layers. So what I'm saying is that I let the node swap within layer, okay, but not necessarily across. This is suggesting through the prior, it's inducing distribution holomorphily within layers. Distribution homophily within layers, but it's allowing for heterogeneity, as we know, across layers, as we know it happens with partial exchangeable models. And after Joanni's talk, you know everything about partially exchangeable models. Well, we know that we have a definity theorem that is saying that whenever we take each row of our random array to be conditional AID given a vector of random probability measures from one of the probability measures from one of the random probability measures and the random probability the vector having a distribution q on the space of vectors of probability measures well x is going to be a partial exchangeable array according to its rows so everything boils down to choose a particular queue what queue do we choose here well as i said we want uh clustering across so basically we want ties also Basically, we want ties also across different rows of the random array. So, a good idea is to go for hierarchical normalized complete random measures. A particular case is, of course, the hierarchical DP, which are this hierarchical compositions, let's say, of normalized completely random measure that I think we have seen a bit with Ricardo's talk. So my talk is going to be a reference stuff. So, my talk is going to be a reference of basically all the other. That's what I'm doing. Okay, if I define Z being the ties of an X that is generated by this vector of random variables, well, I'm just writing it like this. It's a partially exchangeable partition, and the first entry is just telling me, it's just the vector that is giving me the division in layer. Okay, so if you want the division in sub-population, So if you want the division in subpopulation with respect to which this array is the partition is partially exchanged. What about the likelihood? Well, it's just a very classical Bayesian SPM. Okay, so connections are given the partition and some block connection probabilities are just Bernoulli trials. And then the connection probabilities given the partition. The partition they are usually chosen to be beta for conjugate. Okay. Okay, this is just okay. This is what we have done with Daniele, Antonio, and Igor. And this is just a slide in which I'm telling you that it's good. So we have some theoretical results for the predictive clustering and co-clustering probabilities. So we can derive from those prior elicitation results. Those prior elicitation results. Basically, we can tune the hyperparameters of two choices of hierarchical normalized completely random measures so that the learning mechanism is more or less adherent to the divisioning layer. So it's kind of a tuning on how much you trust the division in layer as a prior information. Well, of course, we have a sampler. Sampler, we uh well we do good compared to basically all the state-of-the-art competitors. We have, well, after Giovanni's talk, you of course you know this, and whenever we play with partial exchangeable partition probability functions, we have for free Kolmogorov consistency. So, our model is projective, is basically ready-made for prediction. And this actually, you can see. And this actually, you can see that in the prediction performances, which are quite good. And it's a K step ahead edge prediction, meaning that a batch of K new nodes come into the network, and then we, according to what happened before to the other nodes, and knowing just the layer membership, we predict the connections of the new nodes among them and also with the old nodes. And this image here is just our application to criminalize. Just our application to criminal networks, which is nice. It has a bunch of interpretable results, basically. So, second example. These are the edge color or multiplex net. These are Bernardo snaps. Okay. So each layer has all the nodes. So, or better, each node has a copy in each layer. Edges go just. Edges go just within the same layer because basically the division in layers says something about a characteristic of the edges. So here the nodes are connected by the orange edges, the blue edges, the green edges. And these mocky lines that you see here just to underline that one in the orange net and in the blue net and the green red, well, it's the same person. Okay. And this is very important when we look at the kind of When we look at the kind of invariance that we want for this kind of network. Because do you think partial exchangeability is a good idea here? You see, it's not, right? Because if I swap like freely the nodes here, regardless of what I'm doing here, I'm just losing node identity. That's what's happening. So you can do that, but you're not using one of your information. That is, these people are the same people. Okay, so you're using less information. Okay, so you're using less information. So I don't have a model for this. So this slide is just to tell you multiplex network partial exchangeability is not a good idea. Okay, so it's just an example like this. What's a good kind? Well, what's a good idea? Well, it's fine to swap nodes as soon as, as soon as you remember what kind of swapping you have done and you do it also for all the other brothers. Other, you know, brothers, all the other copies. So basically, this is a rigid swapping of columns like this. And of course, if you don't really care about the ordering of the coloring of the edges, if it's actually a coloring, so there's no natural ordering in the labels that we put on the edges. So if also the edge kind, you want it exchangeable, you can let also rigid swapping of rows of this matrix. Rows of this matrix, right? Well, this is called separate exchangeability, as opposed to the joint exchangeability of the Aldo-Suver theorem, and well, is presented and used for mixed models in a paper by Petter and Giovanni. And well, that's it, basically. It's possible to build up a model for networks on top of this for multiple networks. I think so. Multiple networks, I think so. I don't have it yet. So let's pass to something that I have a model for, and it's dynamic networks, which to me are just a particular case of edge-colored networks, in which the coloring now is not really a coloring, well, it can be. It's a shading, if you want, because it has a preferential direction. Now the divisioning layer has an order. The divisioning layer has an order. So layer one is the connection of my four nodes at time one, layer three is the connection of my four nodes at time three. Okay, so basically what I'm saying here, if I want to infer some kind of evolving clustering, I have to ditch the row exchangeability, basically. Okay, the colon exchangeability is still fine, but I don't want to mix up times. So no row exchangeability anymore. This is the kind. Anymore. This is the kind of exchangeability that I want. Because I want to preserve node identity as before, but also the direction of time. How do we do this? This is what we are doing with Beatrice, which is actually talking just after me. And she's going to tell you everything about conditional partial exchangeability, which is the tool that we did, like the invariance idea that she developed together with her co-authors. Developed together with her co-authors, and that we are going to use here for this dynamic network model. So, again, she will be going into details. Let me just say that is exactly what the title says. Okay, so okay, at time one, we have some clustering, okay? It's Z1 with H1 clusters, and that's let's look at time I. What happens? The I-throw of my array. Of my array, given a vector of random probability measures, and given the clustering at time i minus one, okay, is independent from one of the random probability measures, which one? The allocation at time i minus one is telling me which one. So you see is exactly what the title is promising. We are partially, partially exchangeable conditionally on a partition on another partition. In this case, it is the partition. That in this case, it is the partition at the previous time. Okay, uh, well, uh, Bia proved that X obtained like this is column exchangeable, okay, and in general is not row exchangeable, which is also important. Okay, okay, so first try. I mean, I have a partially exchangeable stochastic block model. I have an idea of conditional partial exchangeability. What should I do? Just model. What should I do? Just model conditionally time by time my dynamic network, which is exactly the first thing that we did. So here, I think it's clear on this image here. At time I, I do my clustering. And then at time I, sorry, at time one, I do my clustering. And then at time two, what happens is that conditionally on the clustering at time one, I just have a node color. One, I just have a node-colored network. So, conditionally, at time i my i equals one, I'm just applying partial exchangeable stochastic block model here. Okay, so that's the idea. And this is what all of this stuff is saying, basically. But I just want you to focus on the fact that we have time-specific probabilities, block connection. probabilities, block connection probabilities here. And that's it. That's what I wanted to say here. We have time-specific block connection probabilities. Okay. Ah no, there's an I was missing something. There was another time-specific thing. And time-specific, time-specific base measures. Okay, for our hierarchical normalized completely random measures. Completely random measures acting at each time. So, this is not really working. It's not really working because we notice that there's not enough borrowing of information, of course, at the level of the probabilities. And as a consequence, there's not enough, there's a very, there's quite a weak dependence among, and that's what actually Petter said the first time he saw this speech. He said, are you sure that the dependence is strong enough? Well, he's not. He was right. Yes, no. He was right. There's kind of a weak dependence among the network across time. And I think that the principal reason for that is that, well, you know, clusters are not in, like cluster labelings are not interpretable. So across time, being in cluster H at time I and being in cluster H at time I prime, there's no meaning in this. There's no relationship between actually between the clusters across time here. So either you work with co-clustering and look at the evolution of that, the evolution of who is with who now and who is with who tomorrow, and you can do that, but it's breaking Kolmogorov consistency. At least in our trials, it's breaking Kolmogorov consistency because you define something that is, of course, dependent on a group of the past nodes. And whenever, let me be clear, it breaks Kolmogorov consistent. Me be clear, it breaks Kolmogorov consistency in the direction of new nodes, not in the direction of new times. Okay, so if you want to predict something for new nodes, well, again, you can do it, but you should know that you're not Polmogarov consistent. So our solution was let's use a common random base measure P0T, so that there's a reasonable interpretation of clustering across time. And we can also talk about and Talk about and define a common, or if you want, evolving connection probabilities, block connection probabilities, and on top of that, we can also enforce permanence. Now, remaining in the same cluster means something, okay, now that we have a common base measure. And this is what we did. So, this is the model. And, well, just at time one, what happens if you want. What happens if you want is a hierarchical normalized random completely random measure with just one sub-population. Okay, if you want a Chinese restaurant franchise with just one restaurant and the base measure P0 is the same one that is appearing here at time I. What happens at time high, again, which random probability measure I'm sampling my Xij from? Xij from, well, is given by the allocation at time i minus one, and that's fine. But now that I have this common ground, let's say, I can define the vector of the time-specific vector of random probability measures as a normalized completely random measure with base measure, a mixture between a permanence component, so a delta in the previous allocation, and And well, just sampling again from the common menu from the common pizzerity. Okay, of course, you have to be careful with the definition of unique values now, but we can redefine the idea of unique values, like evolving unique values in a sense, so that everything works. And on top of that, it's just again, stochastic block models. Okay. So, maybe this is better explained by yet a new restaurant metaphor because we love those, right? So, what's happening is this. So, now I live in the US and like there's a lot, there's this way of going to a restaurant and sitting at the bar and maybe eating there, or maybe just sitting at the bar and then sitting to a table and eating there. So, this is the inspiration. So, this is the inspiration, and this is more or less what is happening. So, restaurant one, we just seat people, okay, and we color the table according to the dishes that they are eating. At restaurant two, we have a franchise that has one restaurant per each dish that is eaten at restaurant. So, everyone is just sent to the at time two, everyone is just, thank you, is just sent to the bar of. Of which is serving the dish that he was eating at time minus one. Then we flip a coin or whatever, the omega probability is telling us who is deciding to stay at the bar and who is deciding to sit in the restaurant. The restaurant sitting is just a Chinese restaurant franchise across times. Okay, so whenever you have all the people that want to sit, you can The people that want to sit, you can sit them across time, across restaurants, like each restaurant in each time is a different branch of the franchise. That's what's happening. Okay. Oh, this is just a prior study to convince you that the dependence is not enough at the level. Well, the dependence is not enough both at the level of the partition and at the level of the network. So this is just. The network. So, this is this is just the ARI for the. Sorry, it's not, you cannot really read. This is the partial exchangeable. This is a naive partial exchangeable model. They are basically just sharing the concentration parameter. This is the naive conditional partial exchangeable one. And this is the bar model for a different probability of sitting at the table after eating at the bar. After reading at the bar. And this down there is what happens at the level of the network. So let's take some characteristic which is proper of the nodes, like the node degree, and we are calculating the same node degree discrepancy across times. And you see that the discrepancy is lower, of course, for the bar model. Okay, that's everything that I wanted to say. Well, To say, well, we have kind of ready made a marginal algorithm for the naive version of the conditional partially exchangeable stochastic block model, which of course has the problem that is very slow growing with time, with the number of times that you want to look at. So, we think that the solution is, of course, to go towards a conditional algorithm, so that's what we're working on. Algorithms that that's what we're working on right now. And you see, we are basically modeling just the dependence time one, time two, or time i, time i plus one. So once you've done that, you can like compose something which is not necessarily linear, but can be some kind of network of networks. And also, I want you to notice that like we preserved Kolmogorov consistency both in the times and in the Both in the times and in the growing amount of notes, let's say. And we also went beyond Markovianity, passing from the naive to the permanent one. And well, this gives a kind of a more solid model to think of edge prediction time-wise, but also edge prediction node-wise. Yeah, that's another theoretical question. Another theoretical question that could be: okay, all of this has been built with the invariance basically of the parameter, okay? The invariance of the permutation. What about the invariance that we are inducing on the observables? So on the edges here. So that's another thing to explore, I think. And that's it. Thank you for listening. Thank you very much, Brinkin. For there, any questions? Something, but are you going to name it the American Restaurant Process? Barkrolling, I was thinking. Okay, so in the interest of time, I think we're gonna move on. But if there are questions, we can discuss them at a break. Let's say.