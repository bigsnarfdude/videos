The last step for this session and this morning is by Finn Lindgren. So, he will be talking about statistical climate risk construction modelling in the USTAS project. Thank you. So, I'm what stands between you and Launch. The USDIS project was a project that ran between 2015 and 2019, and it's the And it's the EU surface temperatures for all corners of Earth. And it was a big consortium project involving too many people to list, but we had an abundance of Met offices involved and a couple of universities, all with their own specialist expertise in the different parts of the project. We also had Dark as an advisor on the project. And so And so I'll talk a little bit about that and also more broadly about things that we didn't do in the project, but what we would have liked to do, but that we can do in smaller situations. So this is what we were trying to do. We were trying to take essentially all of the available data of temperature going back to 1850 on a daily time scale on the whole globe and try to. And try to merge them into a big daily high-resolution global temperature product. So that involved multiple satellites, data from weather stations, data from ships, and I think we have goodies as well. And yeah, so we try to take all of that and make sense of it. And what we wanted to get out is not only sort of a point. Is not only sort of a point estimate of the whole thing, but an actual posterior distribution of what the weather was. So here in the slides, it's typically called an ensemble, but it's not the same thing as a triumph model output ensemble, which is typically not properly stochastic well defined randomness. It's more like you run different starting points. Different starting points, but it's an actual statistical sample. So posterior distribution. So this is the kind of thing we get out of it. So here's two regions and two different times. So I think the lower one is actually the same heat event as we saw in the first talk. Event as we saw in the first talk in the session because Moscow is around here, so the heat is slightly rather down, but I think it's the same event, or at least around the same time, 2010. And what we're showing here is the ensemble of contour curves from the theory sample. So you can see how uncertain the estimate is about the extent and location. The extent and location of the heat at different levels and the cold in Norway and the Alps. And here is a different region, a different heat wave in 2003 in Southern Europe. And here's one in Australia in 2006. So here we can see that the uncertainty about the contours increases when we go out into the ocean in various directions, partly because we have a different kind of Partly because we have a different kind of data underlying the estimates there. So, on land, we would have weather stations and satellites. Over ocean, we may or may not have good satellite information and definitely no weather stations, but we may have ships. But they all measure things differently. So if you're modeling this on a long time scale, you need to deal with all the different aspects of space and time. Aspects of space and time. So you have something that varies seasonally. You have something that varies slowly over time in a smooth way. And now the scales here are different. So this one has a much smaller amplitude than the seasonal model there. And then you have the daily time scale. And then you add them together, and then you get what the actual weather would be. So we're trying to tease out those different components so we can separate out. Component, so we can separate out daily variability from climate change, essentially. And since we have satellite data, that means that we have lots of errors in the data because each satellite has its own types of biases, and those are both related to the satellite itself, but to a large degree. To a large degree, it's errors that are related to the things that is actually being measured. You get different biases of measuring the surface air temperature because the satellite measures the temperature on the ground or on the ocean surface, and then there's a calibration model that converts that into air temperature. So there's uncertainties in that. So there's both systematic errors. So, there's both systematic errors and variability in how this calibration conversion works. So, we really want to feed that forward through the whole system and not just treat that as a pre-processing step, because it's not an exact pre-processing step. And the same thing will hold for weather stations. Weather stations have long-term temporally correlated biases, and typically, what is done is what is called. And typically, what is done is what is called this homogenization process, where you try to find change points and then estimate the shift in the temperature series for the different intervals to turn it into a homogeneous series. And that sort of works okay if you're only looking at change over time and you're not caring about propagating the uncertainty. But if you want to propagate the uncertainty, it turns out that the easiest way to do it. Uncertainty, it turns out that the easiest way to deal with this was to simply treat those biases as random effects in the big hierarchical model. So we use the pre-processing step to find the change points, but then we just estimated the biases as part of the big model. I could, if I wanted to, could feed in like prior information that the change point detection method probably gives you an estimate of the uncertainty. Of the uncertainty or variability of how much it needed to change it. So we could use that as a prior to regularize it a bit more. But in the end, you get things that looks like this, where the red curve is sort of a well sampled observation series, and we're trying to compare that with what came out of the use this model. Of the user's model, which is the black curve that basically follows the red curve, plus these light grey bands that are probably 90% intervals pointwise. So it's on daily time scale, so it's hard to see, but essentially the red curve mostly falls within those bands. So that was data not used in the Data not used in the estimation, so it's a validation check for this. And we have a few different years where we see that when unusual things happen, both of the curves change. So our model was able to capture that, even though we hadn't had that specific data series. But one thing that's relevant for extremes here is that typically what we do is that we take the red curve and And find the maximum, and that is our maximum value that we then feed in all the rest of the extreme value analysis. But there are some places here where the gray curve goes quite a long way further down and sometimes further up. So, if you look at this vertically and also jointly for different time points, you can extract the posterior distribution of what the Of what the maximum for each year was. And it's not the value, the largest measured value necessarily, because you will have an uncertainty around that, both below and above. So I would be curious to know if there has been work on incorporating that uncertainty in the data itself when you feed it into the estimation. When you feed it into the estimation of the extreme spots and how much it actually matters. Because I suspect that in some cases it would matter, in other cases, it wouldn't. But it's sort of something to have in mind that are you modeling the measured numbers or are you modeling the actual temperature? Because it's not the same thing, because all of these measurements are uncertain in some sense. And we have an R package called Excursions that David wrote most of the code for that can do things like if you have, instead of just having samples, if you actually have a parametric model for the Gaussian process and have the sort of full thing, then you can compute excursion probabilities and likely excursion samples. Some likely excursion sets, and if you have samples, it can do that as well. So it's something that we haven't applied to this model output, so I can't tell you what it would look like, but I would be interested to see it. So the computations were a bit of a lot of work, I would say. So So we ended up implementing something that was doable because there's a trade-off between time to implement and time to run. So in this case, it took a long time to implement and to run. So some of the things that we would have liked to do, there simply wasn't time to complete it. So we have more things worked out about what one could do in. One could do in smaller settings where it's easier to implement it. So, what happened here is that we have these different temporal scales. So, we have the long-term or systematic effects, so the seasonal effect that is seen as being the same for every year, the long-term climate change model, and then the daily time scale. So, one of the approximations we did here that we Approximations we did here that we approximated away the correlation between days, which isn't really realistic because the correlation range is sort of on the order of a week in most places. But it was sort of a thing we had to do to just make this run because there are approximately 60,000 days. So we had 60,000 linear matrix solved. Linear matrix solves of about two hundred thousand nodes each running in parallel, but the batch system couldn't handle dispatching 60,000 jobs at the same time, so the dispatching of the batches had to be batched. So you send one batch, and then once that's done, then we can send the next one, and so on. So there was a bit of a manual thing. So for a couple of weeks, someone sort of monitored the system and saw. Monitor the system and saw if things were going wrong and then send the next batch, etc. So it was a bit of a manual thing. But it worked. And this middle one is the largest individual solve, which is almost 2 million nodes in the solve. So there they use the Pardezo library of the cluster to do the solves. But that was that's probably the largest I would ever want to do an exact linear solve. Linear solve. But there are other options. So that was sort of what we ended up doing. So going now to more of the theory and general things of how we did things and how we do other things as well, is that it's mostly based on GAMP, generalized attitudes. So generalized additive models. So we saw that a bit the other day, but what we do here is that we have a spatial or spacetime field, one or more, as we saw before, and we can have linear or non-linear effects of covariates by having Gaussian processes as functions of the covariance. So the gambling earlier today had splines, and splines in this setting. In this setting, act essentially as conditional expectations of Gaussian processes. So, when you have a Gaussian process, that sort of implicitly adds what we think about uncertainty and variability of the actual process as well, and not just the point estimate. So, if we have linear observations, so we have some linear predictor where even though these are Gaussian processes, if we These are Gaussian processes, if we represent them as basis expansions. So that could be fixed-rank-free with a few basis functions, but what we do is to have small basis functions so that we get sparse matrices in bottom places. Then we can always write this as some matrix, in this case sparse, multiplied with all the latent variables defining those weights for the Gaussian presence functions. Process functions and some measurement of noise. Now, this is highly simplified because, like in the Eustace project, a lot of the measurement equation was actually in the latent fields because there were whole spatial fields of measurement biases. But they all mathematically collapse into this. So, traditionally, regarding processes with based on covariances, you would write down the covariance matrix for those. Down the covariance matrix for the observations themselves and then compute the conditional expectation. And you can then also do some matrix algebra to get the conditional variances. And that involves a linear solve with the covariance matrix for the data. So we've seen that for these Lethier approximations. Letty approximations, you can sort of construct an approximation of the Scholeski factor of that inverse, and that's the way you then approach that solve. But for traditional methods, you have to deal with that somehow. But if you instead have the precision for the latent variables that you're interested in doing inference for and treat the data as sort of that's a nuisance that we're not, it's not the data that we're focusing on. It's not the data that we're focusing on, we're focusing on the latent variables. So you can write down the conditional position of the latent variables. Yes, just the sum of the prior position and the data contribution to the material. So in the covariant sense, marginal things are easy. In the position sense, conditional things are easy. And since we're conditional data, that's sort of a natural thing to do. That's sort of a natural thing to do. And then the creeding equations turn into this, and you can sort of show that these are equivalent. But you now get a linear solve that involves the precision matrix for the conditional distribution of the latent variables given the data. So, depending on the difference between your dimensionality between the observations and the latent field, one of the two. Latent field, one of the two could be either more expensive or less expensive. So, if we have a lot of data but a small number of latent variables, then the precision formulation by itself is much nicer because we'd never have to have these big covariances for the data. We simply don't need it. And we can then extend this as well for non-gausing observations, but I'll get back to that later. Later. So, I've said earlier in the week that I would add a slide on sort of more of the connection between these things, and it's all just to do with the Chalesky factor and conditional independence and the global Markov property and the ordered Markov property. So, for the Vecky approximations, you invent an ordering of space, and then given that ordering, and if you Given that ordering, and if you have the covariance, then you can sequentially work out what these conditional distributions are. And that's exactly the same as working out what the position matrix is, but rather the Chalesky factor of it is, because it gives you the directed Markov distributions. Whereas if you work with positions, you start with having something that is already the position. And the sparsity pattern follows the global Markov property, which is for every separating set, you get conditional independence between one side and the other. And what you then do is that you compute the Chalesky factorization, but you first figure out what is the ordering of the nodes that keeps as much as possible of the sparsity. And that turns out to be usually sort of similar to what's typically used with the nearest. Typically used with the nearest neighbor Gaussian processes in the cases where you get an ordering that makes sense or works because there's a relation here, chip here, that this up here is an approximation, whereas this is exact given the position. And the position here, that is where the approximation happens. Because you're projecting an infinite-dimensional process onto a finite-dimensional function space. Finite dimensional function space, and that gives you the precision. But there is a middle ground one can do here, which we explored quite a bit, but never ended up completing the conversational, where you say that, well, this precision matrix for the usage project, if we wrote down the whole model, that has 10 to the 11 rows and columns. That is too many to have. And you definitely can't have the covariance. And you definitely can't have the covariance. But what you can have is the operator underlying it because it's sparse. So you can apply traditional PDE pre-conditioned iterative metric solvers where you use the structure of the problem. And then you don't have to have the Cholesky factor at all. But in cases where it's large enough that you can store an incomplete Cholesky factor, so you don't do the Factor, so you don't do the Chaleski infill, then we sort of come close to the Vetti approximation. It's not exactly the same thing depending on how you construct it, but there is a link between those two ways of thinking about it, and it sort of directly leads to a preconditioner method that you can use. So, for large models, that's an approach, but when we looked into it, it looked like the incomplete. We looked into it, it looked like the incomplete utility factorization is probably not ideal. Whereas, other methods, such as these, I always mix up the two terms. There's multi-grid, which is the term. Multi-level is similar, but it's not the same. Multi-grid, where you basically treat all the different spatial and temporal scales differently, so that you solve the large scale and then you go down to the small scale and you. And then you go down to the small scale, and you go back up to fix this large scale. And that, in theory, can give you like order n computation time for these problems. And there are now people starting to try to plug in the SPDE models into those existing PDE solver frameworks to do that. So we'll see what comes out of that. So we've already seen a bit of SPD, so I thought I'd show you. A bit of SPD, so I thought I'll show you another one which is the space-time model, which gives you non-separable covariance. You can also plug in a spatially dependent range parameter, temporally dependent time decay parameter, and anisotropy in the Laplacian. And the nice thing compared to covariance-based monster models, or many of them, is. Or many of them is that it's really easy to make sure that this is a valid model because you essentially only have point-wise criteria that the kappa parameter should always be positive and the anisotropy matrix should always be positive definite, so bounded away from zero in the eigenvalues. And that's essentially it. It's you have local properties that you can interpret and the global That you can interpret and the global properties in the queries that are emerging from this. And you typically can't compute it analytically, but because of the precision-based tweaking, we don't really have to. So this one comes out with a nice precision structure for tensor product spaces functions, which is just the sum of chronecker products of sparse matrices. So we get a very nice structure of the whole thing. So then you can take this and plug it into Bayesian estimation methods. So the INLA method, this is another thing I probably promised I would show, is how has INLA changed? So INLA essentially does a Laplace approximation to find the density for the covariance parameters. And then it integrates numerically over that to get the marginal. To get the marginal densities for all the latent variables. So the classic method essentially expanded the internal representation of the field with the linear predictor version of the field. So those are deterministically related. So essentially, it does that by treating them almost deterministically related with this delta. And I think I got this matrix right. I wrote that one last night, if I may have. Of mites, if I may have made some mistake, I think it's correct. And then it links with data by adding this matrix here. And that means that this vector is at least twice as big as it usually needs to be. And the more data you have, the longer it is. So it's more expensive. But the new version, instead of using a Laplace approximation for each individual term here, it does a joint variational approximation that you can plug in. And that means that we don't have to have this double. Have to have this double counting of things. We only have the latent variables in the computation. So the new method is really linear in the number of observations. So the order m to the 3 on 2 or something like that would be based on the number of basis functions in the latent representation. So that means that if you have a data-rich environment, it's now still cheap to run and you need less memory than before. So that's the main thing that has changed in the past, I would say. Thing that had changed in the past, I would say, six months. It's been available as an experimental mode before that, but now it's default. Yeah, but the variational pattern you use a dense overall. Yeah, so for the variational approximation, we use the actual modification of the Markov model. So this matrix would be sparse. Matrix would be sparse, but it's slightly the wrong sparse matrix. So you modify the matrices, and that gives you a bias correction. And now they're also doing a variance correction, which is less clear and how much it helps. But yeah, so it's not independent variational, it's a joint part of approximation. Yes. Yes. Yes, so I didn't write down where they come from, but that is the posterior mean conditional data. Or conditional mode, I should say. Yeah, okay. So the the variational approximation is just changing that variance. Yeah, yeah. Yeah, so it's a it's much faster than before and uses less memory, so it's really nice. So the last thing is, I mentioned in some of the discussions before and also before in the talk that the data isn't just the data, it came from somewhere and it's complicated. So I went and visited this ship that's in Dundee in Scotland, the Discovery. So that was a ship used in one of So that was a ship used in one of Scott's expeditions. And you had a look inside, and then you find a hydrology lab. So that looks interesting, lots of sciencey stuff. And you see something in the corner here. What is this thing? It's a Nansen-Peterson water subtitle. So, this is how you get the temperature measurements of the water. So, not the air temperature. Of the water. So, not the air temperature, but the water temperature. So, you take this device, you put it on a rope, you lower it into the ocean until a certain depth. Then you think there's a string you pull to close the valve, and then you pull it up onto the ship, and then you put a thermometer in it, and then you measure the temperature. Lots of things can go wrong. Like, how long does it take you to pull up the water? Will it cool down? Will it cool down while you do that? Will it warm up? Well, that depends on the air temperature. How tall is your ship? How long do you wait before you take the measurement? And did you actually measure the correct depth? So lots of things that can happen with those. So I found one of the most important things with the USETH project was really statisticians working with climate scientists that also know exactly. Climate scientists that also know exactly how the data are collected in reality, so that we can understand what bits of the system do we actually need to model. So they have physics-based models for these specific things, measuring exactly what goes on in the physical sense. But that's really too much to build into the statistical model. But we can build an understanding of what aspects are important to include, and will there be biases? Yes, and but how much is the question? So we can. So we could build in this in this process. So if we could include non-linear predictors. So in the classical GAMS, it's the linear with respect to the latent variables. But here we want to model the mean temperature plus minus the diurnal temperature range, which is non-Gaussian. But if we could do that as a Coplar transformation model from a Gaussian process, then we get a From a Gaussian process, then we get a non-linear predictor in the Gaussian process, and that's something that we can linearize and iteratively estimate. So I'll skip these slides, but this is a nice little model, the five-parameter lambda model that is a generalized Pariti in each direction. So you can model both the upper and lower tail of the data. So this fits much better than like a gamma-analog normal for these stations. And some stations have. Some stations have weird behavior. Like, I've never seen a distribution like this before. It's bounded below, but it's skewed so that the peak is to the right. That's not the usual thing to happen. We can estimate the parameters. We also estimated these tail parameters, and they do come out mostly as negative. But you can see here this is hiding on stationary, and this is just one month of the year. So, if you run an animation, which I unfortunately couldn't find, it's hiding in some. Couldn't find it's hiding in some backup system, then you can see how this shifts around space throughout the season. And that is also a clear spatial-temporal seasonal pattern in the tail behavior. So that comes out even in the very simple model, you can get these non-stationary effects in the marginal distributions. And to get back to the discussion, I think yesterday, that is kind of thing that if you're working on the globe, this kind of non-stationary This kind of non-stationarity, you really should care about. Do we care about the correlation length? Yes, but less so. If you don't handle this, then it probably doesn't matter if you use the wrong correlation length. But anyway, so finally, we can plug this into the same framework by linearizing the predictor, and we just replace the linear contribution from the data with the Hessian of the log likelihood. Likelihood any trade. So, here are references. Unfortunately, Holbot came out strangely today, but that's it. Yes, and these slides should go up online, I think. I also have them on my website, so please go and click that video link. It's a nice animation of the whole results or most of the results of the project. Thank you very much. Thank you very much.