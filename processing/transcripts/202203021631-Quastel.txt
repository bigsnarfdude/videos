Okay, great. Well, thanks, Kostya, and the organizers for the invitation to speak. And yeah, I'm going to talk about the KBZ fix point. I'll explain what that is in a second. And here are my co-authors. So this is a bunch of works variously with the people on this slide. Okay, so what I'm going to talk about is for this audience, basically it's a stochastic Berger's equation or the KPZ equation. It's the same thing. thing. The stochastic burgers are just the derivative, the spatial derivative of the KPZ equation. So stochastic burgers is just the normal viscous Burgers equation being forced by the derivative of a space-time white noise. And Kpz is just the integral of that. In fact, the main contribution of Cardi-Prigi Zhang is to remark that the integral of stochastic burgers is a good model for Burgers is a good model for growing interfaces. Okay, so I think most people here know these equations. And the key thing I want to get across, the message, is the existence of some, a few completely integrable discretizations of these equations. The method's very fragile. It only works in one dimension. Don't ask me if I can tell you something about your favorite model, especially in two dimensions. The answer is no. It's more or less what I'm going to show you. And what I mean by completely integrable, and I'll tell you more as we go, is that there's the transition probabilities are explicit in a sense. So there's a recipe to create them, which is something very similar to what was discovered 50 years ago. Similar to what was discovered 50 years ago for the Cordwig-Dubries equation. Now, one can take what's called the 1, 2, 3 KPZ scaling limit. And in the limit, you get a special Markov process, either the KPZ fixed point or the stochastic Bergers fixed point. And this is a special scaling invariant Markov process with completely explicit, well, sorry, solvable transition probabilities. Transition probabilities. And in fact, the finite dimensional distributions satisfy integral partial differential equations, the Kp equations. One can also take an Euler scaling limit, which I'll describe, a hyperbolic scaling limit, in which stochastic Burgers just converges to the inviscid Burgers equation. And using the techniques, because we have these very Because we have these very precise descriptions of the transition probabilities of TASEP, for example, we can actually solve one of the old problems from hydrodynamic limits, which is the large deviations from the scaling limit to burgers. And the last point is that although stochastic burgers and KPs in themselves are not solvable models, they have some degree of solvability, but not much. There's a technique where you can There's a technique where you can compare them to some solvable models, and so one can actually prove that they also converge for completely general initial conditions to this universal fixed point. Okay, but again, the method's very special and it works for these equations, but not your favorite model. Okay, so let me start by describing one of these complete integral discretizations, and that's this model called TASEP. So, TASEP So, TASEP, as usual, we have either a height function or a u. The u here is just the particle density and the height function you see in front of you. And the height function here has the rule that at every lattice point, it either goes up or down one. So you can think of the height function itself. This is the stationary picture of the process as what I mean stationary picture. The fixed time picture of the process is a Process is a simple random walk as a height function. And the simple random walk can also be encoded by particles on the lattice. It's just whenever the walk goes up, you put a particle there. And when it goes down, you put no particle there. And the rule for TASEF is very simple. Little local maxes jump to little local mins at rate one. Or equivalently, particles are trying to jump to the right at rate one, and the jump is changing. And the jump is achieved if and only if there's no particle in the way. So you'll see, okay, we're a go, and the little local max jumps to a little local min, and a little local max jumps to a little local min, and the particles are trying to jump, and that's the whole process. Okay, so that's TASAP. That's the whole process. It's a little bit important for us that there's a rightmost particle. That's the solvable case. So the height function actually has to just go down after some point, but that point can be very, very After some point, but that point can be very, very far to the right, so it doesn't matter too much. Okay, now you may wonder why this model has anything to do with KPZ or anything to do with burgers. The rate of jumping down, so the derivative of the height function is just minus two times the indicator function that you had a little local max. And because of the discreteness of the process, that function can actually. That function can actually be written as a half times the backwards discrete difference operator times the forward discrete difference operator minus one plus the discrete Laplacian of H. And so this thing is exactly a discretization of the of the KPZ equation, this TAS-UP. Now, TAS-UP does not scale to the KPZ equation, but it's a discretization of it. Okay, so now I'm going to show you the exact Show you the exact formulas, and you should just try to get an impressionistic picture. The formulas are not simple formulas, they're very complicated, but one can do a lot with them. Once you get used to them, one can really use them to prove things, and that's more or less the message I want you to take home. Okay, so if I start with some initial data H, what I really want to know is what are the finite dimensional distributions? What are the finite dimensional distributions of the height function at some later time? So I take some positions, the finite dimensional distributions, the m-dimensional distributions here, I'm just asking what the probability is that the height function at some time and some m points xi is less than or equal to ai. And that's all the information one would need to know to describe the process, just like when you take the finite dimensional distributions of Brownian motion and define Brownian motion in terms of the And define Brownian motion in terms of them. So these things are given in terms of certain Fredholm determinants. This Fredholm determinant is acting on L2 of the integers. And there's a code which tells you, given your initial data and the AIs and the XIs and the T, how to produce an operator whose Fred Home determinant you take to give you these transition probabilities. Transition probabilities. So, what you do is you produce the key objects here, which are these Rs. There's an R for the initial condition and R for the final condition. And what these are is essentially the transition probabilities of a geometric walk being asked to stay either above the final condition or below the initial condition. And you plug these into an operator, the rest of whose terms are The rest of whose terms are kind of straightforward things. Q is just the transition probabilities of the geometric random walk, and e to the t over 2 grad minus is just the transition operator of a plus one process. So these are all explicit things. Now you can calculate the Fred Holm determinant, and that's supposed to give you transition probabilities. Okay, so at first glance, I think it's hard to appreciate this formula, except that. Formula, except that the key point here is that there is a formula, and the formula is valid for any initial data. And it took a long time to find this formula. There were partial results starting in the late 90s, and it was finally found by us about three or four years ago. Okay. Now, what are you going to do with such a formula? What are you going to do with such a formula? Well, what are the kind of things one wants to study for a model like TASAP or its analogs, the stochastic Burger's equation or the KPZ equation? So you could, there's two key scalings which you might want to take. One is the hyperbolic scaling and the other is the 1, 2, 3, KPZ scaling. So let me just describe those. So in the hyperbolic scaling, I think people, if you've seen it before, you're more used to scaling the You're more used to scaling the U, the velocity field. So here you just take a large space-time picture of the velocity field and time and space scale equally. And now the stochastic Berger's equation, now the nonlinear term scales with the epsilons purely, but the but the the uh um the viscosity and the and the noise now have small parameters in front of them and uh well similar thing for the kpc equation but here the noise is not scaling in the right way with the uh viscosity to produce a serious effect on the large scale so all one ends up with is the inviscid solution of Inviscid solution of the viscous Berger's equation, which is the entropy solution of the Berger's equation. So that was proved already about 30 years ago. And that's proved for the TASUP, for example. Okay. In the 1, 2, 3 scaling, we want to actually look much more fine scale and look at the fluctuations. And so we look at a longer time scale, epsilon to the minus three halves. The minus three halves. So epsilon here is the ratio of microscopic to macroscopic space. So now you look at time epsilon minus three halves and look at fluctuations of size epsilon to the minus a half. If you do that, the thing goes racing off, but then you pull it down with this by subtracting C epsilon minus one t. And then the KPZ equation, I didn't show you the scaling for the stochastic burgers. For the stochastic burgers, but it's roughly the same. The nonlinear term scales perfectly, and you end up with a small term in front of the viscosity and the noise. But see, now the terms in front of the viscosity and the noise are scaling in the right scale with each other, because of course it's really the variance of the noise which you should compare to the viscosity. And so, what you've done is basically you have an Ornstein-Ulmbeck perturbation. Ornstein-Ulmbeck perturbation of the in this Berger's equation. And this thing converges to a very non-trivial process, this KPZ fixed point. And so this is discovered by taking a limit of the formula on the previous page. And so now I can tell you what's the KBZ fixed point in the next slide. Okay, so the KBZ fixed point is a Markov process. It's invariant under the 1, 2, 3 scaling. Variant under the 1, 2, 3 scaling, and its transition probabilities again are given by these Fredholm determinants. And the only information you need starting from some initial height function, if you want to know the fluctuations at some time t later, is to know the finite dimensional distribution. So here the m-dimensional distributions are given by a Fredholm determinant. Here it's acting on the M-fold product of continuous L2 of the Of continuous L2 of the positive real line. And there's a code which tells you: given your initial function h0 and your time and your positions, X1 through XM and A1 through AM, what the kernel is, whose spread home determinant will give you these transition probabilities. And the code's as follows. So the dependence. The dependence on time and the x's and the a's of the k is a kind of trivial dependence. So, you basically have a kind of core k, which is we call the Brownian scattering transform of H0. And once you've created that, the flow up in operator land is essentially just a linear flow. Flow. At any rate, just by conjugating with these linear operators, linear semi-groups with the T and the X's and the A's, one creates the K. Now, you may notice that the heat equation here is not only being solved forwards in time, it's being solved backwards in time. But what happens is that if you have this e to the t over 3d. Of this e to the t over 3d cubed, which is the Airy semigroup, it's so regularizing that as long as t is not zero, you can solve the heat equation in either direction. So the whole thing is actually a group as long as t is not equal to zero. So all of this makes perfect sense. And in fact, these exponential of these operators have completely explicit, smooth, nice kernels, which I just didn't write down. Okay, so the question then is: out of your initial data, Then is out of your initial data, how do you do this scattering theory to produce this KH0? And what it is, is it's a kind of hit operator. So you have your H0, and you send a Brownian motion from U1 and ask that it goes to U2 and goes below H0 on the way. And that's called P hit. And you think of P hit. And you think of pH as an operator kernel in U1 and U2, depending on H0. And that operator actually determines H0, though it's not obvious at all. And now if you want it on the whole line, what the thing asks you to do is refocus in the center by solving the heat equation backwards on either side, and then take a limit as algo. side and then take a limit as l goes to infinity which looks like a strange operation except we always have this e to the t over 3d cubed as long as t is not equal to zero to save us from bad backwards heat kernels and so the entire operator makes sense the limit written there is a little bit schematic that doesn't make sense by itself but as long as t is not equal to zero this uh kernel k is a beautiful continuous injection from these Injection from the space of upper semi-continuous functions into the trace class. Why upper semi-continuous functions? Because in fact, here, you want to start with very wild things like a function which is zero at zero and minus infinity everywhere else. That's called narrow wedge, and that thing grows out like a parabola. Okay, so that's the process, the KPZ fix point. That's the definition. Fixed point. That's the definition. When you have these determinants, if you've ever been in this business, especially a determinant of a kernel evolving in such a matter, you'd be very tempted to differentiate the log of the determinant in the various variables, t's and x's and a's. And if you do that, you discover that the finite-dimensional distributions aren't only. Distributions aren't only given by these phenomenal determinants, but in fact satisfy certain integrable PDEs. So perhaps the easiest one to see here is the n equals one case. So at the end of this paragraph, I've written the n equals one case, which is a little bit simpler. And that tells you that the logarithm derivative of the one-dimensional distributions. Of the one-dimensional distributions of the KPZ fixed point are actually solutions of the Kp equation. And something similar is true in n dimensions. It's a little bit more complicated. The log derivative is actually the trace of a matrix, which it and its derivatives satisfy a matrix version of the Kadamso Peshashvili equation. And one of the nice things is that if you look at the special solutions which were known in the past, if you start with narrow wedge, you get the GUE distribution. And if you start with flat things, you get the GOE distribution. These are just special similarity solutions of the KP equation. There's a sort of obvious self-similarity, which one should expect. You just plug it into the KP equation and output the Tracy-Widham distributions. This is only valid for deterministic initial conditions, and it's still a little bit of a mystery, though somewhat understood, what is going on with random initial conditions, where it's not obvious and generally not true that it satisfies the Kp equations. It's true in some cases, which are the solvable cases, but not all of them. Okay. So let me. Okay, so let me just go on and talk about what else we can do. And just so for your interest and maybe people can give me feedback what they think, you've got the TASEP converging to the Berger's equation. It converges to the entropy solution of the Berger's equation, what you get at the vanishing viscosity limit. And now we'd like to know what's the large deviations. To know what's the large deviations from that scaling limit. So, entropy solutions are given by entropy-entropy-flux pairs. So, they're given by this entropy inequality. You take a convex I, and any convex I will do, and then define J by the obvious relation that would make it seem to satisfy the inviscid Berger's equation. But of course, you can't do that with. But of course, you can't do that with weak solutions of the invisible Bergers equation. That's not true. But it is true that the inequality will identify the entropy solution. Okay, so that's Khrushchev's theory of these entropy-entropy flux pairs. And Khrushchev introduced a quantity, which is the error that is made by a weak solution, which is. That is made by a weak solution, which is not an entropy solution, as the positive part of this difference, dti minus dxj, and now integrate that everywhere. So the total mass of that k is the total entropy production for a non-entropy or an entropy weak solution of the Berger's equation. So if K is, if you've got a weak solution where K is zero, you've got an entropy solution. Okay. Okay, TASEP picks out a special I, though, as I said, any strictly convex function will do, but this special I is picked out because it's the large deviation rate function for the stationary measure of TASEP, which is just a product measure. And the conjecture of Jensen and Veridan from about 20 years ago, and one of the Years ago, and one of the outstanding problems of the field of hydrodynamic limits was that the large deviations for the TASEP was the probability that to see a non-entropy weak solution would its large deviation would be governed by this rate function k. I've written it schematically, but of course, this is a large deviation principle. So that, and it's true as logarithmic asymptotics. So what I really mean is that epsilon. I really mean is that epsilon log of p of u being in some closed set is less than or equal to minus the inf over that closed set of k and a corresponding inequality the other way for open sets. Okay, and one can prove this now using using this exact formula for TASAP. So the main point. TASEP. So, the main point here is that although the formula looks very daunting, one can really prove things with it. I will say that proving this is no easy task, even once one has the formula, because the asymptotics of Fredholm determinants is easy in regimes where the kernels are going to zero, but this is actually not one of those regimes. And in fact, one has to do a trace expansion. A trace expansion of the determinant and find cancellations deep into the trace expansion corresponding to certain geometries of this, how this thing wants to optimize this Khrushchev entropy production. This conjecture actually had seemed out of reach because it was thought that to prove it, one would have to understand all the weak solutions of the Burger's equation. And one of the big gaps in our knowledge is that we do not know. Is that we do not know what the weak solutions of the Berger's equation look like. But this method actually just circumvents this because we could just simply do the calculation directly from the exact form. So, and just finally, I want to mention how one proves that the KBZ equation itself or the stochastic KPZ equation itself or the stochastic Berger's equation actually goes to the KPZ fixed point. And the way that's done, so just keep in mind that the solvability is for five special models, which are kind of all look something like TASEP, but they're actually a bit more complicated. And the solvability does not extend to the KPC equation itself. But if I let T epsilon be the generator, the infinitesimal generator of the rescaled TASAP, and S epsilon delta be a rescaled generator of the symmetric version of the TASAP. So the symmetric version of the TASAP is just a TASUP where instead of the flips just going down, they also flip up. And they do that. They flip up at the same rate as they flip down. Flip up at the same rate as they flip down, both at rate a half. And so you have a kind of symmetric process, which is no longer in the KPZ class because you don't have a preferred direction of growth, which everything just flips back and forth. This is actually in the Edwards Wilkinson class. And in fact, this thing I've written schematically as e to the t of those generators, which is the semi-group of the slightly. A slightly asymmetric exclusion process approximates very closely the solution of the KPZ equation. So you can think of the KPZ equation itself rescaled by this delta, and as delta goes to zero, we want it to go to the KPZ fixed point as being extremely well approximated by this particular weekly asymmetric exclusion. But the point is that this symmetric exclusion, because it goes to the Edwards Wilkinson part of the thing, so as sort of indicated by this middle yellow arrow, it's really of order delta. And so in a sense, what you have is that you've really got e to the t t epsilon plus something of order delta minus e to the t epsilon. And so you can imagine that's. epsilon and so you can imagine that's ordered delta if you could find a way to compare these things. t epsilon is going to the kbz fixed point and so um and so this would be a way of showing that the kpz equation converges to the kbz fixed point um and this works uh so this method works and it works as long as um you've got a process where there's a um another Another process which is exactly solvable, and you can show goes to this fixed point. But the difference of the two processes is something you can control very well. So, the difference of the two processes here is this symmetric exclusion, and that symmetric exclusion generator can be controlled by Dirichlet form. So, that's the key point. And in particular, you need both processes to have the same invariant measure. And actually, you need to know enough about the invariant measure that you can control. Know enough about the invariant measure that you can control the difference of the processes by Derek Leif form. So, just let me finish off by pointing out an example which we saw yesterday, which is this Kuromoto-Sivishinsky equation, which is well known to be conjectured to be in the KPZ universality class, even though there's no randomness. If you rescale it according to the 1, 2, 3 KPZ rescaling, you can see that, of course, Scaling, you can see that, of course, the fourth-order derivative and the second-order derivative are getting small. And so the thing would sort of like to be, you know, this Berger's term. And so why not? Why wouldn't it go to the KPZ fixed point? But the problem with trying to prove that is that we don't know enough about the invariant measure of the Kermodo-Sivyshinsky equation. So this thing is supposed to have an invariant measure and presumably. To have an invariant measure, and presumably it looks something on large scales, the thing looks like grounded motion. But since one doesn't know how to prove that and one doesn't have a comparison process, the type of arguments which I just described don't tell us anything. So I'll stop there. Thank you. Any immediate questions to gentlemen? 