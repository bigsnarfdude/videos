Talk about this. So, this will be slightly different topics from what we're seeing, but I guess it's been fairly heterogeneous so far, so it's not too out of the context. So, what I want to try to convince you is that we can use the techniques that we know in optimal transport, mid-fill limits, to try to understand what's happening with some of these machine learning algorithms. So, in particular, I'm going to start, I'm going to Going to start, I'm going to fix it in generated adversarial networks. GANS, it's usually how it's referred. And the idea of GANS is, I'm going to give you an example. So whenever I click this link, it opens this website and this person doesn't exist. Okay. So this, whenever you refresh the page, I'll give you a different picture of a person that doesn't exist. And the question is, how do you actually Is how do you actually get the computer to give you just this array of pixels in this way so that it looks like a person? Okay, that's what we're going to try to understand, and we're going to try to understand how is it related to optimal transcript. Okay, so what's the starting point? The starting point is that we have 200,000 samples of pictures of people. Okay, so we just take pictures of somebody's face and we have this nice data set of 200,000 samples of 200,000 samples of high-quality images, which look like this. And then you just want to create a new example of a picture of somebody that doesn't actually is within your data set. Of course, you could just grab a picture of random and just give it again. The point is to just create one that is not exactly there. All right, so if you've never seen something like this before, so an image usually gets decomposed into the red, green, and blue. This is the RGB channels. And in each RGB channel, you have In each RGB channel, you have an intensity, and when you combine red, green, and blue, you can make whatever color you want. It's just a matter of adding up the intensities, okay? So, just to give an idea of the dimension of this problem is we're going to have 200,000 samples of something that is R to the 1024 times 1024 times 3, which is the number of channels. So it's like basically a 3 million large state space. So it's a really high-dimensional problem. Really high-dimensional problem, and so the idea is: how can we actually try to approximate this in a reasonable manner? Okay, so we're mathematicians, let's start with like mathematics assumptions. What is a mathematical assumption? It's that we don't have finite data, we have infinity. Okay, so we're going to assume that we have access to just instead of thinking that we have a data set, we're going to think that we have a probability distribution in which your data was actually being sampled. And we're going to have access to infinite amount of them. To infinite amount of them. And we are thinking that this k where this probability distribution is actually supported is huge. That was the three millions before. But okay, so if the problem was actually 3 million dimensional, I don't think we would be able to do much. So the other assumption, which is in quotations, because it's not quite what's happening, is that you expect that the expected dimension of this problem is. Expected dimension of this problem is much, much more mild. Okay, so what we would expect is that if you are in R to the K, where K is super large, your distribution, yeah, I think it should be okay. Your distribution P star is something that has a support, which is a much lower dimensional set. And in fact, the idea here is that this support will be something like L. The dimension of this support will be something like L, which is... Of this support will be something like L, which is much, much smaller than 3 million. Okay, and in fact, I'm just going to give you some idea. So, the example I just gave you: K was 3 million, L is 512. Okay, so you can approximate this distribution with something that is 512 dimensional, right? And yeah, it took 40 days of compute time, and it has like the amount of parameters actually would be in a file of 310 megabytes if you want to download. Okay, because if you If you want to download, okay, if you put all the parameters, so it's a really complex thing. Okay, so what do we want to do? We are going to start with what we would call a prior distribution. So we are going to start with something which is in R L and we are going to consider just for simplicity the Gaussian field. Okay, so this is N01. So what's the advantage of the Gaussian? So, what's the advantage of the Gaussian? The Gaussian is super simple to evaluate in the sense if I want to find the sample of the Gaussian. Actually, I don't know how you do it. But anyways, you can ask the computer, you click enter, I'll give you a sample which belongs, behaves like the Gaussian. And so, what we're going to try to do, so you remember this is 3 million, this is 512. What we're going to try to do is map from the space of the Gaussian this latent representation. Gaussian, this latent representation, into the space of pixel space. And we're just going to consider some mapping here, which is going to be this mapping G, which is going to be our generator. I start at RL, and I'm going to look at the mapping of the Gaussian through this G. And this is going to induce a probability distribution in R to the K. And I want these two probability distributions to match. Okay, so the idea is that, I mean, if I was really good at it, I would. If I was really good at it, I would say I was doing something like this, which would be G push forward N. Okay? And so what we want to tune in this type of thing is how do we pick G? Okay, so how do we pick G in a meaningful way? That's more or less idea. Okay, so this G from R L to Rk is called the generator. And then what I'm going to be looking at is the push-forward distribution, or basically. Distribution, or basically, if Z was my random variable with N, I just looking at the composition with G and Z. All right, so what is the objective? The objective is that in some meaningful way, I have some meaningful metric in which I find the distance between G push forward N and P star. I want this to be small. Of course, you know the end of the story. At the end of the day, they decided optimal transport was a good measure. Was a good measure, okay? But in fact, that's not your objective. Your objective shouldn't be to minimize the optimal transport distance between these two guys. If we are in the real world, what we actually care about, and this is odd for the people coming from North Analysis, what we care about is the eyeball metric. We actually care that when I see the picture, it looks like a picture. I don't actually, I mean, I don't know if the distance of these guys is going to be small, even if this distance was like 100, I don't really care. I actually care. I don't really care. I actually care that it actually pulls somebody that hasn't seen these pictures before. Okay? And in fact, this is in like machine learning papers, they will actually use this as a metric. They just pay people with lower wages to actually check if these images were real or not. And they pay them cents on a dollar on how much they pay them for like a group of pictures. And so their actual metric is the eyeball metric. Is the eyeball metric. So this is the actual meaningful metric. So what we are trying to find is a proxy for this metric. I mean, the size of what? So there is this website. There's the link. Amazon Turk, you can just keep a task that is menial and then you can offer a wage for it. I don't know how many people they use, but it's like a sort of outsourcing through like. Outsourcing through like this website that you can do. I don't have the order of magnitudes how many people they use, but we could figure it out. So, okay, so if we had a metric that we can evaluate and we have some parametric description of your generator, what we could have is a supervised learning problem in the sense that I can just minimize over the parameters theta this distance between p star. Of course, okay, but the first thing is type. Okay, but the first thing is like what distance should I be checking? Okay, and why is optimal transport better in these kind of things? So, one thing that is fairly important for like at least the algorithmic perspective is that we do not have access to distribution P star. We only have access to sample. So I cannot put like the L1 distance between these two guys. And because I don't know what P star is, I can only sample it. So I need to be careful that whatever I'm writing as a metric. Whenever I'm writing as a metric, I need to write it with something that can be evaluated with respect to samples. So, what was the original approach? And this was done in a paper by Goodfellow and collaborators, I guess this is 2014, but it's ancient history in terms of machine learning. I should say that this is out of date in the sense of this algorithm is not being used as much anymore, and you should be looking to other ways of doing this, which use. Other ways of doing this, which using diffusion mapping. So, if you know your Orsen-Ullemeck process, they're basically turning the arrow of time in this guy. Okay, so anyways, yeah, it's a different thing. So, this is a little outdated in terms of what it's actually done right now. So, in 2014, the thing that they actually figured out was that, okay, I want to look at some Finipulometric, which would be something like the relative entropy. If you are not familiar with the relative entropy thing. Familiar with relative entropy, think of the L2 distance between these two distributions. But what is the problem? I just told you I don't have access to P star, and now I'm asking you to find the Radonicodine derivative between G pushwar and P star. This seems like there's something wrong or I'm not telling you something. So how did these guys figure out how to actually compute this distance if I only have access to samples? And so then what they realize is I can use a dual formulation. Okay, so instead of looking Formulation. Okay, so instead of looking at the relative entropy distance or the surprise distance, what I can be looking at is the dual Legendre-Fenschell transform in which I'm going to take the supremum over continuous functions. And I'm just trying to take the supremum of f over this guy. And this is what is going to be called the discriminator. So the discriminator, I'm going to have two things that are in adversarial. One is the generator, which is going to try to minimize this quantity. And then I'm going to have the discriminator, which I'm going to have the discriminator, which is the dual variable, like the prices, as we saw in the economic matching, which are telling me which ones are the real samples and which ones are the fake. Okay, so that's more or less the idea. Yeah, so the advantage is that once we fix a discriminator, we can sample this into it. And so once you take a batch size of size m, I can take batches of my normal distribution, I can take Distribution, I can take actual pictures and I can approximate this integral by a Monte Carlo sort of simulation in which I approximate each of these terms just by doing a sum. Okay, and then yeah, I can take that as my value and then try to optimize in G this quantity and then keep it the rain. Okay, that's the idea. That's cool. All right, so all right, so why would we want to change this to optimal transport distance? Transport distance, and this is the way I made the picture particularly. So you can see that you know you have P star. If the support of G push forward n is not exactly on top of the support of P star, this is plus infinity. Okay, and even if I get closer, doesn't really matter. It's plus infinity. So the gradient doesn't actually affect, it's actually affected if I move closer to this distribution. And so I'm going to kind of, this is plus infinity if I optimize in terms of discriminator. I'm not going to learn anything. I'm not going to learn anything. And that's in fact the advantage of the optimal transportation distance: if I change the optimal transportation distance, I actually can measure how far these things are. And it's going to be meaningful for me to move in the gradient of the distance because I'm going to start getting closer to them. Okay. And so this was really unstable and there was a lot of degeneracy. In fact, we are going to be talking about a bit more about degeneracy in terms of the mode collapse problem. In terms of the mode collapse problem, in which you can see this behavior exactly. It's like, because if my initial guess is not really aligned with the original distribution, then I'm going to learn nothing. And so this is really susceptible to initial conditions. And in most of the cases, it's not going to converge. And the usual, the problem they call it is like failure to converge. All right. So what is the alternative? And why we're here is that we can consider the one bus standard. Bus stand distance. Okay, so the one bus stand distance has a clear dual formulation in terms of prices. So the dual formulation in this case, similar to before, I just take the supremum, but instead of over continuous bounded functions, over Lipschitz bounded functions. And now this is a quantity that I can easily evaluate using sample. Okay. And the advantage is I don't have this vanishing gradient problem. All right. So, okay. So what do we do? All right, so okay, so what do I what do I need to do? And maybe I'll uh skip this slide uh too carefully. So the idea is that now I'm going to parameterize both the generator and the discriminator, and I'm going to try to have the discriminator to actually go to be the one buser same distance. I'm going to have the generator try to minimize the one button. Okay, so I have a set of parameters for each one of them, and these are going to act as two. And these are going to act as two populations of bacteria that are competing with respect to each other. Okay, so here I have some population of bacteria that are trying to minimize a certain functional, and I have another population of bacteria that are trying to optimize, are trying to increase the value of the functional. Okay, so in terms of neural networks, maybe I'll go over there. So in terms of neural networks, I'm taking the simplest, simplest case. Again, mathematicians. So you start with some. So, you start with some input, and this input is seen through a lot of cells that act independently. And then I sum them up to get an output. Okay, so this is a single hidden layer network in which what I'm going to be considering is the limit when this layer goes to infinity. This layer goes to infinity. So this is either n or m, and I'm just going to take n or m to plus infinity. And the larger the n and the m is, I can, sorry, I have to scale how I'm taking my optimization steps so that I actually uncover some discretization of some PD. Okay, that's the idea. And I'm talking about populations of bacteria because, I mean, if you're used to something like the Keller-Siegel equation or something like that, you think of what the Or something like that, you think of what the individual particle is doing, the individual bacteria is doing. But if you want to find the PDE, what you need to be looking at is at the population density. Okay, and so the way I'm thinking about this is that you want to look at the population density of these guys. And the point that I need to make, which is in the next slide, is exchangeability. So the relative order of these cells doesn't matter. So if I change these cells. The if I change this cell with this cell, it doesn't actually affect what the output is. So then that means that I can start looking at it in terms of the empirical measure instead of looking at the actual parameters. And so then I can fix my dimension. Okay. Anyways, so the exchangeability is whenever I change two of my parameters, it doesn't affect anything. So instead of looking at the whole parameter vector, I can just look at an empirical distribution. I can just look at an empirical distribution, which now is a probability distribution, but it doesn't actually change the space where it is in. Okay, so now I can take a limit when n goes to infinity and try to find some evolution for the distribution of these guys. That's the idea. Okay, and the point is for this single hidden layer networks, you can do this. If you put more and more layers, then each layer has its own population. Population. Okay, and then you have some of them are cooperating and some of them are competing. That's okay. So, that this is the actual algorithm from the paper of, yeah, I would say this is Arjobsky in Talambutu. This was 2017, and it's called, they introduced this algorithm, which is called Basserstein GAN, Basterstein Genetic Adversarial Networks, if you want the full name. And yeah, so the idea is that maybe I have to understand. Maybe I have to underline a couple of things. You have a learning rate, which is how much this would be like your time discretization. And then morally, what you're doing is with respect of the parameters of the generator, I'm trying to decrease the buses and one distance. And in terms of the parameters of the discriminator, I'm trying to increase this guy. Okay. And so the point is that here, That here, the algorithm is literally just doing that. Okay, I'm trying to do gradient descent in the generator, I'm trying to do gradient ascent in terms of the discriminatory. There's a lot of notation. There's a couple of things I want to underline. So there is one thing which is quite interesting from the PDE perspective is that they're clipping their parameter values. So there is one, so the one bus time distance, you need to have. So the one basis time distance, you need to have that your functions are bounded one Lipschitz. So the question is, how do you impose that the function is one lipset? A way of imposing that your function is one lipschetz is by saying, okay, well, I take my parameters and whenever one parameter goes higher than a certain number, I just bring it back. Okay? And that will ensure that whenever I keep going, I will always have that my function is bounded in the Lipschitz norm or one. It in the Lipschitz norm or whatever by some constant, even if it's not one. It doesn't really matter. Okay, there's a few other things. Let's go to the next slide to try to understand. So, the thing that I want to kind of do in this sort of talk is to give you an idea that, in fact, you can consider this learning rate as a fictitious time. Okay, so this algorithm is iterating a lot. You can think of this as a numerical analysis, a numerical algorithm for solving a PD. Okay, and so the point is you can. Solving a PD. Okay, and so the point is you can figure out what the PD is and then try to understand: okay, if I took a limit, what is the PDE telling me of what is the behavior going to be? Okay, that's more or less the idea. So this learning rate is usually tiny, and you can think of it as a time discretization of some for some ODE or PDE, depends on how you want to look at it. Yeah, then there is this point. I don't think I'm going to take too much time on this, but the Take too much time on this, but the point is that the algorithm wants you want to actually optimize the one bus stand distance, so you want you would actually want to train the critic up to infinity. I mean, you want the critic to be the perfect critic, okay? And depending on this, you have another parameter of how much do you train the critic, so the discriminator versus how much you train the generator, okay? And you should stop me for questions if something doesn't make sense or anything. Make sense or anything to answer things. Trying to speak more slowly, but it's not always easy. Okay, so the clipping parameter is just saying, okay, if I want to clip the parameters of my discriminator, so I make sure I satisfy a uniformly shift spot. Okay. And that will impact what is the PDE that you're going to be getting. Okay. I'm going to skip this. RMS prop is something if you, I'm going to focus on stochastic. I'm going to focus on stochastic gradient descent versus RMS Probe, which is just a variant of stochastic gradient descent, which takes into consideration that the landscape can have flat parts. All right, so let's simplify the problem a little bit. And I think this is where I mostly lose people. So I've been thinking of a generative problem, but the original problem or like the easiest version of machine learning problem is supervised learning. Okay. So the idea is. Learning okay, so the idea is I have some underlying distribution and I have some labeling which would be g star of x. So here this would be images of people and g star of x might want to guess if they have blue eyes or not or something like that. Okay, so this is one if they have blue eyes or not. And I want to try to find a function that approximates this guy. Okay, so what I want to do is minimize over the parameters this quantity here. Okay, so how you would do this? You would do this by taking Would do this, you would do this by taking some sort of gradient descent. Okay, so how would you try to find the minimum of this functional? Is you would take the equation of theta dot is equal to minus the gradient of E, if you want to try to find some local minimums at least, or try to. But the point is that you don't have access to P star as the full distribution. You only have access to samples. So what you're going to be doing is you're going to have an algorithm which you're going to Which you're going to update your parameters going down in the gradient of E, but instead of calculating the whole integral, you're going to sample. And the reason that you need to sample is because, you know, if you use all your P star, you will run out of memory. Okay, that's one thing. And so you can choose what the batch size of this, of how do you approximate this. Okay, so what is an algorithm? So what is the stochastic gradient descent in this guy is I'm going to update my parameters saying it's going to be. It's going to be the parameters at the previous step minus alpha, which is my time step, times the derivative of the functional inside evaluated at some random point xk. And I'm picking them these guys at random. So the way that you should think of this is that this would be like forward Euler for gradient descent, but even more, this is like a stochastic version of forward Euler, okay? Because I'm not going to be evaluating the integral with respect to P star. The integral with respect to p star, but I'm going to be sampling the integral with respect to p star. That's more or less what's going on. So, this stochastic gradient descent is a stochastic discretization of theta dot is equal to minus gradient. Okay. Yeah. And then the next level of information I'm going to use is that these guys are exchangeable. Okay, so having an ODE for these parameters that are exchangeable is going to induce a PDE for. To induce a PDE for the empirical measure. And in that PDE, I actually can pass two limits. So the exchangeability is just telling me that this is going to depend on the empirical measure instead of just the parameters and their relative positions. So if I look at the actual how the velocity field is behaving, the velocity field is depending only on the empirical distribution. Empirical distribution. I don't actually care about, yeah, I don't care about the relative order of these guys. So I can actually write down a PDE for the empirical distribution, which looks like this. So the idea is that the empirical distribution of these parameters is going to satisfy a non-linear PDE, which is basically just telling you, yeah, you can solve this by characteristics. Okay, and solving this by characteristics is exactly theta dot is minus the. is minus the gradient of E. So this is the form that you want to kind of see. So this is the PD that you're obtaining. And there's a one over n rescaling just because there's a one over n rescaling in this particular type of algorithm. But of course, if you do some other things, the one over n disappears. But the idea is if you want to find something non-trivial, you have to rescale time by n. And you actually get, when n goes to infinity, you'll get that the algorithm. You'll get that the algorithm will be satisfying a continuity equation, and this continuity equation is given by this. Okay, so this was done by a lot of people at the same time. I'm not going to say who did first because I don't really know. But yeah, I mean, so this was an interesting result. It's like the idea is that let's say that now I sampled my initial distribution of parameters from some initial distribution, then mu n will converge to a deterministic process. will converge to a deterministic process when n goes to infinity, which concentrates in the unique solution to this nonlinear continuity equation. And in fact, this nonlinear continuity equation is something that we are even more used to it. It's just an interacting particle system. So this guy will be, in the simplest case, this guy is just going to be quadratic and you are going to have some interaction potential that is giving you what the velocity field is. Okay? Is okay. Um, yeah, so if we consider this quadratic energy, the idea is that you can think of the equation I wrote before as the gradient flow of the energy evaluated at mu, where now I am this generator or this guy that I'm thinking depends on mu, okay, because it depends only on the empirical distribution, so I can change that it doesn't depend on the empirical distribution, but it depends on any distribution of parameters, okay? So this. Parameters. Okay, so this guy is stochastic gradient descent is a discretization of the two vast time gradient flow of this energy. Okay, that's the idea. So if you understand what happens with the two-based time gradient flow of this energy, in essence, you're actually understanding what's happening to the stochastic gradient descent algorithm that you were saying before. On average, you're actually understanding what's going on. But understanding, yeah, in fact, actually, it's a bit of an open. In fact, actually, it's a bit of an open problem to understand what happens in the long time loop. Okay, so if you take, so if I take this as my PBE, do you actually know that you're going to be converging to the minimizer of this energy? Okay, so this energy is actually convex in mu, but it's not convex in the Barcetton sense, it's only linearly convex. So the question is, when time goes to infinity, if I take the two Barcetan gradient flow of this, can I? Take the two varieties and gradient flow of this. Can I say I'm actually going to converge to the minimizer, unminimizer or not? Okay, so for my understanding, that's an open problem. There is some partial results in this direction. Yeah. Yeah. V. This guy? V. This V here. Yeah. I'll show you exactly what V is. So V to. So V would be from here, it would be here. So I'm taking the derivative with respect to theta i of the energy evaluated at the vector theta. And so if I actually look at it, it's some functional that depends on the empirical distribution evaluated at theta i. Okay, because g of theta only depends on the empirical distribution. I can show you exactly what the v is in a second in terms of this this exact. Second, in terms of this exact problem. But you can understand that there is a functional. Beyond what it is, there is a functional. In this specific case, it becomes a little simpler because you can write this energy is actually quadratic on the measure. So you can write it as an interaction energy. It's not of convolution type, but it's an interaction energy plus some potential term. And this is a different P. Okay. And so you actually get. And so you actually get this with some constant. And the idea is that the actual, you can actually write exactly what the interaction kernel is and what is the potential. Okay. You can actually, you know this explicitly. All right. I would say semi-explicitly in the sense that you don't know what P star is, but you could always, you know, have a toy problem that you know exactly what P star is. Okay, cool. All right. What am I doing with time? Oh, I'm good. Oh, I'm good. Okay. Cool. All right. So, yeah, so if I replace RMS Prop by SGD, so the idea is that at least, I mean, I don't want to go into the exact sort of details of what I'm saying here, but it's like the algorithm uses something a little bit more complicated than this. But if you distill it to the same sort of minimal properties of what's going on, is I'm going to be, I'm going to update my parameters of my general. To update my parameters of my generator in this way, and the parameters of the discriminator, I'm going to update them with an extra projection. So the projection is just this parameter clipping. So what's happening with a parameter is, let's say I'm in the unit box. And so my parameters omega try to get out of the box. What I'm going to do is project them back to the box. Okay. So that's actually going to make a difference. And that's actually going to make a difference in terms of what's going to happen for the PD. So, this guy is going to evolve with a continuity equation exactly like what we've seen before. Now, the only difference is that my vector field is going to depend both on the distribution of theta and of the distribution of omega. Okay? Well, and the same is true for omega, but I have another extra non-linearity, which is this projection of q. Of Q okay, all right. So, yeah, so what is Q? Q is just the large box, it's a box of the right dimensions, and this gamma C here is the relative speed of how much faster I'm training my discriminator versus how much I'm training my generator. Okay, and here the idea is that what am I doing? I'm just sampling XK and ZK, and then I'm updating my parameters. And then I'm updating my parameters given the sample I've taken. And again, so yeah, hopefully you kind of understood this. You can think of this as some discretization of an ODE or particularly a PD. And the idea is that this is an interacting, this is a two-species system in which they are interacting through these velocity fields. And you can actually guess what the velocity fields are because I wrote you what the energy was in terms of the parameters, which you can make in terms of the measure. Which you can make in terms of the measure, and then you're just going to take gradient descent in one of them and gradient ascent on the other. Okay, that's idea. All right, so yeah, like probably taking all the magic away from it, I have some functional, which is this phi, which is now depending on two measures, mu and nu. Mu is the parameters of the generator, nu is the parameters of the discriminator. And so, what I'm trying to do is go that. To do is go down in mu and go up in mu. Okay, and so that accounts for the difference on the signs. So here I'm taking the two Varsitan gradient with respect to mu and evolving through this guy. And here I'm taking the two Varsettein gradient of mu and evolving on the opposite direction. But there is this extra thing, which is I need to make sure that my distribution of measure does not the support. Does not the support of my distribution does not exit the unit box. Okay, so this is what this projection of piq is doing. Okay, so in fact, this in terms of like the regularity of the of the problem, it's a little funny because this is a discontinuous vector. Okay, so when you are within the domain, when I'm within the domain, I can do whatever I want. Domain, I can do whatever I want. So basically, any particle that is inside here can move however it wants. But when it goes to the boundary, even if the vector field is pointed outward, it's going to stop there. And so what the effect that that's going to happen, the way I describe it is you can see a pigeon flying into a window. So here's your pigeon, here's your velocity field. Pigeon, here is your velocity field. This is your window. What's going to happen with this pigeon is going to just get flattened. Okay. And so even if you have some sort of distribution which is higher dimensional, when it hits the wall, it's going to become flat. Okay. But yeah, if we want to make sense of that, we actually have to at least make sense of the fact that this actually has a unique solution and a measured value solution. Okay. Yeah. And that the whole point is that. Yeah, and that the whole point is that this projection of pi q is a discontinuous operator, and it's a discontinuous operator at the boundary if my velocity field is pointing outwards. Okay, there's a it's going to stop things at the boundary. All right, so this is joint work with Reneca Abreira here and Bruno Suazuna in Brazil. And the idea is that we can actually say, so if we know that the activation function is smooth, Activation function is smooth, then the PDE I just wrote has a unique stable solution and it satisfies this stability estimate. Okay, so the d2 distance between two solutions depends on the d4 distance between the initial conditions. Okay, so this depends smoothly with respect to the initial conditions. And this solution exists for all times. Okay. Yeah. So that's kind of the first thing. So that's kind of the first thing. So if you consider the limit of this algorithm, the idea is that, I mean, okay, we have a candidate of what the limit of this algorithm has to be. All right. And yeah, the observation is what I was saying before is that, you know, if you have this sort of pigeon against the window effect, like the parameters don't see the window until they hit it. Okay. And in fact, this can be quite drastic. Okay. So if you can start with a distribution within your box, which is In your box, which is a path to call it somehow, it hits the boundary and it's going to become super thin. And in fact, even if I take it to the sort of a corner of the box, it can become a single point. So it can actually have, I mean, I guess, a finite time blow-up, but finite time blow-up doesn't imply that this equation doesn't make sense because this actually admits measure-valued solutions. So this can become This can become a single parameter in some sense for the rest of time. So, once it got stuck here, it can go back into the domain, but it's going to be a single parameter for the rest of time. Anyway, so that's something that clearly is not something that you would want. Yeah, so in particular, the thing that is it, the support can coagulate into a single point in finite time. Single point in finite time. And this was actually a problem that the people in machine learning realized. They realized just by doing experiments that they were like, ah, this doesn't look right. So they actually, instead of imposing the one Lipschitz condition strongly like this, they actually just put a penalization, which improves this. So this was something that they had actually observed, but they didn't, I mean, they didn't actually write down the PDE and they say, okay, well, this is what's going on. And they say, okay, well, this is what's going on, but they actually realized that this was happening, okay, even in a later paper. All right, so what's the next theorem? The next theorem is exactly kind of what you would expect is that if I take the limit when n and m goes to infinity, what I should expect is that my discrete solution will be converging to a solution to the PD. So the solution to the PD is the So, the solution to the PD is the mean field limit of this algorithm. And so, of course, if I knew the behavior of my PD, I can actually say what's happening with the discrete approximation. So, this would be numerical approximation of the PD. In fact, if you didn't know how to do a numerical approximation of the PD, you can actually grab the algorithm and use it to approximate this PD. Yeah, so there is one. Yeah, so there is one catch in this theorem. So, to actually get the right scaling C over M, you need to actually have the same initial condition. So, before I was sampling my initial condition with respect of the initial condition in the limit. But if I do that and I look at the Basistein distance, this actually suffers from the curves of dimensionality and you will not get a scaling one over n. Will not get a scaling one over n, but you will get something that is dependent with respect of the dimensions that you have. Okay, so if you try to, I mean, and this is just happened, something that happens for the basis and distance, is if you try to look at the, if you sample your initial conditions from the limiting initial conditions, what you're going to be getting is that something that is depending really badly on dimension. Okay. The way to fix this is to change the distance you're looking at. Okay. So instead of looking At okay, so instead of looking at the buses and two distance, you want to be looking at some uh what is what they call it, RKHS reproducing uh kernel Hilbert spaces, which basically instead of putting the d2 square distance, what you want to be looking at is at h minus s, where the s is large enough. Okay, um, anyways, but this is if you want to quantify in the bust and distance, this is actually what's going on, and you cannot get better than that. Yeah. This one? Ah, it's just two plus epsilon. There is, I mean, the way that these architectures are made, yeah, you have to control the growth in one parameter, and that gives you that you have to have slightly higher integrability. Yeah. It's annoying, but yeah. But yeah, whatever. Okay, but yeah, whatever. So dp where p is bigger than two words. Yes. Okay. Anyway, so I think I kind of gave you the idea, at least this idea. So how do you actually do these kind of things? Is that you can actually compare what would be the forward Euler or projected forward Euler with stochastic gradient descent in which I am changing what V is with some sample version. With some sample version of B. And the idea is that what can you do? So, the easiest thing to do is you take the difference, and then you use the Lipschitz difference between these guys, and then you add them up, and then you want to show that the error term behaves like delta t. But in this case, it will behave like square root of delta t because you have this extra stochastic thing going on. Okay. And yeah, so morally, what you're going to be doing is you take the difference, you write the Doing is you take the difference, you write the equation for Ek, and then what you're going to be getting is I have some stability bound of Ek plus one is less or equal than some constant with respect to delta T K. And then I have this Mk that is measuring how far is the approximation from the actual guy V of theta K. And the point is that the same way that you prove central limit theorem or law of large numbers actually, the same way that you prove law. The same way that you prove large numbers is you just take a sum of all of these guys, and then you realize that they are decorrelated. So anything that is not in the diagonal actually disappears. So that's more or less the idea of what you want to do. But it's morally constrained in these slides. So if you take a sum of independent random variables and you take the that have mean zero and you take the square expectation, they scale in a different way. Okay, and that's the idea because they cross. Okay, and that's the idea because the cross terms all disappear. That's literally the only trick there is. Yeah. So, I mean, yeah, then of course you have to do this in the bust distance and whatever you want. But at the end of the day, this is, I don't know what's no, there's not much more than that. So you can actually grab whatever algorithm in machine learning that is training, and you can think of it in this way. And then you can think that it's actually discretization. Think that it's actually a discretization of some PDE, and you want to understand what the PDE is. Okay, so all right. And the question would be: why? Why do we do this? Why do actually we want to understand algorithms in this way? Okay, so let's give an example that you might have seen at some point. One known behavior that is really bad for this type of algorithms is mode collapse. So here you're feeding the algorithm instead of like pictures of people, you're feeding handwritten digits. Of people, you're feeding handwritten digits from zero to nine. And so, what happens with the GAN algorithm, this is the original GAN algorithm, is that eventually it might actually just give you pictures of one. Okay, if I give you a single sample of this, you're like, well, it looks like a one, that's good. I give you two samples of it. Yeah, looks like another one. But then, if you see like the kind of macroscopic distribution, you'll see that it only gives you once. Okay, so has it learned the right thing or not? That's a question. Okay, and this. That's a question, okay. And this behavior is actually related to relative entropy, okay? So, once the generator has found some piece of the space in which it's safe, it's not going to leave it because leaving it, it means plus infinity. So once it has learned that one is a safe bet, it's going to stay there forever. That's just something that happens. Okay, this is a super toy example, and it's super clear to understand. Example and it's super clear to understand that you're like, oh, well, I'm going to realize this no matter what. But then comes language. And yeah, and this is, in fact, this is actually really funny. I didn't know this had happened in my abstract, but actually, so ChatGPT laps to Delph. I don't know if you guys had heard this, but it laps the word to Del, and it appears more often than it appears in the English language in general. So how can you check that? And it's actually hard to check that you're actually. And it's actually hard to check that you're actually preserving these distributions and it's a bit more complicated than just looking at the picture. So I actually, at some point when I was preparing this talk, I think Ingwon had asked for my abstract and I passed it through ChatGPT so that it would look nicer. And then there was a Delb in there. And then I was talking with a colleague and he was telling me about like how ChatGPT laps to Delp. And I was like, Delp, Delp, I've seen that. I've seen that word before. I've seen that word before, and uh, yeah. So, and yeah, if you want something super sad for science, I need to put this picture. So, super sad for science is you can look sort of the inception of chat GPT. So, this would be time, and here you would be number of delt in abstracts, and it goes like this. It's like literally exponential. It's pretty crazy. So, it's actually changing the way that we speak. And so, you want to try to understand when the And so, you want to try to understand when these types of things are going to happen. This is not what I'm going to focus on, but it's like one of these known bad behaviors that you want to try to avoid. And the question is, how do we avoid them? How we actually can understand them. So, the thing that I'm going to be looking at more is more actually failure to converge. So, failure to converge is a slightly different thing in which instead of actually going to a single sample and making it a lot of times, what's going to be Making it a lot of times, what's going to be happening is that my generator is going to play a cut and mouse game with my discriminator forever. Okay, so there's just Tom and Jerry like going around like forever. Okay, and yeah, so I just started with this data set, which would be a mixture of Gaussians. You take a half of a Gaussian at minus one and a half at a Gaussian at one. And then the generator at some point gets stuck in being a single Gaussian, and then it just oscillates. And then it just oscillates for it. Okay? I'm going to show you a video so you believe me. Okay, so what you see on the right is the discriminator values. So this is saying, okay, most of the samples on the left are fake. And so then the generator goes to exploit that. And then it goes the other way around. Then the discriminator. Around, then the discriminator realized, oh, most of the samples on the right are real, sorry. And so the generator goes and exploits this. Okay, and so these guys are going to turn forever. So this guy is just what I would call a machine learning pendulum, basically. Okay. Right. So get rid of this. It just goes on forever. All right. So in fact, I wanted to actually get something I could get my hands on. Okay. I wanted to actually find the solution that was. I wanted to actually find the solution that was oscillatory. And in fact, I mean, what would be interesting in this case would be to show that all limiting profiles are oscillatory. I wouldn't expect that you can accept a single point. I would expect that all of these guys in the limit will have to be converging to something that is oscillatory. I don't know how to do that. So even like simplifying the architectures a little bit and looking more at the yeah, so taking the limits of what happens with this architecture if I take. So, what happens with this architecture if I take a single parameter and I take some discontinuous activation function, which, I mean, it's okay. The idea is that now I'm trying to approximate a half of a delta at minus one and a half at delta at one, and then I'm approximating it by this type of function. So it would be some function times delta at minus one times one minus that function at delta. Okay, and so I want to see what are the dynamics induced by this problem. The dynamics induced by this problem, and can I actually integrate them perfectly? Okay, can I actually find exactly what the ODE is and can actually see what the guy is? Okay, so if I look at the actual buses and distance between these two guys, then I actually have a unique minimum at theta equals to zero. And so if I take actually gradient descent with respect to this, I would actually converge to this point. Of course, this is concave, so it's a little unstable, but it works okay. But the problem is that I'm. But the problem is that I'm not taking the maximum, but I'm actually training omega at the same time I'm training theta. And so then what this is going to create is this chasing from each other, from one to another. And so if I look at, instead of looking at the sort of one-dimensional projection or the maximum, I need to be looking at the surface. So what's happening is the I have a surface with respect to omega and theta, and then what I'm going to take is gradient descent in theta and gradient ascent. descent in theta and gradient ascent on omega and then you would start going around and around are we am i out of time i don't know five minutes yeah i'll wrap it up all right so what's going to be happening the idea is that i just wanted to actually find something that i could get straight dynamics on and so you you can actually integrate this problem perfectly if you choose your functions right and you can see that the orbits of theta and omega are going to stay And omega are going to stay within the level sets of this function. And so, what's going to be happening is if I know my initial conditions, I'm going to stay in the same orbit forever. So, I can actually identify what the orbits are. And so you can actually visualize what's going on. So, this would be like sort of the contour plot. All right, so the parameter gamma, I haven't really made much of a fuss about this, but what would be the point? Okay, so the point would be: let's say I start here. Would be, let's say I start here. Okay, so start with my parameters omega and theta here. So, what's going to happen is I'm going to follow this path here because that's the level sets of my first integral. And so then when I hit omega equals to one, I cannot keep going because that's like basically the pigeon that has hit the window. So I'm going to stay in this guy until I arrive to this limiting. limiting um to this limiting uh orbit and then i'm going to turn in the limiting red orbit forever okay so no matter where i start i'm going to end up with a periodic solution and this periodic solution more like most likely is going to be the red one okay anything that is outside uh the red um orbit is going to get stuck in the red orbit eventually if i start inside the red orbit i stay inside Red orbit, I stay inside the red orbit. But if I'm outside the red orbit, I get pushed to the red orbit and then I stay in the red orbit forever. Okay, um, and in fact, this is, I mean, in essence, this kind of is showing you that it's fairly independent of your initial conditions. You are always going to be converting to something that is rotating around. And in fact, failure to converge is the main problem of this type of architectures. And so that means that they have to do like millions of Like millions of experiments until they actually find something that works. Okay. So, yeah, I mean, in a sense, this is giving you like a sort of picture of what's going on. I mean, you should expect that there is a lot of basins of attractions like this. And depending on your initial condition, you will get stuck in this red orbit or some other orbit somewhere. Okay. That would be the more general kind of behavior here. Anyways, I think I'm done, and I think I'm somewhat on time. So, thank you. This one I'd like to confirm the notation. So I know Gaussian 0 and 0 1. So, sorry, no, I think I messed up. This should be, yeah, no, you're completely right. I, yeah, no, this should be n minus one, one. Sorry, yeah, yeah, yeah. No, that's a typo. Thank you. Yeah. Yeah, yeah. No, no, no. No, sir, like, this would be the definition of what is the dimension. Be the definition of what is the dimension of it. So it's like I can approximate it with something that is 512 dimension. So it's more about the fact that I can do this and I get something that is good. That would be some sort of definition of the support. But finally, there's like a lot of type of what is the effective dimensions. Yeah. Yeah. It's just a matter of trial and error here. I don't think there is a way. I mean, the point is that these guys have like sort of infinite money nowadays. And so they'll just try 512, they try 1024 and 512 works better than before. Yeah. Yeah, so this is like maybe the magic, like then you wouldn't hope you can approximate it. So the idea is that you can hope that you're approximating it because it has some regularity. Approximating it because it has some regularity. If there is no regularity, maybe you mappings of this. I mean, you have to think that this guy can do whatever it wants. So it can kind of split itself up. Yeah. Yeah. So the point is that this is actually, this can approximate whatever function in L2 you want. Function in L2 you want. So, this actually has the ability to do whatever it wants. It gets stuck in a pendulum because, but I also like, I mean, this is a toy example of a toy example of a toy example. I linearized everything there. I mean, just because I wanted a picture. But honestly, I mean, the idea is that this guy can kind of actually has the ability to approximate whatever it was. That's not a problem. That's not a problem. The point is that it gets stuck in sort of local minima, to call it them somehow. But I mean, it gets stuck in these orbits which are not up. How do we do that? So, okay, so you have the PD dt mu dt mu with a plus or minus or whatever it has to be. So this g depends on mu. You know, you you yeah, mu is time dependent. So you what you want to understand is like what is the what like what is the what is the what is the algorithm going to do so if if i could tell you that this pd is converging to exponentially fast to uh a single mu star then i would say well you are actually converging to g of mu star but that's not actually going to happen what i expect that is going to happen you're going to converge this guy no matter the initial condition is going to converge to some periodic solution and then what you would expect is that this guy converges Expect is that this guy converges. I mean, even the at the discrete time, you're going to have some discrete periodic solution that's loading around. Here? I mean, you. I, yeah, no. I mean, even so, the point is the advantage of neural networks is that they kind of chop information up and then like. Information up and then, like, kind of like you put it through a data grinder. And so, you have a lot of symmetries, if you want to call them, or like the advantage is that you can actually parameterize any function in different ways. Because there is a lot of ways you can find zero. And if that makes sense. So let's say I'm trying to approximate with a neural network zero, and I do this algorithm. You'll see that the parameters kind of move a little bit and then get stuck. Because they sum up to zero. They sum up to zero, but they sum up in a super weird way. So it's like the geometry of it is actually quite complicated. So, in terms of the projection, it's more about that you can see how it can get stuck in like undesirable things. The idea is that you want your discriminator to be able to actually sort of look at everything and kind of discriminate everything. But once you have gotten stuck in this point, you're not going to be able to unstack and actually. Unstack, and actually, you lost approximation capability. To help this in this case, yeah, that's a fair point. I'm happy with this picture because it tells me what I want to hear. But the idea should be that, I mean, I would expect that this is kind of even sort of higher-dimensional thing. So it's like, I mean, I'm. Things. So it's like, I mean, I'm looking at something that has a one sort of single parameter, but I should expect something that has a lot of parameters. Then you would have this picture, but in different ways. So kind of what I would expect, which I mean, I guess I could try to at some point do an experiment is like, I would expect that, you know, I had these two Gaussians here, and then I had this single Gaussian that would get stuck like going from one to another. But I would also expect that another way that you can approximate these two Gaussians and move. That you can approximate these two Gaussians and move on forever would be that you have some Gaussian here and some Gaussian here, and what you change is the relative size of them. And then you get stuck in this like same chase forever, but it's a different chase. So in I mean, I brought up this issue, which is cute, but so this mode collapse happens mostly because you're using relative entropy. And so if you ask practitioners, they will tell you that mode collapse was solved using BasterStam. And then there are other ways of even solving because the Business. Other ways of even solving. Because the basis and GAN is looking at everything at the same time, and it doesn't have this sort of thing that is going to be plus infinity. So it's going to actually eventually look at everything. And there are some ways that you can tweak the value function so that you actually don't have. I mean, there's ways of doing it. And this is something that has to do with the fact that you're using relative entropy. And ChatGPT uses relative entropy. Chat GPT uses relative entry. And that's probably one of the sort of super high-level explanations of why it gets stuck in like not the right distribution. Enough of that. I guess, I mean, I just thought about it. Is there a very simple way to modify this system that you're looking at, like the simple system, so the point is that I'm not looking at exactly this guy because this would be more of a relative entropy thing. Entropy thing. I haven't thought of it directly, but I mean, if you like sort of look of how many papers can like type of algorithms, you get thousands, like tens of thousands. And some of them have done exactly what you're asking for, but I haven't analyzed those particularly. 