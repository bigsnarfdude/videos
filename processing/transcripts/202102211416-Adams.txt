Fantastic opportunity to meet new people and be exposed to a lot of new ideas. It's great to have so many different perspectives represented. I think it's a chance for many different people to share what they love and learn new things to love. Laura and I just sort of wanted to start by sharing some reflections on yesterday's session and hear from any thoughts you all have. Let me pick on you first. Let me pick on you first, Laura. What thoughts did you have from yesterday? Oh no, um, thoughts. Um, first up, yeah, I guess I wasn't, I guess there's a lot of interest in topology-related methods. So maybe towards the end, if we have a little bit more time, we can maybe talk a little bit about that. I don't know. That's a good one. I'm always surprised. That's a good one. I'm always surprising you. So you're surprising me. I like it. Yeah. Yeah, I don't know. I guess I wasn't expecting might be a lot of interest in topology-related methods, but we will be touching upon some of these today. My other reflection more on a personal note, I was inspired to paint a little bit yesterday, so that was kind of fun. But yeah, I think today's you know, the topics we're going to talk about are not. The topics we're going to talk about are not like, so if you didn't, if you couldn't show up to yesterday's session, it won't really, you know, it's not really needed for today's topics. I don't know, what are your thoughts, Henry? Well, yeah, I misspoke at the very beginning yesterday when I said that Laura and I are closest to geometry and research instead of art and education because Laura's a fantastic painter. I really enjoyed. I really enjoyed the conversation and the discussion we all had yesterday. So, you know, I have tabs open on my computer of this chromosome confirmation capture paper that's related to linear dimensionality reduction. And I have a tab open for embedding graphs into negative curvature, and a tab open for what bases are used to compress audio recordings instead of visual recordings. Instead of visual recordings, let me share the link to our slides, which are up on the Burrs webpage for this conference. Any of you should feel free to borrow these slides if you find yourself tempted to teach things in data analysis. And we have the notability file if you'd like that as well to even edit things for your own purpose. I don't know. Another thought is: I think the pandemic. I think the pandemic is really difficult in terms of how we teach, but it's changing the way I want to teach even after the pandemic is over. And I think a visual way of doing things is a lot of fun. And I'll probably keep that going forward. I'll do less proofs at the Blackboard and maybe try to do more drawings at the Blackboard. Any other comments or reflections from audience members? All right. Well, let's jump right in for today. A little recap of yesterday. We talked about why we think data science is important for geometers and educators and artists to be interested in. And we talked about how you can often think of data sets as points in high-dimensional space. We tried to distinguish between We tried to distinguish between unsupervised and supervised learning. In practice, the distinction is not always that sharp. And then we started, I think, with clustering methods. So K means clustering and agglomerative or hierarchical clustering. And then we talked about dimensionality reduction. So we looked at linear dimensionality reduction, namely PCA. Today we'll start with ISO map. With ISO map, which is a non-linear dimensionality reduction technique. We'll go into topic modeling, which I would say fits closest in clustering. And then we'll move to the supervised side of the tree. We won't talk about regression, but we'll talk briefly about several topics in classification: support vector machines, decision trees, k-near sneevers. So. So let's jump into it. I'm going to go into a non-linear dimensionality reduction technique called isomap. So there's a whole slew of non-linear dimensionality reduction techniques. The main linear dimensionality reduction technique, which we saw yesterday, is PCA, principal component analysis, where you sort of flatten your data set onto the linear subspace that fits it the best. The best. Imagine that you have what's called the Swiss roll in 3D. So you could take a piece of paper and roll it up in 3D. And pretend 3D was too high-dimensional for the task to want to do. So you wanted to compress this down to 2D. No linear map down to 2D is going to visualize this Swiss role very well. You know, if you flattened it this way, it just swished down to. This way, it just swished on top of itself, you'd have points that are at different parts of the roll that would really collide. If I flattened everything onto this plane, I'd have blue points and red points and teal points all colliding. I suppose you could try to flatten the swish roll into this plane, but then you just get this curve and you've lost one of the dimensions. One of the dimensions. You sort of soon start to realize that data is often curved, and you'd like to embed it in a lower dimension by sort of unrolling it. Or yeah, so we're not going to linearly squash it, but we want to unfold it or unroll it in a non-linear way. So perhaps the best representation in 2BD that you could imagine of this Swiss roll would be to unroll it and lay it flat and really see this organism. And really see this organization of which data points are nearby to each other. All right, so ISOMAP was invented by Tenenbaum Da Silva, who I was lucky to know as my postdoc advisor, even though I never worked on things like ISOMAP with him and Langford. And I'll be using images from their paper. Mathematically, the insight that's going on here is The insight that's going on here is maybe we don't just want to use only Euclidean distances. In the Euclidean metric, the distance between this point and this point would be the straight line, the length of that straight line between them. Probably a more natural notion of distance between these data points is really you should have to traverse through this entire role. So you can measure or approximate this distance along this curve. This distance along this curve by drawing edges only between nearby data points and then only allowing yourself to walk along those short edges. So if you choose that perimeter correctly, you won't have any edges going directly across here. And so, you know, you'll approximate the distance as the sum of short steps. And then once you have all those distances, there are techniques for embedding those distances. Techniques for embedding those distances into a plane. Multidimensional scaling is what's used. So you use those distances to say, how do I map these data into a lower dimension like 2D in a way that preserves these red distances as best as possible? What questions do you have, if any, so far? Let's look at some examples. So, this is a pretty neat data set. It's a picture, various pictures of a statue. And a statue has different orientations. So, the statue head is moved to the right or to the left. And then the lighting is also in different places. So, they move the lighting around. And you can actually see the lighting direction. And you can actually see the lighting direction encoded by this little knob. So, on all of these images, that knob tells you where the lighting direction is coming from. So, you should think of this data set as being sort of the points on the Swiss roll. I'm not saying that the points actually form a Swiss roll, but they form some curved subset of high-dimensional space. And why is it high-dimensional space? Well, you know. Dimensional space, well, you know, let's say these were 100 by 100 pixel images, then you'd be in 10,000-dimensional space because you have 10,000 numbers, one black or white value for each pixel. What's cool is when you embed from, say, 10,000 dimensional space down to two-dimensional space, you can really interpret the visualization that you get. So when we've unrolled this data set, Unrolled this data set or unrolled the Swiss role, we get these nice coordinates where the horizontal coordinate seems to correspond to the left versus right pose. So is, I think this is a statue of David. I'll call him David. Is David looking to the left or to the right? You know, so on this side he's looking to the right, and this side he's looking to the left, and here he's looking right down the middle. And then conveniently, the vertical axis corresponds to up, down. Vertical axis corresponds to up-down pose. So at the top, David is mostly looking up, and then at the bottom, David is mostly looking down. And in the middle, you know, he's looking at a neutral. He's looking at the horizon, still maybe to the right or left. So it's a pretty impressive reduction down from 10-dimensional space to two-dimensional space that encodes a lot about your data set, the right-left pose and the up-down pose. Down pose. You don't encode this lighting pose. So the lighting has been reduced. The lighting information is not represented in 2D. So you could search for a different representation, maybe in 3D, that does reflect the lighting if you so chose. Questions on this example? I'll do a couple more examples of data visualization because I think it's fascinating. So, again, we have images, handwritten images. These might be only black and white, meaning no gray allowed. And these are all the digit two. And it forms a very high-dimensional space again, you know, one dimension for each pixel in these images. Let's try to map this. Let's try to map this high-dimensional space into low-dimensional space as best as we can. To me, it looks a little bit like the US, but that's accidental. Again, you can sort of get an interpretation of the two different axes. So the horizontal axis is interpreted as how articulated is the bottom loop. So this would be a bottom loop that's very articulated. Very articulated, a lot of style on that bottom loop. Whereas this would be a bottom loop that's not so loopy, bottom line that's not so loopy. Okay, and so you see, you know, flat bottoms on the left and loopy bottoms on the right. And then you sort of interpolate between the two as you go from left to right. And what's the other dimension encoding? It's sort of how accentuated is the top arch. Arch. So these top arches are, you know, a little bit simpler. This one has a little accent on it down here. And then when you get to these top arches, I mean, they're very accentuated. So it's neat that you can interpret this low-dimensional embedding. I'm not saying you can always interpret the low-dimensional embedding, but in this case you can. Case you can. Henry comments that this reminds him too much of greeting, so I apologize for that. Can I interrupt Henry for a question? Please. So in terms of these, choosing these features, like even on the previous slide, you mentioned our axes represent the up-down pose, left-right pose. Are you saying that this is just? You saying that this is just a feature that you're able to, these two features in particular, you're able to extract or interpret after you've done this unrolling and mapping onto a lower dimensional space? Or are you selecting it beforehand? No, these are the features that are going to be most important that I would like. So, could you just explain that? So, could you just explain that? Yeah, definitely. I mean, you're bringing up great ideas here as well. So, this particular algorithm is not trying to find any particular feature. So, no features are sort of biased as being better than any others. So, it really is agnostic. You just have these high-dimensional data points, and then there's an optimization problem that you solve to embed things in lower dimensions as best you can. Best you can. And then, after the fact, you can try to interpret the features, right? And in these examples that I'm showing you, the interpretation is easy, right? But one downside of this particular method, ISOMAP, is that for more complicated data sets, maybe there is no great interpretation, even after you embed down into lower dimensions. And I should furthermore say that. I think I'm cheating here, even a little bit more than that, or the authors are cheating a little bit more. When you embed into 2D, there's no telling how it should be rotated, right? So let's go to this example, which I called a map of the US, right? There's no preference for the US to be laid out like this versus any of these rotations, right? So I suspect the authors have, they did isomap, they got a picture. They did isomap, they got a picture, and then maybe they even rotated it. Maybe not, but they probably rotated it to find the best interpretation. So yeah, the point of these images is that I think they're selling their tool well by saying we didn't ask for right left or up down, but naturally, when you view the data in this way, these effective coordinates, right, left and up down, pop out. And similarly here, And similarly here, you know, this coordinatization is probably something that you wouldn't think of ahead of time, right? If you had to parameterize a space of twos, you probably wouldn't think of top, accentuation, and bottom. And there might be more important coordinates that you would care about. But after the fact, you're interpreting. But ISO map often fails on a lot of problems. So the second idea that you pose, trying to do dimensionality reduction, preferencing. Reduction, preferencing certain coordinates is a great idea. And algorithms like that exist in data science. And you'd often like to say, maybe I want to embed into lower dimensions where I fix one coordinate in this way or I prioritize this coordinate in one way and ask how the rest of the space falls out accordingly. Great question. Other questions? All right, let me do one last isomap example. These are images of a hand. So this reminds me of Carl's talk this morning where we were rotating our hands and arms around. So, you know, this person just This person just held their hand and took pictures at many different configurations of their hand. I guess they didn't do anything so tricky like crossing their fingers or things like that. All right. But after they took all of these images, they embed them in low-dimensional space. And, you know, maybe that image was rotated, right? But they, when you draw your horizontal axis in this direction and your vertical. axis in this direction and your vertical axis here, you can interpret this lower dimensional map as the x-axis is telling you how much is the wrist rotated. So on the left side, the wrist is quite vertical. And on the right side, the wrist is quite flat. And then as you go from the left side to the right side, the wrist flattens out. And then the vertical axis is encoding for. The vertical axis is encoding for us how extended are the fingers. So at the bottom, the fingers are in a fist, and at the top, the fingers are all the way out. And in the middle, you unfold. All right. So this was the organization that the algorithm found. These pictures weren't taken in any order. They were just taken at random. But then the algorithm was able to lay them in this nice 2D. 2D. Pass over to Laura. I think are you talking about topic, Molly? Yeah, I just wanted to add a comment on your last slide. Would it be fair to say, for example, like when we looked at PCA, we said one property it preserves when we're reducing the dimension is like the variation in our data. Would it be fair here to say in ISO map it is preserving the data? Say an isomap, it is preserving the distances between these images. Like consecutive images of these hands look very similar to each other compared to images that are more farther away. Would it make sense? That's right. Yeah, ISOMAP is preserving distances in the geodesic sense. So you can't just hop from one side of the Swiss world to the other. You have to traverse along nearby data points. Nearby data points. Yeah, so I agree with Andrew. Yeah, you're preserving relative interpoint distances where interpoint distances along these geodesics, these curves in the data set, you can't sort of take the straight line distance that might be far away from any data point. Awesome. All right, so I guess we're going to wrap up the unsupervised category. Supervised category of our session by, I guess, maybe one of my favorite topics, which is topic modeling. The reason I really like it, or the reason we decided to include it here, because it captures both clustering and dimensionality reduction techniques, which were the two topics we talked about on supervised learning. So, here I'm just going to talk a big picture. I won't be really looking at a specific algorithm. It's just a very interesting and common machine learning task. So, one way you Learning tasks. So, one way you might have actually seen this, but maybe did not know, is topic modeling, is in word clouds. So, oftentimes, if you go on websites or look at certain journals or whatever, they tend to have on their website like a collection of words, right? Some of them little bigger or emphasized than other words. And these are kind of used as a summary to what maybe the journal is about or maybe what the website is about, and so on. What the website is about, and so on. So let's imagine this together. So, suppose we have like an encyclopedia, an electronic library of many documents, and I would like to sort of place these documents in sort of categories or bins, right? And I want to do that without having to read any of the documents. So, I have a collection of thousands of documents. I want to be able to place them into bins or categories without. To bins or categories without having to read them. And typically, these documents don't come with a tag, right? So I don't know if this document is about art. I don't know if it is about medicine, right? So I want to be able to assign tags to them to be able to see or to say like, hey, this is an art document or this is a medicine document. And of course, naturally, documents could have more than one tag, right? So for example, a document could be about politics and Be about politics and history. So, the idea of topic modeling is pretty much in the context of documents, which is our first application, is I give it or I feed it a collection, a library, a collection of documents, and I tell it like, hey, I would like to figure out or I would like to place my documents into 10 bins, right? So, what topic modeling would do, which would figure out or learn these bins for Figure out or learn these bins for you or these categories. So, in this example, here I gave it about a couple thousand of documents, and these are coming from the 20 news group data set, in case you're familiar with it. It's a benchmark data set used. And I told it to pick out or learn four topics for me, or four categories. And the first, the fourth category it extracted are these ones that I labeled. Are these ones that I labeled as one, two, three, four? And it literally threw that at me, right? Told me, like, hey, you have four categories, and these are the keywords of the categories. So, in order for me to know that this is whatever type of topic it is, I need to be able to characterize it, right? So, how would you characterize a topic? The most interpretable or natural way is using words. So, I can tell Is using words, so I can tell you just like how you write abstracts for talks, right? You're kind of summarizing it using a couple words. I'd be interested in the chat if you can let me know what do you think these topics one through four might be saying. So this is what the algorithm outputs for me. And topic modeling is known to be a very interpretable method, depending on the algorithm you're using. But any thoughts on like how should I interpret these? Interpret these. Yeah, I think done is kind of used maybe as an alternative to God or a more old world, I guess. Great religion, science, advertising, athletics, that's great. Perfect. So I'm a little kind of biased because I knew what the documents or the topics were. What the documents or the topics were, but generally we would not know these. So, a couple of you, for example, agree maybe the first one is a little bit religion. And actually, if you look at the actual data sets, also maybe a little bit more about atheism. Topic two is about space. You can see a few keywords like NASA, space station, orbit, earth, and so on. Topic three sort of is about sale, right? Sale, right? Automotive or finance, great. And topic four seems to be maybe about baseball games or something like that. So that's the idea of topic modeling is I give it a collection of documents. I don't want to bother reading them. I want to be able to give them tags. And the tags I could give them are these, you know, topics, right? So how does that fit into clustering? That fits into clustering and dimensionality reduction? Well, let's look at this little map or this little chart. I start with the collection of documents. So the way we could represent or understand documents are by their words, right? So these are our little building blocks. So for example, I would suspect this document is about space because it's used a lot of these words maybe frequently, right? So the number of times Right, so the number of times a document used the word space or shuttle or orbit would indicate that it is maybe a space document, and this is where the high dimension come from, right? Usually a document is a collection of thousands and thousands of words, right? So it could be represented in this high-dimensional space. Topic modeling could be thought of as really clustering, right? I'm collecting the documents that are similar to each other and placing them in one bin or one category. In one bin or one category, and assigning them this tag. And as you mentioned earlier, we could assign multiple tags to the same document. For example, a lot of our talks in this workshop are intersection of different topics. So for example, if I were to collect a transcript of all of our maybe slides or discussions, right, or maybe just Zoom chats, I'll be able to figure out maybe this talk is about art and maybe it's a little bit more about geometry or a combination. More about geometry or a combination of both. And usually, as you saw, to represent these topics, you can simply use words or keywords. One way to display that visually is where it clocks. So where does the dimensionality reduction come from? Well, for a certain document or for a certain talk, let's consider maybe our session, I can say maybe it's 75%, let's say, job. let's say um geometry or research um maybe 20% education I'm making things up and maybe 5% art great so now I represented our talk instead of these high dimensional space right which is the number of unique words we use into three dimensions right I represented it using three numbers let's It using three numbers, let's say 0.75, 0.2, and 0.05. Right, so that's really one really interesting aspect of topic modeling is we're sort of doing two things at the same time, reducing the dimension and doing some sort of clustering. And one last example is that topic modeling really could be used for different types of data set, and for those maybe who are familiar. And for those maybe who are familiar with the method non-negative matrix factorization, this is the method I've used in the example I have over here. It's a linear algebraic approach, but there are also a lot of probabilistic approach, right? You can already guess that just because we're counting number of words. So if a certain word really appears in this document, maybe has a higher probability of being in a certain topic. So our different ways of doing topic modeling. Doing topic modeling. Another example I'd like to take is the same faces data set that we considered for PCA, but now I'm running NMF or non-negative matrix factorization on it. And the reason I decided to run that here on the same data set is just to look at the differences, right? It's really interesting how different methods. Interesting how different methods do different things. So, if you remember from yesterday when we reduced the dimension, we tried to represent these images in about 100 dimensions. And we obtained like ghost-like images, right? So, we said each image could be represented as maybe 20% of this ghost, 10%. Percent of this goes, 10 percent of another, and so on. What's really interesting about NMF, which is a very interpretable method, is that now instead of summarizing or representing these faces in terms of ghosts, we're representing it in terms of parts of faces. So instead of saying this is a combination of ghosts, I can say it's a combination of maybe the forehead, right? It's a combination of It's a combination of maybe eyes and eyebrows. Let's use a different color, right? Maybe that side of the face, maybe the jaw, right? Which is a little bit more interpretable, right? So how is the face composed? It is composed of the nose, eyes, mouth, ears, and so on. And this is one distinction between these two methods. And the way to interpret it, the same way we did for words. The same way we did for words, we said a document is characterized by words, and now here an image is characterized by a pixel. So we often think of a topic as a collection of pixels, and a collection of pixels might be an area here around the mouth or might be an area around the forehead. So, this is a collection of pixels that we refer to as stoppings. So, this is a really fun kind of combination of both dimensionality reduction. Of both dimensionality reduction and clustering. I don't know. Do you all have any thoughts on this topic modeling method? So the pictures here, what are we looking at? Currently, on the top, is it the original faces? In the bottom, it's reconstructed using the NMF components, the low-dimensional. Yeah. The low-dimensional representation. Yeah, using these parts of the faces. How can you reconstruct these images using like a combination of mouth and eyes and noses and jaws and foreheads and ears and so on? Yeah, exactly. So it depends. So I have a question here: do we only specify the number of categories? Do we only specify the number of categories? So, depending on the method, one really nice thing about NMF is that the only requirement is to specify the number of topics you're looking for or categories. Yeah. Different methods require different categories. Other thoughts or ideas or questions or comments? Fantastic. Oh, Henry has a comment. Yeah, I'm not sure what you're referring to. Deep fakes, correct me if I'm wrong, Henry, but I think deep fakes are where you change a couple pixels in an image and then the algorithm no longer predicts it's a cat, but predicts it's a dog, even though you only change. Predicts it's a dog, even though you only changed a couple pixels in the background to grass. I was thinking, like, the ones where you like get, you know, you take like your data set would be like a whole bunch of like, I don't know, YouTube videos of, I don't know, Henry. Obama is this kind of okay, Obama. And then you basically like make a fake script and then like your algorithm makes a video as Makes a video as if, like, from their composites of someone that looks real giving a speech they never gave. Right. Yeah, so NMF here is a very, very, very simple method. I think for your case, it requires a lot of neural networks, and I think it's GANs, which we might talk about tiny bit at the end, or generative adversarial networks where you would train. Works where you would train, you have a lot of videos of Obama speaking, and then you feed it a transcript and have it seem like they are speaking these exact words. I think these are generative adversarial networks, which NMF is, I don't think, capable of doing that. Yeah. We might touch on that in terms of faintings maybe towards the end. Yeah. And yeah. Other thoughts or all right, Henry, um, you can I'll pass it over to you. Wonderful. Thanks for the great questions. All right, so we have, let's see, gone down the unsupervised tree to clustering, and then we moved over to dimensionality reduction, and then topic modeling was sort of in both. But now we're coming back and going down to the supervised side, and that's where we'll end. We'll talk about various classification techniques. Classification techniques. Okay, so let me first just briefly recall the difference between unsupervised and supervised. In unsupervised, you're given a bunch of data points without labels. But in supervised learning, your data points come equipped with labels. And you'd like to be able to predict something. So from this training data set, if I give you a new data point, can you predict, should it be red or point can you predict should it be red or blue should you predict it's an image of a dog or a cat or you know can you predict the um i don't know the likelihood that um you know that this uh um you know shuttle will will leave orbit or whatever you're trying to predict so we're going to talk about support vector machines as they're commonly um referred to these are quite geometric These are quite geometric. I would say they're quite old, but they're also quite powerful. I'll describe something called a kernel trick at the end, which means that they're still used all the time in machine learning algorithms. So the rough idea in support vector machines is to look for a linear decision boundary. And I'll say a few words on how you can tweak this to even get non-linear decision boundaries if you think that's appropriate. Decision boundaries, if you think that's appropriate. And in this example, you know, here's a pretty good linear decision boundary. Most of the blue points are on the right, most of the red points are on the left. You learn this boundary so that when given a new data point, if it's on the right, you predict it's blue. And yeah, you'll be wrong some of the times, but in this picture, you've done really well. I'm going to start in the easiest case where the data points are linearly. Where the data points are linearly separable. So you could drive a line or a plane and get 100% accuracy. So this is overly simple. Nobody runs this particular algorithm in practice because your labeled data never looks like that. So in this case where the data points are separable, you're trying to find the widest road that you can drive or build between your data. Okay, so try to find the widest road. Okay, so try to find the widest road. This is not quite yet the widest road. If I rotate, I can make my road a little bit wider. Do you see that? So it's a pretty good road, but if I rotate it, I can make my road a little bit wider. Then look at the median of the road. All right. And this is our decision boundary. So now when given a new data point. So now, when given a new data point, we're going to guess that it's blue or red based on which side of this median it lies on. So, if our new data point is this x, we'll predict it's red. If our data point is this x, we'll predict it's blue. Even data points that live inside the road, we'll still make a prediction for because it's on the left side of the median, you know, we'll predict it's red. You know, we'll predict its red. I suppose if a data point lands exactly on the middle of the road, just flip a coin. But in practice, that happens with probability zero. Why is it called support vector machines? So these vectors, which live on the side of the road, those are called the support vectors. And support vectors is not a bad name for them because they're. For them, because those are the only vectors that determine where the road actually is. Perturb some of these data points further away from the road, and the road won't move at all. The road is only determined by what turn out to be the support vectors. There's some nice geometry that's coming up here. Radon's theorem, if you've heard of it, is related to these support vectors. Comments or questions before I get to the non-linearly separable case? All right, so this is still a relatively easy data set, but a little bit more realistic. You know, you have red points everywhere and blue points everywhere. The blue points just happen to be more on the right, and the red points just happen to be more on the left. And the red points just happen to be more on the left. We're going to build a fancier optimization problem that we'll solve. Okay, so given a road, we're going to pay a penalty based on the misclassified points. So here are the red points that are misclassified by this particular road. And here's sort of their distance to the margin that. The margin that you would like them to be on the opposite side of. So, you really would like every red data point to be not only on the left side of this median, but also on the left side of the left side of the road. So, for every misclassified point, we're going to pay this price, the sum of the lengths of the purple edges. We're also going to pay a price for this red point. This red point wasn't misclassified, right? It's on the correct side of. Classified, right? It's on the correct side of the median, but it's a little bit too close for comfort. Okay, so we're still going to pay a price for this red point. And similarly, for the blue points, so the blue points that are not on the correct side of the road, I'm going to pay penalties for each of them as well. And now you're going to find the road that's optimizing two things in balance. So, first of all, you So, first of all, you want the widest road as possible still. But, second of all, you want the road that minimizes the sum of these purple lengths. So you're optimizing two things, width as wide as possible, and you want the sum of these purple lengths to be as short as possible. Now, those two are in contrast, right? Because as you make the road larger, wider, you're going to make these purple edges longer as well. Priple edges longer as well. Okay, so when you're optimizing two things in data science or machine learning, you often have a parameter lambda, which you can tune, which tells you how much you should preference one thing versus the other. And that's the same in support vector machines. You have to make a choice. How much do you prefer a wide road versus how much do you prefer minimizing these misclassified lengths given in purple? Comments or questions on that? All right. So the reason why support vector machines are still in use today is what's called the kernel trick. And that allows you to get non-linear boundaries. So, how would I draw a decision boundary around these points? You know, maybe it's roughly. These points, you know, maybe it's roughly something circular like this. I threw darts at a board, and the blue points are all of my bullseyes, and the red points are all of my non-bull's eyes, right? And it'd be sort of cool if this algorithm, without knowing the game of darts, could learn the definition of a bullseye as approximately, you know, all these points living inside of this boostly circle. So that's not linear, right? That's a curve. And it doesn't just have to be a circle. And it doesn't just have to be a circle. You know, you could have some decision boundary that's somewhat complicated polynomial or non-polynomial shape. How do we use support vector machines to learn that? Well, this is called the kernel trick. So our data is living in 2D, but we're going to think now of our data as living in a 2D slice in a much higher dimensional space. In a much higher dimensional space. Okay. And in these higher dimensions, like here, I'm only drawing the third dimension, we're going to apply randomly generated functions on our data. Okay. So you might have this function, this paraboloid, which you, you know, so this might be something like, here's my x-coordinate, here's my y-coordinate. This paraboloid might be something like, Let's see, you know, x minus 1 squared plus y minus 1 squared. Okay, and that's the z coordinate on this paraboloid. Okay. If you were lucky enough to consider such a paraboloid, then when you applied that function to all of your red points and all of your blue points, your red points would look. Your red points would live pretty high up on this paraboloid. So your red points would be all on this upper bowl of the paraboloid. Okay, whereas your blue data points, when you apply this function, are going to be near the base of your paraboloid. And what that means is in this third dimension, we could now draw a linear separating hyperplane. It might look similar. It might look something like this. Okay, so if you apply this paraboloid to all your data points, you can now use this slice in the Z direction to separate all your blue points near the bullseye, which are low in the Z coordinate, from all your red points, non-bull's eyes, which are high in the Z coordinate. It feels like I'm cheating because I just chose this perfect paraboloid, right? Paraboloid, right? But this is where we're using the benefit of computers. You know, that particular paraboloid has allowed me to identify this circle, but the computer is searching over many different paraboloids, like this one. This paraboloid would give me a poor choice of circle. I don't want to use this circle as my decision boundary, right? But if you search over thousands of different functions that you could apply on your data, Functions that you could apply on your data, then you'll find the good functions. And those good functions allow you to give non-linear decision boundaries, even though after you've applied the function, you're able to separate the data in a linear way. The kernel trick comes in because if I applied a thousand functions to my data, uh-oh, I don't want to do my computer. To do my computations in a very high-dimensional space, the kernel trick allows you to do your computations efficiently, even though you're in some sense mapping your data into a much higher dimensional space in order to linearly separate. What questions do you have? So, how complicated did these functions get in real life? Complicated do these functions get in real life? Good question. So, I think the most common functions are radial basis functions and polynomial functions. Laura, I'll let you say more. Yeah, I think I suspect you might have more to say. Radial basis functions think things like Gaussians, right? Things like Gaussians, right? So, um, you know, sort of like this paraboloid that I've drawn here. Um, polynomial functions allow you to get cubic type behavior or quartic type behavior. And so in Python or MATLAB, you can just call, hey, I want to run support vector machines with radial basis functions. And it searches. So I don't know exactly how it does it, but it's amazing that you can just ask your machine to do it and it will. Laura, do you know? Laura, do you know of other classes of functions that support vector machines use? And can you combine radial basis functions with polynomial functions in the same call to Python, things like that? Yeah, so these are like the typical ones used, like linear, which is a particular case of polynomials, and RBFs. And usually RBF is a parametric function, right? So there are a bunch of parameters that the algorithm would search over. Algorithm would search over, and usually they are highly optimized in like Python or other softwares. So it does like some sort of grid search and find the best. There's a question in the chat, which I'm guessing must be the answer. Hopefully, it would be yes. Like, if you have a radial function, probably you want to make it centered on something like some kind of centroid of the data, some kind of mean of the data. And then you optimize over some parameters of your Gaussian, if it is a Gaussian, to get up. A Gaussian to get a good fit. I mean, I'm just trying to think about how one would actually try and implement something like this. What's under the hood of those things in Python? Yeah, one. Oh, go ahead. No, go ahead. Oh, I just wanted to say that, yeah, one major disadvantage of SEM is there's a lot of parameters to search over and optimize, which could be really tricky. So it's often like a So it's often like a grid search across the different combinations of parameters. But basically, some finite dimensional collection of relatively standard functions with some parameters. And then you search there's some box that searches through parameters to do this kind of to build this machine. Cool. Thank you. All right. Other questions, comments? Lara, I'll pass it to you. I think you're talking about decision trees, is that right? All right, so we're gonna go back in time to talk about decision trees. So these are one of the very, very, very old methods that disappeared for a while and then reappeared again because of a lot of upgrades or additions. Because of a lot of upgrades or additions to it. And the idea of decision trees is the following. So here we have this data set collection of blue and red points. And the goal of classification is I have this new data points over here. I want to be able to predict if it belongs to class blue or class red. And generally, as we kind of mentioned. Generally, as we kind of mentioned a couple of times, data sets are really high-dimensional, so I won't be able to visualize it. But now, if we look at it, we might think that maybe it should give it class red since it is close to the red points. And this is how decision tree is sort of going to base its decision on, or its classification on. So, the idea of decision trees is to try and form like little pockets or what is called leaves. So, try to isolate. So try to isolate, try to isolate these little pockets of classes. So in this case, maybe I have a collection of one, two, three, four, five pockets or five leaves. And if I give it a new data set, if it happens to live in pocket, in this pocket over here, I'll give it the color blue. If it lives in this leaf, I'll give it the color red, and so on. So the idea of decision trees is to learn these Trees is to learn these decisions, these boundaries, these little boxes that we call leaves. Unlike the other methods we looked at, actually the optimization or what's under the hood is a little bit more complicated than other methods. It requires a lot of information, theory, and so on. So, how does it work? So, how can we form this pocket? So, I'll break it down a little bit later, but one boundary that we But one boundary that we could find, or that decision tree finds, is this one over here. So it decides to split these data sets, everything above the straight line and everything below this dotted straight line. After doing that, we notice that we can simply do this cut over here and we would obtain a good or a rough boundary to separate the blue points from the red points. And this is what the algorithm chooses. This is what the algorithm chooses. And now, if you look at this one, it might be a little bit simpler or trivial. You can notice you can do two additional cuts, and you're able to split these red and blue pockets. This is how Decision Tree does it. So, how does it really work? Like, how does it decide on these things? Well, it's kind of looking what is the straight line that is sort of going to maximize one of the Maximize one of the colors in either direction, right? So, what decision is going to give me the most information or is going to minimize entropy, right? So, if I do this decision, right, I know a lot of these data points over here are red, right? So, and then if I do a consecutive decision here, right, I kind of maximize the information. I have a lot of information gained. I can be certain that maybe up to 98% that these. Up to 98% that these data points over here are blue and data points over here are red. And one main advantage of decision trees that a lot of people in a lot of industry really like, even though they're very simple, is because it's very interpretable. So x1 and x2 here might represent some numerical values. Let's say I want to be able to decide if I should give you a loan. Decide if I should give you a loan or not, and maybe X1 is maybe some credit history and X2 some salary, right? So I'm kind of testing certain things. Is your salary greater than 40,000 or not, right? So it's really, what's the word? Like people really like it in a lot of application industries because it's interpretable. You know how your algorithm made its decision. It was looking at the Then made its decision. It was looking at these decisions: is your credit history or your credit score above a certain value or not? They are also very fast, yeah. So I can mention a little bit of an upgrade also on that one and how it could go wrong, right? So we'll mention a little bit about that. So what the decision tree does here, the first question it's going to ask, we're going to do this together, it's just going to ask, is x2, keep in mind it represents a Keep in mind it represents a certain feature you're interested in. Is X2 greater than 40? Maybe it's an age. Is your age greater than 40? And then you would answer by a yes or no. So this is our first decision, our first boundary. The second question the decision tree is going to do is after asking is x2 greater than 40 yes or no, if your answer is no, it's going to go ahead and ask you another question: Is x1 less than 30 or not? Oops. not oops I should be consistent is x1 greater than 30 or not if x1 is greater than 30 so we're looking at this region over here so the decision tree is going to say well I'm going to assign it the color red and if x1 is less than 30 which is the area over here I'm going to assign it the color blue so now what happens if my X2 is greater than 40 so what happens Is greater than 40. So, what happens if I'm in this space over here? Well, the decision tree algorithm is going to notice that it's going to gain more information, it's going to do a more informed or better decision if it does another split, which is the split over here. So it's going to ask you one more question. Is X1 greater than 40, yes or no? If your answer is no, it's going to automatically assign the color red because it's certain to a high probability. Because it's certain to a high probability that your data point or your targets is going to be read, right? It might be like, no, I'm not going to give you a loan. And if x1 is greater than 40, well, the algorithm is going to notice that if it doesn't do a split, it's kind of a 50-50%. So it's going to do a more informed decision if it goes ahead and find another decision because in this nice Decision because in this nice data set that we happen to have here, now it's going to be certain 100%. So if your X2 value is greater than 50%, it's going to be very certain it's the red class. And here it's going to be very certain it's the blue class. So it just kind of says or figures out if I do one more decision, am I going to gain a lot of information? And compared to the 50-50 first time, we notice here it does. We notice here it does. So, this is basically how it decides. So, if x2, this is going to be our last question, is greater than 50, then I'm going to assign it the color red. And if it's less than 50, I'm going to assign it the color blue. And this is basically how it decides how to give you a class. And again, since these are very informed kind of decision, people tend to like it a lot. And here I chose, we had two features. And here I chose to add two features, but this holds for many features, right? It doesn't have to be only two features. And one additional thing is that these x1 and x2 don't have to be like numerical values, they could be categories, right? So it could be simply yes or no, not just x2 greater than 30. Help me out. Let's see what other category we could think of. You know, have you? You know, have you previously had a loan or not? Exactly. So that's a yes or no. It doesn't have to be categorical. One way decision trees could go wrong, or one way the user that's using this needs to maybe know a little bit about it in order to make sure not to go or to fall into the disadvantages is knowing where to stop, right? Because the decision tree could keep on going and try to isolate literally every single case. Literally every single case it finds, right? Try to isolate all of these data points. And this is what we call in machine learning overfitting. You're being way too specific. So these folks or individuals here could be really unique people, right? Maybe they don't fit the standard of these decisions, but they were able to get along or they were rejected a certain amount. So one really key aspect of decision trees is knowing where to stop. Decision trees is knowing where to stop, right? Where to say, okay, this is where I'm going to stop and not keep on going. And practically, there are a couple ways to overcome this. And one way to overcome is a lot of methods like boosting or bagging. And the idea is to run decision trees a couple of times, right? Instead of writing it once, we can run it a couple of times. And people do that on a sub-sample of these. So instead of considering all of these points, Considering all of these points, I consider a collection of them, run the decision tree, and then consider another collection of them, run the decision tree. These are, as Andrew mentioned, they are pretty fast. So even running a couple of them isn't really that terrible. And the idea is to kind of take the majority votes. So if I run the decision three times for a certain point, let's say two of them said you should give it the color blue, I would go ahead and give it the color. The color blue, I would go ahead and give it the color blue. So, for example, here I have a new point, this grade point, and I want to be able to assign it color. So, the decision tree is going to do is ask this series of question. So, maybe this grade point over here, maybe have coordinates, let's say about 60, maybe 55, right? So, this is x1 value, x2 value. So, it's going to go ahead and ask questions: is x2 greater than 40? Greater than 40? The answer is yes. So we go over this way. Is x1 greater than 40? The answer is yes. Now is x2 greater than 50? The answer is yes. So it's going to give it the color red. This is pretty much how decision tree works. I see there's a couple questions in the chat. Yeah, Brian has a nice question. So, can you, in these decision trees, do you ever have slanted decision boundaries? Decision boundaries? Yeah, let's see. So I think decision trees you can do two steps to reach that it's probably not the most common implementation, is that? I've definitely thought about this before and try to but I can't remember what did I conclude from it. I have a different question. So you mentioned how you could build three different decision trees. So maybe you look at different subsets of your data and build a decision tree on each one. And then you put your new data point through all three decision trees and let it vote. So I might have asked you this already, but is that what's called a decision force? What's called a decision forest or a random forest, or is that something totally different? Yeah, it is called the random forest, yeah. Did I answer? And I think the key, the key advantage of random forests is that you don't have to look, this is going back to Brian's question, you don't have to look over all of the X1s and X2s to know which one is your best decision, because as we said, oftentimes they're As we said, oftentimes the data is really high-dimensional. You don't want to search over all of the features and figure out which feature is going to give me the most information if I decide to split up on that one. So in random force, you also pick random features to look or search over. I really like Brian Kosov because I thought about that before, especially compared to SVM, right? For the data set you had, it's definitely easier to split these types. To split these types of data sets using an SVM versus a decision tree, but this also works for decision trees, right? But you have to do more leaves, right, or more pockets. I don't know if it is used in practice, so I'll be looking that up in a bit. Yeah. Other thoughts or questions or comments? Alrighty, so we're gonna wrap up with our last technique before we maybe chat a little bit. That one is a nice one, even simplest. You went maybe from the most complicated to the simplest methods. K-nearest neighbors, I think, are one of the most intuitive methods, and it's more similar to. Methods, and it's more similar to decision trees than it is to SVM. They're more in the same category of classification algorithms. I don't know. I would like to see, do people have thoughts on that? Like, what is another way of doing classification just by looking at maybe a little bit of hint neighbors? So, for example, in decision trees, the reason they are similar is if I want to classify this. If I want to classify this point, I'm looking at its neighbors and being like, Oh, well, all of my neighbors seem to have the color red. I'm going to fit in and be red, right? Other thoughts on how we can do that instead of finding pockets without having to do boundaries. I don't know. I know some of you probably would know what is K-nearest neighbors, but I'm curious. Well, I think I've heard of K-nearest neighbors, but could you just take Heard of K-Nair's neighbors, but could you just take the average over all the data points based on weighted by how far away they are? That's actually really, really, really nice because KDR s neighbors are not only used for classification, they are also used for regression, which we did not talk about. Great. So that's perfect. So we can take the majority vote, right? So if I'm doing classification, I can look at the majority of the people. Can look at the majority of the people around me and take the color they have, right? And if I want to do a regression, it would be averages, right? So in this case, if I'm not trying to classify between colors red and blue, but maybe I'm trying to predict a number, we could take an average or a weighted average of all of these values. And the weights would come from how far the points are from my. Are from my point of views. Brian, is that what you had in mind? Sure, but I was thinking even just doing it for all of the data points. Kind of like gravity, like the ones that are near you, but it might turn out to be the same thing and much more computationally efficient, just to say only take the k closest ones and do that. Gotcha. Yeah. Yeah, I guess for k nearest neighbors, we're looking. For k-nearest neighbors, we're looking at what are my k-nearest, let's say what are my five closest neighbors are saying. If I'm doing a classification task, I would take the majority of these five people and decide on my color. And this is exactly what K nearest neighbor stands for. K is how many neighbors I'm looking at. I'm going to take a few examples. Nearest here is in whatever. Nearest here is in whatever sense you decide. In this case, we're going to take the general Euclidean distance, so the shortest distance, Euclidean distance from a point to another point. Let's see how it works. I have this gray point over here. I want to be able to figure out if I should give it the color red or the color blue. I look at its closest neighbor. Its closest neighbor happens to be red, right? Which in this case might not be a really great decision to just look at your own. To just look at your only closest neighbor, maybe your roommate, right? You might want to kind of extend a little bit. So if I instead look at the two closest neighbors, which is often a bad idea in practice because you don't want to choose an even number because you'll have to decide or flip a coin. So typically in practice, we would choose an odd number. So for example, if I decide to look at my three closest neighbors, I would see two of them have the color blue and one of them have the color red. Color blue, and one of them have the color red, so I'm going to go ahead and choose or take the color blue. And I could alternately look at my five closest neighbors and see what colors they have. And this is usually a decision a user have to make what color or how many neighbors I want to look at. Picking too small or too big is often a really bad idea. So there is some sort of middle ground, picking a number of neighbors, and this is something in practice. Number of neighbors, and this is something in practice. You can kind of run different number of neighbors and see which one is kind of giving you the best results in terms of classification. So, for example, if we take this point over here, also look at his five closest neighbors. You notice four out of five have the color blue. So, we go ahead and assign it the color blue. And this is kind of one of the easiest or simplest tools. Kind of one of the easiest or simplest ways of doing classification, which is called k-nears neighbors, and this is it. Henry, do you have any thoughts here? I know you, we have a question on boronoid diagrams, which I think you wanted to talk about. Yeah, I'll share my screen a little bit. Cool. So in the chat, somebody already mentioned Voronoi diagrams. And so I wanted to say a couple of words about that. And I sort of have a visualization challenge that I'm interested in a little bit. So when you look at one year's neighbors, When you look at one nearest neighbors, right, you could really think of a tool called a Voronoi diagram to describe this. So what is a Voronoi diagram? Let me try to do it here. So pretend our, you know, our data set is all of the blue points and all of the red points. Okay. In a Voronoi diagram, you divide space up into the regions which are Which defines the set of points closest to one site than to any other site. So, this pentagon here might be all points in the plane that are closer to this particular data point than to any other data point. And you can keep going, and you get these pretty pictures. So, describing the Boronoi regions of these data points. All right. So, I think these have a history in epidemiology. I believe it was cholera. There was a cholera outbreak. And somebody plotted these data points, which were well locations. And then they drew the Voronoi regions. And they found that all of the cholera cases were happening. Cases were happening in households in a particular Voronoi region around a well, and then decided to go break the handle on the well to help end that choler outbreak because it was coming from the infected water in one well. And maybe I've got the disease wrong. Hopefully, that's what's Jon Snow. Thank you. Okay, and it was cholera. Fantastic. So, when you're using one nearest neighbors, you're looking at the One nearest neighbors, you're looking at these Voronoi diagrams, and Flora has drawn this particular red data, this particular gray data point, which lives in the Voronoi region of a red site. So that's why she would choose to label this data point red when using one nearest neighbors. But folks also consider what do these Voronoi regions, Voronoi regions. Regions, four-noi regions look like when it's not just one nearest neighbors, but three nearest neighbors or five nearest neighbors. So here's a picture of Voronoi regions. My sites that I chose are, this is an image from Wikipedia. The sites are all these positions, and then you can draw the Voronoi regions, defining the set of points closest to a single site. Okay, but what if I change from k equals one to k? if I change from k equals one to k equals three, right? So what's the set of points in the plane that have these three points as their three closest neighbors? What does that set look like? Let me comment that Voronoi diagrams are used a lot in art, maybe a mathematical brand of art. So these are particular images by Goldman Levin, you know, choosing points in the You know, choosing points in the plane so that the Voronoi regions you produce looks like a human face, either this person on the left or this person on the right. If you do a Google search for Voronoi art, you'll see a lot of fantastic pictures. You know, here's a dog that's been pixelated in various ways. I really like this picture on the bottom. It's a mix of a photograph with a Voyno. With Voronoi regions laid on top of it. If you want to take a real photograph and make it seem a little bit more cartoon-like, you can do Voronoi regions in color space. And so you simplify the number of colors that are used. So my challenge, if anyone's interested, is Voronoi regions coming from this data context. So let's pretend all of my So let's pretend all of my data points are either red or blue. So let me draw a bunch of blue data points and a bunch of red data points and pretend all the regions are either red or blue instead of this rainbow's worth of colors. So this is the picture for k equals one. You know, how does my classification You know, how does my classification look for k equals one? I can classify any point in the plane using this picture. But it'd be nice to have a video or a GIF that changes from k equals one to k equals three to k equals five to k equals seven. I think those are the types of visualizations that would be fantastic to add into how we present data science and think about data science, although I haven't seen that particular visualization yet. All right. All right. So Laura, let me pass over to you to discuss artificial intelligence and art briefly. Yeah, certainly. That was one of, if you remember the first meme I had, the one that talks about AI taking over the world and how terrible it could be, like how silly it could be. So that fits in the It could be so that that fits in the left part where a lot of people in the media you might find doing these sorts of things using generative adversarial networks are similar to our discussion earlier, like having a video of someone and feeding it a transcript and having it look like someone is really speaking those words. So, this is another example a little bit into art where you start with an image, which is the one in the upper left corner. The upper left corner. I think that's an image in Tumigan, Germany. And what this specific algorithm or software did, I linked a few links to code or articles about that, is how can you take different artists and transfer their style to this picture? Like, how would they have drawn this, right, if it were them, right? And especially inspired by certain paintings. By certain paintings, like the sternite and so on. You might be familiar with some of these artists. So, that was one kind of really silly examples of applying AI to these sorts of stuff. I remember about maybe two years ago, and I took one of the machine learning classes. What we did is some sort of similar stuff, but using LSTMs, which is long, short-term memory networks. Short-term memory networks also fit under neural network machine learning, where we took one of the composers music, I think it was Bot, and had a collection of their music compositions and so on, and generate a new music, which is computer generated. And if you're not super familiar with their work, you might not even recognize that this is video or computer generated than it is. Computer-generated than it is surreal composition. So, this kind of both fit in the same context. And I don't know how much, you know, again, these are only really highlighted in a lot of media and a lot of, you know, these kind of news sources. I don't know if people have other ideas of seeing AI applied to art in different ways and maybe what thoughts you have on that. Maybe what thoughts you have on this. And maybe now is a good time for us to sort of formally wrap up. So thanks so much for the time and attention. It's been a fun experiment for us to try to talk about data science in a visual way. We have a panel starting in 15 minutes, but we're happy to take any questions or hear any ideas or any discussion until then. Until then, I've got a question. There's lots of memes around where they say, oh, AI wrote these Valentine hearts, and they're all kind of, you know, crazy and not what a human would do, and so on. And I think most of these are faked. I don't think they're actual because they're too kind of witty in a strange, backward way. Have you seen any of these and what do you think of them? Like writing a novel or writing a poem or something like that through AI? Oh, definitely. I was even taught how to do that in a class. So, this is something people, a lot of classes, you kind of learn as a homework, like how to do that, right? How to come up with either writings that look, you know, some of them are really terrible, right? They don't even are not cohesive and stuff. Yeah, I don't like these stuff. Yeah, I don't like these stuff. And it doesn't bother me. Like, I would never see AI replacing any of these things. So, yeah, I don't know. Are they actual AI productions or are they somebody just kind of faking it to say machines can't do things? Because I would think if you actually programmed it a little more reasonably, you could get results that were a little closer to actual. Yeah, I think so. Yeah, yeah, I think so. I'm not sure about the specific examples you're referring to, but definitely. Henry, do you have thoughts on this? Well, I had one comment that this conversation is making me think of, which is I was at a talk, I forget the name of the artist, and maybe if Henry Segerman's here, he'll still remember, because I was at a workshop that Henry Segerman helped organize at Isterm. Organize at ICERM, geometry and art. And there's one of the artists who makes a lot of, I guess, Twitter pictures of geometric patterns. And he uses AI to produce many patterns. But then I think he hand selects out the ones that he likes the best that he actually posts on his page. Is this tiling bot? Yes, that's right. Is this tiling bot? Yeah, yeah, yeah. Tiling bot. I don't think it's. I don't think it's, he's not using AI. He's just has some random parameters and generates a lot of them. Oh, I see. I see. Okay. I see that, but his idea of random parameters, but then choosing by hand the ones he liked the. Well, I think the paramount is pretty well chosen, so that pretty much whatever you get is going to be interesting, but he does do a bit of curation as well. It's a bit of a cheat. But some things might not look good in some colour colours. Like some things might not look good in some color combinations or something like that. What was the there's some three-letter acronym GPT-3 or something that's been around for AI generated text that seems to be very, very good recently. Does that sound familiar? Yes, that's correct. Yeah. So, I mean, I don't know about like, you know, people making fake memes. Making fake memes. I mean, the robots are here. I mean, they're writing news articles already, as I understand it. I mean, I mean, either the people who did the coding made them, gave them like really horrible parameters or something, or they're fake would be my guess to make them funny. For AI generated text, I recommend looking at the work of Janelle Shane, who goes by AI weirdness. Who goes by AI weirdness on Twitter? I should turn my camera on. And who is the author of a really fun book called You Look Like a Thing and I Love You, which itself was an AI-generated pickup line from a larger collection that she generated using, I think, GPT-2 rather than GPT-3. There's also more recently a program called AI Dungeon, which uses GPT-2 to create a kind of infinite text adventure. Text adventure game that unrolls in front of you, it'll just respond to anything you write, no matter what how crazy it is. Yeah, that's quite good. I'm sorry, because I walked away for a second, so I missed how you got onto neural style transfer. Was there anything else in that theme that had come up? All right, let's just check. Yeah, it was just started with generative art. Yeah. Of art. Yeah. Interesting. Thank you so much, Laura, Henrik. That was great. Thank you all. Any other