So the last 10 years, my group and myself mostly concentrated on intelligent research in biology, neuroscience, and precision medicine. And you know, AI has now become all-catching word for machine learning and data science and statistics. And data science really has this very important component of machine learning, sits in the middle of computer science and statistics. And a lot of the problems my group work on, I will call a scientist. My group work on our call on scientific machine learning. So, for high-stake applications of AI or machine learning or statistics, such as precision medicine, we need to really develop approaches for trustworthy AI or data science. And there are two approaches for called trustworthy AI or data science. One is to share best practices to maximize. To maximize the promise or have a quality-controlled data science process. The other is risk management to reduce the danger after something bad happened. So I and my team have developed this framework, ACOPCS, for predictability, computability, and stability. Try to unify, streamline, and expand ideas and best practices in both machine learning institutions. Really try to develop a quality controlled data science. A quality-controlled data science pipeline with documentation and really integrate these two cultures that became this contrast become very well known in the O'Brien's paper 20 years ago. And we try to use PCS to integrate the two fields and build a platform to develop both. 20 years ago, there was another important 20 years ago, there was another important paper by Leo Bryan on random voice. So, specifically, this is statistical machine learning methodology that for some problems still the best approach, and even beating deep learning, especially a lot of genomics problems. And for the rest of the talk, I'm going to use Use random forest and our enhanced version, our modified version, by adding stability following the TCS framework to random forests for each random forest development. And we have very good empirical results for discovering in at least two substantiative projects: one for drosophil biology, the other for cardiovascular health. Health to discover drivers for a particular heart problem. And then we recently moved to understanding a theoretical version of e-tridge random forest. We couldn't write exactly analyze e-tri-random forest, so we took the important ingredients from e-tridge random forest and made a new method called LSFind, which I'll tell you in the second part of the talk. Part of the talk, and then propose a new relevant generative called local spike-sparse model, which is linear combination of Boolean interactions of features and under independent conditions and not overlapping. So it's still restrictive than what we do in practice. So that's the second part, which is very much integral part of the program to understand a new method called it random form. New method called each of random forest. So, each random forest appeared in the paper, I think it was 2018 with my co-authors. Basu is now assistant professor at Carnell. Kumbir was a student at Berkeley, now postdoctoral UCSF, my long-term collaborator, Van Brown from IBL and Berkeley, and is supported by the NIH ENCODE Consortium and NSF. So, the problem we face in genomics. So the problem we face in genomics is harder than computer vision. Computer vision is pattern recognition. You know you want to look for a cat, you find it. But in genomics, you actually don't know what are the meaningful patterns. So it's a lot more expensive to find the relevant patterns and use it. So it's called pattern discovery, not pattern recognition. And it's known that from biology, this is dry software biology, that we have higher order interactions, not just Order interactions, not just pairwise, but up to order four, at least. So, this is a regulatory network: how the Drosophila embryo develop its stripes segmentation. This is kind of a spine development. You can see that you have four colored curves. There are four very important genes in Drosophila, bicot, hunchback, giant, and crupo. I mean, Drosophila biology is very much fundamental for biology, and like five or six Nobel Prizes have been awarded. Have been awarded based on drop sophomore biology. And these are very important genes to develop the layout of the land under development. And there's also something in Joseph Biology called a French flag model. It says that there are interactions that are high order, and the interactions expit thresholding behavior. So you need abundance of biomolecules. By molecules for them to interact. So it's very much a thresholding behavior for development biology. And also, this phenomenon appears in other parts of biology. And this thresholding behavior is well covered by this decision tree type of mathematical analytical form. And that's why we like to use this mathematical entities. Mathematical entities that drive random forests for a non-linear interaction kind of discovery. So the traditional statistical polynomial interaction don't work for these problems because the polynomial interact doesn't have any biological backing. And also, you don't always have many effects before you have interaction. So for two reasons, these are not good models for finding interactions in. For finding interactions in genomics. And 201, Griman proposed random force on top of CART. So you grow CART, which is the decision tree, very well known in the book of 1984 by four authors, Bryman, Friedman, Ocean, and Stone. So on top of using all the features. Using all the features, that's what CAR does. You actually, for each split, you uniformly select a subset of m try number features, and then you use that to do the split. And then you have a bootstrap of samples, and then you average all the trees. So it's really a collection of trees. And before us, people already try to take advantage of this. To take advantage of these trees to come up with interactions. So, this idea of using co-occurrence of features on the same path implying interaction was not new. It was there at least since 209. And basically saying that if two features got split down on the same path in the tree, any of the trees, then they kind of have potential to interact. So, this is a jump from analytical. From analytical property to biological property of physical interaction. But the problem is that these features are very unstable. There hasn't been a lot of progress really using experiment to show that these such found features biologically meaningful. So this idea of co-occurrence was in the literature, and what we added was adding stability. So random forests were already among the leading. Already among the leading methods in prediction in bioinformatics. So, what we did in 2018 paper was to add stability to random forest. So, there are multiple ways we did that. So, first is we use soft dimensionality reduction. We use an important index to downweigh less important ones. And that kind of have a soft dimensionality regularity. And then we added. And then we added the outer loop of bagging on top of this eatery random force to get stability. And then a computation device we took advantage was this work by Schar and Manhausen called random intersection trees to find intersection paths. So that's kind of have some stability in there too, because a lot of randomness was added. So to expand on that, the first iteration The first iteration of it is random far is just random far sample features uniformly. And then you get importance measures. And then you downweight. So the reweighting also in the literature, we just put them together. And now we concentrate on fewer features. So this is dimensionality reduction and therefore you add stability. And then we generalize the random intersection tree of Sham. intersection tree of Shar Mannhausen from Basket kind of research on finding overlap of two fixed sets. We turn each feature, each path on the tree, on a tree into a zero, one vector. So each feature, if get split down on that path, we give a one, otherwise get a zero. So you have this zero, one zero. So you have this zero, one vector of size of the dimension, and you see how many, say, x1 appeared in all these different paths. So each path become entity mapped into a 0, 1 vector. In the original basket problem, it's like an item you purchase from a supermarket. So you have man, woman, and you want to see what's the overlapping items purchased by man or by woman. So this is just turning. So, this is just turned into a 0-1 vector. And then we added a stability bagging. So, everything with all of this gets backed again. And we use predictive accuracies. It's kind of PCS special case through random forest. Computation, we build on RIT, and then we have the auto-loop of bagging to really see. We only want to look at the At the genes which fall on tree paths many, many times, not just rarely. So, this fixed the problem the early works encountered. So, in the development, we did a lot of simulations actually using the model we later used for threat analysis. And for dress software problem to predict enhancers, on the left, we have eight. On the left, we have 80 predictors. On the left, all the different colors are the AOC curves for the three iterations. So we didn't lose any predictive accuracy. And for some of the problem we did later, actually, we improve prediction accuracy when you add stability, when the original RF doesn't have enough regularity. And on the right, what you see is The vertical axis gives you the names of genes. The blue ones are 3B interactions. You see Zelder, giant, and Twig. And the red ones are pairwise interactions. And we just cut at stability score at 0.5. So kind of, you know, agnostic, not tuned to the problem, just not stability at least at more than 50%. percent and the red ones we got um among the red ones the pairwise interactions were able to do literature search and find that 80 of pairwise interactions we found through ether to random forest are validated in the literature by old technology like ablation experiments. So it's really good success. So it's really good success. And of course, we could have missed interactions. But if you look at scientific discoveries, usually you're never going to find everything. You find things that real and that's success. And the blue interactions are three-way interaction. We become a scientific recommendation system that people could follow up to do experiments. So this is not proven of Not proven of causality or anything, but this is a recommendation system. And then we have an enhanced version now for E3 random force, because for the first version, we say it gets split up, we record it. But for signed E3 random farce, that's what actually we mostly use, we take the sign. So if x1 split less than something, we call it minus. Less than something we call minus interaction, and if x2 got split because abundance is an enhancement, not a depletion, we call the positive interaction. So this is more biologically meaningful. And everything goes through. We just have to generalize the RIT to instead of 0, 1, split or not into minus 1, 0, and 1. And still discrete, we can still use similar method to find the stable path. Stable path and everything just stays the same. So, this is a follow-up paper we're still working on. And the great thing we have been able to do is actually have used this assigned E3 random forest to add a track in the UC Santa Cruz genome browser. So now our results for enhancer prediction are available to all the other biologists who can actually use our results. Actually, use our result as like a qualitative recommendation so that they can help them to design their experiment to try to do well experiment to check all these different possible interactions. And the last project we're building on using etherandum, or actually signed etherandom forest, is this, sorry, right kind of this. Right. This project has been just finished for the last three years. It's a consortium under Chan Zuckerberg BioHub with people from Berkeley, Stanford, and UCSF. And you can see it's a huge team of people. I want to have a shout out for two young leads, my student Tiffany Tan, leading the statistical analysis, and Chen Yun Wang, a postdoc in UN Ashley from Stanford. So we work very closely with UN Ashley that is a cardiology lab. It's a cardiology lab for this project. So, what we were able to do is use RF again as the observe, sign RF, as a way to do scientific recommendation system. And there's a lot more work needs to be done. The signal is very weak. So we're concentrating on a particular heart condition called HCM, hyperspectral cardiomyopathy. Myopathy is like your left ventricle chamber wall gets thickened and then become more stiff and you're more likely to have heart attack. But using the label from UK Biobank didn't result in any signal because there are a lot of false negative. A lot of people have the condition were not diagnosed. So we had to redo the phenotype extraction to go back to MRI and to extract something called left ventricle math as a proxy. Math as a proxy. And then we have to turn a continuous problem to a binary classification problem because this problem has very low predictability. We did a proof of concept for red hair that was fine and stability are both very low. And with continuous phenotype, you don't know what's noise, what's signal. So when you turn into a balanced binary classification problem, you actually know if you can consistently beat random tasks, you know their signal. You know their signal. So we recommended four gene and gene pairs to the UN lab, and they have carried out knockout experiments. And we have been analyzing with them the experiments. The conjecture is that if you knock out some of the genes, the cell size become larger. Therefore, the heart might get more stiff. And so we're looking at cell size as the phenotype. And there's lots. Phenotype, and there's lots of work to analyze the images, and all that. We're still in the middle of that, but the result looks extremely promising, and we think we find some very interesting genes. So, this is all on the empirical side and through NALCA experiments. So, the way I'm doing a lot of the threat work right now, not all of them, is very much coupled with empirical work. After we have seen empirical successes in simulations and scientific collaborations, Scientific collaborations, and we turn into theoretical analysis to get more insight. And so, this is an integral part of our RIF development is to now analyze related methods. We couldn't really analyze our. Maybe some of you here can really do that, but we have to kind of make a simpler method so we can analyze. And you need a relevant generating model. So, this is really saying that in the ideal model, there's this type of model does this type of stability driven like rf will really yield find the correct interactions so previous work on random forest theory have fallen into two categories it's mostly for regression function estimation not for you know selecting the right genes and usually have some continuous um you know smoothness conditions and look at consistency risk of convergence Look at consistent risk of converging asymptote normality. This is not what a match a good match for our genomics problem. And the other lines really look at feature important measure, dealing with noise features. And we had some work too. But nobody's looking at kind of model selection consistency result. But to really study model selection consistency, I think this is really a fitting top for this, you know, lots of talents here in this workshop. It's really for you guys to do more analysis. Really, for you guys to do more analysis under this, I think the relevant models, which we couldn't analyze RF. We had a you know tractable version. So this work, I should really give huge credit to my three co-authors, my former poster, Merber, now back to Germany, Hiu Wang, former student, Xiao Li, and both, you know, sadly lost to industry, but they both, they're just a great team. Both it's just a great team, and they did all the proofs. I mean, my contribution is more. I help them to ask questions and present. So, really, they have they should have all the credits. So, we have a new model, which was already in the RF paper for simulation. So, the simulation for devising RF simulation study was huge part because we only have one data set. We had another data set we want to save for validation. So, we simulated from this Boolean model. Boolean model and then help us to tune the algorithm. So it's a huge integral part of develop methodology. And then we develop a new method, we couldn't really understand RF called depths weighted prevalence. And then we show that under this new model, this LS find can discover all the interactions. And then we have simulation study. And then we have simulation study. But interesting things, we don't have to estimate the continuous parameters. We can go directly for the discrete structure. So here's the model, just in the most simple form, right? So the model we call local spiky sparse model. Don't ask me why, and that was the name. I don't quite remember. But we use it for the simulation. So it's really, we assume that the regression function is a linear combination, it's an offset of Boolean interactions with thresholds. interactions with thresholds gamma and with a linear coefficients. So a biological interpretation is that you have non-overlapping pathways which are represented by these Boolean interactions and they work together in a linear fashion. Sometimes if beta positive they kind of build on each other or they can inhibit each other. And this is just an example of saying that you know this is Of saying that you know, this is a particular form of the LSS model. You want both the genes to be small in terms of threshold for interaction to happen to draw a particular phenotype, for example, the heart disease. But if you have overlapping interactions, we just finished a proposal to study the identifiability. So, when you have overlapping interactions for the different terms, which in biology definitely happen, we just cannot analyze it, that you might end up with different signs. They have equivalent representations, right? You can write the top is like the two orange bits, and you can write it as one minus the off-diagonal orange bits. And they give you exactly the same regression function. Same regression function, and if you interpret the two models, you have two different sign interactions. But the good thing is, actually, if you look at just interactions, they are identifiable. So more work needs to be done. And we just finished a proposal to study this problem. So for the threat of work, we're looking only now, the different booty interactions are not overlapping, which is definitely restrictive than in practice, but. Restrictive than in practice, but hopefully it's a good first step. So now we have to formulate this empirically successful eigenvalue into something we can analyze. So we know that we need to bring in stability. Otherwise, simulations, you're not going to have model selection consistency, even under this model. So two alterations we did. We want to keep stability and we design now to analyze. And we design now to analyze random intersection trees. It's just another layer of randomness. So we just assume that we can just find for all the trees directly without the randomness of section, just say we can actually have exhaustive search and then find the stable path. So how we bring in the stability is that instead of using E3 random forest, we just made it easy by saying that we just know how Easy by saying that we just have the hard threshold impurity index. So if the impurity index has a jump, then it's probably very real. And we're going to only take those. So this is the auto rate, the surgeries we have to do to make it tractable. We can do mathematical analysis. So a very important notion in our proof is this called a stable path set. Called a stable path set. It's actually lots and lots of bookkeeping to just keep everything straight, have the right notations. So we said that we're going to only pick the split instead of looking at the empirical stability. And we're just going to do it. If the split has a certain impurity index larger than threshold epsilon, we're going to keep that. So giving a tree and a pass P, this set gives the indices and corresponding signs of first. And corresponding signs of first appearing stable features with empirical index larger than threat or epsilon. So we're only looking at the first. If x1 of gene 1 gets split twice, it doesn't get counted, which is the first time. It makes things more uniquely defined and keep tracking how many times. And then we brought in this depth-weighted prevalence. Prevalence is also a form of stability. So this is on top of the So, this is on top of the threshold stability, actually also broadening stability through this depth-weighted prevalence. So for any given sign set, so the notation is like the example, one minus one, one positive one, and three plus one. What it means is gene one gets split in less than something, and then gene one got split when. And then gene one got split when it has to be larger than something. And gene three got split when it's larger than something. So that's the notation. So for any such sign feature set, you have the feature names and which way it goes less or larger. For that set, you define this, you can say a statistic or metric called that weighted probability, DWP, which is the probability that this set belongs to the That this set belongs to the stable set. And the randomness t comes from tree construction. This is conditional data. This is just about, this is only in the process because random foreign has so much randomness. And the randomness P is saying that for any path with depth D, we give a probability 2 to the minus D. So that's why it's called depth with. So that's why it's called depth-weighted probability, and that you get this quantity. This is, you have the data. In theory, you can calculate this quantity. And the main results are pretty delicate. I should go pretty fast, I think. So the upper bound is general, and the lower bound we do have. Um we do have neat condition all the parameter in our model the beta and gamma had to stay away from zero with a fixed you know with a fixed amount and so the upper bound is true for any path there's nothing uh just counting for the lower bound we were able to show that's the key for any true union of interactions if you cannot always separate individually the upper bound you achieve with equality Equality. So that's the defining property of the two intersection. And then for one interaction, you have a gap. You cannot achieve the upper bound. So we use this to prove consistency, model selection consistency. So, and then you can devise the algorithm. So the interesting thing is for this DWT-based algorithm, we never estimate beta zero, beta k. Estimate beta zero, beta k, or gamma j. This really, I had this conjecture by going to all the biohub meetings. A lot of discrete objects are figured out without having to estimate in the continuous parameters. I think one example come out is that when you have maximum likelihood estimator from zero to theta, we can estimate the theta at the read one over n. But continuous parameters, we estimate one over square root. So I think there's something like. Square root. So I think there's something like that going on here. We cannot be quite precise. Is that sometimes, I think a lot of them we should go for the discrete structures first before we worry about the continuous ones and mixing them up, I think, can make things harder for ourselves. So I don't have time, but just what I said already. So population case is the key. And then, you know, sample cases VC series. And it was quite delicate, a lot of notation. Quite delicate, a lot of notation, and you can look at the paper. And the insight from the theoretical analysis that this really shows that for you have interaction for L order, you just don't have a lot of data falling into that little rectangle. Because random force is invariant with a monetary transform, so we just prove things for the uniform case. Which is proof things for the uniform case. And the best possible stability through dwp is two to the exponent of minus L. So it's really, that's why it really shows why high order interactions are so hard to find. You just don't have much data. But the problem with our new method, which is easy to analyze, that we really two stream. We're really too stringent, and there's a lot of weak interactions that can be still useful. We're going to do experiments, biological experiments, later, right? So, this way, you don't you're too strict, and then in practice, it won't work very well. And we have some conditions to justify the recommendation of M tri should be kind of same order P, but not too much smaller. So, here's some simulation. Don't worry about it. I just tell you that the result is like. The result is like overlapping Boolean terms and dependent features make things harder. You know, from the proof, you can see that, but noise distribution doesn't really hurt us too much. So that's kind of the takeaway. And relative to surf, it's not really finding exact interaction. That's too much to us. It's very weak signal. We really just want to find the right ones and we use experiment to figure out the interactions. So you can see. The interactions. So you can see that you look at the lower panel, when the blue ones are related to the practical algorithm, and the orange ones are corresponding to the theoretical method. You can see the blue ones, which we use a lot better when you have more than two pathways or one or two. So when you have the same pathway, they're comparable, but when you have more than two terms in their interaction, our practical method is better. Our practical method is better, but we cannot show theoretically better. So, to summarize, I show you like E2 random force as a case study of PCS and the importance of having a relevant model to the problem you want to deal with, which is that's why the new Boolean model. And we can show a model selection consistency and simulation was helpful to get out of the theoretical condition to show that the things we use in practice still better in practice. It's still better in practice, but it's hard to show. And here are the references. And I also have one of the shout out. So we're building a software package called Revertical Flow to make the PCS stability analysis much easier in general. And we're also building another software called PCS SIMSH to make PCS assimilation made easy. So we'll check out these two software packages. One is in R, one is in Python. The first one is in Python, the next one is in R. I think in Python, the next one is in R. And the PCS principles we put into a book, which now pushing to finish the book with my co-author, Arika Butter, my former student, current postdoc. And hopefully you'll see in a year the hard copy. We're going to have a free online copy. So try to take into account the whole data science lifecycle and also use the PCS framework from problem formulation, data cleaning, and then stability then for the modeling stage and interpretation. Stage and interpretation and also inference. Thank you. Sorry, I ran over time. Thanks so much, Bin. Maybe we have time for one really quick question. No, but maybe we move over. Thanks so much again. Thank you for having me. Okay, our next speaker is Andrea. Are you here, Andrea? I am, Gabo. I'm not there.