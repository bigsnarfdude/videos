Who has seen this talk? I think two weeks ago, but happy to hear it again, Morgan. So, as the subject of the workshop was a bit more statistics, I wanted to tell you about a recent project on the most statistics side of my work, rather than probability. I hope that's okay. So, today I'm going to tell you about the asymptotics of probably one of the most used and useful methods in statistics and machine learning, which is Statistics and machine learning, which is the cross-validation method. And despite its importance, a lot of its theoretical properties is still poorly understood. And so I'm going to show you today some surprising behavior that we have discovered for it, which hopefully you will also find surprising and interesting. Just before I carry on, can you see my screen correctly? Yes, thank you. Yes. So this is based on joint work with an amazing postdoc, Wenza Su, who is currently postdoc at NYU and at A currently postdoc at NYU and at the Flight Iron Institute. So, to fix matters, I'm going to take X1 and TXN to be a sequence of IID observations that are drawn from a certain distribution view. And our goal here is to estimate a certain feature detail of a distribution. So maybe I'm interested in estimating the mean, expected value of x1, or maybe I'm interested in estimating the median of mu or any other type of feature. But for simplicity, for two seconds. But for simplicity, for two seconds, let's zoom on those two very simple examples. And so, to do so, I'm going to build an estimator, theta hat, which is nothing as an estimator, a function of my training data, x1 until xn. And so, in the first case, a natural choice could be the empirical average, and in the second case, a natural choice could be empirical median. To evaluate how good this estimator is and how useful, for example, it is for prediction, we can choose a loss function L. In the first case, the natural choice could be the square loss. Choice could be the square loss, and in the second case, another choice could be the absolute error. One of the most important quantities that you might want to know about your estimator is its risk, which is its expected loss on a new independently drawn observation. But as you obviously, you don't know what mu is, you don't know what the distribution of your data is, and so you cannot compute this expectation, and you need to estimate it, and you need to estimate it using data. So, how can you do it? Well, the first naive idea or simple idea that we might have. Naive idea or simple idea that we might have is to propose as an estimator the empirical average of the losses over my training data. And this is what we're going to call the in-simple estimate of the risk. The problem with this very simple idea is that here I'm using the same observation, x1 and txn, both to train an estimator and then to evaluate the performance of that same estimator. And so that estimate of the risk is going to tend to be too optimistic, is going to systematically underestimate what the risk of my estimator is. Underestimate of the risk of my estimator is it is a bias estimated of the risk. So, what is the next natural idea? Well, the next natural idea that we have would be to split my data set into two, into a training set and a testing set. And so, for example, I would estimate a model, let's say, the first n minus m observation, and then I would evaluate the performance of that model on the remaining m observation, what we can call the holdout set or the testing set, by computing the empirical losses over the. The empirical losses over this cold upset, and this is what we can call the um. And here we do we remark that we do not suffer the same drawback or the same problem than previously because here we are estimating our model on a different part of our data set that we're evaluating it on. And so, this is an unbiased estimate of the risk of an estimator that is trained on n minus m data point. However, obviously, the problem, the drawback here is that The drawback here is that there's always some part of the data that is left unused for learning, and this is the whole outset. And so, and this can be a problem, especially, for example, for high-dimensional estimators. So, what would be the next natural idea? The next natural idea is to split my data set into k with different fold. Iteratively, I would take one fold out, and then I would estimate a model on all the other folds, which are faults 2 to K. And then I would evaluate the performance of this estimator on the fold I was just. Performance of this estimator on the fold I just took out, the first one, and I would do this iteratively for a k different fold. This gives me k different estimates of the risk, and the k for causality dated risk is nothing else than the empirical average of those risks. Please note that for the rest of this talk, I'm going to keep on saying k for causality dated risk, but the number of faults k here is allowed to go arbitrarily fast with n, special cases when I'm considering k to be equal to n, in which case I'm talking about leave one out cause validation, where each fold contains. Validation where each fold contains exactly one observation. So, as I said, quasi-deadation is one of the most used and useful methods in statistics. It has the cornerstone of statistical practice. However, it's obviously computationally much more cumbersome to compute than the simplest plate risk. I am, after all, computing K different estimators, which can represent a significant computational burden, especially if my number of false k is large. So, natural question is, is it worth it? Or in other words, what is the statistic? Or, in other words, what is the statistical gain of doing k for quasi addition compared to doing split race? Another naturalized question is: how should we choose a number false? Should we do two for quasi-deadation, ten-fold quasi-deadation, or maybe leave one out quasi-definition? So, is there an optimal choice of a number false? At least statistically speaking, if I don't take into account any type of computational constraint? And last but not least, can we actually do consistent confidence intervals around the K for confidence? Confidence intervals around the care processed risk. So, those are the three questions I wanted to center on today. Before I try to answer them, I wanted to know if there's any question about the setup. I think it's okay. I don't know. So, obviously, as a cross-validated risk is one of the most important and used method in statistics, there has been a lot of work studying its properties. Some of the work that is the most related to Work that is the most related to us is the series of work which bounded the expected square difference between the K-for-quasi-dated risk and the notion of risk under certain stability condition on the estimators. The problem and while those estimates works, we were able to find very interesting findings, but we were not able to answer the three questions I just raised. Why not? What made the analysis complex? What made the analysis complex is the dependence between the four Complex is the dependence between the fold. It's the fact that I'm using each observation to train k minus one different estimators, and then I'm reusing it once again to evaluate the performance of an estimator. So to make this more clear, we can denote like that all the data x1 and xn except the IF1. And so the Livon out for the data drugs can be reseen as this empirical average of the loss of the Xi for an estimated strain on everything but Smith of a distraint on everything but xi. So you could look at this and tell me, well, again, this is an empirical average. So surely I could use one of my classical probability tools to study the distribution of this empirical average. Problem is that while this is an empirical average, it is an empirical average of highly dependent observation. For example, the loss on X1 is dependent on the losses on all the other observations because all the other estimators are trained using X1. And so, this K for quasi-ejectated risk or believe one of the quasi-ejected risk here is an empirical average with a lot of dependence. And so, to be able to determine this asymptotic distribution, we kind of need to control its dependence structure. And so, we do so by imposing certain stability conditions on the estimators. And so, those conditions heuristically ask that if we change one of our training examples, for example, X1, by the pen copy, it wouldn't change too much. It wouldn't change too much behavior of my estimator on a new independently drawn observation, on velocity on a new independently drawn observation, in the sense that this difference here would be a little O of one over the square root of n. Please note that those are very similar to the stability conditions that we imposed in previous work, and that for most parametric estimators, this difference here is going to tend to be on the order of one over n. And this tab. And this stability condition also holds for high-dimensional equation as long as p equals sub-linearly with n, where p is the dimension of my problem. So under both stability conditions, we were able to determine what is the asymptotic distribution of the k-fo-quas-related risk and what is the asymptotic distribution of the split risk. And because we exactly determine the rate of convergence, we can exactly characterize the speed of convergence and compare them. So, what can I tell you? So, what can I tell you? I can tell you that the asymptotic distribution of a k-fo-quasier data risk is asymptotically normal, that it converges at the speed of square root of n, and I know what this asymptotic distribution is. Please note that this holds no matter what is my choice of the number of faults. This result holds as well for two-fold post-validation as for live-1 post-validation. If, in addition, I also know that the number of faults don't go too fast, so we go as a little O of n. As a little O of n, where I also know that the split risk is asymptotically novel, but converges instead at a speed of square root of n divided by kn, and I also know what the asymptotic variance is. It is interesting here to note that there is this additional term hole in the asymptotic variance of the key for cause validated risk, and this term hole here quantifies how much dependence is introduced by the fact that I'm using an observation both to train an estimator and then to evaluate the both. An estimator and then to evaluate the performance of another estimator. And we're going to see that determining what this hole is is key to determining what the speedup is, but we can already know that this term hole counts on the dependence I was just talking about. So before we move on, if we go in detail, let's try to gain some intuition on this. And I want to do so by going back to what was hoped to be true. So what was hoped to be true is that if my number of falls don't go too fast, so for example, if I'm too focused on Coasalidation, and if our model is very simple, this is a parametric estimator, then maybe doing the variance of the K-focal quasi-first false would behave as starts to behave as independently asymptotically, so that asymptotically, the variance of the K-focal quasi-lated risk would be k times smaller than the variance of a split risk. And even though that's not enough to tell me everything about the distribution of my estimator, it was hoped to that it would indicate that doing careful causalization That doing careful cause identification would be like doing the split risk on K times more data. And that was often seen as the best we can hope for, the best case scenario, and what we hoped to be true and we refer to as full speed up. So now I have one question. This is under stability conditions, right? No, so that's what we hoped to be true. So that was even without stability. I guess maybe with stability, but that was just heuristic. Just for hope of heuristic, right? I just wanted to mention that we have an example, an exercise in our book that shows that without stability, the gap could be as bad as half between the estimated and true way. So I'm going to, in many cases, we're going to see that that actually doesn't hold. But that was what, you know, a lot of people think, or at least what I thought might be true. My estimator is very simple under stability conditions. Yes. And so now I kind of spoiled the. And so now I kind of spoiled half of my question, but I'm still going to ask my question: which is: if you're doing two-fold posalidation, do you think that we're in the setting of full speed up for parametric weak true regression? So we're doing two-fold posalidation, parametric rig regression. So very simple estimator. Are we in the setting of full speed up? If you're angry, for sure. I do not hear that. Speak up. Speaker, if you're regularizing a lot, sure. If we're not, I'm sorry, I could not hear that, but if you regularize a lot, then yes, that's what that's the guess. So we're going to see that's actually not the case. And this is part of the surprising behavior that we discovered. So, to be able to answer this question, we need to compare the speed of convergence of the K-focal parse data with canvas behave asymptotically. Behave asymptotic speed of convergence of a split risk. And because we know that both are asymptotic, we know more to compare the speed of convergence, we only need to compare the asymptotic variance. And so we see once again that determining what this term here is is key to determining what the speedup is. If ho is equal to zero, then good news. You are in the setting of full speed up, and doing k for cross-validation is like doing the speed risk with k times more data. You are in the best case scenario, or what we hope to be the best case scenario. If HO, however, is positive, that means that the dependence between the fold is hurting you, and that the causality dated risk is not converging as fast as you would hope for it to do, and you're in the context of less than full speed up. If, however, however, fall is negative by some type of miracle, then that means that the dependence between the falls, far from hurting you, is actually helping you, and that the You is actually helping you, and that the K4 quasi-dated risk is converging even faster than in the case of full speed up. And we're going to see that this happens even for very simple estimators. Before I show this and we go to examples, let's try to answer the two other questions. So, can we actually draw consistent confidence intervals around the K for quasi-dated risk? Well, now that we know that the asymmetric distribution of the K4 quasi-dated risk is Distribution of a capable quasi-dated risk is normal to be able to do so. We only need to be able to estimate what the asymptotic variance of a capable quasi-dated risk is. And if my number of faults grow with n, then we propose such an estimator for the asymptotic variance of a capable quasi-dated risk, and we propose a method to build consistent continuous intervals around the capable quasi-dated risk. To evaluate how accurate those ascent continuous intervals are in finite samples, we also obtain Periscine bounds. We also obtain Perisin bound, which for parametric models tend to be on the order of one of the square root of n, which is not very surprising. If my number of force don't grow with n, then we were not able for the moment to actually propose an estimator for the escortives because while determining an estimator for sigma 1 is quite trivial, determining an estimator, for example, for who is quite complex because who depends in a quite a non-time. Depends in a quite a non-trivial way on all the different estimators, all the different observations. And so that can be a nice, interesting question to look at after. The second question we raised is, how should we choose my number of faults? Should I do two for quasi-edition or leave one out quasi-idation? Well, we noticed that very surprisingly, the asymptotic variance of the K-fold quasi-idated risk does not depend on the number of the choice of the number of false. So the asymptotic variance is So, the asymptotic variance is the same for two-fold consolidation than for leave-one at consolidation. This was, at least for me, quite surprising. However, obviously, the bias of doing care-fold consolidation for most models is going to decrease as my number of folds increases because the size of the holdout set will decrease. And so, that means that for most models, the optimal choice of a number of folds is to choose k to be equal to n and to do leave-won-out cause validation. At least statistics. outpost validation. At least statistically speaking, if I'm not taking into account any computational constraint, so let's try to exemplify all of this by zooming on a class of estimators that while are quite simple are very important for statistics and for which we already observe quite a rich set of surprising behaviors. And those are parametric M-estimators. So here we're going to choose a loss psi and we're going to choose Gonna choose an estimator to follow to minimize the following empirical risk. So, for example, if I choose psi to be the square loss pinalized, then maybe I'm interested in doing ridge regression. Or if I'm choosing psi to be the hinge loss, maybe I'm interested in doing in a classification problem. So, I'm going to choose TAHA to minimize the following empirical risk, and then I'm going to evaluate the behavior of the performance of this estimator. Overperformance of this estimator using a potentially different loss L. And so if I'm training my model with the same loss that I'm evaluating it with, so xi is equal to L, then good news, whole is equal to zero. In the case of full speed up, doing careful cross-readition is like having k times more data to do the speed risk. Everything is wonderful, life is beautiful. However, we're going to see that this is not necessarily the case if you train your model with a surrogate loss. The surrogate loss. So if psi is different from L. So let's look at this very simple estimator problem. So here I'm doing widge regression. And so I'm training my model with the square loss penalized and I'm evaluating it using the square loss. Then very surprisingly, we actually observe that whole is actually negative. That means that the dependence between the fold, far from hurting you here, is helping you pay for pause validated risk converge. For quasi-dated risk converges even faster than if you had k-tamps no data to do split risk. That was very surprising to us. We did not expect to see this for parametric read regression for such a simple estimator. So let's look at this with experiments. Here I'm doing two-fold cross-validation, and we are doing parametric regression. And here we are plotting or writing the waitial region of variances. And so here we are seeing that doing twofold quasi-adation. Doing two-fold quasi-electrons is like having more than three times more data to do the spread risk. This was very surprising to us. And so, here we observe that if we train our model of the survey class, the behavior of the chemical cause-alienated risk is much more complex than was previously thought. Even more surprising, you can see that what the speedup is does not depend only on what my estimator is or how it's trained. It also, I'm sorry. Also, I'm sorry. I'm sorry. Depends on the distribution of my data. So let's see this with a very simple classification problem. So here my data is generated in the following way. At each step, I'm going to flip a coin. When my coin falls on the head, I'm going to generate my observation according to a certain distribution T1. And then my fall. And when my confools on til, I'm going to generate my distribution, my estimator opponents of certain distribution t2. And so I'm generating x1 and txn like this. And my goal is to do classification. And so to do so, I am going to use a simple linear discriminant analysis. And I'm evaluating all of this using a simple zero one class. Then, if D1, if we choose D1 to be If we choose D1 to be, for example, a gamma with parameter 10, 0, 15, and if we choose D2 to be a gamma with parameter 1, 1, then we observe that HO is actually positive, it depends which hurting you, and the K4 causal identity is not converging as we would hope for it to do. And doing two-fold causal identity is only like having 1.6 times more data. However, if you simply change T1, You simply change T1 to be a gamma with parameter 110, suddenly the whole becomes negative. Doing k for quasi-addition is even more helpful than we ever hoped for it to be. And doing two-foot-quasal addition is like having two times more data to do the split risk. And so here we see that the behavior of the k-focal quasi-ali-dated risk is very complex and depends on an integrate way on what my estimator is, how it's trained, and what is the distribution. Strained and what is the distribution of my data? So, this was very surprising to us. We really did not expect to see this. So, let's try to gain some intuition on why this is happening. Well, as I said, the key turn to determining what the speedup was was HO. And HO quantified how much dependence is introduced by the fact that I'm using the same observation both to train an estimator and then to evaluate the performance of another estimator. Evaluate the performance of an estimator. That means that if typical observations on which I'm going to suffer an important loss, that are hard for my models, are actually important learning examples, they are not only outliers, then whole would tend to be negative and doing careful parser edition is very helpful. So, for example, this is what happens with the parametric rich estimator example. Here, as a reminder, we trained our model using a square loss pin. Model using a square loss, penalized and re-evaluated it using a square loss. Examples that have an important square loss or suffer an important square loss are actually important learning examples because they'll help to compensate for the penalization term. This is also what happens in the classification problem. However, if observations on which you are going to tend to suffer an important loss are bad learning examples, which for example tend to be outliers, then doing careful Outliers, then doing K-for-quasi addition is not as hopeful or as useful as we would hope for it to be, and hold will tend to be positive, and K-for-quas ideation will not converge as fast as we would hope for it to do. And so we here, what do we do? We impose certain stability conditions, and under those different stability conditions, we were able to determine exactly what was the asymptotic distribution of a K-focus-validated risk and discover which sets of behavior that was completely. Which sets of behavior values completely unknown. An interesting question to ask would be how to know how tight those conditions are, and what would happen if those conditions are not met. As a reminder, heuristically our condition asked the following, which is that if we change one of our training example, for example, X1 by an independent copy, that you wouldn't change too much velocity new independent observation. And that we as a vase change would be a little O of one of the square root of n. Well, we observe. Square root of n. Well, we can propose different examples for which, when this doesn't happen, the asymptotic distribution of the k-focus data risk is not normal anymore. So let's just look at this very simple example here. So here I'm generating my data in the following way. I'm taking xi to be IID uniform between 0 and 1. And I'm writing or denoting with yi the position of xi in the segment 0, 1. And so if xi is smaller than one half, I'm denoting yi. Smaller than one half, I'm denoting yi to be equal to one. If xi is bigger than one half, I'm saying that y is equal to zero. And here, our goal is to build a classifier for y, and we can do it by using, for example, a nearest neighbor. To not have a quasi-dated risk that has degenerate limits, the loss function that we consider is a risk-L01 loss. Then we observe that the impact of changing one of the training. Changing one of the training examples is exactly proportional to one of the square root of n, and so that our stability conditions are not met. In this case, the asymptotic distribution of the K-focal quasi-dated wisdom is a complex mixture of Poisson distribution. One could also look at this example of this noise example here where we are generating additives in the following way. Xi is IAD minus 1 plus 1. And our goal is to. One and our goal is to estimate the mean of x1, so expected value of x1, and we do so by looking at as proposing as an estimator with simple empirical average. Once again, so that our estimator is non-generate, we scale the loss function by square root of n. And so, once again, the impact of changing one of our training example is now exactly proportional to one of the square root of n. And so, our stability conditions are not met. Then, the cause identity. met. Then the cause identity risk is once again not asymptotically normal, it's also not asymptotically possible, it's a complex mixture of chi-square distribution. And so we observe that when our stability conditions are not met, the asymptotic behavior of the key for parser dated with is even more complex and depends very highly on what the estimator is and what is the distribution of the data. And so how in general to do consistent confidence intervals for K4COSLA data risk is still a very important question. Is still a very important question. We also noticed we don't know how to do this when we are looking at, for example, high-dimensional regression when the dimension goes, for example, linearly with n. Another interesting question, but we don't know how to answer right now, would be, for example, to can we actually predict when doing careful cross-validation would be more useful than in the case of full-speed options? So, for example, can we estimate who and can we predict the sign of whole? And this is something that also I think. And this is something that also I think would be an interesting question to look at, but that we don't have any answer to. So, this is very quickly what I wanted to tell you about today. So, do you have any questions? Yes, go ahead. I understand this paper bounce around the cross-layed disk. Not the actual, like, if you have n sandbus, you. Have n uh samples, you could just like train with n samples. Yes, so this is this is only because we are not looking at the case where we're also choosing the parameter using cross-validation. Is that your question? I'm sorry, I have a hard time hearing, but yeah, so the question is like what would change if you would want some bonds for the actual risk if you're training with n samples? Because I'm like, if you had n samples, that's what's the motivation, right? Like, you want to use. Use the motivation, right? Like you want to use it. Yes, now we understand. Yes, so there's something that I brushed off under the carpets because there's actually two different notions of risk that you can consider. There is the notion of risk where you are looking at the expected loss on a new independently drawn observation conditional on what your estimator is, which this is a one quantity, and I think this is the question that the notion of risk that you're interested in. There's another notion of risk here, which There's another notion of risk here, which where you take the extension expectation both on the training data and on the new observation. So we consider the quasi-added risk as an estimator for both of those notions of risks. They behave very differently depending on which notion of risk you're looking at. In the talk I presented today, we were looking at the case where we were looking at the quasi-dated risk as an estimated for the full. Risk as an estimated for the full risk. So we expect where we take expectation also on the training data, which is a number. If you wanted to look at here the difference because you train your estimator only on N minus M observation rather than on the full data set, so this bias, we did not derive any formulas for the bias because that was for us a very different question. So we did not do so, no. Okay. Okay. Can you say something about the proof? We have a proof is a Stein method. So this is basically based on the dependency neighborhood method of the Stein method. So in the case where you're looking at the expected, where you're looking at the causal dated risk as an estimator of the conditional risk, the proof is much simpler. And you can, for example, use a simple martingale CLT argument and then, you know. CLT argument, and then you know, it's a bit tricky, but everything works out. If you are looking at the quasi-dated risk as an estimator, as an estimate of a full risk, there's more dependence that kicks in when you're doing this. And we couldn't make the matical method work, but the stein method works. There's a bit of trickiness to make the argument work for lever and outpost validation, but at the end of the day, it's just the stein method. And then you get non-asymptotic inequalities for the Komogoro-Smirnov distance or yeah, we look at the Vassofstein distance. Yeah, so we obtain the balance of Wassofstein distance between the K-foul quasi-dated risk and its asymptotic distribution. An interesting point also here is that I mentioned that the asymptotic variance of the K-fukasi-dated risk did not depend on the choice of the number of foods, which surprised us. Number of faults, which surprised us, but the various inbound also does not depend on the choice of the number of faults for the K-focal quasi identity risk. It obviously does for the split risk because this is a different talk. I'm sorry. No, this is a different talk. I'm lost. I'm lost. I'm lost. I'm looking at free probability right now, which is not what I'm talking about. I well, you had two sessions of another talk about free probability, but yes, because and so if you're looking at the split risk, obviously, here the number of faults matter in the very same bound because you know the size of the hold outset will matter. But for the capacity dated risk, the choice of a number of folds doesn't have any influence on the various inbounds, which I found a bit surprisingly, especially for the voidance, but the volume. Reviance, but the variance is a simple two-focus validation and fully one-out course validation was a bit surprising for me. Maybe I'm very easily surprised, that's also very possible. Any other questions? Thanks so much. Thank you. Shai, you're up next. Okay, so I have to somehow. Okay, so I have to somehow share my screen, right?