All right, we'll start again. So we have now two talks to finish today. So Anna Kiriliuk and then Thomas Epitz after that. So Anna will be talking about estimating probabilities of multi-ariate failure sets based on pairwise tail dependence coefficients. Okay. Thanks. So my talk will be more or less on multivariate extremes. It will not have a big spatial component, but I have a tiny Spatial component, but I have a tiny illustration at the end so that I hope it can still be relevant. And so, what is our goal with this project? Well, we simply wanted a general method to estimate probabilities of a certain random vector X falling into a certain extreme set C. So, it's often called a failure set. So, just some classical examples, but you can find your own favorite failure. Find your own favorite failure set, and so I put the large in parentheses because after being here all week, I decided our dimension is not large at all, but okay, so moderate. And we're going to do this based on NIAD observations and in the framework of multivariate regular variations. So that's the same as Marco already explained a bit this morning. And I'll just give the And I'll just give the basics one minute. Okay, and so to summarize how we're going to do this first, and then I'll go into each of these lines. So the reason I wrote that we want to do this for D large is because current general estimates that are applicable for all types of data and all types of failure sets, they work really well in dimension two or three. So basically, if you look at papers that cite estimating failure probabilities in the title, Estimating failure probabilities in the title. They are very advanced, asymptotically justified, possibly non-parametric methods, but they are usually not feasible as soon as you have more than buy or tri-variant data. Okay, so we would like, we wanted to extend this and instead take a really pragmatic approach that makes it fast and easy to estimate such failure probabilities. And so we all know that for general data, there's tons of multivariate extremes models that one could fit. Extremes models that one could fit. But especially if you're not in a spatial extremes problem and you don't have geographical locations, it might actually be quite difficult to find a suitable model because often they either have only one or two parameters, like maybe a logistic model, and then they're too simple if you have a 30-dimensional data set, or they have many parameters and then they can be hard to estimate simultaneously, and so it's quite hard to find a suitable parametric model. Find a suitable parametric model. And so, what is usually done after fitting several models, you've seen it in multiple talks and posters this week, is to assess the goodness of fit. Well, we can look at pairwise model-based summary measures and compare them to the pairwise empirical measures and see if they correspond more or less. And it gives us confidence in the fit-oner models. Okay, so this is often based on the tail-dependence coefficient or extremal coefficient. Coefficient or extremal coefficient, or something like that. So, there's different summary measures that exist. And the idea of our approach is to use a parametric model that matches these coefficients exactly. A simple model, but whose model-based summary dependence coefficients are exactly the ones that we empirically estimated from the data. Okay, so how are we going to do this? Well, we will use a This? Well, we will use a till dependence coefficient, which is a bit less common but which has been becoming more and more popular since it was used in Couli and Thibault. So it is a till pairwise dependence matrix, which I will introduce later. And we will propose an algorithm on how to use this matrix to build a so-called max linear model, which has exactly the same bivariate summary dependence properties. Summary dependence properties as are they. Okay, so now I will go into all of these points in bigger detail. So before I do that, one more point that's different in our approach than most of the approaches that have been presented. So many people said that they separate marginal and dependence estimation. So they first transform their marginals to exponential or Laplace scale or Or Laplace scale, or any scale, and then focus on the dependence structure. We actually did not want to do that because one of the regions we were particularly interested in is just the region that looks at the sum of weighted components of a random vector being large. And we would like to be able to interpret and model this region on the original scale. Okay, so if you've transformed our components. Transformed our components. So, if you look at the probability that X is in this region, so it's just the probability of the weighted sum of X being large. But well, once we've transformed the margins of X, this region is not tractable, we cannot interpret it on the original scale, and so we would like to keep the margins unstandardized as they are. Okay, why? Well, originally, so this is a project with Chen Zhou, which I should have said in the beginning of the talk, and he's much more. Beginning of the talk, and he's much more into finance. So he would like to model the value of Trispuri expected short haul with such an approach than this weighted sum represents aggregated stock returns, but much more in line with the workshop. We've seen several examples, even in the last talk by Jordan, who was looking at aggregated quantities, whether it's in space or in time. So one would often want to use such. Would often want to use such a region as well, maybe even for attribution studies. So, in any case, one of the regions that is not tractable after marginal transformation. Okay, and so a very short summary of what multivariate regular variation is. So, if you paid attention this morning, then you already know about it because Marco showed it as well. So, we suppose that X is regularly varied. So we suppose that x is regularly varying, which means that if we pick a certain norm and we define an angle, sorry, a radius r and an angle w, well then of course the support of w is the unit simplex and multivariate regular variation implies that if we look at the distribution of the angles given that the radius is large, as n tends to infinity, this probability will be characterized by Probability will be characterized by the so-called angular measure, so which is standardized here by its total mass, so that we do have a probability measure here on the right-hand side. Okay, so that's concerning the dependent structure. Multivariate regular variation also implies that the margins of X are heavy tills with positive till index alpha. Okay, so no negative till indices. Till indices unless we standardize the margins in this approach. Okay, so H is the angular measure, and in what follows, we let the norm be the L alpha norm. So often in extremes, we use the L1 norm because we've standardized to alpha is equal 1, and I'll keep it with general alpha. Okay, so for those that don't know regular variation, well, you can see. Well, you can see here for the L1 norm. So these are the observations that are large. And then if they tend to cluster around the axis, that means they're probably in a tail-independent case. And if the angles cluster around the middle of this simplex, then you're in a tail-dependent case. So that's our setup. Okay, so there. Okay, so there exist many parametric X-ray value models, so there exist many parametric models for this angular metric. And so now let's move to the next ingredient, say, of our algorithm. So what is the dependence measure that we're going to use to summarize bivary dependence? So we're going to look at the tail pairwise dependence matrix, which was already defined in Larson. Which was already defined in Larsen and Ressnick for alpha equals two. And indeed, if you set alpha as equal to, which will be the standardization of choice when we can here, you see that this expression simplifies. And then the tail dependence between component j and k is characterized by something that's very similar to a covariance matrix, right? So we have wjwk with respect to the angular matrix. Okay, and so this is. Okay, and so this matrix has been popularized somehow recently when in Cooley and Tibod. So it was proven that it is, for instance, positive semi-definite, and they use this also to do a kind of PCA for extremes. Okay, so just like more well-known dependence measures like CHI, we do have till dependence. Do you have tilde dependence if and only if so the sigma between xj and xk is positive? Okay, so our matrix sigma x, which I will call the TPDM, will have only positive elements in our framework. And we see that if the margins are not standardized, then the scale of each x depends on the diagonal of this matrix and on alpha and the total mass of the angular. The total mass of the angular measure corresponds to the sum of the diagonal elements of sigma. Okay, so if we do standardize the margins, then this will be one, and so the total mass of the angular measure will be D, exactly as we're used to. Okay, so why is this TPDM a convenient choice here? Well, because we're going to look at the specific model, which is sometimes more introduced as a tool. Sometimes more introduced as a toy model, but on the other hand, used quite a lot either in applications or, for instance, a known framework on directed acyclic graphs by Claudia Krupelberg and co-authors. So it is the max linear model. And we say that a vector y follows a max linear model if each one of its components can be interpreted as the maximum shock of. Maximum shock of Q heavy-tailed random variables. Okay, so it has quite a big coefficient matrix, a D times Q coefficient matrix, so many parameters to estimate with non-negative entries. And if our heavy tilt vectors are then defined as Frisia alpha random variables, a max linear vector y, or we can write it more shortly as a multiplied maximum y. A multiplied maximum y is over z, so that means each time we take one line of our parameter matrix A and we take the maximum of those coefficients times the Freschetter. Okay, so theoretically this model is useful in the sense that as the number of Frescher variables tends to infinity, it is dense in the class of multivariate extreme value distributions. Extreme value distributions. However, in practice, often, because there are so many parameters, one picks Q very small, two or three, and then the denseness is not really close by, and indeed it does not represent data very realistically. Okay, so it has also been used for clustering, for instance, and there are several ways to estimate it. So, still quite used. And so, why are we interested in this? And so, why are we interested in this model? Well, first of all, it's extremely convenient to use it to calculate failure probabilities because of its simple form and its simple angular measure, as I'll show later. Many failure regions have an explicit expression. And even if you have a very elaborate failure region that wouldn't have, well, you can very easily simulate from the model, right? So it's still an easy model to use. Okay, so to a classical failure. Okay, so to classical failure, to classical failure regions, the region where at least one variable is large, it has this well-known expression, already used quite often, and the region where we look at the sum of components, it also has an explicit failure problem. Okay, and so that's not the only reason. There are more models that have simple expressions for such probabilities. The main reason is it's very nice. The main reason is its very nice connection with the tail pairwise dependence matrix. So, if we look at its angular measure, it just puts point masses on the Q points that are represented each time by the vectors of the parameter matrix A divided by their norm. And while we can use this expression to simply calculate the TPM, that's what Pooley and Thibault did for alpha E plus. For alpha e plus 2, so expression slightly simpler, even, and to show that the elements of the TPDM are just given by the following expression, which means that if we define a matrix, say A star, which is just the coefficient matrix A to the power of alpha over 2 to correct for our tail index, then we can decompose the T P D M of a MaxLinear model into Linear model into those matrix A times itself transpose. Okay, so what does this mean for our goal? So the question would be, well, can we construct, given TPDM from our data, can we construct a max linear model which has the same TPDM and then use this to estimate failure probabilities? Well, again, we didn't need to do anything here because. We didn't need to do anything here because, like I said, for Q goes to infinity, this has been known for a long time. But Gouden Diego also showed that if we're interested in not reproducing the entire multivariate extreme value distribution, but just the TPDM, then a finite Q suffices to construct a MaxLinear model which can exactly match any TPDM that you need. Any TPDM that you. So that means that for a given data set, we could estimate the TPDM and construct such a Max Linear model if we could get A from sigma, right? So how do we get A from sigma? Well, it's what's called a completely positive decomposition. Okay, so we say that sigma is completely positive if we can write it as A times A transposed, where the matrix A has non-negative entries. That's what we need because the coefficients. What we need because the coefficients of the max linear model are non-negative. And so, this is a well-known algorithm within the linear algebra literature. They even have an upper bound for the number of columns such a matrix A needs to have. So Q was the number of columns of the matrix, which you can see is quite large. And the problem, especially, is that it's a very complex algorithm. Okay, so it takes a very long time to get such a decomposition. To get such a decomposition is far from obvious, and it's definitely not the practical solution that we were looking for. And so the goal that we had in mind was to propose an easier decomposition. Of course, if we propose an easier decomposition, we will probably lose something. Otherwise, it would have had already been used before. Used before, and that means that we're going to look for approximate decompositions, which I'll say on the next slide what I mean by approximate. But let's compare this first to another well-known decomposition because our algorithm will be very close somehow. So we know that if sigma is symmetric and positive semi-definite, which is the case for the TPDM, well, then the Kolesky decomposition will give you a D times D. A d times d lower triangular matrix, but that matrix may contain negative elements, right? And we can't have that. So, our algorithm will also give a D times D lower triangular matrix with only positive elements. And so, what is the price we pay? So, the algorithm does not guarantee that the diagonal of sigma is correctly recovered. So, while the pairwise dependence properties are correctly recovered, Properties are correctly recovered, the variance might not be. We'll see in practice that for moderate dimension, the diagonal will be matched, fortunately, and for large dimensions, well, we might overestimate the variance of the variable. So we might overestimate the diagonal. Okay, and then the next point: the decomposition is not unique. Well, that's the case in our approach, too, of course, and we actually see. Approach too, of course. And we actually see this as a very big advantage because that means that, given that we can quickly generate these approximate decompositions, we can generate loads of them. We can generate hundreds of them or thousands of them. And that will allow us to give a spread for the failure probabilities that we estimate. So then we can characterize the model uncertainty that corresponds to our estimates. Okay, so I said for the rest of the presentation, consider alpha as two. So to simplify notation, but I think I will skip some technical details anyway. And we also need to keep in mind that the tail index and the tail pairwise dependence matrix need to be estimated first in practice. So even if we can generate many decompositions, well, there's still multiple sources of uncertainty that will depend. Uncertainty that will depend on the quality of your initial estimates. Okay, so I think I will just show maybe one or two of the slides to get a little feel for the algorithm and what the criterion is, and then I'll show the application. So to get an idea of how we define the algorithm, we can think the other way around. So we're looking to decompose sigma into A times A transpose. Into A times A transposed. Well, we know that A is lower diagonal. So let's write it already as its top element A11, then a vector without this element and a D minus one dimensional lower triangular matrix. So without the first row and the first column. Okay, because then if we multiply it with its transpose and equate it to sigma, well, we can already see how to pick the element. How to pick the elements of a so that they match. Okay, so we see that a one one squared has to be equal to sigma one one. Okay, so there's that. Then we know that the first column of a minus its first element, well, it has to be the first column of sigma divided by this a11, which we just defined to be the root of sigma 11. So there we have that. And then the following layer say. Layer say can be defined recursively because here we have a completely positive decomposition plus another term. So this has to equal to the matrix sigma minus its first row, minus its first column, minus this other term. Okay, so this is the only technical slide I wanted to show to illustrate why our decomposition is only approximate because well. The approximate because well, can we just do this? The idea is that our TPDM remains positive, so maybe when we subtract this, actually, we will no longer have something positive. So this simple approach will only work as long as all the elements here remain positive. So that means that for each, say, row and column that's not the first one, the elements of sigma minus the elements of A have to be positive. Be possible. Okay, so this is the correction we're gonna do. So, like I said, I will not go through. Oh, yeah, it's still show. So, this, if they're all positive, well, we can just rewrite it and then I can put it to the other side. So, this is my criterion for the algorithm to work. Okay, but the advantage is if it doesn't work, so if there's at least one that is bigger than one, so will the Bigger than one, so we'll define this criterion v1, which is the biggest of them. And so, if there's at least one that's bigger than one, well, then we have to make a small direction here. And then instead of taking the square root of sigma one one, we will multiply with the largest one of those. And that means indeed that our diagonal is no longer matched. Okay, but we can so we set this up recursively and then we prove that a matrix each time. That a matrix, each time when we remove a dimension, remove a layer, it will still remain a valid TPDM in the sense that its elements are all positive and it's still positive, semi-definite, etc. Okay, so here I showed, say, the first layer where I started with the first column and the first line. There's no reason we have to do that. We could first start with the third column and the third line, and we will actually consider different orders of reducing that. Of reducing dimension and filling up the matrix A, right? Because here we found how to define the first column of this matrix A that we're looking for. Because depending on with which row and column we start, we might actually find an exact decomposition in one case and a non-exact in another. So here I define this criterion. So it is entirely possible. And here I started with the first row and column. With the first row and column, that this maximum is bigger than one, but if I would have started with third, it could be smaller. Okay, so what we do in practice, so I'll skip that, what we do in practice to ensure to find an exact decomposition, not an approximate one, is basically we check all types of paths. Okay, so we can take a simple approach where each time we define this di where i represents This di where i represents any column or row, we calculate it and we continue with the lowest one. Okay, then we can do a more elaborate approach where we really build a tree of possibilities. We calculate, so this criterion for all lines or columns and all that that are below one, we stop, we continue with the next, etc. Well, this works very well, but it's not feasible in a high dimension. Or in practice, we take a pragmatic approach, we just start with the We just start with the tree, but we just continue with, say, one branch to find the next possibilities. Okay, and so to tell you what I considered as high, but what I now consider as low, if we have a moderate dimension, say until 20, then we can really obtain thousands of exact decompositions in half an hour. So, if you only want approximate decompositions, that's instantaneous. So, that's no worry here. I'm only looking. That's no worry here. I'm only looking at the exact. So, if you go up, you can still do so, but it might take longer, say, a day if you want a couple of hundreds of them. And starting from D more as 50, then I'm having trouble finding exact ones. So, then it really depends on your failure region, how much the algorithm is of interest. Because maybe for your failure region, it's okay that the variance is overestimated because it's focusing more on the dependence properties, or in other cases, it's. Or in other cases, it might be a very bad idea. So, this really depends on your problem. Okay, so to end with a small illustration, not shooting an application. So, if we look at 35 weather stations in the Netherlands and we take daily maximum wind gusts, so I saw in another talk yesterday that they were asymptotically independent, but for today, but actually, I found mine were asymptotically. But actually, I found mine were asymptotically dependent. So I hope they'll still fit in my framework. Maybe it's because I have gusts or another season. Anyway, so these show just to see because I wanted to check if we need to make a distinction between coastal and inland stations because that's what's often done. So I looked at the TPDM pairwise coefficients of land and coastal stations as a function of distance. As a function of distance. So, of course, they are decreasing. But they seem pretty similar, and the same for the marginal analysis. If we estimate the tail index, it seems to fall more or less in the same confidence band. So, of course, if we're in multivariate regular variation and we don't standardize, then we do have the same tail index for all components. So, the assumptions seem reasonable for this data. And then, so, what we can do? Well, we have So, what we can do? Well, we have this 35-dimensional vector of maximum wind gust, and suppose we want to estimate the probability that at least at one place the wind gust is extreme. Well, so then we'll use this max linear vector y to approximate the probability for a data vector x, right? Since this is what I've been motivating with the decomposition. And what do I take? And what do I take as a threshold? Well, I looked at some practically useful thresholds, okay, because there will be a weather alarm that will be issued when wind gusts exceed 100 or 120 kilometers an hour, code orange or red. And a third threshold that I put in was the record done in February 22 storm, which was actually the hardest inland wind ever measured in the Netherlands. Okay, so here are Okay, so here I have a 35-dimensional data set. I obtained 200 exact decompositions to characterize some model uncertainty. That's no problem to do numerically. And then indeed, I can now have several 200 estimates of each one of those failure probabilities, so which are shown in the box plots. And then the red points are the empirical probabilities based on our data. So, of course, for So, of course, for not too high a threshold, the empirical one does fall within our model-based range. But here, I think there's only one or two observations that are above the threshold. I think one. So, it makes sense that we have no longer, we have more and more estimations. Okay, and so what was interesting to see, so these box files are for the full data set, but I did also an analysis with coastal and inland separately. With coastal and inland separately. And even though the bivariate properties looked quite similar on the marginal properties as well, the full model actually showed that coastal estimates are twice as high as inland estimates, which makes sense, right, at the coastal level. The wind gusts are higher than in inland. Okay, so that's good. So to conclude, so it's a Conclude, so it's a general approach, say, to estimate failure probabilities. Of course, I mean, with my data illustration is not realistic in the sense that it could have taken into account spatial information or non-stationarities. The goal was really to make a general approach, not necessarily for spatial data. And the repeated decompositions really helped to get a feel of the spread of your estimates. Okay, and so in general, the algorithm you can say that it's exact until dimension 40-50, and afterwards it's totally feasible and fast, but no longer exact. Okay, then I'll stop here. Thank you.