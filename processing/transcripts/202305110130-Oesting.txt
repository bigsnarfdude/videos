We will be talking about a very related topic, extremes and biodimensions, statistical theories and algorithms. Yeah, thanks a lot, Kafa, for the introduction, and also thank you and the organizers for the invitation and for giving me the opportunity to present my work at this very nice workshop. And also, thank you very much, Sebastian, for this very nice introduction. As Rafael already mentioned, my talk will be very much related. And as also this graphi suggests, I will also talk about something at least related. Talk about something at least related to graphical models. In particular, I would like to talk about the same types of models Sebastian talked about, Fuster-Reiss models. However, our focus, and here our means the focus of my co-author, Johannes Lijer from Bochum and me, is a bit different from what Sebastian talked about in graphical modeling. So we are not too much interested in the graphical structure, but rather the parameters. Structure, but rather the parameters themselves. So, let me give a short motivation and some introduction to the setting we consider. We are interested in modeling extremes of some potentially high-dimensional random vector X. As an example, just think of this map here of Germany with the weather stations that are That are used by the German weather service, BWD. In this case, if we wouldn't use all of them, which we will not do, but if we would use all of them, there would be 1,200 of them. So we have quite a high-dimensional problem, even in the data we consider. And the aim is we want to model extremes in this graph, not only component-wise extremes, but in particular joint extremes, which means Joint extremes, which means we really want to get the dependent structure between the different components right. And what you would do, then in the Gaussian world, you would maybe consider covariances between all pairs of locations. And if you do this without imposing any additional structure, you would have a large number of parameters of the order of d squared. So here for 1200 stations, roughly 700. Stations roughly 700,000 parameters. So, this is obviously quite an unrealistic setting to estimate 700,000 parameters at once in a chromometric setting. So, typically people do some workarounds, and that's what you are all familiar with, I guess. In spatial statistics, we try to borrow strengths from our knowledge about the spatial structure of the data at hand. So, typically, we assume something like We assume something like spatial stationarity, then use some kind of classical covariance function. So, in Doug's talk, we have seen the big three as parameters. Then maybe some additional parameters, a smoothness parameter, or some more parameters. Maybe you assume some specific type of non-stationarity, including some covariates. You get a few more parameters, but typically in these models, we have maybe at most 10 different parameters. At most 10 different parameters by these assumptions. So, this is at first very nice. However, of course, it comes with some restrictions. You impose some assumptions which might or might not be true and your statistical inference might or might not be robust to these assumptions. That does not mean that I want to argue that we shouldn't do this, but at least we have to be aware of some restrictions and in particular. And in particular, some of the advantages you're hoping for might not be true in this type of model. In particular, one idea you usually have in mind when using only few parameters is you want to get something which is computationally very efficient. And this is sometimes a problem, at least in my own experience when working with, let's say, Neussler-Weis models. Let's say Neusler-Ice models, Brown-Wrestling processes, extremity processes, you get typically some likelihoods that are not that easily detractable and do not have nice mathematical properties. Likelihoods are typically very flat. They are definitely not convex or something. And this typically means that whenever you do optimization, you have to be very, very careful. Have to be very, very careful to get reasonable solutions. Typically, optimization procedures are very much sensitive to your starting values, for instance. So, even if you get a result, you have to be very careful and do lots of checks to be at least confident. You never have a guarantee, but to be at least confident that you get some reasonable results. And therefore, Johannes and myself were asking ourselves, well, how far can we go? Well, how far can we go when trying to avoid these assumptions because they do not give us all those advantages we would like to, but rather try to go with the full model, try to estimate those parameters. And that's what I will be talking about in the next minutes. So first I will give you some brief overview. Already Sebastian talked about it, so I can be very brief on that about the setting multiple regular variation and the Husserl Reif models. Then I would like to present the Then I would like to present the method we used, score matching, and finally show you some numerical results on that. So, what is the setting we are dealing with? Sebastian just showed it in a slightly different way, but essentially these assumptions, as Sebastian talked about, are also called the algebra of regular variation. And what does it mean? It means if you consider some. That means if you consider some norm of that vector, condition on that norm being large, and standardize your norm by a vector u, that's what you know from multivariate extreme value theory. We've seen this, for instance, in some other talks at the beginning of the week, that this should, as u tends to infinity, this should behave like a Puritan random variable. And multi-tirgular variation means that if you do not only consider this norm, this radial part, but also the angular part and the pole. But also the angular part and a polar decomposition, then this joint distribution of the radial part standardized by you and the angular part behaves like independent random objects, namely a Pareto random variable and some random vector living on the unit sphere with respect to the norm we are considering. So that's what we want to assume from now on. And what is typically used as the result What is typically used as the result in the way Sebastian formulated it, namely, we take the product of these two, which means we take the vector divided by u, condition on the norm being large, and then this converges in distribution to the law of the product of this Pareto vector r and this random vector w. And here for simplicity, we want to focus only on the dependent structure, which means we ignore the fact that r could be. Ignore the fact that R could be any Puritan distribution, just set alpha to one. And we will consider a specific parametric model for W. As Sebastian already mentioned, a popular choice is to use something which behaves like a log-Gaussian process in some transformed way. So, here I'm working with slightly different scale than Sebastian. That's why I have Log-Gaussian processes, not Gaussian processes. Gaussian processes. And this gives these assumptions, this type of model is then the Hussler-Reis model. So let's have a closer look what this Hussler-Reis model looks like. And Sebastian already talked a lot about this matrix theta, which we will use to parametrize the Hussler-Reis model. In particular, we get this density here. And if you look at the density, at first sight, it looks very much At first sight, it looks very much like a log down so density. You have this exponential path here. You have a quadratic term in log x with this precision matrix theta. You have a linear term with this, what would be the mean vector in the log Daussian world. You have these terms here, which are essentially the derivatives of the log transformation. And all the rest can be just summarized in some kind of normalizing constant we have here. Normalizing constant we have here. So this looks very much standard and is very well known in the Gaussian world. So we could try just to use whatever we know from the Gaussian world. However, there's one problem, Sebastian already mentioned. There is some reason why I put this word precision in inverted commas, namely that it's not a regular matrix, but as Sebastian already mentioned, But as Sebastian already mentioned, the rows and the sums of the symmetric matrix are summing up to zero, which means you don't have full rank, but only rank D minus one. This is not that good. However, there's a very nice property also mentioned by Sebastian, namely that the zeros in this kind of precision matrix have the same or an analogous meaning to the Gaussian world, namely conditional independence now in the settings. Independence now in the setting Sebastian just introduced, which means sparsity of this matrix, as already explained, is a desirable property. However, we have this problem here. We have a non-regular matrix, and this means differently from standard Gaussian theory, we cannot just choose any positive definite matrix, and we cannot choose just any mean value. Any mean vector mu, but there is some relation between mu and theta. And to make this thing integrable, we need a specific choice of mu, which is essentially a function of theta. And this also means that the normalizing constant looks different than in the Gaussian world. It's not just the determinant of the precision matrix. This could just be zero, but something more complicated. But something more complicated. We needed essentially just the integral of this density over this L-space Sebastian introduced. Which means we have at least two problems when doing statistical inference. First, ignoring this, but just working with the consequences. Namely, first thing is we cannot just estimate any mu, but we have to be very careful what we choose. And this normalizing constant is Constant is hardly tractable at all. So we were thinking about a way to get rid of these two problems essentially and to make statistical inference work. And what we did is we decided to work with score matching approaches. So the idea is not to work with likelihoods, but with scores of this model, which means we first calculate the log likelihood. I guess I Likelihood. I guess I don't have to say much about this. This is very much standard. But something that is, of course, always important when you work with log likelihoods, that this normalizing constant just becomes an additive constant, which means once you calculate the derivative with respect to x, you get rid of this normalizing constant. So once we work with the score, so the gradient of the lot likelihood, we do not. We do not have this normalizing constant anymore. But we have this nice term which is also linear in theta. But there's one problem still left, namely the problem that we have this mean vector mu, which is some rather complicated and particular nonlinear function of theta. And if we just try to work with the scope function, To work with the scope function and try somehow to optimize it with respect to theta, this would be quite a difficult problem. So Johannes and me thought a bit about how to get rid of this and we chose a very easy option, namely we just ignore that mu has to depend on theta. We just take a flexible mu. So of course, I mean, I can just write it down, but I mean, I can just write it down, but is it really reasonable to do so? There's, of course, a problem with it. If you do this, you do not necessarily get a valid density anymore for arbitrary mu. So we would be estimating something which is not a valid model, maybe. However, at first sight, at first, let's just ignore this problem and see how far we can get if we just work with arbitrary mu. If we just work with arbitrary mu. So we did this and we decided then to do score matching, which means so that's something that has become quite popular in the last almost 20 years. And for instance, also Anthony and Raphael de Fontville have used this in the context of extremes in a slightly different setting. And the idea is the following. You take the model score with parameters mu and theta and compare it to the score. And compare it to the score of the unknown data generating density that I just denote by p of x. You compare them by taking the distance, for instance, that's computationally the most convenient one, the squared Eutelian distance here of the two gradients. You do this for some random for some random observations, and you take the expectation of this square distance. Square distance, and this is, of course, something that you could do, but you have to be a bit careful because it's not guaranteed that this expression is finite. So, what you typically do is you include some weights. Here we do it just by component wise multiplication with the different components of the gradient, and under appropriate assumptions on your Assumptions on your model and on the weight function w or the weight vector, this thing exists. It's just a quadratic form. And what you could do now is you could try to minimize it, which means you take, instead of the expectation here of this expression, which we denote by O for objective function, you plug in your data into your function O and you take the arithmetic mean instead of. Take the arithmetic mean instead of the expectation and you minimize this. So you do some empirical risk minimization, just a standard procedure in principle, and your solution, your arc min mu hat and theta hat then just is the score matching. So that's already good, but there's something I just talked about a few minutes before that I just ignored so far. I just mentioned it also. I just mentioned, and also Sebastian put a lot of emphasis on this, that a sparse matrix theta is something that is desirable. And of course, if you do this minimization, you will get an object which is typically not sparse at all. So you do some regularization, you add some penalty term here. Typically, if you want to get a sparse solution, you take the L1 norm, just Solution: You take the L1 norm just in the Lasoux estimators, and you have some tuning parameter here that controls the degree of sparsity in your solution. So, you get a regularized score matching estimate. And how does optimization look like? So, what you have to do is you have to minimize this regularized objective function such that some constraints are set. Some constraints are satisfied. One is that the matrix theta should be symmetric. The other one is that the rows and sums should sum up to zero. And what can we say about this? Well, one thing is these constraints can be automatically guaranteed by some appropriate reparametrization of theta in particular. I don't want to talk about this too much because. To talk about this too much because it just gets a bit technical, but essentially, instead of working with theta, you just work with the triangular matrix and then use this to get the original theta. You have lots of data-dependent terms in this objective function, which looks like something very complicated to calculate. However, you can just summarize everything via Everything via somehow appropriate definitions of matrices, do this just in one pre-processing step, and then optimization can just be done with linear terms because if you have additional some appropriate assumptions on your weight function, you get a very simple expression that you can optimize. In particular, something that when I looked at this for the first time. When I looked at this for the first time, I did not expect, namely, that you just have to minimize something that does no longer depend on the unknown data generating density. Because if you don't know this and it's part of your optimization problem, this looks a bit weird, but you can get rid of this. With me, somehow you can write down this optimization problem in a nice form, which is already good. And even more important, And even more important, you can show that this optimization problem is a convex optimization problem in μ and theta, which means you can make use of all the results from convex optimization. And in particular, you can implement efficient algorithms, for instance, using coordinate-wise descent, and get some optimization procedure that really guarantees you that you converge to the true minimizer of the To the true minimizer of this function. So you don't have these problems I mentioned at the beginning, like sensitivity with respect to starting values and inefficiency and stuff like that. So this is about what gives us hope for the practical implementation, just some theoretical results. Namely, if you just consider the non-regularized estimator, so you set this tuning parameter up. So, you set this tuning parameter R to zero, then you can show that there's strong consistency. In particular, your estimator for theta converges to the true theta, which means, in fact, even though you started with a potentially non-valid estimate because you got a wrong mu, which leads to non-integrable function, what you can just do, you can take your estimate for You can take your estimate for theta, calculate the μ corresponding to the theta, and you end up with a valid Chusler-Reis model. And you converge to the right one if your data are generated from the Kusler-Reis model. So this means, even though we did something that seems to be really, let's say it looks really weird to work with non-valid densities, potentially at least. In your final result, In your final result, it doesn't matter anymore. And then there's one result. I think most of you are not very familiar with high-dimensional statistics, so I will just give you some short look at this, but you can get some oracle inequality provided that your regular realization parameter is sufficiently large to dominate the noise in the model. You can get just some upper bound for the difference between. Just some upper bound for the difference between the estimated parameters and the true parameters, depending on the tuning parameter, depending on the data x you have, and also depending on the sparsity of the true parameters. I don't want to talk too much about this, but rather now just give you some results from numerical experiments. So, does this work in practice? Practice. And we started with the following setting. We simulated a Fusser-Reiss model with a specific type of precision matrix theta, which has just these roughly 3D non-zero entries close to the diagonal, which at first looks like a very special choice, but it's rather natural because it's very closely related to the Hüster. Very closely related to the Husserl Reis model you get once the Gaussian underlying Gaussian process you start with is a Brownian motion. And if you do this, you get such a matrix and you can consider this matrix in different dimensions and solve the optimization problem with different choices of the tuning parameter R. That's what we did. And I here start with the results for dimension 20, which means you already have 190 parameters. Already have 190 parameters, so quite a lot. And as you can see, here are the different sample sizes. The pre-calculations, somehow transforming all the data into matrices, are pretty fast. And then the optimization procedure, not only for one tuning parameter, but for a whole bunch of tuning parameters, I think six or seven, takes essentially one second and is not. Second, and is not affected by the sample size too much. So, in fact, it even gets, that's what we will also observe in higher dimensions, gets a bit faster the more observations you have, because the score behaves in a nicer way somehow. Depending on the choice of the tuning parameter, you get different degrees of sparsity, you can get some very sparse solutions, and it's very difficult to interpret this root means. To interpret this root mean squared error here of the results, but at least what you can see is that it roughly decays with the rate you would expect, namely one over square root n. So here they are better by a factor of roughly 10. In fact, it's maybe 20 if you multiply or if you have a sample size which is 100 times as large. And now let's go to higher dimensions, so dimension 80, which means you have more than. 80, which means you have more than 3,000 different parameters. You can see that, okay, now the pre-calculations also take some time, but it's still okay. And the optimization takes between five and ten minutes. So given that it's 3,000 parameters, I think that's still quite good. And it has been done just on a standard laptop, so not on any clusters. And you get similar results. So the root mean squared errors scales with mean squared errors, Gaels with the sample size as expected, you can get non-sparse up to very sparse solutions, so close to what we expected here. Just to summarize, I hope that Sebastian and me both convinced you that these Huiesler-Reiss models or more general multi-world Parita models are interesting in the context of multi-world extremes. However, as I mentioned, if you do this in a very As I mentioned, if you do this in a very flexible way, you have a large number of parameters. You are in a really high-dimensional setting very easily. By the way, I should have mentioned, so here we have 3,000 parameters and 500 observations, so it's really high-dimensional. But as we have seen that if we do some reparameterization and use the score matching approach, we get something that is computationally efficient. We even have some theoretical. We even have some theoretical results. So, this parameter should work asymptotically, this algorithm should work asymptotically. And in the simulation study, we have seen that also in this high-dimensional setting, we can do very fast and efficient inference. So finally, here are some references. I'm looking forward to your questions. And thank you very much for your attention. Thank you. So we have time for questions.