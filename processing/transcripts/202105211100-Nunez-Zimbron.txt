Well, thank you. Yes, thank you. Thank you very much. I'd like to begin by thanking the organizers for this wonderful workshop. It's been a real blast. And also, thanks very much for the very kind invitation. So today I want to speak about joint work with Widow de Fipis, who is in de Filipis, who is in CISA in Pieste, about the behavior of harmonic functions in a certain class of metric measure spaces, which by now are known as RCD spaces. I will clarify why in a few slides. But the idea is that they admit a notion of reaching curvature bounded below and dimension bounded above in some synthetic sense. And in this way, they And in this way, they generalize Riemannian manifolds with reaching curvature bounded below. And even though these phases are not as regular as manifolds, of course, one can make sense of reaching curvature lower bounds and work with only these to recover many of the typical results one has in remaining genetics. But there are also some different things, of course, and Things first, and one of the differences comes with harmonic functions, and this is what I'm going to discuss today. And this has actually several applications. So, let me start by two maybe well-known theorems in geometric analysis, and then two at first unrelated questions about these two statements. About these two statements, which maybe are a bit naive, but we'll see. So, let's grab a complete remaining manifold, n-dimensional remaining manifold, which has rich curvature bounded below by some by some k. Then first, it's well known that, let's say, weekly harmonic functions are leap sheets, are locally leaped. And more precisely, what I mean by this is that there's a constant which depends on the local. Which depends on the lower bound of the reach of curvature and the dimension, such that if we take the supremum on the ball of radius r over 2 for some r of the square of the gradient of u, then you can bound this by c over the volume of the ball of radius double that radius r by the yield norm of the gradient of yield, right? So, with this, you can, of course, bound the gradient. You can, of course, bound the gradient of u and obtain the local Lipschitz rate for all harmonic functions on the book. And then let me state a completely unrelated statement here. There's also a known and interesting inequality by Calderon and Sigmund, which says that there's always a constant which depends only on the lower reach of curvature bounds, such that you can. Curvature bounds such that you can control the L to norm of the Hessian of U by C times the L to norm of the Lafession of U plus the L to norm of the gradient U for any compact list for the smooth function U. The thing that these two results have in common maybe is that the proof follows from Bogner's identity, which is a well-known identity in geometry, which I will recall in a few slides. In a few slides. But maybe other than that, there's no a priori relation between these two results. And maybe I can ask a couple of very naive questions. So is it possible to improve these two statements in some way? So for example, given that leaveshitz regularity is not too far from C1, is it possible to give quantitative estimate? Possible to give quantitative estimates on the modulus of continuity of the gradients of harmonic functions and in such a way that this estimate only depends on a lower bound on the reach curvature and the dimension, and maybe even an upper bound of the dimension. And for the second statement, is it possible to obtain a similar statement, but for a p different than two, for the L P norm different than two? So maybe if So, maybe if one takes a direct approach at this, one starts to realize that this is kind of hard to prove, and one starts suspecting that the answer is no to these questions. So, maybe if one suspects that the answer is no, one could try to follow a different approach to prove these two statements. So, let's recall a theorem here that actually Raquel mentioned. That actually Raquel mentioned yesterday, a very important and deep theorem by Gromov is the Gromov's pre-compactness theorem, which says that if I consider the class of Riemannian manifolds, n-dimensional Riemannian manifolds, which have reached a curvature bounded below by some fixed K and diameter bounded above by some fixed D. Same modulo isometries. Then this plastic has compact closure in the ground. Class has compact closure in the ground of housework. But I guess the not very comfortable part about this is that the limits may not be manifold. They may not be smooth. Maybe it may have topological singularities or metric singularities. But maybe since the limits are so weird, let's say that life is good and everything works out and we can find a limit space where there is some Uh, where there is some at least some harmonic function which is not C1. Then there's a known result from Ambrosio and Honda, which says that harmonic functions are preserved under Gromov-Hausorf convergence in this class of spaces, MNKD. So if this is the case, and if we assume, say, by contradiction, for example, in the first question that I gave about the model of continuity of gradients of our. model of subcontinuity of gradients of harmonic functions. Say that we assume by contradiction that it works, right? We can find such an estimate. Then this estimate would be preserved under Gromov House of convergence and would pass to the limit if one can make sense of the objects involved in the statement in some generalized sense. And this would give a contradiction, right? Because there are functions, harmonic functions, which are not T1 in. So, this is maybe the rough idea of a way to try to prove these two statements that they do not hold. So, for this, let me try to clarify what I mean by these more general limit spaces in which one can make sense of all the objects involving these two statements. And I'm basically gonna spend the bulk of the talk speaking about these. Of the talk speaking about these spaces, and the class of spaces is known as RCD spaces. So, this will be metric measure spaces in which I can make sense of this argument precisely. So, let me first try to convince you that one can make sense of reaching curvature lower bounds in an abstract setting. So, first, let's recall the Wagner's formula. So, during this talk, from now on, I will just for simplify. From now on, I will just for simplicity assume that all manifolds are closed, complete, and everything is super nice, right? So let's grab our remaining manifold and some smooth function. Then it holds, then this formula holds. So the one half of the Laplacian of the square of the norm of the gradient of u. This is exactly the same as the Hilbert Schmidt norm squared of the Hessian of U plus the remaining metric computed in gradient U, gradient Laplacian of U plus. U gradient Laplacian of U plus the Ricci curvature in the direction of the gradient. And from this identity, you can obtain a say a priori weaker inequality, which is called Botner's inequality, by using Cauchy-Schwarz here in the Hilbert-Schmidt norm of the Hessian of U. And if you assume a lower reaching curvature bound, then you can change this term by this, right? And change this rich curvature term by k. And you obtain this so-called box. And you obtain this so-called Wagner's inequality. And what is, I think, very surprising, at least for me, is that this inequality actually characterizes lower rich curvature. In the sense that if you have a manifold and any number k, real number k, then it's equivalent to have a richie lower curvature bound by k than to enforce that the Bochner's inequality holds for any smooth function. Function. So, if one wants to speak about reaching curvature bounds in an abstract setting, perhaps it is easier to define what Buckner's inequality means in an abstract setting than to define what Ricci curvature means. Because for defining Ricci curvature, you need basically a smooth structure, a manifold structure, and you need to define the Riemann tensor, which already involves taking several derivatives, and this may not. And this may not be available for Gromov-Haushart limits of manifolds, for example. But let me now try to convince you that even though Boekner's inequality involves several objects that look like derivatives, one can make sense of this inequality in an abstract. So, for that, let me give a bit of an introduction to calculus and metric measure spaces with the objective of showing that maybe we That maybe we cannot define, for example, the gradient of a function in some way, but we can actually define what it means to have the norm of the gradient of some suitably good function in an abstract setting. And we can also make sense of a laplacian in some generalized setting. And we actually do not need to have a Riemannian metric for this object to make sense. We can define this object in an abstract way, we will. In an abstract way, we will see. So let's start with a metric measure space, XDM. And I'm going to require some minimal conditions for everything to work. We want to make the space complete and separable. This is fairly standard in all the theory. And we want this reference measure to be a Borel measure, so that it's compatible with the metric structure, and so that it gives finite mass to bounded steps. So, now how do we define this object, the norm of the gradient of F? For that, let me remind you of a variational characterization of this object in the case of Rn. So a classical proposition says that if you have a C1 function on Rn, and say you have any other continuous non-negative function G, then it's the same to say that G majorizes. Say that G majorizes the norm of the gradient of F, and then to say that this inequality holds here with G plugged in here, right? Here, this has to hold for all T1 curves in Rn, right? The idea being that if you have this inequality, then you can recover the norm of the gradient of F by taking the minimal G that satisfies this inequality, sorry. This inequality, sorry. But now, actually, you can make sense of this term here in a very general way. So if you take a continuous curve in a metric measure space, we will say that this curve is absolutely continuous. If there is some integrable function in 0, 1 such that this inequality holds here, I'm putting gamma sub t here to the node gamma at t, evaluated at t. Gamma at t, evaluated at t. You can actually see that this inequality here is sort of like a curved version of this characterization, right? And for absolutely continuous curves, it turns out that this infinitesimal limit exists, right? And you can denote this as sort of like a norm of the derivative of the curve, which is called in this abstract setting the metric of the gamma, the curve gamma at t. At p. So now we can make sense of this object here, and then since we can make sense of this object, we can make sense of this inequality for any metric measures based with the conditions we gave. So if one aims to define this subject, the norm of the gradient of some function, one can instead maybe make sense of this inequality and in some sense take the minimum of g that satisfies this condition. This condition, and this is what Heinonen and Costkella actually did. And they said that a Borel function g, non-negative Borel function g, is an upper gradient of f if for all absolutely continuous functions, this inequality that we said before holds true. But now let me show you that this definition is somewhat too restrictive. It's too sensitive to small changes in the object symbol. Let me show a simple Let me show a simple example of why this is not maybe the definition one needs. And the problem here is that there may not be minimal gradients. So let's set the space x to be r with the usual distance and the Lebesgue measure. And let's take f to be the function which is one on zero and zero out of one. And for each epsilon bigger than zero, let's take a look at the equation. Bigger than zero, let's take an upper gradient of this function f to be one over the absolute value of x, if the absolute value of x is less than epsilon, and zero otherwise. So when you plug this function in this inequality here, you can see in the right-hand side, maybe let's just disregard this term for a little bit. You can see this is infinity, right? And on the other hand, this other side is one. So this inequality is going to hold. So, this inequality is going to hold trivial. But now, and this means that g epsilon is an upper gradient of f. But now, if you want to take sort of the minimal g that that satisfies this, and you take the limit when epsilon goes to zero, you see that this is actually the zero function, which is not an upper gradient, right? Because this side is one and this side is you. So, this is this is a little bit bad because we wanted to recover the norm of the gradient of f and we. Norm of the gradient of f and we can't really take a minimal upper gradient here. So we need to change the definition. So now I'm going to recall a definition by Gigli. And let me, before I even say the definition, let me say that this is not the first definition of this type. There were prior definitions by Chiger and Shannon Gallingham, which relaxes a little bit the upper grading condition, but all of them are. Condition, but all of them are a priori, sorry, a posteriori equivalent. So I'm gonna go with this because it's maybe the easier one to explain in a talk. And let me first reinforce a little bit the regularity of the G. Let's require that g is cell 2, and it's going to be clear in a second why we require that g is del 2. And we're going to say that a g, such a g is a weak corporate gradient of x if curp gradient of x if the inequality that we said before holds, but it only holds for almost every absolutely continuous curve. And you may be now wondering what do I mean by almost every absolutely continuous curve, right? So what I mean is that I have an integrated version of this inequality when I test against probability measures in the space of continuous curves, which are called test plans. Which are called test plans. These test plans basically are probability measures which are concentrated on absolutely continuous curves, which is something that we are already obligated to do because we need to make sense of this term here. But they also have a measure condition, which says that these curves in which the plan is concentrated cannot approach each other too much. They cannot concentrate, measure too much. And I don't really want to go into details, but this is certainly enough to make sense to properly test this weak upper grading condition. And if we're doing things correctly, then this is the definition that's going to work. Let me define the so-called Sobolef class, S2, which is the class of functions which have at least some weak upper brain in it, right? Then if you take some Then, if you take some stubble F class function f and you look at all of the set of the weak upper gradients of this function in L2, and this is why we require L2 here, then you can see this is a closed and convex set. And now L2 is a Hilbert space, so this means that we can always find some element of this set which has minimal L, right? Which is something that we really couldn't do with the definition. That we really couldn't do with the definition of Heinonina and Coskell. So now we can recover this notion of the norm of the gradient of F by just saying that this will be the minimal weak copper gradient of F in this sense. And if we are doing things correctly, then this has to be compatible in some way with the Rn case. And it is in the sense that if we have some F in the sub-left class of Rn and In the sub-left class of Rn and intersect with L2, then actually you can prove that this is a sub-left one, two function in Rn. And this minimal weak upper gradient that we defined here actually coincides with the norm of the distributional derivative in the usual sense. So this is a good notion for this. So now, whenever we need to speak about the norm of the gradient of some function, we can speak about sublife one-two functions in the sense that one two functions in the in the sense that we take functions in the sovolev plus intersect with l2 which say the usual uh sobolev one two norm and they will have a properly defined minimal weak of okay uh okay so given that how do we uh start defining the other objects that appear in wagner's inequality say for example how do we define this inner product of gradient f gradient g right uh so the very natural So, the very natural thing to do is to define this by polarization, right? So, we have this properly defined, and this is an L function. So, we can define by polarization this inner product, and this will be an integral function. But at this level of generality, there's really no reason to expect that this is, say, for example, bilinear, right? Because we have not really required anything of the Soble F1, two space. Anything of the Soble F12 space, so it's really just a fury of Anak space. So we need to actually enforce this if we want this to be bilinear. And this is a condition that in the literature is known as infinitesimal Filbert Kennedy. It's defined by several people in different contexts, Ambrosio, Gigli, Sabarea, and Rayala, mainly in several works. And what it means is just that the And what it means is just that the W12 space is a Hilbert space, right? And if you require this, then actually this object that we define in this way is actually L infinity by linear. Okay, so this works if we require this condition, and now we can pass to defining the Laplace. And one very natural thing to do here is that if one already has, in some sense, defined the norm of the gradient of a function, one can take a A function, one can take a distributional approach to define the Laplacian, right? So instead of trying to define directly, we can try to enforce the integration by parts formula to hold. And this is what we do. So for any function, sublf12 function, we will say that it is in the domain of the Laplacian, which we denote like this, f in the delta, if there is another L2 function such that this integration by part. This integration by parts formula holds when we test against every other G in W12. Actually, it's enough to test for Lipschitz functions, but that's a theorem one has to prove later. And if this holds for some H, then we will denote this H as the Laplace. So now we have norm of the gradient defined inner product of gradient F, gradient. inner product of gradient f gradient g defined and the plausion of f defined for uh suitably or for good enough functions so now all of the terms in the book nerd's inequality are properly defined but we have the problem that all of them are defined really only up to almost everywhere equality right so it doesn't really make sense to require bugner's inequality in a pointwise sense so what we need So, what we need is an integrated version of this. So, for it to make sense, we can take F to be in the domain of the Laplacian. And since we are, for example, taking inner product of gradient u, gradient of Laplacian of U, this object will only make sense if we also require that Laplacian is a double equant. So, let's require that. And we're going to test against all functions. Against all functions in the domain of type of the Laplacian, which are also essentially bounded, non-negative, and for which also the Laplacian is essentially. So now we take one of these functions and multiply all of this inequality by that g and integrate, right? And then we recover sort of like an integrated version of or weak version of this Bochner's inequality. And of course, in the smooth setting, you can recover this from this in a straightforward way. From this, in a straightforward way, right? So it makes sense to require these weak Wagner's inequality in the abstract setting. Okay, so what I said in the beginning is that if one wanted to speak about reaching lower curvature bounds, since in the smooth setting this is equivalent to Bochner's inequality, then one can try to make sense of this. So we have made sense of this, right? And we can make a definition now, which is due to Make a definition now, which is due to Ambrosio Gigli and Sauri, and it's the following: so we will say that for any real number and any non-negative or say positive number n, we will say that the metric measure space XDM is an R C D K N. I'll say in a minute what these letters mean. It's an R C D K N space if the following holds. First, the mass gives. First, the mass gives finite mass to bounded set, which is something that we even required from the beginning, right? Also, if the space is infinitesimally Hilbertian, so that this inner product object is bilinear. And maybe the important part is that also the weak Bogner's inequality is satisfied for these parameters A and N. And there is a technical condition we also have to require to exclude some. Required to include some pathologies here, which is the so-called sub-LF2Leap sheet property, which means that if we have a sub-F1-to-function, which has minimal weak upper gradient bounded by one, then one can always find a leap sheet representative. And in some way, this condition is what allows one to link all these functional analysis tools with the actual metric geometry of the space. Geometry of the space. For example, one can very directly prove that if I have, say, for example, a sub-appointed function which has minimal weak upper gradient equals to zero, then this function has to be locally constant, right? Because it has a lipstick represented as lipstick constant. So this is really the link between the functional analysis and the geography. So now let's say briefly what these letters mean. R stands for remain. means R stands for Riemannian which is really this item two here this is what encodes the Riemannian kind of behavior of the space right and then these other two letters stand for curvature and dimension in the sense that this weak Wagner's inequality is a condition that involves both curvature and an upper bound of division right something that I want to emphasize for example is that this 10 doesn't have to be an integer as in the smooth case right As in the smooth case, right? This can be actually any positive number. Okay, so another, or say morally, intuitively, what one can think is that RCDKN means you have a Bagner's inequality and your sublev wants to face this negative. Let me just, for the sake of completeness, clarify that this definition is not really the first definition that came into the picture. That came into the picture. Historically, there's a previous definition by Lot, Schurm, and Vilani. And let me comment a little bit about that definition. I'm going to speak about so-called CDK infinity spaces. It's a relaxation of the spaces I have introduced before. And let me clarify what the relation is here. So, first, let's graph the space of Braille measures, which are probability measures and have. Which are probability measures and have finite second moment. And one well-known thing is that one can define the so-called two-Basserstein distance on this space, meaning that one can take the infimum of this integral of d squared on x times x against all couplings of these two measures, mu1 and mu2. Meaning that what I mean by coupling is that this is a measure on the product such that when you take the push forward with the canon. That when you take the push forward with the canonical projections, you recover your both measures that you're measuring. This is a well-known metric space, and you can define a Shannon entropy, which sort of tells you how similar are two measures. So to define it, let me for the moment assume that this measure is absolutely continuous with respect to the reference measure. To the reference measure, and you define the Shannon entropy by let's say rho is the density of this measure mu against m, and you define it by taking this integral, the probability problem, right? If mu is not absolutely continuous, we just say that this Shannon entropy is and something actually incredibly surprising for me is that it turns out by a theorem of Sturm and von Renesse, which actually builds upon work of Which actually builds upon work of several people: Cordero de Rasquin, Macan, Schmukenslagen, Otto, Vilani, several other people. It turns out that it's equivalent to require in a smooth manifold that which curvature is bounded below by k than to say that this Shannon entropy is k-convex on geodesics of this Basserstein space. Meaning that if you have two geodesics, sorry, two measures in this Basserstein space. Measures in this Basserstein space, there's always a geodesic joining them, and the Shannon entropy along this geodesic satisfies this inequality, this convexity inequality. But you can see that, as before with the Buckner's inequality, this characterization makes perfect sense on metric measure space, right? Without any smooth structure a priori. So one can define a space to have reaching curvature boundary below y k. Curvature boundary below by k by requiring that this inequality holds, right? And this is what Lot Schurman Bilani said. Just let me mention that there's a way to make sense of including a dimensional term here. I'm not going to do it because the inequality is horrible, but you can do it, right? And as I said, this is the definition by Lodge, Norman, Vilani. And we say that Xdm is a CDK infinity space. Is a C D K infinity space if the Shannon entropy is convex. The bad thing here, or maybe an uncomfortable thing here, is that C D spaces can include Pinsler manifest. So if one really wants to focus on Riemannian geometry, one needs to reinforce somehow this C D condition. And this is the condition of infinitesimal 2. So R C D spaces, as I defined it in the As I defined it in the previous slides, are in fact equivalent to CD spaces as defined by Logic Turn-BÃ©lani plus this infinitesimally Hilbert condition, meaning that the W work space is Q. All right, so let me give some examples of what types of spaces can arise in this setting. So, of course, if we did everything correctly, Uh, did everything correctly manifolds with Richie curvature bounded below should be examples of this type of spaces, and they are right, just because they satisfy Wagner's. But we have more stuff, which are the so-called which limit spaces, which are defined to be Gromov Hauser limits, let's say measured Gromov Hauser limits. So they are Gromov Hauser limits in the usual way, but we also have convergence, weak convergence. Have convergence, weak convergence of the measures in duality with continuous functions. And these types of limits were extensively studied by Chigera and Holding in the late 90s. So a lot of things are known about these spaces. We also have some other spaces which are called Alexandro spaces, which are metric spaces which have, in some sense, a sectional curvature rounded below. Sectional curvature rounded below. The way one defines this in the metric setting is that one looks at geodesic triangles in these spaces and one compares them to geodesic triangles in constant curvature space. And one says that these metric spaces have curvature bounded below by some k, where k is the constant curvature of the model space, if the geodesic triangles are fatter, let's say, than the geodesic triangles in the model space. The model space. One can make this statement very, very precise by using the classical theorem of toponogo, which compares distances between vertices and opposite sides of this triangle. But now, these spaces are certainly generally not manifold. That's maybe the important part. And Petrunin proved in the non-negatively curved case that these are actually also RCD spaces. And in the general Cases. And in the general case, it was shown by Zhang and Zhu. And there are also several constructions available in these spaces, which are not available in small manifolds. For example, one can take cones and suspensions of RCD spaces, and these will also be RCD spaces, which is something that already in manifolds we cannot hope to do. And we can also take, for example, quotients of isometric and measured perception. Of isometric and measure preserving Lie group actions. This was shown, sorry, the previous theorem was shown by Christian Ketter. And this part about the quotients was shown by Fernando Galas-Garcia, Martin Kell, Andrea Mondino, and Gerardos. So we had a lot of constructions to work with in this category of. So now, given that maybe I hope to have convinced you that one can speak about Richie curvature. That one can speak about Ricci curvature in this abstract setting. Let me present our theorem joint with Windows Eclipse. Let me first present an informal statement of the theorem. So if we have an RCD KN space XDM and we have a function in the domain of the Laplacian with a good enough Laplacian, say in Lp for some P bigger than N, for example, harmonic functions will trivially satisfy this condition. This division, then what we can show is that the norm of the gradient of u is equal to zero at any sufficiently bad singular point of the space. And maybe the key point of this statement is what do you mean by sufficiently bad singular point, right? So let me, instead of telling you, let me first show you that this theorem works to actually solve the first two questions that we had in the. First, two questions that we had in the uh, so first I claimed that uh, because of this theorem, one can prove that for any n in n and d bigger than zero and any modulus of continuity omega, there are sequences of remaining manifolds in mn0 d and points in each of these manifolds such that you can find harmonic functions defined on the wall of radius one around each of these points. Radius one around each of these points. Say that you have the stubble F1 to norm normalized to be one for each of these functions. But if you test the continuity of the norm of the gradient of UI against this modulus of continuity, this blows up. Okay. So you cannot really prove some uniform modulus of continuity estimate for the norm of the... Continuity estimate for the norm of the gradient of harmonic functions. And for the other unrelated question, actually, I claim that one can show that for a choice of these parameters and p bigger than n, there are always sequences of remaining manifolds in m and zero d and smooth functions ui such that the right-hand side of the Calderon-Sigmund inequality, you can keep it normalized. Normalized being one, but the norm of the LP norm of the hash and blows, right? So you can also not make sense of a Calderon-Sigmund inequality for P different than to it this way. Okay, so let me give you a sketch of the proof of this. Maybe let's focus on this first theorem, which will follow from the previous theorem. So let's say by contradiction that So let's say by contradiction that there's no such sequence, right? So there's actually a modulus of continuity, say W, that depends on the DNN, such that this holds, right? I can estimate the norm of the gradient of Q at X minus the norm of the gradient of Q at Y by this, where D is the induced Riemannian distance in any Riemannian matter, right? For any two points in the pole of radius one. any two points in the polar radius one half right and this uh i emphasized uh holds for any remaining manifold in mn0 right so now uh you can actually do some some tricks some uh rescaling and covering arguments to improve a little bit this this estimate and find a modulus of continuity omega s dn such that this is true for any x y in the wall of radius one minus s at x0 right At X Euro. So now let's take this estimate here. And now, if we have this estimate here, let me present to you some pathological examples due to Otsu and Shioya. And these examples are built as a Gromohausor biblium. So let's start with a tetrahedron, a regular tetrahedron in R3. And this will be the first space in my sequence. And now, what do I do to define the next spaces in my sequence? Phases in my sequence. So for each of the phases of the tetrahedron, let's take some barycentric subdivision. And for this very center, meaning that we take the barycenter here and join it with all the vertices in the face. And now we want to take this very center and push it a little bit out of the face, right? And we do this for all of the faces. And now I do the same for each edge, right? I take the middle point of I take the middle point of each of the edges, join it with each of the vertices, and push it a little bit out. So the next step of the sequence should look something like this drawing here. And the important part here is that one can actually show that since this space is the boundary of a convex subset, an open convex subset in R3, one can actually show that this is a space with rich. This is a space with richie curvature bounded below by zero, and we do this trickery of pulling out this barycenter so that this convexity is preserved in the next step, so that the richie curvature bound is also preserved, right? So now we have a sequence, and let's do this at infinitum. So now we have a sequence of spaces which are non-negatively curved, and it turns out that the limit is, well, it's topologically a sphere, but the metric is extremely. Sphere, but the metric is extremely bad, right? It has a tense set where the injectivity radius is zero, right? Which corresponds to all of these barycenters that we have been pulling up. And this is what I mean by sufficiently bad singular points. So for now, let me just take my word that these points are sufficiently bad, right? And let's call this limit space X. And now it's easy to always take a point here in your space X and build. In your space X and build a non-constant W12 harmonic on the say the radius, the ball of radius one, right? Now, if you hear yelling behind me, it's because there's a guy stowing water, which comes around this time in my house. So I apologize for that. So now, by the work of Ambrosio and Honda, as I mentioned before, you can actually show, let me backtrack a little bit. Let me backtrack a little bit. These spaces are not smooth, but you can always sort of round all of the edges and the vertices here and actually approximate each of these spaces by smooth manifold, which are non-negative, right? So you can safely assume that this limit space X is approximated by non-negatively curved manifold. So now by Ambrosia and Conda, one will be able to find for every R in 0, 1, a sequence. R in 0, 1, a sequence of harmonic functions defined on the balls of radius R in each of these mi, which suitably converge to my harmonic function u in the sense that not only the functions converge, but also the minimum weak of gradients converges. And by the previous modules of continuity estimate, which by contradiction we assumed that that was true, we can know that the minimal weaker gradient of this harmonic function is continuous on the ball. Function is continuous on the ball of radius one, my space, right? But since we have a dense set of these sufficiently bad singular points, our theorem with Widow will say that actually the minimal weaker propane end is zero all throughout, meaning that this is a constant function, which is a contradiction, right? Because we said u was non-constant, right? So this is very easy to prove one has. One was one has this theorem by Peter and so let me now put the true statement here and maybe focus on the remaining minutes telling you about the proof of this statement. So the true statement is that I have a metric measure space which is an R C D K n space and we have a function u which has a sufficiently good Laplacian in L P for P bigger than n. For p bigger than n, then I claim that the minimal weak operating is zero for all points which satisfy this condition. I want the point to satisfy that the Bishop Romoff density at this point is finite, but also that the tangent cone at this point has a diameter strictly less than five. Now, if you're not familiar with these two concepts, let me clarify a little bit what I mean here. Bit what I mean here. So, first, let's talk about the tangent code. What I mean by tangent code is any metric space which comes from doing the following procedure. So I step at x at my space. Now I take a ball of radius, say, Ri, where Ri is a sequence that goes to zero, and now I rescale the metric. I rescale the metric in the world of Ri operatus Ri by a factor of one over Ri, and I also rescale the measure by a factor of Ri to the n, right? So I'm blowing up my space in some sense. And now I take a measured Gromov-Hausdorff limit of this signal. If such a limit exists, then I'll say that this is a tangent cone at x. If you do this procedure, of course, in a manifold, what you will see In a manifold, what you will see is the tangent space, right? At your point. And there's a somewhat bad thing that happens in RCD spaces. And the bad thing is that the tangent space may not be unique. So if you do this procedure, you may end up for different sequences of RI, you may end up with different spaces. Fortunately, there's a very deep structure. There's a very deep structure theorem by Mandino and Neber that says that the situation is not so bad. For almost every point in the space, the tangent cone is actually unique and it coincides with a Euclidean space, the usual Euclidean space of some dimension that depends on the name. Actually, later, this theorem was refined by Brue and Semola to say that actually for all of these points. That actually, for all of these points where the tangent cone is unique, the dimension does not change. So, now what do I mean by singular points in this previous sense that I said? I mean any points such that this does not happen. Okay, so what about a bishop-chromoff density? So, bishop-chromoff density is sort of a way to measure the behavior of your measure as you blow up. Behavior of your measure as you blow up when you compare with the models based on curvature exactly K and the measurement. So, to be more explicit, it's the limit when you take the measure of your ball of radius R quotient against the ball of your radius of the ball of radius R sorry in the model space of Principal's curvature K. And a theorem by Guido de Filippis and Nicola Gigli says that this limit exists. Collagli says that this limit exists for our CDKN spaces, and that if the Bishop-Gromoff density is finite, then actually the set of tangent cones is made up of true cones, like real cones, right? In the sense that metrically, the space looks like a cone, right? And the measure looks like the measure you would expect in a cone, right? Which is something that, surprisingly, maybe does not happen if the Does not happen if the Bishop Gram of density is not fine. Okay, so now that we have this concept, maybe it's a little bit more clear what the statement of the theorem says. And now let me spend the last maybe five minutes telling you about the proof. For simplicity, I'm gonna center, I'm gonna focus on the case about you being harmonic. Harmonic. The theorem is a bit more general than that, but this is easier to prove. So now let's first focus on the case, not on an arbitrary RCD space, but on a space that looks like a cone. And this cone is what I call a sharp cone, meaning that the section of the cone, x, has diameters strictly less than five, right? So the tip of this cone will be a sufficiently bad singular point in this tense. That singular point in this tense that we mentioned. So, now because you have this cone structure, if you have a harmonic function defined on the ball of radius one centered at the tip of the cone, you can always write this harmonic function as a combination of the eigenfunctions in the section of the cone, right? With some coefficients, we depend on the dimension. Coefficients will depend on the dimension and the eigenvalue. So, now if you just do some computations, I hope you can believe me that for all r in 0, 1, you can write the L to norm in the ball of radius r in this way, right? It's some combination of the radius with some exponent that depends on these coefficients alpha i with some quotients, with some coefficients. Quotients with some coefficients and some constant with the pens, which depends on this alpha i, ai, and these eigenvalues. Right, the precise values here are not really, really very important, but if you look at the specific case of the ball of radius one, then you can combine these two observations to obtain a sort of decay property, right? So you can look at the average L2 norm in the ball of radius r of the volume of radius r of the of the gradient of u and obtain that this uh is less than or equal to one than some radius uh that this radius r sorry and some exponent here that depends on alpha and let me emphasize that this alpha one depends as i said on the dimension and the first eigenvalue okay so now if we have this sort of decay property this the The sharpness of the cone actually comes in because you can see by a theorem of Obata, which gives me a bound on the first eigenvalue, if you use that, you can see that actually this exponent that I have on the radius is bounded by some delta that depends on the gap of the diameter of the section and pipe, right? The sort of like the error in which bisection of the cone is. In which bisection of the cone is not maximum. So now you have a decay property that looks like this. And then you can already see that maybe if you take a lean stook here, then you can obtain very directly this here. This is not really the full statement that I wanted to prove in this simpler case, but let's save this simpler statement for now. So now, what happens in the case? So, now what happens in the case of an arbitrary R C D K n space? Let's take an arbitrary R C D Kn space, and the proof actually is just a perturbation argument in some sense, right? If I want to prove the theorem at some point in my space, I'm going to blow up. And if my point is sufficiently bad, if my point is a sufficiently bad singular point, then the tangent cone will be a sharp cone. And I can use Sharp code, and I can use this analysis that I did previously. So, let me be a bit more precise. So, let's take X sharp point, so a point with this finite Bishop-Roman density. And now I claim that a decay property like this holds, right? So, the L to norm of the ball of radius R over two is less than or equal to one minus some delta that depends on K, N, and X. n and x times the L to norm on the ball of radius R for some fixed R0 which depends on it. And I claim this because if this is not true, then what I can do is blow up my space, right? So take a sequence of rescale spaces, which will converge to a tangent cone and take a sequence of harmonic functions defined on these balls of radius ri and these harmonic functions. And these harmonic functions by Ambrosia and Conda will converge to a harmonic function on the bubble of radius one in the tangent. And since everything converges also in the minimal weak upper gradients, then this estimate will pass to the limit and this will sort of violate the decay property we have before. So we need to have some decay property of. Some decay property of this type, also in the general. But now, as before, you can also take LinkSoup to obtain this, right? And it's a standard hardnack type inequality that's obtained essentially by the work of Jang, Coskella and Jang of this type, right? You can always obtain a hardnack inequality of this type. Since my function is harmonic, maybe. Since my function is harmonic, maybe this will go away. And you can see that from this, we obtain that actually the minimal recovery gradient is zero. So I guess that's the proof. It's not really difficult once one has all of these tools available, right? And I guess this is all I wanted to talk about today. So thank you very much for your attention. And yeah, thanks. Yeah, thanks. Well, thank you very much for the talk, the very interesting talk. Are there any questions from the audience? Sure. So if you have a space that's, say, mostly smooth, like you just have an isolated conic singularity or Just have an isolated conic singularity, or you have stratified space. What do your RCD conditions look like? Do you know? I don't precisely know, but I know that this has been studied. This is a paper by Laria Mondello and probably someone else, I want to say Christian Ketterer. I'm not really sure. Unfortunately, I don't have the precise relation, but this has been studied. This for sure is known. It's not. Okay. Thanks. Yeah. Sure. So we still have time for a little question. Could I ask a question quickly? It's kind of in a different direction, but I'm just sort of curious about some of these other types of Gromov-Hausdorff limits where you maybe don't have this reach. Have this Ricci curvature bounded below. Like a lot of sub-Ramanian type spaces arise in this way, where you can have the Ricci curvature getting sort of unbounded from below. Do you know if there are these kinds of sort of synthetic approaches like you've been doing where like you have, I don't know, I guess that's sort of a question in a different way. Yes, I do think there should be something like this. And I guess I don't know if this maybe will answer your question, but if you look at it. But if you look at it in the abstract, these grammar house of limits really don't have any reason to be nice. For example, the sequences can collapse, right? So you could expect maybe without other conditions that the tangent cones are really, really horrible, right? They could collapse to weird spaces. Yeah. Thank you. Should I refer to previous question? You is it are you referring to Riemannian manifold or general space? Anyway, just general space. Yes, just general space. Totally manian cannot be R C D, I think. Yes, no, certainly Riemannian, they have to be very nice. Yes. Can you actually, Jesus, state your main result again? Yes. Yes. Yes, okay, finite. So then at the point which is a density, it's not finite, you don't have any conclusion. Yes, I don't really know what happens here. Unfortunately, I have to think more about it. Yeah, so this is like a feel like a non-collapsing case, right? Exactly. Collapsing case, right? Exactly. This is the precisely the non-collapsing case. So if one wants to study the collapsing case, yeah, this argument does not work. And I have a question. Do you know if it's possible to relate this result to computations or for? Computations or for this result to be useful in computing the homology or co-homology of these ICDKN spaces. I believe I asked you something somewhere a while ago just to compute like H0 or H1 because I think Gigly has some conditions that involve these objects. Right, that's a very interesting question. Unfortunately, I Unfortunately, I don't know the answer to this question. I would need to think more about this. I apologize. I don't really, really know. It would be interesting. I mean, they're very hard to compute. That's why I keep asking. It's just apologizing. Yeah, unfortunately, I can't know. I'm sorry. Thanks, I really enjoyed the talk, by the way. It's great. Thank you.