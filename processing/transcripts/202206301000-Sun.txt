I'm Mila. The topic I'm going to talk about today is the principal stratification for quantile causal effects under pressure compliance. And this is a joint work with my supervisors, Johanna Nashley-Hilbert and Erica Modi. This work actually doesn't fit in the extreme, but more lies in the causal inference. A little bit of background of the principle stratification. The principal strategy. The principal strata are latent pre-treatment variables that capture how individuals will respond to treatment. The principal stratification framework can be used to define principal causal effects across different strata. So there's one key information here is their pre-treatment variables. So that means the principal strata will not be affected by the treatment assignment and they And they characterize the units in your analysis. A particular implementation of this principle stratification focuses on the treatment of compliance behavior, or you could say it as the intervention compliance behavior. So in the following talk, I might use the intervention or treatment exchangeably. And of course, for the climate data or environmental data. And climate data or environmental data, you might also want to use the exposure as the word exposure here. So I also like to give one example here. For example, you have a binary treatment. So you have then the treatment has two levels, and then for each unit, you will have two potential compliance, either compliance to the treatment or compliance to the control. Compliance to the control. This bivariate potential compliance should be viewed as pre-treatment variables that characterize how the unit complying to the treatment. So in a traditional principle strata analysis, the compliance is binary. That means we only consider a unit or individual will compliant to the treatment. Will comply to the treatment or not complying to the treatment. Therefore, there are going to be four groups: the compliance, which means if I give you the treatment, you will take the drug. If I put you to the control group, you will not take the treatment. And the second one will be the always taker, and third is the never taker, and also the defiers. So the in the traditional analysis, the compliance is binary. The compliance is binary, and the estimation focuses on this average treatment effect within your principal strata. For example, the compliant average treatment effects, which also usually combine with assumption that there's no defiance in the population or in your population in your analysis. So, there are some challenges here. In a more general case, the compliance may be partial. That means the compliance is a That means the compliance is a continuous variable. One example could be a patient who may only take a fraction of the assigned pills. And in order to identify their principal causal effects, we will need a joint distribution for the potential compliance. And this joint distribution or the joint model typically depends on some unknown association parameters. But because, like I said before, Like I said before, these compliance are latent variables, and you have a missing data problem. So, modeling the partial compliance can suffer from lack of identifiability. And in addition, in some research areas, the primary interest might be in the causal effects in the tails of the distribution instead of the mean. So, one example could be: one might want to know the determinants of the Want to know the determinants of the lower tail of the entire breast weight distribution. In this work, we propose a cobular-based approach to estimate the association parameter between the two potential compliance with some reasonable assumptions. Here, I say two potential compliances because we consider binary treatment. And we aim And we aim to learn the distribution of causal effect in a context where the compliance may be partial. And this work is motivated by the desire to study the impact of widely implemented stand-and-home orders in the United States to mitigate the risk of COVID-19 transmission. So, before going to the details, here are some notations and assumptions. Table one is just an example of the data structure we have. Of the data structure we have. So we have repeated measurements where each unit ID, we have a measurement at time t0 and a measurement at time t1. We use the dead to denote the treatment and as denote the compliance. Here ST0 is the potential compliance being assigned to potential treatment 0 and ST1 is the potential compliance being Potential compliance being assigned to ST0 is being assigned to the control and ST1 is being assigned to the treatment and time T because the because of the counterfactuals at each of this time point you could only observe one of the two potential compliance so the question marks here denotes the missing potential compliances and of course the why is And of course, the y is the outcome, and x are some pre-treatment covariance. The x, which can be split into two subsets, the x1, which is, I call this time-invariant covariance, because at the two time points, they are common. And for the x2, uh, it's time-related, they are different at the two-time point, and also a deck to uh. And also a DAC to weigh this data structure. So you'll say because the S0 and S1 are those principal strata, they are not affected by the treatment. So there will be no direct arrow from the treatment to the potential compliance. But however, you say there is a backdoor pass from the treatment to the compliance. However, Compliance. However, if we control on this X2, this path will be blocked. Therefore, we can identify the treatment effects. And also some assumptions. The first two assumptions are quite standard causal assumptions. You might say it quite often in the causal literatures. I think it might work to just introduce this a little bit. Just introduce this a little bit because they are not covered in the previous talk. So, the first one is called the stable unit treatment value assumption, which is a suitable assumption. It actually has two parts. The first one is non-interference assumption. It says the potential outcomes are unrelated to the treatment status of the other units. And the second one is the consistency assumption. So, it says there are no different forms or versions of Different forms or versions of each treatment level that would lead to different potential outcomes. This, um, I can give you an example for this one. Suppose you want to know the effect of aspirin to the headache and you have two treatment levels: saying the taking the aspirin or not taking the drugs. So, suppose you and me are both in the treatment groups, but I In the treatment groups, but unfortunately, I took an old pill and it doesn't have the four effective dose. Therefore, the effect of my pill on my headache would be different from yours. So, in this example, you actually have two different versions of the treatment, and then the consistency assumption will be violated. Violated. And the second one is the unconfoundedness assumption, which has been introduced by Sebastian, so I will not talk about it here. And another common assumption would be the positivity assumption, but in our data set, we have a repeated measurements, so it is automatically satisfied here. And another two additional assumptions here. The first one is we assume the dependent structure. One is we assume the dependent structure of the two potential compliance is time invariant. That means the copular parameter only depends on the time invariant covariance. And we assume all the units in the data are measured under the two intervention levels. So here's just Speshin has given the formal introduction. Introduction of the quantile treatment effect here, only show how we can understand this intuitively. In this figure, f and j are two cumulative functions. Then for fixed quantum level tau, the QTE is just the horizontal distance between the two cumulative functions. And of course, if f and j are two conditional CDFs, then the horizontal distance A would be the conditional QTE. Conditional QTE. For the principal quantile treatment effect, it is actually a conditional QTE conditional on the principal strata S derived S1. So formally, it can be written as a difference of two inverse conditional CDFs for fixed quantile level time. So now Um, now we go to the copular imputation part because we want to estimate the principal causal effects. We will need to impute the missing potential compliance, then we would want to model the joint distribution. So here we propose to use a copular model because it can separate the marginal components from the joint distribution from its dependent structure. So, in the previous assumption, So, in the previous assumption, we assume the dependent structure among the two potential compliance is time invariant. This condition on the covariance, this implies that the copular of these two potential compliance only depends on x1. In other words, it means the copular of the two potential outcomes at time t is identical to the copular of the outcome. To the copular of the two terms and the second at time t1, and it's identical to the copular of the two observed potential compliance. This is quite important because in practice, we cannot observe the missing potential compliance, but under this assumption, we could use the observed potential compliance to model the genre distribution, then transform it back to impute the missing potential compliance. The missing potential compliance. And of course, we allow the marginal components to depend on both the time invariant and time-varying covariance. So formally, we use the UN weight denote the margins, and we model the joint distribution using the observed compliance. And CDX here is the copular parameters, and J is a non-link function, ensures the correct range of the copular parameters. Correct range of the copular parameters theta, and the eta is a calibration function. So we impute the missing potential compliance by the conditional mean of the underlying conditional copular and then transform it back to get the potential compliance. Here's one example. If UT1 is missing, that means ST10 is missing. We observe ST11. So we could impute the So we could impute the U21 using the empirical mean of condition of the conditional copulas, using the equation 2. Because here we use a single imputation to account the imputation errors, we use a bootstrap method proposed by Charles Seder to estimate the variance. In brief, their method is down by for each bootstrap samples, you repeat the same imputation procedure as in Same imputation procedure as in the original data set to account for their imputation errors. Another important thing is that in practice, you never know the true relationships between the two potential components and the two time points as shown in the red box. So the blue ones are missing, so we never know their true relationships. Here, two relationships are translated. And here, two relationships are considered. We call it as the commonatonic relationship and non-commonatonic relationships. The commonatonic relationship is quite similar to the equipercentile equating of compliance assumption in Dean Rubin's work. Basically, it assumes there's an explicit function to link the two potential compliance. Though this actually in fact specifies an underlying conditional copular of the Underlying conditional copular of the entire vector. However, in practice, well, if this is true, of course, you could use a common relationship imputation method to impute the missing potential compliance. But in practice, you cannot verify this assumption. Therefore, we consider one additional relationship, which is the non-common atomic relationship. Relationship. In that one, we do not assume any underlying dependence between their four terms. So in the simulation study, we consider the two relationships. We consider Gaussian T, Klan and Gamba cobular families. We conduct a sensitivity analysis and we assume the marginal models and the calibration functions are correctly specified. So, because of time, I'm going to skip all the simulation tables just to put the result summaries here. If the data are generated from a common tonic relationship, the commonatonicity imputation method, which assumes the common tonic relationship is correctly specified, performs better than the non-commonatonic method, which is the copular imputation method proposed in this work. But there are differences. Work. But their differences are not large. However, if the data are generated from non-commonotonic relationships, the commonotonicity imputation methods perform much worse than the copular method. So therefore, the inference based on a commonotonic relationship leads to biased results if this assumption is violated. However, the copular imputation method is applicable in a more general setting, no matter the conotonic relationship. The commonatonic relationship is true or not be true. In the application, we seek to estimate high PQD of stay-at-home orders on COVID-19 transmission. Just a little bit of background. In March 2020, the U.S. states began to issue the stay-at-home orders. And as of May 20, these orders were lifted. So we consider two interwinning. So we consider two intervention levels at each of the two time points. That equals to one means being interwinded, that means the state issued a stay-and-home order. And that equals to zero means not interwinded, means the order was lifted. So we have a counter-level data, we have some time invariant covariance, time-varying covariance, and the compliance is their stay-at-home rate. It's continuous and the license. Is continuous and lies in the range from 0 to 1. And each of the two time points, the compliance is partially observed, which is partially observed. Therefore, the data structure is exactly as the data, the example data I showed in the table one. And the outcome, we define the variable case rate change. It is defined as the difference between the weekly new cases after the incubation period. This is after the incubation period and the baseline per 1,000 inhabitants. So here's a heat map. It estimates the 90s PQDE across all the selected counties. I said selected counties because there are some states didn't issue the stay-at-home orders, so they were excluded from the analysis. And because there are some spatial correlations. Are some spatial correlations between them, but we didn't consider the spatial correlation. Therefore, we randomly draw 60% from the data. So, this is the estimate for those selected counties. Just recall that the treatment is being issued a stand-and-home order. Therefore, it will have negative causal effects on the outcome. Then, the smaller of the The smaller of the QDE, the stronger the causal effects will be. The points below their dashed line are the counties that are more likely to comply to the stay-and-home orders. And the points above the dashed line, the counties that are less likely to comply to the stay-and-home orders. We see from these small sub-figures that for the counties that are more likely to comply, That are more likely to comply, there will be a stronger causal effects. And also, our county map with those estimates and black areas are just the states or the counties that are excluded from the analysis. So, for now, it looks like the method is built. The method is built for quite restricted structure because every unit has to be has the repeated measurements. But however, the method can be extended. One obvious or easy extension could be that if you have a relatively small fraction of the units in the data having access to one of the two interventions, this method is applicable. And in a more general And in a more general setting, without the repeated measurements, the proposed method can be generalized to a matched pair setting. So for a matched pair setting, here's another deck to show the data structure. We have covariance X2 and X1, X2 and X1 all affect the potential compliance, but only X1 is the compliance. only X1 is the confounder of the compliance and the outcome. See here are some confounders that are used to match the units in the data set. And also our table showing the structure. So here instead of having the unit ID, we will use the paired ID. We match the unit I and I prime using some confounders say here, and therefore Say here, and therefore, we're going to have a pair. And for this pair, we could have some covariance here, the x. And we split the x into two different subsets, the x1 star. All those covariance that are common between the two matched units, which is analogous to the time-invariant covariance, and X2 are different amounts. Are different among those matched peers, which is analogous to time-varying covariance. And therefore, we could model the joint distribution of the two potential compliance from those observed Si0 and Si prime. And the following procedures are just exactly as before. So, to conclude this work, here are some summaries. So, we propose copy, we propose. Conditional covular approach to allow the covular parameter to change across according to a collaboration function of the merit covariance under mine assumptions. And we focus on the linear collaboration function just for simplicity, but this can be easily extended to more complicated dependent structure, such as a local likelihood-based model or generalized additive model. The proposed copular imputation approach performs reasonably well, no matter the relationship is common autonomic or non-commonotonic. And the approach is motivated by the COVID-19 data with repeated measurements, but is not limited to this data structure. Right, so that's our talk. And here are some references. Thank you very much. References and thank you for your attention.