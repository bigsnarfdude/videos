I have a chance to share my own work. So, I'll be talking about Bayesian standard parametrics for sequential decision-making. I've been interested in these sequential decision-making problems for quite some time, and they seem to come up in a number of applications that I've worked on so far, and ranging from osteoporosis treatment and cancer, and HIV. And I think it's an area in causal inference where Bayesian modeling shines particularly bright. So I'll start off with a quick overview of causality just in general. I mean, I don't know if it was the majestic scenery at the mountain the other day, but I was looking out and I was wondering for the first time, it occurred to me that maybe not everyone's as obsessed with causal inference as I am. It's a shocking thought, but I'll go over a quick overview and then we can dive into the application description and then get into the methods. Most of the talk will be. Methods. Most of the talk will be in a setting with incomplete outcome information, incomplete covariate information. And if there's time at the end, I'll sketch out an extension to how we can think about these problems when we have incomplete covariate information as well. Okay, so quick primer on sort of causal inference methods. We've seen a lot of great talks in this subfield this week, but just in this case, here with a single treatment decision that I'm going to denote as A. That I'm going to denote as A. So there's treatment option 0, treatment option 1, and a single survival time outcome T. How do we actually define a causal effect? Causal effects, or at least these population-level causal effects of a treatment A on some outcome T is going to be defined as contrast of expected potential outcomes within a common target population. This part is oftentimes ignored, but it's very important that we keep in mind that there is some well-defined target population that we want to learn about the treatment. That we want to learn about the treatment effect in. And these potential outcomes here: this T superscript one or T superscript zero, these are random variables just like any others that represents the potential survival time had someone been exposed to treatment option one versus zero, right, potentially counter to the fact. Okay, so what I mean by population level S demands being expectations of these potential outcomes is we might be for interested, for example, be interested in this quantity here. Example, we'll be interested in this quantity here: the probability that T1 is greater than five. So, this would be interpreted causally as the proportion of the target population that would have survived at least five years had we treated everyone with treatment option one. Okay, and here's like the greater, the fundamental problem of causal inference. We never observe a world where we had where we treated everyone in the target population with treated one. Okay, the only thing we observe is data from a subpopulation of patients who happen to be treated, right? Happen to be treated, right? Especially in observational data settings. Okay, and that subpopulation of patients who happen to be treated may not be representative of the target population. It could be that the people who got treatment one had, you know, systematically more likely to have kidney infections. And so they had worse survival. And then you see worse survival among the treated group. And you don't know, is it because of the treatment or is it because they had kidney infections? So we call this confounding because it's difficult to tease out the effect of the treatment from these confounding factors. Or if you're an economist, you might call this selective. Factors, or if you're an economist, you might call the selection bias because the people selected into the treatment group or a biased sample of the target population. But at any rate, these two probabilities are not equal. The probability, the survival rate, five-year survival rate had everyone in the target population receive treatment one is not the same as the survival rate among those who happen to get treatment one. And that's the issue. So we talk a lot about, like, well, you know, well, association isn't causation. Well, this is what we mean. Well, this is what we mean. Associational contrasts are contrasts of survival rates among those who happen to get treated with option one versus those who happen to get treated with option zero. Causal contrasts are explicitly contrasts of potential outcomes. But again, it's difficult to map these things to things that we actually observe. Okay, so more generally, what we want to do is actually, we want to know the joint distribution of the potential outcomes. And I'm writing this thing out excessively as an integral over the joint to show. Excessively, as an integral over the joint, to show you that if we have the joint, we have everything. That's the name of the game. All causal inference is about finding the joint. Okay, but all we have is realizations from the joint data distribution, distribution of people happened to get treatment option one and had some survival time. Okay, so the very, so kind of like a standard workflow that causal inference people have, number one, is to ask the question. It's very difficult, especially when you're sitting down with a collaborator because they work so hard and spend a lot of money. Work so hard and spend a lot of money collecting data. And then the first meeting you have with them, they're telling you how great the data is. And then you're saying, Okay, so what's your question? And then they just keep talking about the data. And they say, no, no, no, what's your question? And then they keep talking about the data again. And so it actually requires quite a bit of work to actually formally define what is it that you're interested in knowing in terms of potential outcome. What is the ideal experiment that you could run in hypothetical universes if you can dimension hop? Universes if you can dimension hop, you know. And then once you've asked that question, there's this issue of identification. And by that, I mean mapping the distribution of the potential outcomes which you don't observe to the distribution of the observed data, which you have realizations from. Okay, and that's really the important. That's where all the causal inference work goes in. And identification requires untestable assumptions, different types of assumptions depending on the different settings. We saw principle strata. Settings. So we saw principal strata estimands. We saw settings with interference. They all required different identification assumptions to make this mapping happen. None of this is statistics. None of this is Bayesian. If we're interpreting statistics strictly as drawing inferences about a population from a sample, this is not statistics. The first two. Statistics comes in at the third step. Once we've identified, asked the question, we've identified the quantity that we want to estimate in terms of. That we want to estimate in terms of the observed data distribution, then we estimate the observed data distribution or its functionals. And that's where we can get all of our fancy machine learning, Bayesian non-parametric, all that stuff comes in step three. It's the last step. Okay, some might argue that it's the least important step because if you screw up number one and number two, it doesn't matter what kind of AI, machine learning, blah, blah, blah you use, like you're estimating the wrong thing. So these two steps are quite important. What do you want to know? What do you want to know? Can you even know it? And then, okay, let's actually estimate it. So, this requires untestable assumptions. It's unavoidable. So, if you don't like untestable assumptions, causal inference is not the right game to be in. This requires modeling assumptions, which we can relax, but there's kind of a cap in how far we can relax these modeling assumptions. Okay, so I'll basically go through that workflow within this application. Go through that workflow within this application of leukemia, okay, and in a sequential decision setting and not just a single decision setting. So, this pediatric myelukemia is a cancer of the blood and the bone marrow. It's essentially uncontrolled cell growth and it could happen in either of these places. And it's typically treated with chemotherapy. So, there are many different types of chemotherapies. One of these popular class of chemotherapy agents are these amprocyclines. So, they're very good at preventing rapid cell growth. Good at preventing rapid cell growth, they're actually suppressing cancer, but they're also cardiotoxic. So, if you keep people on the anthracyclines for high doses too long, you might induce heart failure. This presents physicians with a complicated trade-off. If we treat too aggressively with anthracyclines, then we might kill the patient. If we don't treat aggressively enough, we might kill the patient. And so, strictly speaking, here, anthracycline actually lowers ejection fraction. Ejection fraction is the proportion of the blood that pumps out of. Is the proportion of the blood that pumps out of your left ventricle every time it beats? You want that to be high. If not enough blood is actually pumping out of your left ventricle when your heart's beating, you're in trouble. Actually, heart failure is defined partially by what your ejection fraction is. How they measure that is they do an echocardiogram and then they put a stylus on the monitor and they actually measure it during a scan. So the goal here is to balance this trade-off, knowing that people, you know, in clinical practice, we often use ejection fraction. clinical practice we often use ejection fraction to make anthracycline treatment decisions. So our goal here is to estimate the causal effect of anthracycline treatment rules on survival. And we have this just incredible data set here from the Children's Oncology Group. It's from this AML 1031 trial. It's a phase three trial where patients were enrolled upon diagnosis, and then they had to move through four treatment courses. And at each course, they may or may not have anthracycline included in their chemotherapy treatment. Chemotherapy treatment. Okay, so importantly, anthracycline was not randomized in this trial. It was part of the modifiable backbone. This is why we need causal inference methods, because we need to deal with the confounding. The fact that the treatment that we're interested in is not a randomized agent. Okay, so a lot of this work is in this published paper in biostatistics. You can check that out for all the details. I won't go through lengthy simulation studies or results here. Here's a schematic. Here's a schematic of what the AML 1031 trial looks like, but it's actually like a nice template of what sequential decision-making problems look like in general. So, here, if we look at just subject one here, the Y's denote the time of the treatment courses. So, there are four treatment courses maximum in the trial. So, the time of the first treatment course is time zero for everyone. That's when we begin a follow-up. This patient had their first treatment course. They ran an echocardiogram, they checked their They ran an echocardiogram, they checked their ejection fraction, they saw whether the heart was healthy enough, they made some decision about whether or not to include anthracycline. Some amount of time, W1 units later, at time Y2, they start their second treatment course. They run an echocardiogram, evaluate heart function. If it was good enough, they would include anthracyclines and so on. This patient dies W4 time units after their fourth treatment course at time T. Okay? Subject two here was actually withdrawn from the study of W3 time units after initiating their third treatment course. So we never observed their survival time. We only observe a lower bound. We know that they survived longer than C. We don't know exactly when. So we have some incomplete information on the survival outcomes. Subject 3 here never survives through their full treatment sequence. They die W2 units after their second treatment, of course. Okay, so this LK here is going to be... So, this LK here is going to be my notation for a vector of covariates that is measured at each one of these time points. If you want to keep things simple, you can just think of this as ejection fraction that's being monitored. There are other variables too. That informs the anthercycline decision, A sub K, at the Kth treatment course. So, was it included or was it not? What's really interesting about this setting, which is something that the vast majority of analyses tend to ignore, is that the timing of when these treatments happen vary across These treatments happen vary across subjects. And the reason is that in these trials, you often can't specify, oh, you're going to get treatment two in five weeks. You know, it takes a lot of time for your body to recover after chemotherapy, and different people recover at different rates. You actually have to wait for subjects to attain hematologic recovery for their neutral counts to recover, and then you start their next chemotherapy course. And people's neutral pill counts recover at different rates. You have to wait. That's why this person sort of, they went on to treatment two a lot faster. They went on to treatment two a lot faster than this person, for instance. And this timing could actually be informative of treatment decisions. You might think that people who recover faster from their chemotherapy treatments, they might be more likely to get on recyclines. They're doing very well. They might also be more likely to have higher survival times. Okay, so this is something that we call informative timing of the treatment, and that's something that we have to deal with. So our contributions here are: we're going to have these. Here are we're going to have these Bayesian semi-parametric models that are essentially going to capture the continuous time transitions between subsequent treatments and death states. You can think of it roughly as a multi-state model with two different states after each course. This avoids a lot of the discretization that is super common in causal inference models for time-varying treatment strategies. What they do is they just partition the follow-up into like 100 weekly intervals or something. 100 weekly intervals or something, and then they use these discrete time hazard models to model survival. But then it opens up all these questions of, well, how do you discretize? How many intervals do you choose? We avoid all of that by taking a continuous time approach. And then we allow for covariate-dependent sensoring and death before treatment course completion. And then the nice part about the Bayesian inference is that we get full posterior inference for functionals of the joint distribution, which is really what. Which is really what the causal quantities end up being. So I'm going to define some notation here. I'll denote the history of a variable up to the kth treatment course using the overbar notation and the future using the underbar notation. And for each subject, I observe kappa, the number of treatment courses that they actually realized, either before censoring or death. And for each one of these K treatment courses, one through Kappa. K treatment courses one through kappa, I'm going to define a waiting time from treatment course K to the subsequent event. A subsequent event could either be a death event, it could be the next treatment course, or it could be a censoring event. I'm going to subtract off the previous treatment course time to get at the gap time or the waiting time. Okay, and this is the indicator. Did someone have a death event? Did they have a subsequent treatment after course K, or were they censored? Okay, and then this L sub K is the available ejection. The available ejection fraction history at the time, right, right ahead of the kth treatment decision. Okay, so all the information ahead of the kth treatment decision, I'm going to call H for history. Okay, so it'll involve all the previous ejection fraction history, the previous treatment decisions, and the previous K minus one courses, as well as the timing, which is important. It's important to keep track of the fact that people move through the previous courses at different times. Okay, and then the observed data for all these subjects. The observed data for all these subjects. I'm just going to be curly B. Okay, so far, so good. All right. So, how are we actually going to define treatments in this sequential setting? It's kind of easy to do in a point treatment setting. There are only two options, zero and one. These dynamic treatment rules, which are quite common in causal inference now, are a very convenient way of defining treatment strategies in sequential settings. So, what they are is, at the treatment course, we're going to define a mapping, the function from the space of The function from the space of the history that's available at the KF course to a set of feasible treatment options. And this feasible treatment options idea is very important for positivity concerns that we'll get to shortly, because not every treatment will be feasible at every course for every subject. Certain subjects, because these things can be fairly prescriptives, it might be that older people are ineligible for certain treatments. So it's important when you're constructing these rules that they actually map. Rules that they actually map into treatments that are clinically feasible. Otherwise, your clinical collaborator would tell you to get out of their office. It's never happened to me because I pay attention to feasible sets. So all the rules together, I'm just going to call that rule R. This is very distinct from static treatment rules, like always treat with anthracycline, never treat with anthracycline, right? Because here the treatment with anthracycline is dynamically determined by whatever the state of the history is at the Kth treatment. State of the history is at the kth treatment course. So, for example, at the kth treatment course, treat if ejection fraction is greater than some threshold tau. And there are some clinical sort of rules of thumb for what they use for what this tau should be. Usually it's something like 55% or something. Where do they come up with that? Well, it's what my attending physician told me when I was training, and that's what their attending physician told them when they were training. I don't want to make fun of it because actually the end result. I don't want to make fun of it because actually the end result is that those rules of thumb are actually pretty good. So, and usually only a handful of these variables, H sub K are going to be used to determine treatment. Like here, I'm only using L. Those are called tailoring variables in some papers because they're tailoring the treatment dynamically. So, now I need to define my potential outcomes. What are the things that can vary? Had I potentially treated according to a different hypothetical strategy? Well, if I had treated according to one Well, if I had treated according to one strategy, someone may have survived through more treatments, right? So actually, the number of treatments that someone survived through has to be considered as a potential outcome of strategy R. And for each one of those potential treatment courses that they would have survived through, there's potential waiting times. Okay, so W1 is the waiting time from treatment course one had I treated according to strategy R up to time one, and so on. And the survival time is just the sum of these waiting times. Sum of these waiting times. Someone who dies after treatment course two, you add up how much time elapsed between treatment course one and two, how much time elapsed between treatment course two and death, and then you add it all up. That gives you the total survival time had you treat it according to strategy R. Okay, so some of these population level target estimates that we're interested in now is the potential, the portion of the population that would have survived past time point T had we treated everyone according to his dynamic strategy R. And we could do this under two different rules, R. Do this under two different rules, R and R prime, and we can construct causal effects that actually contrast these two different rules. At a given time t, we could find an optimal rule over some class of rules. I say that as if it's easy, but actually kind of hard. But to do any of these things, to really target any of these things, we need to identify the joint distribution of the potential outcomes, right? The potential number of treatment courses and the potential waiting times. Like, if I could simulate from this distribution, I could just calculate t, right? Calculate T, right? If I could just simulate like a thousand draws of these things, I could calculate a thousand draws of t. So we need to identify this f star, the joint distribution of potential outcomes, and we need assumptions. So the assumptions are kind of standard, except for the key sort of non-standard thing here is the timing, which we need to account for in order to ensure this exchangeability assumption. So, what exchangeability is saying is that it So what exchangeability is saying is that a treated patient at course K is exchangeable with an untreated patient at course K, as long as they match on all of the same history. If you have two subjects who are exactly the same in their observed history, but they just differ on the treatment, they're exchangeable with respect to their potential outcomes. That assumption has to hold. It shouldn't be the case that people, even after controlling all these things, that the person who got anthercycle would have systematically been more likely to die anyway. If that's still true, after conditioning. If that's still true after conditioning on the available history, you're in trouble. And this is untestable. Sequential treatment positivity, this is extremely important. This tells us that within each subgroup of the population defined by the history, at course K, there has to be some patients who have either anthracypine and non-anthrocypine. Otherwise, you can't learn about the treatment effect in this subgroup. This is testable and it's manageable partially by defining your Partially by defining your rules such that they actually map into feasible sets. And also, importantly, it puts a cap on how non-parametric we can be when we're adjusting for these confounders. Let's say I just had a few covariates, I could put all my data into a table, and then I could compute average survival time in each of these cells, and then I can average them together. But then the minute you start having more and more confounders, your cells start getting sparse. Cells start getting sparse. Sample means great. It's machine learning, it's non-parametric, but it's not going to do you any good if you have sparse cells because then positivity will be violated. So this fundamental trade-off here is where a lot of statistical innovation happens, right? Double robustness, non-parametric. It's all about balancing robust condition, controlling for adjustment for confounding factors, but also knowing that you can't be too non-parametric if you want positivity to hold. If you want positivity to hold. So, positivity requires some smoothness, the question is like how much. It's an interesting kind of presents an interesting statistical problem. We have to have this non-informative censoring, which is stated more precisely in the paper, but roughly speaking, the cause-specific hazard sensoring has to be unrelated to the potential outcome. So at every given point in time, it shouldn't be the case that the patients who are most likely to die, let's say, are the ones who are getting withdrawn from the study. I do not think this holds in this application, but that'll give me another. But that'll give me another paper to write. Censoring positivity. You want to estimate five-year survival. There have to be some patients who actually survive five years in your data set. Otherwise, you're in trouble. This is also manageable. If the largest observed time in your data set is three years, just set your estimate to be three years survival. It's the most you can do. Otherwise, you'd be extrapolating from your prior or something, and we don't want to do that. If you believe these assumptions, okay, you can actually achieve identification of this F star. And what I mean by identification is this is a distribution of potential outcomes. They're parentheses R's here. Nothing on the right-hand side here has parentheses R's. These are all observed data objects. In blue are what are known as the sub-density functions for the waiting time from course K until an event of type S. So S equals zero is a subsequent treatment, S equals one is a death. S equals one is a death event. Okay, so here's this distribution evaluated at K of R is equal to two. So here's the distribution of the ejection fraction ahead of treatment course one. Conditional on injection fraction and a decision that you've made according to rule R, we have some waiting time from treatment course one until the subsequent treatment, where we have a distribution of the ejection fraction at course two, conditional on the history. Conditional on the history. The ejection fraction could depend on the waiting time since your previous event. And then you have your waiting time from course two to death. So that's it. So we call this non-parametric identification because we made no parametric assumptions about these distributions. Use your favorite model to model them. It could be a crazy neural net thing, it could be proportional hazard models, whatever you want. It doesn't matter. Whatever you use, it's It doesn't matter, whatever you use, it's going to be targeting something well-defined. This is a very good thing about causal inference, something that Eli hinted at in his talk, which is defining your parameters of interest outside of the context of any particular parametric model that is going to be wrong anyway. So we need estimates of these things. We have estimates of these things. We could plug it in, evaluate the integral, that gets us an estimate of the distribution we care about. Or if you're a Bayesian like me, you'd get posterior. You would get posteriors over these things that induces a posterior over F star. And the interesting thing about this G formula algorithm is what it's called is that it captures the outcome treatment and confounder feedback over the courses. For a long time, like before the 80s, people were just fitting standard regression models that were not capturing this feedback and they were giving biased puzzle estimates. And this is a general case for an arbitrary k. So you just have a product here. You have to multiply more of these waiting time models. To multiply more of these waiting time models, it's the same idea. You have to fit models for these sub-density functions. Where are these sub-density functions coming from? It's really the competing risk idea, right? So at every course K, the subsequent course and death are competing to get to you, whichever one gets to you faster is the one that happens to you. That's why the sub-densities end up popping up in the cheap formula. So, how do we model these things? We're going to specify Bayesian models indirectly on these sub-densities by modeling. Directly on these sub-densities by modeling the hazards. Okay, so this is a result from competing risk literature. You can always express the sub-density function of an event of type S completely in terms of these cause-specific hazards. This cause-specific hazard is the probability of having an event of type V conditional that you have remained event-free up until that time. So you need models for these intensities, one for each of the possible outcomes. That induces a model on the sub-density. As a model on the sub-density, and for that, we're going to use these proportional hazard models. So, we're going to use separate models for each cost-specific hazard. One for at every course K, we're going to have a hazard for the waiting time until death from course K, and then another hazard for the waiting time until subsequent treatment. And this, if you're familiar with the survival modeling, this is a fairly standard hazard model where Where the baseline hazard here, the baseline hazard of the event of type S is being multiplied by various effects of the covariates. And so the unknowns here are the unknown baseline hazard and then these covariate effects, which you're going to have to specify priors around. For these, we just end up specifying normal 0, 1 kind of priors, or I think there's 0, 3. You actually don't want to be too wide because these parameters correspond to hazard ratios. So, you know, even like a normal 0, 3 on the betas is pretty wide on the hazard. 03 on the betas is pretty wide on the hazard ratio scale. For this, we're going to specify a gamma process prior for the unknown hazard. Strictly speaking, the gamma process is a prior over the cumulative baseline hazard, which induces a prior over these induces a posterior over or a prior over these hazard rates. Here's an example of what posterior estimates would look like. So, in red, here are the frequencies. So, in red, here are the frequentist estimates. Trash, trash, trash, trash, right? No one believes, not even frequentists, that the probability of having an event at these time points is zero. It's just that your sample size at risk is like two, right? The Bayesian estimate and these under S GAN process prior is smoother. It's nice. It's a good way of balancing bias-variance trade-off. I won't talk too much about these covariate models. Suffice to say, you can be as parametric and non-parametric as you want in our sample. Non-parametric, if you want. In our sample size, we don't really have the budget to put full bar priorities on these things, but if you wanted to, you could. We just specify, I think we fit beta models for the ejection fractions since they're proportions between zero and one. I won't talk about MCMC. We did develop software that backends to stand for this, although maybe I should have back-ended to Pidgin. But I didn't know about Pidgin. I'm sorry. But suppose we just. But suppose we have this mth draw, mth postpheric draw of all these parameters. Computing the causal effect is fairly simple. You simulate from the transition process under a given rule R. Simulate an L, plug your L into your hazard model, simulate a waiting time until the next treatment, then simulate a waiting time until death from your cost-specific hazard. See which one comes first. If someone died before they had the next treatment, then that's their survival time. Treatment, then that's their survival time, the waiting time till death. Else, move on to the second treatment course. Simulate your second round of ejection fraction, plug that into your rule, get your treatment, simulate the waiting time to death. In this case, I only have to equals two treatments because otherwise I'd go on forever. And add up these simulated waiting times to get the simulated survival time. And then you average and see, okay, what proportion of these B Monte Carlo simulations, how many of these patients survived past. How many of these patients survived past time point t? That gives you the mth posterior draw of your causal parameter, causal survival rate. Do this for capital M draws, you get a whole posterior set of posterior draws that you can use for inference. This is the actual data that we have. You can see there are only 292 subjects, which is very small, but there are only 500 new diagnoses of pediatric AML every year in the United States. So this is actually quite big in this area. In this area, about 63% of patients make it through all four treatment courses. We adjusted for confounders. I won't talk about it too much, but what's interesting is that if you look at the distribution of the waiting times between the courses and these data sets, there's a long tail of subjects who take especially long to recover from their chemotherapy. So it's really important to take into account the timing. What we did with these ejection fraction rules, so you can ignore this. This is just the So, you can ignore this. This is just the math version of what I wrote in words here. This is a rule that we really cared about, which is treating with or withholding the anthrocyclines if ejection fraction was less than some threshold, tau2, at the first course, and then withholding it at the second and fourth course if there's a tau 1% decline from baseline to a value of tau2. This is exactly like what people use in clinical practice. They say if there's like Use in clinical practice, they say if there's like a 10% decline from baseline to a value of less than 50%, we're not going to give you anthracyclings anymore. Where those cutoffs actually are, well, there's a range of potential cutoffs that we explored. Turns out the clinical one used in practice is actually pretty good, which is not surprising. I mean, there's a handful of specialists who treat these patients across the country. So they have pretty good knowledge of the population. So it looks very complicated. At the end of the day, the result is something. Complicated. At the end of the day, the result is something that is very familiar to clinicians. It's the survival curve. Survival curve had you treated according to a 10% decline to a level of 50%. Here's one under R prime where you just looked at the percentage change from baseline. It doesn't matter what it fell to. And you could compute these survival curves. In blue, these are like credible bands. And then you have these pointwise sort of intervals of the risk rate. Sort of intervals of the risk ratio of survival. You can see that it sort of overlaps one. You could do these for any one of these cutoffs, which was very appreciated by the collaborators. Here's an interesting case where we specified a rule that treated with anthracyclines at course three. Look at this thing. These intervals are the size of a school bus, right? It's crazy. And the reason is no one in the data actually got anthracyclines at course three. It was prohibited by trial design. It was prohibited by trial design. This is what happens when you specify rules that map to infeasible treatments. And this is a very good thing. You get high posterior uncertainty. Your posterior estimate is saying, I don't know. Like, I don't know. There's nothing in the data that's telling me about this intervention. I really like this feature. If we use something, if we used kind of a worse model that didn't respect the data generating features, we'd get a very tight estimate here, even though we were essentially extrapolating from our model. Essentially extrapolating from our model. So I kind of like the honest uncertainty estimation, but I don't know. Maybe you guys can tell me how you feel. I won't talk about the extension because I don't think there's time, right? Yeah. So I'll just do some marketing. Paper package that implements these Bayesian models. There's a whole vignette for it. This master student working with me did a good job. It's like stupid easy to use in terms of the interface back ends to stand. And if you want to learn more about Bayesian estimation causal. Want to learn more about Bayesian estimation causal effects? There's this intro paper we wrote up for SAS. Thank you so much. Thank you very much for the very nice talk. We have maybe time for one question. Maybe if there are more questions, we can discuss them in the break. All right, well, thank you for the super interesting talk. I was kind of curious about that. I was kind of curious about that assumption that you even pointed out. I mean, the moment I saw on the slide, I was like, okay, I'm a little bit skeptical about censoring. The censoring one. Yeah. So then the question, I guess you already know that it's a bit sketchy, but the question for me is, how do you deal with that? So how I would deal with it, maybe in a subsequent paper, is that you'd have to do sensitivity analysis. You never see the survival times for patients who were withdrawn from the trial, but you can make assumptions about it. Is there any opportunity for other data collection? Like, is that. For other data collection? Like, is that within bounds? I know nothing about medical studies, so I don't know if that's even acceptable. No, there is Kelly Goetz, who's the person who's collecting this data, is currently actually going and looking at the patients who went off protocol and trying to get survival times for them. And that could actually help us calibrate sort of priors on that sense, too. Is there a value in like survey data, or would that be just too viable? I'm not sure. Yeah, think about it. But yeah, I would probably. But yeah, I would probably do a sensitivity. And I know for a fact it's non-random. People are withdrawn from the trial if it's unethical to keep them in the trial because the treatment's not working. And those patients are definitely sicker than the patients who are staying on the trial. All right, well, thank you. Yeah. Thanks. Okay. Thanks. Thank you very much. 