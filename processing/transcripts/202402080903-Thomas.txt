Thank you very much, Carla, for the introduction, and thank you to the organisers for putting on this meeting and for inviting me here to this wonderful place and for giving me the opportunity to speak. So I was asked to give a talk on effective feeder world team and also possible aimed at, well actually graduate students and postdocs, but there are definitely experts amongst those individuals here. So I took that to mean though, maybe not experts. So what am I going to try and do here is place a particular Place a particular result in some context because there's obviously the history, what is Feudowilki theorem, there's the effect of Feudowilki, and then since this result that I have in mind is maybe already a year or two old now, and there have been developed some things in the meantime, I'm hoping at the end also to say something about those as well. So let's see how this goes. You keep me on. All right, so graphene sets, there's a few things that we need to cover here. And the first one of these, The first one of these, Patrick and Sets, has already been mentioned a couple of times this week by Patrick and by Kim in particular. So let's just start there as our prologue. It's some sense a suitable setting for effective. We'll talk more about what that means in a minute. A few hints have already been given. Introduced by Hervansky, they are solutions to triangular systems of first order polynomial divergent equations. You've already seen the definition at some point this week, but I will give it to you again for my own purposes in just a moment. Again, for my own purposes, in just a moment, but let's cover the bases on the first slide. They are analytic by definition and o-minimal by Wilkie. For example, the real field expanded by all total button functions is an o-minimal structure. And they include the exponential functions and some other things there that I always put in talks so that people can amuse themselves trying to prove that they don't want to investigate the talk. And here is an example as a moti sort of slightly motivating example because Sort of slightly motivating example because the words diaphantine applications appear in the product of elliptic curves, g elliptic curves, defined over the complex numbers, and then the graph of its exponential map, which is given by the Weierstrass functions and the elliptic curves when restricted to a certain fundamental domain, is a set defined by Fafian functions with some effective complexity depending only on G. Now, there's lots of things in that that I have not yet. Now, there's lots of things in that that I have not yet spoken about, so we'll get to more on that story later. Okay? But that's maybe something to have in mind as we go along. Alright, so definition. I just want to give you my definition that we work with, just so I can make some more things precise later on. So here it is again: definition of a Fafian chain. So we take a simple open set in the reals. I won't define that precisely here. You can think of it as a product of open intervals. You can think of it as a product of open intervals. Basically, I forget who said, just do the thing that means that you're not stupid. Make sure that the result that we want that's going to cover the bottom of the slide, in fact, holds on your domain. You have a Faffian chain, if you have a bunch of analytic functions in a sequence in order such that there are polynomials that give you this triangular first-order differential equation of each function's derivative, first-order derivatives depend only on itself and Itself and the preceding functions in the chain and the variables in a pole afterward. And then, okay, so this is the format of the definition due to Gavansky. I think he didn't actually define a Fafian function in this format, but he works then with functions that are defined in this way. They are polynomials of functions of the Fafian chain. So let's say that, so this is now commonly understood to be what we mean by Fafian function, although it's equivalent to do exactly what Patrick did, which is just to take something in a Faffian function. Which is just to take something in a Fafian chain. But I'm sort of trying to be sort of true to the original presentation just because I want to state the motivating thing people keep referring to as Havansky theory, if you've heard that during the course of this week, which is this, although I am not stating it precisely, so I guess I'm also cheating. If you take a collection of G1 up to GM, which are all Zafian, and they have a common chain in the background, the F1 up to FR, then the number of non-denerate roots of this Then, the number of non-degenerate roots of the system, where they're all zero, is bounded from above by an explicit function, which I have not written here, of the complexity of the system. Okay? So what does complexity mean? That's the first thing I need to clear up. So per Havansky, well, I'll put it on the next slide, but just while we have this all here, I've highlighted a few things there in this gold colour. We have the number of variables n, we have the number of functions in the chain r, and we have something to do with these polynomials, the phijq. Okay, what exactly? Okay, what exactly is that? Let me try to repeat enough on the next slide from this that I can tell you more precisely what is going on there. So there's the setup, u simple open domain, and then a Faffian chain F1 up to FR given by the polynomials Pij. This is the same statement of the theorem, except that now I've made it explicit that GJ for each j is given by a polynomial called Qj. And Havansky defines the complexity to be the following collection of numbers, so it's actually several. Following collection of numbers, so it's actually several numbers, namely n, r, and the degrees of the following numbers and qj. Okay, so that's there's data in this differential equation system which we can extract, and that's in some sense the complexity. Now, for our purposes, we have a slightly different focus, so I want to be a little bit, I want to define complexity in a slightly different way, which is in some sense a little more precise, in some sense a little less precise. So, here is the definition of complexity we will use for the purposes of this talk. For the purposes of this talk. So, if you just think about one Faffian function, g of x, given by a polynomial q in terms of a Faffian chain, f1 up to fr, there are two different notions that I want to consider. One is called the format, which is just really supposed to be just a bound on the number of variables and the number of functions in the chains, and the degree, which is a bound on the degrees, but in a way that makes sense if you really think about what degree means, and you think about zero sets of these things. And you think about zero sets of these things and how they should behave. So it's the sum of all the degrees of the polynomials inside. So, in particular, I'm no longer distinguishing between the sort of role of the Q and thus the role of P's. So, Patrick's definition is really fine for the present purpose. Just to take a vacuum function to be something in a vacuum chair. But I want to distinguish between these two things because they play different roles in the balance, and that's going to be important for applications. Okay, so I think another thing I should probably say is. I think another thing I should probably say is Gauss's observation, which is really also the point here with formats, you can also think of a bit like dimension, which I think gives a good intuition as well as we go along. Okay, so we've got a sort of notion of dimension for a function and a notion of degree for a function in this context, the Fafian function. Okay, that's Fafian functions. Fafian sets is what was in the title of the talk. So let's start with the restricted Fafian sets. So you fixed your Fafian, you can cover a Fafian chain. Actually, Actually, yeah, probably I don't want to fix it. I went around writing this slide a few times, and yes, there's different ways you can do this, and I think I've ended up with an amalgamation of two things. So maybe don't fix the chain. But fix the U that we're working with. Just because of everything else that's on the slide, that's the way to clean up what I've got. And then fix an open box B whose closure is inside of that zip at the moment. Okay? So a restricted semi-Fafian set then is some subset of that B. Is some subset of that b of this form. It's given by a bunch of Faffian functions either being equal to zero or being positive, strictly positive. Okay. And then we can define these two notions, this format and degree, these two measures of complexity for such a set X, just by looking at those Faffian functions, the G's and the H's here in the definition, for which we already have a notion of format and degree. So take, again, So, take again in such a way as that the format behaves a bit like the mention of the degree behaves the way a degree should behave. So, take the maximum of the formats inside and the sum of the degrees inside as the two definitions. Okay? So, then we can impart a definition to these restricted semi-Fafian sets. And from there, we can consider restricted sub-Fafian sets, which are projections of semi-Fafian sets. Semi-Fafian sets. And the classical thing to do is, as I've just, which is what I'm defining here, is just to say that a sub-Fafian set downstairs just inherits those two measures from measures of complexity, performance and the degree from the set upstairs. So just take the same numbers downstairs. The important thing here, which we'll return to later, is that notice that in this definition, exactly what B is as a subset of U is not part of the definition of the complexity. Part of the definition of the complexity. It's all to do with the data that's coming ultimately out of these differential equation systems, which none of which depends on anything. The only thing it depends on that has anything to do with B is the number of variables, so the ambient dimension, not B, the exact choice of box B itself. Okay? And let me say something that looks a bit non-theoretic. This collection of sets that we've just defined, the restricted self-affection sets, they are the bounded sets. The restricted self-faffing sets, they are the bounded sets definable in this structure. This overhaul structure are the restricted faffian fractions. Okay? Alright, so there's something else I'm going to need because this is obviously restricted by nature. We can also consider this without this restriction, which we're going to want to consider both. So this is the easy way to do it is to define restricted first, because to define it without the restriction, let's just remove the restriction. Alright? Let's just remove the restriction. And the definition, otherwise, is identical. So just forget about the fact that there's this box B and we're just working that U. Okay, and all the other definitions, everything else is the same. You've got your semi-Faffean set, you inherit the format and the degree somehow from the Faffian functions dividing it, and then the sub-Faffean set is a projection of the semi-Faffean set. Literally, take the same format and degree from the Gauss steps. And in this case, well, we can't say something sadly quite as nice as. Say something sadly quite as nice as in the movie's case model theoretically, because of graph insets, they're certainly definable, they're not fair. This would be, these two collections would be one and the same if we knew that our faith will model complete. That's an open question. I guess that's an open question. That's not ready for a question. Okay. That's it. Well now read the question. Right. So I think here endeth the prologue. And so my hope is that, unlike, you know, like a Cohen Brothers movie or something, I will actually. Like a Colin Brothers movie or something, I will actually be able to explain what that has to do with the rest of the talk at some point later. You won't be left in suspense. Okay, so let's start with the main part of the talk, which is really about the two of people. Okay, so again, apologies to loads of experts in the room and online. You don't even have to quite be different. So this is just for people that maybe somehow not seen this yet in their young mathematical lives. So what are we interested in? We're interested in counting the density. We're in counting the density of rational points. Let's say for now, that's points with rational coordinates lying on certain sets. And we want to be in a situation where we can actually say something, so we are going to work in a minimal expansion of the reals. And we are going to throw away any obstructions to the nice bounds that we're thinking of. So we're going to make sure that we don't have too many points by removing algebraic contents to our definable sets. So we get this transcendental part of x. By removing infinite. Of x by removing infinite connected semi-algebraic substances. So now we have a hope of being able to say something. This is what one can say. Fix an epsilon greater than zero, it's epsilon because you should think of it as being small. And with that exponent epsilon in mind, there is some constant, depending somehow on the set x and on the choice of epsilon, such that, okay, this could be more simply stated, I will say it in words. Be more simply stated, I will say it in words more simply. If you look at x, you look at this transcendental part, and then you look at the points on x, all of whose coordinates are rational numbers. The numerator and the denominator of all those rational numbers should be at most some h. So you fixed your h. For a fixed h, there are only finitely many of these rational points. And you're interested in knowing how many of those lie on this transcendental boundaries is x, how that grows with h. How that grows with h, as h goes to infinity. Another way to say that is the points of height at most h, the rational points of height at most h, lying on x tracks. Okay, and there's the definition phi. And the answer is that bell, ch the epsilon. Okay? Let's talk about that. That is, that's the focus here, the ch length. Now, brief remark for those who have not seen this before. The bound is known to be the best possible settlement of generality. That is to say, the statement is for all ominable expansions of the real field. And there are counterexamples, sort of artificially constructed by hand by Pat Philip, which happen to be definable to any improvement to that, as in a slower growth rate of H, like a power of the logarithm of H. Okay? But I haven't put this on the slide, but I wanted to remark. I haven't put this on the slide, but I wanted to remark for many years it was conjectured, indeed, conjectured alongside this theorem by Wilkie, that in certain situations, like the real exponential field, you could in fact improve this to a power of the logarithm. So some progress was made in this direction, and then it was slow for a long time, and so question interest turned to other questions like the one I want to talk about today. Like the one I want to talk about today, but I hope to come back to talk about that at the end. Okay, just a remark, though, about what one can do to improve this theorem in a different sense. One can improve, and it was proven by Piler, that instead of considering rational points, the same bound, instead of considering rational points, you consider algebraic points of bounded height and degree, or semi-rational points were considered by Hamilton and Pilar. Same bound, semi-rational to mean some of the coordinates are rational or terms of the algebraic in terms of something called block counting. In terms of something called block counting, if you've ever heard of that, that's where those that term comes in. I'm not going to state things in terms of that, just slides. There's not enough space on the slides to make the statements. I'll try to keep the statements in the same type as the one here, just for the simplicity. Okay. Right, so let's think about a different problem, which is the central one of this talk. When can you make these bounds effective? What does that even mean? All right, so it means in the It means, in this particular case, when can we find this constant c effectively in terms of suitable input data from the set x and the need from epsilon? And it's clear epsilon's a number, right, so that's okay. But what does it mean to have input data from x? And the goal is, the viewpoint is diaphanic applications. Why? Because there are many diaphaning applications of this theorem in its ineffective form. I'll try to motivate for those again who don't think. To motivate for those again who don't think about these kinds of problems very much, what's going on here. So, many of these applications, so-called unlikely intersection problems, have conclusions of the flavour, there are finitely many points such that, and something can be concluded from that fact. And effectively, the goal is somehow effectively in the Kila-Wilki theorem could, as long as all the other ingredients were also effective and had a favourable limit and so on, you could change that to there are at most this many points of a certain kind. And that could even lead to an explicit. And that could even lead to an explicit description of them. So, again, an interlude with a view to some of the further applications I'll talk about later on, let me just give you, in one slide, the standard basic example of an application of this theorem, which is the very first one due to Pilo and Zanio, which is a reproof of the so-called Man-Number conjecture, which was already the theorem you already know. So, I've stated a version of that here, which is the one that most closely matches what I want to talk about later. Closely matches what I want to talk about later. And again, maybe lots of things on this slide that something you've got no idea about. I'll just try to give you a flavour of what's going on in one slide. Let's see how this goes. Okay, so take an abelian variety, for example, of elliptic curves, like I mentioned earlier, and take an irreducible sub-variety of that thing, both defined over a number field. And now there's some assumption on that variety. It doesn't contain a torture translate with positive direction of being a sub-variety of A. Then V contains. Then V contains only finitely many torsion points of X. So we're counting torsion points on this thing, and we have something as a supposition which is supposed to remove the possibility that you actually have lots of these torsion points. The proof idea that has some such, what does this have to do with counting rational points, upping final sets of abnormal structures? There is a way to, a correspondence between torsion points of, let's say, given Of, let's say, given exact order n on the v, the variety here, and rational points of the same height on a definable set, definable set, which happens to be definable in R. Okay? And because of this condition, where we've removed some risk that there might be actually lots of these points, same as we remove the algebraic part of a set to leave ourselves with a transcendental part of a definable set, to remove the risk. Of a definable set. To remove the risk that there are lots of rational points, it turns out that these really are sort of corresponding conditions. And because we made that assumption, they proved that this set S actually is its own transcendental part. It has no algebraic part. We've gotten rid of any potential problematic part. And so, counting point of view. And so this Peeler-Wilki theorem applies actually to S itself, and so it. And so it bounds from above indirectly via this correspondence: the number of torsion points of exact order n that lie on b. Great. There's something else that needs to come into the story at this point, which is that there are known lower bounds for the number of these points. So if n is allowed to be arbitrarily large, or could be arbitrarily large, then there would be a contradiction here. These lower bounds, the so-called Glauer bounds, so that can't happen. n has to be bounded. The order of torture has to be bounded. The order of torsion has to be bounded. And because there are only finitely many points of any given order of torsion, they actually have to be finitely many points. And that's where the conclusion comes from. But you'll notice that the conclusion is actually that there's a bound on the order of torsion hidden behind this. And so the question, my reasonable question is, what is that bound? Right? Not just that there is one, but what is it? And I'll say more about this later. But that's just something to have in the back of your mind. To have in the back of your mind, let's say. And as I say, many of you know this already, but for those who don't, that's too much to try and type in in two minutes. But maybe there's something you can hold on to there, hang something on later on. All right, let's go back to talking about the Peter-Bookie theorem and effective. Okay, so there are a couple of things that we might have to worry about here, and I've already hinted at some of them. One is, one is, what does it even mean to have input data from X and be effectively computable? Computable in terms of that input data. But perhaps because of the prologue, you might have an idea of where this is going and where we might have such a measure of some kind of complexity data. But then there's also another problem, which is if you just want to look at the proof of the theorem in the original setting, is it effective? Are there obstructions to effectivity? What's actually going on in the proof? Okay, so how did I decide to do this? Years. This is the theorem up front. Okay? So, not talking about all our minimal expansions of the reals, we're talking specifically about those sub-Faffian sets, the unrestricted ones, which all live inside of R with Faffy functions. Now, this is a bit hard to pass because I've tried to be very literal about what depends on what, but the idea is simply that if you have one of these sets, you know it's formatted, it's formatted. You know it's formatting, it's formatting a degree in the background, F and D, or bounds on those things. And you've still got your epsilon from before. And the idea is simply that you get the correct bound, CH to the epsilon, but where C is a function of the format and a function of the degree of the set, but specifically a polynomial dependence on the degree. That's what it says. Sorry, it's a little difficult to. Sorry, it's a little difficult to write that down in a nice way that makes that obvious, but that's what we're going to talk about right now. So that constant has several features that aren't present because they'll probably be not looking for them, but you know, and I mean when applied to the special case, the original theorem when applied to the case of subfapping sets. And one is that it's uniform in the format and the degree, right? It doesn't, the thing that, what is the input data from X here is the format and the degree, and you might remember that goes back to the Fafian functions defining it. Goes back to the Fafian functions defining it and the data from those differential equation systems. So if you have another collection of functions defining another set and it happens to have the same data, the same number of functions in the chain, the same balance in the degrees of the polynomials, you'll get the same constant C. And as I said, it's polynomial, this C is polynomial in the degree of the setbacking set. And the thing to know here, if this is new to you, is that in the diaphaning applications, In the diaphantine applications, the reason we distinguish these things is because the degree of the set here, in the sense of faffine and degree, somehow corresponds to degree of, let's say, variety in the anograph, for example. And that's where this shows up in the applications. So we can get better control of what's going on with regard to the degree in the applications by understanding what's going on with the degree. And in particular, the fact that this polynomial the dependencies polynomial in the degree is very nice, as I hope to sort of show you later on. To sort of show you the other one. Okay. And just a side remark, because it's not the main focus. We can't, because remember I said, of course, model competence of RFAF is still open, so is this everything in RFAF? Well, oh, no, that's where I'm going. But before, apologise, what I've put on the slide first is just to say, if you know about these other things involved at counting algebraic points and rational points, I already said it, I wasn't going to make the statements more generally, but they also apply. More generally, but they also apply. We get those versions. So, but that I said it before. But the other thing that we also get is a version of this statement for the more general setting of RFAF, but without the polynomial dependence on the degree, so it's not so good for applications. But in case that might be useful, I'll mention it. Okay. What else? Context, right? What was already known before this result and This result, and some of these things are slightly in an orthogonal direction. You know what I mean? There's some overlap, but there's also some distinction. For example, Beni has a result which actually does get the power of a logarithm instead of the space of the epsilon bound. In the Netherian setting, what is the Netherian setting? If Fafian functions are defined by triangular systems, so each function's derivatives can only depend on itself and the previous functions, throw away that requirement so that the system is. Throw away that requirement so that the system is no longer triangular, but otherwise keep the definition the same. You don't get these same nice effective bounds, but you get, you can still sort of define a notion of complexity. And if you do it slightly differently and you also involve the heights of the coefficients that Bob Ellis involved, then you mean he gets this result that actually a better bound in terms of dependence on h. And we, together with Garrett-Jones, we already had a result for the same setting but sets of dimension up to two. Up to two, and two, there's no real restriction there, as you can see. The result is better here. We did not have the polynomial dependence on the degree, and the method was cumbersome to apply, and that's probably why we stopped it. Dimension two. There was a better way to do things, and so we did it. I get all dimensions here in this result. Okay, so, right. One of the things we wanted to talk about was what goes into the proof. Right, we now have a context to work in, the notion of. Have a context to work in a notion of complexity, but how do we get an effective bound? What goes into the proof of these things? What is still left for us to do when we think about the original proof of the field of working theory? So quick overview of the things that go into the proof of the field of working theory. I've tried to put a lot on the slides, I'll probably go through some of these things fairly quickly because they aren't so much of concern for this particular problem. Already known, something like that. But nevertheless, if you haven't seen this before, you might like to know how to orient. Seen this before, you might like to know how to orient yourself through this proof. So, the first part is what I call the Diophantian part. This is the part that uses something like the Mongieri P-bit determinant method. So, forget about definable sets for now or subfaffian sets, and just think about images of function maps satisfying some nice smoothness conditions. So, I've tried to write this in a very general way, but I'm not sure if that necessarily conveys everything. So, I'll say specifically what it means for Piggy Log in a second. Because this is at least the strategies are also used for similar points. Are also used for similar point counting type results. I'm just trying to be quite general about this. So some smoothness conditions are satisfied by your map phi, and now you're looking at the image of phi as your set, and you're counting rational points on that set. Okay, so that's why you're interested in the rational points here. And the whole point of this step is to say that they lie in a certain controlled number of zero sets of algebraic factor spaces. Okay? So what does controlled number of algebraic factor space? What does controlled number of algebraic number surfaces in the context of PilarWilki? That means order h to the epsilon. Except I've written it here, I want to say something about what goes into this step, so I've written it slightly differently because maybe that's not the same epsilon. In fact, the epsilon is going to depend on the degree. So you should think of this as saying, I want to put all the rational points in this image into a certain number of hypersurfaces of degree, let's say D. I pick my favorite degree. Okay, and then there's going to be an epsilon that. Okay, and then there's going to be an epsilon that works, it's epsilon prime. And the key feature is that that epsilon prime is going to get smaller as your D gets larger. A bit higher degree of polynomial, your epsilon prime or a bit smaller prime. Okay, and once, so now we have a D in mind, a degree for these polynomials, these hypersurfaces, then C will depend on that D, this C here, and then the smoothness condition will mean something in terms of D, in particular this, that from. This, that phi should be r times differentiable, well, r depends on b, and up to order r, the derivatives of phi should be bounded by 1. Okay? So there's some condition there that again depends on b. All right. The good news is, it may be a little painful at first sight, we don't need to worry about this step really at all, because this part, you just look at the original proof and you can see that it's effective. How does it actually work? We're given an epsilon. How does it actually work? We're given an epsilon in the statement of the Punelabhi theorem. So pick a degree D to work with in terms of that epsilon so that the epsilon prime you get from that D is actually smaller than the epsilon you started with. So then you can forget about this epsilon prime and just work with the epsilon we were given initially. But then you've got a degree d that you're working with, and everything else, the C and the R, the condition you need, everything then is okay defined. It's something in terms of. Divide. It's something in terms of d, but that's really in terms of the original epsilon you started with. And everything there is effective. So we've got a lot of information here. This is the key thing: the ch to the epsilon, original epsilon, many hypersurfaces, and c is given in terms of epsilon, and the order of differentiability that we have to worry about is given in terms of epsilon, and that's all effective. Okay. But now we have to worry about not just functions that are nice like this, but then the next step is just to. But then the next step is just to try to see that your definable set is covered by functions that are nice like this. And that's the second step, which is the parameterization part, which is except this theorem says exactly that. Except because I made the functions all bounded, have derivatives bounded by one, that includes the function itself. So there's a small technicality here, which is that now we're only worrying about functions of 01 to the end, but really all you have to do is fold up your original set into that box and Box and everything that was rational of height h outside is still going to be rational of height h inside, and you don't have any pieces, it's like 4n something like that pieces. And so that's not going to make any difference to the bounds. And this is saying exactly that. We can cover our definable set by functions of the required niceness for any order r that an r is going to be given to us by x in the course of the book. Alright, so what we need to worry about this then. Alright, so what we need to worry about this then, if we're going to prove an effective version of this result, we need an effective version of this. What does that mean? It means where the size of the parametrization, the number of functions covering your original definable set, is somehow effective in x. And of course, in x1 as well. The problem is that the proof uses compactness. Intricate inductional dimension, it's ineffective because, in addition to general o-minimal theory, which is also something I'll have to worry about, right? Cell decomposition, which I'm going to say. Will have to worry about cell decomposition, for example. At the inductive step, we use compactness for a certain purpose, use it in order to represent families of parameterizations of fibers uniformly in parameters. So there's something we have to worry about, compactness. If we're keeping track of what we're going to have to worry about, that's the takeaway from that. And then there's another part, which is that we've reduced the problem, but not entirely, right? We've reduced the problem to counting points that lie both on these nice functions. That lie both on these nice functions, the images of these nice functions, and on algebraic hypothesis. So now we need to count points on those. But obviously, the whole point of doing that is to make probably counting easier. And so that's the last step, so-called zero estimate step. And for Pele Wilkie, that means, of course, bounding that number by CHP epsilon, where C is given in terms of epsilon. And again, this is, I think, uniform finite is essentially what's needed here. So cell decomposition, again, can. It's needed here, so cell decomposition again comes in. So we need to worry about two things: an effective way of using what I call general o-minal theory, but probably really don't mean cell decomposition, and a way to avoid compactness, but still the same parameterization is uniformly found. Those are the two things we picked up along the way that we're going to have to deal with if we're going to make an effective version of the other one. So far, so good. I know this is quite rapid, but I hope you can just see where these little highlights are coming and because we're going to see how to deal with it. That's the job. Okay? Okay, okay, so back to the theorem that we're trying to prove. Right, so what is our approach to proving this theorem? Bearing in mind some of the things we're going to have to worry about in proving it. Well, the first thing I have to do is actually tell you that we do prove this theorem, but we do it by reducing it a few times to something that we will actually be focused on proving. And the first reduction is just to worry about not subuffin sets in general, but the restricted ones. Restricted ones. Why is it enough to worry about just the restricted sets and still hope to get an unrestricted result? That's not usually how this works. Turns out that it actually works in this case. So what we do is we prove this, where instead of subfaffen, you've got restricted subfaffene. So let's say we did that. Come back to that in a minute. Now here's the trick that works in this case. Take an unrestricted subfaffene set X, fix a height H, right? And then restrict the domain of all the defining functions to some box. That's going to make a smaller set, y. But if you don't restrict too much, you can still keep all of the rational points of 5h that you wanted to be counting. Because there are only finitely many of them. So just make sure you don't restrict too much so you don't lose any of them. Okay? So we're counting all the same points that we were counting before. If we count them on y, this restricts. If we count them on y, this restricted version instead of x. And the point that I made earlier is that the format and the degree of these things do not depend on the restriction take it. And we've got a restricted set. So y has the same format and degree. And in particular, anything we've learned about it. So it doesn't depend. Excuse me. If we apply our bound, the bound does not then depend on Bound does not then depend on the restriction taken. So it also applies to the original set. So it does actually work in this case because the bounds that we get do not depend upon the restriction taken. Because the format and the degree in the restricted case does not depend on the restriction taken. Something particular to this context, I guess. Okay, so we get the theorem for the unrestricted case if we can get it for the restricted case. So that's what we now think, this is what we think we want to do, is prove exactly the same statement there, but we'll. Same statement there, but with restricted self-raffin instead of self-raffic. The reason for doing that partly is because that's where we could prove things. But we have to be a little careful. Okay, so here is, so we're now working just in the restricted subaffin case. Restricted faffion case, restricted sub faffion, or you could just think of R, there's restricted faffion factors depending on your reference. Okay, remember that these are the things we need. We need infective. These are the things we need. We need effective cell decomposition, maybe, and a way to avoid compactness. So, dealing with the second one first, because the answer is fairly boring, just you want to work in families, work in families the whole time. And it's extremely messy, but it's just a question. Having the patience to write everything down carefully, which quite a while, but you can do it. That's not really actually a big problem. The bigger problem is the effective version of ominormal theory, the effective cell equality. Version of ominormal theory of the effect of cell decomposition. But the good news is that, in conjunction, really with what we were doing, Vinymini and Vorobiov worked out a way to get an effective cell decomposition of the right kind. I need to be quite careful about this. We're finding some notion of Gabriel and Voribov. So, Gabriel and Vorbiov had a notion of effective cell decomposition for faffian sets, in particular restricted frappion structure, but some But somewhere in the background, I keep emphasizing for some reason we care about polynomial dependence on degree, and it doesn't have that. So let me try to explain that. First, though, the way you get a better result is by changing your notion of format and degree. So let me try to say briefly what that means. For the purposes of the talk, to be honest, that's enough. You have to change the notion of format and degree in a certain way. The notion of format and degree in a certain way. And we'll come back to this later, which is why I'm actually writing it out explicitly, but we don't need to know exactly the explicit details for the purposes of this talk. But the idea is that before, we just took a sub-Paffin set, and we thought, okay, it's a projection of a semi-Faffian set, just have the lower guy downstairs have the same format of degree as the guy upstairs. But now, we want to be a bit more subtle, and this might make the numbers worse, but in a controlled way. And again, control zero is going to be its worst polynomial. It's worse only polynomial and the degrees. So now, what I'm saying is that you take your restricted self-affection set and you write it as a projection of connected components of restricted self-afraid sets. And then the new thing, the star format, is the maximum of the original formats of the XI's that you're using. And your star degree is the sum of the original degrees of these guys. Again, for the purposes of this talk, it's probably mysterious as to what use this is, but um Use this is, but I should also point something else out because this can get a little bit confusing at first, second, or third reading. In either case, right, sets could have a whole bunch of different formats and degrees. It depends on the presentation that you choose. And the goal is to say, oh, I have a set which has format at most this, or star format at most this, star degree at most this. It means I can find a way to write it, present it in this fashion. It presented it in this fashion, that means I can then say that it has those numbers for its format degree. Okay, just okay, so there's no uniqueness about this, but if we do this, then the nice thing is, using some work of Cell, the point is that if you have a restricted sub-Faffian set with a certain format F and degree D, then it has star format actually F and star degree that's polynomial. And star degree that's polynomial in degree D, really, because this is Cell's work is on the number of bounds on Betty numbers with restricted sets. So, sorry, restricted Faffin sets. So this is already the key start, is that things maybe get worse, but I mean, polynomial way. And fortunately, they get better because they let me prove things. And so what we actually prove is the version of our statement. R statement in terms of star format, star degree, but that implies R statement, because if you start with a set with format at most F, degree at most D, well, then its star format is at most F, and its star degree is at most polynomial in D. And then your bound will depend polynomial on that polynomial in D. So it's polynomial in D. So we'll find. Anyway, I'm mentioning this so that I can say some things later on. They're not so important for the purposes of understanding our results today. Understanding our results today. But here is the cell decomposition result, which again, independent interest. This is cell decomposition for a collection of restricted self-affeement sets. Okay, so if you know they're bound on their format, star format, and star degree, then there's a cell decomposition, and the key features I've tried to put there in gold. The star format of the cells depends only on the original star formats of the sets. The star degree depends. The sets. The star degree depends polynomially on B, and the number of cells depends polynomially on D. Okay. So if you work with the original format and degree, this is, okay, so I haven't attributed this to Gabriel and Vorobyov directly because they have a weaker statement which also involves some kind of linear map. But I think that the claim of Vinyamini and Vorobyov is if you sort of followed their approach, then you could get rid of that aspect of it, but you wouldn't get the nice result. But you wouldn't get the nice result because you would get this problem: that the format of the cells you get would depend on D. And think about applying a cell decomposition and then applying it again to get a refinement of your original cell decomposition. The problem is that where here it says, oh, that the star format of the new cells depends only on F. If they actually depended on D, well, then when you feed this through, you would lose the. This through, you would lose the polynomial dependence on, for example, the number of cells here. Because now things would depend on the format, which from the previous step depends now on the degree, and everything goes that way. If you want this polynomial dependence on D. So if you want to iterate these things or perform operations on your cells or do anything, you need to have that there are polynomially many cells in D, as at the format, and zone on F. That wasn't the one-hour thing, though. Something else. I could have realised it. Something else is done. I kinda realized that's a good problem. I've got a chunk of good things. All right. So you get this. So this is the basis, essentially. This is the general o-animal theory, if you will, that we're looking for. Okay, so let's just let me just walk you through the thing. We've gathered everything we need to do, and now I hope you see that we've kind of gathered all the pieces that we need. So let's just walk through this. So there's just one ingredient. So, there's just one ingredient I should add, though, which is very important for preserving that polynomial dependency degree as you use that cell decomposition result. There is a bit more to prove. But the good news is that in order to prove this effective CR parameterization result, so this is for restricted South African sets, that's the effective CR parameterization results I mean, as I said, to avoid compactness we work in families throughout, and to preserve that polynomial. And to preserve that polynomial dependence on degree throughout this complicated induction, actually we use an improved version, a refined proof. It's an improved statement of the CR parameterization. Due to Vinymiovikov, where the parametrizing maps are what they call cellular, where instead of just covering your set by these functions from 0, 1 to some k, the dimension of x, each coordinate map of each parametrizing map depends. Of each parametrizing map depends, so if the ith coordinate map of a parametrizing map depends only on variables x1 up to xl, and it's injective in this. So there's some extra subtlety there, which means that when you're taking derivatives of things and so on, lots of terms disappear and things don't blow up. You get to retain the follow-up dependence on the derivative. Okay, so that's very nice, and that helps everything work. Okay, little side remark, because that's the restricted Faffian result, and I already explained how to get. Restrictive Grafian result, and I already explained how to get the sub-Fafian, the nice result there from that. Just with regards to the bigger structure of our graph, we don't have the polynomial dependency degree. You can do something similar, but you use, in that case, you get the more effective cell evaluation, which is an effective minimality due to where it gavriels. But there's no polynomial dependence in the degree point, so that's where it goes away. Okay, how am I doing? I reflected off that, but I think I still have. Reflecting off that, but I think I still have some maybe a few minutes. Okay, fantastic. So, I wanted to talk about two other things, just so that you know where this is going. One, the third part of my title, which is Diaphaene Applications. I already hinted at that, but let me say what we can now do with this result, or some things that we can now do with this result, and then let me say something about the story since then. Okay, so I will probably have to skip over one or two things here, that's fine, we can skip that. That's fine, you can cut those later. So, here is sample application, number one. I mentioned the manomorphic project earlier. There were, before us, there are effective versions of this now. But this is the bound that we can get using our nice effective field-worthy statement. Okay, so you've got your product of elliptic curves, now they have to have complex multiplication. I'm not going to say anything about these things, the things in the I'm not going to say anything about these things, the things in this statement that I haven't already said anything about. The key thing to focus on is before we said, okay, we get a bound on the order of torsion, this is the bound that we get. And it is polynomial in the degree of the variety, which is coming out of the polynomial dependence on the degree from Philomurky's table. Okay? And I promised something from the very first slide. What does this use? Well, there's an effective gauge. This used? Well, there's an effective Galois bound analog due to Gao, and then, sorry, this is not very well written, so I have to tell you what it's supposed to say. The Gal bit is the Galois bound, and then the Joan-Schmidt is the thing I mentioned on the first slide, which makes sure that the things that we need to work with are actually strictly Fafian. So here is a more precise version of what I said on the first slide, that this thing was somehow Fafian defined from Fafian functions with some effective complexity depending only on G. This is what it actually means. This is what it actually means. You fix your for each vertical curve, you fix the standard basis for the lattice, and then you can define this thing. And this is a more precise version of what I said on the first slide, and that's restricted to every traffic and set, with format depending on the G, and degree, which is polymer. Okay, so that's just some ingredients that we've seen along the way and how they go into this, but of course, I'm not saying anything about the group here. And in particular, And in particular, I'm not going to say anything about the other application I have in mind. Maybe anybody wants to see this slide more quickly, more slowly, sorry, they can ask about this at the end. But we also have, if this is in any way familiar to anybody, there's an effective version of this result of Patrick as well. Okay, let me say, try to say something about the story since this work, although it's not actually in published form yet, but if any like that won't take too much model. So, epilogue. Take too much model. So, epilogue. I mentioned a conjecture at the beginning from Helpy that in the specific case of Rx, we should be able to improve the bound from h to the epsilon to some power of the logarithm of h. Okay? And it turns out that that's actually a theorem. That's true. And what I think is fascinating about this is that the proof essentially is inspired by thinking about this different problem of effective bounds just for the Peter-Wilkins here. Okay? Okay? And in particular, tricks like that trick in the middle that I talked about to get from the restricted to the unrestricted case. Where if you have your bounds in the restricted case in a sufficiently uniform way, you can extend them to an unrestricted case. Fantastic. Okay. So there's more work involved, of course, but the theorem is due to be no contact. And there's the statement. More precisely, there's your power of the logarithm that I keep talking about. Power of the logger than I keep talking about. And as I say, they first establish an effective version of this for restricted sulfafrian sets. So actually, that improves our effective feelability for restricted sulfafian sets. But there are ingredients that we use that they don't improve because they have to go by a different route, of course. And for example, they have a sort of improvement. The kind of parameterization they use is actually a weaker statement, but it's sufficient for the purpose because it has features that are improvements. Are improvements in terms of the number of maps you need, but they don't cover the whole set, for example. So there's some trade-off going on there. And so the effective CR parameterization is still sort of still standard view. And there I said at the end, that's how it kind of cuts. That's the mote. I said the problem on the slide. Okay. But there's more to this story because the way that they present their results indicates a certain philosophy that may be the philosophy. Philosophy that may be the philosophy that people use going forward to think about these kinds of results or trying to attain these kinds of results. And it's been mentioned a few times during the course of this week, so I thought I should say something about it, which is this idea of sharply or minimal structures. Okay, so the philosophy is the shift in perspective. Now, start with any unminimal expansion of the reals. Okay? And suppose you have assigned all your definable sets a notion of format and delivery, which could. Full matter, which could be completely arbitrary, but when I say, I mean, there's a very minimal requirement, which is just that if you pick two numbers, let's say f and r, and you think of all this, and you designate some sub-collection of your definable sets by f and r, you should think of those as being the sets with format at most f and degree at most d. So that set collection should live inside the one for, let's say, f plus 1 and d. And also for f and d plus 1. If you're a format at most f, you're also a format at most f. Format at most f, you're also a format at most f plus one. If you're a degree at most d, you're also degree at most d plus one. So as long as you you kind of organize your allocation of numbers, and that's what I mean by assigning a notion of format degree. Of course, that's on its own not enough to do anything. But what they define is that a structure sharply or minimal if this assignment of format degree obeys certain axioms which are motivated by desirable conditions for diaphanic applications, like the kinds of things we'll be talking about today. So, for example, the zero set of the polynomial and degree. Of the polynomial in degree D and n variables should have format at most A and degree at most D. Definable subsets of the line. Ominomality says that there are finitely many connected components, finite union of points and notebooks. This says that if they have format of most f and degree of most d, then they should have k connected components where that number is polynomial in the degree d. So that's quite a strong condition. So that's quite a strong condition, but what you're looking, I hope by this point at the talk, you can imagine that that might be a desirable thing to have. And then some other properties, which are in fact held by star format and star degree, and let you prove that effective cell decomposition yourself, where the number of cells is probably only on the degree, and the format of the cells depends only on the format of the original sets. Okay, two more slides. By design, that means that the results that I've mentioned here, which talk about restricted self-affirmation, Which talk about restricted self-affirmative sense or would apply to such structures. Right? They're the ingredients that you need to get the effective cellular composition and so forth. So, our effective Fuel-Wilking theorem in that structure, the improvements for restricted submitting sets that give you polar logarithmic bounds, our effective CR parameterization, of course, it all comes from the last one, which is the effective CR. And here's my understanding, this is the more recent development still, which is. Still, which is that if you so what they've done is, so this is later in 2022, separate paper, is that they've identified a weaker list of axioms and call structures that satisfy that weaker list of axioms pre-sharp. And the idea is that if your structure is pre-sharp, it has a notion of format and degree, right? You can define a star format and star degree by analogy to what I did earlier, and what I put you through on that earlier slide. And what I put you through on that earlier slide. And if you do that and you work with the star format and star degree relative to the original format and degree, that structure relative to the star complexity is sharply or minimal. So then with respect to these notions of format and degree, the starred ones, you get everything from the previous slide. So what is pre-sharp? What is sharply or minimal? R with restricted factoring functions. And semi-algebraic sets. And semi-algebraic sets, I'm being a little dismissive, but definitely that's a thing. You know, the nice effect of solving composition for semi-algebraic sets gives you that in that context. For counting problems, that's not necessarily so interesting. But maybe, I don't know. And R is the restrictive Franklin function. So there's a question there, maybe. If this is the right philosophy, then the goals will be trying to prove that this thing, that overall structures can be endowed with the notion of complexity that fits this framework. I was a little hesitant to put. I was a little hesitant to put this as the sort of closing question because I strongly suspect that out there these three guys are working on this and have much more that we don't know about yet. Maybe that's, I don't know, you'd have to ask them. But at least for following this story in the future, it might be worth knowing something about this topic and thinking about things from that different point of view. Okay, thank you very much for your attention. Do you have any questions? So for this last one, it's very hard to imagine how to show that an expansion is not sharply over? Very good point. So I was sure I'd be running out of time at this point, so I didn't put any points on the slides. But I was talking to some people yesterday, and I think we've all heard that these guys think they can know how to show that R. Anne is not. Show that Rn is not. And it has, right? So this is an important thing, but I'm not exactly sure how this goes. But I can sort of believe that it could be the case that you could define an analytic function which somehow... The idea, so the thing I didn't say, I just said further properties held by this format and degree. They're very simple to state, they just take the whole slide and we're running out of time. And it's things like when you perform Boolean operations or projections of Cartesian products, the format should only go up a little, and the degree should not change. Up a little, and the degree should not change at all. So, if you take a family, maybe I mean, there's a little bit of fuzziness here because how are you actually even define these notions? I don't want to go out that load, but I'm thinking, you know, if you've got a family, parametrized family of sets, then there should be something you can read off uniformly from the way that the family is described. And I can believe that there would be analytic sets where the number of connected components that they have, for example, they all That they have, for example, they all, in principle, have the same format and degree, more or less, because they're in the same family, but the number of connected components maybe grows exponentially within that family. And that's bad, because the key thing here is this is just stated, of course, for steps in the line, that the idea is that this is what lets you extend that to higher dimensions, and that the number of connected components should still be both rounded in that degree. So, this could be a problem. But I don't. So these guys, but I'm not exactly sure what the precise example is. That's not quite clear to me. But we several of us have believed that this belief heard this at some point from them, and I believe that could be true. Yes. So that kind of thing, maybe. Tom, please. I have two questions. Please. So, first about this RN thing. I thought that they had some kind of theorem that if you know that the structure. Kind of a theorem that if you know that the structure is sharply a minimal, then you can get power of some log bounds and blah blah key. Is that only for pop-in? So my understanding is that version, yeah, I guess I didn't write it up here, but I mean it's it's somewhere and it's contained in the slide. I mean maybe I've overlooked something, but my understanding was that they proved their resolve in the restricted only in the restricted. Yes. No, because sorry, I should be more precise. They proved it for these sharply o-minimal structures. Okay. But if Rn were restricted were sharply ominal, then it would have these kind of bounds, but it doesn't. Rn is opt. If I'm stating everything correctly and I'm a bit nervous about respecting someone else's work, I believe, yes, that would do it. Yes. Okay, so maybe somebody out there will correct me, but that makes sense. But the real question I had for you was: you have. You have, so you have these new results, and you have this nice dive into an application to mana mumford. Can you do universe or mana mumford using results? What do you mean by? I mean that if you have a family of maybe land varieties and family of sub-varieties, then the number of number of points, number of torsion points outside of the Side of the translate technologies is bound. It's a good question and I can't remember anything off the top of my head that says that that, I mean, I don't, it's not something we have. Whether others see how that you could get that from where we are or what their structure was, that's possible, I'll have to ask them. So it's not something we do. So it's not something we do but I don't know how far away that is from the sorry. There are a lot of questions I'll ask. Who's in charge? There are two people that are um Benny Zach once showed me Benny Zach once showed me, like he constructed an example in RN that definitely would like shows that it could not satisfy you. But are there like there could be substructures of Rn that do? Or sorry, redux of Rn that do? Is there only somewhere in between? So the reason that the log bounds don't hold is because of these very often. Bounds don't hold is because of these very artificial examples, right? So, where exactly they live somehow. Can you even make sense of how close they are to be as a self-f. You can't tell where specifically I think these are the things that are yet to be explored in terms of making sense of these notions outside of those. So, at least well, there's not much more that I can add at this point. But I'm sure that other people have ideas about maybe how you could say this stuff, but I don't think anything is that concrete yet. But I don't think anything is that concrete yet. Yeah, it's a good question. Yeah, on slide 12 I got some questions that I got to maybe the question I answered. That was in this. Yeah. So you were saying if X has format less than or equal to F and degree less than or equal to G, so the result only applies to those X's. So those x's. So my question is how do you how if you're given an x subf set how do you determine this that is that is a very good question and that's I think probably why our extra statement is not I didn't let's make it as an effective statement. I think if you know you're looking at a Safafian set and you know it has this format and a degree, then you can take That in a degree, then you can take from that. But yes, at the end, there are all these conjecture statements. You know that it's to be written in that way, but you don't have the effective level completeness that lets you actually identify what those numbers are. So that is indeed well-spotted, if I can say that. I don't have any further questions to the public breaks. Now we are back at 10 years.