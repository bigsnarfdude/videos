So let me start by thanking the organizers because of the invitation and because they are indirectly allowing me to keep a promise that I gave to myself 50 years ago. It was June 1973 on the occasion of my junior high school final examination. I presented a small what we called research on Canada and I said to the examiners that my The examiners that my idea was to live in Canada. And especially, I said something very nice on the Vancouver area. So it's 50 years from then, and this is my first time here. So at least finally, I made it. So, okay, so what I want to say is something about the very classical Helvin transform, not fancy stuff, maybe a little bit generalized. Not maybe a little bit generalized but not singular Radner transform. And what I want to present is some joint work with these people. So Giovanni, who's here, Francesca Bartolucci, Ernesto DeVito, Matteo Monti, and Francesca Odor. So what I want to discuss are some general results on the one hand, and then some special cases that somehow defy the general. Defy the general structures that I had defined earlier. What I want to say is: first, I will start with some very general elementary background on Radon transfer. Maybe I shouldn't do that because you all know a lot. And then I want to present these general results on the so-called dual pairs. And then I want to show how similar results hold for these. Hold for these classes of spaces, so symmetric spaces and homogeneous trees. And these are somehow results that are known to some extent, but I want to give some sort of a particular viewpoint and somehow also discuss a general program that I am interested in, which has to do with transferring a lot of analytic ideas. Analytic ideas that hold in certain manifolds to more discrete structures, to graphs and such objects. And in particular, I'm not going to spend time on discussing that, but I am interested also in some potential theory that is normally developed in these general continuous setups to transfer those to graphs and subjects objects. So, general background. So, many thanks to Samuel Siltanen for these images. So, let's start with the most elementary Radon transformer you can think of. You take a rectangle and you let the angle change and you get the yellow profile there. If you take something just slightly more complicated, like the letter S, and you let that And you let that run, what you get is something quite surprisingly complicated, or at least more complicated than you would expect. And that is a manifestation of something that is quite mysterious. And in fact, in this particular instance, it reveals that the Hellen transform of Superman is Batman, which is quite interesting. And the reason why this is... And the reason why this is to some extent interesting is because it brings about superheroes. And when talking about analysis or harmonic analysis, superheroes are L2 functions. And what I want to discuss are some very basic and elementary results about L2, essentially. Okay, so let's do some honest mathematics. So let me. Honest mathematics. So let me recall very briefly what the Hadam transform is in R2. So this is just a, as we all know, it's a set of integrals over the lines in the plane, and you can parametrize lines using two parameters. One is, of course, a vector on the unit disk, and the other one is a signed distance from the origin. So, of course, there is some parity clearly involved, because if you swap the unit vector on the other side, The unit vector on the other side, and you take minus the distance, and you get this exactly the same i. Okay, so that's one thing, and of course, you can do exactly the same game in higher dimension, and you have hyperplanes, and then again, you need a unit sphere and some type of a sliding parameter. So, these are natural parameters when dealing with very standard Haddon transforms. So, the set of parameters is circle times r. Circle times R modulus, some equivalence relation, some parity. Together with the standard transform, we get a so-called back projection operator, which is just integrating over all the lines that go through a given point, or if you are in higher dimension, all the hyperplanes that go through that given point. And that is given by integration again. Now, in order to Now, in order to invert the Radon transform, as we all know very well, a natural thing to do is to bring in Ries potentials that are Fourier multipliers defined for every alpha less than or smaller than d on the Fourier side by multiplication by mod c to minus alpha. And if you do so, then there is an inversion theorem, actually a series of inversion theorems. Series of inversion theorems that readers follow. That for any alpha, you have a way of inverting f with this very complicated, complicated-looking formula, which brings about that Adam transform and the back projection, and of course, there is potentials. Now, if you choose a very special value of alpha, namely d minus one over two, which is of course smaller than d, then these two exponents. Then these two exponents become the same, and that formula becomes nicer, much nicer. And it becomes, as you see, with the same, let's say I called it alpha dot here and there. So this is one particular instance of a theorem that is due, to the best of my knowledge, to Helgeson, which proves that if you take the mapping that sends Take the mapping that sends F into its Radon transform, and then you apply the appropriate Ries potential. This extends to a unitary operator from L2 of Rd to onto L2 of the parameter space under the constraint of having an even function. Okay, so this is the basic theorem that I want to discuss. Theorem that I want to discuss in several different versions. Now, just a remark is that for D equal two, the alpha dot, so the right exponent here is one half, and for d equal three is one, as Alan has implicitly said several times before. So, what I want to do now is to introduce very briefly, very Very briefly, very in a very succinct way, the point of view of Helgas, which brings about a very abstract idea, which is the following. Suppose you have an input space X, and you have several manifolds inside X, actually foliations, and actually many foliations. So, as in the case of R, you have lines, parallel lines, and then you can twist lines and have several different ways of covering your input space with. Your input space with manifolds. And then you have a label space. So for each of these manifolds, you associate a point. And so you have a space, an abstract space that describes the set of all manifolds. So it's the parameter space that we are dealing with. So the idea of a general, very general Hadon transform is then that whenever you have a family of subsets, Family of subsets of the input space endowed with a natural measure, then you define the Radam transform as the collection of all integrals along these manifolds. So you just have to pick out what these manifolds are and select measures, and then you have a Radam transform. Now, Hergerson's point of view, as in all of his career, I would say, is that a nice way of being able to performing analysis is to. To performing analysis is to bring about groups. So his point of view is that you have a nice, forget the technicalities, locally compact, cyclone countable, who cares? It's a nice transformation group on the space of inputs and on the space of labels. So you think of these two abstract spaces as homogeneous spaces. That means that the group acts on the set, on the set. On the set X. So it transforms the point of X in a nice way, and it does so in a transitive way. So and then we have a nisotropy K, which is another, generally speaking, nice, or for example, compact subgroup of G. And you assume the same kind of geometric structure on the space of labels. Okay, and this is actually exactly what happens if you look at the standard Radon transform. So, what you really want You really want is now to bring about measures because you want to do integrals. So, the assumptions that generalizing slightly, Ciao Demetrio, welcome, generalizing slightly Hergeson's constructions, we define what are dual pairs with measures. So, we assume that both the input space and the label space carry measures. Carry measures that are, I mean, behave nicely with respect to the action of the group. And this is technically expressed by saying that both x and psi admit relatively g-invariant measures. What does that mean? That if you integrate a function to which you have applied the group, so the translated function, where translation means the action of the group G, then you get back the same integral up to a factor. The same integral up to a factor which only and solely depends on alpha, on g, on the element by which you have translated the function. And you assume that the same happens for xi. So, okay, so now you have measures on the big space here and on the big space there. And now all you need is to have to have a canonical way of selecting one ground manifold on the space X, assuming that. Is X, assuming then there is a measure there. And then you will create with the help of the group all the other manifolds that you want to have in order to perform integrals. And then you also have a way of carrying around the measure. And so you have all the analytic tools that you want to have. So this is just summarized very shortly by saying what I said here. So this is done by picking. Is done by picking one nice measure there, which we also assume to have this invariance property, and then you move everything around in a natural way. Let's not discuss the technicalities. You move the manifolds, you move the measures. So now what you have ended up with is a set of manifolds. Each of them has a measure, and they live inside a space that has a nice measure. And the set of labels also has a nice measure. So everything is now. A nice measure, so everything is now measured theoretically, so you can do analysis. Good. I just let me just point out that these characters alpha and beta and gamma that have the that describe the invariance properties of the various measures will show up later in some way. Okay? Good. So now we have these dual pairs with measures, and of course. And of course, now we are ready to define the very general Radon transform. What is it? Of course, the Radon transform at point psi, so at manifold psi, is just the integral of the actual manifold sitting inside X that corresponds to that label ψ. So it's the integral over that map. So this is a very general construction. Good. So now there is another item that enters into the picture, which is using the language of harmonic analysis is the action of the quasi-regular representation. What does that mean? It means really a very simple thing that you can act on functions by translating the ground space. The ground space. So if you have a function on x, you look at the new function that is obtained by shifting the ground space by g. And this is a representation, and it's actually unitary if you multiply by the correct factor that unitarizes it. And the same thing happens on the label side. You can play, you can define these transformations on the radon side, just to just be explained. Just to be explicit. Good. So now you make some technical assumption, which is the irreducibility of these representations. And then let's not discuss too much this, which means that inside of L2, there is a nice space of functions where you can take Radon transforms without too many problems. Okay? Let's not enter into the technical details. So once you So, once you have this, we have a first general statement which says that the Helman transform is a closable operator and its extension is densely defined and it's what is called a quasi-invariant, I mean almost intertwines the two representations. This means that if you first translate a function and then you take the Radon transform, this is the same. Radon transform. This is the same thing as translating the Radon transform up to a price, which is given by this multiplication by G. And this multiplication by G is built out of the three multipliers that describe the invariance properties of the measures. Okay, so this is the first result. The second, so this is the remark that I just made. So, this is the remark that I just made: that the price you pay is a combination of the prices that you paid for the invariances of the various measures involved. The second result, which is what I want to discuss to some more extent, is this unitarization result, which says that there is the equivalent, the release potential in this situation. So, there is a self-adjoint. Self-adjoint operator which is with whose domain includes the image of the Radon transform. So you can apply this operator to all Radon transforms if you want. And this operator maps into L2 of the label space. And this has the same property I discussed before. So it's the same invariance. Okay. So this operator I, if you first apply, if you First, apply, if you go back here, then apply i, and they go back there. This is the same thing as applying the operator multiplied by chi. This, in turn, says that if you compose the closed version of the Radon transform with this operator, just as in Helgeson's theorem, you get something that extends to a unitary operator, which we will call Q, which sends isometric. Which sends isometrically and isomorphically L2 of x onto L2 of psi. So it is a unitary operator that intertwines the two representations now, this time with no price to be paid. Okay? So if you apply first, if you take this on the other side, if you translate functions and then you apply the operator, it's the same thing as applying the operator and then translating functions. Operator and then translating functions. So this is the full, clean result that extends the result that I introduced that was proved first in its baby version by Herguson. Now, again, the price that appears there is that character G. Now, let me now go to one of the nice consequences of this theorem, that is an inversion formula. Formula. Okay, so if you, in addition to assuming what we have assumed so far, you also assume that the representation pi is square integrable. That means something that happens very often in signal analysis when we deal with something useful, which means that there is a so-called admissible vector. So a function ψ that builds the. That builds the whose coefficients of the representation, so the integral of f against the moved versions of psi, this you assume that this mapping as a mapping from the Hilbert space to the functions on group is an isometry. In this case, what happens is that this is a weakly convergent identity. This means. Identity. This means that these projections of F along all the directions spanned by the versions, by the moved versions of psi, can be glued together to give you back F in a weak sense. This is the inversion formula in the weak sense. This means that the representation is nice. If this is nice, If this is nice, then if this is true, then our second result is that we have an inversion of the Radon transform by means of this formula. If you look at it, you have some type of, you start with an admissible vector, and then you assume that under the action of the unitary operator is in the domain of I, then you start. Then you start with your Radon transform. You take the coefficient of the representation, you multiply that by the moved versions of the original admissible vector. You pay the price. You pay your price, which is chi. You integrate out and you get back F. Okay, so this is an inversion. And in particular, if you want to see If you want to see a moral proof of this, this is very easy. Because if you look at the coefficient, what we have called the voice transform, so the coefficient, the scalar product of f with the translated version of psi. You apply q both sides, and then you use the property of q, and you get that. And then you now plus. You now plug in the definition of what Q is: I times the Haddon transform. You move I to the other side because it's self-adjoint. And then you perform yet another movement and you take this on the other side of pi hat and you pay the price. Okay? So you have an identity between your wavelength. Your wavelet coefficient, and this coefficient, which is built out of the Radon transform. Okay? So you now use the reproducing formula, which is here and tells you that if you integrate out these coefficients, you get back F, you plug in place of that, this, and you get the theorem. Get the theorem. Okay, so this is the inversion formula. And if you want to see a nice example of this, Demetrio came right at the correct moment because it's Shield. And this observation was actually made by Demetrio several years ago with collaborators in the case of the Shilit group, inverting the Adam transform with Shilds. With shirlets. So these are the usual Shirlets matrices. Let me skip the fancy movies because you all know what they do to lines. And so what so they move lines and into lines, of course. And so what you get is the following. The Shearlett transform is an example of. Is an example of a square integrable representation and it gives back functions by taking Schiller coefficients, right? So if you apply, if you now use the affine version of the Radon transform, which means simply that you parametrize instead of with angles with tangents of angles, nothing more fancy than that. Then what happens is this formula, which gives you. Formula, which gives you an expression for the Shirelet coefficients. And that says that you start with a Hadon transform. You then apply a one-dimensional wavelet transform, and then you convolve with a scale-dependent filter at this, and this gives you your shield coefficients. So, if you want to invert f with using the Haldon transform, you use this formula. Transform: You use this formula. You start with radon wavelet convolution, you get the Schiller transform, and then by inverting the Schiller transform, you get back F. So this is one example where the thing works. How much time do I have? Okay. Good. Now, okay, this was a general theory that we, with Giovanni and Ernesto and Francesca, we proved. Francesca, we proved. And now, my question was: okay, I've spent my young years, so when the dinosaurs were still around, and actually some of them were in my department, and studying symmetric spaces, of which this is a basic example, the unit disk. The unit disk is a basic example of symmetric space where there's a bunch of canonical manifolds that are natural. Manifolds that are natural to consider. One are the green lines that are the horror cycles, so-called. So they are circles tangent to the big circle. And the others are these circle arcs. They are portions of circles that are orthogonal to the boundary. And those are geodesics in the hyperbolic metric. Okay? So Hergerson proved his theorem in the On in the opinion setting and spent all of his life studying symmetric spaces. So I took his very few papers, tried to find a version of the horocyclic unitarization of the Radhan transform, and didn't find it. And then Tanga Velu told me that he did not write it because Harish Chandra already knew it. And so he was ashamed of publishing something that. Was ashamed of publishing something that his master already knew it. So I'm not claiming that is a new result, it's true, but it's not easy to locate in the literature. It's not there. So we made an exercise and we tried to prove the unitarization result for symmetric spaces. So if we want to apply our big machinery, our general theory, there is a major problem. The hypothesis on the representation, on the quadrature, On the quasi-regular representation, it's wrong, not true. So, you cannot apply that machinery to prove the unitarization theorem. So, you need some more machinery, some techniques that I'm not going to discuss. You have to tune things, follow diagrams, be smart, of course, use the ideas of Hergerson that were around, of course. And then, in the end, you come up with the result. You come up with the result, which is there is an equivalent version of the risk potential that does the job. Forget, of course, the technicality. It says that there is something nice, some weird operator lambda, that combined with the Radon transform gives rise to a composition that extends to a unitary operator that sends L2 off the symmetric space. L2 of the symmetric space, the unit disk, for example, onto L2 of the parameter space with flat, which means which takes into account the symmetries, okay, as the evenness in the case of the standard Haddon transform. And this operator intertwines the representations q, p, and pi hat on x and xi, neither of which is in general. Neither of which is in general irreducible, okay? But it's an intertwining result, okay? And so this is done by chasing funny diagrams. What I want to say is something that to a harmonic analyst rings a bell, a huge bell, is that the operator that gives this unitarization, this lambda, is the inverse of the highest channel. Of the Harrischandra C function. What the hell is that? This is the density C is the density of the Plancherel measure for semi-simple Liquids. So it's a very crucial density that appears in the general theory of semi-simple groups. Good. So this is the key point that raised, made us raise the natural question. Natural question, okay. Can we do something about discrete structures? Okay, now, so I want to use another sort of slogan among harmonic analysts of a certain age that there is a parallel between rank one symmetric spaces, so symmetric spaces where dilations are one-dimensional, and homogeneous trees, which I will now discuss. Which I will now discuss. Okay, so this the third part of my talk, hopefully, I will rush up a little bit, is about the fact that the formal diagram that allows one to prove the result for symmetric spaces works for homogeneous trees. Okay, of course, you don't have to follow, I will ask you questions after, but you don't have to follow any of this, okay? The point is, though, that there is something here. Something here that resembles very much that. Okay. And in fact, let me briefly, briefly and quickly discuss what homogeneous trees are. A homogeneous tree is one of these guys. It's the same guy disguised. It's that picture that counts. And or if you prefer this picture, if you want to understand in what sense homogeneous trees and symmetric spaces. Trees and symmetric spaces are related because there are theorems that say that you can isometrically embed homogeneous trees inside the unit disk. Okay? And so this is one other way of looking at the homogeneous tree pictures. And so, and now let me just make a parenthesis that I'm not going to discuss at all, which is what I started with, that these pictures are what I have in mind. Are what I have in mind in trying to carry out this potential theory program that I am developing with some collaborators and students, whereby we try to understand the similarity between graphs, trees, homogeneous trees, even non-homogeneous trees, and some kinds of manifolds where the potential theory is done and clear. And so, what is a homogeneous? So, what is a homogeneous tree? It is an undirected, connected, loop-free graph. And this picture was suggested to me by Demetrio last year because we were in El Escorial and Demetrio showed me these mulberries. So they are trees. They are not trees, though, from the mathematical point of view, because the gardener of some king managed to link the trees, the branches of these trees, and make loops. Make loops. It's hard to see, but here there are loops. So these mulberries, too bad, they are not trees. So these are trees and a Q homogeneous tree is a homogeneous is a tree that has the same number of branches at every note. Okay. And so you fix an origin and then you define distances in the natural way. So how many times you how many. Way, so how many times you how many steps you have to march from one point to the next, and um, and there is a group acting on the tree, which is bijections that preserve distances. Trivial. There is a group that fixes the origin, blah, blah, blah. And so you can realize homogeneous trees as formally the same way you realized symmetric space. Symmetric spaces. And then there is a boundary of a tree. And I'm going to go fast here. And the boundary of a tree is, let me just be very fast, what you get by marching out to infinity. And you can think of this boundary as a circle, really. So there is a technical and nice way of doing this by identifying. Nice way of doing this by identifying infinite chains that coincide and so blah blah blah, you get a notion of boundary. What is that good for? The notion of boundary is good in order to define horocycles, which are the equivalent of the tangent circles to the unit circle. Okay, these are what I want, what I'm after, and they are defined as follows. They are defined as follows. You fix a point at infinity, you call it omega, and you want to define the index between these two vertices. You march out to infinity and you consider any path in that direction with a plus. And then when you go back from infinity to the second point, you count them with a minus. And then the index that you associate to this is the number of pluses minus the number of minuses. Okay? The number of minuses. Okay, so once you have this notion of index, you collect all the vertices that have the same index. Okay, and you get these things that you here they look like if they were finite, but they are not because the tree is infinite and each of these dashed line intersects infinitely many vertices. Okay, these are the horror cycles. Okay, so you can. Okay, so you can let's skip technicalities. Let's just say you start with something which is the tree, which has some sort of a mathematical structure. Via the notion of boundary, you define these horror cycles, because you have to hang your tree from a point at the boundary to define what these horror cycles are. And therefore, you have something which is formally and proof. Which is formally and provably, of course, a dual pair. So you have the tree, which is your input space, where your data are, and you have a label, I mean, manifolds in there, these horocycles. Okay? And so, and these are, this is just playing the game of going from symmetric spaces to discrete structures. Okay. And so. Okay, and so in the end, of course, you have a radon transform, which is what we are after, which is the sum of a function defined on the tree over all the vertices on the same horocycles. So integration means just sum up all the values along the horocycles. Good. I see tired faces, I see sleep. Faces. I see sleeping people, including myself. And so, if we want to apply our general machinery to this dual pair, it doesn't work because the representations are ill-behaved. Okay? So you have to cook up something special. And once it comes to cooking something desperately difficult, let me just go quickly, no technicalities. Quickly, no technicalities, you're too tired, myself too. You imitate Helgeson and you prove exactly the same theorem using the technique that we've used to prove it for symmetric spaces. This time, there is an operator on Schwartz functions on the label space, and it is a multi-a Fourier multiple. It is a multi-a Fourier multiplier, which is given by the inverse of the Harris-Chander C function for trees. Exactly the same. So the operator that you use to unitarize the Raden transform is formally the same object. Although chasing diagrams and definitions to understand exactly why this works, of course, requires some. Works, of course, requires some work. And then you extend this operator to Schwartz functions with two decorations. This lower decoration means the parity that is involved in taking Radon transforms. And this is an integrability condition that I will not discuss. And then you can extend this definition here to a mapping lambda. Here to a mapping lambda, and this extends when composed with a Radon transform to a unitary operator from L2 of x to L2 of the Hohr cycles with the correct parity. Okay, so it's exactly the unitarization result for these discrete structures. Okay, so this is one instance of a, I don't know how to call it, a structural To call it a structural phenomenon that happens when comparing certain kinds of continuous setups with certain kinds of discrete setups. And of course, this calls for many more questions and investigations. One thing that we are trying to do is to understand if the various notions of harmonicity and holomorphy and Begman space. And Begman spaces and spaces of functions of various kinds that are well defined and well understood on the unit disk or on holomorphic symmetric spaces can be transferred to discrete trees, to trees, to graphs, to discrete structures. And of course, the steps are homogeneous trees. And then the next step is non-homogeneous trees. I think that I. I think that I have abused myself and your patience. So, thank you very much. Thank you, people. Perfectly on time, and especially for the very interactive and lively talk. Are there questions? Okay, I have a question that yes. Sorry, from this question, you will understand that my level of understanding of everything is not so deep. So when you were mentioning the square integrable, I mean, when you have a square integral representation, so when you usually work with the Radon transform along lines, let's say, so would it mean that you need to have like a square integral version of the delta function, a delta? Of the delta function, a delta on the on the line, okay. Not quite. I mean, the idea is there is that your group is, for example, the similitude group of the plane. So, which which means whatever makes triangles similar. So, it's dilations, rotations, and translations. So, that's the similitude group of the plane that the natural representation on L2 is square integrable, which means that if you pick out a nice. That if you pick out a nice L2 function and take coefficients, so inner product, with all possible translated, dilated, and rotated versions of that, you come up with a bunch of functions that you can integrate out against the appropriate measure and reconstruct your L2 function on the plane. I see. So it would also work with some, let's say, a smoothed version of a delta function in a sense. Delta function in a sense. As a wavelet. Yeah, exactly. But now it's a smoother version of a delta function scales. It has to have integrability properties. Oh, yeah, yeah, yeah. Okay. Okay. That's nice. Thank you. Thank you. Giovanni. I'm not very used to hyperbolic space, so I apologize for the. So, I apologize for the naive question. But if in the study, well, in the Euclidean setup, you have a square integral, irreducible and square integral representations and everything is easy, in the hyperbolic setting, you don't. And I mean, can you comment on why you have this difference between the two cases and if there's something substantial or deep? Okay, uh, one, okay. You have I have to say something. Kill me if I you know, if I get too long, you have to tell. Long, you have to take into account the representations of the group that acts naturally on the unities. Okay, what is the group? It's SU21. And the representation theory of all these classes of groups consists of three bits. The so-called discrete series, which are the square integral representations, and then the principal series, and then the complementary series that have continuous. Here is that have continuous parameters. Once you write out the regular representation, sorry, I couldn't remember the name, okay? The square integrables ones are not enough. You need too many parameters, and that makes the And that makes the regular representation on the unit disk not irreducible. There is a piece which is irreducible, which is made out of the, you know, the discrete series, but the other pieces fill it out. It's a representation theoretic fact, if you want to say it in another word.