Welcome back to the last section of this five stat. And having heard the great talk by Andre earlier from the physicist's point of view, now it's Mikael talking about the statisticians' feelings about what happened at the meeting and so on. So Mikael goes with you. Thank you. Thank you. So we will be doing this on the board. This is because of our electric theaters last night. So given the meaningful conditions last night, those will. Given the medieval conditions last time, there'll be this medieval way. So, yeah, so I'll be sharing a few thoughts. Actually, I have six points to kind of note. Kind of some of the things that kind of, at least for me, are sort of the highlights of something that I'll be thinking after this meeting. So we'll see if you agree. So the first point that was a major topic of discussion, which I think is also should be a major topic of future work, would be to kind of understand how can we go. Would be to kind of understand how can we go beyond profiling and marginalization. So the first point is most of this meeting has been about the situation where we have basically a parameter of interest theta and then we have some notice parameters new and we discuss. And we've discussed extensively how you handle this situation. And kind of the two prominent ways of doing this are basically profiling or marginalization. So, you know, what you can do right now is either you profile or you march in. And we had plenty of discussions about the pros and cons of each. And, you know, there maybe some communities prefer one over the other and so forth. And maybe one of them has a more Bayesian flavor to it. Maybe one of them has a more Bayesian flavor to it, maybe one of them is more frequentist. But the issue that was raised for both of these is that they don't guarantee coverage. Like they might guarantee coverage for the point of profiling or the point where you kind of do your toy slash bootstrapping, but they do not guarantee coverage for all TITAS and new. So to me, it's an interesting question. Can we somehow fix that? Is there some way that one good guarantee coverage for IRS? Guarantee coverage for irrespective of the value of the register. So I don't pretend to have a solution here right now, but let me just mention a few things that might be, maybe they're trivial, but it is kind of helpful trying to think about what could be done here. So one way when this, or how this can be done for sure, is if you basically construct a simultaneous confidence set for all of this together, and then look at some kind of projections of that. Some kind of projections of that. And if you kind of see what that means, you'll probably also see that this will be very conservative. But at least it shows that this can be done. There's something that would guarantee coverage for both of this. So basically, the idea would be that if you do like a 2D kind of a cartoon of this, you would have something like this where you have your parameter of interest here, and then you have your new parameter here. And if you are able to construct some kind of a simultaneous confidence set proposed, so a confidence set that maybe with 95% probability, how 5% probability covers the true value of the two parameters. Let's say this is the true value here. If you're able to construct that kind of a simultaneous set, which you can, in most cases, you can do by Neumann inversion and so forth, then sort of something you could do is to basically just project this set down on your variable of interest, which would be right down here. And you can easily see that you can. Can easily see that you can easily see that, like, you know, whenever this set contains a true value, then this protection will also contain the true value. So, if this is a 95% simultaneous confidence set, you are able to get 95% confidence inputs for your parents interest by this kind of projects. So, that should be by the same right here, and you can. But this is a issue here, like the reason people don't do this versus the reason people prefer this version. The reason people prefer this versus this is that this is very concerned, right? It's very easy to see that, like, I mean, there are these situations, like, for example, maybe I have a fluctuation where my set ends up being something like this. So, this is the case where I'm in that 5% of cases, then my simultaneous set does not cover the truth, but you can see that this would still cover the truth, the projection would still cover the truth. So, you can like, you know, you can easily see from that that this has to be very conservative. And, you know, if this was 95%. And if this was 95% set, then this made sort of like a 99% set like that. And the other thing that wouldn't also work out is that if you have a very high dimensional noises per meter, like in Hansophysics, this is usually something like thousands of noises per meters or even more, then I think this problem becomes even more severe. So probably in these hunts of physics situations, this is not doable, but it shows that there is a sort of, there clearly is something that would guarantee coverage irrespective of. Coverage irrespective of the true value of the SQL. So clearly, there has to exist a solution. There has to be something that would still guarantee, but would be better than this. So I know at least one place where we know exactly what that solution is. So that is the case. So basically, what we would want is like you have for one parameter of interest, a confidence interval that guarantees coverage irrespective of the values of necessary. So one place. So, one place where actually all statisticians would know what the answer is is if you do linear regression. If you have something like a situation like this, where you know you have a model like all statisticians are kind of familiar with this is this is like our usual linear regression model. So, you have some kind of a vector y, which are your observations, and you're going to explain this using some matrix x times regression constants beta plus some browse center. Simple sense beta plus some browser noise. This is the usual linear regression model. You basically fit it using either maximum likelihood or least squares, which in this case give the same solution. And so what you want to get are these betas. So now let's say that only your beta one is your parameter of interest. And all the rest of that, so this is some vector, some p variable vector. So the remaining p minus one variables would be your New York parameter, Newton's variables. Uh, parallel nuisance variables in this case. So, you only care about the first component, the rest are nuisance variables. So, um, the classical statistical solution for this, like you know, what all statisticians know, is that you just basically construct the kind of the plus minus some number of times your standard error around the central value estimator, your least curved estimator. And that has the property that, irrespective of the value of the new scarabs, it will have. Of the newspaper close, it will have coverage like that. The interval you would use is basically there. You have your beta one at plus minus, especially if things are costing you like a C1 over two, and then your standard error for your eta. So, so here, so this interval, this will have. So this interval, this will have like this, this is like a 95% confidence interval for all values of beta, including the noises parameters. So clearly in this case, this can be done. And I briefly talked about this with Ulat, and Ulat is saying that in this kind of a Gaussian situation, profiling could also give the same solution. So I didn't check that, but that sounds true. So clearly, in kind of Gaussian situations, we know how. Gaussian situations, we know how the construct is kind of interval for a single parameter of interest that would guarantee coverage irrespective of the values of the noises, variables, and risk parameters. So, this tells me that there has to be some kind of a channel, like this is a fully general solution. This you can basically always do as long as you just construct these simultaneous sets, which you can through Norman inversion. And this is, of course, very specific. This is not general, but there has to be something between these two. There has to be something between these two that would be more general. And I don't know what that is, but in the discussion, it would be maybe nice to hear if you have any thoughts about that. So I think this should be a major topic for future work is to figure out like how do you go beyond this in such a way that you can barricade coverage because I think there has to be some clear. We are looking at a version of this problem. So hopefully going forward, we'll have also to say about this. Okay, that was point one. Um, so I'll have a few, I have already got six points, but the remaining ones are going to be a little bit shorter. This is really kind of the main thing I wanted to highlight. Um, so then the second thing that we discussed a lot is basically what sets should be reported. So And there are at least two points that have been discussed along these lines. So, the first one is the shape of the set. So, we've at least discussed kind of ellipsoidal shapes and then hyper rectangles. So basically, and this goes back to the talk by Galen, you might remember the nice figure where it had, basically, had something like this, where you would have two parameters, like theta one and theta two. And then the ellipsoidal sets that are used a lot in physics would have some kind of a circle, some counter type of a shape. Gaussian contour that was a shape. But then he also had this kind of a box shape. And the point there was that this is in some sense actually easier to work with. Like this is this diagonally projects on your axis. You don't need to worry about correlations. You just communicate the marginal information. So in some sense, that is easier to work with and easier to process. And also yesterday we talked about our work on unfolding. So it just turns out that it was relatively easy for us to create this book. That it was relatively easy for us to create these boxes, but then it's unclear how you would create this advertisement in that case. So, there are also cases where it's also easier to create these boxes if you're a fine coach. So, I guess what I'm trying to say is that there's nothing, so the reason electroids are common is Gaussianity, but if you are in a non-Gaussian situation, which in many complex situations end up in non-Gaussian situations, then there's no longer anything special about ellipsoids. And one could very And one could very well consider some other shapes as the sort of confidence that you report. So that was one thing that I explained discussed. And the other kind of dimension of this discussion was whether we should be reporting simultaneous sets or one at a time sets. So maybe let's put it in here. So this came up also in Galen's talk and came up yesterday in our unfolding discussions at least. And so most of the time in a much what is done is that you would report the one-at-a-time interval in using the form of a histogram and then correlations, which I think really is reporting the. Which I think really is reporting the ellipsoidal confidence set. But what you could also do is to report basically this kind of hyperrectanal set that has simultaneous coverage current base. So let me try to maybe illustrate this with the picture. So let's say you have some kind of a histogram here. And then you have some kind of error bars or a confidence interval around your histogram. Um, so so if these are one-at-a-time intervals, then as you all know, the meaning of this is that let's say we do this at 68% level. So, the meaning of them is that you know each interval would contain the true value of Telbin, 68% of the time. How is that different from the simultaneous? So, if these were simultaneous intervals, the statement there is that all intervals together would contain their true values sixty eight percent of the time, or whatever confidence level you have. Time or whatever confidence level you have. So, so, so, so, so, the coverage guarantee would be simultaneously for all bins instead of one bin at a time. And, and, and again, in some sense, for the user, this might actually be easier than, again, doing this ellipsoids in the sense that, for example, if you then want to ask, okay, like, you know, let's say you record a simultaneous collection of these inflows, and you want to ask that, okay, are my experiment are my. Are my experimental results consistent with some theory prediction, for example? So, if you have reported simultaneous intervals, the only thing you need to do is to basically say, okay, well, I overlay my theory prediction here. Maybe let's say the theory prediction ends up being like this. If this is a simultaneous interval, the only thing you need to ask is, is it contained in the intervals everywhere? And you're done. That is your goodness of fitness, in a sense. You don't need to account for any correlations or covenants matrices. It's right there in the... It's right there in the graphics. So, again, you know, I guess it depends in the end what people want to do, but you know, this is one option that is available and some statisticians are quite familiar and comfortable with. Okay. For the next one. Actually, for this, I have a picture I want to show. So let me see if I have current objector. And I practice a moment if I took a moment for it to start. I'll uh make it warm up and I'll start explaining what's warm up. So the third point I want to make is that is that um systematics are not unique to astrophysics. So So I thought it might be nice to see an example of systematics in a context different from Hanzo physics and maybe to see a little bit what is done in other fields. And I think actually in this field, you are doing very well in comparison to some other fields. So you should be proud of yourself and the fact that you're having meetings like this. This. So, this is so some of you might know that I also, besides particle fish, I also work on climate applications. And so, one thing we've been doing is to produce testimonies for measuring the heat content of the ocean, which is very important for studying climate change. And so, we are one of the groups who are doing this, but there are many other groups who are doing this. And recently, and this actually, this is from a paper that came out like a week or two ago. So, we got together. Do a goal. So we got together basically with all the groups in the world who are producing these estimates and basically got together to try to kind of compare those. And this actually something that is done a lot in climate science. It's called inter-comparison projects or experiments where basically different groups who are trying to, one way or the other, predict or measure the same thing, get together and try to compare things on an appose-to-appose basis because there are always these small details that might be different. You know, details that might be different. So, in order to really compare, you need to get together and try to make sure that it's really comparable. So, that's what we did here. This figure is showing the results of this kind of an inter-comparison. So, this is showing that the trend in the ocean heat content, OXC's ocean heat content. And this is really basically an almost direct measure of the rate of vibrant change because most of the heat that is trapped on our planet goes to the ocean. So, if you're able to measure the trend of the change in that heat plant, then that's basically a measure. The change in that heat fund, and that's basically a measure of parasons. So, this is an incredibly important point to be able to measure. And, um, so, and what you can see here is if you look at this, this, I don't know, some 50 different groups, there is spread. There is quite a lot of spread. I mean, they all agree that these are positive quantities, so that's good. The sign is kind of agreed on, but there's a pretty large amount of spread. Like, this is what, you know, 28%, something like that. You know, if you look at this thing here. Look at this thing here. So, um, and the point I want to make is that this actually really reflects systematic answering. So, so the important thing here is that these are not like independent experiments that collected their independent data sets and you would see like statistical variations. These are more or less using the same data. There's only one data set that contains information about this. It's called Argo Floats, if you heard of those. All of these come from the same data. They see all of these come from the same data. So, so these points here would be statistically almost 100% correlated. So, really, this thread is about the systematic uncertainty. And I have been learning that a lot of it is about the status of volume choices you make. And yeah, so partly the reason these kind of studies are done is to try to understand what is the community-wide systematic uncertainty in the modeling that is done. But you can see that that's more or less where. Like, that's more or less where you know, the state of the art here, but I think one could do much more, right? And actually, some of the techniques we heard about in here, like, for example, the errors in errors thing, I think would be applicable here. Like, clearly, these are discrepanced. If there was any sense called network of error associated with this, it's probably underestimated and it probably wouldn't kind of match the spread here. So, I think there's scope for things like the errors and errors and so forth. And so forth. And that's the nice thing about being said. We can come to meetings like this and also pick up interesting ideas from your community and translate to other communities. And we've been doing that quite a lot. And in fact, I mean, just a very concrete example of that is all this like good free inference stuff. It really started from particle physics. It did not start from statistics or machine learning. It's really this community who started to produce this kind of frequent disconfidence that's with high-free free inference that is now being then used in statistics and other fields. In statistics and other things. So there's actually examples where you can actually, you know, this community might be the origin of some of these interesting new techniques. Anyways, I thought this might be interesting to see like another example of system IX. Let's turn this both. So the fourth way is very simple. And this goes more to specifics of some of the talks we have seen. So, one question I want to pose, and these are again like you know things to think about for future work, is how to best learn micro trace search? So, I mean, so as we have seen, likelihood ratio is a fundamentally important quantity for producing confidence tests, doing hypertensive tests, and so forth. And I'm actually thinking of this in the context of the model independent searches we saw yesterday. So we saw both Daiat's work and Puras's work. And both of those were using likelihood ratios to do these searches of new physics. Of new physics. So, I mean, in both of those works, basically, it was based on, you know, one way or the other, getting an estimate of this ratio between your stem null hypothesis and then some kind of a broad space of alternative hypothesis. But I think, like, a very important high-level difference in those two verbs was how is this actually estimated? So, as far as I understand. So, as far as I understand, they the approach. So, I know what Kurosa is doing, but well, because we read it as work, but in terms of Gaia's work, I believe what they're doing is, so the Rosa and Al, what they do is that they take the numerator here, and then they say that this is a neural network, and then they go ahead and fit that. So, they make this a neural network. They make this a neural network, and then one way or the other, they learn that ratio based on that. And then, what Purvas was talking about is a different approach. So, what is done in that work is that you train your data versus Monte Palo. Was Monte Paulo. So you train a classifier to separate between the data and the Monte Parlo. And then it can be shown that this likely tracer is basically, if we assume same samples as remote, it's basically the classifier output one minus the Pelsi output. So H is your classifier. So you can be assuming that the likely direction in that case is simply a simple function of the classifier you have to have trained. So notice that in both of these verbs, they're aiming for the same test statistic. They're aiming for the same test statistic, but this is estimated in a very different way. And I think it's an important question: like, how do these compare? What are the pros and cons of these two? They're different estimators of the same thing. So what are the pros and cons of these two different approaches? And I know, well, yeah, Gaia is there. So I know that Gaia has done some comparisons. I look forward to seeing those comparisons. Then we'll have some clarity on this. But this is right now, until we see Gaia's results, this is an open question, which I think is. This is an open question, which I think is very important to try to kind of get some understanding of. Okay. Then two more brief points, and then we can open for discussion. So the fifth point is about something called model distribution, which in some sense model distributions is more or less like the same thing as systematics, but this is a term that is used in certain fields of statistics and applied math. So if you just like, you know, if you want to find related work from those fields, just like search with molly discrepancy, you'll find plenty of papers about. You will find plenty of papers about how some other fields deal with systemic answer in this. And I specifically want to show this kind of stuff is done in a subfield of AquaMath called computer model calibration, which also citizens are also enrolled in there. And they, I think they have sort of their own way of handling systematics. So let me just mention one thing that they do. That's a very common approach there that might. Very common approach here that might be also helpful, especially for the theory uncertainties that we heard about in Frank's talk. So, there's this very influential paper, just give you the reference here. It's called the authors of Kennedy and O'Hagan. I believe it's 2001. So, what they do there, so this is an approach. So, this is an approach for handling uncertainties in simulator models, and that has become very popular in certain areas of applied mathematical statistics. So, the basic setup is something like this, that you have some data yi that you want to model, and then you have some kind of a simulator for that. There's some f of x given theta. So, there's some kind of control variables x, and then there's some variable theta that you're interested in. And this is some kind of a simulator. And this is some kind of a simulator, it's a computer model, one very theater. And this is supposed to be a model for your observations. So, if this was a perfect model, then you know, in this field, we would usually put there's some kind of an error there and do some kind of a regression type of a thing. And, you know, that's what you do if there are no kind of systematics or model instead involved. But what these people, what these authors proposed in this paper was that to recognize that, okay, these models are never perfect. Okay, these models are never perfect. So, we need some kind of an extra term here that they call the model disseminating terms. And they basically add this kind of an additive effect that depends on those axes. And what they do, when the key contribution was to model this as a Gaussian process. So they were basically say that this delta is a Gaussian process with some mean and some currency function. And then they show with what conditions you can. With what conditions you can estimate this, there's, of course, a little bit of an ill-point model, so you need to make some assumptions and input some priors and so forth. But this kind of an approach of like adding a Gaussian process as basically like an epistemic model discrepancy term is very popular in kind of engineering type of applications. Like, you know, if you if you're modeling the strength of a, you know, I don't know, the wing of an airplane or some kind of a chemical, you know, experiment or. Uh, you know, experiment or you know, something like that. Uh, in that kind of situation, this is a very popular approach for modeling model discretizer systematics. So, I think in terms of what we have heard in this week, I think the theory answers that that were discussed might be kind of amenable for something like this, actually, the difficult ones where you don't even have sort of a Taylor expansion, like you only know the first term and nothing else. So, maybe. term and nothing else. So maybe that rest could be modeled as a case like an epistemic Gaussian approach to account for that answer. All right. And then the last point I want to make, and this really kind of goes into like where should we go from here. So I want to say a few more, a few things about collaboration and interdisciplinary collaboration between statistics and physicists. So, as we probably all agree, these kind of meetings are great. You have great discussions here, and we can get to exchange thoughts and results and so forth. But how do we keep this going? How do we keep in touch after meetings like this? How do we actually start working on the products? Do we actually start working on the products and actually collaborate on these questions that are so interesting and that we have discussed in here? And the issue is that with these meetings, what kind of tends to happen is it's all very exciting when you're here and then you go back home and then that's kind of the end of it, right? So the question is, how do you actually kind of have like an in-depth, active collaboration beyond meetings like this? And the main thought I have on that is that what something we have found to be very kind of effective is. Very kind of effective is to basically co-supervise students. So basically, have a PSC student that is co-supervised by both a statistician and a physicist. And it doesn't even need to be a PS, just some kind of a project that is supervised by two advisors from the two different communities. So then you have sort of, there's a buy-in, there's an active, like a weekly meeting. There's like a, there's sort of a, you know, everybody kind of knows that there's an agreement to kind of work together on something. Work together on something. So, we've done a lot of the work you have seen from CMU here this week has been through this kind of protest where we actually work with, there's a student who is advised by both a statistician and a person from the Heinz-Finnisch community. And that really helps to kind of, first of all, it helps the acceleration of the transfer of information because otherwise there's a very the feedback loop is very slow. You know, if the only way you can get feedback is to come to these meetings every, but Come to these meetings every what five years or no? So, like it's a very slow feedback loop. So, if you actually actively collaborate, I think the feedback loop is faster, and you might be able to make progress faster. So, you know, I guess that suggestion here is that for statisticians, maybe look for physicists as co-advisors, and maybe for physicists, look for statisticians as co-advisors. That might actually keep the progress going and get us moving forward faster than just, you know, Than just coming to meetings. Okay, so that's all I want to say. So I guess we will now open it up for discussion. So thank you very much, Miguel. And I certainly very much appreciate the last point that you mentioned. And it's certainly worth taking up this idea. And maybe people have other ideas as well for sort of increasing the interest. Sort of increasing the interaction. So, any questions specifically on Mikel's or yeah, we'll guide to your first how to take a confidence region and take it down to a confidence interval. There is a completely general, always working solution for that. Okay, I'm very curious to hear about that. Yeah, that was the good part, the bad part, yeah. So you So you have this region of whatever shape, and you made it a 95% confidence region, project it down, now it's suddenly a 98% confidence interval. Well, if you've made it a 93% region to start with, or maybe 92 or whatever, it's sooner or later you would have 95% confidence in it. Of course, the hard part is to know what should it be up there, so if it's down there. Well, in general, It's down there. Well, in general, if it's possible to do it, you can do it. Just do your confidence study, see what it is down there, a chest up there, do it again. So probably not doable in most circumstances, but when it is, then yeah. I mean, probably the one to call is that you would have to do it over the whole space of your ePAL and you sit the picture. Yeah, I mean that's the that's it. That's the reason why that's difficult. That's the reason why that's difficult. But I should mention the techniques that Anne talked about. Like, they, you know, in those techniques, you that is basically what is done there. That works fine as long as this is a reasonable dimension of the space. Like that, you know, that I don't think that would work that there's like a thousand basis parameters. This is just a handful of parameters. There is now a way of doing the calibration, but the eye dimension situations are the really difficult one. Yes, Monica. Just because I mean, so I don't remember. I think I remember in Gaia's talk also, the neural network was sort of built as almost a product correction on the H0. So I think it actually connects these two that actually, in the absence of systematics, one may be like the logic of the other. And maybe sort of an interesting connection, maybe how you handle systematic uncertainties, whether you train sort of directly or. Or using the classifier. I don't know. Yes. I fully agree. And this was not, I know that really there's this exponential and there's the Haskellian, there's a cancellation. This was a graph approximation of what is really done. But I think that is the essence that the neural network is used to govern one term and then there's some cancellation happening, which is very different from what we are doing when we do this as far as I can. My point is, they may not really be that different in the sense that, you know, the log, like if you were to take your classify. Like, if you were to take your classifier and cut off your sigmoid, and then you have the logits, which is just one computation different, you can interpret that as the log of the likelihood ratios. And you basically can do that interpretation. And then I think they become very much more connected than having to do this full translation. I was a comment about this. In our implementation, we rely on current methods, we do use a cross-entropy loss instead of our loss without systematic students. Without systematic consideration, it's easier to set up, and we can recover the same test statistics and have this exactly the same result that we had with our standard implementation with neural networks. And yeah, also in that case, we start from the output of a logistic, basically. The statistics up to really similar to what you're doing right now. Just the main difference is the way we set up the algorithm. The algorithm or so all this regularization that we perform in order to find a guy square, these kind of things there and slightly different because of the model. Yeah, I mean, we have seen that we can really run the same strategy with that approach as well. And I think, so just to clarify, like one way to see that there, I think in the previous approach, and so there. The previous approach, and so there's this new approach now, but in the previous approach, and I mean what we discussed, I think there's a fundamental difference in the sense that, like, in this approach, both the data and the Monte Carlo, they enter as actual samples, and you're actually using both data points, and you use actual points from both samples. But in this approach, and you can correct me if I'm wrong, but my understanding is that basically only the data sample enters as individual points, and the reference sample, the Monte Carlo, only affects the normalization, right? So it's the The normalization, right? So it's you kind of only use the normalization information from the other samples, that's that, right? Do you mean that we train only on data, or is it when you do when you do when you do this thing and like in the final last one, so I think in some of our previous discussions, my understanding is that like every day, there's this asymmetry between the two samples. Otherwise, you have access to. Have access to yeah, yeah, that's true. So, the two samples enter in a very different way in this approach first. Yeah, so yeah, maybe a comment about it. It's true that we are trying to use the cross-century, but our is an unbalanced concentric plus anyway, because we have this unbalanced between the two. Yeah, this is actually um, I think Anthony wanted to say something. Yeah, just a couple of things. Um, the first was about the first point about marginalization in the profile. I mean, the I mean, of course, you know, we've got statisticians involved a number of tricks for this. One is to construct a different mighty with marginal profile conditional. Another is to use higher order things, which you can think of as a way of getting one of the other different language groups when that's difficult to write some of them. In other words, could you use more formalization of the users? Possibly the other one is this idea of orthogonalization which hasn't. The other point I wanted to make was about the confidence bands. It's a bit of a from a statistical point of view, it's a bit of a funny question to say which should we do? Of a funny question to say which shouldn't do why don't you do both? I mean, we when we plot graphs that estimate our curves, we will typically put on say the 95% point-wise confidence and throw in, you know, that gives us two lines, a line above and a line below the estimated curve, and then the similar joint confidence band somehow estimated which curves. And there's no reason, it seems to me, why you shouldn't do the same. Yeah, yeah, it's a very good. Yeah, that's a very good suggestion. In fact, there's a point I actually forgot to mention, which is there's a effective trivial procedure for constructing simultaneous intervals. It's something called quantum correction that all statisticians would know about, but maybe not physicists. So, I mean, there is a really simple procedure that allows you to take the one-at-a-time intervals and turn those into simultaneous intervals. So then, you know, doing what you suggested, like, really wouldn't even be much extra work. You'll already take what you have and do this correction. What you have and do this correction, and then you have simultaneous influster reference. And if a theory agree is okay with one and not with the other, do you say that it fits, but it doesn't fit? That's the conclusion, I guess. Well, what I typically do in practice is if you like to bootstrap your setting, I generate a graph with an estimate and these confidence bands, and then I simulate 13. 1700 examples of bands from the fitted model, and then you can look and say, Well, that bump there, it's only there in half of him, it's clearly not a feature. Or it's there every time, it must be a stronger. I think it's an interesting discovery because we almost never use simultaneous Hungarian, so maybe we've got something to learn there. Um, I think the next one was Alessandra, and then um. Cassandra and Andre, and then I think we moved to general points. You could make a general point after. Let's get in about Smovenius confidence in tricks, so I'm not sure whether it's the same idea that mine, but it was a broad way attached by Douglas Bates while I was doing my PhDs 25 years ago and saved my life when I was grinding at the library. The library for this high-order stuff. And so the situation is: I wanted to, I had, for instance, some parameters, wanted to produce the two-dimensional projections of this confidence intervals. And you can get pretty good approximation of the contours with just four points using cubic interpolation. And what you need to do that is. To do that is, I have to think about it, and I would have to look it up again. Actually, what you do is you profile that one the match tested for one graph. So that was Face and Walt wrote the book, and he used it in 1980 about non-linear models and it's appendix A6. Okay, and and And it's really pretty good. So you get an approximation of this allopological contours. And so if you really, really want exact ageball, you have to check for coverage. But I think I did it and coverage was pretty good. But they give you an idea of how you know those how to match like a little graph. And it's really very quick to translate. Yeah, that's the Google can create. Yeah, that's the book I can get. It's one of the classical books about non-linear regression. Is it called non-linear regression? Non-linear model, something like that. And it's a wonderful book, especially depending. Great, great. If you can put it on the Slack, it would be great to have that. Okay. Great. Okay, great. Yeah, so I want to echo what Louis said, that we do not usually do simultaneous continuous intervals. And I was trying to think, since Frank did not raise his hand, I was trying to think what this means in terms of what theories can produce to them compared to those. And there are, I believe, you correct me if I'm wrong, but they're going to be like statistical uncertainties. You know, statistical uncertainties of how you compute something in a given bin. But then there are going to be like parametric dependencies on, I don't know, output S. And that's going to change the decay. And I can understand how to do that or to compare that bit by pin, but now you have this box in as many dimensions as there are pins. So the theory is not going to be a point inside that box. Point inside that box, it's going to have like a sort of a trajectory inside that box. So I don't know exactly how, well, I think the answer is as simple as like if you really do like simultaneous steps, so like let's draw like a two-dimensional. So let's say you have in some high-dimensional space, you have a simultaneous confidence rectangle. So then, I mean, so really that the way you would test theories is that, okay, you put, you know, you compute the You compute the theory value for this, let's say the one and the two here. And if it's inside, then the theory is accepted. If it's outside, then it's not. But then what you're saying is that really your theory is maybe a collection of points, something like this. I think simply you just basically look at each point in the set and compare, and that should be fine. But then what does it mean if some points are also? Right, so you can so if you have something like this, then these, I mean, to be, I mean, these points you would reject, right? I mean, you know that these versions of the theory are not consistent with the data, and these are. Yeah, I think we really have to. This is a very interesting project because we really, I don't know, we really don't use these commonly at all. I don't remember just before we leave. But just before we leave this talk specifically, my point about the question of ellipses versus rectangles is illustrated by a cosmology plot of the fraction of dark matter, sorry, the fraction of matter in the universe, which is mainly dark matter, and the fraction of dark energy. And different cosmological observations give you different ellipses. Different ellipses, which have different correlations. And so, although sort of both these methods have a big uncertainty, particularly in the fraction of dark energy, together they give you this sort of small region, which defines the fraction of dark energy precisely. Whereas if you sort of draw rectangles for the intervals, you sort of lose that useful information about the different Information about the different correlations. So, I mean, that's sort of the reason we like ellipsoids or something, whatever the shape actually turns out to be, rather than sort of hyper rectangles. But I think we should move on with people having general ideas about the meeting and the future. So, Wolfgang, you want to say something? Yeah, I think Siri does. Yeah, I think Sarah does. Yeah, can we go first? Oh, sorry. Okay, yes, first of all, Sarah. Mine's probably less high-level. Okay, Sarah, go ahead. She wanted to pick up on point six, which is this kind of elaborated thing. Okay. And part of this is, you know, I found it very, very useful, but I've also not been to any of the five stats workshops team before, and it's maybe a really naive point. And it's maybe a really naive point. Um, which question whether or not you thought about promoting the workshop series to a more formal collaboration, even if you don't want to put loads of bureaucracy into it, because then you might have a structure by which you could have more collaborative projects that could come as or more follow-ups or discussions that by status. I'm also thinking that it might also then, you know, potentially. Also, then, you know, particularly if you're wanting more kind of students and ECRs to get assimilated to some of these initiatives, I think, I don't know, it's kind of a suggestion, and maybe it's so obvious that you've talked about it a lot of times and there's readmitless work, but that could be a good way to find extra ways to get potentially pots of money to do follow-on studies rather than just yeah, thank you. Yeah, thank you. So just a general comment. Any suggestions about future BISAT activities, which includes seminars as well, Oleph and I will be really happy to together. So thanks. We should discuss it further. No, put idea. Well, yeah, this question of how to get more people from statistics to come into physics and so on. To come into physics and so on. Of course, interdisciplinary collaboration. It's what you're doing at Carnegie Mill. And that's like absolutely the best, no doubt about it. But I think it would be very helpful for a lot of statisticians if there were a fairly small list of important problems and described in such a way that a statistician has a chance of understanding it. So a likelihood with three terms. With three terms instead of 300 would be very nice for a description of the problem. Then you have a data set that a statistician can look at and if they have an idea for a method can apply to it. It doesn't have to be real data, but it should be real good fake data at least. Something that's relevant. Not that you devise a method and then you go to the physicist. And then you go to the physicist, and they're not like, does it look like aloha? And then, last, if you even had that on top, it would be super great if there was already some existing methods that one could then use as a benchmark. So I have my ideas, I developed this, and now I might be embarrassed to go to the physicists because I'll be doing way better than you guys. Anyway, so if one could put together a list of four or five important Important problems that could be boiled down to this level and then publicize it in statistics departments and something could come out of that. Yeah, Michael. So I understand the motivation for that approach. But in my experience, at least on the machine learning side, it doesn't tend to work very well because it's sort of the physical sort of data generator. Data generators and sort of then wait for the answer to come back from machine learning people. And one, it often doesn't make any sense. And two, it sort of lacks the feedback to really build an actual collaboration. So anytime we provide stuff to machine learning people, like in absence of a real like discussion, like weekly ongoing discussion, I've not found that the outcomes particularly useful. But I would say it's great to have these data sets as needed for discussion, but and also. Discussion, but I don't know that they use for that. We just release them and then wait for someone to have a look and see if they want something. No, it's just pretty useful. I'm sure the statisticians who come to this meeting, certainly if they come with the first night, are utterly intimidated by the whole thing. I mean, and they can't see, I certainly couldn't see for the longest time, a way to even get started to think about these problems. So to provide them with something definitely. Be definitely, I think it needs to be coupled with the conversation to be totally which one you say that way, but I think it would be great for the physicists when they try to do this, focus your mind on what the important parts of the problem are. I just wanted to follow up. There was some discussion here, there was a presentation that mentioned the data from the Higgs Machine Learning Challenge, which is now almost 10 years old. Machine learning challenge, which is now almost 10 years old. And I think there were interesting things that came out of the machine learning community by having to put the data out there. It wasn't SG boosted. I think at some level, a product of that or being consolidated by that. So we have at least one example that worked. Maybe there could be others along those lines. But I don't know, Michael. I'm not sure next to you who's came from that. I don't know. But if you look at the solutions to that challenge, they're, let's call them gobbledygooks and. Let's call them gobbledygook that nobody would ever use on an experiment. They're just super complicated mixtures of mixtures of models. And it was like clear that someone could classify this data really well. But it wasn't clear to me that, okay, maybe the actual features. No, actually, you're right. In some sense, everybody saturated at the optimal limit. And the winner was determined by some noise. But okay, it had certain flaws, but nevertheless, it had the benefit. Unless it had the benefit that the data from that challenge has been out there and it's been used compared to. Oh, definitely. But I think the places where it ends up being the most useful has been when machine learning or statistician picked it up and also sort of starts working on it with a physicist. Get together, like former team that has, you know, representatives from both communities. I think that is really, and also that have the actual problem. I have a fairly realistic problem. A fairly realistic problem, like you know, to actually solve the actual problem, like you know, yeah, the simplifications are nice for conceptual things, but usually like then, you know, it's only the actual problem is different from that. Yeah, okay. And today, I'll just make three points. The first is that you have to realize the statistics world is much smaller than the physics, any fewer of us, and we have tons of interesting things to work. Tons of interesting things to look at, but I'm not particle physics. So, having a strong barrier, a golden barrier is very off-putting. Many of us will have training in other fields, and they're kind of frankly very interesting. So, like, you know, how to estimate the extent to which and so on and so forth. Or manufacturing, or and so on. So that's just a bully point. There's not so many of us as there are physicists. To many of us, as there are physicists. The second is about reducing barriers. I have the privilege of teaching our second-year physicist probability and statistics. There are 14 weeks and I managed to feed, because I have to do probability properly first, I managed to 13, four and a half weeks, that is 10 hours, actually 10 times three quarters of an hour, of statistics. But it's not very much, right? And I suspect that in many academic And I suspect that in many academic institutions, statisticians never talk to physics undergraduates. So, something the physics educators can do is to push for proper statistics courses in your curriculum, given by statisticians or mathematically trained people. Not because physics courses taught by physicists or on statistics for physicists are bad. Are bad necessarily, but there's an underground, there's a level of language that's different. And I was a bit surprised, for example, to learn a few years ago, to discover that the physicists had reinvented Neymar-Pearson theory 18 years after it was discovered by, you know, existed in the statistical literature. And of course, congratulations, but quite a political waste of time. That goes in to my first course. We need language parts. Well, but one way to avoid the language is actually to have a proper statistics or physicists course taught by somebody who teaches statistical language. Yes. In the first place. Yes. Okay. And then we know what you know, we all know what a parameter is. That was just okay. Okay, any others? Did you have a third point? I think you said you have three points. Okay, I think Lydia, and then I think we should sort of draw to a close because we're half an hour over the schedule as it is, and people do have airplanes to catch. I think those of us who are still around can carry on this conversation over lunch and hopefully. Conversation over lunch and hopefully beyond into the future. And I think we should aim for these interactions to happen much more frequently than the sort of year time scale of by staff meetings or a few years later. No, I just want to say for me personally, I really enjoyed talking to people at this meeting and figuring out which part for a statistician is bottomly go and which part is the part that I can formalize into a problem or simplify. Into a problem or simplify in some way such that the relevant information is still remaining because I think that before this meeting, I could not have done that on the wrong side and worked on ourselves. Okay. I really appreciate that. Good, thank you. Maybe I haven't been making notes of who said well. Maybe people who said things could just send me a very short, sort of one-sentence summary of what they said just in the last half hour or so. Okay, I think I'll have once to say time. I think I'll have one to say exactly what we're not. Maybe we should we should talk better about Linda caller. Oh, okay. The seller, the seller officer. What's wrong? Sorry, what's from? Oh, it's very nice. So Linda, I'd like to come to say goodbye first. Thank you so much, everyone. We're enjoying this last week. I hope you had enjoyable stay with us. And the weather kind of behaved because just before you arrived, I think a week fire, we had a small square and freezing temperatures. So thank you so much again for coming. I hope we had a productive workshop and I. We had a productive workshop, and if you want to leave any testimonial, we appreciate that. If you need anything, you can stay actually in this building. Let's say you'll decide this until evening, come back here after lunch. You can get very late. Otherwise, yeah. And having your conference with us, and this is the largest group today after actually fluent from the COVID break, so it's really nice to speak. So it's really nice to speak. We in our turn would really like to say thank you. Thank you to Burrs for having this, providing us with the ability to be here. We did tell everybody it was a sort of great place for meetings and having discussions rather than just sort of lots of formal talks. It turned out to be even better than we expected from that point of view. So thanks to you and Thanks to you, and thanks as well to Carlos for looking after the Zoom. And we also appreciated the fact that yesterday evening you gave people who wanted to look for the northern lights a great opportunity. And we leave donate. Thank you very much. Thank you very much.