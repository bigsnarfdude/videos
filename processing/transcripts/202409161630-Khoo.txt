All right. So, okay, let me get started. So, I want to change to my actual slides. Yeah. So I guess we need to read maybe Okay. I think it's okay. Yeah, I can just click. Okay, so today I'll talk about some use of convex optimizations for physics application. Yeah, again, it's rather recent work and feedbacks are very welcome because the results are rather primitive at this point. At this point, I figure it's a good chance to present to the community where I learn a convex optimization from. Okay, right. So, yeah, today I'll talk about convex relaxations for Foucault equation. So, before talking about the Foucault equation, let me just first briefly summarize what has been done in the past, well, I guess one of the main techniques in the last 20 years. Last 20 years, how in terms of convex optimization and how it's used to solve a global optimization problem. So, say you have a global optimization problem, you want to minimize some function P, right, and it's non-convex. To overcome this non-convexity, there's a way to change it to like a linear problem. So instead of minimizing over P, you test your P against measure mu, right? Now you minimize over this linear problem, right? Problem right in terms of this measure mu, where mu, of course, is restricted to be non-negative and integrated to one. So the minimizer of this problem will be a direct delta situated at x star, where x star is the minimizer of the first problem. So now instead of having to deal with a convex, well, a non-convex problem, you deal with a linear problem in terms of the measure, except the measure is like a measure supported on a very high-dimensional space. Well, now, well, to make this a little bit more useful than having to do with the person of dimensionality, so what people do is, well, you first, well, you go back to this high-dimensional space, then you go back down. So now you look at the moments of this measure mu, right? So what you do is like you pick some monomial x alpha, x beta, where alpha, beta are some multi-indices. You form this moment matrix M. Now, if you Now, if you do that, if your cost function originally p is like some polynomial in x, where p alpha is the coefficient, then you get a linear problem in terms of the moments. And now the normalizing constraint just becomes, again, some linear constraint on the moment matrix. Well, however, the tricky part is the However, the tricky part is the half-space constraint, mu non-negative. So what people typically do is you just enforce some necessary conditions if you are like testing it against some non-negative function. You take some non-negative function, you test it against mu, right? You ask for the results of the test to be non-negative. Then the question becomes like, how do you choose this selected set of test functions q? What you do is, well, again, this is what people have done in the list. Is what people have done in the literature is we'll pick Q to come from like a span of a square of polynomials, or I should say, a convex span of the square of polynomials. Now, this is equivalent to saying that this moment matrix M has to be positive semi-definite. So, the moral of the story is the following. You change your non-convex problem to like a linear problem in terms of the measure, and then you drop some necessary. And then you drop some necessary condition that this has to be non-negative. Then, because you optimize over a larger set, because you dropped some necessary conditions on the, well, you dropped some conditions that a measure has to be a measure, now you get a lower bound. So this is kind of like the workflow of this type of method. Okay. All right. So now I'm asking about. I'm asking the question whether this type of approach can be applied to other measure problems. And if so, what kind of moments we actually have to choose to actually make this problem solvable in a cheap enough computational cost? Because if you think about the moment matrix, you're trying to form a moment matrix of low-order polynomial. So that means, right, the size, the problem size n. The problem size n goes like, well, d to the power degree times d to the power degree, where degree is kind of like the cutoff, where you cut off the degree of the monomics. So in some sense, right, at the level of degree two, you are basically out of luck. So it's a method that is kind of too expensive. So we're going to try to study this issue in a setting where we're In a setting where we're trying to solve for the Foucault-Planck equation. So, there are many ways to think about the Foucault-Planck equation. One way is to think about it as like a PD description of overdamped Langevin dynamics. So, here you have a gradient of the potential as a drift term, and then you have some white noise added, right? And you're scaling it with the square of temperature. So, it's kind of a commonly used to model, say. Kind of a commonly used to model, say, the interacting particles with some damping. So now, well, now the law of the SDE is actually a function that satisfies the Fokker-Planck equation, which is stated out here. So L is the Fokker-Planck operator. It comes from having a diffusion term, right, acting on mutely. And this term is held like a drift term coming from this. trick term coming from the gradient of a potential mu. So in some sense, this kind of like generalized the global minimization problem, because if you solve the global minimization problem, you kind of like get one direct delta in one of these like kind of like the global basin. But then like because the fact that you add some brownian white noise, it facilitates transition into other basins. So instead of getting having a direct delta situated at your Arctic altar is situated at your minimizer. You have some more spread out and smooth out distribution that can be supported over all this locomina. What exactly over what space are you trying to solve this now? Yeah. So you're saying like uh what space this So you're saying like what space this measure mu lives in? Right, so this measure mu is going to be, well, because there's diffusion, I'm going to assume it's a C infinity function all the time. Yeah, so because it spoke out the distribution into the Gibbs measure. So okay, so to start, of course, we want to study the dynamical process of the Fokker-Pan coefficient as well. But then, yes. There's no, yeah, there's it's really just a PDE, yeah. But then I'm going to convert this PDE into some sort of constraint satisfaction problem. Yeah, yeah. Well, in some sense, there is. In some sense, there is, right? One can think about this process as some type of gradient flow in minimizing a variational problem, which is like the Gibbs variational principle. But then I'm not going to take that route. So, okay. So, in order to, well, okay, I'm going to start with studying how to solve for the stationary equation, first of all, right? Before studying how to solve it in the dynamical. Uh, solve it in the dynamical setting. Okay, so uh, I want to solve l mu equals to zero. L is the fork performance operator, and like I said, this is exponential size indeed. So, it seems hopeless, but there are some hope, okay. So, usually, right, the the the the the potential right that you're going to encounter in physics takes this type of a paras form, for example, gravity, columbic, and whatnot. Okay, so uh, because of this, uh, something more competitive. Something more computationally feasible can be done. For example, mean the approximation. So, what you do is you take this equation, L mu, right? You integrate out all variables but one. You leave out i variable and you integrate. Now, after you do this integration, you are left with some one-side operator, li, acting on mu i, where i is the mu i is the i-th marginal, and then a sun-shot sum of a bunch of terms, right? Of terms, right? So Lij acting on muij, muij is the two marginals between i and j. So this is trying to summarize the force of a j dimension acting on the i dimension. Basically, that's what it means. Okay, so now after this integration, it becomes like a, seems to be more doable, right? Because you're only dealing with one marginal and two marginal. But then in order to get a closed set of equations, right, what people do is you take this muij, you decorrelate it. You take this muij, you decorrelate it. You commit a model error. You split it into mui times muj. Then this gives rise to a set of nonlinear equations with OD number of parameters where you need to solve for each margin. Okay, so again, so this type of approach is like PP5 by the following method, right? You take mu, you split it. Right, you take mu, you split it into some like lower-dimensional answers. For example, in this case, you take mu, you're going to split it into like products of individual measure in terms of each dimension, right? So you're restricting the solution to a non-convex subset of the probabilities in facts. So basically, there are many more high-dimensional methods, tensor method, or even neural networks that follow this type of philosophy. You restrict the set of solutions to like a smaller class. So today we're going to. So today we're going to look at a different way, right? We're going to try to relax the probability syntax actually to a larger convex set. So you're going to try to, again, do what we have seen before. Instead of enforcing the non-negativity, you're going to test it against some chosen set of Q, right? So of course, right, like in the worst case, you're going to test it over n to a power d number of functions where n is the dimensions and v is a dimension and n is like the discretization per dimension, okay. Per dimension. So, of course, you can pick your favorite test function q and then make it more manageable. So, our favorite test function is the following. So, we're going to, again, take q to be square of some function. But then this function, right, s, actually is not coming from a linear combination of monomials. Okay, we're gonna take like take it from the span of something we call. Expand of something we call the cluster basis. So, what's a cluster basis, right? A k cluster basis is basically a set of k variable functions. For example, one cluster is the following. You formed, well, you fix some variable xi, right? You form all the basis regarding that variable xi. Then you enumerate over all the dimensions, and you enumerate all the basis per dimension. So, basically, the set of one-dimensional functions. Dimensional functions. Okay. So, no, I just pick it to be Okay, so there are two answers to that. The first answer is that is that no, we just pick, for example, 5J to be, for example, Fourier basis per dimension or something like that. Or like logo or email. Yeah. But then if you want the best. If you want the best uh results, you probably want to pick eigenvector functions of a certain operator, which is not something I would be showing in this uh in this talk. Okay, all right. Okay, so uh uh so now uh let's continue well uh let's look at this uh stationary equation again, okay? Now, uh, instead of uh applying the correlation to this mu ij. The correlation to this muij, we're going to impose a bunch of constraints on muij. Basically, well, the first thing is non-negativity. So, suppose I take an example of testing against a square of one cluster basis. Basically, it means I have some set of single variable functions. I add them together. I squared. I test it against mu, and this has to be non-negative. So, again, this ended up giving rise to a positive semi-definite constraint that looks like this. You have a That looks like this. You have a matrix G, where on the diagonal you kind of like put your one marginals, and on the off-diagonal, you put your two marginals. And then this guy is positive seven definite. So I'm writing this in the matrix language because you can kind of always do that if you impose some suitable discretization according to like kind of like what functions you you you pick. You can pick the the quadrature that uh best uh integrate those uh That best integrate those functions. Now, okay, so there's a more obvious one, like local consistency. Basically, you have two marginals, right? If you integrate out xj, it becomes a mu i. These are kind of like more obvious to see, and then mu ij has to be non-negotiable as well. And you have a normalizing constraint again. So, this gives rise to like a linear semi-definite program of size nd by n d, right? Nd by Nd, right, because of this matrix G, okay? Well, where N is the number of bases per dimension. So the question is: how can this work, right? Because if you look at it, you have D number of equations, but the number of variables you have are O D squared, right? So it seems like it will give rise to some non-unique solution, right? Okay. All right, so we have some All right, so we have some kind of preliminary results towards this, like how to explain how this can work. Okay, so let mu star be the ground truth of for Kopan's solution, and mu i star be the ground truth i marginal. Okay, so I'm making the following assumptions. The first one is the existence of a mean field approximation, okay? So suppose like mu star is actually eta one close to a mean field, okay? So eta one close in a certain sense is actually in terms of Consense is actually in terms of chi-square divergence. So coercivity is kind of like the standard assumption guaranteeing the uniqueness of the equilibrium measure of mu star. So it's saying that the second eigenvalue of the operator has to be lower bounded. Now, the third one is actually the part that is Uh, the part that uh that is like uh kind of like more non-trivial compared to the type of assumptions that people typically make in the literature, which is a good outer approximation. So, it's basically saying, right, if I take my Lij, right, I act on mu ij, I take a sum of it, right, basically calculating the forces acting on the i side, right, use I sum over all j's, right, calculating the forces acting on i side. It behaves just like I take my mu ij and split it into mu i continue j, okay. And split it to be right continuously. Okay. All right. So then, if I make all these assumptions, then you're going to from perturbation theory, you're going to get a pretty straightforward results that you're going to get, like what you're going to get is you're going to get perturbation divided by spectral gap. Basically, that's what you're going to have. So then the natural question is that what kind of a system actually satisfies this assumption? Okay. So in particular, the third one is kind of like. The third one is kind of like the, it's, I'm basically stating what I want to prove, right? Like, okay, right. So I want to say that I want to show that my mu ij is going to be close to mu i times mu j in terms of the force that they give rise to. And I just state it right there. Okay. So then what kind of assumption, like what kind of system will satisfy this type of assumptions? So here's an example, right? It's actually pretty simple. An example, right? It's actually pretty simple, but I think it does illustrate some points that how the method can work to give a mean field approximation. Okay. Right. So again, like what I want to say is that the method returns a mean field that is close to the true mean field approximation. Now, okay, so suppose, right, like again, like I'll go back, go back to looking at the Fokker-Point equation. In this case, the one-side operator is really the diffusion operator acting on the I-th margin L. Then this Lij muij is coming from, well, this is the force of a J dimension acting on the I-th dimension. And I integrate out all configurations of the Jth dimension. So this is like the average force of the Jth dimension. average force on the jth dimension on the i th dimension. Okay, that's l i j. So then, okay, if uij happens to be separable, which is very nice, it will actually satisfy assumption three. So you go through the algebra, you're going to find out that lij mu ij is actually going to be lij mu i times mu j f j. So in this case, like it's kind of trivial, right? Because like if you have a potential that looks like fi plus fj, you know from separation of variables. You know from separation of variables in undergrad PD class, it can be trivially soft by looking at each dimension independently of each other. Okay, so now let's try to add some perturbation to this type of potential, right? You add some Bij, okay. So where I'm gonna assume my Fi is order one, right? So Bij coming from taking my polynomial and I synthesize it with some. polynomial and I synthesize it with some basis, synthesize it with some coefficient, and then that's how I create my Bij. So I'm going to create a Bij, right, like this coefficient, come be like drawing from some random Gaussian distribution, and then well some Gaussian distribution, but these are random variables. Then when sigma is large, it's going to be non-separable. All right. All right. So in this case, if I make all these assumptions, I'm going to be able to show that theta 2 is square of d sigma. So this one is really coming from kind of like the, well, kind of like the spectral norm bound on a random matrix. And your lambda is actually order D. So if I calculate the error between mu i and mu i star using the previous theorem, I'm going to get. Previous theorem, I'm going to get eta 2 divided by lambda, which is kind of like sigma divided by square of lambda. So, really, what it's saying is that the non-sub probable part can be as large as sigma equals to little o squared of lambda for this method to work. So, the point I want to illustrate with this example is the following. So, usually, like in physics, this uij, there is no obvious separation between xi and xj. But turns out But turns out if you sum up all the other forces that acting on slide I, some central limit theorem will occur. And you don't need to require Bij to be order one. You can have Bij to be something really, really large. Okay. So that's really like the idea. Okay, so let me show you some numerical results. So I'm gonna show it in kind of like a model problem, where you have your potential u, right, like coming from the following. You have xi interacting with xj, saying that xi has to be similar to xj. And then you have a one-side term, right, penalizing each xi has to be close to plus one and minus one. close to plus one and minus one. So this is a 10-dimensional problem, right? Like if I choose some, well, choose a set of parameters and here the blue dotted line is the ground truth one marginal and the red one is what we recovered. You can see that there's a pretty good agreement. The one marginal distribution error is about 10 to the minus 2 in terms of L2 error. So, okay. And well, often in physics, right, this type of potential UIJ actually has more structure, meaning this potential U actually has a permutation symmetry. So, for example, Uij, all of them, is equal to a single potential U tilde. Think about like Coulombic or gravity, everything interacts in the same way as each other. So, now in this case, right, all the two. in this case right all the two marginals is the same right just gamma two and all the one marginals is also the same right just gamma one okay right so in this case right instead of by working with this big matrix gij you can symmetrize it basically you sum over i naught equals to j and divide and you divide by d times d minus one now again this gives rise to a size reduce semi-depth program so this is how a gap So this is how gamma 2 will be represented once you symmetrize it, right? You're going to represent it as like a PSD matrix lambda, which is of size n by n, subtracting out the diagonal, because once you symmetrize this guy, you need to subtract out the diagonal to get your gamma 2. So in this case, there's no explicit d-dependency. You can just solve a single equation over here in order to retrieve the one-marginal and through the two marginals. All right, so once you do this, you can also try to do like an interacting particle system. For example, I'm looking at a soft Coulombic potential in a box. Say you have like some repulsion between Xi and Xj. So I make it soft, mainly to make it easy for me to discretize. And I put it in a confinement potential, a quartic confinement potential so that things don't escape visibility. Don't escape infinity. This is a four-particle system, and I do it with two different temperatures. And this is what I get. Again, the blue dotted line is the true marginal, and then the red one is what we get out of our method. What if I say, but I need to cancel the negative two, actually? What can I do now? Oh, well, then you basically get the approximation. Well, then you basically get the approximation to the true distribution through the mean field. So you take the mu y and you multiply them together. That's an approximation to your to the full distribution. So you get this accuracy, right? Right. And now I'm saying, well, I need higher accuracy. Can you, yeah, okay. Understand your answer. Right. So yeah, that's right. That's right. Exactly. Yeah, that's right. That's right. Exactly. So, so it can go ahead. Okay, that's the next slide. Okay, okay. Yes. So how do you like generalize this to like such that the area is smaller and smaller? So what you do is what you do is the following. So instead, okay, so you're going to pick some function s, you test it against your forkliftung equation, right? And then again, this function s is going to be chosen from k-cluster basis. So actually, in the previous basis. So actually in the previous example, I subtly picked the test function to be a long cluster basis. So of course you're going to step up this, the number of variables involved in this test function. Then you also need another test function to test the non-negativity of the measured mu, so which coming from squaring the k cluster basis. And then that's how to reduce the Say that again? Yeah. The click masterpieces. That's one thing I can play with. You can play with that. Yeah, that's right. So here I'm picking it just by increasing the number of clusters within the basis. But of course, there are ways to reduce the, for example, right? Like if you use k equals to two, you're gonna do like, you have addition to the number of bases, which is a lot. But then you can. But then you can, based on physics, you can kind of like sub-select a sparse set of bases from that issue and still get a faster computational box. Yeah, but okay, yeah. So let me end at this point. There are various control versions and thank you very much for listening to my talk. Yes, dusting cube. So actually, that's exactly the point I'm also wondering. I'm actually at a point. So, okay. So, at the level of, okay, so. Okay, so your question basically amounts to saying that, okay, so this is how I think about it, right? So there are two ways, right? If you just look at the level of like solving for one marginal, right? You either are going to solve this equation, which is the mean field approximation, or you're going to solve like, well, use my method. Okay. Of course, there are other methods. So yeah, actually, I'm still trying to understand whether there's a regime where if you just directly apply a If you just directly apply a non-convex method to perform this iteration, whether it might converge to some spurious roots of this equation. But our method, well, okay, that's something that I don't know yet, I would say. Then, okay, the bigger question is the following. Okay, so let's say you want to step up the hierarchy. Like, let's say you go to using k equals to 2, k equals to 3. So in statistical mechanics, there's this. So in statistical mechanics, there's this set of methods called BBGKY hierarchy, so which results in very, well, more non-convex problem, not just like mu i times mu j, you could have mu i, mu j, mu k. So I suppose like having a convex method in that type of, you know, if you're kind of like stepping up the hierarchy, that might actually helps you. Because usually once you get kind of like an equation that is like too high order, you're gonna run into. That is like too high order, you're gonna run into the spiritual opinion. So, yeah. So, in the optimization regime, what the relaxation can give you, for example, is lower bounds. Is there an analogous thing that this setting? So, yeah, in this setting, the deliverables are the marginals, which is, in some sense, all you care about in a high-dimensional setting, because even I give you the probability distribution, there's no way you can, I'm going to look through the space and get a feeling of the distribution. So, you're going to try to get some summary statistics out of it. Some summary statistics out of it. So, the deliverables will be the marginals. You mentioned briefly before, you can also pick the Fourier basis at some point. Yeah, that's right. Can you was that meant for the cluster basis or I didn't really get that and you do that for which examples would you choose that and why? Would you choose that and why? So you're saying, like, is the base choose of Uber basis the best, or is that? That's maybe your answer, but I'm not asking. I mean, I didn't think so because I mean, really, can we context example? You mentioned this as an example. I was just wondering, which what would you choose that? And did you actually choose that? Okay, if indeed I want to choose it. Okay. So, okay, if indeed I want to choose it, basically I want to, well, be. Well, be able to, okay. So, if I choose a basically, I want to choose like what I'll do is I'm going to form the moments of this type of test, right? So, in some sense, like I'm going to choose the basis such that if I capture the moments related to that basis, then I can recover the distribution very well. So, that is the type of basis that I'm going to be choosing. Yeah. So, in some sense, I would choose. I will choose it to be kind of like the eigenfunctions of this one-side operator, Li. Yeah, that's exactly the point. Why? So that opens up the whole box of harmonic analysis because you know Ferbasis diagonalizes convolutional operators. But why can you use a wavefront basis that approximately diagonalizes another set of operators? You have this whole toolbox at your disposal now to you that you can use that. Yeah, that definitely yeah. Yeah. With mine. Yeah, that's it. Yeah. Yeah. For the sake of time, we're more button if you should.