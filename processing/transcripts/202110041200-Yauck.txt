Embed the activation data into a capture-recapture framework. So, and we'll try some simulations and work on some real-world data set. So, the mobile ecosystem. So, I think I'm going to start with some facts, some about facts. So, according to a recent report, According to a recent report, I mean relatively recent, it was 2018 by the Pew Research Center, that more than about 80% of the U.S. population, adult population owns a smartphone. It's a pretty substantial number compared to some other places. For example, my home country. So it's pretty interesting and this opens up opportunities for entities to For entities to make profit with that. And so, with that fact, I think one of the consequences is that people are more and more inclined to using smartphones to access information, the weather, to log on social media. And so you have this new reality when it comes to social media and To social media and smartphones. And so, with this, you have the mobile ecosystem, which is constructed by many entities, which I'm going to present right now. So the mobile ecosystem, you have mainly for actors. So you have the user of the app or the owner of the smartphone who can carry out daily activities. Who can carry out daily activities like checking the weather or just looking into his or her account? Then you have the advertisers. So the advertiser is a company placing ads on mobile. So if I'm wrong, please correct me. So while I was working on this subject, I felt kind of obliged to explain how the data set that I'm working with was generated. So that brought me to the mobile ecosystem. So you have the advertiser. So, you have the advertiser, a company placing ads on mobiles. And then you have a publisher. So, a publisher is a company selling ad space to the advertiser. We have examples of companies here. They are lots. And you have what you call a supply-side platform, which enable publishers to manage the advertising space to fill it with. Space to fill it with ads and receive revenue. So, with these actors in the mobile ecosystem, they interact through a complex scenario, RTB actually. So, David touched on that. So, pretty much what's going on is that users of the app carry out daily activities by activating the app that they have downloaded. Activating app that they have downloaded on their smartphone. So the app is owned by a publisher who charges third parties to display ads. And the publisher and the advertiser meet through this supply-side platform that provide the software to target a specific audience that the advertisers want to reach. So each time I maybe fire an app or go to Facebook, for example, an RTB takes place. For example, an RTB takes place. The transaction between the advertisers and the publisher is recorded in a log that contains information on the ad impression. So you have maybe the ID of the ad, the time of the firing of the application, the location where that occurred. So what happened is that when that happens, you have You have marketing platforms that accumulate an impressive amount of impression data from across the United States, for example, on a daily basis. And we worked on the data set provided to us by a marketing platform called Ninth Decimal, which is a California marketing platform about data activations at 11 or the dealership of a major brand. Of a major brand in the US from June 2014 to November 2015. So, this was kind of the structure of the data set. So, you have the smartphone IDs, you have the time of the firing of the app, and you have the locations. Now, the reproposing of this data create opportunities for businesses and financial entities to extract information on the scale of the patterns and movement of people. The patterns and movement of people who attend, for example, entertainment venues, who visit retail stores or restaurants. And so the businesses are leveraging this information to provide measurement of food traffic visits. So this is pretty much the goal of the analysis from a marketing entity standpoint. So the problem that was brought to us was how to That was brought to us: how to estimate the food traffic over that period of time using principled approaches. So to explain what was done from our point of view, I have to introduce the world of recapture, as most of you are not familiar with it. Not familiar with it. So, briefly, to explain that, I'm gonna separate it in two parts. So, you have the usually the population process, which explain how the population, the evolution of the population over time, and that underlying process is not observed. So, to gain information on the population, you have on the other side an observational sampling process. So, there are two parts. So there are two parts. So the population process. So briefly, how to explain that. So we are interested in an open population that is evolving in time. For example, over I sampling periods. So you have at sampling period one, you have the population size n tilde one. And between sampling period one and two, you have some additional. You have some additions, so the B ones, and some deletions. So, some people survive, some do not. So, you have this process that's going on over time. And this is described by a process that we have here, which pretty much says that the size of the population at period I equals the number of survivals plus the number of new additions. That's pretty much it. New additions. That's pretty much that. So, this process is governed by two sets of parameters: the survival parameters and the birth or birth parameters. So, you have these sets of parameters. So this is usually in an ecology framework. So, when we are interested in animal evolution. And then, to gather information into the population with this underlying process, what we usually do, we have a We usually do, we have a sampling procedure. So, usually, when we are trying to count the number of animals from species over time, what we do, we organize a captive-recapture experiment. So the experiment consists of marking the animals. So, you provide tags. So, if you can see here, the ibex, you have tags on the ears or marks on the body. Or marks on the body, and then you put them back into the population. So you wait an amount of time, a month or two, a month, a year. You organize another session of capture, recapture. So some of the animals that have already been tagged would be recovered. So you have a history of capture of all these animals. So the principle of capture is we are using the Capture is we are using the information of the animals that we have tagged and captured to estimate the number of animals that we have never seen. So, providing kind of an estimate of the size of the population. So, that's pretty much the idea behind the capture-recapture sampling approach. And usually, the design could be more complex in a way that we could organize, for example, could organize for example capture we capture experiments every year and um and within every year we organize experiment um in the second so the year is the sampling period in year one for example we have secondary occasions on l1 maybe month for example on six six seven month and between the years we assume that the population is open That the population is open, so we have addition and deletions, um, and within each year, we assume that the population is closed, so there is no death or immigration. So, we use generally this sampling scheme to collect information on the population so that we can estimate the size of the population and the survival rate. And the way we organize the data is we have the data set. is we have the data set within each year or within sampling period and the data set between years or between sampling periods. So we have two sets of data that we have that we can use to estimate the parameters of interest. Now how does this so so we have this data set so I'm just going to present how from a classical standpoint how we do this analysis. So we have this data Do these analysis. So we have these data sets. So the capture histories are zero and ones. One when we capture an animal, zero otherwise. And the data set, so we have the capture history, the capture entry for each animal. So I'm going to just go briefly throughout this notation. So we have the capture frequencies, the semi-capture frequencies for each, for all the individuals that have been captured. For all the individuals that have been captured. And we have the between sampling period information for the open population models. So the parameters here are the size of the population, the survival rates, and the number of births. And these could be parameterized using a login model. The model that we have here, pretty much, is that we assume the number of capture The capture frequencies follow a Plasma distribution where the expected value of the number of captures depends on the logged in parameters, the gamma and the beta, which in turn depends on the demographic parameters. And then we fit this model, then we use bootstrap to estimate the variances. So this is pretty much how, in the world of capture, we capture how we do it. So we collect the data using a capture. So we collect the data using a capture-recut experiment. Then we use a loginear model to kind of estimate the survival rate and the size of the population. So this is pretty much how it goes in the copy recapture world. So now the idea for me is just to take these tools and try to apply them to the activation data that we have from the marketing platform. But to do that, Marketing platform. But to do that, I have to make the argument that the fact that we are the experiment that makes us collect the data could be modeled as a capture, recapture experiment. So to do that, I'm going to hear, let's just imagine. So what we have here is that assume that we have the data, how people How people are marked or registered in the activation data set. So, you assume that I visit a store, and while visiting a store, I activate my application. I go to Google or Facebook, for example. Then I will be marked, right? So, because that information is collected by the supply-side platform, it Supply-side platform, it means I am marked now. So I am detected and marked. Then on a daily basis, we collect all this information. So in the second day or second week, some other people will be marked because they went to the store and just activated the application. So some people will be marked twice, some people will be marked once, and some people will never be marked. So the idea for us is For us, is from a statistical standpoint is to estimate the number of people that have never been marked here. So, here already we are making the argument that the fact that people are recorded in the log impression could be modeled as a capture-recapture experiment. So, pretty much the initial data that we have from the marketing platform could be organized. Marketing platform could be organized this way. So we have the smartphone IDs of the users, and we know when were they detected while in the store. So while in the store, some people have been detected in a couple of days. Over time, we have the information on the capture history. So for each individual that visits a store, we have its capture. Store, we have its capture history. We know when the app was fired, when it was detected. And this information could be organized as a capture-recapture data set here. And so N1 and two represent the number of individuals that have been captured or detected in the log impression on day one, day two, etc. So pretty much we So, pretty much we could see the activation data set as a capture-recapture data set already. So, the idea for us now is to estimate the number of individuals that visited the store or the food traffic on each day using the capture-recapture models. Now, we could reorganize our data set as one coming As one coming from the design that we presented earlier. So each week we have the category history for the individual that own the smartphone and that have been detected. So we have the data set for 76 weeks, pretty much. So we got an impressive amount of information provided by the marketing platform. And the data set could be organized as a capture-recapture data set. Capture recapture data set. Um, so for 76 weeks, so the capture history is billions. So we have billions of possible capture histories from this from this data set. So pretty much we will have later to deal with the problem of the dimensionality here. But now the idea is how to make the argument that we could use category capture. Category capture models to estimate food traffic. So, we assume first that the underlying process for customers visiting a store in a given week is described by our process. So our AI1 process that we described earlier. So, we assume that people come and go according to our process that is governed by. Process that is governed by the parameters of survival, phi, and the parameter of births. So we could check the terminology. So in capture, recapture, the PI represent the capture probabilities of animals. So in an app activation setting, we could call it detection probabilities. So if you find an app and that action is detected by the supply-side platform, that you Platform that you have been detected. And the survival probabilities in an animal environment where animals survive between periods could be called a returning probability. So the customer come to the store on one day, leaves, has been detected, comes the next day. So that the probability of the customer coming the next day or the next week is. Next day or the next week is phi. So phi is the returning probability of customer. So B here represents the number of births in a capture recapture setting. In an app activation setting, it's we call it new customers. And the population sizes in a capture recapture setting, we call it food traffic. So we kind of have a new terminology here for the app activation data set. So I don't know if you have questions. If you have questions, um, stop no, okay. This is very nice, Mamadu. Um, I could imagine that one application area is to understand the demographics of your customers. Yes, um, yes, that's an interesting. I'll come to that later. Oh, okay, sorry. Yeah, go ahead. Yes, okay, um, thank you. So what we have to do here when What we have to do here, one challenge that we have to overcome is how to model the customer's behavior. So it means that how to model the customer's behavior, the fact that the customer activates an app, come to the store, leaves, come back again. So what happened is the process could be described as follow. So the person actually visits the business during the week. Visit the business during the week. She activates an app on her mobile device while on the location, and this activation is recorded by the platform providing the data. So the person in the population is recorded in the activation data set when all of these three conditions are simultaneously fulfilled. So the goal is to select a model that characterizes. That characterizes the variations in customers' detections within weeks. So, we could assume that these three events do not depend on the individuals and do not change over time. In that case, we could use the type of variation zero, meaning there is no variation between individuals and over time. So, everybody has the same chance of detecting. Everybody has the same chance of detection and over time. So, the second model that we can have is that we assume that all the individuals have the same detection probability, but the detection change over time. For example, on the weekdays, we have low detection, but on the weekend, we might have higher detection rates. We could also assume that there is a behavioral change. It means that one Change. It means that once you are captured, once you visit the store, you might not come back again. So there is some variation there, or that there is some heterogeneity. It means that maybe the detection, the fact that you visit a store or activate your application depend on some individual characteristics, your age, your sex, and some other behavior. So there is some heterogeneity here. Heterogeneity here. So there are different types of models that we could use here to model the customer's behavior. So there are already some methodology that have been developed to select the appropriate model here. So we did this exercise already. So we select the correct model. And now the goal is to estimate the number of customers. The number of customers or the food traffic at each week for 76 weeks. Now, the issue here is that we have kind of 76 weeks of observation. So we have billions of captured histories. And this problem is a high-dimensional problem that current softwares cannot really deal with. So you cannot maximize the likelihood, a Poisson likelihood, for example, using current. Using current approaches. So we have to deal with this dimensionality. And what we did was we have a new estimation procedure for the parameters of the model. So the idea here is a data reduction in which we use sufficient statistic, meaning that we are dealing with a score function instead of the like. Score function instead of the likelihood. So we have the score function. We try to express the score function in terms of the sufficient statistics of our model and using the sufficient statistics to estimate our parameters. So this is the approach that we use. So precisely how it was done. So usually we have the Poisson likelihood and the score function. So the score function for the Poisson likelihood is pretty straightforward. And It and so you have the design matrices Z and X for the open population of closed population model. So this is pretty much it. So I'm not gonna spend too much time explaining this, but pretty much what happened is you solve these equations. So you have three sets of equations with the parameters, and the idea is to solve them simultaneously to obtain. To obtain the food traffic estimate, the returning probability estimates, and the number of new customers. So we use this procedure. It's pretty straightforward in R, and you can estimate the parameters for each week separately using our procedure, and it goes very, very fast. So this is an interesting approach to deal with the dimension. To deal with the dimension of the data set. And once you do that, the next challenge is how to estimate the variances. So, to estimate the variances, we develop a parametric bootstrap. So, the idea of the parametric bootstrap is to generate the sufficient statistics pretty much that enter the estimating equations. So, you could generate the sufficient statistics by trying to. Um, to by trying to really model the process again here, the process that we described earlier. And using this procedure, we could generate the sufficient statistics that enter the estimating equation, and then we could solve them and to repeat it many times just to obtain the estimates and the variances. So, if there are any questions, If there are any questions, we'll try to go through some simulations and an application, and then we could have some discussion about it. So this simulation that we're using is try to investigate the validity of our infringement procedure for food traffic for the 76 weeks. So we have three scenarios. Two scenarios. So, in capture recapture, usually we assume that the units arrive on the first week, on the first day of every week, and leaved at the end of the week. So, we have the first scenario. The second scenario, we assume that half of those individuals arrived on day one and half on day two. And the third scenario, the arrival are constant for every of the seven weeks. So, this is a large department. Weeks. So, this is a large departure from our initial statement. So, this is our initial assumption for the model. These are departures from these assumptions. And so, detection probabilities 0.3.5, survival or returning 0.6.8. And we have it for nine weeks of seven days each week. And we are interested in estimating food traffic at times five. So, the results show that we consistently estimate. We consistently estimated the food traffic at week five under the first two scenarios. And when we deviate too much from our first scenario here, we are doing worse. But the most interesting thing is that despite departures from the initial assumption, we could consistently estimate the relative change in Consistently estimate the relative change in foot traffic. Some people call it lift. We are able to consistently estimate this parameter under all scenario. So this is pretty much, this is kind of interesting. So now in terms of application, like I said, initially the question from the marketing From the marketing platform, whereas how to estimate for traffic for the 76 weeks at the 11 other dealerships. So we kind of first selected the models for the customer's behavior. So the best model was the model of heterogeneity, means that the detection probably depends on the individuals. But unfortunately, we didn't have other covariates. We didn't have any covariates. Covariates. We didn't have any covariates. So we kind of use this model. And we fit a closed population model in which we said the population doesn't change. We fit also another open population model with this less information and our model. So over the 76 weeks, this was the evolution of food traffic. So the most important thing is the blue lines here that represent Lines here that represent our original model here. So you could see some kind of evolution here of the food traffic. Now, an interesting question is, what does this represent? I mean, we are just estimating the food traffic. So the number of individuals that visited the store over time. Now, could we use this information to estimate? This information to estimate brand lifts, for example, that's an open question that I will come to later. But based on our analysis, we estimated an average growth rate of 5.5% over the 76 weeks, and that there was a significant increase in food traffic in 2015. Now, we cannot say. We cannot say what caused the increase, right? Because we cannot assume that the detection probabilities do not change over time and that there are other characteristics of the population, for example, age or sex that could have an impact on people visiting a store or activating their application. All we can say is that we see an increase. Is that we see an increase in food traffic in 2015, but we do not know what caused it. And so this will bring us to the discussion here. So pretty much for me, the discussion is that we made the argument that we could use data from app activations to estimate food traffic. To estimate food traffic using capture-recapture models. So clearly, we need to maybe, if we had more information on app detection and individual characteristics, maybe we could model the detection probabilities better. The second thing is: could we go from food traffic to brand lift? There, we have to make the argument: is people visiting a store These people visiting a store a direct consequence of them seeing an ad on their smartphone? If we are able to make that argument, the second thing would be what are the individual characteristics that has an impact on food traffic? Then we are talking about causal inference in that sense. So the general point is that if we are able to embed this approach in a causal inference framework, A causal inference framework, then I think we would solve the problem. And currently, I'm working on this project to embed this approach in a causal inference framework. And then this could be maybe more useful, but we would need more data, individual characteristics that we don't have right now. But I think this could be an interesting avenue in terms of research going forward. Avenue in terms of research going forward. And this work has been published in the journal of the American Statistical Association. So, if there are people who are interested in this, pretty much interesting in accessing new data sets, developing the causal inference framework for this problem. And if people are interested in collaboration, have access to data sets, I would be more than happy to collaborate and work with them. And work with them. Thank you. Thank you so much, Mamadou. I'd heard an earlier version of this talk, and you've done so much more. This is great. I was wondering if you could tell us a little bit more about any difficulties you had in acquiring the data set from the 9th Decimal group. Yeah, so pretty much the 9th Decimal. Um, the Niodesimal group, there was a statistician in the Niodesimal group who was working on this subject and he contacted us with this problem and he offered us access to the data set. So the other thing was that we will solve the problem that he sold that he can use, that's the solution, and then in return, we could use the data. And then, in return, we could use the data set to publish a paper. So that was kind of the trade-off. But this guy works in the marketing platform in California. It's pretty much a major auto brand in the US. And then they collect data daily on a daily basis in the auto dealerships. And so, yeah, this is kind of a collaboration. We have access to the data set, they have access to the solution, and we could publish papers together. That was kind of the deal. Thank you.