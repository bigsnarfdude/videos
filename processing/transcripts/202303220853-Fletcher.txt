The work I'm going to present today has been done over four different NSF proposals over the last nineteen years. The first one was actually Melias Ripanski, who's the head of data simulation at SIRA, in conjunction with Mike Navon at Florida State University, which was an old program at NSF called Mathematical Geoscience Collaborations. Unfortunately, that division no longer exists. Unfortunately, that division no longer exists, but I was Media's first postdoc, and the initial part of the research I'm going to show is what Media and I developed in the mid-2000s, and then move on to the work that my four postdocs, Senna, Jakir, Mike, and Anton have developed since 2013 on the three NSF grants that I've been successful in securing to do non-Gaussian data simulation. Simulation. So, through some of the talks I've seen in the last couple of days, I added a couple extra slides yesterday to kind of relate back to some of the mathematics, but also why we're looking at non-Gaussian distributions. Specifically, I'll be talking about the log-normal distribution, and more recently, we've worked and developed the reverse log-normal distribution. So, the log-normal helps if you've got a variable that has a lower bound. That has a lower bound. So, something that can't go negative, and you don't want it to go beyond zero. You want your data simulation system to be able to stop that from happening. Because the last thing you need is the DA system to throw out an unphysical answer. Now, that happens at NOAA, and the DA system has to be reset, and they'll go and lose an analysis cycle and have to redo the forecast from the previous one. So, you're trying to not have that happen. That's what's called a dropout. That's what's called a dropout when an unphysical solution comes out of your DA system. Because the Gaussian is trying to fit its nice symmetric curve to your error, and it creates an increment that goes beyond the lower bound of the physical variable, which is not included in the data assimilation system. That information is not there. It's in the model, but it's not in the assimilation. And then you have the other situation where you have another bound. So relative humidity can't go beyond supersaturation. So, relative humidity can't go beyond supersaturation. So, you don't want your data assimilation scheme to create an increment that's going to push it beyond the bounds of supersaturation. And so, we're going to be presenting the theory we've developed for the schemes that can hopefully help stop, prevent those unphysical solutions from occurring. So, here's an outline: we'll start with the error definitions. And that may not just do the Not that may be. Oh, we'll just do the difference. No, we cannot do the difference between two random variables when we go log normal or reverse log normal. And I'll explain why when we get to that part of the talk and show you how we get around that. I'll go into the first the log normal based variational approaches and then what we call the mixed Gaussian log normal variational approaches. And this comes from a discussion with Melia. From a discussion with Melia way back in 2005, 2006, when he said I developed the log-normal variation normal approach and we could put it into the maximum likelihood ensemble filter, but then we had the fact that we had Gaussian observational errors and we had log-normal observational errors. And it's like, how do we simulate all of them? Do we simulate them separately? Which is what Melia's approach was. And I'm like, no, there must be a way for us to keep this all together in the probabilistic formula. All together in the probabilistic formulation of Bayes, so that we can have those interactions between those observational errors be captured by the data simulation system. And you can. So we defined what has had several names, and I will get to their names when we get to that section on the mixed distribution, but it is possible to combine multivariate Gaussian, multivariate log normal, and create a PDF that has four covariances between Gaussian and log normal random variables. I'll present that when we get to that part. I'll present that when we get to that part. And then I go on to the non-Gaussian based data simulation, even though that's non-Gaussian, and we're going to start looking at the reversal of normal and what that entails and what we're going to be doing with it. The other thing to take into account that, okay, and this is meant to be a talk on machine learning, which I only found out two weeks ago and had to add machine learning to it, but my postdocs have been doing all that work. I was posed a question from the division head and Division head at the Naval Research Laboratory in 2011. It's like, Steve, it's great. We've developed this log-normal approach, but when will we know when to use it? You know, we're making the argument that it's not Gaussian all the time, but it's also not log-normal all the time. So we need to know how can we switch between which cost function to minimize to find our best state, given that we believe errors are log-normal, Gaussian, reverse log normal. And in the 40-bar case, they could change. In the 4D bar case, they could change throughout the assimilation window. I'm not going to present those results because Jakira is still writing them up, but he has been able to implement a full fixed distribution in time where the summation in 4D bar actually has innovations from the Gaussian observational errors, log normal observational errors, and reverse log normal, and can minimize over all of those inside the simulation window. I don't have those results to show you, but it's been done. And then finally, And then finally, so to answer that question, can we use machine learning to determine when the distribution is going to change? Can we train it on different components of the model such that when those components are in a certain region, then we know that the Z component, the Lorentz 63, which is the example I'll show, is log normal, Gaussian, or reverse log normal. And they've been able to show some really, really good results with the Lorentz 63. We've had one paper published on that in ESS. On that in ESS last year, and we have a current paper using the reversal of normal camera filter, which just went to QJ last week. And then I'll finish with some conclusions and what our plans are going forward with all this new research. So, errors. When so back in 2005, Mita and I were looking at how to define log-normal disputed errors. Define log-normal distributed errors. And this is a paper by Steve Cohn that's part of the compilation of the Journal of the Japanese Meteorological Society that came out of the WMO meeting in 1996 that defines the errors as the ratio of two independent distributed log-normal random variables. Promise I won't say that again. And so basically, what you're going to do. And so basically, what you're going to do here is we have the problem that the difference between two Gaussian log-normal random variables is not a log-normal random variable. It is also known not to be Gaussian distributed. Therefore, we wish to define the error structure so we keep the property of the underlying PDF that we're using to model it. So, the log-normal distribution is geometric, which means you have the lovely property that the ratio Have the lovely property that the ratio of two log normal random variables is also a log normal random variable, as well as its product. So the product of two log-normal random variables is also a log-normal random variable. And whilst I won't show that, that property became very important in some work we did later on, the second one. So, if we know the error is defined as the ratio, then we know that we can apply a log-normal distribution to it. That was the observational error, which is an extension of what Steve Cohn did in 97. He did direct observations. You were directly observing a log normal variable, and then you had your approximation to what that log normal was, and therefore it's just a direct observation. But he didn't do the background component, he did the logarithmic transform, and so the background component of his Kaman filter he developed in 1997 was Gaussian, because the other property. Because the other property, which will come up in a bit, is that the logarithm of a log-normal random variable is a Gaussian random variable. You all are confused. I know it's 9 o'clock on a Wednesday morning. This is not a lecture, okay? But I'm just setting the background for you to see. We were very meticulous about how we were designing these systems to make sure that we kept as many of the probabilistic and statistical properties to ensure that everything. To ensure that everything would go through. So in 2007, Milia and I developed the background component for a log-normal distribution as a ratio between the true state and the background state. And therefore, that remains log-normal, and we can apply a log-normal, multivariate log-normal distribution. So basically, get to the next part, which is, okay, so we've got these error structures. How are we going to build the data assimilation system? Which data assimilation system? Data simulation system, which data simulation system are we going to build? Are we going to build a variational ensemble, Gammon filter, all of the above? I think we've done all of them now. But we started with the variational approach. So in 2006, paperwork published in the QJ, we were able to show that you can take Bayes' theorem, because Bayes' theorem is independent of distribution, I can I can put any distributions into this expression. So I can put a multivariate log normal distribution in here, or I can put a multivariate Gaussian, or I can put a multivariate mix, and then just work through the mathematics that develops everything. And so we took the theory from Andrew Lawrence's paper in 1986, which shows very clearly how to develop the variational approaches from. Approaches from Bayes' theorem and how he did each term. But now, again, sorry, here's your next part of the lecture. Sorry, the three descriptive statistics of a log-normal distribution are different. For a Gaussian, they're the same. And for any symmetric distribution. However, when you have skewness, mean, mode, median separate. And so you have to now work out what descriptive statistic is it. Descriptive statistic: Is it I wish to build this whole data simulation system around it? Do I wish to build it around the most likely state, which is the one that maximizes the probability of being the correct event to occur? However, do I want to do the median because I want an unbiased estimate of the state at that time? Or do I want the mean? The minimum variance, the one that minimizes the spread and stuff. The Aussie and they're all the same. Log normal, you've got to have to pick one. Pick your favourite. Maybe. Maybe there's some more on that as well we found a couple years ago. And so we decided to follow the maximum likelihood approach. At the time, the reviewer in 2005 said, you just picked the mode because that's the easiest to find because you just have to differentiate. Like, no, that's not the reason why we picked it. We picked it because it's unique and it is bounded by the variance, which means as the variance increases, the log-normal mode, which I'll show in the next slide. The log-normal mode, which I'll show in the next slide, I think it's the next slide, decreases to zero. And so, when you're developing variational data simulation systems, if one of your components is going out of whack, you want the other one to compensate for it. So, if your background is getting too far, you want your observations to try and pull it back. But you also want your analysis state not to be weighing the background more because it's actually inaccurate and you want the observations to be given more weight. And it's the same the other way. If the background is good. The other way, if the background is good and the observations are out of whack, you really want your solution to stick to the one that's more accurate. So, what happens is as the variance increases in the mode, in the log normal distribution, it tends to zero. The mode does. And so, basically, it's pretty much telling the DA system, ignore that component, go with the other one. So, that was one of the reasons we like the mode for this development. So, this is the definition of the multivariate log normal. Of the multivariate log normal? This term. The term there is what's going to give us the mode when we take the maximum likelihood approach. And so, as I mentioned earlier on, and something to take into account, the distribution here, the covariance matrix and the mean state, is of the expectation of log of x. Next part of the lecture. Next part of the lecture. Log of x, not x. Okay, so these are important properties. The natural mean of a log normal distribution is not this term here in the exponential. I'll show the definition in a sec. So you have to remember that you're creating these covariances in terms of the expectation of log of x and not x. This again becomes important when you're trying to solve certain problems. So we can take the look. So we can take the log-normal distribution, put in those two error definitions I showed earlier on, put them into Bayes' equation, take the negative log likelihood, we're getting there, okay. And this is the 3D bar cost function for a log-normal distributed random variable for the mode. So the solution of this is going to have this extra term here. So it involves this. That involves these extra terms here. Now, if these terms work here, the solution of that equation is actually the median. Let me rephrase that. Is a median of the multivariate double normal distribution. When you go beyond one, in PDF theory, the median, and the other reason why we didn't like to pick the median, is non-unique. Even for a bivariate gap, Even for a bivariate Gaussian, for every x1, there exists an x2 that if I integrate between those two points, the CDF of the distribution is 50%. Therefore, it doesn't matter. It's always non-unique. You would never know which median you found. But the median this does find, you do know, because it's the median that if I use the property that the log logarithm of a log normal random variable is Gaussian. Random variable that's a Gaussian, that is the state that comes out of the Gaussian distribution, the Gaussian transformed cost function. And under the preservation of percentiles, the median 50% percentile in Gaussian space is also the 50% percile in Lognombal space. And so I know this is that state with those first order terms removed is the Gaussian equivalent in 50%. So, as I said, the reviewer said we picked it, it was easy to find. It was easy to find, but that's not the point. As I've just mentioned, this state is non-unique. This is. It's unique and bounded by this negative exponential term here. However, when you go to a mean, you can't take the expectation of a vector. Of a vector. You have to do the vector of expectations. And so you have to do it component-wise and then develop the covariance matrix, which is what's going to combine everything. Whereas in the log-normal mode, that's implicit in its solution. There it is. This is unbounded with respect to variance. As the variance increases, the mean goes towards the tail because it's the minimum variance estimator. It wants to be the one that maximizes. Be the one that maximizes the description or minimizes the description of the spray. However, we found some interesting properties I'll show in a bit that maybe they all work. And you have to add another condition to work out which one. So As I mentioned, I believed that we did not have to assimilate the observations sequentially, that we should be able to minimize the errors simultaneously across Gaussian and log normal random variables at the same time. And so we were able to develop what we referred to, and it has had many names, but this is my favourite one, we were never allowed to keep it, Efflettosikansky distribution, or the FZ distribution. And the original paper we submitted to the Royal Statistical Society, and their response was, Well, anybody could derive that, why would we publish it? And like, well, because nobody has. Okay, I looked everywhere. I did an extensive literature review to try and find a definition of this distribution, and it wasn't anywhere. But the statisticians knew how to do it, they just didn't tell us. So my analysis lecturer, who was also my undergraduate, Analysis lecturer, who was also my undergraduate advisor at the University of Reading in the UK, would be proud because I had to do a complete induction proof to show that that was a PDF. And I hate complete induction. I just dread that in the exams in the first and second here. But I did a complete induction proof. And this now gives us the foundation for us to build these PDFs that can have covariances with a Gaussian that are non-Gaussian. And so, the really, really important part, even if you don't take anything else away from this, just this bit, and there's some pictures on the next slide. It's okay, I think there is soon. Anyway, so the mode of the mixed distribution. The important part is we have this correction term now here to the Gaussian mean component or the Gaussian mode component. That means that we can affect the Gaussian. That we can affect the Gaussian random variables from their covariances with the log-normal variables. And that was not possible if you did it sequentially. And the log-normal state is the same, just its covariance is with the other log-normal components. This is important. So if you have uncorrelated Gaussian log-normal random variables, this term disappears because these terms are all zero. Is because these terms are all zero and there's nothing there. However, if there are any correlations, this is a way to make sure you get that relationship between the Gaussian and the normal random variables. For example, temperature and mixing ratio. Humidity and temperature are changing, you're observing them, you wish to make sure that they impact each other. And I will show that this does do that. So, I hope you have, yes. So. So, as I said, it is a nice picture. You look really confused over there. So, here is a correlated bivariate Gaussian distribution. It's not circular because there's a correlation which makes it an ellipse. An uncorrelated bivariate log normal is triangular. So it goes like this and around and then comes back. When you add a covariance, it starts to bring in. starts to bring in this structure where you have this curvature coming in. Causing to notice first is that the bivariate mix distribution as it's called now actually kind of captures both components. It's got some symmetry that we would expect from the Gaussian. The Gaussian, skewness from the log normal, some of the some symmetry, but there's obviously the skewness, triangular but not fully triangular. The next thing to notice is where the Gaussian mode has gone. It should be at zero, as we see here, if correlated, even in correlated Gaussians, it should be there zero. It's moved down, it's here. Here, which means it is allowing those covariances with the log normal to affect the Gaussian component. I'm sorry. Okay. So we were able to take the mixed distribution and generate 3D bar and 4D bar for that. And I've put it in this notation just to make it easier to see the equation. Just to make it easier to see the equation. And I'm being nice to you because I don't have the incremental versions here on the screen. They're horrific, but we have also done incremental 3D bar, 4D bar, for each distribution. And we can define this cost function now, which is a 3D bar cost function that enables Gaussian and log normal background errors and observation errors to be simultaneously minimized and allow them to interact with each other. And allow them to interact with each other. And so this was presented in 2007, this form in the, I'm going to butcher this, I apologise to the Germans right now, the meteorologist Schistris journal. And it was part of a special edition after the Adroid workshop when we'd been in Erbergoegel in Austria. And when I was presenting this, And when I was presenting this, I remember Mike Fisher, those of you who know Mike Fisher, he was the head of data simulation at ECMWF about 10 years ago. He said to me, oh, well, the logarithmic transform, that just makes it easier. It's like, no, you're using the property that it's Gaussian. However, when you invert back to log normal space, you've got the median, not the mode. And so you don't necessarily have the most likely answer. You have the unbiased answer, but what does that mean if your distribution is skewed? What does that mean if your distribution is skewed? So, I've got some pictures coming in. All right. So, basically, Anton, my first postdoc, we implemented a 1D bar retrieval system using the mixed distribution approach for temperature and mixing ratio for AM2A and AM2B. And we published it in QJ in 2016, and we looked at the And we looked at the O-A statistics to work out how much of an impact we had from using this mixed approach to fit the observations. Now, one thing I haven't mentioned is the fact that the mode, being a function of the covariance matrix, is very sensitive to your estimate of that covariance matrix. Okay? So in this Okay, so in this, and this also leads into the machine learning part. Okay, we're getting there. Because basically, what we see here is we were able to fit better to the temperature channels by using approach. We were getting better estimates of the moisture field, which enabled us to fit better to the temperature observations. However, at this point here, suddenly the transform approach, which is equivalent to solving for the median, beats the mode. And we're like, well, wait a minute. So this property that you So, this property that you need to have a good estimate of the covariance matrix, because it's constant throughout this whole window here, kicks in. Or did it? This is the part that when I come back to where I said, we'll do in a sec, about which statistic do we use to minimize the error with. So, we've implemented this NSF on the previous grant. NSF on the previous grant gave us funding to put C1DO in near real time. So C1DO runs twice a day over the full swath and we produce these figures twice a day. And actually, this is an older one, very old one, but we actually have three swaths now. So we actually do retrieve in this area here over Hawaii. We don't have a land mask or cloud mask, so we don't do the retrieval over cloud or over. We don't do the retrieval over clouds or over land. However, we can compare it to NIAS, which is the NOAA operational retrieval system that uses the Lockheed Meat transfer. And we can see that we get slightly different answers. But what we also see there is that we just assume a Gaussian. We're incorrectly mapping or modelling moisture fields. But as I mentioned, As I mentioned, when we were doing the retrieval system, we discovered that we did the test case where we knew what the answer was, and our first guess a priori state was the true state. We just expected the log normal to give us it back, and then we would know that it was working, and that the median approach would show a bias because it didn't give us the answer. We got the complete reverse. Median fitted exactly, and the mode introduces a bias. And what we do. And what we discovered was, and this was knowing to do with the covariance estimation, that there are ranges of where any one of the three statistics can minimize your error, either the mode, median, or the mean. That became a really important result. Again, getting away from that mindset from Gaussian that they're all the same and that this is the one. So if your a priori state is underestimating your true state, the log-normal mode. State, the log normal mode, the mean approach can actually give you that correction to get you over to the correct state. If you're within a certain bound where you're quite close to the true state, the median just creates little changes and is actually the best one to use in that area. If you are overestimating the true state and you wish to pull the solution back, the mode is the best one to use. Anyway. Anyway, recently, and if Tijana is online, we were actually able to develop a log normal camera filter. So for 60 years, they've been trying to develop the log-normal camera filter. And the problem you have is you can't interchange the logarithm and the model. And so we said, well, why? Let's not. Let's just stick with the nonlinear model and just work through the derivation. And just work through the derivation, remembering that a log-normal PDF is defined in terms of the expectation of log of x, and so is its covariances. So, when we did that and defined the median cost function, which we knew from our previous work, we were actually able to derive the full set of non-linear Kalman filter equations that minimizes using the median for the log normal errors. And the important thing, and the reason why I added this slide yesterday. And the reason why I added this slide yesterday, sorry, was because of this term here, our log-normal counting gain, and how people were showing this as their solution solver yesterday. And this is how we get the extra terms that enable us to use log-normal observational errors, log-normal background, and can update the analysis error covariance matrix through this non-linear forecast error covariance matrix. So that was published in February. We actually had some very, very good results. We actually had some very, very good results with the Lorentz 63. All the references are at the end of the paper, end of the presentation. And so we just to summarise where we're at, we've developed a lot of different components now, but we're trying to get this towards pre-operational testing at NOAA, where we can sort of take the GSI or JEDI, depending on which one is actually the one that is the one we're told to work with, and try to implement these. And try to implement these. We've done the full-field 4D bar, like incremental 3D bar, 4D bar, have done the mixed count filter, and I've also just recently been able to complete the buddy check quality control for a log normal error as well. So as I mentioned, is log normal the only error that we need to be dealing with with moisture at least? And the answer is no. And the answer is no. Mike Goodliffe, who was my second postdoc, was looking at the WARF output. And the reason for this date is, whilst many of you may not know this, there was a horrific forecast bust that day. Horrific. But basically, they evacuated all these islands thinking that Hurricane Chris was going to come in and destroy them. By next morning, there was nothing left of Hurricane Chris. But a full evacuation of the Auto Kilis islands had occurred. What was causing the problem? This love-normal dry air was being ignored by quality control as being too far away from the wet background state. And as such, it didn't know what to do with it. So it was just keeping all this wet, the bias from the model, kept it wet, and therefore it enabled the hurricane to intensify. So we're in the process at the moment of So we're in the process at the moment of doing this study now to show that log normal people know that the system is going to be destroyed because of the dry air. However, as you can see, there's another distribution, this blue one. Yeah, the blue one. That's showing a, so this is the skewness coefficients. So it's showing a negative skewness, which means, or positive skewness, that that is reverse log normal, that it's got an upper bound, that it's skewed to the right, not to the left. Right, not to the left. And here's an example. My postdoc's from Belgium, so he took a city in Belgium, I think it's Antwerp actually, and did a climatology over the relative humidity for September of last year and found that we have a semi-log normal signal here over Colorado, but that there's a reverse log normal signal in Belgium at the same time. In Belgium at the same time of the year. So, if you're doing a global simultaneous minimization of your cost function and you assume the same distribution everywhere, you're introducing biases in Colorado and Belgium because that is clearly not Gaussian. And the other thing to notice is the Gaussian approximation here is going beyond the physical lower bound of zero. Same here, it goes beyond the physical upper supersaturation bound. Bound. So just very quickly, this is the univariate log normal, a reverse log normal distribution. It's now defined in terms of zeta, which is this upper bound, equals i. And we can then now do exactly everything I've just shown for the variation or for a reverse log normal. We can also build up a multivariate PDF that enables Gaussian log normal and reverse log normal errors to be simultaneously minimized. Be simultaneously minimized. That PDF is in my second edition of the textbook, so that one there. We go into details and show how to do all that. And just recently, Senna generated the reverse log normal calm filter with a mixed Gaussian. And that, like I said, just went to QJ last week. So, to the machine learning very quickly. So, what we're trying to do here is: when do we know? When can we predict which distribution we need to minimize with respect to? Which version is. With respect to, which version of DA should you be using. So, this is just the Lorentz 63 model, and Mike, in his 2020 paper, showed before any machine learning approaches, just building climatologies through different subsections of the Lorentz 63 component, showed different distributions. You have Gaussian, showed reversal abnormal, showed log normal. But we didn't know what the reversal abnormal was back here at this point. And so, even just from climatologies, we can see the difference. So, even just from climatologies, we can see the distribution is changing depending on where we are on the Z component of the Lorentz 63 attractor, given the values of X and Y. So, we used two different back then, two different machine learning approaches. We used support vector machine and a neural network. Full details of how we implemented and set up those. And set up those approaches can be found in Mike's 2022 paper. And the SVM actually did quite well. It was able to predict fairly accurately, and we were able to reduce the error by using a log-normal or Gaussian approach with the Z component of the RAN63. And in my previous postdoc to the current one, put in a K-nearest neighbor approach and Nearest neighbor approach and showed that it worked exceptionally well and was incredibly fast. Zenner has also implemented it with a slightly different distance measure and has shown to match the results. But again, as we're all dealing with machine learning, you've got training, then you've got verification, and then you've got prediction. And what Jakir did a study of was how many steps, how many Steps, how many time steps do I need to train this? Do I really need to just throw a hundred thousand time realizations to work out what's going on? And he actually showed for the Lorentz system, given these different radius to determine the skewness, and that's how we were detecting a non-Gaussian signal with skewness, because the Gaussian would be zero, non-Gaussian would be non-zero. He was able to show that 36,000 time steps is enough to train the K-nearest neighbor. That if you actually give it more than that, it actually degrees. That if you actually give it more than that, it actually degrades its performance. He's writing that up right now, so I can't give him what you have. But this is the big file. So that's just a summary of what we found, that you can actually give it too much data, but also even using the same amount of data, that for some reason K nearest neighbor works really, really well in the Lorentz system. In the right system. Next. Silence have got animations in, that's what's slowing it down. But anyway, so Senna, as I said, used a different form of skewness test and was able to show that the k-nearest neighbour was able to predict correctly the log normal, the Gaussian regions, and then the reverse log normal. Regions and then the reverse log normal, and it actually showed that these other areas around here also would display because the K-earse neighbor was allowed to go beyond the trajectories of the system, showing other regions where if the solution goes into here, these will also be log-normal or Gaussian. And it had a predictive rate of approximately 98%. Pretty good. So Yeah, I'm doing better. So, sorry for such a hard, heavy section in the morning. Those of you who've been to my talks before know that's kind of what I do. But what I'm trying to do is I'm trying to lay the foundations to convince you that we can go non-Gaussian and that we can use machine learning to be able to optimise when we should go non-Gaussian. If we can stay Gaussian as long as we can, that's great as well, because that's easy to solve, we've got all this theory, we know what we're doing. But when we have to go non-Gaussian, we need to be pretty sure that we're in the correct. To be pretty sure that we're in the correct version of non-Gaussian. And so we've shown that it can do quite well. I mean, we're in the process now of developing the reversal of normal version of the C10 retrieval because as I was looking at the output two weeks ago, I noticed we were underestimating part of the atmospheric river in off coast of California because we were trying to fit a Gaussian, we were trying to fit a lot of normal to it. And actually, we were going, you know, we were being drawn. And actually we were going, you know, we were being drawn this way towards a dry air and wanted the m the mechanism to actually push us towards super saturation. It's an atmospheric river, we want it to be full of water. And so now we're in the process of developing a reversal of normal version of C1 DOS, and hopefully have that running in real time as well. And so we can see, and then also using machine learning to try and detect actual global atmospheric conditions when we should be using retreat, just a retrieval. C1DO is only defined over seven vertical levels. Is only defined over seven vertical levels. There's temperature and mixing ratio and then surface emissivity. So we're only inverting 20 by 20 arrays up. We can do them quickly and we can show that there's an impact because we are actually using physical observations that are assimilated into the global NWP systems. We have also developed the MLEF to be LMELF and RLNMEF so that we can then use So that we can then use ensembles. And eventually, the goal is to combine the MLEF with our full incremental 4D VAR to have a full non-Gaussian mixed hybrid 4D VAR system and to do extensive testing. And there are the rec releases. Get on time. Thank you for the time. We have just time for questions. Great presentation. I wish to have heard this a decade ago. We did not make it beyond a toy model of your 2006 paper. There are several issues. Let me start with a practical one. Whether, from a practical viewpoint, as I understand it, is whether we take Whether we take Gaussian or log normal is dependent on the for measurements, for the preciseness of the measurement. So if you approach close to zero, then the tendency would be more adopting the log normal because we all have positive semi-definite quantities, especially in chemistry. And if you go away, we would like take the Would like take the easier way and go to the Gaussian. Do you have a recommendation how to tackle with this transition? So, I mean, that's what we're using the machine learning for at the moment, is to try and determine in advance, can we predict, okay, we're seeing these circumstances, yes, it's this, but we know within a certain time period upwards it will transition to the next phase of the distribution. Phase of the distribution. And so in the Lorentz system, that's what my postdocs have been doing. But also, Jakir has actually done it, as I mentioned, even through an assimilation 40-bar window, those observations can change. And so he's trained the machine learning, he had trained, he's left now, he's actually at NOAA in the Marine Division. But he's writing it up like, I said he's writing it up right now, but basically he was able to determine from the machine learning when to switch in the observation window. When to switch in the observation window, and it knew which one in advance. He hasn't shown me the accuracy of it yet, but he showed that it worked, and he literally did this three days before he resigned. But it is doable. The K-nearest neighbor, at least in the Lorenz 63, is very good and has quite accurate predictability of when the error is going to change in a certain skewness window. And so, given, we kind of optimize it that you need about 12. It that you need about 12 time steps either side, 12 time steps window to determine the skewers. And then it's trained on knowing what the x and y values are from the skewers, that when those x and y values are obtained, then it knows that the z component is going to get ready to switch distribution. So that's what we're doing at the moment to try and tackle that, to determine when. Is this inside the same optimization process? Yes, it's inside of the 40 var window. He actually It's inside of the 40-var window. He actually uses. So, when he's setting up the cost function, when he's doing the summation term to determine the observational innovations, this does not put up the gradient? Well, but the thing is, so what I didn't show is in the 2010 paper in TELUS, is that for 40-var, you can define what's called a Bayesian network through the window. And so you solve what's called a multi-event Bayes problem. And so that great. And so that gradient of that distribution is, because it's defined in terms of the individual PDFs, that is the correct probability Bayesian approach for what's going on in that window. And it's completely independent of any assumption on the distribution. So in theory, the posterior mode of that expression is the best estimate. So there may be some numerical things going on, but at the moment in the Lorentz system, But at the moment, in the Lorentz system, he has it running and it's stable. We have a certain let's see showstop of us. You have to calculate in some way the roots of the square roots of the covariance for preconditioning issue. And that starts to become in practice then quite challenging. And if our experience is if the There is some change with the minimization, and there's some little bit of jump, you'll never find the real minimal. So that's why we're moving towards the MLEF in its formulation, and we have also developed the MLES. And so basically, the MLEF creates the Hessian preconditioner, and as such, because you've got the Hessian preconditioner, which is not quite. Hessian precondition, which is not quite the Hessian, but it's a very good approximation to it. The MLEF can capture all of those information and actually only takes five iterations to solve, even for high-dimensional problems. We're still basically assessing it with the log normal, but the Gaussian version is very effective. Media developed that in 2005, and we've now developed the log normal version of it and the reverse log normal. We haven't done the reverse log normal or log normal MAES, but we've Log normal MLES, but we have its formulation is similar to the MLEF, but now with a four-dimensional array rather than a three-dimensional cycle state. So we're in the process of trying to find ways to do it through time with preconditioning. That's one of the problems. The center at the moment is trying to implement it into the GSI, and those preconditioners are causing us trouble trying to get the log normal version into A lot of normal version into it and working out the T and the T S and U D transforms. But we are working on it and trying to find ways to do the preconditioner as well. Excellent. Maybe an additional question? Yeah, I'll ask a quick question. So I wonder if the distribution is not skewed, that's a symmetric distribution, but uh like the katosis like factorial number size and the common number distribution. No, I mean, so basically, an easy example is the exponential distribution, of what you're saying. I mean, it comes, it's symmetric, but it comes right up to a point like this and is present in nature. No, we haven't looked at that because again, I don't, I don't, you've got to have to differentiate across that peak. And the, it may be, it may be L1, it's not going to be L2 or whatever it is, across it. Whatever it is, cross it. And so we're trying to. So there is work that Craig Bishop has done. I mean, I am going to talk: the gig filter, which tries to look at those points and sort of use different types of distributions to model that. The inverse, so the gamma, inverse, gamma filter developed. Looks for so like when you have pre-sip and you do have pre-sip and you don't have pre-sip, and it's meant to be able to do a delta function to know when there is pre-sip and when there isn't, and that's what inverse gamma does. Isn't and that's what inverse gamma does. He builds up all the peak priors and posterior thing, yeah. So, but no, the log normal would be smooth across that peak, so it probably would assign again, it would underestimate the right peak, the right side of it, because it'll skew it, or overestimate and go skew, and probably underestimate on the left side because it wants to go this way. But the thing is, the exponential isn't smooth at the centre, that's the problem. Centre, that's the problem. It comes to a point and comes back down again. So, yeah, trying to develop stuff you're differentiating going into that point, it's not going to be numerically very stable, similar to what you were mentioning. So, it's the log normal would not be a good approximation. But the Gaussian is going to do the same thing, it's just going to fit a real bell curve over that peak. Yes, it has a local field. So you mentioned the nearest neighbor. For the regular prediction, the discontinuity can be an issue. Because the nearest neighbor can drop one rubber. Is it a mixture better? I don't know. We were shocked by how accurate the K-nearest neighbor was relative to the other two. It's a low-dimensional system. It is a low-dimensional system. Yeah, I'm not. We were surprised, but it beat SVM and neural network in that low-dimension. So he hasn't looked at. So I haven't he hasn't looked at discontinuities. I mean, that's something I can talk to Senna about when I get back next week. We're trying to use it with the ward system to basically, you know, if you've got a front, then it's going to change across that front and see how well it does. Mixture of what? Mixture. When you do the nearest, this is the nearest neighbour, you do the mixture of the nearest neighbors. Okay. Yeah. Okay. Yeah. Definitely talk to him about that. And now we're switching.