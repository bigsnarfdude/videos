I said that I'd be talking about delays in physiology and that there could be transport, communication, processing, but in physiology they all tend to get mixed together. So I want to start today with some examples. Don't worry about the math on these slides, it's just there for some window dressing. So this is the Goodwin operon model, which basically describes the transcription of mRNA, then the translation Then the translation of intermediate protein, and then the production of an effect protein. And there's a schematic of it here. And if you want to know the details, there's a 74-page paper we published this year in JMB. But this you've got to think of, this is a process going on inside a single cell. And we've modeled this with, well, I should say we, the model was first written down in the 60s without delay. 60s without delay, and then considered with constant delay in the 70s and 80s. And the dynamics don't change when you add a constant delay. But we added state-dependent delays defined by threshold conditions, and that does change the dynamics. So the idea here is when you have a piece of mRNA being transcribed, that takes time. And also it's a process going on on the strand of DNA. And so not only does it take time, it has a speed that it goes. Take time, it has a speed that it goes along and transcribes. And that speed you call V for velocity, and there is a length that you have to transcribe. And if the velocity varies, then the time it takes you to get to the end is going to vary as well. And so we have this, the delay here is implicitly defined as the integral of the velocity over the distance you have to go is equal to the delay. Tau J. Well, tau j is tau m or tau i. Okay, every j gets replaced by m or i because I've got two of these delays and they're modelled the same way but they can have different functions. This form, you'll see it again in a minute. Okay, so if you look here, this is the Burns-Tannock cell cycle model, which was developed around about 1970. Was developed around about 1970s, it's 50 years old now. And in 1978 already, Mike Mackey had written this down as a delay differential equation formulation. If you delete this beta term here, it becomes the Mackey-Glass equation, but actually it's a bit more complicated. But he was looking at the two equations at the same time. And the cell cycle, the cell has a resting phase, then it enters the cell cycle, it goes through gap 1, then edge. It goes through gap 1, then S, then G2, then mitosis, and then it divides in two. So this is a cell dividing in two. Interestingly, here, the S phase is the synthesis phase. That's the phase where the DNA is being transcribed. So basically, the delayed differential equation on the previous slide is just inside here. And now, rather than modeling all of the proteins and DNA, everything has to be done in the cell cycle, we model it as a single As a single delay differential equation. And then you get to what I've spent most of my time in recent years working on, which is hematopoiesis, the production of blood cells. And your body produces about 10 to the 11 blood cells per day. So that's 10 to the 11 burns, 10 cell cycles per day. So if I wanted to model everything in what's going on in this biology, I'd have 10 to the 11 copies. I'd have 10 to the 11 copies of the previous equation, and each one of those would have a bunch of copies of, and soon I've got more equations than stars in the Milky Way. So we have a macro model to simplify these things. Here is a model for the production of whitebud cells that's a few years old now. Don't worry about the details here, okay? This is Erika's model. Its rate of change is stuff coming in minus stuff going out. Minus stuff going out. Or sometimes I do it the other way around. Actually, no, radical. This is stuff going out, this is stuff going in. So these are stem cells which are either differentiating into precursor cells or some of them are differentiating something else and some of them are entering that cell cycle. And here they are coming back from the cell cycle some time later with an amplification factor. And then the cells that differentiate into precursors of white blood cells. Into precursors of white blood cells, they have to spend some time dividing, so I get an amplification factor AN and the time Tor n that they spend dividing and maturing before they get into the bone marrow reservoir is given bonus. There's this weird term here and it's a ratio of two velocities and it's the velocity that the cells are maturing at when they finish maturing divided by the velocity they're maturing at when they started maturing. out when they started maturing. We'll come back to that one later. In fact, I'm going to do it now. So there's a slight danger with this concept of doing rate of changes in minus out. Okay, so if I have this as my model for the number of cells in the bone marrow reservoir, so these are the ones going into circulation or dying, these are the ones coming from stem cells. And if I decide the Cells, and if I decided to just say, oh, I'm going to make it state-dependent, so I'll make the delays depend on torque, this equation is wrong. Can't just add delay like that. And the reason why you can't just add delay like that is if you think of the maturation process as a conveyor belt, and you think of your precursor cells as just suitcases being thrown on this belt, and for simplicity, imagine they're thrown on. Imagine they're thrown on the speed of one a second. The belt travels at some speed, that doesn't matter. If it's traveling at constant speed, if you put them on here at one a second, sometime later they'll fall off here one a second as well. Okay, but what happens if you have some event, you have some cytokine signaling, and you say, oh, I want to speed up the maturation. So you throw your suitcases on here one every second, and you keep doing that. But before they get to the other end, you double the speed of the belt. To the other end, you double the speed of the belt. Well, now they're going to fall off two a second until the ones that were put on at that new speed start falling off, then you go back to the speed you had before. And so the rate that they're falling off the end of the belt is given by this ratio of the velocity the belt's going at this end to the velocity it was going when they were put on at this end, multiplied by other stuff. So if you change the rate that you're putting them on, that changes. Change the rate that you put them on, that changes this term here. So you get this term. And this scave picture is actually taken from this paper by Bernard, which was actually a commentary on our paper. Certain horses of mass balance. Yeah, if you did. So we didn't have this in our paper. In our paper, we had a PDE argument and we did that mass balance so that we had conservation. Balance so that we had conservation of mass. Exactly. So you can do that that way and make that work. So that's what I said here. Now, this correction term in the state-dependent case, I found it as far back as a paper of Housemiths in 93, and it appears sporadically in models since then. So the people that actually do the model from A structure PD and solve PDE and pay attention to the boundary conditions get this term. If you just do in minus out, you dug. If you just do in minus out, you've done it. So half of the models are missing this, and half of them have it. But it needs to be there. We've recently even generalized it to a case where the conveyor belt doesn't have an end and things fall off the side with a probability distribution. So you have your maturation age is not fixed, but it's probabilistic. And you still end up with this ratio along with a hazard function and all sorts of other things. And all sorts of other things. Interestingly, this implicitly defined, so this is the version for the blood cells. This is a bit problematical numerically. So we several interesting things go on here. First of all, if you just use Leibney's rule and differentiate this and rearrange, you can actually create an extra differential equation for the delay. And sometimes in simulations, it's more convenient to use this as a To use this as long as you pay attention and use the correct initial condition, otherwise, you get the wrong answer. And secondly, if you rearrange this, you can write it like this. And because we always keep our velocities positive, then we get that the delayed time, t minus tau, is actually an increasing function of current time, which is nice for some of the theory of the entities. If your delayed time is a decreasing function of current time, then you get weird things with things hopping. you get weird things with things hopping back on the conveyor though, which we don't want. Okay, this is supposed to be a talk about uh dynamical systems, so let's go back to the dynamical systems formulation. So I had um I had u dot equals, ignore the t here, I had u dot equals f of u t before for my ODEs yesterday, and now I want to add a dependence, and let's do it for some discrete time. And let's do it for some discrete time initially. So u depends on t, so f depends on u of t minus 4 as well as u of t. And now you can see with this set up here, if I want to get a unique solution of the initial value problem, which remember is what I need for all that dynamical systems theory to work, it's not sufficient just to tell you what the u of t0 is. It should be u of t0 as well. I need to evaluate the right-hand side at t0. The right-hand side at t0, so I need u of t0 minus tau. But then when I start solving the equation forwards in time, for every value of s between t0 minus tour and t0, I require the value of u of s to evaluate this right-hand side here when t gets to s plus tau. And so to actually, for uniqueness of the IVP solution, I need at least to define an initial function on this. Initial function on this time interval from t0 minus taught to t0. And it turns out, provided f is sufficiently nice and Lipschitz is enough, then that's enough for local existence universe. Now, if you remember my moral from yesterday, the phase space of a dynamical system is your set of initial conditions, and so now for my DDEs, it becomes this space of initial functions. Okay, let's make it even worse. Because I have to specify an initial function over an interval, then I have an issue at time t0 because the derivative of this function can be anything or needn't exist, even. Most users like to be really simple and take it as a constant. But there's absolutely no reason why, when you evaluate the right-hand side of the differential equation at time t0, that that ought to be equal to That ought to be equal to this. And so, what you end up with is you end up with: if you think of this as defining u here, and then this defining u positive, you get this discontinuity in the derivative at the initial time, and that's called a breaking problem. Okay, if I assume my f is smooth and I can differentiate as much as I want to, if I differentiate, then you get this, and you see you get u. This and you see, you get u dot of t minus torr in here when I define the equations for u double dot. And because u dot has a discontinuity at time t0, u dot of t minus torr is going to have a discontinuity at time t0 plus tau. And you can differentiate as much as you want, and you'll get that the k plus first derivative is discontinuous at this time. So there's a couple of interesting things about that. About that. We have a smoothing thing then. So u gets smoother as we go forwards in time. It has all these breaking points, though. So it'll have, for all finite time, it'll have a discontinuity in some derivative. But if you're solving this numerically using some kind of Runge-Kutter method, then if you want, if you know, people like to use the fourth order of Runke-Kutter, okay? Your solution has to be four times differentiable to get the full. Differentiable for to get the fourth order. So if you so you have to you can't integrate across these breakpoints. Now, if the delay is fixed, that's very easy because you just you choose your step size so you integrate to them and then you do the other side as well and everything works nicely. The other thing that's nice, of course, in dynamical systems, I'm interested in variant objects. So if I'm at a steady state, my function's constant, that's smooth. If I'm on a periodic orbit, so imagine, imagine. So imagine this solution gives me a period orbit. Well, what happens in that case? Well, in the periodic orbit, remember, we go around, we come back, and we keep going around. We do that infinitely often. So over infinite time going around and around the loop, you get infinite differentiability. So a periodic solution has to be infinitely differentiable. And Neussbaum proved in the 1970s already that for a constant delay, the solution is actually analytic, not just split. Although, very interestingly, if you have delays that vary in time, then you get really weird stuff going on. You can get a curie orbit that's infinitely differentiable all the way around and is analytic at some points on the period and not analytic on others. Sorry, say that last part one more time. You can have a puric orbit where the solution is infinitely differentiable everywhere around the orbit, but it's not analytic everywhere. Everywhere. So to be analytic, it has to be not just infinitely differentiable, but it has to be equal to its Taylor series. That last step can fail. And what happens if it fails? John May, Pran, Roger Newsbaum write 70-page papers. And why does that happen? There's banners bases and all sorts of. Well, because some of the machinery doesn't work. I mean, I find that stuff fascinating, but I don't. I find that stuff fascinating, but I don't understand it all. And I'm not going to try and explain it to you. I just want to let you know that there are some complications out there. In another, what it says is when you try to, the reason why people sometimes have trouble with or avoid DVEs is that they're difficult to use. There are a bunch of pieces that you have to pay attention to. Yeah. So, Tony, is there a way to do it, if you want to initialize one of these things with data? One of these things with data that is do people have versions of, say, data assimilation or whatever that cope with these, you know, you look at a window and it's missing the first half of the version? Right, so you need to speak to exponent stochastic differential equations to get an idea of what's going on with that, right? And the guy that I know is an expert on all that stuff, it's my PhD advisor, Andrew Strip, and he moved into that like immediately after I've been. Into that, like immediately after I finished my PhD, and he's a clever guy, he works really hard, so I decided I'm never going to touch those questions. So, you need to ask him? I think the answer is no. I've got to talk to Andrew about it, and I think it's not something that exists. I think he's not uninterested in it. I mean, I think he doesn't like delays for a variety of sort of preferential reasons. But like the infinite dimensional problem that Tony is talking about is that bothers me. This is really the bothers me. But I'm curious. Can I ask another question, Tony? Yeah, can I ask? So, okay, so I have a discrete delay, like minus tau set up. Is there an ODE in finite dimensions that is Markovian on its state space or whatever, that is the same as that minus tau lag. But not necessarily, or sometimes. We come back. I'll answer a relative. I'll answer a related question before the end before I get time. Three minutes. Probably. If you and Will are splitting. Oh, right. Yeah. Well, then my middle one. But you guys can talk. Yeah, we can talk. We can talk. I can show you the slides afterwards, okay? So now I want to write my DD as a dynamical system. So I'm going to write, these things are traditionally written this way, as little functions use subtitles. Written this way as little functions u sub t. So every time t, you have a function defined over the delay interval. And this is that the set of these functions for the different t gives you your solution. And your phase space is actually the set of continuous functions defined over this interval. And of course, this space of continuous functions over some finite interval includes all the polynomials, and so it's infinite dimensional even for scalar problems. Now that's really bad in one way. Now, that's really bad in one way. It means you have to deal with infinite dimensions. It has two good things. Andrew Stewart doesn't work on the problem, so it doesn't go physical to get there. And secondly, because the scalar problems are already infinite dimensional, they can have really interesting dynamics. So the Mackey Glass equation is a scalar equation with a single constant delay, and it can have chaotic dynamics. Okay, you can linearize the You can linearize that the mess, but you can do it in RD as well. That's maybe, if I'm out of time, let's just stop with this. So, these are a few examples. This is bifurcation diagram for Mackey Glass. Down here, the steady state stable. There's a Hoff bifurcation. You get stable periodic orbit, period doubling, another period doubling. And then after the second period doubling, there's a rapid transition to this gray area, which is chaos. There's period doublings here. But then there are other weird bifurcations. But then there are other weird bifurcations here. There's even a cuss point and bistability of periodic orbits. There's a fog bifurcation, and this is a boundary crisis bifurcation. This is the Burns-Tannon model. This is showing chaos in that. This is the operon dynamics, and this is showing a periodic orbit which is coming into touch at steady state. So this is very close to homo clinic. And here is a continuation in Continuation in parameter space of a periodic orbit, and it goes around this corner, and it's going homoclinic sort of steady state here. Okay, I want to talk about distributed delay differential equations, but I need to let Will talk. So, what I'll do is I'll talk about those on Thursday when I've got the number 20 minutes. Okay, so I'll stop there. Yes, sir. Don't know where you were, so. All right. Thank you, Tony. Left alright. Awesome. All right, so I'll roughly pick up where I left off yesterday. I'm going to show you some more recent work that I've done with Dave and George and Bargov, who will speak later this week. I'm also going to try to use this talk. This week. I'm also going to try to use this talk as a, well, a demonstration of what a barrier might be to how dynamical thinking can be used to translate. Okay, so let me quickly summarize where we were. So I had this recipe, right? The talk yesterday was about this recipe. I have a way to produce this hyperbolicity or chaos, right? I want these ingredients. So I want something we Ingredients. So I want something weakly stable that's invariant, like a fixed point or a limit cycle, so a fixed point or a periodic orbit. That weakly stable thing, I want it to be such that there is some sort of shear effect near it. So these two things make my system excitable, and then I want to kind of activate that by gently kicking the system so that the gentle kicking will interact with the shear, and that interaction will produce the chaos. And that interaction will produce the chaos. So that's the recipe. So then yesterday I said that let's get at this by trying to find it in the simplest possible model. So that's this linear shear flow model. Graphically, I represented it this way. Let me get to the equation. So remember, I wrote down this linear system in its unforced state here, and then I added this pulsatile force. This pulsatile forcing. So I have this cylinder flow, right, where everything just collapses down to the z equals zero level while spinning, right? But crucially, the rate of angular rotation depends on the height. Then I took that simple system on the cylinder and I applied a sequence of kicks to it. And I wanted to know: okay, so will this system produce the chaos that my recipe is looking at? Is looking for. And the answer is yes. So going back to this picture for a moment, I want to tune the parameters so that the geometry in this picture is put into the system. So I want my limit cycle, which is just circulating from left to right at the zero level around the circle. I want that limit cycle to be kicked in such a way that I make a wave and then That I make a wave, and then between kicks when the system is just relaxing, I want that wave to stretch and fold. So, to make that happen, we said we needed kind of four things to work out. So, I want this key diagnostic to be large. So, I want a lot of shear. I want to kick with some non-trivial amplitude to get things started. I want to contract weakly during the relaxation between kicks. And then finally, I want. And then finally, I want enough time between kicks to allow that stretch and fold picture to take place. Right? So if I tune the four parameters this way, geometrically at least we're in position to see this chaos. Okay, so is that a theorem? The answer is yes. And the theorem is based on kind of thinking about time. On kind of thinking about time in the following way. So the system I wrote down is continuous time, right? But the kicks I'm applying are these sort of instantaneous things. So the way we think of this theorem is in terms of a kick relaxation cycle. So what I do is I first kick, okay, and then that happens first, and then after that I post-compose with what I'll call Compose with what I'll call relaxation. Okay, and if I do these two things together, I get something called Ft, which is the time T map. Right, so for the theorem, what I do is I look at this kick-relaxation cycle. So I kick, and then I let the system relax until I kick it. Until just before the next kick, and that is an operation that takes capital T units of time. If I just think of it as one object, it's a time t map. So I start at a point in phase space, I kick it, I let it relax, I see where it ends at the relaxation cycle, and that's the result of applying this function. So this is just a phase space function that takes inputs to what happens to those inputs after kicking and relaxing. And so we have a theorem about. And so we have a theorem. Well, we, Lai Seng Young and Don Wang have a theorem, which says that the geometric picture that I outlined is rigorous. So the important part of the theorem is right here. So if that factor that we identified as the key factor is large, then you will see chaos as long as T is large enough. Okay? So that's the story. This is a complicated thing to prove, and I'm thing to prove and honestly it uses many of the major developments in dynamics since the 70s. So hidden in this is a lot of serious mathematics but the take-home message from this model is that we can produce this chaos as hoped from that recipe I outlined with a simple model. And I think the significance of this is that if we can do this for a kind of a simple model that we cooked up, it seems reasonable that this geometry It seems reasonable that this geometry would then be available and important in just many nonlinear models. So that's the reason for this model. Yeah, now so n goes to infinity here in the sum. So this means that you have to look back infinitely long. Oh, yeah, so the sum, that's just my way of writing down the... Writing down the kicks. So this is just convenient notation for the idea that I run the unforced thing between kicks, and then every multiple of capital T, I just stop the differential equation, and I literally just apply the kick. So this is this time interval. Yeah, so capital T is the time interval between kicks. So this is just a compact way to write. So, this is just a compact way to write flow, stop the flow, kick, flow, stop the flow, kick. Do that over and over again. Yeah. Yeah, the delta there is meant to be like a delta kick. Okay, so now let me tell you about some recent work after I give a geometric picture of the theorem. So, the theorem says this: like, this is how to think about this geometrically. So, really, About this geometrically. So, really, okay, if there's no shear, this is what the geometry would look like if I now switch from the cylinder to the plane for ease of illustration. So, this is how the dynamics would look. So, if there's no shear, right, then when I kick, say the limit cycle is now this red circle, I deform into something, star-shaped in my example, but then the relaxation just completely kills the effect of the kick. Of the kick. So kick, but during the relaxation phase, shear doesn't do anything because it's not there, and so I just basically contract to what I had before. So this kick relaxation cycle in the no-shear case would just do that, right? So you wouldn't see anything interesting. But in the case where shear is present, this is what can happen. So the first kick is as before, but now during the relaxation phase, these During the relaxation phase, these saw teeth emerge, and the point is, I'm going to do this over and over again. So, this sort of generation of saw teeth amplifies during the next kick relaxation cycle. So, over time, as I iterate this process, I generate this very complicated geometric object on the right there. And thinking in terms of attractors as in the Lorentz attractor that Tony showed. As in the Lorenz attractor that Tony showed yesterday, in this situation, this dynamics here is going to produce an attractor that's just kind of a diffeomorphic copy of the limit cycle. So the attractor up here is just something that looks just like the limit cycle, nothing new. But the attractor down here is a very complicated fractal object consistent with chaos. So there are a couple of key pieces. One is that when you wrote out that dependence on the shear, the hope is, or in general, people have been able to find, that you can identify the parameters that allow this feature to be present. The other piece that he hasn't mentioned is that it matters where on the orbit and when on the orbit you kick it. On the orbit, you kick it to get this transition in statistics. So, in one case, you have a set of statistics for the top with no shear, you have a set of statistics for the bottom with shear. The trick is, is that with this phenomenon, when the shear is present, which is difficult to detect, where you kick it on its orbit and how often you kick it can change the system from having one set of statistics to a different set of statistics, which Statistics, which, if you're thinking experimentally, completely destroys reproducibility because it matters when you kick on the orbit. So if I ate 10 minutes from now, this phenomenon wouldn't be there. But if I eat now, the phenomenon is present. Does that make sense? That's a big problem for experiments. We eat. We have exactly the same problem in a model we were working on where we were applying chemotherapy periodically to the production of blood cells. And you give a drug called GCSF to offset the killing of the white blood cells, which is quite accident as a side effect of the chemo. But where you give that in a cycle of chemotherapy drastically changes what the What the level of white blood cells in the blood is. And that's the limiting factor for the amount of human. So that's the, as he's going through this, that's the piece. But the hope is that in that previous slide, he showed you there was some shear terms that you could potentially calculate so you'd know with a lot of these models when this phenomenon may be present or not. Make sense? Sense yeah, to your point, Dave. So we so there so we know some things and we are missing some things in our knowledge. So so so we have been able to quantify shear in a variety of nonlinear systems. So I put a few references here. Maybe I'll focus on two. So we've been able to quantify shear for periodic orbits in general. So I cooked up that sort of fake model where I could look at the parameters and I said, oh, hey, look, sigma, that's shear. And I said, oh, hey, look, sigma, that's shear, we're done. But in a nonlinear system where somebody just hands you a periodic orbit and says, go, what's the shear? So we know how to quantify that in general. It can even be done for certain particular periodic orbits in PVEs. So we do know how to quantify shear for nonlinear systems, which is good progress. But this talk is about a situation we understand less well for sure. Less well for sure, which is what if delay is present? So I think we've heard many talks yesterday about systems that have delay in them. So delay is a prevalent feature of mathematical physiological models. And so I guess, so Dave wanted to know, well, okay, so if I have a system with delay, how does this sort of shear-induced This sort of shear-induced chaos story works when delay is present. So that's what we tried to look at. So, what I'll show you next is some numerical experiments that support the claim that that very simple model I showed you first, that the dynamics that that model produces are relevant in kind of real systems. Okay, so let's see what that's about. Okay, so I'll talk about. Okay, so I'll talk about a case study that we did on the Altridian model. So, this is a model for the glucose insulin system. I won't go into details because the structural details are all that I care about for this talk, but basically this is a compartment model with three compartments here in blue, red, and gray, and a number of feedbacks between them. Most importantly, for my purposes, in this model, there's one feedback that's delayed. So we have a bunch of That's delayed. So we have a bunch of connections here, one of which in the model is delayed. The differential equations look like this. So don't worry about the details. Let me just highlight what the color terms are doing. So the state variables are up top. There are three of them. Alright, and now the key feature that I want to show you in this model is the delayed feedback. Feedback. So here in blue is the nutritional driver of the system. So this is the external glucose signal that drives the glucose dynamics. And then what's happening here is the plasma insulin is signaling glucose, right? But not directly, right? So I don't have an IP here. So the plasma insulin signals the glucose, but it goes through this filter. It goes through this filter. So I have these three fake variables. These are not variables like these. These are sort of this delay filter, and then the delayed version gets fed into the glucose equation. So this is a delay like Tony was describing, but not explicit. This is not a DDE. This is not a DDE. This is a six-dimensional ODE where the delay is being represented as this three-stage linear filter. So it's a delay system where the delay is being represented kind of approximately, I would say. Okay, so this is the system we wanted to look at to kind of validate our claim that the chaos that the recipe predicts can be. That the recipe predicts can be seen in models that people use, as Dave likes this model. I think it's okay. Dave likes this model on some days. And I particularly hate this model. And Bruce does not like this model. But then another point I can make is that I think that we use this model does not matter because I think it's all about the geometry. So this model is controversial, actually. It's probably the right thing to say. I don't dislike, I don't, I'm not. I don't dislike I don't I'm not in love with this model it's just that when you estimate uh things with data it ends up working better than all of the other ones. That's the only reason. I don't have that intrinsic like that because you're right. And I dislike the model because that F4 is supposed to be a representation of liver absorption or production and at a steady state insulin level. State insulin level, it gives you the wrong result. But your new model will fix this, right? We'll see. I mean, I also found that if you didn't observe insulin and we tried to use this model with data, it wasn't any better than an exponential decay, right? One steam. That's such it was often better. Means that it's not a good idea. Yes, I will return to the delay story. But yes, controversial model. Okay, so what did we see? So we basically possible to talk. So when you're, first off, I don't understand any of this, and that's my upfront disclosure. But the question. But the question about an assumption of steady state insulin, steady-state insulin or steady-state insulin delivery, or simultaneous action on the liver, skeletal muscle, etc. None of that is real. So as a clinician, I could never, even if I'm infusing insulin, I cannot count on that being steady state unless I've done a pancreatic clamp where I've given spin. Clamp where I've given spatostatin to stop insulin secretion. But I also simultaneously can't count on the liver and the skeletal muscle to be behaving in concert, right? So the liver production of glucose is going to be one component and the skeletal muscle or other tissue uptake of glucose is going to be an entirely different component. Just as I'm trying to understand this, As I'm trying to understand this, I'm going to keep asking you these questions because Dave told me that was my job here. Very good. Yeah, thanks, Jane. So I don't actually know enough to be able to comment. So maybe Bruce can? Yeah, but not to. It would take all of your time. So go forward and comment. And in the end, what he's going to show you is all that matters is that there's. To show you is all that matters is that there's a delay. Right. Yeah, I think one message that I hope to convey is that this model was used as a test case, but the geometry is the story and delay is the story, so the model details, at least with respect to the emergence of this chaos, probably don't matter very much. It doesn't mean that we don't care about them, it just means that the phenomenon will be. Right, yeah, yeah, yeah, yeah, oh, yeah, yeah. This is not to say that we don't care about the modeling details, just that the Care about the modeling details, just that the emergence of chaos does not depend on them. Probably. Okay, so what did we do here? So we basically repeated the type of experiments that I showed you for the toy model. We represented a nutritional signal as sort of just a continuous feed part and a kicking part. So think meals and continuous feed. And we wanted to see if the same dynamics And we wanted to see if the same dynamics that happened before. Yeah. We wanted to see if the same dynamics that we saw in the linear shear flow model would happen here. And in two minutes, let me convince you that they do. So why does delay matter? I have to answer this question. So the delay in this model, which is represented by this parameter Td, the delay actually produces oscillations. And this is a very general phenomenon in systems with delayed negative feedback. Systems with delayed negative feedback. The delay induces an oscillation. So the delay here is putting us in a situation where the unforced system where the kicking is off produces a periodic signal that's weakly stable. So that's part of the delay-induced uncertainty recipe. And then when we kick, so we turn the kicks back on, okay, we see a very, I think, striking phenomenon emerge. So if you look, let's So, if you look, let's see, let's look at one graph for time savings. Let's look at this one. All right, so now I'm kicking the system, as I did in the toy model, and what we observe is that a certain curve goes from negative to positive as the time between kicks increases. This curve is plotting the most positive Lyapunov exponent in the system. Positive Lyapunov exponent in the system. And what we see is that that Lyapunov exponent becomes positive as this time t increases. Take-home message, the dynamics that we saw in linear shear flow happen in the Alfridian model also, for what we think are very general reasons that I think will be relevant for many physiological systems. And let me just finish real quickly by saying that. And let me just finish real quick by saying that our collaborator Bhargav Karamchit will speak about another aspect of this. We think that this DIU phenomenon is pathogenic for obesity. And to make that claim real, I'll have Bargov explain why we think that's true. And then you guys can tell me if we're crazy. Awesome. Thanks. Thank you very much. Thanks, guys. 