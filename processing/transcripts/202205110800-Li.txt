So, this is the afterline of my talk. So, first, it's not the inwards problem, but the forward problem. So, first, I will introduce the problem setting and the mean result. And then I will introduce the methodology how to solve this problem. So, mainly, it will consist of two parts, the model, multi-scale model reduction in the spatial domain. Reduction in the spatial domain and the model order reduction in the temporal domain. So then I will give more details on the theoretical investigation on this problem. And it will follow with several numerical tests. And then I will conclude this talk with the conclusion and several future work. So the problem I will consider. The problem I will consider is the following. So it is the time fractional diffusion problem with a soft term Ew to infinity. So with a certain smoothness in the time domain. And the initial data will belong to L2. So we have seen a lot of this type of time fractional derivative, so I will not give the details of that. Details of that. So here, the diffusion coefficient kappa will have multiple inseparable scales, which is important for practical applications. So for example, in the underground water modeling, we have to use such type of diffusion coefficient. So sometimes we need to model it use the log normal Gaussian field. So therefore, So, therefore, it has multiple scales. And if we discrete it directly, it will result in a very large linear system. So, the second part is this fractional derivative, which will describe anomalous diffusion process and it will bring the non-local non-local. Bring ye non-locality for this problem, and therefore it is quite challenging. So here, in this work, I'm also interested in the case when the final time capital T to be very large. So in this case, for that theoretical result, it tends to infinity. So because at last I can show that this algorithm will be independent of the final time. Be independent of the final time capital T. So, here is the main challenge for this problem. So, at last, I want to derive the point-wise in time and the error in L2 norm. So, the main challenge is the following. So, first comes from the multiple scales in the diffusion coefficient. So, therefore, So therefore, we have to resolve the micro scale feature in this coefficient in order to arrive at a reasonable solution. So therefore, at last we have to solve a very large linear system. So the second kind of difficulty is the non-locality of the fractional derivative. So especially in the case when the final time is when the final time capital T is very large. So because of this fractional derivative, so we have to have a huge numerical computation and also the storage. So therefore, I would like to develop a numerical algorithm that will be dependent on the final time. So the goal of this talk. So the goal of this talk is that so first I will derive a new model reduction in the temporal domain by means of the summation of exponentials and the number of the terms in this summation will be independent of the final time capital T. So therefore, we can reduce the cost in the numerical computation and also in the storage. Then I will analyze the stability of this time-steping scheme rigorously. And so because if we use the sequential time stepping algorithm, then it will also be very slow. So at last, I will obtain a stable iterative scheme based on the parallel algorithm. Parallel algorithm in order to achieve a fast convergence rate. So, this is the main result of my work. So, here there is the parameter sigma, so which can be between 0 and 1. And here, this un will be the solution at time tn and u kn. So, k is the iteration number. So, because at last I will have a Last time we will have an iterative solver. So, therefore, there is the iteration number. And this n will be that time at Tn. So, the error will be bounded by the summation of two parts. So, the first part will be the error coming from the new summation of exponential approximation. And the second part will coming from the Part will come from the iterative solver. So the iterative solver will account for the last term. So here there is two time stepping size, either this to F, which is the fine scale time stepping size, and it is very small in order to get a very accurate fine scale solution. So the cost time stepping size is So the cost time stepping size is this cost C. So at last, our iterative server will based on this cost time stepping size in order to have a faster convergence rate. So another parameter is this L. So we call this the level parameter L. So this comes from the spatial discretization. So in order to derive the model order reduction in the spatial The model order reduction in the spatial domain. So we use this level parameter L to characterize the number of the basis functions in the spatial domain. And this term eta, this capital H is the match size in the spatial domain. And here usually we have two, we have also two. So capital H will be the cost scale match size and small H will be the size and the small h will be the five scale mesh size in the in the spatial domain. So at last we want to achieve the convergence rate of order capital H, even though the cost scale match size capital H cannot resolve the phi scale in the diffusion coefficient kappa. So this notation, it will depends on capital H and L. On capital H and L. So this comes from the spatial approximation. So it will be quite complicated. So it will be the summation of capital H multiplied by this cafeteria in the L2 norm to the one-half plus two to the negative L over two and also another term. So this gives the approximation property in the spatial domain for the multi-scale basis functions which are. functions which will which i will describe describe later okay so therefore in order to have the convergence rate of order or capital h plus tau f to the alpha or one minus alpha then we have to make sure that the iteration number k should be order of logarithm or tau f over logarithm or tau c so in the numerical task we can show So in the numerical test, we can show that k equals to 3 should be sufficient. Okay. So next I will present the two part for my result. So first technology is this multi-scale model reduction and the second part is the model reduction in the temporal domain. So in the spatial domain, So in the spatial domain, so the strategy is that we want to solve the problem on the cost scale, which will not resolve the micro scale feature in this problem, which is represented by the diffusion coefficient kappa in this case. But still, we want to obtain certain accuracy. So of course, there is no free launch. So the price we pay is to obtain a number of costs. Cost-scale basis functions, which will have fine-scale approximation properties. So, in order to obtain this, we have to solve a number of local problems. And the approximation property of those local multi-scale basis functions will determine the accuracy of this solvent. So, this is the framework for this multi-scale model. Worker for this multi-scale model reduction. So, first, given the five model with the micro scale feature. So, we want to replace it with the cost model or reduced model. So, from this model, you cannot see the micro scale feature. So, there is only the macro scale parameters. Then, coupled with the inputs, so for this case, it will be the first term, the initial data. The initial data. So, we want to have the output, which is a good approximation to the output from the five-scale model. So, here is a briefly state of art on multi-scale model reduction techniques. And here I will not give the details. So, the main idea is that to discretize the spatial domain with a very coarse grid. With a very cost grid, and then couple this cost grid with problem-dependent local multi-scale business functions, which can be sold by local solvers. And usually, they can be sold in parallel. And at last, we would like to have the accuracy depends on the cost grid size. So here is the general framework for this method. So first we will generate the cost grid. Here we cannot see the micro scale feature. So in this example, it is the interface between the red region and the blue region. So the red region, we will make the diffusion coefficient take value of 10 to the 6. And in the blue region, The six and in the blue region, it will take the value of one. So then we will for each of the cost cost grid i, we will assemble this cost neighborhood. And in this cost neighborhood, we will propose proper local problems in order to get all the features of the solution in this cost neighborhood. So, this will be done in the second in step two. So, actually, there are many. So actually there are many methods to derive the local multi-scale basis functions. So this one will be based on the Wi-V list, which means that I will use the WiiList as the boundary condition for these local problems. So I will give more details in the next few slides. So after getting those local multi-scale based Getting those local multi-scale basis functions, we can either use the partitional unity functions in order to derive all the global multi-scale basis functions, or we can just use those local multi-scale basis functions and that couple with that non-conforming Glock method to solve the problem. So, this is the idea. So, here is one example of the multi-scale business. One example of the multi-scale business function. So, in this example, the green line is the cost grid and the dark region is the cost neighborhood. So, in this cost neighborhood, we want to get the mid-feature of the solution, even though we do not solve the global problem, but only this local problem. So, therefore, we should propose a proper boundary condition and also the source term. And also the source term. So, if we use the traditional numerical method, then we would like to use, for example, the polynomial, piecewise polynomial basis functions. Then we cannot capture the solution in this region. So if you look carefully of this multi-scale basis function, you will see that it is not polynomial because it will take a piecewise constant in some region. So, but this will reflect. So, but this will reflect the property of the solution. And this is the reason why the multi-scale business function will work very well. So because I propose to use Wivolace to develop the multi-scale method, so here are two examples of the Wivoles if you are not familiar with that. So the first is one-dimensional high-wivelists. is a one-dimensional high uvolus up to level L equal to two. So you will see that the high uvulis is piecewise constant and it is discontinuous. So when the level parameter L become larger, then the support of this wheel is also getting smaller and it become more less important. So therefore, if we use So therefore, if we use the vivolist, there is the natural ordering of the importance of the basis function. So this is the motivation for the to use this vivid in the multi-scale method. So the central one is this one-dimensional hierarchical basis function, also up to level L equal to two. So this one is like the head function and also as the level parameter L getting rather. Getting larger, so the support of this base function also getting smaller, and it is getting less important. So the window-based edge multi-scale business function that I propose in this work is not so I propose to use the local multi-scale business function computed from this local solar. So here I will use the Here I will use the zero sum, but I will use the wibulist as the boundary data in order to get those local multi-scale business functions. And there is one actual local multi-scale business function, which is defined in step three. So this comes from the decomposition of the exact solution in this cost neighborhood omega i. So initially, at uh so initially that should be three parts but one part is uh we can drop it because the uh it will it will have the accuracy or order capital H so which is the accuracy we want so we can drop it so therefore we only have those two parts so uh here is the related work on edge multi-scale basis functions so actually there are several groups on Actually, there are several groups working on this type of method. So, one is this component mode successes techniques. So, we also developing the decomposition of the sobel space using the spectral problems. But here, we want to avoid the local spectral problem because usually the local spectral problem will be very expensive. So the second one is to have the enrichment of the traditional multi-scale basis function by La Siegender polynomials. So which we are also proposed as boundary condition. But for the La Sienda polynomials, we can only have the approximation property when the solution is very smooth. But usually for the solution from the multi-scale problems, due to Problems due to the existence of that type of high contrast multi-scale diffusion coefficient, the solution actually has very low regularity. So the third one is by Ho and Liu. So we propose to use the adaptive edge multi-scale business functions also in order. basis functions also in order to improve the traditional matistical basis functions so for this project for for their work they also need to use to use the spectral solver in order to identify the important mode so and so the last one is Baba Bushka and his collaborators so we propose to So they propose to use the edge, they propose to use a certain number of basis functions on the also on the edges, but their focus is on the application instead of the theoretical result. Okay, so now we come to the second part, which is on the temporal on the time stepping scheme. On the time stepping scheme. So, first, the first one is this L1 scheme, which is, I'm sure everyone knows it very well. But this L1 scheme usually is very expensive, especially when the final time capital T is very large. So, coupled with this L1 scheme and the Galakan method, we can And the Glaucan method, we can get the first solver, which is this Glaucan L1 scheme. So for this scheme, we should solve it using the five-scale match size in the spatial domain. So here, and also here we only solve it using the full five-scale basis functions instead of the multi-scale basis functions. Of the multi-scale basis functions. So here it is denoted as v small h. So the solution from this scheme will serve as the reference solution. So as we know that this approach will be very expensive and also it will require lots of storage. So first we want to I want to improve this LY scheme by the model reduction in the history part. So I will divide the time fraction time the time fractional part as a summation of two parts which is the look so one is the local part and the other Local part and the other is the history part. So we will do nothing on the local part. But for the history part, we will approximate by summation of exponentials in order to speed up the algorithm. So the main result in this part is the following lemma. So we can show that we can approximate the history part. The history part very well. So, the history part means that it will be at least one time step away from the current time t. So, for example, if we want to calculate the time fractional of this way at time tn plus one, then the local part will be from time tn to tn plus one, and the history part will be. Plus one and the history part will be from time zero to tn so for the history part it can be speed up by this by this following lemma and this result is like the approximation of the one over t plus one plus alpha which is the kernel which is almost the kernel in the history part so we can show that if we can show that if t is larger than this parameter alpha over e then we can get this this estimate and this estimate will have the convergence exponential convergence rate which is independent of the final time capital T so this lemma is based on the seek approximation sick quadrature to analytic functions which will take the integral form will take the integral form from negative infinity to positive infinity. So in order to derive this lemma, we have to reformulate this 1 over t to the 1 plus alpha as the improper integral of analytic function. And then we will use the C quadrature to approximate this integral. So, based on this result, we can show that if t is away from zero, so if t from top f to capital T, recall that top F is the fast scale time stepping size, then we can, if we want to make sure that the approximation of this summation part is good enough, is epsilon. Is absolute error from this one over t to the one plus alpha. Then we only need the summation term n exp to be the logarithm of tau f to the one plus alpha multiply this epsilon and the square. And here you will see that the number of the terms in this summation will be independent of this capital T. So in the previous So in the previous work, we also derived this type of summation or exponential to approximate this type of functions, but usually they will depend on the final time capital T. So here, using the summation of exponential approximation to the history part, we can arrive at a new local time integrator. integrate written in this form and at last we will show that the history part in order to calculate this time fractional durative part we only need to calculate ne xp term for each time step for each time stepping so therefore there will be a very huge reduce of the computational cost and also the Computational cost and also the storage. So, at last, we can get this following recurrence relation to calculate the history part, to update the history part when t comes from tn to tn plus one. So, we only need to update those parameters. So, because So because the number of those submissions will be much smaller than MF. So MF will be the total number of the time steps for the fine scale time stepping. So therefore, we will reduce the storage and also the computation. And especially it is independent of the final time capital T. So therefore, there will. Capital T. So, therefore, there will be a huge reduction in the computational cost and the storage. Okay. So, the next stage is that we want to make sure that it will also work when the final time t is very large. So, a very long time is very important in many applications. So, for example, in So, for example, in the financial market, we also want to price the options, either the European options or American options in a very long time. So, which means that the expiring date is very long. So, in 10 years or in 20 years. So, therefore, the computation for a long time is very important. So, because currently this multi-scale SOE scheme is only So, therefore, it doesn't work very well for very long-time simulation. So, that's the reason. So, we've proposed this parallel in time scheme in order to handle long-time simulation. So, the parallel computing is very important, and also it. Important and also it has a very long history. So it was started in the almost 60s. And so there are many type of parallel in time computing method. So one of the most important one is this parallel algorithm. And it becomes very popular after the publication of the work by Leon Smeady and Turini. Medi and Karenik in 2001. So the idea of this algorithm is that we will have the cost time stepping and also the fine time stepping. But for the cost time stepping, we will take the sequential computation. And for the five-time step in, we will take the parallel computing. Parallel computing computation in order to correct the solution from the cost time stepping computation. So, this is the basic idea. So, first we will parallel compute the solution using the cost time stepping and we will have a rough solution of the solution. Then in each of the cost region, we will have the we will have the verified time stepping computation which can be conducted in parallel then we can get a correction of the solution and at last the most most important the most important is that the iteration number should be very few so otherwise it will be there is no advantage in this algorithm and usually we can show that the convergence rate of The convergence rate of this parallel algorithm will be exponential convergence with respect to the iteration number. So, this is again for this type of computation. Okay, so now we can incorporate all those parts into the final algorithm. So, therefore, we will use the multi-scale. The multi-scale space as the multi-scale space as the to discrete to discretize the spatial domain. And then we will use the summation of exponentials in order to incorporate the powerial algorithm both in the cost. Cost stepping computation and also in the five-stepping calculation. So, those four part is in the step six. And here we will treat the history part as the parameter. So, which is denoted as this BPN. So, this part will not have the parallel correction. Carrier correction. Okay, so this is the main feature of this algorithm. So we will treat the history information as the parameters. So therefore, it will also reduce the computational cost. And we can show in the simulation that this is very important. So if we update the history. So if we update the history part, actually it will converge much slowly. Okay, so next we'll show two results. So the first is the convergence rate for the multi-scale SOE scheme, which means that we will use the multi-scale space for the spatial other spatial. The spatial domain on the spatial domain, and also we use the summation of exponentials for the time discretization scheme. Then, so we can show that this scheme will be stable and we can get the following convergence rate. So, in contrast to the results we have seen in the LY scheme, so usually the convergence rate. So, usually the convergence rate will be first order. So, here our convergence rate is only one minus alpha order. So, alpha is the exponent, is the parameter from the fractional derivative. So, for this result, this result is the reason we got this result because we have to derive the stability of this scheme. And so, this is the type of stability result we derive for. Result we derive for this algorithm. And also for the numerical test, we can show that it also works better when alpha is smaller. So therefore, it is consistent with this result. So there is one important part for this result is that the accuracy of the summation of the exponentials is not. summation of the exponentials is not is not important. So in theory, we give the accuracy which is characterized by this parameter epsilon. We can show that it doesn't mean that if the epsilon is getting much getting smaller, then the accuracy of this scheme will be improved. So this is not the case. So we have the requirement of this absolute Requirement of this absolute, and actually, it is very very loose. So, the next is the final result I have shown in the first part. This is the error estimate for the WMP algorithm. So, here we will incorporate the period algorithm and the And the error from the parallel algorithm is the last part. So at last, I will give several numerical tests. So first, I want to test the performance of the SOE scheme. So for different of those parameter alpha. So recall that alpha can take values from zero. That alpha can take values from 0 to 1. So, the first one is for alpha equals to 0.1, and we can show that if epsilon is very big, relatively big, for example, it is larger than 15. So actually we can get almost the same accuracy as epsilon very small. So, for example, when epsilon is 6.8 multiply 10 to the negative. Multiply 10 to the negative 5, we can get almost the same accuracy. And from the theoretical results, the requirement is epsilon should be equals to 0.5. So this requirement of this epsilon star comes from the stability of this scheme. So this is the case for alpha equals 0.9, and we can get. And we can get almost the same result. So, for this case, the optimal parameter also equals 2.5. So, therefore, the message is that we don't need to take epsilon too small. And when epsilon is too small, it also will be very expensive because then it means that the number of the terms in the SUOE will be very large. OE will be very large. So, therefore, it will increase the storage and also the computation. So, the next I test the performance of the WEMP algorithm, which is the final algorithm. So, although also for the different cases of alpha. So, the first one is for alpha equals to 0.1. So, for this one, so. So for this one, so the upper plot is in H1 error, and the lower plot is on L2 error. Recall that our final result is only in L2 error. So we only need to look at the plus, which is quite small. So here the error is married in percentage. So this is the relative error in L to norm. So we can see that the So we can see that the relative error in L2 norm is always almost a five percentage. So even though if the epsilon is very large, we can get almost the same performance. And here we can see that our iterative algorithm can work very fast. So it will converge in three iterations. So the next So the next is for alpha equals 2.5. So we can get similar results. And here you will see that the iteration number is slightly larger. So because if we take iteration only equal to one, there is slightly larger error compared to alpha equals to 0.1. So for this one, we also see that the requirement of the parameter epsilon is Parameter epsilon is very, very small. So when alpha equals almost 20, we can also get almost the same accuracy. So the next is for alpha equals to 0.9. And we will see that if for the iteration number equals to 1, then the error is very big. So if iteration number equals to Iteration number equal to one, so the relative error in L2 norm will exceed 50 percentage. So, so therefore, it means that when alpha is getting large, then we need to take more iterations. But for this one, we can also see that after three iterations, it will converge to the Glaucan SOE scheme, which means that there is no power algorithm. Power algorithm. So, so the last result is for a slightly long time simulation for t equals to 10. So, for this one, we can get similar results. And here, so the plot is for different type of alpha. So, the first is alpha equals 2.1. So, we can make sure that after three iterations, we also get the convergence of. The convergence of this algorithm to the Glaucan SOE scheme. So, for the second one, we can see the same. So, when alpha equals 2.5, then also three iterations should be enough. And for alpha equals 2.9, we can get the same result. Okay, so this is the conclusion of my talk. So, mainly I presented mainly I presented one algorithm called SWEMP. So there are three components. So the first is that I use a very different spatial discretization based on the multi-scale model reduction. So the second is that I use the summation of exponentials in order to approximate the history part. And the third part is that in order to have To have a faster algorithm for the long-time simulation, so I use the power algorithm. So, and for those algorithms, I can show the stability of the algorithm and also the convergence analysis. And also, I presented several remarkable tests to verify those results. So, currently, I'm working on applying this algorithm to Pra-American. Apply this algorithm to American options with long time and also jump diffurance. Okay, that's all for my talk.