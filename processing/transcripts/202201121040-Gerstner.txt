I'm going to put that info here in the chat. And I suggest that we loosely organize these breakout rooms around a couple different topics that seem relevant to the talks this morning. The first breakout room I suggest on the topic of learning, probably most pertinent to Christina and Blake's talks. And the second one, I suggest being on the topic of dynamics, most pertinent to probably to Wolfram and Tatiana's talks. Although, again, those are just suggestions and feel free to sort of move around. Feel free to sort of move around and to have that discussion grow organically, however, best suits you. And then with that, then I'll introduce our next speaker is Wolfram Girshner from EPFL, who will speak about dynamics of memory retrieval in the hippocampus. Go ahead, Wolfram. Thanks very much for the kind introduction. Let me try to share the screen. The screen and go full screen. Okay, does it come? Looks good. Uh, yeah, it looks great to me. Okay, thanks for the kind introduction and thanks to all organizers for inviting me. It's really a pleasure to be here virtually, and I enjoyed the talks so far. So, when I submitted my title, I thought I should say it's still an unpublished work, but there Still unpublished work, but then it came out in the last day of December. So, if you want, you can look it up now in PLOS Computational Biology. So, what is it all about? Memory, concept of memories. We all have concepts in our mind. So, if for a second you close your eye and think about an apple, okay? Maybe it looks like this. If you open your eye again, or like this, or like this, even. Like this, even. So, Apple is not one visual input. Apple is a concept, and it could encompass as well the taste, the smell, the texture. And so, this is one possible concept. A fruit. Now, I live in Switzerland and here Roger Federer is a concept. Roger Federer is a tennis man, but of course, he's also well known. He makes publicity for all sorts of things. Sorts of things, and so Roger Federer is something that we associate many things with, and then there are also places that are concepts. Famous one is, for example, Sydney Opera House, this is, this view is it, this view is quite different, it's also Sydney Opera House, and a sketch like this as well. So there are concepts, and we have these concepts in our memories, in our brains. And Rodrigo Viroga published this. Published this famous paper together with Edsard Fried and Christoph Koch. It's about sort of these Jennifer Aniston neurons. Here's the Sydney opera neuron. Whenever you show a picture of Sydney Opera, it goes up this neuron. And it's not picture-driven. It's really concept-driven. Because if you just show the name Sydney Opera on the screen, the same neuron goes on. But if you show an image of the Pisa Tower, then this neuron would not. Then this neuron would not respond. So it's activity of neurons in the brain, in the hippocampal area, and it's really memory of concepts. Now it's always several neurons that respond for the same concept, and it's also one neuron that would respond to several concepts. So it's really some kind of distributed code. And when you see one of these keys, like Sydney Opera, then you would retrieve it. Then he would retrieve it. So this is data from humans in the hippocampus formation in hippocambus or nearby. This is recordings that are done in preparation of a surgical intervention. And often these problems happen in these memory-related regions down here. And that's why there's data from human patients available. So we have concepts, distributed coding. Concepts, distributed coding. So, this looks to me like associative memory, attractodynamics, which was, of course, a very hot topic starting in the 80s with John Hopfield. And it pushed really a wave of physicists into the field of computational neuroscience, of neural networks. And the basic idea is exactly this: so, concept of an apple: first time you see an apple, some neurons go on, some Neurons per one, some texture neurons, some color neurons, some taste neurons. And now, if you have happy learning, then these neurons that are active together would have connections that are strengthened. And that's how the item is memorized, how the concept is memorized. And later, it's sufficient to just show a partial image, and then you can sort of complete the full image, you can associate the taste, you can associate the The taste, you can associate the smell, sort of this kind of pattern completion. All this is well known, concept is recalled. Now, when we do this in physics or in computational neuroscience, we say, okay, we have this network. At the beginning, it's naive. Then, this cell assembly for a certain concept is formed, and we can reorder neurons so that they are all down here. So that they are all down here, sort of these 400 cells make the concept. This is stronger connectivity between those cells. So that would form one of these attractors. And you can check these attractors by first running the network without a stimulus, then it gives a short stimulus here. And you see that these 400 neurons down here get active. And then we have the mathematical tools to try. The mathematical tools to transform these spiking activity into fine rate models, and then we have two stable fixed points. And this one here corresponds to the higher activity, and the other one corresponds to the background states. So that's the standard theory. Now, what intrigued me when I heard the talk of Kian. Of Firoga is that it's many concepts in the same brain area, and there are also associations between concepts. So we have the concept of the Apple, we have the concept of Roger, we have the concept of Sydney, but now it just happens, say, that when I visited Sydney, Roger Federoff is there too. And now in the future, when I see the Sydney Opera House, if I want. House, if I want, it's easier for me to think of Roger Federer, but it's not mandatory. I can also still think, well, Sydney Opera House, that's this nice building in Australia. So associations like this one are, and this is more recent work of Piroga, is associations like this one are encoded in neurons in hippocampus. And here's the basic idea. And here's the basic idea. Very conceptual, and that was put forward by Rodrigo Viroga and others. It's kind of straightforward. So there's one concept and it's sort of the red concept here. And yellow nunions means these are active. So whenever I see Roger Federer, it's these neurons that get active. If I see the Sydney Operaus, it's another set of neurons get active. And if I see the two together, And if I see the two together, then both get active. So yellow means active neurons. And now the idea of these building associations is that the more shared neurons I have, the easier it is to build the associations. So I have to introduce two concepts. One is gamma, the fraction of neurons participating in a concept. So let's look here, concept one. Say I have 100,000 neurons total, 200,000. 100,000 neurons total, 200 neurons are active in that concept. So, gamma, the relative fraction of active neurons, is 0.2%. And that's true for the first concept and also for the second concept. And now, the other thing is the fraction of shared neurons. So, how many neurons would be active in both of these concepts? And, for example, two out of the 200. example two out of 200 neurons might be might be one possibility c is this fraction of shared neurons so in the following all the analysis and the simulations always have gamma and c the fraction of shared neurons so now the idea the simple idea is that if i have more shared neurons then associations become stronger now a bit of theory Now, a bit of theory. So we have these different concepts. For example, concept one would get the index M1. And we measure the similarity by comparing the current activity, these rates, with the concepts. And this C variable just means a neuron participates in the concept if C is one, otherwise, if C is zero, it does not participate. And so these fraction of neurons active. Fraction of neurons active in a certain concept is exactly gamma is this mean. Okay. And you can measure, or Rodrigo Veroga estimates this from his experiment later and says it's about 0.2% or 0.5%. It's below 1%. So it's a very small fraction. And now this similarity, so physicists used to call it overlap. So, physicists used to call it overlap. I will call it similarity. This similarity is just normalized such that if I recall this concept one, then the similarity has a value of one and the similarity with other concepts like the green one here would be very small. So that's the normalization. And now we have a standard fine-rate network. Phi is this transfer function. Everything is positive. So it's us. So it's usually rates, and this is sort of the standard input weight, the input weights to neuron eye. And we use these Hopfield type connectivity adapted for this low activity, low fraction of activity in each concept, which is essentially the Zodigs-Fegelmann rule from 1988. And that means with this, I can rewrite the total input. The total input in terms of just these similarities. And this is the sum over all the concepts that are stored in the network. So you can transform this neuronal dynamics into a dynamics of these concepts of these similarity measures. So we get these low-dimensional dynamics that was one of the topics of day one, if I remember correctly. Correctly. And now, since we have two concepts at the same time, we now rewrite the dynamics in terms of these two concepts that are currently of interest. Say we have stored 500 concepts, but we are currently interested in those two. And first, we give a stimulus for the first concept, and then the first concept goes up. And then we give a stimulus for the second concept. And it's a very weak stimulus here, so it's an epsilon stimulus given here, and therefore it does not respond. But now I could give a stronger stimulus and at some point it will respond. So what we can do is we can analyze the dynamics now with these two variables, the two similarities, similarity with concept one, similarity with concept two. With concept two. And then we get a fixed point structure of the dynamics that falls out of the mathematical analysis. And this is the resting state. And then if I give a stimulus that puts me across this green saddle point, if I give the stimulus of concept two, I go towards this fixed point. If instead I give the stimulus of concept one, I go towards this fixed point. If I give two stimulus, If I give two stimuli, I can end up here. Okay, so that's the basic structure. And now for the moment, this fraction of shared neurons, C, is just the random one, like you would have in the ZORTIX model or in the Hopfield model. Just if you store random patterns of some mean activity, mean participation ratio, gamma, then ratio gamma, then there will automatically, just by the random process of generating patterns, some neurons will be shared. But now the idea is that I can increase this fraction of shared neurons C above gamma. So another question is, is it really good to have more shared neurons? Okay, so we said with sort of the numbers I had before, we would spontaneously have two shared neurons. Now let's say we have four shared neurons. Does it then become more likely that I can go over to the other side? And yes, this is indeed the case. Now we have increased the fraction of shared neurons. I have more neurons participating in both concepts. And now if I give the first still, I give the first stimulus, it automatically associates already the Sydney Opera House. Okay, now, but in a way, this is problematic, because that means whenever I think of Roger Federer, I must also recall Sydney Opera. And that's not really what I want. So if I do it like this, then Then, with this random fraction of jared neurons, I always have this joint fixed point that is now the only shared sort of the only non-trivial fixed point. I have only a resting state or joint recall. And that means that the two concepts are merged. I necessarily always think of it together. I can no longer think of one without the other. And that happens for already a sort of a fraction. Already a sort of a fraction of shared neurons that's not so high. Okay. Now, if I reduce this fraction of shared neurons, then I get into this regime, which I think of as the interesting regime. I can recall one of the concepts, say just Sydney Opera House or just Roger Federer. And then if I give a weak stimulus, and now it's so once I'm here, it's now a very weak stimulus that's sufficient. A very weak stimulus that's sufficient to pull me over to the joint. So now, say this is Sydney Opera House, and I give a little bit of just a very weak keyword, Roger, and then I will go over here. So these fraction of shared neurons that's just a little bit above chance makes it possible to keep the original fixed point, but also generate the joint. Generate the joint fixed point at the right place. Now let's go and look it at a little bit further and actually go back to C equal gamma. And it's also possible to have this single recall. And I will come to this on the next slide. On this slide, I would just like to mention I would just like to mention that even though I show here the analysis where I give one stimulus and then the other in a group of two, a pair of two correlated concepts. In fact, we have stored here many, many concepts. So we have in a network of 10,000 neurons, we have stored 2,000 concepts, which means that the memory load in the sense of capacity of Hotfield networks. Capacity of Hopfield networks is 0.2. And we can even go up to a memory load of one, and it still works. And that's, of course, the result of Michel Zodek's that for these low activity networks where the fraction of participating neurons is low, then you can go beyond this alpha of 10%. That's more standard for the balanced networks. Okay. Now, moreover, even though I only talk about P equals only talk about p equals i i talk about groups of two concepts so pairs like roger federer and sydney or some other person and evil tower in paris but we can also have groups of three correlated concepts and four correlated concepts and this value c max of what i can store is always a sort of the value c max of the fraction of Of the fraction of shared neurons that allows me to have single concept retrieval and joint retrieval is also always in the range of 0.15. So this is a good regime. So you shouldn't have too many shared neurons and then it works nicely. So we can do these capacity calculations even for a high load of concepts loaded into. Concepts loaded into this memory network. Now let's look at alpha equals 0.2, and I mentioned it in passing already. So even for this random fraction, I have this joint fixed point. So even before I have ever seen Roger Federer in front of the Sydney Opera House. Front of the Sydney Opera House, it already exists as a joint concept. And that also doesn't make sense. Okay, now, why is that? Well, in these low activity networks, basically different concepts have very little shared fraction of neurons, so they interact very weakly. Act very weakly. So it's absolutely possible to have three or four concepts running in parallel. Now, one way to get rid of this is to say: actually, it's nice to have explicit inhibition. So the total activity of these neurons goes over to these inhibitory neurons that feed something back. And then basically that means the total number of active neurons is controlled in a soft manner, you know, sort of just by the inhibition, by the soft inhibition. Just by the inhibition, by this soft inhibition. So it's now more difficult to recall two concepts simultaneously. And so now, with this random fraction of shared neurons, this fixed point here has disappeared. And I need 5% or 10% of fraction of shared neurons to have both the single fixed points and these joint fixed points. And if I go up to 50%. And if I go up to 50%, then the single fixed points have disappeared, and it's again bad because now it's merged. Okay, so inhibition helps to sort of enforce the stability of these isolated concepts, one concept at a time. And then these extra association neurons, the fraction of shared neurons of 5%, a bit more than random, helps you to create these fixed points. Create these fixed points where you can recall both concepts at the same time. So, this is now, in my interpretation, this is now the good regime. So, I can, at the start, I can learn different concepts. They don't interact. But if I want to create associations, I can, and I don't need a lot of shared news. Just a small fraction is sufficient. Sorry, I've urgently changed something with my battery. I don't understand. I think I'm back. I use the socket to get better light. Now we have the lighted light conditions no longer as good. Okay, so back to the talk. Association state exists in this case only if the fraction of shared neurons is above minor. So for me, that's a relevant regime. So for this section, indeed, shared neurons support associations, global inhibition. Global inhibition controls the overall activity. With inhibition, I can sort of manipulate such that the association state exists only if there is this extra fraction of shared neurons. Now, so far, the recall of a concept is a fixed point in this dynamical system. Now, let's go to association chains. So, what we do now is to say, Is to say, okay, I have this inhibitory feedback and I make this oscillatory. So I have a pre-factor here for this connection strength that's oscillatory on some slow frequency, say theta or so. Otherwise, it's the same. And moreover, I have for each neuron, I have adaptation with a dynamic threshold. Threshold. Okay, so now, in addition to my fixed points, I have two time scales: I have an external drive, which is periodic, and I have a time scale of adaptation, which of course will do the following. Adaptation, once you stay in a fixed point, at some point these neurons will adapt away. So the pattern will become less stable. And with this setup, we want to implement. We want to implement the following. Let's just do it for two associated concepts. So, for one cycle of the oscillation, t equals one, I have concept one. Then I, via the shared neurons, I switch to concept two. Via the shared neurons, I switch back to concept one and then back to concept two. So I go into a cycle. And what we show here in a simulation that this is indeed possible. That this is indeed possible for several associated concepts. For example, here, four associated concepts, you can go up to eight or ten. Okay, now since this is a theory workshop and we are all sort of theoreticians, let's do a fun thing and say, well, we have this phase planar analysis. And now let's do it for this more complicated situation. So I have my OC. So, I have my oscillatory background, and now it's for two concepts. I want to switch between the two concepts. Now, we do a kind of separation of time scale analysis. We say, okay, there are phases where the inhibition is weak, and then there is phases where the inhibition is strong. Okay, and now for each of these cases, weak inhibition, strong inhibition, I can look at the phase plane. And so, for weak inhibition, I have For weak inhibition, I have these individual fixed points that I discussed before. I don't have this joint fixed point here. And then when I make inhibition stronger, for these values that we have here, so the fixed point that used to be out here is now actually here. It's somewhere here in the middle, but it's a joint fixed point. So I come, say, from this, I start here. I come from this fixed point, I go over here. And now, because of adaptation, And now, because of adaptation, when the emission goes back, I cannot go back here. I now have to go back over there. So, that's basically the dynamics analyzed in these two phases. Oscillation between weak and strong inhibition changes the fixed point structure and adaptation switches you over to the other concept. And now we found numerically and that these kind of so gamma is. That these kind of so gamma is the fraction of neurons participate in a concept. So, this is the regime that people think is valid in hippocampus. And then, with 6% of joint fraction of shared neurons, you get these patterns and maximum 50 or 60% and beyond, the pattern no longer exists. So, this now becomes very stable for this kind of low activity patterns to have these association chains. To have these association chains. So the shared neurons support associations. Now we have oscillatory global inhibition together with adaptation that gives this chain of associations. And the analytical approach shows that for this case, indeed, it's a non-random fraction of shared neurons that is required to make these associations possible. Now for the For the final few minutes, I would like to go back to experimental data and sort of talk about these groups of concepts. So we have now data, and that's more recent data. Emmanuela DeFalco published this with Krioga. So it's one neuron here, data from one neuron. And these are And these are different sports people. I think all of these are basketball players that were active at that time or were famous at that time. And so you see that the same neuron goes active for several basketball players and also for, what is it, a biker. And then it doesn't go on for other famous persons. And then you would have other neurons that go on for Mother Teresa or Paris Hilden and so forth. Okay? Okay, now what they did is to say, okay, it's one neuron, and I've, you see here, these stimulus numbers. This is stimulus 59, 60, 61. And so this neuron sits down here. This neuron says it responds to all these different stimuli, 59, 60, 64, 66. I think this is 66. You exactly see this pattern. And then they say, okay, this is this one patient. One patient, he's obviously interested in sports, but how much are these objectively associated? And then they look at the web and just search for these terms, Kuipe Bryant, Lance Armstrong, and then you see also that there is sort of this association between these subgroups of people. Okay, so there's a correspondence that means so things that appear. So, things that appear often on the web together, which sort of should reflect that they often appear in real life together, lead to the fact that in the end in the brain it's associated. And now here comes something which is we have heard in this workshop several talks mentioning correlations between pairs of neurons, correlations between patterns. And so I have a group of two concepts. Of two concepts. That's not a problem. I know how to define the fraction of shared neurons. That's this. Now, suppose I have a group of three, say, three basketball players. Now I can organize them in this way, that pair is to have the same fraction of shared neurons, and it also creates sort of these extra cells that respond to three concepts. That's what we have on the previous figure, on the external figure. But then there are suddenly. But then there are suddenly many ways of doing this. I could also generate it in this way that I say, well, they share neurons pairwise. And well, I just made this graph by hand. The idea is that pairwise, it's again the same number, the fraction of shared neurons, 4%. But when they share neurons, it's actually the same. And so this is like an indicator neuron for this group of basketball players. Basketball players. And then I can come. So we didn't implement this, but you can do it even in yet other ways. You can say they avoid to have these third type of things. And so we did several ways trying to match experimental data. We had sort of this indicator neuron model, which looks like this. And we have 100,000 neurons. And for the first pattern, it's always 200 neurons. has 200 neurons and they share a certain fraction share a certain fraction neurons if i add the second pattern so this is 392 and then eight are shared with four you start you see that different models different ways of generating these higher order correlations give different numbers and it becomes bigger over here okay why do we do this because there's exponential data that we re-analyze That we reanalyzed. It comes from this paper of Emmanuel De Falga with Rodrigo Veroga, and the data is black. Now we played with different models, sort of a hierarchical model that's often copied from what people use to generate correlations between spike trains in models. And if you use this kind of model, it just doesn't fit. And the indicator neurons doesn't fit either. And then this iterative model fits. And then this iterative model fits. So, what's plotted here? So, they are you find a certain fraction of neurons that respond to two concepts in the set that you study with these humans. Then you find neurons that respond to six concepts. You find neurons that respond to nine concepts. So, it's really this kind of distributed code. Really, this kind of distributed code. Each neuron responds to one or several concepts. The exact numbers depend, of course, on how many stimuli you test. So for the simulation, we tested exactly the same numbers as for the human patients. And then only one of the models we came up with fits. So just to point, so we know a bit more. We talk a lot about pairwise correlations, but we know in fact a bit. But we know, in fact, a bit more from this experimental data. So, overall summary: indeed, shared concept cells enable associations, but too many shared cells is not good because then you would get this merge of concepts. Now, we can use the association, the shared cells, either for static associations or chains. So, when you start off Apple, then you felt. You then start to think of other fruits, of peers, of peaches, and so forth. And it's organizing groups of concepts, that's the experimental data, and the pairwise correlations are not enough. And just as a quite open remark here, it's just in the standard Hofffield network. We do not learn, we do not train. Synapses are fixed here. Everything is pre-trained. But of course, we are also interested in doing this with local learning rules and so forth. Local learning rules and so forth, but that's not here. So these are the people who did it. Chiara Gestalti has been a PhD student in my lab. She's just finished. Emmanuel de Falga and Rodrigo Pieroka were the experimental collaborators. And Tilos Walger is a mathematician, former postdoc, now independent group leader and professor in Berlin. And thanks very much for your attention. For your attention. Awesome. So there's lots of questions kind of floating around in the chat. Ega, do you want to go first? And if your question's already been answered, then we can go to someone else. But go ahead, Ega. I think Sarah touched on this a little bit, but it's a simple question since you were quantifying. You were quantifying participation in a concept by looking at how the activity of neurons increases, and you quantify the similarity scores across neurons. And I was wondering if you see empirically any neurons that get suppressed reliably for a given concept. And if so, some neurons would participate, so to say, by reducing their activity from their baseline rate. Yeah, so that's a question for the empirical data. Note for the empirical data. So, of course, the data is very, very noisy. And so, what they do is to say they look at that sort of baseline activity, they look at the standard deviation of baseline activity, and the baseline activity is low rate. So, it's difficult to go negative. And so, if you ask for two sigma, two standard deviations, they only detect positive deviations. That basically it's a two sigma criteria, if I remember correctly. Two sigma criteria, if I remember correctly, that they use associations happen for increased activity, generally speaking, because the baseline rate is too low. Yes, baseline rate is noisy and low, so what they see is only increase of activity, if I remember correctly. All right, thank you. Yeah, welcome. Sarah, go ahead, please. Hi. Welcome, I was trying to relate this, the early part of your talk. The early part of your talk to results that came a long time ago out of the Israeli group. I think it was actually Hanook, good friend with one of his students, who studied the case of Hebbian learning and Hoffield networks for patterns that were not orthogonal but actually correlated. And they did the capacity studies theoretically, and they found that as you not fully correlate, not correlated in the Truly correlate, not correlated in the sense of sharing neurons, but just correlated, you know, just correlated in the fluctuations relative to the mean. And they found, I think, the effects that you were discussing, that as you increase the overlap essentially between these patterns, you increase the capacity of the network, but you decrease the retrieval ability because the correct. Ability because the corresponding basis of attraction became smaller. So, I was trying to understand what is it in the first part of your talk, what is it that you are doing that goes beyond that very early study, which really comes from the late 80s or early 90s? So, potentially we missed this one study that's very well possible. So, just two remarks. Just two remarks of caution. Here, since we are interested a bit more in the link in the experiment later, we really have rate neurons where the rates are always positive. Whereas a lot of the earlier work had sort of these tension functions where you go negative and positive, and things change a little bit. It's not a one-to-one mapping sometimes. And the second And the second word of caution is: you said correlation beyond sort of the randomly shared things. In the end of 80s, beginning 90s, a lot of people were talking about correlated patterns when they just looked at low activity because they said, okay, sort of the inactive neurons. That also, if you don't correctly. Correctly renormalize, that's also some correlation, yeah. And in a uh, so, for example, in the spin network, um, plus one, minus one, you have the symmetry, high activity, low activity. Suddenly, you have many down spins that are correlated. That's just a word of caution. I don't know this exact paper you are referring to. I would be happy if you could send me a link. Yeah, I'll send you the link. No, there were other papers. No, there were other papers that addressed the question of overall small firing rates, but that's not the paper that I'm talking about. And the fluctuations were always measured with relative to the mean, as you were doing, you know, psi i minus mu times psi j minus mu, both in the heavy and learning rule. So positive and negative fluctuations relative to the mean. So that's in graded neurons. So that I thought was the same scenario that you were studying. You were studying. And also, from the sequences point of view, then one should also remember the sine fire chain work and work by Cantor and Sompolinsky of adding to the Hebian, not invoking modulated inhibition, but adding to the Hebbian rule a component that ordered the memories. So he told you from memory I, from memory mu, you go to memory mu, adding another Hebbian term that fails. Another heavy end term that favored, a term that favors staying at that fixed point, and it turned that favors transition to a well-specified fixed point that allows you to store a sequence in this heavy mechanism. Yeah, thanks for these pointers. That's exactly the kind of things I did with Leo van Hemen and Andreas Hertz during my PhD. So these, so you may. So you mentioned Synfire chains. So if I do, if we implement stuff with spike and neurons, SYN fire chains are easy and it's super fast. Just sequences are way too fast because it's on a millisecond time scale. Way too fast for sort of these kind of cognitive processes we are talking about. And in the early rate-based networks from the 80s, yeah, that was, I think. Yeah, that was, I think, was Kleinfeld and Sompolinsky, and then it was maybe also Gutfreund, and there is also Andreas Hertz and colleagues. Often the time scale was not fixed. You just write down tau and then you see these transitions, right? But here we actually sort of try to fix the time scale and have a transition that happens on a slower time scale. And it just sort of you need adaptation. Of you need adaptation to make this possible. Yeah, that they also consider that case, you know, fast synapses versus slow synapses, and the slow synapses trigger the transitions, and the transitions have a very slow time scale compared to stay, the time of neural cycles, the time of 10 millisecond cycles within which you stay in a fixed point. So you recover that memory and then on a longer time scale, you made a transition. So that was in the counter-something. Cantor, uh, in the counters on Polinsky paper, as I recall, yeah. Just for those who don't know this work, if you look at this formula, it means that on the right-hand side here in the weights, you have mu and on the left-hand side, you have mu plus one. So when you are in pad on mu, you try sort of you force to go over to the other. So we really, we don't want to learn sequences here. We have, we learn study concepts and Static concepts, and then it's association chain between static concepts. It's more related to the sort of what Michel Tzotex did in his group during the last 10 years with Romani and Rickhan Soni. I had the reference over here. Yeah. Thank you. I think other people asked questions. People asked questions. Thank you. No, no, it's fine. So thanks for playing this up. Yeah. So temple structure has a long tradition, and the difficulty is to get something meaningful on a slow time scale. And that somehow matches this, yeah, these it makes a link to experimental data. So the really new thing is that we link to the story of Biograph. Awesome. So we're at the end of the time we have budgeted for Wolfram's presentation. So thanks, Wolfram, for a great talk and for some really wonderful discussion. What we've got next on our schedule is about 40 minutes for continued discussion, including what's ongoing now about temporal sequences and hop field networks. Thanks, Veronica, for posting the breakout rooms. The breakout rooms. So, right, those are the links for the breakout rooms. Loosely speaking, what I had suggested is that breakout room one could be for topics more related to learning, so more related to Christina's and Blake's presentations.