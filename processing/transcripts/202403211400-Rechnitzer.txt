And I talk about something that I've been playing with on and off for a while, which is trying to find better algorithms for sampling random closed curves while keeping the topology of those curves to fix. And this turns out to be a nasty problem. And this has worked together with Nick Beaton and Matthew Clisby, who are both in various places in Melbourne. So I first came to one of these meetings a distressingly long time ago. A distressingly long time ago, 21 years ago, and there were a bunch of talks about putting knots or considering knots in closed curves. It's the first time I've sort of been exposed to this. And as part of this talk, I think I came across possibly some work by Javier showing the knot distribution in phages, in phage DNA, and the fact that there's some distribution of knots that occur somehow naturally in DNA or RNA food. In DNA or RNA for this particular beast. And one of the questions that came up about this is: what should the typical distribution be? And so, what should the distribution of knots in a closed curve look like? And as part of this question of what does a trefoil look like, and of course, when you do this, you have to think about, well, which trephoil, like what do you mean by typical here? And so you just put some probability measure on the space of closed curves in R3, and you use that probability measure. That probability measure in order to study what it means to be difficult. And if you think about this for a little while, you realize that the word just there is covering up a very large multitude of sins. And this is actually not trivial. How hard could it be? And the answer is hard. And so two of the measures that I've worked with for quite a long time, or one of them I've worked with for a very long time, and one of them I'm getting to know and love more recently is the self-hoarding polygon in Z3. This is what I've played with. In Z3, this is what I've played with for a large time. It made a large amount of my PhD thesis, which was also a distressingly long time ago. And what you do is you take a simple closed curve and you embed it into the cubic lattice so that for a given length, all possible confirmations have the same probability. So I'm not a simple closed curve. I'm not a solo-world polygon. I can't remember how long it is. And sort of similar to this. Similar to this, and perhaps mathematically a little bit more tractable in certain circumstances, is the family of equilateral random polygons. And here you take a closed curve in R3 defined by a bunch of unit length edges, and the edge directions are chosen to be uniform on the sphere, and it's conditioned to close. And so here's one here. And one of the things that you should notice about this almost immediately. Perhaps you should notice about this almost immediately: is being on the lattice with this condition of self-avoidance makes these things more bloated, more open, whereas the equilateral angry and polygon is a bit more compact and generally topology, it's a bit more complicated. Okay, and as soon as you get involved in this field, you realize that analytical results are incredibly difficult. It's very hard to prove rigorous results about these things, which is maybe a little bit depressing, but also helps one develop a research program. Research program, and so there's a huge amount of work that has been done by people like Whittington, Sumners, Millet, Chris here, Artine Rensburg, Wallandini, Deguchi, Candarella, Michelai, Grossberg, lots of people who have done a lot of fantastic work in this area and good reviews of stuff. And so, of course, because analytic results are hard, and at some point you actually have to try to answer some question by some method, you resort to randomism. And it's a bit sad that one has to do this, but the analytic That one has to do this, but the analytic stuff is just very, very difficult. And so you resort to random sound. And there's still plenty of interesting stuff to do here. It's not such a consolation, but there's still good things to do. And so if you want to do this, if you want to study, say, what does a random trefoil look like, there's two ways you might do this. And one is that you sample a superset of the topology that you want. So you might sample all closed curves and then sieve out the topologies that you're in. And then sieve out the topologies that you're interested in. The other thing that you might do is you might try to sample only closed curves of that particular topology. So if your algorithm would somehow be ergodic on that space of fixed topologies, it would only see trevoroles as it does whatever it does, as opposed to produce everything and then sleep things out. And to be somewhat blunt, both are problematic. Neither of these is entirely satisfactory. Neither of these is entirely satisfactory for various different reasons. Again, that's not to say that great work hasn't been done trying to do this, but there are problems with both of this, which again helps drive what one might attempt to do in this area. And so if you sample a superset and then sieve things out, one good way to do it for equilateral animal polygons is relatively recent work by Jason Canderello and others, where they found a really beautiful, very beautiful method, a method for producing completely A method for producing completely independent samples of equipment and polygons in time about n to the five halves. The other thing that you might do, which is a bit more closely related to what I'm going to talk about today, is you might try to sample self-avoiding polygons of fixed length by what's called the pivot algorithm. Indeed, when you don't control topology, this is far and away the best way to approach that problem. So if you just want random polygons with no control of topology, the pivot algorithm is easily the best thing. Pivot algorithm is easily the best thing. And this is a very beautiful, very geometric description. So you take yourself or hypothology here, you choose a pair of vertices on the surface of it, like these two here, and then that defines a nice little segment here that you might manipulate, and you pull this thing, and you just take that thing and you apply some lattice symmetry to it so that it preserves the location of the end points. And so, if you think about what this is doing, when you apply a symmetry to it, this has the advantage that it may. Symmetry to it, this has an advantage that it makes a big change to the confirmation. At the same time, it kind of messes with your topology rather severely in that when it makes that huge change, it doesn't obey any sort of strand passage restriction or anything like that. Okay, so it really doesn't respect topology at all, but when it works, when you don't care about topology, it's incredibly efficient. And by some extremely clever work, Clever work, Nathan Clisby, worked out a way to do this for self-modding walks where you can actually produce a statistically independent sample in log n time. And if you think about that for a moment, it means you can produce a new sample in less time, considerably less time, than it takes to write down that sample, which feels quite weird. And again, we'll talk about that a little bit more in a moment. Of course, when you do this, when you sample a superset and then sieve things out, the problem is that you have to. And then sieve things out. The problem is that you have to do the actual sieving, and as I think everyone in this room knows, doing that sieving and testing for topology is really hard and very, very slow. But there's a worse aspect to this that as the length of your polygons grow, we can, I think, blame DeWitt and Stu and Nick Pimager for informing us that a particular topology becomes exponentially rare in the space that you're sampling. So not only does it take longer and longer to do the identifying, at the same time, To do the identifying at the same time, it's harder and harder to just find something in that space. So you're kind of being mucked around two ways or being picked upon by the universe in two different ways. Namely, long polygons mean lots and lots of crossings, which mean even harder to ID, at the same time as long polygon means that the topology that you're looking for is increasingly rare. So this is a problem. So instead, what has really been the workhorse in this area is BFACX. So this is sampling polygons of only a fixed topology. And so this is a fantastic algorithm, which given that it's now 40 odd years since it was developed, and it is still perhaps the go-to method that we use for this particular problem, it really states how good it is. So you take a small configuration and you make little Configuration and you make little small deformations to it. You make some sort of length neutral moves where you sort of shuffle things around, or you make some moves like this where you take an edge and you push it out to make the polygon a little bit longer, or you take a slightly longer polygon and push that back in to make it shorter. But it works by this sequence of little local deformations. And this has been proved by Bucks and Stew a while back for the simple cubic lattice and by myself and Bucks slightly more recently, but incredibly. Slightly more recently, but increasingly and distressingly longer ago, on the body-centered and face-centered tubic lattices. Now, because of the way this works, you start with a small conformation and you make little deformations. If you tune it right, what the thing does is it makes a simple random walk on the length of your polygon. And so, if you think about this for a little while, you realize that if I want to actually sample a long polygon, the time it takes me to reach that size, because it's a simple random walk-in length, It's a simple random walk-in length, is roughly whatever that length is squared. And so it takes an increasingly long time to sample polygons of large length. At the same time, well, the good thing is you don't have to worry about topology. Topology is always conserved. But at the same time, one of the things that I worry about more and more when I think about this algorithm is we don't really know how long it takes to produce or just explore the confirmation space very well. And so one of the things. Space very well. And so one of the things that I worry about now is if we have, think about a configuration of a polygon, which is, say, trefoil, trefoil, figure eight, figure eight, going around a closed curve, how long does it take for that confirmation to mutate into trefoil, figure eight, trefoil, figure eight? So for one of those trefoils and figure eights to pass through each other. And I'd be surprised if anyone has ever run a simulation that's long enough to see that. Now that doesn't necessarily mean it's a problem. Doesn't necessarily mean it's a problem. It depends what we're trying to observe. Different observables won't care about that. But it's something that makes me worried, that we don't really know. That being said, it's 40 years and we're still using this algorithm as the basis of a hell of a lot of work because it really is the best thing we have. Another thing that was tried not so long ago by Salon Ferrari was a sort of topologically constrained pivot where. Constrained pivot where you take your polygon, you pick two points that define an axis along your polygon, and you think about the surface that is mapped out by where the polygon segment is now and where it's going to end up. It maps out some surface, and you ask yourself, well, in the process of deforming this thing down and expanding it out there, does it cross any other edges of your poly? If it does, throw that away because. Throw that away because you might have messed with the topology. You might have induced some sort of strand mass. If it doesn't, great, do it. And so this was used by Zaverari to do some sort of topologically constrained pivots, but it's computationally extremely intensive. They could only make it work for fairly short segment lengths. And so that means they were constrained to only explore fairly short pivots, where this Where this segment that you're manipulating is length five knots. The problem is that's probably okay on moderately sized polygons, but we know by work with Madras and Sogal back in the 80s, that's not going to be sufficient for long polygons. It's just not going to be ergonomic. All right, so what can we do to speed things up? Well, we can be lazy. We can be lazy in a very strategic way, and as part of this good data. And as part of this, good data structures are really key to what you need to do here. And one of the things that sort of, again, after playing with this more and more, and also in my other projects, which involve a lot of database hackery, realizing that the data that you store in the computer should look like what you're going to do to that data, not what you are actually modeling. So we're talking about polygons here. So the usual way that we do this is we stick a polygon in memory as a polygon. We have a doubly linked list. We have a doubly linked list and we store the vertices. And because it's a doubly linked list of vertices, it's easy to traverse around, etc., etc. We store each vertex explicitly. Maybe we have a hash table so we can do intersection checking or something like that. But if we do this, naively, we have an n squared operation to check for intersections when we make some sort of pivot. Like when I make a pivot to the polygon, how do I know if things cross? Well, I kind of check every edge or every vertex with every other vertex. Edge or every vertex with every other vertex. So that's bad. And after we've made a pivot, if it's successful, well, I have to rewrite the length of that segment, which is going to be order n. So I have to make some order n update if it's successful. And, you know, that's track. Don't get wrong, it works, it's good, it's a lot easier than a lot of other things, but it's kind of too much explicit computation. So we change things a little bit. So if we do it a different way, we can make a doubly linked list of angles. List of angles, or quaternions or whatever, that tell us how we go from the current step, which way we turn to make the next step, which way we turn to make the next step. And so we can store a doubly linked list of angles, condition that they sum up to the right thing. And then we can compute the position of vertices by doing some sort of sum. Now, one of the things that's good about this, well, it still takes order n squared time to compute intersections, but if we make a pivot. Sections, but if we make a pivot that's successful, the only thing we have to update is these two versus so the update becomes a lot faster. This is where storing the data in a way that looks like what we're going to do to the data is much more important than the data looking like what we're modeling. Come on, come back. But this is still too much explicit computation. All right. So, one of the things we can do is we want to do some sort of lazy computation where we Some sort of lazy computation where we pre-compute a bunch of stuff and only update it after we actually need to. And so, a good way to do this is just to start lobbing together pairs here. So, I know how the first two steps move, and I'm going to pre-compute some summary of what those first two steps do. Where does it start? Where does it end? What's my new frame of reference for when I make my next term? And I do that again. I do that for all of these localized pairs. Localized pairs. And if you think about it, if I'm trying to work out where I am on the polygon, this allows me to skip. So instead of having to go, this goes to this, goes to this, goes to this, if I want to know where V2 is, I can just do it in one step because I've pre-computed that. And I can do V4 and so forth. And I can do this recursively. I can start globbing together those pairs, pairs of pairs into fours and so forth. And this, if I store this stuff correctly, and I think. Store this stuff correctly and I think about it for a while. This means that I can do things like if I want to find out where vertex 6 is, I can do this in far fewer computations than I would if I was doing it very naively. Because I can skip ahead four steps and then go two steps. And I'm not storing all the vertices explicitly, I'm storing it in this sort of skip list sort of tree, in a slightly sneaky way which I will attribute to Nathan and not me. Okay. Okay. And so this is one of the main ideas behind Clisby's log N method for doing pivots on walks, not closed things. It's also, for those of you who know what Jason Canderello was doing with his very, very fast equipment, polygon problem, actually quite close to what's going on in there. And so you think about what you actually have to store. Well, in a leaf node, you just have to store the fact that you take a step and some sort of bounding sphere that tells you. Sort of bounding sphere that tells you information later on when you're actually going to do topological testing. But an internal node, what it stores is, well, I know what happens on the left and I know what happens on the right. How do I put that together? So the internal nodes tell you what symmetry I apply to get from this walk to that walk. When I've taken my last step here, where do I go to to take the first step of the other step? And then also, well, I know some displacement, I go from here to here. Displacement: I go from here to here, I go from here to there, and then I can put that together. Okay, so it's storing this sort of summary information about what happens under this node. Okay, and so we can use this to compute a position in log n, and you can also see this algebraically by doing silly games with brackets. If I take this sort of huge vector to get to v6, I can start bracketing things together, and then I can bracket things together. And then I can bracket things together a bit more, and I can bracket things together a bit more. And if I pre-compute things correctly, and I'm happy to talk about this later when I'm not quite so time-constrained, you can compute positions of vertices much, much faster. And by storing information about bounding spheres, you can compute intersections much, much faster. All right. Okay, so this sounds familiar because I tried it before. I tried to be clever and keep the tree structure fixed, and when I did that, Structure fixed, and when I did that, it doesn't handle the circleness of the polygon very well, caused some strange bugs, and I couldn't debug it. So I went back several years later after some distractions. I'm happy to talk to anyone who would like to know about attempts to make open source alternatives to GradeScope and things like that. It's a very good way to waste a couple of years of your productive life. But I redid it from scratch. Redid it from scratch using a dynamic tree structure. So, not this fixed, complete balance binary tree that you saw, but where the tree structure is allowed to change. I coded everything again from scratch in Rust just to be completely buzzword compliant. And it's looking promising, but I don't know if it's actually quite there yet. So, one of the ideas is that you need is you need to rearrange the tree, because that allows you to move the polygon around in different ways, that allows you to focus on different bits of the polygon. Allows you to focus on different bits of the potential. So being able to manipulate the tree allows you to pick different vertices for doing things while respecting the circleness of the tree. And so different ways of different embeddings of the tree or different trees will just can represent the same polygon. So you can manipulate the tree without changing the underlying polygon. And so the way that we manipulate the tree is by doing branch rotation. So here's a left and right branch rotation. Left and right branch rotation. These take order one operations to do. You can, again, because of the circleness of the polygon that you're modeling, you can switch the nodes at the root, which is another operation, which is order one. And you can put these things together so that you can shuffle things around. And if you want to pivot this segment here, you can shuffle the tree around so that all of the fixed segments right to the left of the tree and the segments you're going to mess with. Tree and the segments you're going to mess with right on the right of the tree. And then, if you apply a pivot to it, all you need to update, the only thing you need to change is a symmetry here at the root vertex. You don't mess with the rest of the tree. So again, this is this game of the data that you're storing needs to look like what you're going to do to it, not what it's actually modeling. And the thing you're going to do to it is you're going to pivot things. So it should look like a way of doing that as quickly as possible. All right, so if we ignore nods for a couple more slides, So, if we ignore nods for a couple more slides, we'll get to them very soon, I hope. First thing you realize when you get to this point is that equilateral random polygons are much easier to work with than self-organic polygons. And this is me turning my back on most of my research career where I stayed firmly in the lattice. It's nice and safe on the cubic lattice. But for the equal electron and polygons and doing this, the lattice really gets in the way. So, if you're an equal electron or polygon, you can pick any two nodes on the polygon. Any two nodes on the polygon, draw the axis between it, take half of that, pivot it around the axis, and you get a perfectly legitimate polygon. Easy. If you do this on the lattice, it's a pain. Basically, because you have to respect the lattice, the symmetry group that you have is much smaller. And so, if you pick two nodes at random, you're not going to do anything that's going to keep those two nodes fixed. Going to keep those two nodes fixed. It's only going to work applying rotations basically if you happen to pick two nodes that lie on a line parallel to one of your axes, maybe 45 degrees. So it's much, much more restricted where you can do any of these operations. And so to make things work, you need to do what's called an inversion move, so where you pick two vertices which don't work, and then the only thing you can do is essentially read those intervening edges backwards. And that gives you another operation to do. That gives you another operation to do to keep things ergodic on the lattice. But that requires much, much more complicated tree messing around. In particular, you're going to take half of your tree and flip it around and read it backwards. So this requires much, much more delicate mechanics at the level of the tree. So I haven't done that yet because that is going to be a bit of a nightmare. So when you do this with no topology, you end up with an algorithm that allows you to basically Allows you to basically do a pivot in log n moves for logo operations. The data structure update is also order one after that. And if you do some simple testing of these things, you generate an effectively random eclipse random polygon in log n pivots. And it should be compared to the n to the five halves that it takes to generate a sample using Canterellum. Relevant. But you still have this bottleneck of identifying the topology. So now, and I see that I'm running a bit low on time, when we put the topology in, what we do is we think about these edges that we move when we do the pivot. So we start off with an edge here, we do some pivot to it, and it ends up as an edge there. And the way we usually think of this, because it's the pivot algorithm, is we think about some edge being turned out in space and back. Being turned out in space and making some nice circular arc, but we don't have to think of it that way. It just needs to be a deformation. And so it's much easier to think of this edge just being dragged directly across in a straight line. So when it does that, it maps out some quadrilateral, which is almost certainly not planar. You can triangulate that in two fairly obvious ways. And so if there's an edge that is intersected as that hole. As that polygon segment is pulled across, as it is pivoted across, think about here's some other chunk of your polygon, you pull this edge across in the process of moving a pivot, that would create a strand passage, which is what we want to avoid in order to keep the topology fixed. And so this means we need to look for, are there any segments that cross either of these triangles? And segment triangle intersection is a very standard problem in computer graphics. You can use Molotrombor if you want something a little bit faster, you can look. Trombore, if you want something a little bit faster, you could probably find something a bit more exotic, but that's fairly standard. And so once we're down to the level of two edges, one of them being dragged, and we're checking the other one, we can test for these intersections. And we can avoid having to check all pairs of things by doing some careful sort of bounding sphere, bounding cylinder intersection checking. So, in particular, if I have some chunks. If I have some chunk of my polygon here and I know what cylinder that maps out, and I've got the fixed chunk of my polygon here, which isn't moving, and I know there's some bounding sphere about it, I can check whether those two things intersect each other by just a little bit of algebra, really. It's quite easy to detect if this cylinder intersects that sphere. And if it doesn't intersect, then I know that that chunk of my walk, I don't have to worry. On the other hand, if they do intersect, I can They do intersect. I can quickly focus in to see which bits might intersect basically by bisecting this and bisecting that and checking the possibilities. And looking at the time, I could go into this more carefully and I have some animation here, which I won't really do, but part of what I want to point out is that most pivots fail, and so we want to make sure that our algorithm fails quickly. And then it's successful. And then it's successful after eliminating those possibilities. Okay. So, does it work after all of this, getting it coded carefully? Now, it's slightly complicated, that answer. So, here is 1024 edge square after about 16 million pivots, so 2 to the 24 successful pivots, not attempted pivots, and it's still an unmarked. And if I do something similar, I want where am I. And I can do the same thing with a trefoil. I can take a simple embedding of trefoil, again, do about 16 million pivots. It's still a trephole. It's hard to see the trefoil in there, I grant you. But you do some testing with one of my favorite Python libraries now, namely Topoli. If you don't know it, you should. It's great. You can do some testing with that fairly easily. Okay, so that's one thing. So that shows that topology is being conserved. What about some distributions of things? What about some distributions of things? Well, so you can look at how the length of the segment, how successful the thing is compared to the length of the segment you're trying to pivot. You get some data there. You can look at, well, what are the angle distributions that are successful? You get some other interesting numbers here. So in particular, I have no idea what this distribution is or what it might be because we're talking about stuff outside my pay grade, maybe. But one of the things that we might try to speed things up is you kind of don't. To speed things up, is you kind of don't want to choose angles uniformly, you might want to choose them non-uniformly. Similarly, the length of the segments, we know long segments are going to fail a lot, so we don't want to be trying long segments all the time. We want to have a good distribution of those segments to speed things up. So we can start thinking about these things. Of course, we look at trajectories of things. Here we have what RG looks like. So these are 256 edge. 156 edge unknots generated by Tipoli using Cantarella's algorithm and having the Homfley polynomial tested on to check if they're unknots. And that took about 15 minutes to do. And so that is completely uncorrelated as it should be. If we look at doing something very, very simple with this pivot algorithm, if we do two to the twelfth iterations, you can see it's highly correlated, as one might expect. It's a Markov chain, there should be some work between. It's a Markov chain, there should be some Markov chain. So you start skipping some things. So this is skipping every 256, and you can see it's a lot less correlated. Skip 2 to the 12, it's still, it's looking pretty good, but it's not perfect. You notice there are these slightly wide dips here, which are a little bit concerning. So you do a little bit more, you skip every 2 to the 14, and this is still looking pretty good. So I haven't done sophisticated statistical testing on this yet, but I will. But I will. So then you go and you do the same thing for 1024 edges. And here I point out that doing this by Cantarella's algorithm and checking for topology basically becomes impossible. It's too hard to sample or to check the topology. It fails too many times or it just takes far too long. And so you get something like this. This was 2 to the 12 samples in about 6 hours, skipping every thousand. And it looks pretty good, nice and uncorrelated. You have these things here. Correlated, you have these things here where you can see there's this big canyon here, and you can see this big canyon here is quite a long period of time where the algorithm has quite atypical low rates of duration. I know I'm getting a bit long, and I'm going to finish up very quickly. So that needs to be explored. I can talk about it with anyone who can get interested to hear it. So, next step: a lot of code review, careful analysis with those canyons in the RG, maybe look at contact maps to understand. Maybe look at contact maps to understand what's causing that. Try different distributions of angles and things. Maybe try some simulations against the bending energy. Mix in some VFACF moves, all sorts of things, try to get back to the squared cubic grid, and of course, a huge thank to all constants.