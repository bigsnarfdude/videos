So, this talk is kind of complementary to most contributions we have seen so far. My main interest is not so much in solving concrete problems, but to understand properties of classes of complex functions and to find, say, the best setting of some related problems. Setting of some related problems. So you will probably not learn new techniques, but maybe you will look at some old problems from a new perspective. Here's an outline of the talk. And an apology at the very beginning, this talk is not about matrix problems in which most of you are interested, matrix factorization, and it is not about. And it is not about applications. So we start at the very beginning, back to the roots. What is a Riemann-Hilbert problem? Here you see four types of conditions which are associated with the name of Riemann and Hilbert. The first one is a linear transmission condition. This we have seen in many talks here, not a scalar. Not a scalar one, but a matrix case. The second one is a linear boundary value problem. In the first case, the functions phi plus and phi minus are the unknown functions, and here this w is unknown. The next, the third one is a non-linear boundary value problem. And the last one is a very general non-linear, what I call a transmission problem to distinguish between. To distinguish between the boundary value problems and in the middle here and the transmission problems, I have used these names. So Folk Lauren knows that the origin is Riemann's thesis, but almost nobody has checked what exactly Riemann has written. And it is one of my missions to make this more popular. To make this more popular and to bring some insight into this old text. In fact, Riemann introduced a new paradigm for studying complex functions. And to my surprise, this is widely ignored even by contemporary historians. I was trying to find some reference and those. Some reference, and the only one I found was from 1891. I did not get the original text of Heinrich Burkhart, but I found a review in the Central Platt of Mathematics. And here's a translation of this short or important part of this review by Heinrich Borkhardt. And key points are. Brokhard and T points are that Riemann has a new definition of functions by boundary and discontinuity conditions. So in fact, the linear problems one and two are not explicitly stated and investigated in Riemann's work at all. So they are used. The transmission problem Chunk problems and the scalar transmission problem are used extensively as tools in his studies of abelian integrals. But if one reads Riemann's thesis carefully, and this must really be done carefully, I will show it to you what it means, we indeed find the non-linear conditions, the non-linear boundary condition and the non-linear transmission condition. What is interesting is that the famous Riemann mapping theorem appears in Riemann's thesis just as an illustration of this non-linear boundary value problem three. So what about Hilbert? Hilbert's name should be associated only with the linear problems. He did not consider the non-linear cases. I will show you one slide about the contribution. One slide about the contribution of David Hilbert. So, as I already said, Riemann established a new paradigm for investigating complex functions. Here you see the German text, and this is the translation. I'll just click it through so you can see the English text. So, this is his intention to get rid of expression. To get rid of expressions for functions in some sense, and to characterize functions and classes of functions by boundary conditions and discontinuity conditions. This will be made more explicit in a minute. So I call this Riemann's project, which was established in 1851. And we should keep in mind that Riemann was at the age of 25 when he. When he invented or established this. So, what Riemann calls continuities, I will call a transmission condition. So, here you can practice your German. Reading Riemann is not a simple thing, even for a A simple thing, even for a native speaker, because he uses an ancient and in some sense also complicated language. What is important for us is this middle sentence. These are just three sentences. I think I must move this a bit, you know. Okay. So let's do the translation. I do not translate this. Translating is not enlightening, but I try to interpret it. And this is from another crowd of Riemann Thesis, where he writes these five lines. And the color-highlighted words are interpreted in the following. So Bekrensungsteil. What does he mean by Bekrensungsteil? mean by Begrensungstele? This is kind of cryptic. What is meant is that we have a dependence between function values. And then he is speaking about two Zwiberdingungs Kleischungen. His equations are always real, which means that we have one complex equation between the values of the unknown function w at two different function w at two different points t minus and t plus which depend on each other. So this is static votrickend means we have a continuous dependence between these two points on the boundary. So and if we interpret everything as a boundary condition we get this type of non-linear problem. The values of the unknown function at some point values of the unknown function at some point t are related by some function capital g to the values at another point t plus here and another point t minus which are related by such a condition. Of course we can also assume that this function g depends explicitly on this point t minus. What we get now this is a nonlinear boundary condition Condition of a complicated form. And now the topic of this talk and of Wiener Hopf in general is not a boundary problem, boundary value problem, but a transmission problem. How can we bring in this transmission condition here? Riemann considers domains as Riemann surfaces. So, and if he speaks about the Zweipe Krenzungsprungte, then this may be the same points geometrically on the boundary, but on different sides, say of a slit. So they are associated with different prime ends in the language of conformal mapping. So, and what we get then, this is this non-linear transmission problem here. Problem here. Now we have, in a sense, two functions: one limit on the left and one limit on the right, or up and down, if you wish. So, and this indeed contains, of course, the linear case, which is the standard Riemann hero transmission problem, which is a hero, so to say, of many talks at this workshop. Okay. Okay, so what do we learn from this? One has to know what to look for, and one has to interpret Riemann's writing to find this transmission problem at all. So what about Hilbert? Hilbert considers the two linear problems, the transmission problem and the boundary value problem, as well, mainly in two papers from 1900. Mainly in two papers from 1904 and 1905. You see it here. And in the first paper, he introduces this conjugation operator. So the talk was given in 1904 and the papers published in 1905. So he introduces the conjugation operator, which is named after Hilbert later. And he also considers this matrix. He also considers this matrix version of the transmission problem, which plays an important role in his 21st problem to construct the Fuxian differential equations with given monotromy group. But this leads us too far away. So, but here is another contribution which I feel obliged to mention here. To mention here because it deserves much more attention than it has. So the person is Fritz Noeter. Fritz Noetter is a brother of the famous Annie Noether and the son of Max Noetter. And he discovered or invented some very important fact. He studied this linear Riemann-Hubert problem, the boundary value problem, and And in parallel, he was considering singular integral equations. And he found that there is an intimate connection between the winding number of the symbol of the singular integral equation of the Riemann-Rebel problem, which is just this coefficient, and the functional analytic index of a linear operator, say of a singular integral operator here. And this is Operator here. And this is the definition of the index, how it is used in functional analysis nowadays. So, this was really the first place where this functional analytic index appeared. So, Fritz NÃ¶rder had a traumatic life. I will not comment too much on this. You can read these few lines. And if you would like to learn more about it, then you can read a recent. Can read a recent paper. This is, I think, freely available in the Mathematical Intelligence by David Robert, the member of Fritz Neutral, just appeared by accident, right for this conference. And this is about the life and also about part of the work of Ritz Neutral. But when I read it, I saw that the index does not appear there. And so I wrote a compliment. I wrote a complement two pages in the intelligencer about the index. So now the title of my talk is Rienman Hilbert versus Wiener Hoff, the two sides of a metal also. Very briefly about Wiener Hoff, you know all this, the classical papers by Wiener. Papers by Wiener and Ropf are concerning these things, these integral equations on a half-line, which is translated by a Fourier transform to this Wi-Nor-Hoff problem, where we have an analytic function v plus in the upper half-plane, or we have to find an analytic function in the upper half plane phi plus, and an analytic function in the lower half plane, phi minus. So in the classic. So, in the classical case, these functions, the coefficient, the kernel k and this inhomogeneity z is our analytic in a horizontal strip. And if we consider this not in this strip, but just on the real line, then we exactly get a linear Riemann Hilbert transition. Riemann Hilbert transmission condition. And solving this equation is the core of the Wienerhof method. In the scalar case, this is easy, but as you know, it is quite difficult in the matrix case. So if we generously neglect everything from applications and all this background, then the main difference between Wiener Hopf and Riemann Hilbert is just. Is just the conceptual difference concerning the specific properties of the kernel function k and the asymptotic behavior of the analytic functions phi plus and phi minus. So in this sense, this is what I had in mind when I announced this title, that the two problems are two sides of a metal. As it turned out during the preparation of this talk, I found another interpretation. I found another interpretation which we will discuss of this title, which we will discuss at the end of the talk. So now this is the main topic of my talk. What are representations of functions? More specifically, we are interested in representing a function small f which is defined. Or F, which is defined on a curve, typically a curve, say an interval or a real line in the complex plane or on the Riemann sphere. And we would like to represent it by boundary values of analytic. And I say analytic, I mean holomorphic functions in the complement of this curve, in the neighborhood of this curve without the curve itself. Curve without the curve itself. So, this is the classical case. Jump relation of Flamler and Sorotsky give an additive decomposition of a function f, which lives, say, here in a curve or an interval of the real line, and the solution is given by the Cauchy integral with density f. So we have a minus, but I call it an additive decomposition. And interestingly, this is a local property of the Cauchy integral. If we change this f outside some sub-interval, say, then we have the same relation here. We can also express this in terms of functions on the curve itself. Itself. Then this decomposition is often written with these two projections, these or sometimes segue projections, P and Q, which act on functions defined on the curve. These projections are not local. So if we change it in some part, then everything changes globally. So another way. So, another way of representing or decomposing a function, this is just Levi-Roff method. This is a multiplicative decomposition. So, we have the Kirnel function. This is the function which we would like to decompose. And this Wiener factorization gives a representation as a product of an analytic function in the upper half plane and an analytic function in the lower half plane. The lower half plane. So, in these classical applications, the index and winding number of k is zero. If we have a non-vanishing index, we need a third factor. And for me, it is convenient to transplant everything from the half planes to the unit circle. The unit circle. So we have analytic functions in the interior, respectively, in the exterior of the unit circle. And then this factorization can be written like here. So we have a factor t to the power k, where this k is the index, which compensates this winding number. This factor here is by no means unique. We could We could put any function with an appropriate winding number in here, and of course, this would change the factors. But this is kind of a topological thing here. In some sense, this factorization lives on three parts of the decomposition of the plane. So, this K plus lives in the unit disk, this lives on the unit circle, and this lives outside. In the exterior of the unit soccer. So, this I already mentioned, but as announced at the beginning, I will not speak about this matrix problems. Okay, so we have seen two types of decompositions or representations, additive and in multiplicative decomposition. Multiplicative decomposition. There is a third one, which is also related, I would say, even closely related to Wienerhof. And this I would like to present in some more detail. I learned about this representation in a paper by John Silvestro and Dale Weinbrunner many years ago, where they studied a non-linear scale. Studied a nonlinear scattering problem in layered media. So, when they investigated this problem, they introduced what they called a nonlinear Ries transform. Remember, the Ries transform is just another word for this jump relation expressed in terms of the projections P and Q. And it is a representation of functions defined on the real line by boundary values of analytic functions f plus in the upper half plane and f minus in the lower half plane. So it fits well into our setting. But this function f is not represented as a sum of f plus plus f minus, but as a fractional expression, f plus plus f. expression f plus plus f minus over 1 plus f minus conjugate times f plus. So we need in order to make sense we need conditions on F. You can maybe already guess what these conditions are. I mean it could happen that we have zeros here in the denominator. Denominator and this has to be avoided, of course. The right-hand side of this representation one resembles the structure of a Pluschke factor, which is a conformal automorphism of the unidisc. So this is a Pluschke factor. I write it with a plus here. Usually it is written with a minus, but since we have a plus here, I also have a plus here. So five of So phi of z is z plus a over 1 plus a conjugate z. And this is a conformal automorphism of the disk, which maps the point minus a in our case to the origin. And this is the reason why we use the name, or I would like to use the name Pluschke representation or Pluschke decomposition. So here is the result of Sylvester and Weinbrunner. They assume that we have a function f which is less than one in modulus. So say this is the L infinity norm of this function f defined on the unit circle. And not on the unit circle. In that case, it's on the real line. So this is a natural assumption if we. Natural assumption. If we maybe I can go back now. This is a natural assumption because this plushke factor plays in the unidisc. But besides this natural assumption, they need additional technical conditions. So they do not take care too much for these conditions because they are satisfied. Conditions because they are satisfied in the applications, and under these assumptions, they prove the existence and uniqueness of this representation. And here's the result. We pick a number, a negative real number A and consider functions f, which are in some space, which is constructed in their paper. This page A. Paper, this page A E. This consists of analytic functions in the upper half plane, and they admit certain growth here. So this is a growth factor, if you wish. And they have to have finite energy. This is kind of an energy for functions. For functions which attain values in the unit disk. You see, they cannot be, the norm cannot be equal one on a too large z because then this energy would be infinite. But they can have some kind of singularities. This log can have kind of singularities, but it must be integrable. Must be integrable. And the energy must be not too large. And then this F has a unique Pluschke decomposition or Pluschke representation, where again, the factors F plus and F minus are in some spaces which are constructed especially, which are adapted to this type of problem they are considering. So, when I saw this, I had the idea: well, it would be nice to prove the same result in weaker assumptions without these additional conditions here, but just with the natural assumption that the norm of f is less than or equal to one. Yes, and this could be achieved. It is not a new result, I must confess. It. I must confess it was obtained together with my colleague von Wolfostorf, who passed away a few years ago. It was in, I think, published in 2008 also. But I think it is a nice result and it does not have a single citation. And so I would like to promote this result. And I think you are the right community to. To apply it, maybe. So, in fact, every bounded measurable function, the space L infinity, it is now translated to the unit disk, which is not a big deal because we can apply this Cayley transform, which maps, say, the upper half plane to the inside and the lower half plane to the outside and preserves analytic functions in both domains. So, every function. So, every function with norm less than one, bound and measurable, admits such a representation. Now, we have, of course, to think about the boundary values. These f plus and f minus are functions in the Hardy spaces, h infinity in the interior and h infinity of the in the exterior. These are bounded, just bounded analytic functions. functions and the norms f of f plus and f minus are also not exceeding one. And this is a normalization condition where the minus function vanishes. So this exists always, but it may be not unique, as we will see in a few minutes, if this function has norm equal to one. Has norm equal to one. But if it's less than, strictly less than one, then the functions are uniquely determined. So one may interpret this as a hyperbolic form of the Ries decomposition. Why? If we equip the unit disk with the hyperbolic metric, this is this row here. This is this row here. This metric is defined as a logarithm of this expression here, where this p is the complex, the complex pseudo-hyperbolic distance in D. So this P has exactly the form up except or up to the sine minus here as this expression here. So while the reason So while the Ries decomposition writes a function f with the Euclidean distance of f plus and f minus, the Blushke decomposition writes the function f with the complex pseudo-hyperbolic distance of the two functions, f plus and f minus. So it is remarkable that the Ries projection is not a bounded operator on L. Is not a bounded operator on L infinity. But in this case, we have a bounded norm not only of the function f but also of the functions f plus and f minus. I think this may be useful. So this approach of the Rustra and Weinker is in the spirit of classical Wienov techniques with given functions that are analytic and Functions that are analytic in a script. And we treated the problem as a non-linear Riemann-Hilbert transmission problem. Here you see the paper, which means we do not require anything except conditions on the functions f on the boundary itself. So, if I Cayley transformed, this can be mapped to the unit. Unit disk and its exterior. What we get is the transmission condition here and linear. This is also called a linear fractional Riemann-River problem. Maybe this can be seen better if we look at the original formulation with this fraction. And such linear fractional Riemann Hubble problems have been studied for quite some time. Quite some time. I mention some names here. Some of them are in the audience, I think. But the known results did not apply to the case at hand. And I cannot resist the temptation to show you more or a related general result, which does also not cover this. Does also not cover this Pluschke representation, but it is about more general non-linear Riemann-Hilbert transmission problems. Here you see the boundary condition TP. So given a function capital G, this capital G lives on the product of the unit circle with the complex plane. The complex plane. Here we have this product. This is given and it has complex values. And we are looking for two functions, f plus and f minus, which are holomorphic, in this case, inside the disk and inside the circle and outside a circle. So here are the assumptions which we need. This function g need not be analytic. Need not be analytic in the second variable. It can have a Cauchy-Riemann operator, a derivative with respect to z-bar, which is different from zero, but the analytic part must be dominating. This is the first condition here. So we have a dominating analytic part. Analytic part. And the second thing is kind of winding condition which we need. You know, all these linear problems have an index and the nonlinear problem has something similar. So we need the compensating function small g with binding number zero that is real Paul Teus positive. In some sense, this guarantees. In some sense, this guarantees that all our auxiliary linear problems which we get have index zero. So, and the result is that if these conditions are satisfied, then we have solutions F plus, which are in the Hardy space inside and outside the unit circle, and which are continuous up to the boundary, up to the unit circle. Up to the unit circle, and that's a normalization condition. And if we have a little bit stronger condition than here, then we also have uniqueness. So this is an outline of the proof for the plushkin decomposition. As I said, this result which we have just seen does not immediately. Seen does not immediately apply. This is clear because this function here is not even defined on the product of the circle with the plane. We have zeros typically down here. And so, but with some subtle technical modifications, we were able to adapt this proof. This proof of the general theorem to the Pluschke decomposition. And here are the main steps. First, we assume that we have a continuously differentiable function. Then we approximate an arbitrary function by a sequence. Of course, there is no sequence in general which converges in L infinity, but we can find a sequence which converges in L two. Two, and then we find converging subsequences in L2 with limits f plus and f minus. And then finally, one can prove that these functions are in fact the factors in this Plushka decomposition of this f. So, I think I have enough time to go through this first step basically. Basically, we work in Hardy spaces better in Hardy-Sabolyev spaces, Hardy spaces in the domain and on the boundary. We have a boundary function which lies in the Sobolyev space with appropriately chosen p. Then, this is a nice trick which we applied. I think it was invented by von Wolferstorff earlier to. To treat such a non-linear boundary value problem, one can differentiate the boundary condition along the unit circle, which yields a quasi-linear Immanuel-Lilbert problem, which is linear in the derivatives, but non-linear in the functions. And this allows one to reconstruct the function by from the derivatives by integration, and the result. Equation and the result is a compact operator. So, what we need finally is a fixed point of a compact operator. And these fixed points are the solutions of these we are looking for. And this compact operator now involves a linear Riemann-Hilbert problem with conjugation. This conjugation. The nonlinearity is now in these coefficients A and B, and our assumptions guarantee that this is an elliptic problem. And finally, we applied Liri-Schauder theory to prove the existence of a fixed point, which requires a priori estimates for the solutions. So this is not. This is not a perfect result. In the sense, I mean, if you take the Riemann mapping theorem, this is a result which is perfect. I even call it eternal, because there is nothing which can be improved. There are no additional conditions you need, and you have existence and uniqueness. This is not exactly, so in this case, this is more or less. This is more or less a trivial function, but it has modulus one, constant one, which admits two different representations. So, uniqueness is not always guaranteed. Also, we can have a function with norm less than one and factors which have norm equal to one. And this can even happen for continuous functions. Even happen for continuous functions. Here's a counter-example. You need not understand this really. We started with the two factors, F plus and F minus. One is a conformal mapping of the unit disk on such a teardrop region, the region with the blue boundary, and the other one is kind of reflection here. And so these singularities say at the tips are kind of. Tips are compensated, and this results in a function f, which has norm less than one. What we see here are the traces of these functions, the boundary functions. Okay. Now, this was the serious part. Now we have some entertainment, which is more kind of philosophy. No, this is not really the philosophy, it comes a bit later. So what I learned some years ago is that these jump relations are the core of the concept of hyperfunctions. So hyperfunctions were introduced by Sato in 1950. In 1958, after pioneering work of Grotendieg. And if you read the original papers, this is kind of, I would not say disgusting, but it's really hard stuff. But the basic idea is quite simple. And meanwhile, there are also presentations which one can easily access. And I really would like to. I really would like to recommend this to you if you are not already familiar with these hyperfunctions. I think this is a very useful tool. I'm a bit too old to make use of it, but especially the younger listeners in the audience should really have a look at it. So the basic idea is quite simple. One tries to One tries to represent the function on a set, say an interval or on the real line or a curve by analytic functions in a neighborhood. So assume that's an interval on the real line, then we have the simple situation. And the idea is to generalize in some sense. Generalize in some sense this Sovatsky-Plamel, or to use this Sovatsky-Plamel jump relation to represent functions on this set J, which need not even be functions. They can be generalized functions. So we take these analytic functions in a neighborhood and say that two such functions are equivalent if there exists an analytic functions in a neighborhood. In a neighborhood V now, which contains this J, such that one is the sum of the first plus this analytic function. And the resulting equivalence classes are the hyper functions on this, say, interval jet j. And if we split this function, this This defining function or this representing function f into an upper part f plus and the lower part f minus, then we can interpret it the given function f in a sense as this f plus minus f minus. But we do not require that the limits of f plus and f minus at the boundary really exist. This is the generalization. Generalization. They may exist, and then we have a classical jump relation, but they need not exist. And here are the two simplest examples. The Dirac delta impulse is represented by the Cauchy kernel. You see, this Cauchy kernel is a reproducing kernel, and the delta impulse is also a reproducing distribution. Or the Haversite jump function is represented. Jump function is represented by a logarithm. So, both are fundamental things, as well as in the classical interpretation and in the complex interpretation. So, in fact, this concept of hyperfunction is an alternative to distributions, which can make, say, complicated objects living on this line, like distributions, simple. Distributions, simple. By simple, I mean analytic functions in some domains. But of course, this has a price one has to pay, and this is the translation from the domains to the boundary. So I learned this from Roger Penrose's book, A Road to Reality, and this is really an enlightening text. If you would like to use it in your work, then I would recommend this book by. Recommend this book by Wolf Graf: Introduction to Hyperfunctions and their Integral Forms. And this also contains, I think, a chapter or a paragraph at least about Wienerhof techniques. So now we come to the entertaining thing, philosophical remark. The Greek philosopher. The Greek philosopher Plato describes a situation where people are sitting in a cave and they only can see the shadows at the wall of this cave. Maybe they are projected kind of from outside or maybe from some other place. What they see is the shadows and in the original text they also hear some sounds, but this is not so important for us. And they think their And they think their reality consists of these shadows. Okay. So, and later, the philosopher Socrates explains a philosopher is like a prisoner which is freed from the cave. And he comes to understand that the shadows on the wall are not the real objects. So he has some. So he has some insight in reality. And we can translate this to mathematics with respect to this hyperfunction approach. To understand the nature of a function, say on a curve j, we have to consider it as a shadow, say the trace or the boundary function of analytic functions, which live outside in a neighboring domain or in several domains with boundary. In several domains with boundary j. And this can be reversed from this point of view. We may also think of the primary objects as the shadows on the wall of the cave. We should not call them shadows, but they are now the primary objects. And anything else which plays in three-dimensional space, say. Three-dimensional space is virtual, like a hologram which is constructed from information at the wall. This is not my invention, but if you go to the internet, then you find a lot about this holographic principle. This is a constituent of physical theories, which states that a description of a process in a volume of space can be sort of. Can be thought of as encoded on a lower-dimensional boundary of the region. Here's a quotation of a famous physicist, Leonard Zwitzkin. He goes very far and claims the three-dimensional world of ordinary experience, the universe filled with everything, is a hologram, an image of reality coded on a distant two dimensional surface. Than two-dimensional surface. And to my surprise, just when I was preparing this talk, I read the new volume of Spektrum de Wissenschaft, the German newspaper, which is kind of twin of Scientific American, November 2024. You see where they announced that this holographic principle was used. Principle was used as a useful tool to get progress in understanding the strong coupling force. So now, if we come back to mathematics, our two things on the one side, Plato and Scave Electric and the holographic principle, such as the modification of the setting for decomposing. For decomposing a function on a curve. What do I mean by this? If we think about a horizon, then a horizon is normally the boundary of one region and not of two domains. So we could look for a decomposition of a function into two functions with a common. Functions with a common domain, in the same domain. And then it's clear what we have to do, because this is exactly what we can do in the case of the real line or in the case of the unit circle. So one function has to be analytic and the other one has to be anti-analytic. So here we see these two operators, Cauchy-Riemann operator, and then the conjugate of it, also sometimes called. Those are sometimes called voting operators. So, both classes, analytic and anti-analytic functions, are invariant with respect to conformal mappings, so that this type of representations can be transplanted from a standard domain onto more or less any other simply connected domain. To be careful, say a Jordan domain simply connected and bounded by a Jordan curve. Jodenkov. Here's an illustration. If we would like to decompose a function on a boundary of some domain, we can say simply connect it. We can extend this H to a harmonic function. Then we split it. This is another G. This is the same G as this one. We can split it. We can split it inside the domain in an analytic and anti-analytic part, which is quite simple. And then we have to determine the boundary function. So step two is simple. Step one and step three are all, yeah, we have to find techniques which work. So here's a this idea applied to this Pluschke decomposition. Decomposition. We, in order to get this first for the unit circle, we reflect this function in the exterior across the unit circle or about the unit circle to an anti-analytic function. This has the same boundary values, and if we have a And if we have a Jordan domain, then this makes sense that, or then we know that the conformal mapping extends continuously to the closure of the domain, and we can transplant everything inside and on the boundary to this arbitrary domain. So, and here we see the same result holds now with an analytic function and anti-anti-analytic function in all. Anti-analytic function in arbitrary Jordan domains. So finally, yeah, this is something which I would like to show you because I was surprised when I discovered it for the first time. It's a very trivial thing, but you can teach it to your students. We would like to go from the plane to the sphere. And this image on the left illustrates the usual stereographic projection in textbooks. It has two drawbacks. First of all, the unit circle is mapped to a larger circle. This can be easily avoided when we shift the plane upwards. And I always wonder why this is not the common definition. With the common definition of this diagraphic projection. Now we have this nice decomposition of the Riemann sphere into upper, lower, and equator. And the equator is our unit circle as it was before. But there is a more serious problem, namely, if we project from the north pole, this geographic projection reverses orientation. This also appeared in. This also appeared in a talk this morning. This does not look like a big deal, but for some applications, and in particular for graphical representations of functions, this has traumatic consequences. One should better project from the south. So I have only a few minutes, so let me be very, very brief. What we see here is a phase plot of a function. This is a special. Of a function. This is a special way of or a special version of the main coloring, which represents a function as an image by color coding the values. What I like, this is face plots. Face plots basically use only the argument or the phase here and encode these values by a standard color wheel. And here we have a more sophisticated version which also. Sophisticated version, which also highlights some contour lines where the modulus of f is constant. What we see here is a polynomial, and this polynomial has four zeros, which we see here where all colors meet. So look at the orientation. Red, yellow, green, blue. In this way, we have the colors moving around and we walk around a zero. Now we have this. Now we have this stereographic thing, and we project it to the sphere from the North Pole. And now look at the results which we get here. This is the image of a zero, but the colors are reversed. Red, yellow, green, blue. And for those who are used to work with phase plots, they would interpret this as pearls because poles and colors can be distinguished by different orientation. Different orientation of the colors. So if we project it from the south pole, then we see the inside on top, the unit disk and plus one, zero on top here. Of course, we can rotate everything around, but then we look at this complex plane from the wrong side. So now back to the topic of the talk. We would like to interpret the representation, say, this Ries decomposition on the sphere. So we have a function f minus, which is analytic on the lower half side, and we have another function, which has nothing to do with it, in principle, which is analytic on the upper half sphere. And now we project this F minus from the norm. This F minus from the north pole to the disk, and we project F plus from the south pole to the disk. Now I made this transparent. So what we see here, this is an analytic function, the projection from the South Pole, which lives on the upper side, and this is the projection from the North Pole. This is an anti-analytic function, but we just can turn around. Can turn around this, say, solid disk. Now we consider it as a metal, and then we have two analytic functions. One is imprinted from the upper half-sphere sphere, and the other is imprinted from the lower half-sphere sphere. So, this is now the metal setting of this Ries representation. These Ries representation represents the function which is given on the edge of this metal, an arbitrary function by analytic functions which are analytic on the upper side or on the lower side. Well, I mean, this can be obtained more easily than I did it here, but I find it quite suggestive to use these things. If you would like to look Like to learn more about face plots, I can recommend these two things. This is an advertisement, a book, and a series of calendars. This, the current calendar, is the last one we stopped to produce because it took too much time and we would like to do more serious things. And among the things which remain to do, a few are mentioned. A few are mentioned here. If you think the representation in this metal version, metal version, and then this is completely symmetric with respect to the two functions on the top side and on the bottom side. But the representation itself is not symmetric because one function has this complex conjugation. Question is there a symmetric representation. Is there asymmetric representation? I do not know. And of course, there are a lot of further questions which can be considered. First of all, what about matrix functions? If we speak about the Plushke representation, we need assumptions on the norm. So typically, we would consider contractions, matrix contractions. Are there applications in physics? Applications in physics. I know only this one by Sylvester and Weinbrenner, but maybe you find something else. I guess there will be applications in operator theory. And of course, there are also many questions about numerical methods. We have good numerical methods for these boundary value problems, but not for the transmission problems. General nonlinear transmission problems. Problems. Okay, that's it.