I think it does. But it's a little bit of a reality. So we just like So I just keep researching all the time. So next talk is uh are you offer Uzani? Thank you for the opportunity. I will speak on Timo's birthday. This talk is based on three papers till I'm joined with my wonderful collaborators, Timo here and Evan. So the main thing, the main thing of this talk, one of the main things of this talk is One of the main things of this talk is random growth interfaces. So imagine you have this two-dimensional surface growing in time, like here in this picture. And so for each point in time, you can identify the interface. What we're interested is in the dynamics of this interface. So the KPC class is a very large, it's a huge class of such models. In fact, it consists of huge families of Of huge families of models that similarly have nothing to do with each other, but in fact, they can all be seen as these random growth interfaces in one way or the other. This talk specifically I will focus on two such families. One is the metric-like models, and the other one is the particle systems. So, for the family of metric-like For the family of metric-like models, perhaps the most well-studied model there is a last passage proponent. I'll briefly, because I'm sure most of you are familiar with it, I'll briefly describe it. You have an exponentially weights IID across the lattice. You take two points, X and Y, such that one is above and to the right of the other. Then you look for an upright path starting from X, terminating. Starting from X terminating in Y, that collects the most weight. For example, here is such a path, it takes only upward steps, and if it maximizes the weight among such a path, then it's called the geodesic. Now, as Ban mentioned, this is not truly a metric, but it's close enough and it helps to think of it as such, and so I will use geodesic, even though it's not a geodesic, and I'll use it. And I'll use distance and other notions that not completely fit here, but would help the exposition. Okay, so how is this thing related to random growth interfaces? This is seemingly a metric. So what you can do is you can assign the subsets of vertices A0, let's color it in red, and we assign weights to it. Assign weights to it that are independent of the weights in the bulk. What I could do now is for each point in the bulk, I can measure the distance to A0 simply by looking for the path starting from A0 and terminating at that point that maximizes the weight of itself. This will be the distance between the set A0 and that point and the park. Now fix T, positive. And now when coloring red, all those points whose distance to A0 is Whose distance to A0 is equal or smaller than nt. And this point you can see we can identify an interface for time t. This will be gamma t. And clearly, as time goes by, this interface will sort of grow northeastward. So we have this dynamics on interfaces. And a natural question then is what is the stationary, is there a stationary distribution for Is there a stationary distribution for this dynamics? Now, clearly, this is going to infinity, this downright path is going to infinity, so there's not really, we can't ask for a distribution on downright paths that would be stationary. So, we'll tweak it a bit and what we say is that it is up to some translation on the lattice. Namely, if we take these different interfaces at different times and we pin them to a same reference point, then the distribution Then the distribution would be equal across tax. So, is there such a stationary imagery? Well, it turns out that there is. If you take any row between 0 and 1, there's this bi-infinite IID random walk H rho. Namely, we can assign on A0 these weights that would look like a random walk that we call H rho, and this would divide. Grow, and this would give rise to this kind of stationarity in the sense that I mentioned. The tilt of this random mark will depend on. Okay, so let's make this a bit more complicated. Now, what we're going to do is we take two rows, row one and row two, between zero and one, and we know that for each one of them, we have this h row one and h row two that will give rise to this stationarity. Stationarity. And what I would like now is to put them here, couple them in some way on A0, these two random walks. Apply the dynamics using the same valve rates, so that each point in time you'll have two downright paths, one gamma rho1 and gamma gamma rho2, in red and green. And now what we would like to And now, what we would like to have is a coupling of HRO1 and HRO2 on A0 that will give rise to a joint stationarity of the two interfaces. In other words, I would like the joint distribution of these dashed lines in red and green to be the same as that of the solid red green lines. And is there such a coupling? And it turns out that there is. And it turns out that there is, and this is exactly what I mean by stationary multi-type distributions. Namely, there is some sense of types, a couple of types. I'm using the same bulk noise to force the dynamics, and I'm looking for this whole thing to be stationary. And if you give me rho 1 to rho k between 0 and 1, I can find a coupling or I can find a coupling of these random walks that would give rise to this type of stationarity. And this is a result of the explicit construction of this coupling is by Fan and Tim. Okay, so we know we have this family, we have a bunch of these distributions that are stationary under this multi-type dynamics. type dynamics. But now it would be nice to have this process that couples all of these distributions in one space. And in other words, think of h rho as a process in rho, the indexed in between 0 and 1, such that if I take any rho 1 to rho k and I sample this process, then I get the right distribution that I'm interested in. And fortunately, there is such a process. And fortunately, there is such a process, it's called the Huzzmann process of the exponential LPP, and it's super useful in studying infinite geodesics in these types of metric-like models. This is a very non-exhaustive list of papers studying infinite geodesics through a Boisman process. Through the Bruismann process, you can see in the The Buzzmann process, you can see in the first passage, last passage, and also a positive temperature. Super useful. Okay, so one of the main attractions in this business of KPZ is the KPZ universality conjecture. This says roughly the following, that the long time, under some scaling, the long time behavior of the fluctuations is universal across Is universal across models in the class. What this means is that if you take a model in the KPZ class and you scale it macroscopically, then over time you see this kind of deterministic dynamics of your interface. You start the time-zero interface, and then you see some kind of a deterministic evolvement of this interface, like hydrodynamic. Interface, like hydrodynamics. So, if you want to see something interesting and random, then what you should do is you should fix a direction and look closely around the endpoints. And it turns out that for the KPC class, the right scaling of those little windows is t to the two-thirds and t to the one-thirds in height. And in fact, what the conjecture says. In fact, what the conjecture says is that if the initial fluctuations scale into something reasonable, then so will the limiting profile. And this mapping between the initial scale profile and the limiting scale profile is called the KPZ fixed point. And strictly speaking, this is a Markov process whose transition probabilities were obtained by Matecki, Costell and the But let's keep a step in the brain. Okay, so what do we know so far? We know that there's this model, this called exponential LPP, and we know that it scales to the KPC fixed point. We know that there's this something called Bozmann process that somehow relates to this model, being its stationary multi-dimensional distribution. Multi-type distribution, sorry. So now, what we would like What we would like to know is: here are a few questions, interesting questions. Can we scale the Buzzon process into something interesting? What would the scaling be? And if there is such a interesting limiting object, then what would be the relationship between that object and the KDC fixer? So here is a result in this direction. It tries to answer these first two questions. To answer these first two questions. What is the scaling and what does this thing scale to? And what it says is the following: you take the Bozeman process, you fix a density, this is like fixing a direction, and you perturb on the order of n to the minus one-third, and you scale diffusively, and then what you get is that you, in some topological sense, you convert. A topological sense, you converge to a non-trivial limiting object, which we call the stationary horizon. So, put simply, we started with these bunch of random walks, coupled random walks, right? And when we scale them, we get the stationary horizon, which is just a coupling of Brownian motions indexed by their drift. By their drift. And this is a simulation by Ed. Okay, so you can see here Brownian motions, the different colors are the different drifts. And one of the interesting aspects of this model is that whenever if you look at a compact window in the drift and space, you'll only see finitely many lines. So if you want to see more and more lines, you just have to zoom out and To zoom out and you'll see one. Okay, so in other words, we answered these two questions: what is the scaling and what are we scaling to? Okay, so this is true for exponential LPP, but we believe this is true in general, for this family of metric-like models. By now, we have some, we have. We have some partial results toward that, but we believe that this should be HMM. Shortly after I posted my paper, Timo and Evan had been working on the Buzzman process of the Browning LPP, and they discovered the stationary horizon as the macroscopic Buzzman process of this specific model. And the reason why, so this is a And the reason why, so this is a unique thing for this model, because it's already diffusively scaled. So the stationary horizon appears as a macroscopic Guzman process for this model. At this point, we were very happy and we decided to join forces and try to answer the last question. So what is the relationship between the KPZ fixed point and the state? KPZ fixed point and the stationary horizon. And the result is the following. It says that this relationship between these two microscopic models carries over together. So the stationary as it means the stationary multi-type distribution of the KPZ fixed points. What I mean here by, so just to be precise, is that what we mean is by if we What we mean is by if we apply, so the basic coupling in this for the KPZ fixed point is to the directed landscape. Namely, if we start two initial conditions and we push it through the directed landscape, then we have this kind of multi-type station activity. Okay, so we know this. We know that the, or we believe that for the Boson process, the stationary rhizom is the scaling limit of the Booson. Is the scaling limit of the Bozeman process of models in this metric-like family? Then we asked ourselves: well, is it possible to find the stationary era in another model, another families of models? So we set our eyes to the next family of models, which is that of particle systems. So at this point, we know this part, but it's not clear what the candidate or the Business. Candidate for the Boisson process would be for the interacting particle systems. Okay. So as last passage correlation is the most and well, perhaps most well-studied model in the family of metric-like models, TASEP, the totally asymmetrical exclusion process, is perhaps the most studied object in that of interactive particles. In that of interactive particle, in the family of interactive particle systems. I'll briefly recall what it is. You have particles, you assign particles to the lattice wherever you'd like, and each particle has a Poisson clock, and these Poisson clocks are independent of each other. Now, if a Poisson clock rings for a particle, then if there is no particle to its right, then it makes the Right, then it makes the jump. If there is a particle, then the jump is suppressed. Okay, so we have a model, but then we need some notion of types. So the types here come in classes of particles. So what I'm going to do now is I'm going to have two types of particles, one black and one red, and I'm going to call the black ones first class particles. ones first class particles and the red ones second class particles. The dynamics is very similar to before, it's that of the original TASAP. The only difference is that if a first class particle wants to take the position of a second class particle, then it does it. The other way around is not allowed. So second class particle cannot take the position of a first class particle. In other words, first class particles see second class particles as whole. See second-class particles as wholes, while second-class particles see everyone as particles. And you could do this, you can add as many particles as you wish, you could be infinitely many classes of particles. The story is as before, if you see everyone below you as holes, you see everyone above you or your class as particles, then dynamics is the same. So, this is allowing. So, this is allowed, this is not allowed. Okay, so now we have a model, we have types, so we need stationary measures. So, are there stationary measures for these types of multi-type passps? So, the answer is yes. You take any row 1 to row k between 0 and 1, and you can construct, and there is a stationary map. construct and there is a stationary measures for this model such that rho i is the density of the the particles of class i. We also have invariant measures for this model. Okay, so if you recall what we did before with last passage, we need something that would sort of couple all these measures together. This would be our Good campaign for scaling. So, what is this object that couples all these distributions together at once? This is what's called the TASEP speed process. This was introduced by Amir and John Valco in 2008. And what it is, it's simply, I'm not going to go into, we don't have time to go into what's the main motivation there, but it's in a nutshell. But it's in a natural, it's a random object that assigns numbers between minus one and one to each side of the lattice. Now what's wonderful about this object is that you can read off from it all those stationary distributions for the particle, for the multi-type particle systems. The same way we did for the same relationship between the Bozeman process. The relationship between the Bozmann process and the last passage percolation holds here as well. So, what do I mean by how do we do that? How do we sort of glean the information from the Tuscan speed process? You assign two thresholds, V1 and V2. And now for each site, you look at these points are called speeds, because it's a Dussell speed process. So if the speed is below V1, we assign a particle, and if it's between V1, And if it's between V1 and V2, we assign a second class particle. If it's outside the range, then we assign a whole. And it turns out that this distribution, this random configuration we end up with, has the stationary multi-type distribution with these prescribed densities. In other words, we can get everything, we can add more thresholds, and we can get all the And we can get all the stationary distribution. Okay, so we have a candidate. We have a candidate for scaling. So, how do we scale? We fix a macroscopic, in the same way we did with the last passage, we fix a macroscopic velocity, v and we take a window on the order of n to the two-thirds in particles and n to the minus one-third in speed. To the minus one-third in speeds. And then what I'm going to do, I'm going to take a threshold and I'm going to scan for the different particles inside that window, and I end up with, let's say, this. And conveniently enough, I only did like three types of particles, so it's easy to draw these things. So we have, but you can have as many as you like. It doesn't have to be three classes. It doesn't have to be three classes of particles. Let's say we ended up with this configuration. Now, what we're going to do is this: we're going to take only the first class particles and we're going to interpret them as, we're going to translate them into an interface. How do we do that? Whenever, so this is very, this is well known. Whenever you see a particle, you go down. Whenever you see a hole, you go up. See? And I do the same. I have a reference point from which I start, and then I go. From which I start, and then I go the same way on the left, and I construct an interface. Then I add the red points, the second class, and I consider all particles for going down and up, and I have this one, the red interface. And I continue onward for the blue, and I have a blue interface. In other words, what I did is I started with this. I gleaned an information. I cleaned an information from that window. I translated it into this coupling of these random blocks. And here is the result. If you scale this, if you do this translation of particles to interfaces and you scale them diffusively, you again converge to the stationary horizon. Much simpler. Much simpler. You take the thing here, you flip it, and it converges to opposition phrase. Okay, so happy birthday to all. Is there any other question on? Yeah, I was wondering, can you make this construction of the session horizon slightly more? Construction of the session horizon is like more specific. How are these barrels most helpful? So that's a good question. To some extent. So I can maybe tell you that the joint distribution of two lines, of two prescribed row one and row two, mu1 and mu2. But that's an interesting, maybe it should go into the open problem session. So there should be some determinantal object in the background. For example, this should be, if you take a section of this, of these lines, right? If you have a point process, what's the distribution of this point process? There should be some integrable thing going on here. I don't know what it is. It should be very interesting. We just know it exists. We know it exists. We know a bit more in the results. Bit more in the results of Timor and Evan, they have a bit more results about the finite dimensional distribution. But again, there should be more information. How many you have a fixed distribution? Because we have bounds, but we don't have something, we don't have the distribution. If that's what you have, that would be great, but we don't have that. So so to prove this, do you go through LPP and the boost one process, or do you just work with K7 directly? Right, for which piece of result. That's a very good question. So there is this, so we can't go to the last message because there's a so you can when you when you couple LPP to When you couple NPP to exclusion process, then you use the weights on the bonds, right? But if you have these two, you start with two profiles where you go down and up. So seemingly you use different clocks for different bonds. So this coupling doesn't work well. So we actually have to go back to Qs. We have to go back to Qs and work with that and the result of Pablo and Martin and some other. And some other properties of Q's. I remember well, the two species exclusion special measure originally comes from all angels work. It's some algebra. But is it not a good starting point to do some single part of it? So so he you're saying he had uh algebraic properties then? No, he had he had an algebraic description of it as far as the matrix on that sort. I think it's the red. Yeah, the red and spew maybe. Maybe that's... I feel like these kind of Carlin McGregor thing maybe should... I don't know. I haven't tried and... Okay, that's my intuition. Maybe it doesn't work. I don't know. But there should be some kind of integrity. That would be very nice to have. Because we have this kind of. So I didn't mention it because it's a 30-minute talk. So I didn't mention it because it's a 30 minutes talk, but we have lots of results on infinite geodesics and how we can study convoys in the speed process. And if we want to get things that we want to have more information on those, it would be nice to have or required to have inform deeper information into these st uh um statistics of this model. 