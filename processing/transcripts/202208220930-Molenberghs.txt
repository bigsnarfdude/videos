Thank you very much, Lila. It's a great pleasure to be here. I hope you can see my slides all right. Just let me know. Otherwise, if there is anything that's not working technically, do give me a sign of life. So good morning, good afternoon, and maybe to some good night at this point. I would have loved to be there, of course, like many other people online. But at the same time, it's good that we have. Online, but at the same time, it's good that we have the technology to still meet, even if we cannot always meet in person. It's my second workshop of this nature. My previous one was a good number of years ago in Canada when indeed there was still snow on Tunnel Mountain and on Jasper Lake. I have been working on surrogate markers actually since the 24th of April 1995, which for me started during the Which for me started during a conversation with Mark Boyce, and therefore it's so nice that we are back to back. And in a way, I think it's fair, Mark, to say that our presentations can be seen as a package deal. So we have also been able to interact a little bit on what both of us would say. What I would like to do is give a bit of an overview, also an historic overview where it came from. Historic overview, where it came from, with the definition, and some of the original quantities that Laila was already referring to. I'm afraid I will have to voice a bit of, let's say, counterpoints or BEMOLs to the proportion explained, but that's what we are here for. The 1990s was, of course, the time of HIV-AIDS and CD4, which is a very interesting case study. Which is a very interesting case study and a story in its own right, how it came into being, in many cases, without too much formal validation or evaluation of CD4 as a surrogate. And then we went on to other quantities such as viral load, fire load below detectable limits, etc. And yes, indeed, at this point, there are still huge open questions like what are good surrogates for Like, what are good surrogates for Alzheimer's disease? Lala already referred to that also. And in particular, also, there is a lot of exciting work ongoing around correlates of protection, as they're often called in the world of vaccines. And some of our PhD students are also working on that. So, what I'd like to do is gently build the house. And by saying that, there should be By saying that, there should be a disclaimer. I'm not going to talk about each and everything that has been done. That's not possible within an hour. And of course, the experts who are working on various more recent developments are, of course, present. And we are all looking forward to their presentations. So before going to even the ground floor, the foundation is why do we even look at a context, a concept called A concept called a surrogate. Why do we look at surrogacy? And with Leila, I share the feeling that the words, whatever they mean for Webster or Oxford, they have grown on us as people working in this research field, and we're very comfortable with them indeed. Primary motivation is: yeah, the true endpoint may be rare or distant in time. Think, for example, of cancer, and then the surrogate can solve hopefully. Surrogate can solve hopefully at least one and hopefully even two of those problems by being a bit more frequent. Think of tumor response when the actual survival is a rare and distant event. But there are a number of other motivations, like the true endpoint can be invasive, it can be uncomfortable, it can cost a lot of money, etc. And of course, as time goes by, we all know that randomization is a bit like Randomization is a bit like a bottle of milk. It's good in the beginning, but then it grows bad as time goes by. A lot of other things intervene. There is confounding, there are intermittent events, etc. And by having, let's say, a somewhat earlier surrogate, hopefully you can also solve that. A given surrogate may not solve the entire world, but it may address at least some of those issues. Lile already referred also. Lile already referred also to terminology. I'm not in a position to say this is the terminology, but maybe a bit more modest. That's how I will use the terminology throughout this presentation. The clinical endpoint, but I will call it in line with most of the literature true endpoint, although we can debate that as well, because in certain areas, think of mental health. What is the endpoint for me may not be the endpoint. May not be the endpoint for you, but okay, let's talk about the variable on which we want to have an impact with our therapy. And then a biomarker, and that will come up later in the meeting also in presentations, but also in the panel discussions. A biomarker is essentially, if I summarize it, an indication of something. And that something can be an Be a pathological process, but it can also be a process under normal, healthy circumstances. Of course, we will use surrogate endpoints that are biomarkers, but you can also have surrogate endpoints that are of a different nature. Think, for example, of scales like we would use, instruments, like we would use in quality of life or in mental health and in a variety of Mental health and in a variety of other areas as well. So, the surrogate endpoint is a marker, a variable, let's say, at this point, that is intended to somehow substitute for the clinical endpoint. And what I mean by substitute, it's too early for that. That will come in a few slides from now. Let me give you, I could almost say a toy example or a textbook example because we've also been using it in textbook textbooks. Textbook textbooks, one that has been beaten to death a little bit. Age-related macular degeneration. We have a group of patients that are followed for their degenerating visual acuity. So as time goes by, these patients, typically elderly patients, progressively lose vision, among others, because of elevated intraocular pressure. The trend point here is visual acuity. The trend point here is visual acuity at one year, and you could think of visual acuity as the number of letters you can read correctly on a vision chart with 72 letters. And then, and that's perfectly legal, although there are many other examples, the surrogate would be the same endpoint, but earlier. If you think about what I said earlier, gaining time, looking at something earlier than what the true endpoint prescribes is, of course, also fine. But in many other cases, you will look. But in many other cases, you will look at something different, hopefully, somehow related but different. So, the treatment here is placebo versus interfer on alpha. We have about 200 patients split over 36 centers, but the center effect right now at this point will not immediately be relevant, but it will become relevant as the presentation progresses. So, what you see on the horizontal axis is change in. Horizontal axis is change in visual acuity, change versus baseline, in the first half year and then at the end of the year. Definition. So this I would call the ground floor. The definition as issued by Prentice. And of course, we can say this is a definition, one definition, but it is fair to say it is a seminal definition of the area. The area. Prentice stated: You have a good surrogate when testing the null hypothesis of no treatment effect on the true endpoint and on the surrogate endpoint comes down to the same thing, is what I could call it. It's all nice and well, but of course, we immediately are confronted with a few issues. One of them is: yeah, if you have a trial in which you don't reject an In which you don't reject the null hypothesis on the surrogate, and neither do you on the true endpoint, is that showing surrogacy, or is it simply lack of power? You feel that very likely it's or in many cases it will be the latter. Also, it's not because it works in one trial, and this will be a recurring theme in my presentation, it's not because it works in one trial, even if they're both significant, that it would work in other trials. Would work in others. So, the definition, while very intuitive, immediately poses a number of problems. You will see for a while that whatever we propose, we will immediately give counter arguments. Not to be negative, but that's part of the discussion, of course. Prentice himself, in the paper in 1989, came up with an evaluation framework, which is based on Which is based on three classical linear models, except for one thing. So it's a three-body problem. You have the surrogate S, you have the true endpoint T, and you have the treatment allocation 0, 1 for the sake of this presentation, let's say placebo or standard versus the experimental treatment Z. The index J is used to indicate patients. Why J and not I? Well, I could say why not. And not i, well, I could say why not, but I will save the index i until a few slides down the road. The one little thing that from the 1989 perspective was a little different is the two regressions of S and T respectively on the surrogate are correlated with each other. And that variance-covariance matrix sigma is, of course, a quantity that we are going to use in what. Are going to use in what follows. So these are three models and they lead to three of the four apprentice criteria, the preamble criteria, I would like to call it, because the apprentice criterion is number four, to which I will come in a moment. And the idea is that you'd be happy if all three are significant, happier if the significance is stronger. Is stronger. So here we go. Apprentice criteria number one, three, and four. If you look at the top of the slides, don't look at the bottom yet. That's coming in a moment. And now that you've looked at the bottom, let's look at the top again. So in a linear model, and by the way, Leila already hinted on that. We are by no means restricted to linear models. I will be using those when I can, knowing that there is a lot of Knowing that there is a lot of work in binary data with logistic regression, survival, repeated measures, etc. To some of those, I will refer, but not to all of them. But when I can, to illustrate a concept, I will use simple linear models. So you have the three treatment effects, which essentially are testing whether the distribution given for one and two at least, the treatment allocation is different than the unadjusted distribution. Unadjusted distribution and the same for the direct relationship between T and S. Now, 4 refers to the indirect relationship, or rather the relationship of both the treatment and the circuit on the true endpoint. Let me rephrase that because this is a pivotal quantity on this first floor of the building. So I'm counting the ground floor as zero. Forgive my European bias. Forgive my European bias. And now we're on the first floor: the apprentice evaluation criteria, which are of a hypothesis testing nature, obviously. So back to apprentice criterion four, the apprentice criterion, it says that ideally after correcting for the surrogate, there is no effect of the treatment left hand. That's what you see here. Left hand, that's what you see here. So, unlike one, two, and three, it's not a classical one, it's basically an hypothesis test of an equivalence nature, which poses the exact same problems that we were already referring to as with the definition. And that's where Prentice comes on the table. And as I take it, Lila's quantity, the proportion explained or the proportion of treatment effect explained. Treatment effect explained, proportion of treatment explained, PTE or PE. I will use the abbreviation PE, but it's exactly the same quantity. So what you do is you relate with each other the two treatment effects. Let me call it the unadjusted and the adjusted treatment effect. And here is the reasoning behind the quantity. If the two treatment effects are the same, that means basically whether you Basically, whether you adjust the treatment effect for the surrogate yes or no, there's no difference. Well, then the proportion of treatment effect explained will be equal to zero. If, on the other hand, everything is explained away by the surrogate, beta s would be equal to zero, and then the proportion explained would simply be one. The rationale behind the quantity, and that's how Friedman, Grobart, and Shatskin came up with it. Grobart and Shatskin came up with it. There is an epidemiological background in these colleagues. It is to do, of course, with the attributable fraction in epidemiology. The thing is that there are major issues with definition. I would call the quantity ill-defined for reasons that I will explain in a moment. I'm not saying that the concept is not right or not good or not valid. I think Not good or not valid. I think the idea behind Prentice Criterion 4 is extremely important and is here to stay, but I just don't think that the proportion explained is the right way to quantify it. Mark and I, at the very end of the previous century, 1998, have a paper where two other quantities are proposed, the relative effect and the adjusted association. And let's call that the second floor of the building right now. Floor of the building right now, still estimating, but estimating two different quantities. The relative effect is a little bit like a conversion factor. If I have the treatment effect on the true endpoint and I have the treatment effect on the surrogate endpoint, beta and alpha respectively, and now it comes, if that relationship would be relatively constant, essentially you can think of a regression through the origin, well, then of course in a new trial, Then, of course, in a new trial, you can suffice by evaluating the treatment effect on the surrogate endpoint and through the relative effect, convert it to the treatment effect on the true endpoint. Of course, just like with everything that we have discussed so far, if you're doing this with a single trial, it's a long shot that the relationship is going to be linear and through the origin, of course. Through the origin, of course. Yes, if you convert currencies in, for example, Euros to one of the old currencies in Europe, yeah, then that's the relationship indeed. If you convert centimeters to inches, that's the relationship indeed. But of course, here there is statistical uncertainty and we are not sure about the exact relation, the regression relationship, as it were, between these two quantities. So, even though we propose So, even though we proposed it, I'm already putting some question marks next to it. The adjusted association says, okay, for a given patient, and now this is important, left relative effect in blue is trial level. The green quantity, adjusted association, is patient level. Given the treatment effect, what is the association between the surrogate and the true endpoint? In other words, in each of the two treatment arms, is Of the two treatment arms, is there a predictive power of the surrogates over the true endpoint? Okay, this is for the RMD trial, a quick evaluation of these quantities. And that's done a very long time ago. You would see that it's game over from the beginning because you have a non-significant first and a non-significant second criterion. The proportion explained would be estimated to be. Would be estimated to be 0.65. Now I will refrain from saying that 65% and blah, blah, blah. I said it anyway, but you will see why I refrain from that. You do get a wide confidence interval, but I don't even think that this is the most important problem. It's a problem, of course, especially because the entire unit interval is in it, and you might already think we have to constrain it to the unit interval. But I think there is more fundamental problems with it. Think there is more fundamental problems with actually the quantity and therefore the point estimate rather than the confidence interval. Relative effect is not great neither, but also there you heard the criticism. I think we should look at that more in a replicated context where you have more than one replicate of the treatment effect. So, in a meta-analysis, the adjusted association, though, is relatively high and it's also And it's also fairly precisely estimated. 0.75, just remember the number. Talking about the proportion explained, and then I will move on. In linear models, this is just for linear models because there the algebra, as we know, is always a little easier. The relative effect and the adjusted association can be expressed very easily in terms of regression coefficients and variance components, but that's what it is now. The proportion explained is. Now, the proportion explained is actually a combination of the two, but it does mean that, and we are used to that from conditional relationships in multivariate normal models, etc. They do combine variance components and regression coefficients. But it does mean, actually, that first and foremost, a quantity at trial level and a quantity at individual level are amalgamated. Are amalgamated, but more importantly, there is a variance ratio. And in some papers, we have delved into that more deeply. There is a variance ratio, which is actually immaterial, but can actually, if you change scales, make the proportion explain go all over the place. So it's not guaranteed to lie within the unit interval in this particular setting, which is different for binary data, admittedly. But that, of course, I think is a bit of an issue. But I will not put a quantity. An issue, but I will not put a quantity to the side. Maybe the exact parametric form, yes. I would rather like to keep the concept behind the fourth criterion of apprentice and then return to it as we proceed. So in summary, the two level two quantities, so the quantities at the second floor of the building, the relative effect and the adjusted association, they have their Association. They have their interest, but you heard my criticism towards the relative effect a moment ago. But what I would like to retain from them is: hey, we're separating the trial level and the individual level. How predictive is the treatment effect? One. And how predictive is the outcome at the level of an individual patient? It means that we are at two levels of the hierarchy, a trial as a whole and a patient within the trial. Within the trial. And by the way, as a side remark, if you can do something for two levels, you can do it for more levels. Let's say center within trial, trial within meta-analysis, and then at the deepest level, patience and perhaps even repeated measures within patients. All of that is possible. A lot of it has been done, but let's keep it simple for now. So I'm moving towards the next floor in the building, the third floor, where we are generally going to Where we are generally going to use multiple trials. In some cases, you will see that we illustrate things with centers within trials. At some point in time, that was a poor man's choice. Still to this day, there is a big need for the availability of multiple trials in the same drug class, so to speak. But things have improved relative to what they were, for example, in the 90s and the nilis. And the nilis. But now we will extend both quantities, the relative effect and the adjusted association, to more symmetric quantities that live at the level of a meta-analysis. The trial-level surrogacy basically tries to answer the question, how close is the relationship between the two treatment effects? And at the individual level, how close is the relationship between the two endpoints at the level of an individual patient? So, L. So, Albert et al. in Statistics in Medicine centuries ago, well, 25 years ago, more or less, basically referred to one, issues surrounding the PE, and then second, a meta-analysis, and third, and that's something we should never forget, a good reflection on the biological mechanisms may be useful. We can do a lot of statistics, of course. We can be very clever. Be very clever, but factoring into the extent possible when available biological background information, of course, is never a bad idea. Okay, my index I is appearing right now. So we have a surrogate and a true endpoint still, but for patient I, sorry, for patient J, I should say, still patient J within trial I, which means that the Which means that the parameters become mixed effects, and you can split them into fixed effects and random effects. The error structure is still the same, at least for the measurement error terms epsilon, just like before. But the trial-specific effect, as I said, are split into fixed and random, and they're also given parameters, variance components, in other words, the D matrix. If we still apply this to the If we still apply this to the ARMD trial, remember it's not a meta-analysis, it's a multi-center trial. We can look at the level of the center and see how the treatment effect at center level relates to the two treatment effects, I should say. So six months versus 12 months. And we can, and at the third floor of the building, of course, we now have an additional tool. Have an additional tool. We had a definition which was qualitative. We had the criteria of apprentice which are of a hypothesis testing nature. We went to the quantities, the stream, the string of quantities initiated by Friedman and colleagues, which are of an estimation nature. And now we go one step further and try to predict how well we can predict the treatment effect in a new trial, trial zero, given the validation. Given the validation or the evaluation exercise. And that, of course, in its simplest form here can be summarized by an R-square measure, 0.7 in this case. Of course, and that's the beauty at the individual level, we can do exactly the same. So the adjusted association that we had a moment ago is now called the individual level surrogacy. The only difference is we calculate that quantity within a high. That quantity within a hierarchical model rather than a pair of linear models. But the concept is the same, and the quantity is also not very different indeed. 0.75, whereas the individual level R is 0.7, very close. The R square is 0.48. Okay, just to open it up a little bit to other areas, we have in the first column here everything that we have done. Everything that we have done or re-estimated rather for the age-related macular degeneration, so the apprentice criteria, what we call the single-unit validation measure, PE relative effect-adjusted association, and the trial-level quantities. And then to other areas where we use survival data, time to event data, advanced ovarian and colorectal cancer, second and third column respectively, where progression-free survival, which means time to progression or death, is used. Progression or death is used as a surrogate for overall survival. You see, by the way, that the proportion explained in the Ovarian case is 1.34, and in all cases, the confidence intervals are extremely wide. The relative effect in terms of confidence interval is not doing great, to say the least, but the adjusted association is nice. But if you look at the bottom, that's what I really want to do: the R-squares, we see that the trial. The R-squares, we see that the trial-level cirrogacy is very high in ovarian cancer, somewhat lower in colorectal cancer. Nevertheless, that's the area in which it has been used more than in ovarian cancer. And the reason is relatively simple. Something which is not part of the framework here, call it a limitation, is what do we actually gain? You can have a very high R-square at the trial level, but if the true and the circuit endpoint happen, True and the surrogate endpoint happen very close to each other in time. What are you going to gain? And that's a bit a problem in the ovarian cancer case, unlike in the colorectal case, where in spite of a somewhat lower R-square, you can gain a considerable amount of time. Mental health, that's so still the same idea of reviewing all of the quantities, but now in different trials. The whole slide is devoted to schizophrenia, where the surrogate is the pants. The surrogate is the punce, the positive and negative symptom score, and clinicians' global impression, which is a seven-point ordinal scale, is the true endpoint. There is apprentice criteria. You will see that in one schizophrenia study, that's the left and the middle column. And the right study, it's very different. Of course, study one, as you can see, is split either in. As you can see, it is split either in 138 investigators or in 29 principal investigators, which doesn't make a difference for the single trial evaluation. But if you use, and the same is true for the single unit validation measures, and yes, we already had a moment ago a proportion explained of 1.3. We now have a negative one with an infinitely large confidence interval. So there you go. If you think that some of the criticism for the Of the criticism for the proportion explained is actually purely mathematical, technical, ivory tower kind of stuff. Well, in these examples, I think we add to the issues, just like with the relative effect, as we said earlier. If you look at the R-squares, trial and individual respectively, what you actually see is a relatively coherent picture, of course, not to the decimal place, but we do get. But we do get a decent R-squared trial, admittedly a bit higher in study two than in study one. But in study one, it doesn't really matter how we split into units. Okay, if I go on and say, what now if we would consider not a single endpoint, not a single surrogate and a single true endpoint, but we make it longitudinal. But we make it longitudinal profiles. We can follow a longitudinal profile, and that may be our true endpoint, but we can just as well have another profile, which is a longitudinal surrogate. Well, of course, and at this point, forgive me for not going into all details, we can just extend the framework and you can make the models as simple and as complex as possible. Of course, within the meta-analytic framework, Within the meta-analytic framework, the concern still is: am I able to come up with a single quantity, an R-square trial, an R-square individual? So, if you make the model, let's say, extremely complex, very non-linear, etc., that may be a great modeling exercise, but you may shoot yourself in the foot, of course, in that way. The thing is that now the treatment effect may indeed not necessarily be a single quantity. You may have an entire set of treatment effects, but in particular, Treatment effects, but in particular, even if you still have a single treatment effect, you have a variance-covariance matrix, which is not two by two, one for T and one for S. You have a T block, you have an S block, and you have a mixed T and S block, like in good old-fashioned multivariate statistics. And together with Ariel Alonso, we developed a number of quantities that really refer to the To the classical multivariate or root statistics, as we sometimes call them. There is the variance reduction factor. What part of the variance is explained away by the circuit? And I think, in a way, you can consider these quantities that have a similar spirit like the proportion explained, even though they take a very different form and they're in a different context, even. But how much remains? Even, but how much remains after explaining away through the surrogate? That is the overarching concept that is also present in the apprentice's fourth criteria. Okay, there's a number of quantities, the variance reduction factor, there's a canonical correlation-based quantity, there is a Wilkes lambda or R square lambda quantity, etc. It's, for those people familiar with multivariate methods, it's all variations to the same theme of quantum. Variations to the same theme, of course. And then I think a quite relevant step is to the likelihood reduction factor because, here in the likelihood reduction factor, we free ourselves from multivariate normality. We just take multivariateness if needed. So this is something you can apply also when you have binary data, time to event data, count data, etc. You basically look at the reduction of the variance. At the reduction not of the variance, but of the likelihood rate, the deviance, as it were. And again, I would like to stick to the general concepts and try to indicate that we use in a somewhat more complex context, still the same ideas. How much of the variability is how much of the information per? Let me say first, how much of the variability is explained away by the surrogate? How much of the variability. Away by the surrogate, how much of the variability is captured by the surrogate, and then I think it's time to move to another floor of the building right now. Information theory came in in an effort to basically unify previous proposals. Because the thing is, and that's something that I haven't gone into too much detail yet, Mark will refer to it to some extent. Mark will refer to it to some extent. Yeah, the meta-analytic framework, if you apply it to binary data, to time-to-event data, to data of a different nature, where you have, for example, a binary surrogate or a categorical surrogate for a time-to-event to endpoint, etc. In many proposals, including the ones that we put forward, the trial-level surrogacy is still an R-square, but the individual-level surrogacy takes all sorts of forms. Takes all sorts of forms. It can be an ots ratio, it can be candlestow, any other quantity that appears in a copula, for example. So that's not something that our clinicians really like if it becomes too heterogeneous, as it were. So information theory was called in to help a bit. And it starts not from variance to begin with or variability or information, but Or variability or information, but entropy. The entropy is given there is the expectation of minus the logarithm of the density we are studying. Bear with me for a second. I will try to give it a bit more flesh and bones in a moment. The conditional version, of course, is the same thing in a conditional distribution. And you can already see where that's aiming at. The unconditional version is for the true endpoint. Is for the true endpoint. The conditional version is for the true endpoint given the surrogate. The amount of entropy explained away by a quantity, and that quantity will be the surrogate, of course. And I basically said it, and that's what you see here. The EP is not a PE, it's not a proportion explained, it's the power entropy or the entropy power, which you see in the middle of the slide. You see in the middle of the slide. The definition is such, of course, that for the normal density, sigma squared pops up. It's as simple as that, but it can also be applied to other distributions without any problem. And you might say, hey, I recognize the form of the proportion explained. What can I say? It's indeed true. Is that that same quantity indeed? You can do that for a single trial, but if you have multiple trials, you can basically. You can basically calculate the quantity for every trial separately and then, with a convex combination, come up with an answer for the study, for the meta-analysis as a whole. The nice thing is that some of the previous quantities, and I'm referring to anything from the R squares over the variance reduction factor, the likelihood reduction factor, the variance reduction factor, the canonical colour. The canonical correlation quantity, etc., they all nicely telescope into each other in simple settings. But progressively, later quantities have better properties than earlier quantities, which is what you would expect, of course, if we have been in our right mind when proposing those. The most important thing is that the likelihood reduction factor, but especially also the information theoretic approach. The information theoretic approach is applicable to Gaussian and non-Gaussian settings alike. So you get quantities that, at the end of the day, even though you have to take a deep breath and go through some more complex theory, at the end of the day, you have quantities that are uniform and therefore it's simpler. It's a simpler landscape than what we had before. And the likelihood reduction factor, by the way, does converge to the Does converge to the information theory approach when the number of subjects per trial goes to infinity. There is, of course, a relationship with apprentices criteria and with the proportion explained. To summarize what you find on this slide, let me say that the information that you get from the true endpoint itself is, of course, always. Is of course always larger than the information that you get from the true endpoint through the cert. That's a way to read it. But very importantly, as I already alluded to, by the way, the proportion explained in its form and the information theory quantity right now are identical, except I think that the later quantity is more principled, it's not scale variant, etc. Variant, etc. So it's in that sense, I think, a bit nicer, but the spirit is the same. How much of the information filters through the information on the treatment effect on the true endpoint filters through the treatment effect on the surrogate. And then an important quantity also to which Mark will also return is Fano's inequality, because it's interesting, and I think we've all It's interesting, and I think we've all done that. We often zoom in on a surrogate. We have a true endpoint, and then is this a good surrogate or is another surrogate better, etc. Taking the true endpoint for granted. What else are we going to do? Well, wait a minute. It's perfectly possible that the true endpoint is so wild that it doesn't allow for a good surrogate. Let me give the following example. You have CD4, and that's your true endpoint. Just imagine, that's your true endpoint. That's your true endpoint. And can I find question mark a good surrogate for CD4? Well, the answer is that may be difficult because CD4 is a very variable quantity. It naturally fluctuates a lot. If somebody coughs in your face, it may already be visible in the number of T4 cells, one. But also, the assays to determine it, even though they have improved, would still come with quite a bit of variation. Come with quite a bit of variability. So it basically means that the true endpoint itself might be scrutinized for good measure before embarking on a surrogate. So it's always a good debate as is this now a good, true endpoint? And if we finally agree on that, then we can look for surrogates in that order, maybe. That's what this slide wanted to say. But good. Let me illustrate the information theory approach for a moment. For a moment, right now. So, I take the schizophrenia trials that I was already alluding to. We have for the continuous outcomes here. So, the continuous outcomes is I'm treating the outcomes, CGI and PANS, as continuous, but I also can dichotomize them, and that's what's done next. The variance reduction factor 0.39, the R-square trial, which is easier. Square trial, which is easier to interpret because it's zero to one scale, 0.85. But if I look at the binary outcome, and this is what I said earlier, if you use the basic meta-analytic framework, but now for binary data, which I did not elaborate on, if you think, hey, I've forgotten what he said about that, not much other than that we would be using not a multivariate normal, which I have been using extensively. Which I have been using extensively, but rather a copula, for example, or any other model, you would get, again, at the trial level, still R squares, and it seems to agree all a little bit, 0.5, say. But at the individual level, you have in the probit model, you have a correlation parameter. But in what we call the Placet-Dale model or the Odds ratio model, you have guess what? An Odds ratio. Model: You have guess what an odds ratio, and yeah, we know the familiar question: when is an odds ratio large, of course, because it can go to infinity. The R square H though, which is the information theory approach, does provide us with a number between zero and one. But hey, there is a little problem in the binary case: that is, it has a ceiling below one. And therefore, the R square h max is a slightly modified quantity so that it's risky. So that it's rescaled between zero and one, and then we can make more sense of the number 0.40 than the number 0.27, where we don't even know what the ceiling is going to be indeed. Good. Same thing for the age-related macular degeneration, where both outcomes are now binary. And that means we are looking not at the number of letters you can read. Read, you can read on a vision chart, which is, of course, continuous or quasi-continuous, I should say. But we look at numbers of lines of vision lost. Do I lose two lines of vision at six months or three lines of vision at 12 months? Yes or no? We have the R-square trial and we have the R square H max quantities, which are indeed of an R square nature. Okay, advanced colour. Okay, advanced colorectal cancer. I talked about continuous data at length, binary data briefly now. Let's move to time to event data, where your two models would be Cox models, for example, your base model with the treatment and your extended model with treatment and also the circuit included. I'm comparing the results of two data sets here. And again, at trial level, you can work with trial level, you can work with the R-square. You will always work with the R-square trial, no matter what models you use, separate models that you combine by a copula, Clayton copula, Hugard copula for example. But at the individual level, and I'm not showing those details, you would get the typical association parameter of the Clayton copula or another parameter from the Hugard copula or at best. At best, a transformation towards candles tau. But with the information theory approach, you do get your R-square measure, which is still interpretable the way it was before, and you do get fairly good results for that matter. Okay, let me also comment on the concept of prediction in a new trial. So we have a new trial, and in that new trial, we have the surrogate. New trial: We have the surrogate. And now the question is: based on the analysis of the surrogate, based on the evaluation exercise in the meta-analysis, what can I now say about how well can I predict the variability in or how well can I predict the treatment effect in a new trial? So in other words, is the variability hampering me to some extent? And Mark Boyce and Thomas Berzikovsky have a nice paper on that. Have a nice paper on that. Well, the prediction variance depends on three sources. Briefly, the variability in the new trial. It's a finite trial, so there's variability. The variability in the evaluation exercise. And pretty much, let me call it this way, Fano's inequality. It's not exactly Fano's inequality, but it basically says how large is the R-squared trial. So I can have a huge. So, I can have a huge validation exercise, a huge new trial. But if the R-squared trial will be low, my prediction would still be lousy. And of course, that's what it should be. Otherwise, nature would not be right. So, indeed, that's what I expand on in this slide. I'll be brief about it. But even if that's what I just said, even if you exclude variability in the trials, the relationship between The relationship between the two endpoints. You can have a billion patients altogether. If that relationship between the circuit and the true endpoint is weak, the prediction will remain a difficult thing to do. And why did I say Fanos inequality? Because actually, the variability in the true endpoint treatment effect will keep playing a role as well. Okay, so I'll skip over that because it's essentially what I've said already. What I've said already. And then to conclude, in the last few minutes, let me say a few words about more recent developments where we use potential outcomes, a word that I haven't used. There are a few words that I haven't used. I haven't said principle stratification. I have now. I haven't talked about causal inferences. And I could say, well, people will talk about it based on a lot of expertise and at length. A lot of expertise and at length in days to come and later today in days to come, and that's absolutely true. But let me say a few words about it: about the way we have been looking at it. So with potential outcomes, you of course say there is a true endpoint for a given patient under placebo and under active. So the individual causal effect is the difference between the two. We look at its expectation and the question is, can we? And the question is: can we come up with a surrogate that predicts this sufficiently well? It's the same idea of surrogacy, but now embedded in a first attempt to embed it in a causal framework. So we have three outcomes. And for those people who say, hey, it's a bit asymmetric, isn't it? Because you have two T's and only one S. I agree with you. It's asymmetric. Nevertheless, I ask you to bear with me for a few slides. Ask you to bear with me for a few slides and then I'll symmetrize, if that would be a word. So, if they would be normal, then of course the difference and the surrogate would also be normal. And from there, you can derive the predictive causal association, which is how well does that difference and the surrogate relate with each other. We can come up with a measure for prediction accuracy, which looks terribly much. Which looks terribly much like Fanos inequality, indeed. Of course, a fundamental issue in causal inference, and actually everywhere else, also where you don't seem to see it, it's maybe swept under the carpet. That's the lack of identifiability because you're using two variables at the same time, only one of which at most is observed. If you have no missing data, half of the data. No missing data, half of the data is missing, so to speak. You can do several things. You can come up with minimal, plausible, statistically plausible, even more so, clinically plausible, identifying assumptions. Perfectly fine to do so. Be my guest, I would say. But what you can also say is, let's not do that and let's just do a sensitivity analysis. I estimate what is estimable, and for around the rest, I perform a sensitivity. Around the rest, I perform a sensitivity analysis, leaving the entire range of correlations free. That's one solution. Another solution is limited to what I consider a plausible or what the clinicians and we together consider a plausible range. Yet another solution would be to basically put a prior on it and approach it in a Bayesian fashion, etc. Do I choose between those options? I choose between those options? No, I'm just laying them on the table for further discussion. Of course, we can go one step further and symmetrize, as I said, in an attempt also to move to the end of my presentation, where I have the two T's, only one of them is observed, two S's, only one of them is observed. And of course, it's evident what we're going to do right now. Look at the two differences, the relationship between them, and derive a quantity from there that captures. From there, that captures the so-called individual causal association. More stuff is not identifiable. So, what we're going to have is a bigger sensitivity analysis. And I can assure you that those people who have been working on that, there are several, including Fenny Ong, one of our colleagues at Hasselt University in Belgium, who is in the audience, and also Win Van Derelst and others and Ariel Alonso. It can be It can basically be a time-consuming exercise and one where some numerical techniques and tips and tricks have to be used to simplify matters. So essentially what we're doing here, in a way like always, is use as much information as possible from the design, i.e. randomization, what comes from the data we will use, and the rest we either do, we either identify. We either identify via assumptions, and if we don't do that or do that only partially, we do a sensitivity analysis. Of course, it's perfectly possible to also embed this in a meta-analytic framework. A lame way to formulate it is you bring in the two indices. There is the I again, it's not the J for patient only, it's also the I for trials, and you can come up with a meta-analytic ICA. Analytic ICA or ICA or Individual Causal Association. So, for example, here you do have an example where we have at the top the two, for example, for ARMD, think about that, we have the classical evaluation exercises. And at the bottom, you see the result on the individual causal association from a sensitive Association from a sensitivity analysis. A negative view is anything between zero and one goes. A positive view, or you can say almost like going to our sub-Atian view is, yeah, we do get useful information here on what is a plausible value for the ICA. And then I would conclude, there is some evidence that we have a good surrogate here because the distribution points to the right in this case. Points to the right in this case, and it doesn't always in other examples. There are also examples where you would have to conclude that there is very little to conclude in terms of surrogacy, but such is life and that was not different with previous approaches. So to conclude, we could ask the questions: are surrogate endpoints useful in practice? The answer is not yes or no. The answer for us is the assignment. No, the answer for us is the assignment is yeah, but we have to come up with a clever methodology. And fortunately, there are a number of schools of thought available around this workshop's table, fortunately also. And yeah, every approach has their advantages and disadvantages, philosophically, statistically, computationally, interpretation-wise, etc. Not to forget clinically, but by interpretation. But by internal discussion on the one hand and by putting the thing to the test, because the proof of the pudding lies in the eating, we can make progress, correction, we have made tremendous progress, I think, over the last, let me count, 33 years, ever since the definition of apprentice. So methodological conclusions. I still think that overall having Overall, having a meta-analytic setting. And I don't mean to say our meta-analytic approach. I think that would be arrogant. That's not what I want to say. But having replication at the level of trial, replication at the level in which you're interested, of course, also at the individual level, is beneficial because there is no such thing as a surrogate for having multiple trials if you have only one trial. Have only one trial. There is always going to be assumptions, not at the individual level. And there are cases where the individual level is all that you care for, but in many other cases, you do care also or sometimes solely for the trial level. You need a good model strategy, and depending on the way you go, you go for a joint model for the surrogate and the true endpoint. Surrogates, perhaps plural, true endpoint. Perhaps plural, through endpoints, perhaps plural. Or, and I forget to emphasize that, if you go to the information theory approach and for some of the approaches later, you don't need a joint model, but you need a pair of models, a marginal and a conditional model. And Laila was referring to that in her introduction also. Sometimes they're compatible with each other, sometimes they are not. And sometimes we don't care about that, but if we do care about that, there's also ways of. Do you care about that? There is also ways of overcoming that indeed. And of course, it comes as a benefit. It's not always possible, but it comes as a benefit if we can come up with a method that produces similar or the same quantities over various data types, because we will render interpretation easier and we will therefore do good service to our clinical colleagues. So, on that note, I would like to thank you for. I would like to thank you for your attention, also, for the honor of being able to speak early in the workshop. And from now on, I can look forward to everything else that's going to come. So, thank you. Okay, thank you. There was applause here. I'm not sure if you could hear it. Okay, we do have a bigger open. Uh, we do have a bigger open discussion after the next two talks, but um, and I want to have our break at 10:30. But we have a few minutes for a few quick questions if there are any. I do want to say thank you so much for that talk. That was great, a really great overview and introduction. I mean, I think more than an introduction, but um, it was very helpful for me too to see the connections between all these different approaches. And if you Approaches, and if you weren't familiar with all those approaches, hopefully that convinced you of the importance of this workshop and what I said at the beginning: that there's so many different tools available, and this is going to be an exciting week. So, thank you so much for that, Keert. Any questions in the room first? Yeah, I'll just take more time at any questions. Mike Yellow, University of Michigan. So, this is more of a comment than a question, but I think you're absolutely 100% on the mark on this idea of the real goal of the inference has to be how is this marker going to work in the next trial? I think, and so I don't know if the multi-level modeling approach definitely fits well in that, but it may not be the only. Fits well in that, but it may not be the only way to think about it. But I do think that, and along with that comes the idea of what's the population, what are the universe of trials we're going to think about, what are the universe of treatments. So that's an area I don't think has been really well covered. And some of that is maybe not so deeply technical as it is more philosophical. But I'll just stop there. Yeah, thanks for that. If I can say something, it's If I can say something, it's absolutely true. And there are indeed a few aspects. There's what we sometimes call the class question. Suppose that if I jump ahead and even if we have been able to, let's say, evaluate it and make sure that in the next trial it will work, what is going to happen if we drastically change therapy? If there is a major breakthrough, this may send us back to the drawing board. It is what it is, but we need to think about that. Think about that. So, a bit of a negative comment almost, but it's not meant that way. Is if you have, let's say, the next trial around the corner of the same nature, we've had hundreds of those. And I think in oncology, Mark can comment on that very well. There were times where that was the case. Yeah, then the time-honored surrogate can basically help you in your next time-honored seasoned trial. But if suddenly there is a major breakthrough, you may have to. You may have to pause and say, Hey, here we better wait until we have the true endpoint and we have done the evaluation of the circuit again, or at least update our information. Why not on that? So, yeah, I think alongside the statistical discussions that we have among us and for which this workshop is intended, it's also important to have clinical discussions among the clinicians, and of course, most importantly, between us and them. Okay. Oh, yes, Larry. Thank you for that really nice talk. I just had a question regarding trial design. So there are many complex trials, adaptive trials, platform trials, etc. Do you find it challenging to convince a trial designer to use your surrogate method, let's say, applied to a very complex trial design where they, you know. Design where they, you know, they've already in their mind decided the type of trial they want to design. And then you come with them with all of this plethora of methods, and they say, Well, how can I fit this within what I already want to do? That's a good question. And I think it's many people may have various opinions on that. But if let's say it's us as academics talking to them as, let's say, trial listing companies, etc. Companies, et cetera. I think the third kit on the block may help here sometimes, and that's the regulator. Because, of course, it's actually a four-way conversation. I think there are four parties involved here. The researchers, academic or non-academic, it doesn't matter. Industry, the regulators, and not to forget the patients also. I think that's something that I would like to really throw on the table. It's very important. Table, it's very important to also think about patients, have discussions with patients, advocates, etc. Well, sometimes regulators pose requirements and then, of course, sponsors will follow it a bit more easily than if we come up with an idea that might be interesting from an academic perspective, but it's not necessarily directly of interest to them. Frankly, why would they do that? So, I think if we basically can convince them that this will help them in. Convince them that this will help them in their next trials, even though it may now take a bit more time and cost a little bit more money. That may work. Would have been more difficult, I think, in 1995 than it is in 2022 in the meantime, because progress has been made. And I'm sure Mark will talk about that. And if not, I will ask him to do so. But of course, going back to the class question a moment ago also is that is sharing of data across pharma, across industry. Pharma, across industry. That is a very important thing, of course, because it's to everybody's benefit that data are shared across sponsors, across competitors, let's face it. And everybody would agree, but when push comes to shove, everybody would basically take a step back and say, I don't know yet. But also, their progress has been made, but more progress is needed, I think. Great, thank you. Let's take a break and we'll come back at 11. Thank you.