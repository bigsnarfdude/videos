All right, so thank you very much for the invitation. I am really happy to be here. I am really happy that there's been so many people who decided, you know, despite all the difficulties to actually make it to Canada and then participate in this workshop. It's been wonderful. It's been really interesting. I'm looking forward to the remaining talks. So, what I would like to do, I would like to tell you a little bit about our experience and the kind of work. Our experience and the kind of work that we've been doing with a couple of my friends and colleagues at Ohio State related to the COVID pandemic. We called this general methodology DSA or the dynamic survival analyses, but it basically can, it's been known in the literature under various different names. So it's just a sort of working name for the set of methods and concepts that are somewhat related to survival and out. Survival analog. That technology that I will be looking at existed. So it's not like we completely developed it, but it really hasn't been used in this way in the past. So that's maybe an aspect of novelty that I would like to emphasize. So this is one of my really favorite quotes related to applied math. Related to applied math. And, you know, I am an applied mathematician. I've started as an applied mathematician. My interest in applications comes through this suspicion that there is some really interesting and beautiful mathematics undermining the Neva model. So I totally agree with what Juan Carr√© said here. So from that perspective, you know, if you look at me, don't judge me by my data, because that's typically not the type of The type of starting point for many of these considerations that I have. So, this is an Ebola epidemic, 1820. Let me see if I will be able to run a video. I think I can. So, okay, I can run it. On the screen, but not on the one that I'm sharing. I didn't think it would. Why are you sharing the minutes on? I think I'm sharing the screen. Or maybe not. Okay. You're sharing. Let me see if I can share something else. It took us quite a while to actually make this movie. This is the movie of a 2018-2020 Ebola epidemic in DRC as it was captured by these various efforts of the Various efforts of the College of Public Health of Kinshasa University. And what I am trying to illustrate here is that there is quite a bit of spatial structure to this kind of epidemic. Basically, it started at some population center, then it died down in one place, it moved on to the north, it again created a second wave. Again, it created a second wave, and then this wave came back to the population center. So, the idea is that, generally speaking, this kind of dynamics can be quite complicated for a variety of reasons. There's a number of different type of spatial and temporal patterns that, in order for us to be able to capture correctly, we really have to have a very detailed model. So that's kind of related to what I was saying yesterday about the bias versus variance notions. Bias versus variance notion. So, here I would have to have a very, very detailed model in order to be able to capture the spatial structure. And it's certainly possible, but under many circumstances, the type of information that we're collecting is not really always amenable to that kind of model. So we would have to do something that is a simplification or maybe some kind of compromise. So, we did some of this in this effort of Ohio State and then some of my colleagues related to modeling COVID. So, even though we didn't have data that was spatial data across the state, we were still asked to provide some kind of initial Estimates and predictions of what will happen in Ohio, particularly in the very early stages of the epidemic. And the reason was very simple. There were people responsible for preparedness of the Ohio Hospitals Network, and they approached Ohio State and asked them, Well, how many beds do we need? So that kind of led to these questions. So, how do we actually calculate the number of beds that are needed given that? Given that the capacity of the hospitals and the trends of the epidemic, and what do we know about how to deal with this kind of problem? So, this basically led to this basically six months or so activities that were that resulted in some modeling. And then, most of it was done before the actual vaccination schedules were rolled out. So, after vaccination. Were rolled out. So after vaccinations, some of that modeling was no longer relevant because the parameters changed, and we were not at this point necessarily anymore needed in order to be able to make some viable commitments. So here is a COVID epidemic in Ohio over time. What I want to bring to your attention is that there's actual Is actually interesting structure, secondary structure in this curve related to the variabilities over time. So as you see, generally speaking, if you had the smaller incidents, this is the daily incident rate in Ohio over the entire duration of the epidemic. And the idea here is that we have these different patterns: you have peaks and valleys. Patterns, you have peaks and valleys as you would expect. But then you also have these notions of high and low variability. So basically, as you're reaching the peak, it seems like the variability of these daily incidents is changing and the amplitude are different. It has to do somewhat with the weekly cycles and some other structure, but it also has to do with the fact that it appears that some kind of Poisson process is at work, right? Coisson process is at work. So basically, with the higher intensity of your counts, you will also have higher variability. So, this is at least one example when you could try to use these secondary structures. So, you could actually use the fact that the variability of your process should increase as you're approaching the peak. You could try to use that very variability to identify where you are in terms of peaking or not peaking. So, that's just an example of. An example of a quick statistical test that one could run formally in order to look for the location of the peak, which is based on the notion, very simple one that says that as you're getting approaching the peak of the epidemic, your variability will. So we needed to develop something in order to be able to handle both the types of peak-related variabilities or temporal patterns, as well as to some extent. Patterns, as well as to some extent, spatial patterns. What I have shown you over here earlier is an example of this kind of temporal spatial pattern, which is projected down on some kind of incidence rib. So I'm basically obtaining these kinds of curves, which are basically a result of some type of spatial-temporal process that I am not observing, but I am observing its projections and realizations when I'm aggregating. And realizations when I'm aggregating the incident. So then the idea is: can we build some kind of mathematically flexible tool that would allow us to do these kinds of models in fairly general settings? So the answer is yes. And there is a fairly principled way to do it. I will not discuss it because I think that. It because I think that that gets very quickly very technical. But I will show you an example and I will use a simple SIR model to illustrate the idea. So here is basically the perhaps most important insight. So the epidemic models in general can be divided into these two groups nowadays. There are some other ways you could do it, but that's one. You have ecological models. You have ecological models, which are sort of looking at the aggregation of your information. You're basically looking at averages. And then you can look at individuals and individual histories. This is typically where you would get into the realm of agent-based models. You are building very large simulation, or you're building some simple model which will attempt to look at the behavior of the system of agents in aggregation. Agents in aggregation. So, the idea of this thing that we call dynamical survival analysis is that we would like to borrow some structure from these simple models to feed it into these agent-based models in some sensible way. So, in a way, build a modeling scheme when we will still be looking at the individual-level behavior or individual level. Behavior or individual level probability events, but they would be guided by something that is consistent with the overall features of the model. And the reason for this very often could be this, that, you know, it's very hard to collect individual level data. It's much easier to see some global trends in this ecological model. So maybe as a first way of trying to build Of trying to build agent-based models, we can try to use information which is contained in here. It turns out that there is a sort of general way of doing this, and we call it an iPad model. So if you remember Steve Jobs' pitch for iPads, he was looking at an iPhone and a laptop, and then he was asking this question: Is there something in the mid? So that's kind of what we are trying to do. We're trying to build an iPad out of these. iPad out of these ecological models and agent-based models. And how do we exactly do it? So first to the ecological model. For the purpose of being consistent here, I will use a stochastic model. It's a little bit simpler to explain. A stochastic ecological model, it's a very simple Markov model in which we have two types, basically three types of Basically, three types of individuals or molecules that are interacting with each other according to a Markov process that basically follows this simple rule. If there is an interaction of two molecules or individuals of label S and I, then that interaction results in the change of the label of an S individual into I individual according to this simplified chemical reaction. And then you have another type of possible interaction at some point. Possible interaction at some point, these individuals who are marked as eyes are recovering into R. So, this is a stochastic model. There is a very elegant, simple algorithm for implementing a stochastic system like this. It's called the Gulesky algorithm, and it's based on the simple idea that the Markov process with continuous time of this type can be decoupled into a Markov chain and the process which keeps track of the timing of the primitive. So, this is an ecological. This is an ecological model in a sense. If we aggregate this model up, we get a differential equation, and this is the very often sort of initial way of analyzing an SIR type stochastic model. This model is surprisingly versatile in a way that it, because it's stochastic, it can actually handle very often quite a bit of variety of models that are not exactly like. Of models that are not exactly like this. So it's robust with respect to some of its features. But it certainly has limitations. There is another way of thinking about this model, but now from the point of view of a single age. And this is known as a Selkie construction or the Selkie model. And the Selkey construction is basically relevant to what I would like to describe, and it goes like this. Let's say that we have a population of individuals. Population of individuals where each individual is endowed with a random threshold. Let's say that this threshold follows some exponential random variable. Now, initially, there is some number of individuals who are already infected and some number of individuals who are susceptible who are endowed with distress. Now, let's say that we're interested in the random variable t, which will be Which will be a random time that the initial susceptible individual, specific susceptible individual, will change its status from S into I. Okay, so T is a random variable. It's a timing of that change. Let's introduce this quantity AN, which is basically overall amount of information. Overall amount of infection in the system. So it's an integral over i, where i is the total number of individuals who are infected at time t. So what this thing does, up to some scaling, when I n is the total size of the population, I scale it so that the whole quantity does not blow up. So this is basically what we would call an infectious, cumulative infectious pressure, right, on the system. Pressure on the system because it basically counts the number of individuals who are infected, but over time. So it basically counts the amount of infection that we have by time. So if Ht is the history of infections across these individuals, then we can basically write something like this. So if I have an individual who is initially susceptible. Who is initially susceptible, then the probability that this individual stays susceptible by time t is basically this. This is the probability that, that's how I define it, I guess, that's the probability that the individual's threshold, this exponential random variable, is greater than that cumulative infectious pressure. So, what it means is that I have introduced basically a mechanism for switching. Basically, a mechanism for switching susceptibles into infected type by following this cumulative infectious pressure. And if individual threshold is lower than this cumulative infectious pressure, I can switch that individual from susceptible to infect. So, the way that you can think about it is: so the probability that this individual stays susceptible to time t is. Susceptible time t is this, it's in terms of the specific individual threshold. But then you can also think about it because I took this Q variable threshold to be exponential, I can rewrite it in terms of this cumulative pressure. So basically, I can write it also in terms of this variable capital T, which is the time of switching. So this E to the negative cumulative infectious pressure. Negative cumulative infectious pressure, it's nothing else, but it's a survival, it's a tail of the probability distribution of capital. So basically, this is sure. So it's not an assumption, it's a construction. It basically gives me a way of creating probability with exactly this cumulative hazard. No, no, no, no. This is this is basically so if I take this thing to be exponential, and if I use this as a threshold, that basically creates a survival function, which has the desired property, which is basically that this survival function will now have a cumulative hazard, random cumulative hazard, which is exactly this. So that's not a feature of a construction, that's just a consequence of what I want. And it turns out. And it turns out, so that's also important, that if I assume that the recovery times, because I didn't say that I should have, that the recovery times of these agents in this construction are exponential, then this is exactly equivalent to the Gillespie alpha. However, this construction is more general because it does not assume that the process is Markovian. It assumes that you may have a history that's associated with recovery. That's associated with recovery, with the amount of time that you stay infected. So now, just a very quick example of how this works. Let's say that I start with a single infection. And what I've plotted here is this cumulative hazard. So if I have cumulative hazard, it can only grow or stay flat. It's a non-negative function which is integrated over time. These i's are basically. Over time. These I's are basically piecewise constants because I am just counting the number of individuals for infection. So if I have this AN moving forward, it kind of looks like this. You know, I keep growing over time until I hit the first threshold. These are Q1, Q2, Q3 are the order statistics. So I have basically ordered my thresholds from the smallest to the largest. So once I hit the first one, I increase the number of infected people by... Increase the number of infected people by one. So then this quantity increases. So the slope here goes up. So I continue to go up now at this slope over the next period of time until, say, one of these individuals recovers. Then I basically come back down. I continue on. I hit the second threshold and so on. At some point, I might have run out of infected individuals, in which case this curve will go. So this is a way in which you can think about implementing. Which you can think about implementing a very simple agent-based model when you would have these individual thresholds, and these thresholds will determine whether or not you get infected and at what time. So what can we do in general? This model, because it's non-Markovian, is actually more complicated to simulate than a Kulesky algorithm in general. But there is one really important But there is one really important insight that might be relevant to what I want to talk about. If you guys generally remember under various circumstances, but for a moment, let's assume that I'm going back to the Markovian systems. I will assume for a moment that my recovery times are also exponential. Under that condition, this quantity I t, which is the count of infected, divided by the size of my population, and that quantity. My population, and that quantity in probability converges to a deterministic quantity. This is basically the law of large numbers for these Markovian brothers. And what is this deterministic quantity that it converges to? Well, it's a piece of a differential equation, which we all know. This is the so-called SIR equation ODE. So we can basically write this macroscopic ODE, which as part of its trajectory has this iota variable, and this iota variable is a limit. And this IOTA variable is a limit in the appropriate sense of this i quantity, which is random, divided by. So then the idea is as follows. Why don't we just replace this a n, which is a random cumulative hazard, with a limiting hazard for very large n. That limiting hazard would be maybe a good approximation of the cumulative hazard. What it does, it basically creates. It does, it basically creates a model in which we will no longer have dependence on the history of that process for a single individual, because that quantity now will be replaced by a deterministic one. So that means that we somehow made all these agents independent in the large limb. So, what is another way of thinking about it? Is another way of thinking about it? Well, you can rewrite and play a little bit with the parameters of this ODE model to rewrite this in terms of partially solved trajectory. So for instance, S of T, you can write it as the exponential of this quantity, which you basically see as nothing but the exponential of this negative limiting cumulative. In the similar fashion, you can write iota in this way, and you can In this way, and you can write R in this way. So if ST can be written in this form, then statisticians in the room would recognize: oh, this kind of looks like a survival function, or at least a partial survival function, with this cumulative hazard. And that quantity R actually looks like a scaled cumulative hazard protocol. It turns out that you can show that that quantity after appropriate scaling is actually a density function. Is actually a density function. So, somehow, magically, these SIR equations are uncovering for us a certain probability law, which is associated with a very large population in which we believe that these SIR dynamics, the stochastic dynamics hold, and therefore these stochastic dynamics can be approximated by the simple differential. So, if you look at the S. So, if you look at the SIR model, then basically what this all means is that this S, the number of individuals who are the proportion of individuals who are susceptible, you can think about this as an improper survival function. This I here is a certain type of density. It turns out that this is a density of recovery. And this R is a cumulative hazard. So, this is the total amount of infection. This is the total amount of infection that we have had before, which is basically consistent with the usual interpretation of the SIR model. So fine. Once we have this, we can then think. So what these curves are doing, they're describing a behavior of a single individual who is a part of that very large population. And moreover, that single individual is independent of all the other individual independent. All the other in the limb. So I can use this to basically create a probability law. That probability law naturally will have the property that once you aggregate it, it will converge to the SIR model. So it will converge to the probability law that the stochastic system would have converged to in the appropriate sense. So this is basically the idea of so-called dynamical survival analysis. Dynamical survival enough. So, another way of thinking about it is: once we have these SIR equations that we can use as probability laws, then instead of looking at the entire population, we could look at the random individual and sort of look at the transfer times. You could look at the times at which this individual would switch from S into I and then from I into R. So then the statistical model changed. Model chain because instead of counting people of different types, it seems like all I need to do, I just need to monitor a certain amount of individuals to see when they switch their time, at what time they switch from one type into another. And then in order to close this argument, I also need to know what is the final size of an epidemic. I need this in order to be able to ensure that I have the proper probability law, but that's actually relatively. But that's actually relatively simple. And then I can build an agent. And this agent is super simple from the point of view of implementation. It basically does the following. Let's say initially it's susceptible. Then with probability one minus tau, where tau is basically this limiting size of the epidemic, it never gets infected. So it stays, it's a survival. With probability tau, it gets infected. And then if it gets infected, then it necessarily has to. Gets infected, then it necessarily has to have the survival function st. So the density of that would be the derivative of that function. So you can actually write it like this. And then after exponential amount of time, it will recover into the necessarily into the recovered compartment. So then again, you know, by looking at these differential equations, you could actually derive the collective joint convolution of these two density functions, and it looks like this. So the point is that I have. So, the point is that I have a complete probability law for a single agent in such a way that it aggregates to the correct one. This is not a feature of an SIR model. This can be done under most reasonable circumstances for much more complicated systems. And in fact, we have done this. I think Wasur will be talking about it. But you are assuming here that Tau is known, right? Yes, because Tau, so I am assuming, so I didn't talk about statistics of that. I didn't talk about statistics of this yet. That's where I'm going. But for now, yes, I'm assuming that the tau is known because tau is expressed in terms of the other parameters of the system. I agree that when complicated model, right, right, right, right. So, so, so I do not disagree with that. That's right. So, I'm assuming that Tausen. Why is it interesting? Because, for instance, you can write a likelihood function very easily. So, you know, the whole idea of this model kind of The whole idea of this model kind of came around because we were interested in estimating parameters and very complicated models, and we didn't know how to do this other than building some really very complicated augmented functions based on the Markovian likelihood. And then we decided, well, maybe we can just approximate it by something different. So that was the idea. The nice thing about this writing here is that you don't need to know what is the size of your population because you're not using. Because you're not using that, you're only using these timings. You have to have some number of timings of individuals either in the switching from susceptible to infected and from infected to recovered. So from the point of view of estimating parameters of this model, you don't even need to know what is the size of your population. All you need to know is what were the timings of these different events. Any of them. So that's convenient for a variety of reasons. I am very quickly running out of time. So, let me just tell you one more thing here, because this is basically really what I wanted to tell you. This is just the beginning of my presentation, but the rest of it, maybe Wasure will give some examples. So, hopefully, we will be able to get a sense how this can be used. But, you know, mathematically, what is interesting, and that's the reason that I actually showed you an. And that's the reason that I actually showed you an SIR model because there things are a little bit better, more tractable. There's a question: so, if you have these agent-based models which are constructed using this probability law, how good or how bad is it actually in terms of approximation? How big of an error are you? So, it turns out that this is actually a very good approximation in terms of the size of the population. One way to work through this is To work through this, if you build this agent-based model from the Selkie construction, you can basically create this collection of stochastic processes which are keeping track of the status of a single individual. So you basically have these individuals, and then at some after some exponential time, that individuals switch into I and then switch into R, we can keep track of each. Of each and every one of these processes, they satisfy the conservation law like this. And then you can, from the point of view of the trajectory of the system, you can think about it. So if you're an SI, this SI now, S sub I, it's a status of an I-th individual, and it's an indicator whether that individual is susceptible or not. Whether that individual is susceptible or not. So let's say that this individual starts at one. So he or she is susceptible at time zero. And then what this is meant to do, it's basically telling you that at some time, so yi is a Poisson process. So basically at some point of time of this Poisson process, this quantity will become one, right? So the intensity is if this was one times i, so this is the proper type of intensity. Proper type of intensity. This is the Selki intensity in my construction. So with this intensity, this process will become one, and then this will be one minus one. This will become zero, and then this process will be turned off. Okay, so this describes you the agent behavior in the random setting when you have a population with the random cumulative infection. And then this is what we are doing by doing the approximation. We are replacing. Approximation. We are replacing this s and not s, we are replacing the i over n with this iota, which is a piece of the trajectory. I'm keeping the same stochastic process. I'm exactly on the same probability state. I'm basically just switching the argument of my intensity function from the random one to a deterministic one. Okay, so I can now compare these two things. I can say, try to count the number of susceptible individuals under this true. Susceptible individuals under this true agent-based model, and the number of susceptible individuals that I will have over time under this approximate. So, again, without going too far into details, there's a very beautiful exponential inequality that says that these things become very close very quickly. So, this is from our paper from last year when we basically studied a little bit more complicated model, but we basically derived something like this. If you look at the average number of agents who are Agents who are still in S state by time t. And if you compare it to the number of agents who are in state t under this approximate model, then that difference is uniformly small and the rate is basically exponential. n is going to infinity, so at some point this quantity will become bigger than any finite t. t, by the way, here is no longer. Here is no longer this random variable t that I was using initially. It's just the time horizon. Okay, so the point here is that it seems like this approximate quantity that we obtain by approximating circuit construction is a good approximation of the overall average behavior of the number of agents in the original model. The original model. And again, this is applicable to a number of different settings. But I think it's really easy to understand looking at the simple SIO. So what does it do from the point of view of, for instance, monitoring? And we did deploy it at Ohio State. Matt may be talking about it a little bit. Talking about it a little bit. We didn't really use this specific model, but we had some ideas about how that kind of model could have been effectively used, for instance, for students' monitoring. The idea is that instead of counting infections, you can potentially pick up just a subset of individuals from your population and monitor them over time in terms of their timing of switching from Of switching from S into I and then into R under the assumption that this timing of switching is giving you the cumulative hazard of infection. So as long as you can calculate approximation of this cumulative hazard, you should be able to recover the parameters of the model, at least the type of model that I'm talking about. So that means that it would be enough instead of trying to It would be enough, instead of trying to look at these entire populations and measuring the amount of number of individuals of different types at time t, it would be enough to monitor specific individual randomly selected over time and look at the amount of time it elapsed between these different individuals becoming infected. So, we called it the human sensor network, and Eben was talking about it a little bit. It actually turns out that what he was talking about. About under certain conditions, for instance, under mass action type law, is exactly something that, if aggregated, agrees with this model. So in large populations, the type of studies and hazard functions that Evan was writing will basically give you these limiting stoky hazards. So there is a number of ways in which this entire model can be extended. This entire model can be extended. I don't really want to go much further into this, but it turns out that we only need very little information in order to be able to use it. So for instance, if we have some information about S dot, which would be equivalent statistically to incidence rates, then we can try to measure the amount of infection. If we have some information about I, which then would be related to like PCR testing. Would be related to like PCR testing in the population, we can do that. If we have some information about seropositivity, we can easily augment this model by adding the additional state for people who are seropositive and introduce things like weighing, etc. That allows us to build up these agent-based models, which are flexible and specifically targeted towards the kind of data that we are collecting. That we are collecting. So, what it does then, it basically allows you to build various types of models. I don't really want to go into this one. We had a specific model when we were looking at seroprevalence, and then we're comparing it to the amount of infection in the wastewater in Jefferson County, Kentucky. And this is a CDC-funded project. So, this was just a way of showing that once we fit the data to a specific type of Specific type of testing based on random samples, we can actually recover the overall pattern of infection, which was very similar to what we're getting from the wastewater. So these things are not, they're not up to scale. So quantitatively, they're not comparable, but qualitatively, they seem to be after some adjustment. So the point being here that these kinds of methods seem to be usable in various different Visible in various different settings. And we will have some more examples in the next talk. So I just want to say these are dynamical networks. It's one example. The other one is non-Markovian systems. Whereas, again, I don't know if Wash will mention that or not, but in some of our modeling attempts with Francesco and Wasur and a couple other colleagues, we have basically developed these. We have basically developed these extended self-econstructions when we had complete freedom of selection of the type of infection or contact interval distribution versus infectious period. And then we did some comparisons when we were looking at, say, Markovian models like this, we were getting way overestimation of the number of cases. Whereas if we were looking at these more Markovian models, we're getting things which are more closely resembling the real. More closely resembling the real system. So, this is just to point out that there is a bit of flexibility built into these models, and that can be adjusted to what's needed. The other thing is we can very easily handle changes over time in parameter. This is what we did in this Ebola paper from the ERC. So, you may have some families of parameters which will be time-dependent and then can be very easily incorporated into these models. Agents are independent. Each agent Agents are independent, each agent potentially could have a different parameter that leads to a kernel model that also would give you the types of differential equations or partial differential equations that I think Wash was talking about, or maybe to some extent what Gabriel was showing. So you can think about these types of equations are being limits of these DSA models under certain conditions on heterogeneity. Conditions on heterogeneity urgurate. Okay, so there's lots of people who have been involved in these projects, some of them very young. We started them early. But I would like to thank both the agency and the people for all the efforts. And, you know, it's been a very stressful six, seven months due to COVID, but I think we learned a lot. And I would be very eager to see how some of these things that we were able. Some of these things that we were able to learn and apply might be useful moving forward. So, thank you for your attention.