All right, so the last one is Jean Weijang. Hello, everyone. My name is Jean-Lui, and I'm a postdoc at the University of Timewa in Sebastian Underfest Bruce. And this is joint work with David Boning, Sebastian, and the PhD Vladimir. As you can see from the title, so here our interest in the stochastic process is driven by our exponential tail Lavilois. And let me start with. And let me start with two popular examples. So, the first one is this Langausian Ornstein-Bundberg process, which is defined in this ST here as the stationary solution to this ST, and Z is a Land Gaussian lemming process. Example two is this type G Mitan SPV learning fields, and again, it is defined as the stationary solution to this SPV, and M dot is. Is SPV and M dot is the so-called type T Levilois. And one thing these two examples have in common is that both of these Lawyer laws have considered in these papers have exponential trends. And so this needs us to think about the question: what is the extreme of dependence induced by this Langaus intercesses? And so here we focus. So, here we focus on the commonly used summary statistics for extremal dependence, as you have seen many times in talks before. So, we have this time measure, and when chi is equal to zero, we speak of isentropic independence, and when chi is bigger than zero, we speak of isentropic dependence, and when chi is equal to zero, we use another measure eta to describe. Your beta to describe the diseased extreme dependence. So, as you can imagine, it is not easy to deductively work on this continuous index stochastic processes. So, what we start is some discrete models, which are approximations on the stochastic processes when we discretize the time domain or the space domain. And the discrete models we consider are the linear transformations. Models we consider are linear transformations. So, as we can see, x1 and x2 are both linear transformations of random vector y with IID components and exponential tails. And we assume the coefficients AIJ are lambd negative because this is what we get from the model from these two examples we have seen before. And if we further assume that the distribution of yi is absolutely continuous, then we can show that. Then we can show that the stimulant dependence of X1 and X2 are essentially determined by the largest coefficients on the first row and on the second row. So if the two sets Arcmax A1i is equal to Arcmax A2I, then we get asymptotic dependence. And if these two sets have empty intersections, we get asymptotic independence. And otherwise, we need more assumptions to draw a conclusion. And how do we understand this without? And how do we understand this without the intuition behind this? Is that so? If we assume that arcmax A1i is equal to Armax A2i is equal to 1, then the most contribution for the tail of X1 and X2 would both come from the component Y1. In other words, the tail of Y1 would determine both the tail X1 and X2. And this is why we get asymptotic dependence, and otherwise we get asymptotic dependence. Otherwise, we get asymbolic independence. And based on this result, we can show that this Langhausen OU process, which can be written in this stochastic integral form, and if the stationary solution has an exponential tail distribution, then I can show this process is asymptotic independent, and the residual tail dependence function is in this form. In this form. And for this type G model, which can also be written as a stochastic integral, where the function g is this green function of the differential operator. And in practice, people use finite element approximation for fast inference and simulations of this model. And so this finite element approximation is also in the form of linear transformations and it is isymptotic independent when the fine when the mesh is fine enough. When the mesh is fine enough. However, it is not easy to get the limiting residual tier dependence function when using this finite element approximation because the coefficients appeared in the linear transformations involved integrations of basis functions. Instead, we can consider the approximations of the stochastic integral, which can also be written as the uniform of linear function. Of uniform or linear transformations. And we can show that when this function g is convex and g0 is bounded, we need limiting residual time-dependence functions on this form. And I will stop here. If you want to know more details, please come to my post. Thank you.