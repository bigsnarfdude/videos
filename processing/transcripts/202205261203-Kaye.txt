Thought about so far in the workshop coming from computational quantum physics. And a lot of this talk is just meant to kind of introduce some of these problems, put them on your radar, which are integral equations problems that kind of have a different flavor from the integral equations problems that typically come from reformulations of PDE. Come from reformulations of PDEs. And I think some interesting sort of issues come up, and this community might have useful things to say. So let me just start with maybe I can move this. All right. Oh, I can. Okay. Sorry. Okay. Sorry. I want to be able to see my own slides. Yeah, so let me just start with a word on sort of what the quantum anybody problem is very briefly and why it's difficult and what are some things that people do. So we have a system of n quantum particles, maybe sort of some electrons in a collection of atoms or some chunk of material. Chunk of material. And our system is described by a wave function, which is a function of sort of n different sets of three spatial coordinates, one for each particle, and a time variable. And the problem is that, well, this wave function evolves according to the time-dependent Schrödinger equation here. H is the Hamiltonian of our system, which is divided. Which is divided into kinetic and potential energy parts in particular. And uh oh, okay, my clicker has gone, but yeah. Well, that's not working either. Sorry. Yeah. Maybe you can have a thing. Oh, beautiful. That works. Okay, there we go. Right. So, you know, this problem immediately appears intractable. It's a high-dimensional PDE. If we put down m spatial grid points per dimension, you know, we have m to the 3n numbers to keep track of if we want to store the wave function. So, you know, people need to do calculations. What are some methods they use? So, first of all, you might kind of take So, first of all, you might kind of take the attitude that, well, maybe for some systems, the wave function itself is compressible in some way. Maybe there's some kind of high-dimensional, low-rank structure, and this leads you to sort of tensor network methods. And for some systems, this has been a successful approach. You may turn to what's called density functional theory, which has been sort of the dominant method, I would say, over the past. Dominant method, I would say, over the past several decades for a wide variety of physical systems. The idea here is that instead of sort of propagating the full n-particle wave function, we will propagate n single particle wave functions, 3d PDE, according to the time-dependent Schrodinger equation, with a potential whose job is to sort of in some effective mean field manner take into account the effect of sort of the quantum. Take into account the effect of sort of the quantum high-dimensional quantum particle interactions. And the observation of density functional theory is that there exists such a potential called the Kohn-Chan potential, which depends only on this sort of total particle density here. Now, of course, we don't know what this is, and sorry that there exists such a potential that allows you to sort of recover the full n-particle wave function from the single particle wave functions. These single particle wave functions. So, of course, we don't know what this potential is, otherwise, you know, many body physics would be solved. But people have come up with a variety of good guesses that have allowed them to sort of do their work. But unfortunately, you know, sometimes the system is so-called strongly interacting or strongly correlated. There are strong interactions between these quantum particles, and this kind of effective independent particle picture doesn't cut it. Independent particle picture doesn't cut it. So, the class of methods I'm going to talk about today are called many-body Green's function methods. And the philosophy there is that instead of trying to track the whole wave function, we often really only care about sort of specific physical observables, maybe some expectation values or correlation functions. So, can we sort of write those down and solve some equations for those directly? So, let me give a little bit more of a detailed. Give a little bit more of a detailed description description on these many-body Greens function methods I've written without C dagger and C. So, to really write down precise definitions, I would need to use so-called second quantization formalism, which is probably not so familiar to people here. So, you know, I'm just going to give you a zoomed-out sort of hand-wavy idea of what these Green's functions are, and then I'll write down some equations. Write down some equations. So, the first thing is that, you know, in many-body physics, sometimes we're looking at a chunk of a material with a huge number of particles. And in this formalism, the particle number itself can vary. So with that in mind, the single particle Greens function is this function g, and it really describes the correlation between sort of adding a particle to your system at one point in space-time and removing a particle at another point. And removing a particle at another point in space-time. There are n particle Greens functions where you think about adding and removing n particles at the same time. And this is thought of as kind of a way of probing your quantum system. So you add a particle here that perturbs the system, that perturbation propagates in some way, and then you kind of measure the system by pulling out a particle there. This is a physical picture you can have. To make things a little more concrete, let me describe a Let me describe a physical experiment that people do called photo emission spectroscopy. So, in photo emission spectroscopy, you have some quantum system, maybe it's a molecule or some chunk of material, and you send in a photon, you shine some light on the system, and that kicks an electron out. And by comparing the energy of the photon, which you know, and the electron, which you measure, you get some information about. You get some information about the energy landscape of your system. And this is the kind of experiment that's sort of very well directly modeled by these green sections. So as you can imagine. Okay, so, you know, well, this audience in particular is very fond of Green's functions and knows all about Green's functions. Why are these called Greens functions? functions. You know, how are these related to sort of the Green's functions we know from PDE theory? Well, if you have a system of non-interacting particles, particles that aren't seeing each other, with some Hamiltonian, then this single particle Green's function I described is exactly the Green's function we know and love. It's the response under sort of the evolution of the time-defendant-Schrödinger equation to a delta function perturbation at some point in space-time measured at another point. And what happens when you include And what happens when you include quantum interactions is this differential equation becomes an integer differential equation. You get this interaction term with a kernel called the self-energy, which in general depends on the Green's function itself, sort of the solution, the differential equation. That will be important in the talk. And I've kind of written this in an ambiguous way because this is a broad theory and This is a broad theory, and you'll see some concrete realizations of this kind of thing. But the idea is: you know, quantum interactions get you from a differential equation to an integral differential equation. So why are these things so important? Well, many physical observables that people really care about that come out of experiments can be written in terms of these Greens functions, as I mentioned, because practical experiments often probe a system by sort of adding and removing particles. These Greens functions can capture really truly Functions can capture really truly many-body effects, even in macroscopic systems, without tracking the full wave function. And by the standards of many-body theory, these things are low-dimensional. And well, they satisfy, it turns out, non-linear Volterra integer differential equations, which makes us happy. And I think, you know, in general, these kinds of methods really lie at the heart of methods which are trying to go beyond. Methods which are trying to go beyond things like density functional theory, effective one-body methods, which sort of get rid of the truly many-body character of the problems. So, yeah, I think it's useful to kind of compare and contrast the challenges arising in these methods with those arising in kind of integral equations coming from PDEs before I move on to something a little more concrete. So, one good thing here. One good thing here is that you know, some things are easier, some things are harder. So, one thing that's easier is in general, you know, we don't have to deal with complex geometries with these integral equations. The space is typically sort of dealt with by essentially, well, often where you can think of, you know, a lattice of atoms, and at each lattice point, you have a bunch of orbitals or basis functions. And so, kind of spit the x and x prime from before. The x and x prime from before is dealt with by introducing some orbital indices or basis function indices. That's usually how space is dealt with. And we don't really have to deal with singular kernels. We'll generally have smooth kernels, maybe sort of discontinuous at the diagonal, but otherwise these kind of quadrature issues don't really come up. We still have to deal with non-local integral operators. We need fast algorithms. And the first sort of new And the first sort of new thing is that we have what I'll call kernel nonlinearities, where the kernel depends on the solution. I'll come back to that. And then sort of going towards more realistic physical approximations, which are harder, eventually sort of evaluating the kernel becomes by far the computational bottleneck. I'll just briefly mention this at the end today because this is an interesting direction. But today I'm sort of going. Interesting direction. But today I'm sort of going to focus on, you know, mostly on what happens when you combine these two issues, one of which is old and one of which is new. So let me start, just to write down something concrete, let me start with what's called the equilibrium Dyson equation. And I'm not going to sort of say in too much detail what I've done on this, just mention it. And then we'll go to the non-equilibrium Dyson equation, which I'll describe what I've done in more detail. So in the non-equilibrium, So, in the non-equilibrium Dyson equation, or the case of time-dependent Hamiltonians, you can think of this as: you know, we have our quantum system, maybe it's a lattice, and we apply some strong field to it, some strong electric field, and that introduces some interesting dynamics which we want to track. Yeah. Sorry to interact, but I think we got lost. Otherwise, the previous slide, you mentioned no video on the face. I didn't understand that one. Well, so yeah, maybe I shouldn't have said no geometry. We don't really have complex geometry, right? We still have space, but it's sort of dealt with, you know, first of all, we don't have, you know, complicated domains with boundary conditions to deal with like we do in PDEs. And sort of the spatial degrees of freedom become, you would sort of get Green's functions gij, okay, instead of xx prime. So we can sort of get rid of that as a So, we can sort of get rid of that as a scary thing that is looming in the background to some degree. So, yeah, so the case of time-dependent Hamiltonians is like in photo emission spectroscopy. You have sort of a strong applied field. I'll get to that. Let's start with time-independent Hamiltonians, where you're just sort of looking at a system in equilibrium. And here, the single-particle Green's function satisfies what's called the equilibrium-Dyson equation. Equilibrium Dyson equation. And for a simple enough self-energy, which was this sort of kernel that takes into account particle interactions, you can eventually kind of apply and mathematize the equations into something that looks like this, okay? Which is already going to be interesting and new. Let me just say here that, well, g became y, okay? So it looks, I don't know. So y is a single particle green. So, y is a single particle Green's function. K was the self-energy sigma from before. And I've suppressed the X and X prime dependence for the reason I just said. And note that in equilibrium, we have a time-independent Hamiltonian. So the Green's function becomes time translation invariant. That's why y is a function of only one time variable. So, yeah, the challenge is going to be how do we sort of propagate these equations to large time and Equations to large time. And to get some of the interesting physics, you need to do that. You need to look at sort of correlations between adding and removing a particle at well-separated times. But let's stare at this kind of complicated looking equation a little bit and contrast it with what you would usually call a nonlinear Volterra integrator differential equation in the applied math literature. So on top, we have sort of our standard nonlinear Volterra integrator differential equation that people have studied a lot. Equation that people have studied a lot. And you sort of have the nonlinearity hitting the solution here. The interesting new thing we have to deal with is these kind of kernel nonlinearities where the kernel itself depends on the solution. And so you can think of, you know, this first thing is the old version. This is like a standard weekly singular kernel, and then we get a y squared on the solution. Whereas what we care about in quantum physics, all the... care about in quantum physics all the time is things that, you know, in the simplest case, look like this. Okay. So if you know something about Volterra equations, you know that sort of the computational bottleneck is the fact that at each time step, you have to sum over all previous time steps. Okay, so just think of f of y is y, right? At time step t, I need to sort of compute an integral over. To sort of compute an integral over all previous time steps, we get an n squared cost. You know, you can also say that we need to apply sort of an integral operator, cost is n squared. And the same thing happens here. But for the first case, you know, there's decades of research on fast algorithms to do this, like convolution quadrature, sum of exponentials compression. But these methods all sort of require a priori global access to the kernel. Often they use Often they use explicit information, analytical information about the kernel, and some might work if you can only evaluate the kernel numerically, but you can't have something like this, right? Where you don't know the kernel yet. So I'm not going to describe it, but we did some work on this and basically wrote down an n log squared n solver for this class of equations, okay, by sort of doing something similar to what people have done before. What people have done before to compress this integral operator, but in a way that respects kind of the causal structure of these equations. So instead of going into that, you know, this was useful to see as sort of the simplest case. I want to go into a little bit more detail in the non-equilibrium case, which I think has a slightly more interesting algorithm, which again is for the case of time-dependent Hamiltonians, like photoemission spectroscopy, where you have some strong external field. Strong external field. And now we get, you know, well, the real equations look much worse, but this is kind of a prototype which will capture, you know, what we have to deal with. So we get something scarier looking. Okay, here's my picture of photo emission spectroscopy again to remind you. So let's stare at this scary looking thing for a minute to see kind of like what's the computational problem we have to deal with. So let's go through this a little bit slowly. So the equation up to So the equation up top, what you want to do is, first of all, y is, you can think of as a matrix that we're filling in. So y is again the Green's function, and the system is no longer time translation variant. So we have two time arguments again. And so finding the solution is you can think of it as filling in this lower triangular matrix because it turns out that this t prime variable goes from zero to t. Goes from zero to t and so what's going to happen is we're going to fill this in sort of one row at a time. And each row corresponds to a fixed t. And then if in the top you fix t, you see that that's a Volterra integrative differential equation in T prime for each fixed t. Okay, that integral differential equation again has a kernel, k, which at t and t prime depends on y at t and t prime. So here's how this works. prime so here's how this works um we say we have these rows okay now we've filled in this row and that allows us to fill in a new row of this kernel matrix and well this integral operator you can think of as a matrix okay it's a lower triangular matrix in fact and that matrix the one that we need for the next integer differential equation we need to solve is exactly this matrix Is exactly this matrix here. Okay. So then we fill in a new row. Okay. And using sort of this kernel. And that allows us to fill in a new row of the kernel. And this matrix is now the integral operator in the next guy, and so on. So we have a sequence of Volterra integral differential equations. And the solution of the first m minus one determines the kernel for the m up to some nonlinear iteration. Okay, everything I just Linear iteration. Okay, everything I just said is true sort of up to nonlinear iteration on the most recent pro. So that's this kind of interesting structure. And you can see that we have an n cubed cost, right? Because, well, if we take kind of n time steps in t and at each time step we have to solve a Volterra integral equation with our kind of history integrals, that costs n squared and n times n squared is n cubed. And we have n squared memory because we're filling in a matrix. Okay, so this is. So, this is prohibitive in many simulations, really limits you to short propagation times, and is a serious problem limiting sort of computational physicists. So I did some work with Dennis Golesch, who was with me at Flatiron. He's a physicist. And basically, we said, well, if we look at here's sort of a contour plot. Here's sort of a contour plot of one of these kernels, a color plot of one of these kernels. We kind of observed that for many cases that people care about, if you just sort of impose this hierarchical low rank structure on this matrix, these blocks are low rank. Well, yes, because it's hierarchical low rank. So these kernels tend to have this kind of hierarchical low rank structure, just looking at them, you know, after you've computed them. So So, of course, that allows you to, let's say, store a truncated SVD representation of each of these blocks. And again, these rows are being filled in one time step at a time, as you saw before. So we need some efficient algorithm to update sort of these truncated SVDs as new rows come in. I'm not going to go into detail on how that works, but you can imagine it can be done. We have such an algorithm. So, of course, this reduces our storage and it also reduces. And it also reduces the cost. So, at the nth time step, this is sort of what we have. We have a compressed representation of each of these blocks. What can we do with that? Well, without going into too many details, right, computing these history integrals is in some way equivalent to just applying an integral operator, which is a matrix, and we have a compressed representation of that matrix, right? Now, you have to sort of do Matrix, right? Now you have to sort of do things in the right order because you're time stepping, but this is the idea. Okay, we have a compressed representation of this matrix, and we use these truncated SVTs to apply those blocks. And when you do all the counting, you get down to n squared log n complexity, as long as those ranks are constant. In practice, they sort of grow very, very slowly, like logarithmically. So you would get something a little worse, but still, you know, quasi-n. You know, quasi-n squared, and you get down to n log n memory. So here's just some timings and memory. And for the tests problem we threw this at, you know, for something like 250,000 time steps, we reduced what would have been a five-month calculation to about a day and four or five terabytes of memory to a few gigabytes. So you can really get big savings for practical. Really, get big savings for practical problems doing this. I'm working with collaborators on sort of a full software implementation for general systems in an existing library that had implemented the n-cubed method. And I think in a rather black box way, this will allow physicists to sort of propagate to much larger times. There are a lot of interesting directions to go for this kind of class of problems. You know, maybe the most. You know, maybe the most important is coming up with a quasi-linear algorithm. And I think this may be possible and have some ideas about how to do it. And then there's sort of a lot of good, honest work. You know, combine this with high-order adaptive time stepping, parallelization. And then, well, I think one day we'll probably run into a problem that doesn't have this hierarchical low-rank structure. Although so far it's been pretty robust. And maybe there are other compressibility structures which we can then use. So, all in all, you know, to kind of bring these solvers to the level of performance that we're more used to in PDEs, there's a lot of interesting work to do. So if I have a couple more minutes, I just wanted to kind of introduce quickly one last problem that's hard and interesting, which comes up when, so far I've sort of been So, so far, I've sort of been talking about the class of problems where, yes, the kernel is some nonlinear function of the solution, but it's some easy-to-evaluate function of the solution. And this is kind of a useful regime for a variety of problems, but for the most cutting edge sort of physical approximations for strongly interacting systems, evaluation of the kernel becomes by far the bottleneck. Okay, so let me just Far the bottom, okay. So, let me just kind of show you what that looks like. So, you get, you know, you may have heard of Feynman diagrams. This is this is what Feynman diagrams are all about. So, you get these kind of representations of these kernels, which have a diagrammatic representation, but in math, you know, one of these kernels on the easier end of the problems that people care about would look like this. Okay, so they're sort of a bunch of nested integrals. That's what these diagrams are. A bunch of nested integrals, that's what these diagrams represent. And what's the problem? Well, if we got rid of these deltas, okay, so each of these y's is sort of like a different grant, and we're back to something that's time translation invariant. So if you got rid of these deltas, well, this would just be sort of a sequence of, you know, one-dimensional transforms. And okay, even if we compute those with n squared, we don't really care and we can probably do better. But the problem is, these deltas. These deltas, which I call linker functions, they call them hybridization functions, they sort of link inconvenient times together so that you can't decompose things into this kind of, you know, 1D transformed structure. So it's an annoying problem. And of course, you know, everybody just uses Monte Carlo because sometimes these things are sort of 20 nested integrals. But it's a very interesting question, right? You could imagine. Interesting question, right? You could imagine that maybe using some kind of compression tools that people here know about, you could chop up this problem in some clever way and do a lot better. So I've started to think about this and for sort of the simpler end of things, have some results and ideas. But this is a nice problem, I think. So yeah, many body green functions are kind of an interesting direction, which I think. Think these methods haven't gotten a lot of attention in the kind of numerical analysis scientific computing community, but these are really integral equations problems, and they're necessary to go beyond, or for a lot of the kind of directions that physicists take are necessary to really go beyond things like density functional theory. And they have a really different flavor than some of the integral equation problems we're more used to. They're easier in some ways, harder in others. Harder and others. And sort of the last thing I said: this question of: you know, can some of the fast algorithms, compression methods that people here develop be of use in these kind of medium to high dimensional problems like diagram evaluation. Maybe they can, maybe they can't, but it will be interesting to see. So that's it. Thanks very much. Great. Thanks very much. Great. Thanks very much, Jason. Time for a few questions? Here. Yeah. You can take the mic. I'll fuck up close because then you can take it back. Would it be relatively quick to describe these deltas in there in that formula, like what they might look like? Oh, what they might look like. Those functions, yeah. Yeah, so okay. So, okay. There are sort of many, many different classes of problems packed into this slide, and it really depends on the problem. So what I'm working on are, you know, this class of problems called imaginary time Greens function methods. Okay, imaginary time Greens functions deal with when you kind of have a statistical ensemble of particles and you want to look for kind of equilibrium properties. And there, Properties. And there, essentially, there is a kind of low-rank structure in those deltas. Okay, so you can write down some kind of actually very convenient low-rank expansion and then, you know, sum over a bunch of kind of sequences of one-dimensional transforms. And that's what I'm doing. But that's the simplest case, still very important. Case still very important and a big class of problems, but even that, right? Or maybe if we can really push that, we could get to sort of you know 10 dimensions, maybe. But then there are other sort of types of things that look like this too. So, yeah, this is a big, a big area, but that's one simple case. Yeah, that's exactly what I was looking for. Some of them are separable. Right, I figured that's what you were wondering. But you know, if they're sort of have some hierarchical. Sort of have some hierarchical low-rank structure. I don't know. Can you do something? I'm not sure. It's not obvious, but any other questions? You mentioned that the Green's functions are smooth. Do you mean also, I guess, what happens for t equals t prime? Do they, I guess they have a singularity at t0, kind of like, well, yeah, so. Yeah, so right. The question is sort of what happens when t equals t prime? Yeah, in general, you would get a discontinuity, but not a sort of blow-up singularity or anything. And I have to think a little, you know, I'm not a good enough physicist to sort of say immediately off the top of my head what the physical interpretation of that is, but it probably has to do with kind of, yeah, statistics of, you know, what happens when you swap two fermions or something. You swap two fermions or something. But yeah, I can't give a great answer. But they typically, well, they may have discontinuities, maybe not, but you don't have these sort of singular quadrature issues. You don't struggle with it. You just do an expansion. You do everything in terms of basis functions and then don't worry about it. Yeah, I mean, we treat these things as triangular matrices. So we don't see any kind of. So we don't see any kind of disk, you know, we don't see anything. We see a function on a triangle. So Jason, this imaginary time is essentially along the heat direction. And do you ever need to go back to real time? Yeah, so excellent question. Yes, you can sort of think of it like that. You can think of it like. It like that. You can think of it like, you know, let's solve the Schrödinger equation by going to imaginary time and solving the heat equation. I think sort of everyone here knows that it would be difficult to actually solve the Schrodinger equation that way. This concept has not penetrated the physics community as much as it's known to mathematicians. And there's a huge, you know, a huge field of kind of analytic continuity. Huge field of kind of analytic continuation methods where they're trying to do just that and have kind of varying degrees of success, you know, not too much success, but sometimes they can get something. But there are sort of problems where you can't do that. So it kind of depends on what physical observables you're looking for. And yeah, I'm not going to say too much about what those are because I don't. Those are because I don't want to talk physics because I'm not a physicist. Have another? Okay, thank you. Jason, can I have a question? At the beginning, you list a slide that allows green function to interaction with each other in both space and time. Can you go back to one of the beginning slides? One slide with the right hand side, green function, two green functions interact with each other. Well, tell me when. At the beginning, probably, just a beginning size. Like here? I'm not sure of the file earlier. Here? Is that here? Yes. And here that you see that you And here that you see that you integrate both our space and time. And then these two time do you do you have order of those for example those three time t bar and t prime? So yes order. Right. So I sort of intentionally left this ambiguous because lots of different things can happen. But yes, you do get some kind of time. Yes, you do get some kind of time ordering, and sort of the, you know, the more concrete prototypes of what you would get here are written later. So this, you know, in the simplest case, would be something like this, where you get a Volterra equation. And you could think of, you know, putting ij indices on y to represent xx prime. Okay. So that's. So that's this is more like what you get in practice. I see. Okay, yeah, very good. Thanks. But it's sort of a very general theory. So now. Okay. Yes. That's my question. I think Carilla has another question. Yes. Yeah, another one. For the last slide, where you had the deltas. Yeah. I was wondering about something. I was wondering about something. So I think one of the approaches that I've that can kind of come up when you have this big multi-dimensional integral is, I suppose if would it be possible to okay, so all these deltas depend on some combination of the T's, but would it be possible to do some kind of Cholesky decomposition of the, I mean, the arguments to those, the mapping from the T. To those, the mapping from the T's to the arguments of those. So that if it's Chileski, you had a triangular sort of change of variables, which means you can write that as a bunch of iterated 1D integrals. Yeah, I was kind of wondering about this. I mean, are these kinds of approaches tried? Yeah, well, I mean, not too many. There aren't too many approaches which have been tried besides sort of various flavors of Monte Carlo. Of various flavors of Monte Carlo and quasi-Monte Carlo, things like that. I'm trying to try some of these things, but but yeah, I don't know, maybe we'd have to write it out, but it sounds like something like doing a low-rank decomposition. Right, so that's a that's another possibility is to sort of look at the integrand and try to approximate it with some kind of tensor network. And those. tensor network and those ideas are just starting to be uh it are just starting to occur to people so we'll see we'll see what happens so uh just a quick comment so jinfang huan has a paper on evaluating integrals very similar to what you have here but i mean you know it's the difference is that you know the integration limits are Integration limits are fixed in Jin Fang's paper. But I mean, you know, he does the evaluation of intervals in something like 10 or whatever, even higher dimensions. Yeah, so I have to look more closely at that. Well, first of all, sort of the limits of integration. Yeah, that is another kind of category of these problems where the limits of integration can be fixed. Limits of integration can be fixed. And I haven't looked at that paper closely, but I thought it was describing a Monte Carlo method, but I could be wrong. No, it's not Monte Carlo. Okay, I have to take a closer look. Well, someone has to ask the inevitable question. Uh-oh. Given that you're speaking in a workshop on oscillatory problems, that's. on oscillatory problems or day on oscillatory problems. What are the roles? And the pictures you've shown show somewhat wiggly looking off-diagonal behavior. Are there roles for oscillatory compressions like butterfly and things like that in any of this quantum world that you're in? Yeah, so I mean, on So, I mean, on the day that we meet a problem that sort of the hierarchical low rank compression or other variants of it fails, I will try that. But so far, you know, yeah, sort of something which is oscillatory is not necessarily high rank, right? So, so far, sort of interest. So, so far, sort of interestingly, we've gotten away with this pretty effectively. But, yeah, certainly, you know, you could imagine a situation where these blocks are not low rank, but maybe they're butterfly compressible. And can you sort of update these butterfly compressions on the fly in a similar way? I don't know. You know, these are, there's a lot of, there are a lot of interesting questions to look at. So saying a matrix of the form e to the i x minus y yeah e to the i k x minus y for arbitrarily high k is one rank rank one. It's a bit of a shocker. Yeah. But it's true and a linear combination of a few of those is is low rank. So I suspect that's the kind of thing we've been encountering so far. But you encounter that in a lot of problems that people care about. In a lot of problems that people care about. And it's sort of important to remember that so far they're not doing anything, right? There's no, they're only using the nqed method and sort of trying to use higher order time stepping, trying to parallelize things, or just simply cutting off the history integrals for specific problems when sort of eventually, when the history integrals decay fast enough. But ideas of compressing things and getting computations. Compressing things and getting computational complexity reductions in the case where you are dealing with sort of a dense system, that's pretty new. So I think there's a lot left to do. Well, Jason, maybe you shouted out already, but the dumb example would be you're looking at, it's basically an imaginary time heat equation. So if you replace T with I T in the heat kernel, you have something that behaves very weirdly in a very That behaves very weirdly in a very weirdly oscillatory regime. Uh-huh. Correct? Right? So that, right? We looked at that, and that's exactly butterfly compressible, I believe. I see. Okay, interesting. I think you knew that. I think we had this conversation. Maybe I'm imagining it. Yeah, I don't know. I need to understand better what the comment is, but we can talk. Okay, we can talk. All right. Any other questions? Virtual or non-virtual? Okay. Well, then let's thank the speakers for today for three great talks. And we will finish. Great.