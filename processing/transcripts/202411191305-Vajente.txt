Official, we need some ground rules for our meetings. So, please, you're on record, so try to be here anyway. So, here's the idea. I would like first to have a brief session on brainstorming. The way we're going to do it is with some online collaboration. So, every one of you has a computer or some fissimeter computer in front of you. So, this is the time you can open, you actually okay to use it during the talks, not in order to do your job. So, how it's going to work? How it got worked? I kind of summarized and I apologize as things are very small, but to put three slides in one slide, uh text is gonna was very small. But I took sort of the conclusion slides from my talk, from Derek's talk, just to give you a first reminder what we discussed this morning. There were there was discussion about many problems, many tasks that would be great to find a way to have machine learning contribute to those. Machine learning contributes to those. In texture, in instruments, noise hunting, in noise reduction, in control, identifying glitches, removing glitches, find new glitches, all of that. So everybody, go to this link here. I promise there's no malware there, trust me. So once you go there, I'll give you a second to just open up I'll give you a second to just open up that button and I'll show you on the screen. So this is a called a Padlet. I don't know if many of you probably have used this tool. So the idea is that everybody can add a little sticky note there and you can write something, a title, you can put some graphs, plots, images, you can upload a video if you want, but don't go too crazy. So the idea is that you see the two big That you see the two big topics. One is problems, and the other is techniques or solutions. So, okay, everybody got this? Yes, problems. Yes. No solutions. If you scroll across. If you scroll, this is the first step. Oh, the techniques are. This is the finest technique. If you zoom out, you'll see that there is more that media eyes. So here's how it works. I'll show you exceptions. I don't see it in my experience. So this is going to be interesting. So you can go down here on plus and add the new thing. So you can test the topic. And then you can write something here. You can even add an image, you can add a link. So if you want to add a link to something. So, if you want to add a link to some algorithm, some paper, you can put it there, then you can publish. Once you publish, you have your topic, you can move it around. And there's two things you can do is try to put it wherever you want, and you can also connect to a post. So now, since this my test topic is very similar to this other test topic, we will connect it. And this way, we can start also grouping things together. Then, I really like this test topic, and I Like this test topic, and I really dislike this test topic. Please use the dislike in a reasonable manner. Remember the ground rules. So here's the idea. Let's take about 15 minutes to just think back of the discussion we had this morning. And if you have a particular problem that you like or you would like to be solved or tackled or machine learning, add it there. If you have a specific technique that you think would be useful, Technique that you think would be useful, either you already have an application or you don't, just add it there. And then I will try to move things around while we discuss, while we think, and then we'll connect them according to topics. And hopefully later we can start a discussion from this and see if we can connect techniques to problems, problems and solutions. Questions? It's easy, guys. Alright, let's take about Let's take about 15 minutes and dig into this. You send those slides, like, they were pretty small of the slides. Yeah, I don't know what's the best way to share that. Those were from. I can put them. How about I put them up? Seconds. Good. Maybe I'll leave them up for five minutes and then or maybe even take a picture as well.   Is that right?     Can you guys move the comments at the left corner if you can post it? Because I cannot read. Because I cannot read them. I don't know if you have the same issue. Yeah. No, I mean when you post the contents, they are all like some of the less important parts. Yeah, yeah, so you can't address the control. But you want to use that tell you what happens in this property. I'll often have to recognize that.  You should be able to put it to my auto, you should be able to drag them outside.              Okay, I'll just wait. Okay, that's a hand.        Are we running out of webinars? Yes, we're ready. Okay, so unfortunately, this projector is a little bit small, but so what I did while you guys were adding ideas was to try to collect them together by topics, but at the end I gave up on that, it's too complicated. But there are at least three of them that have a lot of arrows going into them, so a lot of people felt connected to those three topics. To those three topics. And one is robustness to changes and retraining needed. Another down here is continual incremental learning to handle changing detector environment, which is a similar related topic. And a lot, at least three other topics were connected to supervised breach classification. So I think I don't see any other that have three or more connections. So clearly, those three topics are some. Three topics are somehow interesting. Then I highlighted in yellow the only two topics here that are related to actual control. And there's maybe only one down here that I can find which is directly related with trying to discover new noise sources. Everything else is either very, very few techniques and solutions or Techniques and solutions, or related to glitches. So, anyone wants to get started with any of those topics? I try to zoom in as much as I can. Apologies if your falls at the bottom, but there's not much we can. So, about ten years ago, me and Mako both made some unsupervised rich classification outcomes. Supervised rich classification algorithms, and we wrote two papers about it. This is before Gravity Spy existed, which I guess some people wasn't around then, or they just forgot about it. But you can use them to devise methods for virtual classification because they're I don't know why I can just use it as fine now, but we do it. That's a good question on that. Yeah. How um I don't know. And how did the unsupervised clusters coincide with the gravity spy classes? Well, we didn't have gravity spy then, so we didn't compare. But like, did you mean of the gravity spy out, for example, like 22 different classes? Did you see 22 different clusters or like a whole bunch of different clusters or no, like a I suppose working in hindsight? Or can you say anything about that? So I think we've begun to that mechanism. We didn't look at that next week. We may go wrong, like physically may go wrong with fake bridges and then just slide between notes. But definitely green on slides are not authentic types of supervised approach because of the excellent classification accuracy, but you seem very separated. So you would assume that the supervised approach should find that itself. Itself and it would be interesting if you'd like to have that. It also raises the old argument of why is a type of glitch because at the moment we define a glitch type of why it looks like in spectrum. But really a type of glitch should be, you know, does it have the same cause? But you know, if you have a blip flip bridge and a pump you might have some unsupervised method that says they're the same thing, but then people say, Oh, they're different things because they look slightly different quite eye. Because they look slightly different by your eyeballs. So then we have an argument: like, is the unsupervised classification right? Because it says these are the same thing, or are they not the same thing? Because they look slightly different. And you enter your final culture, you don't really know what the right answer is either. Sure. With unsupervised searches, could you run into this problem where you are getting some clusters which are, let's say, you will not encounter them in a real world setting, for example. That information, for example. And if you use that, you might get some glitches in the data which basically you're not able to explain. Or are we trying to explain every glitch we found? That's another question. We're talking about my first question. So please raise your hand if you want to make a comment. There's other people in the room that wants to make comments. I think you can hear your part because you were talking to. Hear your part because you were talking to the other side, but I wanted to refer to what Jane was saying. Actually, I also have Hannah Superman's blitz together with Marco, but we don't look at the H of T, we look directly at oxides. And I think there's other folks thinking also in the same direction, obviously. And I think this relates well with the one that's in the blue there. How can we actually? How can we actually make sense of all this information? That's quite big because it's 20 megabytes per second to actually restore a couple between glitches. Well, this is an open problem, right? And there's no one hammer for everything. So at the end of the day, maybe we should tackle like smaller problems of this big problem, which is like what is a glitch. I hope I won't violate any of the rules that you've shown in the first slide, but I'd like to move away from bleach classification because we have been doing glitch classification for fifteen years, since the beginning of machine learning. I I think there are plenty of tools to classify glitches, but I don't think that's the the priority The priority, I would say that the priority is trying to clean the data and substrate the issues and so on. So, I'll try to refocus more on the unsupervised keyword in that stuff. What I don't hear too much is development of methods that are unsupervised. What I'm here is that we might not be able to retrain continuously, that we don't have enough data to train. We don't have enough data to train, and that the simulations we have may not be faithful. So, maybe try to move away from algorithm that needs to be trained and try to develop something that trains itself, like the continuous one. So, if I can add a comment. So, here somebody wrote that we have very few or no method development on unsupervised machine learning. Machine learning. People comment that that's not true, so why don't you add a comment? You can add a little comment down here. Oops. That broke it. Okay. You should be able to add a comment and refer to the work that you exist. Because at the end of the day, if the only thing we get at the end of this day is that we have a bunch of reference previous work in this thing, that would be fantastic. I'd be very interested to hear the kind of characterization experts on this. And then, you know, we classify magnitude spectrograms and just say if we separate the choice of unsupervised approach, should we also consider phase spectrograms? Or just the phase both magnitude and phase together simultaneously if we want to build multiple request models? To build multi-request models? Why shouldn't we just build? I actually put a poster in there somewhere, but is the phase not important and why shouldn't we consider that? Simultaneous quick managing. There are other... Okay, go ahead. Phase is important, for example, when you want to investigate wavelet coherence between two glitches, and the phase can tell you a sort of delay between glitches. Delay between reaching one channel and the other. So don't scatter fades. And also for a bit of a bit of strategy in the tense here, as usual, as we should as well. Sure. But it's enough characteristic features. I think one of the limitations is just the processing component, as well as the fact that there's As well as the fact that there's, I think, like a specific terms of coupling that can be of interest, and you can kind of build connections between them. But for like a single glitch, I think for early phase, I guess like we marginalize over the phase. That being said, I think there's definitely something to the idea that spectrograms, while really natural ways for humans to look at the data, because it allows us to deconstruct everything. Construct everything, like there's obviously more information in the raw time series that people can extract and tells all the algorithms too. So I think designing something that is going to be doing better than us, I think having the constraints where we basically have done data reduction to make it trackable for humans, maybe I think something that is maybe harder for humans to double check, but maybe has more potential. But yes, we'll attach to it the one. Yeah, I mean, it's just a question. So, if you don't include phase information, like that effectual, like, if you turn to subtract it, I'll just add. Well, certainly for the subtraction, you need I'm just even just talking about the characterization point of view. That's you know, you see these space magnets. Maybe a computer has. I think there are cases of bleaches where because it is important. Even looking at a single bleach, for example, in talking about glitch, glitch is something pretty easy. It's just, I don't know, in the time domain, it's a delta or a stab, which is like a delta thing. But maybe. But it may make different if the delta is upward or downward or if it's at value. This can tell you information on the origin. For example, we have seen Virgo, I'm talking from Virgo, glitches in some magnets attached to the mirrors where a burst of electricity, well, it was very short, small electric signal, but enough to keep the mirror always in the same direction. In the same direction, and knowing this was important. So it's not just the shape of the bleach or darn the whatever was important to have. Looking at the time series, we have been able to see the HFT. Oh, looking at DART. Okay, DART is more important, the differential abrad control. We see the, you can appreciate the delivery, but the change, the delta L over delta. The delta L over delta T from the kink that you see in the time series, from that estimate the current needed, the capacity of the coil. So and that will point you to where the source is. Um I think From another perspective, I think it's it's very but I also think that the way that we understand the world can be different to the way that a machine learning algorithm understands. So the way I understand the dog, it's because the dog has ears and it has eyes and lungs, but maybe the machine learning algorithms come. The machine learning algorithms come up with another representation of what it does. So the fact that we can understand what is going on doesn't mean that the machine learning algorithm gets all the information that it needs. So we could do both things at the same time, right? Like that's how this glit is with the frequency components that it has. But also we can think of But also, we can think of other ways that the machine learnings are better informed about the things that and we are talking about which classification, yeah. But I think it's the point of the classification is that it's like a first step that's a simplification of the problem that we have. But this sort of ideas can also lead to eventually some, for example, explainable. Some, for example, explainable machine learning. I wanted to mention a neat paper by Areksander Vonderescu that came up, I think, over the summer that did some of this semi-analytical modeling. And what they showed is that, so like Derek and Francesca have been saying, there's not a lot going on in face in the sense that the the short butchers tend to be very simple, where they do this, or they do ninety degrees of that kind of degree. Degrees of that flavor is the capture. Yes, it's very important for characterizing and subtracting, especially modeling those glitches accurately. You want to think about building not a defining feature of the glitches much as frequency, duration, and some rather practical. Similar to what Marco's suggestion, I think if you look back at the literature that we have on glitches, I think some of the main areas where there's a wide variety of robust methods we've put together is on the first, the classification. I know there's lots of these that have been around for a while. Similarly, there's a lot of really great edge replication tools. So that's something where there's kind of just pick whatever your favorite algorithm has. Whatever your favorite algorithm is, as well as correlations when it's either straightforward or we have all of the data. So I feel like the biggest questions that are open right now are how do we how do we deal with the lack of information or the more complex correlations across the detector? Across the detector. So I feel like the straightforward analytic correlations, like Geboe was talking about, we have pretty great tools there. As well as just looking at stray data, we're very comfortable. But it's that big dimensionality missing dimensions where we really need additional research. Wondering if there's some possibility of using machine learning to try to find sub-breakers? Subreads are interpretable. I don't know what that would actually look like, but I'm looking for some combinations. So, I confess my partial ignorance of machine learning, modern machine learning techniques, but what are the problems that every time I focus on data machine learning? Topic on databases. The moment where they run away screening is when I give them this number: 28 megabytes per second of data. They say, well, there's no way we can process all of that and get anything meaningful out of it. You need to reduce it. Give me first a subset of this data, and I'll show that my algorithm works. At the point there is the interesting thing as along the line of what you guys were discussing, not only. Of what you guys were discussing, not only for glitches but also for other noise, is that we want to discover the correlation so that we don't have to classify glitches, but we have to predict them. So we can look at everything but the strain and say, hey, there should be a glitch there. And yes, there is a glitch. And then we call that a glitch. And then we can subtract using all those dangerous techniques that subtract using the strain itself, which always recognizes kinkoal. Because unless you know that's a glitch, you are deleting all. You are deleting all your new gravitational wave signals that you generous model. Maybe you're going to say the same thing as me, because I was going to say we used to do this with principal component analysis, so we need to get to find some correlations before we go to techniques, right? I was going to say something different about my critique of classification methods, right? It comes from the fact that I saw. Comes from the fact that I saw so many papers saying taking a chunk of data, training the machine learning, and the final result was: okay, we found six different classes of bleach. We found five classes of bleaches. Okay, good. And then what? Right? So there is a next step. I think we have explored very much the classification of art. Now it's time to. Now it's time to, you know, say, try to find correlations, try to subtract those bleaches, try to, you know, I understand okay if there are three classes of bleaches or four classes of bleak bleaches. If I don't know where they come from, I don't think it's useful for for commissioners. Yeah, and thank you. That's exactly what I was about to say. The point that is important from my point of view is that uh From my point of view, is that whenever we find a class of glitches, let's classify them, but then come back to me and tell me, hey, I found a correlation of this glitch with a microphone somewhere. Because the discussion of what the two talks this morning after mine was making it very clear that it's really hard to subtract glitches. It's really hard to get them out of the data. So, the best solution for every part is to have less glitches of detector. So, every time we find a new class of glitches, we need to be able to. Of breaches, we need to be able to understand the physical origins of somebody can co-direct things that is possible and feasible. So that's an important next step. And possibly not at the end of the run, right? So there were five different classes of bleaches at the end of the run. Regarding that point, I have a possibly really naive question. But do you have to do it at the full scale of a Liver detector, or can you use A liga detector, or can you use Geo600 as a prototype to try and do this and reduce it? Every single detector is different. You can't even, half of the time, you can even take what you learned at Hanford and put it in Livingstone. Hardly any time you can translate something from LIGO to Virgo. Systems are nominally the same, but particularly completely different. Bring back H2. I'm not gonna. I will correlate that also the same effect. That also the same detector can differ from month to month due to various circumstances, changes in the working point of some control need, changes in manufacturing of some apparatus. Well, I think regarding this I think this is like a very important point because um like during the Like during the face-to-face and versus one, um, like he gave a presentation about T-C and how they observed like the evolution of glitches through time with TC. T-CNI is like very standard, well, let's say, generalization of PCA. But I think what was super relevant there is like There is like how, like, how differently a machine learning person will tackle this problem rather than a person that actually works in the texture characterization. So, having this view of, well, actually, we can look at populations of places through time, that adds more information that we are not adding per se. And I think that's super interesting, as well as some other data or presentation can inform better or much learning approaches. Just a sort of side point, but very I think relevant to the last point just made is. So if I want to try and think about this problem and I want to try and approach this or give it to a student, I want some sort of toy example that I can build up, right? And I don't quite see if you're going to just say that it's all just data there, 28 megabytes per second. There must be some smaller representation of just the number of Representation of just the number of senses you have, but I can then start by. Yeah, if I can just answer these. Yeah, sure. I mean, there are, I don't know about glitch, I'll give you the example of noise subtraction, I'll do more on that. So there are examples of noise subtraction that we successfully did, both linear and marginally non-linear. So a toy model would be to just, and I tried that with several groups, to say, okay, I'll give you the strain data, one hour strain data, I'll give you 30 channels. You 30 channels. Only three of them are relevant, which I know which I are, but I just blind the renaming of them to see if your algorithm works. So that's what we can do as a proof of principle. So what typically happens is that I never heard back or never heard back. Or other times, you know, yeah, it works, and then that's the end of the interest because you do a proof of principle, but then either. Proof of principle, but then either the algorithm is not really scalable or it is scalable with very hard effort, a lot of person power, a lot of resources. Because going from 30 channels to 4,000 channels, I don't think that the amount of effort scares anywhere near linearly or power low, it's more than it's like exponential. I think you were first there. Um, so how many of the dead carbon here? How many of the dead car holds here? Part of this problem also stems from the fact that people who are trying to work in a debt car are trying to work with an online debt vectors and the channels that they are looking at. And trying to blindly look at those 25 megabits per second here without knowing what are the development things that you should be looking at. The dimensional reduction. The dimensional reduction also comes from the fact that there are so many different channels. We, the commissioners might know, there are these channels which should be affecting the length. These are these channels which should be affecting the angles. There are these channels, whenever you see glitch in DAR, you're likely seeing some glitch in either the pop channels or the REPL or one of those channels, right? The fact that how many people who are looking at this data binding trying to just put portion of Trying to just boot pushing push machines. Will that affect anything? What the channel setting is? Well, I can respond to that, but you have to. I was going to talk about the toy model. Well, as a response to that, I think the Dead Sharp people know very well what are the right channels to look at. But that's a good thing, it's also part of the problem. The reason I'm the one who brought there, we have 20 megabytes per second of this. We have 20 megabytes per second of data because we understand some of the noise coupling, we understand that environment couples or detector, and we're fairly good. I say we actually, the Decharg group, they're fairly good at monitoring all of those channels. The problem is that I want something that can discover things that we were not expecting, that we don't know. So there is a part interest of just throwing some general purpose data mining. Purpose data mining, data crawler, I don't know what you want to call it, that is able to find stuff into that big haystack. Yeah, but actually, really on that point of discovering things that we don't know, I think that's also a big limitation in creating these test cases. In some cases, um, it's we just don't even have test cases. Like in glitches, you know, if we don't have when we have generic modeling, that also means it's very difficult to It's very difficult realistic toy models to work with. But also, like in the example like I was giving, you're using cases that we successfully called solved with analytic methods at the base, then you're kind of constraining yourself as reproducing the same type of problems we've already solved. So they're not actually attacking the things that we can most help in. Alternatively, I think it's really that we don't necessarily know maybe good to say this, but the right questions to. Everybody just say this, but the right questions to be asking. Like, we don't know what you should be modeling for these couplings. It's just something that is a completely open-ended space. So if you have the question of we know how to do something that we just want to do faster or more efficiently or something like that, I think that's something that we're well suited to support. But this open discovery that's a lot of this commission hard to put into a simulator. put into a simulator. So comment is similar for that just set to one of your yeah you said that the people that analyze the data don't know what is this data. Then I'm question do they really know the problem and what is the problem either? Because I'm saying if you don't know what you are analyzing the data where that you want That you want to mine, also, expected that you have not understood, and somebody had not convinced you what is really the problem. So I think that the approach, the result, are highly effective, in any case, the supervision of expertise. I had a general question. Has anyone tried? Has anyone tried for generating like using diffusion models for classification and then generating draws from that diffusion model? The follow-up question to that would be, what outcome are you thinking of that? No, I just want to address the data like more articles, which is here. I want to be able to get a posterior predictive distribution on the Predictive distribution on what glitches look like and then be able to draw from it. And I want that to be not be a generative, I want it to be a generative model, but I don't want that to be something that I can write down and it can be able to. With regards to the compression of the 200,000 channels, is it? Let's say 4,000. 4,000 fast, yeah. 4,000. 4,000 fast, yeah. And related to your question before about human sort of experience and knowledge of those things, what is your guess at what fraction of those channels are actually going to be important? That would then guide, it would sort of say how effective machine learning compression could ultimately be. Well, that's a difficult question to answer. So To answer. So it depends on the time scale. For every specific glitch or every specific instance of noise, two or three will be relevant. But those two or three will change for each single glitch. And I think there are probably a thousand classes of glitches. So I would say I wouldn't feel confident of saying I can exclude half of it and make sure that I throw away data that is not relevant. So maybe 50%. So maybe 50% of those, which is, yeah, doesn't change your life. Yeah, maybe one of the other big challenges of the data reduction and also information is wide variety of problems. Maybe like a good example of something that I've been working with some people to track down is that where we're trying to find noise, the source of noise that we think is the way to. Source of noise that we think is related to the temperature and air conditioning of the sites. So basically, you usually don't track whether the fan is on people's office and stuff like that. It's actually a common public thing. Or have an understanding of which air conditioning unit is tracked by this set of channels. So for every individual problem, there's this new set of XL channels and a new set of channels that we're going to have to do the dimensional direction on. So I think really this highlights. This highlights that I think a lot of these are very interdisciplinary questions that are going to need a lot of people coming together to try to solve. I think that's like any team that can bring both of those types of knowledge, on the data analysis side and the instrumentation is going to be successful. I think it's very difficult for someone coming from just one area of knowledge to solve these types of difficult things. So, to jump to people back, to answer your question, we did not use diffusion models because back then they did not exist. We used generative oversight networks, either gamely, tongue, that CDB GAN, but we use essentially web licenses and talk team. Because the truth is we we were trying to answer well when you do generative URI, the answer that you try to to solve is Try to solve is what is a glitch, right? And for that, you need to extract your glitches, and then again, using base wave for us, it was very expensive, so that's why it was very limited. So, that's what's essential with it, but I think there's also a lot of consumptions involved. Patrick? Yeah, I'm going to ask a question that's sort of not actually showing up there. It hasn't occurred to me, so I'm curious. We actually have a lot of We we actually have a lot of digested knowledge that is available in documents, inside document control centers, that's in A-logs that are associated with all of the detectors, where experts report on investigations, changes, and such things. Has anyone thought of retraining some of the LLMs or small LMs to LMs or small LMs to actually digest that and use it as an export chatbot for people who want to figure out what they're doing with these big 28 megabytes per second of data. So there's this person called Nikola Mukonos. So he's an MIT postdoc, and he, in the last ML face-to-face in Barcelona, he gave a talk about large language models for Models for what is called LIGO GPT. So he also has a pre-project about text binding that is called Hey LIGO. Right now it's working for LIGO specifically because something wrote for Google and makes sense. But essentially the whole point is that when you ask something to Judge GPT about what you're doing, there's a problem with privacy. Even if you pay premium, can you actually trust that your data is going private? So I think that So I think that large experiments such as like a house or three people, we should start thinking about using these tools also for education. For example, the texture characterization could be a very good point to do the storage of things. But yeah, so people are thinking towards the storage. The one positive thing to look out for, if that is something that actually tightened production, is just reteaching. It's just large memory and their genes require it. If just a few people are asking questions every few minutes, like probably do that before we do. But if the more useful it becomes, the more it gets out of hand. So to answer this, I did talk with someone from Google at some point and I had the same question about activities. Same question about activity itself or all these other things. And what they said is that essentially what is very costly is the training itself. By sustaining the API, it's nigh. And essentially, they said something like, I don't know if this is realistic. For example, like one day of Instagram is equal the consumption of what ChatGPT had done so far from existence. But again, this is something that people will need to touch for sure. So I have one very stupid question to me. We have been talking about subtracting dwitches, but the problem is subtraction of which is may produce artifacts which may be completely non-final. For example, which you see for 129, we saw that it might show up as PC1 and stuff like that. Pieces when I'm stopped like that. So instead of focusing on subtracting out bleaches, why not focus on methods that can actually do a much better job? For example, there is this score-based inference method that was introduced by the flat iron group recently. And then there is simulation-based inference, which plot also would not depend on the noise. But currently, the way it is trade itself depends on the noise. So measures like these can actually do a much better job rather than trying to subtract a big. Than trying to subtract our features. The reason being is can introduce non-prefer. Does anyone have any comment on that? I think that there is there are similar problems if you if you have a Have a pool that can't, like, there's going to be ultimately systematics included regardless. I think, like I suggested in the talk, that seemed like a really fantastic idea to have these types of, of doing the same type of modeling of glitches and the signals at the same time. But I think there's the same type of issues with generality. Generality that you'd have with just a glitch-focused approach. So I know what you're referencing, and I thought that was really exciting to see. It would be great to try to explore that type of approach more. But I think that's the big challenge: dealing with the wide variety of different glitches, and even if you're Even if you're necessarily training, would we have to basically retrain a model for every single week or every single day as the new types of glitches come in? Can we actually develop something that can be robust to a wide variety of glitches? I think that's the big challenge. So, yeah. Just a quick comment on people mentioned retraining the the Retraining the burden of retraining every day. But if your model hasn't changed that much, the retraining cost is minimal. Was it also about training? Well, it was about transfer lear this idea of transfer learning and the lack of expense. It's not it's not expensive. It's um I mean I showed some stuff earlier. I mean I showed some stuff earlier where I have simulated data and that might take just say thirty epochs to train what we're talking about with six minutes per epoch. Transfer learning, when okay there's an imperfect application in a real case, model converged at the tree ebox, which maybe that's a big deal for certain things, but of course make that faster and yeah, I think it's not Well there are definitely ways to do things like this in a fast way to keep hope of data. And I thought up something about continual learning there, which is a pretty big area of machine learning at the moment, but it's getting bigger. And it seems very suited to learns continuously from data streams. And if you can incorporate the right ideas into, for example, learning PSPs, learning what's normal and learning continuously what is deviating from this normal. Model that could be another alternative to transfer learning. But again, even with transfer learning itself, I wouldn't be too afraid at this crazy additional cost. You go and then I'll go because I want to go back to what we were talking about five minutes. I have a different question than you are, so maybe now. Uh so suppose I wanted to Uh so suppose I wanted to give a this I I was emboldened by this idea and I want to give a perfect twist to some clients. Um and I know like we don't know the correlations at the moment, etc. But I wanted to generate data which has only got glitches in it, but it also has the 4000 sensors. What's the tool I use? I would argue that it doesn't make a lot of sense. So we don't have a So we don't have a. If we had a model, think about it, if we had a model that could predict the 4000 sensors and the glitches, then we would run that model through the 4000 sensors and we would have solved the problem of predicting the glitches. I'm slightly confused about this because if you don't have a, if you can't even fake the data off the sensors, how are you gonna train them? At what point? Yeah, so some of us have made actually machine learning algorithms that can make glitches and then I mean you could just take them, you could take the data of existing panels and just add the glitches in there. Go first because I'm the party pooper, so. Yeah, I think that aside from generating letters in H of T, that they are doing essentially like a digital twin, which is essentially that you take some observed channels that you know where gluces are correlated, for example, try to simulate less machine learning of those things. Maybe it's Um maybe it's it's maybe it's not like like like super well, but but it's a starting point I would say to kind of generate this type of data. But I think I have to comment on that. I think I would really I think it's important uh that we move in the direction of looking at the auxiliary data to predict that there is a glitch in the H of T. glitch in the H of T. But we need to move away from the one-to-one glitch, meaning that we don't want to look at a certain number of silly channels and say, oh, there is one glitch at this time with this frequency and this magnitude, and then at the same frequency and correlated magnitude, they get a bleaching measure of T. That might be the case for a class of bleaches, but we know for sure that we have For sure, that we have several classes of glitches that are related to completely different behavior. For example, when one of the angular channels has a bigger scars out of zero because of wind, because of an earthquake, something like that, then you get closed saturation of some actuate of some way, it's a highly non-linear process, and that going far from the from zero trigger or generate a glitch. Or we had in the old days, we had our DAC system, which our digital error converter. System, which are a digital error converter, that would create a glitch every time you cross a specific level from the way that we're built. So that would be something that is capable only if you have a system which is smart enough to say, I have a glitch in HLT, and I'm not looking for a glitch at the same frequency, or even a glitch at all in a zero chart. But I have to be smart enough so that I correlate the presence of a glitch in HMT with a level crossing, with an excursion from zero, with a deviation from statistics with higher velocity. Deviation for statistics with higher velocity, higher derivative, all kinds of things. So that's probably where we will find a lot of reasons, a lot of causes for the switches. I wanted to go back to something that you said 10 minutes ago, when you said that you can give one hour of data and that 30 channels, and either you don't hear back, you don't hear back, or you hear a model, and that's the end. Or you hear a model, and that's the end. So I think we started in Marguerite Virgo doing machine learning with Liches probably 15 years ago. I just finished with Chris, Sion, Elena, a review about machine learning and gravitational waves. And we have how many references, how many papers we found. I mean, there is a huge literature now, but very. But very few models or algorithms ever get used, going to production. And what I'm hearing here is a lot of ideas for student project or let's try this model, let's try this model. And I know that I have the sense that we'll develop all these different models and they will never produce. Since there is an expertise in this room, Since there is an expertise in this room, can we choose a pressing problem that we have? For example, detector characterization, noise reduction, and so on. And all work together with the machine learning group and try to solve that problem and bring it to production. Maybe to you. Maybe two utopia and then I think that's the problem with like how the things that jobs write because we're always getting told oh you've got to write those papers so people want to people write methods papers because they get jobs but if you run a tool in production it's a bit of a thankless job where it's a lot of work and you don't have unless you like you go and get a letter from Patrick or something you don't really have any evidence for your job. And then you don't really have any evidence for your job applications. May I answer briefly to that? I don't think that writing a paper or a new method that will get five citations in ten years and then will never be used in the collaboration is going to help you with jobs. I think it helps you with jobs more if you say, I was a part of a team that solved this problem and now we can detect that 20% more BPHs. And more BPHs, or because without methods, we solve this problem, we make the detector more sense now. I want to give you an early gift from talks tomorrow about algorithms that are in production and running and even only training on all through data. No, what they're saying is that there are algorithms that are machine learning based and they are in production. Than the guaranteed action. But if you look at the ratio between R and E and what eventually gets at the end of the production, it's minimal, right? That's true. And I will maybe also add to that that one of those took, asked Mervyn about this, probably about two years to get from the algorithm and then very minimal changes all the way starting in production. So it's partially. Partially my questions but answered by some of the remarks, but just to ask a point: what is getting in the way between this basic research demonstrating potential utility and every as a functional project product. Now, I can definitely know like me who are a skeptical of Michigan Murphy. We're not skeptical of machine learning, right? So we often get in the road in terms of process and other things. So I can imagine that that is true. So we're on now to confidence people. Are there other things that show? Mentioned you did the review, so you've actually opened up the data and you have to know what's there. I actually have a question. Have a question. I think it's pretty much in line with what you said. Like, when you're in early career scientists and you have like four years of PhD, let's say that you're very good, you create your machine learning algorithm, it does the proper tests that you have to do, but then at the end of the day, you graduate, and maybe you don't even continue within academia. Not so many talks for everyone. So, at the end of the day, how can you, as an early career scientist, build a team around something that you A team around something that you believe that you think it's important that you have shown some results about it, that it continues throughout the collaboration. How can we do that? It's a generator. Yeah, so I guess in terms of our use of all these things, I think there's specifically on instrumentation and checker errorization, I think there's far fewer blocks than many other areas of the science to actually get things that. Science actually gets things of use, but at the same time, there's a potential for limited scope because I think, like, you know, there's a lot of constraints that we put on a lot of graph physical analyses, but it's basically free to write what you want. So, if you have a machine learning device that has instrumental knowledge, I don't want to offer up the a lot necessarily, but I mean, like, there's also hundreds of specific detector parasitization problems that we're Specific detector parasitization problems that we're looking for people just to tell us answers on. So I think there's definitely a forum for people to voice their solutions that people are welcome to hear them. One of the other kind of big challenges that talk about the best thing is to solve the problem and get rid of that type of glitch or that general noise. A lot of these solutions are very tailored. So once you have created this Once you have created this pipeline that can solve this problem, you've solved it, maybe the technique is more generic, but in a lot of cases doesn't have as much use because you're doing such a great job in solving the problem. So there is, I think, that is a roadblock, I think, to having a long-duration projects in this type of instrumentation, unless it's something that's really generic, like a Bruco or a simulation folder that we can build something around. Once we figure Once we figure out why Livingston is going up and down by 20 megaparsecs, I don't know if we'll need to use it again. So I think that's kind of one of the pros and cons of this in terms of access. I think the person pair is another roadblock in making production because if you're like a small group or just one person who comes up with a new method, then you're not enough people to run that tool and produce. People to run that tool in production because you can't run a pipeline when you're just one person. You can't speed it 24 hours a day sometimes for a call on holiday or whatever. But if like usually the things that I see ends up doing production when basically like a faculty member who has several postdocs and several PhD students, and then they have a whole group of people who can make production or multiple groups that team up together, but like if me and I have one. Like, if me and I have one postdoc come up with a new tool, it's very hard to have enough hours to work on it. So, speaking of roadblocks, if I may be blunt, right? So, one reason is what Jade said is often lack of manpower, especially in groups and so on. But there are other two reasons. and so on. But there are other two reasons that I saw over the years. And the blame is spread all over. One attitude of many people doing machine learning research is that they develop a new method, they publish a paper, they feel good, right? And then they move to something else. And then they assume that because they published a paper on CQG, machine learning techniques or whatever. Learning techniques or whatever journal, and then people at the sites or their chair or instrumentalists should go take that paper and implement it, right? That's not the attitude. So that's part of the blame. Another roadblock that I observed is that often, and this happens with young people, develop these maybe worst methods. Methods. They present it to our working groups. And there are working groups and the senior people saying, oh, this is very interesting. Oh, this. But then we keep doing what we are doing. Maybe with good reasons. But there is a little bit of lack of support for trying to, especially for young people, to make it. Sometimes just the word of encouragement. You know, sometimes just a word of encouragement and say, okay, you know, this is interesting. Now go and talk to Dabriella because he might have good input for you to bring this to the next stage. That's never going to happen. I see often this attitude saying, okay, very nice, right? But we have been doing machine match filtering for 30 years and keep doing that. Maybe it's a little different in like the areas like parameter estimation, what I've seen, but I think one challenge is that people come with an idea, and to be frank, when it's first presented, when it's first going to be published on, is at a way lower level. And so you get a paper, you get a presentation a week on a new ML, whatever method, and all of them are far less promising in a broad sense. They might take a specific piece. Sense. They might take a specific piece and perform well, or whatever. It's very hard to know what these things should be then elevated at that level of the working group. And in fact, I don't feel comfortable anointing any particular analysis. I think it makes more sense to say, thank you for reporting to the group. This is very interesting. If you want to take this to the next level of production, these are the targets. And talk to us, right? But to then say, all right, everyone in parameter estimation, this is the hot new thing. This is the hot new thing. Let's get this pushed through. No, I'm not saying that, but even what you just said, that often it doesn't happen. Right? So, telling people what to do to bring it to the next stage. I'm not saying that somebody shows up at your call with a new machine learning and you you all jump, but I don't say that. But often I mean, I I often saw people dismissing People dismissing these things, right? Right away. Maybe it doesn't happen. But also, people ask hard questions sometimes, right? Like, have you tried this? Can it do this? Did you do this task? And if the person says no, maybe that seems discouraging. So I think all of the working groups have had, you know, experience. You know, experience and maybe having some people, like its own little dedicated working group that's just dedicated to helping push things to production regarding like it's been done in a few cases, and so some of those people could share their expertise. Like, if there were actually like there were some actual resources. There were some actual resources generalized across the collaboration. I don't know what this would look like, but I think maybe having some general support. It seems like we all have this problem. And so if we all contribute a bit of person power to a shared resource, that would in turn like us more projects actually. More projects actually you get part of? I want to go back later on to what Derek was saying and adding on that. So, in the debt chart and even more in the instrument science and all the stuff that presented, we don't quite have this problem because there isn't really an established solution for a lot of things. So, I really encourage any of you, if you have students, there are people that want to work with that, come talk to them, talk to me. We have problems for you. We have problems for you. The only issues there is that I speak for myself. I will not allow you to develop your own tool for the sake of the tool. I will give you a problem and I promise you a hard problem and then we can work together to solve that. Then if you have the right tool, great. If you invent your new tool, even better. But you know, there's no existing solution. So anything that works works. But the difference there, what I've seen many times is that even together. What I've seen many times is that people get discordant with a heart problem. Well, yeah, they go hanging for foods that are long gone. And all people come with their own tool and they want to solve their own problems. And if you come to me and you tell me the problem and the solution, I'm interested only if I agree with you that the problem is interesting. Not because I want to diminish your idea, because I have a finite amount of time and energy I can devote to that, right? I just wanted to add more wood to the fire that Marco started playing. So something also when I started, for example, my PAC, one of the products that my supervisor gave me, I took two months to just understand the problem from the machine learning side. So then it was very hard for me to understand the problem itself. Coming from another field, I had to train myself on this. Field, I have to train myself on this. On top of that, at some point I understood the problem, I solved, I did a first proof of concept of a solution of the problem, and then I went to present it to someone that was expert on this problem, and they told me, well, your axes don't look quite right, because apparently, if you study a problem for 30 years, you have a way of expressing your results. And that's something that is not standard, for example, in machine learning. So having that connection. So, having a connection between these two worlds is highly non-trivial, especially when we are just starting. And I think, again, following with the previous point, having like anything, like, for example, I was reading Ph.D. thesis, trying to understand how to present the things. Because at the end of the day, they are standard things that people are doing. But coming from the other side, it's not a standard at all. So, we need like literature, we need tools to actually. Like, like, tools to actually understand the way to present it, the way to take it to the next level. It could be talking with someone, it could be maybe a thesis, it could be anything. We need something. Let me say, Gabriella, you said, what you said is very true. I think we should identify problems first. Move from this uh tool ba development based attitude to a problem solution attitude. Problem solution attitude. So try to identify a problem and then we can focus on that and try to solve it. But historically, I think the machine learning in LIGON Virgo has been the opposite. Take a tool and try to apply it to something. I think we should. Yeah, so one problem that I'm worried about. Problem that I'm worried about is that if we get a signal that's not on compact binary and that it has a glitch overlapping with it, but because we don't even know what the signal is supposed to look like, are we just going to think that the glitch is part of the signal and then mess up by our astrophysical interpretation? Like if I just use supernovae as an example, because I know about supernovae, if you have a glitch right at the start of the gravitation away from the supernova, you might think that's like a big spike, means the star is rapidly. Spike means the star is rapidly rotating when really it isn't, and that could get messy. So I knew that it is both a collection signal. But it's a different question. So I don't know if I want to, if someone wants to say something about that, I mean, this would be the benefit of having more and more detectors going forward, because it's good and look. Because it should look similar, I mean, in multiple detectors, right? And I guess, you know, having more than two detectors online is like benefit in this case, of course, it'll make the primary estimation you do. I don't know if you guys do it all day, but like, you know, there should be a way, and hopefully, when that time comes, a better understanding of glitches will be developed. So, hopefully, we will have. So, hopefully, we will have better tools. Maybe by analysing artillery channels, we could draw good correlations there. We can have a way, yeah, it's never going to be perfect, right? But we can have a good way of at least getting a good approximation of what it should look like and get the glitch out of there. We've seen this glitch before, and you know, hopefully as the technology improves, hopefully machine learning should offer a way. You know, but I'm not even doing all that, so I think anything can be done. So, I think anything can be done to a question now? Yeah, so I've got a so coming back to what Marco said and what Gabriel you said as well, and I'm a victim of what my background is, so you know, feel of the question and response to this as well. But I'm slightly confused about what you mean by problem-solution as a pathway to sort of adopting a tool, because, for example, when I think about parameter estimation, and I want to When I think about parameter estimation and I want to validate something, it's really easy for me to generate an adequate representation of HFT to then be able to use the tool to then understand whether something is working and then what do I need to change. But at some level you said that you can't even generate, tell me what the simulated data of 4,000 channels looks like. So what w what am I supposed to do to try even a tool? How do I approach the problem? Different problems at different Different problems have different ways of being happened, right? So what I was just saying is that we should identify a problem and try to solve the problem. I'll just start off. So I think one other, I guess, a suggestion. One other suggestion is really it's an open exploration component where you need to create your own data sets that could, you know, that's hopefully informed ideally by some type of physics about what you think might be happening. Like Gabriel was talking about the simple quadratic correlations that people are nonsense, and then that was the ended up being a good model. Up needed to model. What I would might suggest someone take a look at this is you start off by assuming some basic correlation, seeing if you can get that to work, and then double check to see if that matches what the real life solution actually is. The big challenge is that 95% of the time the process, because you're just kind of guessing on the problem, is not going to result in the solution. So then you have to redo the entire process by that. Process by that. So, really, I think like my, like, that would be my approach at least. So, I think it would be much better if people have these types of, I guess, if we can bypass your problem and not have to come up with that training, if we can somehow have these more generic ways to attack the problem. But what you just said is what I'm looking for, right? Like, some level of, like, all right, this is the type of correlation between channels I can try and mimic, and then I could see. Try and make, and then I could see if I, but like, I don't see how that's any different from how you would approach any other machine learning problem. Like, you have to try and first try to generate some data. I guess the challenge that I see is that it's a very large parameter space, and therefore, there's like when you ask what is the training set, like your guess of a functional form is more or less as good as mine. So, that's kind of the answer to that. You might as well, yeah. Let me jump back to what you said, Gabriel, about project for student form and what also Marcus said. Because maybe I want to ask for your feedback on this feedback from people in writing for the MOOC. So in working groups, which we are way understaffed. Are way understaffed. We have much less resources than what we should need to work properly. I have the feeling that the approach is that every student that comes, I would like to contribute to some extent, even developmental. We cannot reject one person. So but even even if it's person ends up doing something for themselves, like uh developing their tool, uh developing their own tool, developing their their own problem. So do we do you know any way to, you know, steer or I don't want to say force these people to be more focused on the actual problem of the tech rather than doing their analysis, their own project and without rejecting them, of course. I mean, uh okay, rejecting is an option, but I don't think we are in that position for that sharp and general operation to one out. And general coordination going up. But I'll make a remark, which I think has already been made by somebody else. I'm not sure who it was, but I think actually it is true that there is a very ingrained sense that students have publishable, a smallest publishable unit that they can get together, deliver, and do themselves. And that's really. And that's really to our community. I see it all the time. So, in many ways, the only way to adjust that is to train actually some of the other folks to again go back to this point that taking the hard problem of that hard problem is really highly valuable. Really highly valued. And the trouble with that is it's not a visible return in academic community normally. And it's hidden inside of letters of recommendation. And it is there. So somehow we're not actually doing a good job. But the problem is, right, a student comes to me and they're looking for the quick fix. And they're looking for the quick fix and publishable item, right? Whereas what you'd like them to do is take the hard problem of the job and help and collaborate. Problem that we have don't have don't we don't don't we don't know the solution. It's not like an exercise that you can assign a student in your class. I mean, I want some student to fix the 25 minute pictures of Virgo, but we have not managed to do that in one and a half years. Actually, really the hardest talk is just on Patrick's point, I think there's a lot of challenges in terms of like the sociology and the requirements for students that make it hard for us to do these things. Like you say, put a student on a hard problem. What if they don't solve it? Are they going to get a PhD? Not everyone's going to award a PhD to a student who struggled for four years in a group of 20 people and then said, I didn't. Group of 20 people, and that's it. I didn't figure it out, right? And even if they make progress and we value them and they solve a problem, that's great. Then no one outside of the collaborations can hire them as postdocs, for example. Because what is the output of this person that they can understand and then themselves foresee this person as making a good contribution to their work? So, already we're kind of limited by the number of spots that we kind of have as. Spots that we kind of have as a community even offer in that sort of situation. So I don't have any solutions to that. I actually have a lot of sympathy for people that prototype on a simple problem that can be published, get themselves a few publications, and then if you're lucky and the method is promising, there's some room and some run-up to maybe convincing a few other people to help. And then maybe the most promising and fast-developing solutions end up in production. It's not a perfect system, it certainly doesn't. It's not a perfect system. It certainly doesn't work always. But maybe the flip side also is it should be maybe senior faculty that are tackling the hard problems that take five years to solve. I mean, obviously we have our own challenges. Can I get a grant if I don't do anything for three years? But at least I won't get fired. You know? Yeah, maybe then I'll go back to what Francis was saying. So I think an important part of our responsibility towards students is not necessarily to get them to get a PhD, but to learn what it means to work in a group, what it means to science and do research. In that sense, it is formative to tell a student, I know you would like to work on a nice and shiny and well-contained project, but that's not how life is. The students go. Life is because the students go on and work industry, work in any research environment, unless they found right away, they start right away their own groups. They have somebody who tells them what to do and they want to work on a very small cognitive machinery until they prove away. So learning that is also an important thing. And then we are digressing from machine learning applications here again as well. Just on that point, as much as it As much as it sort of paints me to say this, I guess, I think if that is the approach, if that is what we want the students to do, then I think we do need to shift the approach to something that is more tool focused rather than former focus. And I don't personally like that because I like thinking of but if if people like leave academia, then I think no one's gonna ask, is this the problem that you solved for This is the problem that you solve for the LIGO or Google detectors, but they're going to ask what are the tools that you can observe in my context. I think that's a hard thing. But Patrick, I think as a community, we should really change our attitude. I was. I was two weeks ago an advocate for tenure and promotion of one of my colleagues. Not Libra Virgo, not Gradation Waste, but the person also is part of collaborations, large collaborations. And I had to explain forcefully that the fact that the person was only the corresponding author. Only the corresponding author in the papers out of 50 that he has was how collaborations work, right? So yeah, maybe we should try advocate for changing this attitude that you need to have certain number of papers. But you you can get a PhD if you if you help solving good problem that prevents us in detecting the next general violation or something that kind, even if you never find papers doing it obviously very very fast but since uh the community is pretty But since the community is pretty large now and most of the people who will be going for jobs will probably be going for jobs within the community, is it possible for the community to have some sort of an open source journal or something where the bar is a little lower for publishing incremental work that doesn't necessarily solve the full problem, but you still start from the problem and go backwards, and some parts of it is solved, or at least some. Is solved, or at least some demonstration of their knowledge of this is visible. Mainly, so that if they have to go somewhere else within the community, then they can still show that they have done something that was peer-reviewed and publication-worthy, maybe not in a proper journal. And if they go to the industry, they also still have something talented to show. It's kind of like the journal for open source where people are publishing a paper about some code they wrote, which doesn't matter. But that's the bar is too high for bars, right? The bar for journal of open source software is higher on the software side than any other astrology journal. A little bit different topic, but Topic, but going back to kind of the focus on the instrumentation, calibration, characterization, and hiding. One thing I might say about detector characterization, that some of the instrument-focused challenges, I think, do have a lot of big challenges and difficult problems that need to get solved. But on the But on the, let's say the interface between the data analysis and the hyper recognition, there's a lot of these commentary publishers. I think there's a lot of low-hanging fruit here that people are just not addressing right now. So I think that there's, you know, I think in these really difficult problems are big, high-risk, high-reward types of things that we really, really want someone to solve because we haven't really. Want something to solve because we have readers, but there's also really simple things that can use expertise and a combination of different categories that we could solve. We talked about using, like in my presentation, like no one's done analyses of what a weak glitch might do on parameter estimation. I don't know if there's studies that know what should be the low frequency cutoff of parameter estimation. How does this impact lensing? I know, like we saw some. I know like we saw some really good examples in tester general relativity cases where they did a lot of comparisons of data quality issues on GR. But there's I think quite a big parameter space that we still haven't explored. But it does require people to get invested in the problem and see it through. So I think there's definitely space here for really exciting. I'm very biased, but I think there's still a space for some really exciting projects. Space for some really exciting projects that you can bring students and faculty alike to attack, but they're not, they are definitely focused on the detector and the noise and things like that. And I feel like if I had to say, why has no one done that, is it's it's harder sometimes, again, when you talk to other people, to do that connection to astrophysics. So if that's your key goal, you know, understanding an SNR4 glitch is maybe not as key as Not as key as doing a population analysis, but it's definitely something that's open-ended. So it's sometimes hard for me to say that there's just not work to be done different than some of the other activities that I think people have been focusing on. Actually, going back to the previous topic about how to pre-publish in a sense. So if we look at certain people. Certain people. For example, so in my institute, we are two parts, CW and ALICE. So, what ALIS people do is like their thesis is actually a book trying to solve a problem, and this problem is solved among 30 people, for example. And they do have these intermediate reports where they essentially say what they have done, what they discovered, and so on. So, even it's possible even that they don't actually have the short author papers. Have the short author papers at the end of the paper. Rather, like the reports extended in a book format. And again, they get hired because their community is bigger and also it has built over the years. But all those documents are internal. They are not peer-reviewed by journals or or open access and so on. They are reviewed internally and that requires like like other sort of infrast infrastructure. That has its pros and cons. It has its frost icons. Yeah, sticking on that topic for a second. I don't know if it's necessarily what we want to do in a widespread way, but one could imagine that students could write technical reports, upload them to the DCC. You could even send them through a PMP review from someone that you specifically asked: hey, can you review this not just for, it's not going to be a paper, but can you review this with an eye to? Can you review this with an eye towards the quality of the document, any mistakes I've made? And then once it passes review, you can make it public. I mean, so we have mechanisms for making these technical modes public. We've done it many times with quite important pieces of our analyses. So there is at least a mechanism if you're in that pinch of, you know, you don't have something you could publish, but you want to get something that people can look at. Yeah. Maybe along these lines, this nothing the collaborations could think about, but organizing special issues, journals, some topics. For example, I get emails all the time that there are these open source journals, right? That they want to. So there could be a special issue on problems in detector characterization. Detector characterization or technical problems in instrumentation. Then our people do development and look at these problems, right? These technical reports could become papers on these special issues. That would be right. So maybe that's something that even at the working group level could be done. Yeah, there's also a following up on that. I'm hearing the name. Following up on that, I'm here for the name, but the like young career acknowledgement thing that Sylvia asked for. That could be a good place to like review it. Or if you're hiring, maybe. I feel like it's uh partly because of a fundamental misunderstanding of how we work as a collaboration by some people outside. By some people outside. Like, I went to the last Gravitational World Physics and Astronomy workshop, and there was a discussion, and some people were putting their hands up and saying, why doesn't the LBK only met two postdocs to be an author on the next BNS discovery paper? And then we all know who to go and talk to about it. And I think that's because they genuinely think like two people did all the work and wrote all the paper, and the rest of us just did nothing with Mario or all the shit for free. Which is why, like, when we apply for jobs in Branza, they don't always listen to us when we try and explain, like. Is listening to us while we try and explain how we earned our authorship. So, I feel like maybe if we make maybe these kinds of meetings, if we're giving talks, that we explain better the process and the contributions that everyone is making behind the scenes to get to these papers, then maybe the outside world will understand more that it isn't just two postdocs writing all of our discovery papers while the rest of us do nothing. So, I think I'll just reorient just a little bit with a question. Reorient just a little bit with a question and see what reactions we get. I am you know the last day and a half I've heard really lots of exciting things being done but I guess if you look at if you look at machine learning where it is, what's what's the most exciting sort of result or challenge? Or challenge right now. What's exciting? Scaling up. It's an interesting choice, right? Scaling up models is the most exciting. I was going to say, like scaling up for ET and Cosmo Explorer, maybe. Okay, that's a different scaling up just in general to about a year. Okay, so the next generation detection. So the next generation detectors is really the. Okay. I think that a few groups can do parameter estimation in minutes, right, using machine learning accelerated techniques, some slightly different directions. And especially the ones that are using it as an internal accelerator for some Monte Carlos, that you know that the results are still unbiased. That's incredible. I mean, like, these are early days, and not all of these things are robust, extensive. Not all of these things are robust, extensible, they're not yet reviewed, we may not have all of the software infrastructure to put them into production within the LBK, blah, blah, blah. The fact that it can be done throws a gauntlet on the ground and also, I think, gives some hope towards the really difficult data analysis challenges for Einstein Telescope, for Cosmo Explorer. And I think it's also really cool because, again, people have figured out ways to internally kind of validate the black box ML. And I think one of the biggest challenges And I think one of the biggest challenges, which is absolutely fair, because I think people have to prove that their tools can overcome it, one of the biggest challenges is showing that your ML generated results are robust and believable. You have to answer that question. And I think people are starting to do that. Can I just counter that slightly? Because I work both on the VM side and the gravitational wave side. Gravitational waveside. I think we're in a very, in some ways, very fortunate position in parameter estimation where GR is and our waveforms are, I mean, there's issues with them, but they're really, really good. And I think I draw the parallel to Rubin is coming online next year. You know, the first image is out. The data alert stream for Rubin is, you know, you can think about that as a proxy for the amount of data we might want to handle in 3G here. And our models for almost every other transient. Models for almost every other transient path of one A is ridiculously crap. And we don't know what to do with them. I think there's like a little, I think there's an issue there. And I just don't want to sort of restrict ourselves to just thinking about gravitational waves and gravitational waves only in that sort of context. I can just chime in. I think it's all very exciting. It's all very new and promising. And I think that a lot of people with real hardcore, there's a lot of people with a lot of expertise in machine learning and collaboration. But relative to other fields where I'm making more money or something, that's where a lot of really good machine learning people will go. And maybe they don't have the expertise to really initially at least fulfill a lot of the physics requirements. But I feel like there's a lot of opportunity almost everywhere. Opportunity almost everywhere. I think everyone should be very excited about it. I certainly am. And yeah, I think it's really early days. I think this is only going to get accelerated and get more powerful. And as we all come together more, we can solve these problems together. I think there's no getting around, Machinery. We have to take it seriously and take seriously the problems of generation and O5. I think I'll find um we should try and vectorize and analyze on it. I think this is a fantastic speech to finish the session. Um I'm gonna read something about it. So I think this is a little bit before turnout. Yeah, it's very useful and very trivial.