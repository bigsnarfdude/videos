If a new moves, you should see the location move, right? These are also physically motivated in the sense that there is some underlying parameter time that changes and you know how the physics should move. And then you had the case of the Gaussian uniform where there's no underlying physical parameter and that's do you agree that that's where the arbitrariness arises? Yes. Okay. Yeah, and most of the case times where And most of the casines, when we are talking about our nuisance parameters and how they transform throughout the space, we honestly either do not know that it is even a Gaussian 2 uniform, we just don't know the shape at all. And we get the bit of the point of which was also raised yesterday of the two-point systematics, where you have two points and you really don't know how to assess which one is correct or how to assign the uncertainty, and then you get a large layer of arbitrariness. Layer of arbitrariness. And that this method, the last method that I showed, is not going to help that. Generally, we still say, okay, we just do something linear, because if we don't know what it does, we might as well do something that is computationally cheap and easy to implement. This actually goes to a question I was thinking about. Have you thought about how to assess an uncertainty due to the method? So you've talked about propagating the statistical uncertainty through it. What about uncertainty due to it? That is actually. That is actually included here. But it is actually quite non-trivial. I'm not actually sure if I can explain it without writing it up fully. It's a couple of pages. But it's, yes, basically you have to take into account that your underlying model is also making an oversimplification. And also, what I okay, let me see. See, try to explain this. Okay, so what I see here: if we just have two parameters, you can still kind of see what this interference term does. But if you get to several different, if you get to more parameters, these interference terms get infinitely more complicated and also start to become at an order which is not matching to the physics model that you put inside where you use the generation. So, there you get a choice of where you cut off the interference. The interference effects, and this is also again why you sometimes get in practice some negative values because you start cutting off places. But taking into account the uncertainty on that becomes quite involved, let's say. Yeah, if you go back to this uh slide where you have like the uh caution to uniform. To uniform. Yeah. So I think it's, I mean, it's easy to say that, you know, kind of this moving Gaussian thing, that the bimodal thing is wrong. But it only easy to say because you kind of set up the problem. So somehow I feel like it needs to be somehow informed by the type of densities you think are possible or like some kind of prior I mean there are physical processes that, you know, more from unimodal to bimodal than unimportant. Models by model and manipulation. You could have set up a problem where you say, okay, this is the way that I do it, and then the vertical interpolation would be correct. And so somehow it comes with our, somehow our assumptions need to be folded in what we think is possible. And so do we have a good way to express what the type of dense abuse is that we think. Well, that's kind of what I tried to say here, because of course I only took three. Of course, I only took three examples here, but I just wanted to show a little bit that in different situations you want to choose a different interpolation or morphing method depending on what you know the underlying physics is going to be. So if you know there's a varying width, you might as well do vertical morphing. If you know that there's a varying mean, you know you shouldn't. And there's other cases where you know some underlying Lagrangian model. Underlying the Grantian model, so coupling model where maybe going to more the later effect of the Grantian modeling, this might be more effective. Speaking of which, with vertical morphing, you're doing a good job with varying widths, right? Yeah. One could argue that the uniform is a quotient with infinite widths. Why doesn't it work not? It could. I mean, I think it would work in that case. It does. It looks nice. It does. It does. It looks it looks nice, no? It's kind of a disappearing peak. I mean, if you're considering it as a varying width where it goes to infinity, it stays at the same width there. While in my conceptualization of the problem where the Gaussian has a sigma that varies from a definite value to infinity. Yeah, but that's because you would set this up as a Gaussian to a Gaussian with an infinite width. And I set this up as a Gaussian to a uniform. Well, that my whole point. And what I'm my whole point is that above you're showing that Gaussian conglomeration does a good job. It doesn't do a good job. I think it's because it doesn't integrate to one. If you were forced to integrate to one, it would look like it was. It should be done to a truncated case. No, because it has the same width as four. I would like a result like the Bookmap Bride, where the width has increased, while you say that it is a good job, but it doesn't need a good job. The sigma is staying constant. By conceptually ambiguous, you mean it's just not obvious which is the one you are? No, of course, of course. But I actually have a question about these 15 points that you chose. I mean, 15 is the minimum you need. And have you thought? I mean, is it just too complicated to go to more than 15? No. Okay, so we have tried going. No. Okay, so we have tried going to more than 15. It's a very good question. So we've tried going to more than 15 points. The problem is that then, because we have our model that we input as the way between how you go from between the points, if then one of those additional points is not exactly where you would expect it in the model with 15 points, you would get a discontinuous point inside your continuous description. So you do reduce the uncertainty. You do reduce the uncertainties, but you lose the continuousness of the model, which is kind of one of the goals that we wanted to have: that it's continuous, because that also, the fact that it's continuous allows us to fit it. All right, go ahead. Sorry, Mike. Going back to your picture of the shift. So, I mean, the issue here with the rays is really just that if it has an infinite width, then If it has an infinite width, then you have to truncate the distribution to lie between A and B, the limit. And then you can have a Gaussian that's uniform. But if you don't truncate it, then of course it's got to be zero. Because if you integrate, then it will integrate to one over the whole real line. So you have to, if you're going to morph it, you have to morph it to a truncated Gaussian distribution. I understand what you're saying. So the thing is that what I did here. You're saying. So, the thing is that what I did here is very much, very literally what I explained here. So, if you now have only two points, and let's say you look at this same bin here, and you have this one and that one, and this is the case for all of our points here, they go to zero here, then all you have is two points on this distribution. You have some point somewhere and zero. So, it's not going to be a Gaussian which has the same cumulative. Cumulative distribution, like it's not gonna, it's gonna be truncated. Because they are not in asymptotic, so because these are not continuous distributions. Yeah, because you only have two points and therefore it just breaks down. If you would have a third point, and it is actually a Gaussian that is going from a very narrow peak to an infinitely infinite width, and you have three points as input to the linear interpolation, you would already get what you just described that you expect to happen. You just described that you expect to happen. So it's a little bit dependent also on how many input parameters do you have or how many input distributions. Because, unlike with the Lagrangian morphing, adding additional points to a linear piecewise interpolation always gives a better distribution, gives a better uh description. Well do you expect the same line at each point along your uh your x-axis? Are your x-axis? Piecewise then, your interpolation. Do you expect what really is twenty-five different? I mean, the method itself does not expect that. The method itself just looks at every single bin and just happens to do whatever it wants. So if you look at this bin here, it's just zero to zero to zero, right? It doesn't expect the same sort of polling distribution that you expect somewhere here in the middle. But would you expect it to change smoothly? But but would you expect it to change smoothly as you ch as you go along with X? Or could you would because that's the obvious way to gain information. I would. The model this this in this model does not. All right, thanks for Michael. Oh, I was just a psychopath. I mean it seems like there's like a general issue here is that these sort of vertical morphings aren't well defined if the distributions don't have overlapping support. Or if more generally one is nearly zero. Generally, one is nearly zero density, then I guess that will turn into huge errors. So, like in this example, the far right figure does not have overlapping support with the far left. There's just no. Yeah, that's exactly why you get these weird behaviors here, because there's, I always like this 2D plug where you have sort of an evolution over time, where you see that there's just no overlap. So, like the Gaussian to uniform, I would say, okay, yes, it's true the Gaussian has support. Yes, it's true that Gaussian has support over all of 0, 1, but if the value is 10 to the minus 12, it's basically zero support. So I also wouldn't call that a good method for morphing and rebuilding, like your example, and maybe I'll disagree with that. All right, so maybe we should slightly generalize our discussion on this one slide, but maybe Thomas next. Yeah, okay, so you mentioned continuity. Do you have differentiability? Looks like the piecewise linear is not differentiable. It's not differentiable. And the horizontal, I've seen differentiability columns. How does the Lagrogen work? Is it differentiable? I think it is differentiable, but I have to admit, I did not check it. I will check it later. And there was a big break? Yeah, it was going to be on that slide. Can you go back to this expert example with the three peaks? I think I want to generalize it with the whole thing now. I mean, the question is when do we really need all this stuff mostly? And I mean, that example which you showed me could be like a signal shape, right? That would be a Gaussian signal shape, and the Lusos parameter could be as an energy scale or something, right? I mean, you see this is shifting. But I think in general, we have like a two-dimensional problem here. And what is along with it. Yes, what is along the problem? And one is along with the yes, one is along the mass itself, and one is this along the Lucy's parameter, of course. And what we are often doing actually is that we are doing this interpolation, like I showed here. And like in the combined tool or other software, we are often doing quadratic interpolation or spline interpolation or whatever, right? And outside we are doing a linear extrapolation, right? And um but often we are facing the situation that we Often we are facing the situation that we, if we have Monte Carlo for the three letters of the Lussov parameter, separate Monte Carlos, that we have often only for the nominal Monte Carlo few statistics, and then for the variations you have less statistics. And I just wanted to mention that sometimes, I think in my view, we should attempt to do direct 2D analysis of fitting. I mean, in this case, it could easily fit, right? It could fit the Gaussian. And as some sort of looses parameter, the As far as Newton's parameter, the peak position, right? And this model building is a huge reduction and we gain statistically a lot. I mean, I think we should do more. I mean, we do do that as well. But the point that I was trying to raise here is that if you have a parameter in your model, A parameter in your model already, if you have quite more than one parameter in your model, then the nuisance parameters are often determined for some place in our parameter space. So let me see if I can take a picture. Okay, here. So if we have this as our actual parameters of interest, let's say, then we know what the nuisance parameters are here. Maybe we know it at one or Maybe we know it at one more point sometimes, but generally speaking, we don't. Generally speaking, we do the same thing in this one point where we have an up and a down variation of some parameter of the nuisance parameter, which is additional to this problem. So I think if you just have one signal parameter and you have the nuisance parameters, I think we all know that we should do a fit where we minimize all of them at the same time. Where we minimize all of them at the same time. And you saw it a little bit in the previous talk by Nicolas because he showed also what the values of all the nuisance parameters are for the best fitted value of our signal. But how to do that in a multi-dimensional signal space is a little bit more complicated. And I would love to hear some advice on how to do it properly, by the way. Don't think has advice. Yeah, yeah, well. I was going to make the point that, so I think Lydia here is really focusing on. So I think Li Lydia here is really focusing on a case where you have a real sort of physics backing as to do the interpolation you want to do, right? So if it's a physics argument, and of course the case where you know it's a Gaussian, you have a similar kind of situation that you know it's. So the case that you worry about is where you don't have a physics argument, you can still have a statistics argument as to what to do, and that's going to be the next session. Optimal transport happens to be the middle column in that thing. So it's not like we have no clue. It's just if you can't use a physics thing, you have to go to the next bed. You have to go to the next best thing, which isn't intentionally compared those three on the bottom row that the case is to me, but the middle one is the best. If you don't have any physics reasoning, the middle one is the best. At least you have a statistics reason. Okay, so I first want to make a comment. So I wouldn't even call it motion, I will call it the decomposition of the signal into the physically motivated base. Into physically motivated bases. And the second thing is, of course, the same relies fundamentally on the linearity of the detector response. But if the detector response is non-linear in certain places, for example, isolation requirements are obviously non-linear, is there a concern from those saints to the validity of the message? So I think this is a little bit similar to if you're trying. This is a little bit similar to if you're trying to go, in my head at least, from trying to go from a Gaussian to a uniform when you don't know why it goes from a Gaussian to a uniform, in the sense that if there's non-linearity in your, for example, in your detector response, it's very difficult to know how to go from the one area to the next area in the morphing function unless you add extra information. So, generally, then you define your space into a region where you Space into a region where you know that something is valid, make a model there. Take another region where again you have a different model, for example, because the detector response behaves differently, and make a new model there. For something like optimal transport, I don't think that would be very sensitive to non-linearities. But here, you know, when you add two uh of your uh vectors, I would say. vectors, I would say. Then, depending on the amount of non-linearity that causes, that might give you a result which differs from what you would expect if you do a complete simulation vector with that main result at the simulation level. I think that's why we have our next move on optimal transfer. You first have one more. I wanted to just comment on what Olaf was saying. So I agree it's only good to just in include these things in like as parameters and like this. So if you feel there's like a moving bump or something, it should just be a parameter that you fit. But I don't think that you could get around the problem, right? So you might simulate the thing at three positions of your bump and then you still need to interpolate. So let's say for bumps that's easy, but let's say you're That's easy, but let's say you're doing a SUSY analysis and you're observable as a neural network discriminant. So now the parameters you want to put into your likely are the masses of the SUSY particles. So you need to somehow interpolate between the densities of neural network outputs. So nobody tells you how to do it. I think, but that's also, I mean, you always bring up this point of two-point systematics. And I think bring it up because the don't really know what to do. Up because they often don't really know what to do, right? And this also related to the small thing a bit. I mean, this is one aspect of it, right? Do I see them at the same problem? In principle, you could see them at the same problem because if you have the two points and you don't know how you can take the difference between the two and just say that that's your uncertainty, or you can try to say that we're somewhere in the space that it spans. And if you're trying to say it's somewhere in the space that those two solutions span, then you get to something like. Then you get to something like an interpolation method. So, in that sense, it's related, I think. All right, the APN. Can I just pick up the thread again that Igo brought up? So, in principle, as far as I understand this, this should be exact, right? This kind of physics-inspired interpolation, you know, there is no sort of non-linearity involved. This is going to give you, if you interpolate somewhere, you're going to get the exact same answer in terms of any bin distribution that you would get from a dedicated force itself. This is exactly for detector simulation. Before detector simulation. It's also exactly the same. No, it still detects after detectors before. No, but that relies, as I said, on the linearity of the detector response. But it's a full event, right? It's not like you're taking half of an event and you put together, you like add two separate parts of events to get the new events. No, no, you get full events, but if you look at different matrix elements, suppose in one matrix element you simulate an electron, and in another matrix element, you simulate a jet which goes. Example, if you simulate a jet which is going in the same direction, in the detector, you would not reconstruct that electron because the isolation requirement would kill it. Right, but then sure, but then that should show up when you propagate it through your detector. So if you do it this way, you know, everything is fine. All right, I think this is going to copy. Can I talk about this? We're going to have one last comment. You're going to have a lot of comments. So you're both writing your post. Excellent copy. And then we can tell the story to the comment. Okay, all right. Thanks again, Lydia. Thanks, everybody.