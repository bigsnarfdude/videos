I want to tell you some thoughts about a notion that was recently defined by Liana Hamad and Puya. Um I think it's a very nice notion and related to many other things. Related to many other things. And so hope we find new new ideas can help us understand it. And what I'm going to tell you is part of a work or an ongoing project with Daniela Vahan, who is a student at the Festival. At the festival. So the the fer the definition is the definition of rent. This is the definition. They gave. I just wrote it. Did you see there is this new Indian movie, RRR? Did you see this? I actually really love it. No, I actually really love it, but and so H. So as in all definitions of rank, there are two parts to the definition. One part is to define what is rank one thing, and the other part is just to use linear algebra like sum. So here's the definition. The rank one case, they call it. And they call it blocky matrices. And what are blocking matrices? In one sentence, it is a blow-up of the identity matrix. Locking matrix. Somewhat more formally, so all identity matrices are Glocky matrices. Are blocking matrices. So, and the identity matrix has rank one instead of rank N in the universe. And the collection of blocky matrices is closed under simple operations. So, we can duplicate rows and columns in the matrix for free. This does not change the rank of the matrix. Of the matrix. We can add a zero, a zero of rows or a zero of columns to the matrix. You're expanding the matrix now. Yeah, now we change actually the size and we can permute rows and columns of the matrix. And still we are in the collection of blocky matrices. Collection of blocky matrices. So all matrices that are obtained in this form are blocky matrices. They have rank one. Orially, this looks like something like this up to a permutation sorry up to a permutation of rows and columns. Okay, this is how it looks like. In fact, this is how it looks like. This is a matrix of rank 1, this matrix. Now, having these matrices, the blocky rank of any matrix, and we'll be working over the real numbers, although it's not very important to do it over any field. It's the minimum, the integer, so that you can write matrix as the linear is the linear is the sum of blocky matrices. Okay? So this is the definition of rank. It makes sense. So I didn't write HPI is a blocky matrix. I didn't write about it. I'm qualified. Sorry? Yeah, HBI, yeah. Yeah, HPI, yeah. For LBI BI is blocking and LBI is the real number, although it's not, you can do this over other keys. And to me, this is already like a reason for celebration. It's a very nice celebration. For celebration. It's a very nice definition. And the reason they came to this definition from a story that I will not tell, maybe you can talk about it later, I want to tell you a few things that we thought of when trying to understand this. So the first question you ask about, I think the first question you ask, is what is the complexity of a random object? Simplest type of question you can ask. So the first chapter is random matrices or generic matrices maybe. So the state So uh the statement is uh if m run from the bolar matrix uniformly triangle then what should be the belonging rank of the matrix? L is one guess, other guesses? Very high. Other gases? Very high. Very high is another gas. Very low is also another gas. So the actual thing is n overloading up to some. This is what happens when you just try to calculate it. But if you pick just all one matrix, uh it will be rank n, right? Rank one. Rank one. No, all ones. This is one block of ones. This is a legitimate This is a legitimate okay. You start with the matrix of size one. Yeah, the one by one again. Number one. Oh, and you know it. Sorry, I have a question. So the random response to the communication protocols keep the quality to that. So this is what happens, and maybe So this is what happens, and maybe this is not a hard thing to prove. This is an exercise. The hardest part of this exercise is this ending, that if you have a linear subspace, this is a linear subspace and it has dimension d, bene. Then when you intercept it with the hyper key, this set is small, it applies two to the most. This is like the hardest part of that exercise. Does it make sense? What dimension you just regular linear dimension? Documental within our subspace, regular dimension, because within our subspace. So, this is this is a low market or it's also absolutely not. Yeah, I will just, yeah, this is the next question. Yeah, indeed, so this is what the calculation gives, unfortunately, perhaps. But then you try to understand: is this true or not? So, this is the next. The next statement the next statement and here now it's this is what turns out to be correct so for all so this is another claim for every and this is not random every end by end n by n boolean the rank is at most n over log n times log log n this is true for every single boolean matrix but there is a same reals what but if you go to reals does it become n or not then it changes dramatically this is boolean matrix And somehow these matrices are Boolean, so you sort of, if you just have n different mean numbers, need n matrices. And is it always a post n? It's so for general, it's most n squared. Right? If we if we have just a matrix of generic numbers, you cannot generate them. You cannot generate them. Like if you have a matrix of all the numbers are transcendental with respect to each other, you cannot generate one from another because all of these are Boolean. Makes sense. It's important that it's most interesting for Boolean to speak of this part of it. So there be stochastic than Birkhoff and Neumann n minus one or maybe if there are beast of Hauski no I think you need kavateodorin. This gives n squared plus one. Okay, so this is a true for the This is true for every Boola matrix, and it is nice in a way like a random function. This has a non-trivial formula, but this is a channel proof, this formula. And there is a way to get the non-trivial formula. Some similarity in this formula. And this is also an exercise. An exercise. The hardest part of this exercise is what is called Savankievich problem. It says if there is an n-by-n matrix with many ones, let's say at least f ones, then inside it there is a block of ones of size log n by square n. And uh this is the Howard of the So the claim was obtained with the AI is being zeros and ones. Sorry? Ah yeah. Yes, yeah. Yeah, it's actually a partition to these number of blocky matrices. So just an actual sum. Just an actual sum of blocky matrices without coefficients. Like all coefficients are one. What do you do each time you extract a block with like edit? You extract a diagonal with that blank. I mean like so this is the second step, right? Is it tight? So up to this log log n, it is tight and I don't think It is tight, and I don't think the log-log n should be there, but this is what we got. So, okay, so this is the first question. The second type of question, which is much more interesting and much more difficult, is to understand the complexity of specific matrix set. So, specific Specific matrix. So the matrix I'm going to focus on is the greater than matrix. This is a very explicit matrix. And there are two statements here. So the first statement is that the block event of this. Locky rank of this matrix is at most what? Log n or up to maybe log n plus 1. And the picture is very nice. This is a blocky matrix. This just this. This is a second blocky matrix. Then this is a third log matrix and so forth. You get logar matrices. And this was actually proved in the context of shoe algebras, like in the 70s or 80s, by two people. I can't pronounce their name. Waukien and Pelchianski. So this was like proved a long time ago. I mean this can be interpreted in another way, right? The greater than has easy solution using equality oracles. Yeah, yeah. Which is exactly the picture. Yes. And the second statement, which is what we prove Statement, which is what we prove, is that the block rank is up to a constant login. This is, in fact, the hardest exercise so far. And I will. So somehow it's quite difficult to use the linear algebra. And what we actually prove a lower bound for is for Lower bound for a stronger thing. So you can define a functional blocking n. It's the minimum. So the matrix can be written as a point product, pointwise product of some function. So there is some function of taking arbits to Function of taking r bits to one, you apply it pointwise on the matrices, you get a matrix. So this is a other version of this definition, and the lower bound holds for this definition, which is much stronger. Yeah? There exists a So this generalizes some proofs about covering graphs in combinatorics and versions of this was proved in the context of machine, like call it closure properties of little stone classes. So this was sort of studied before. So this is a And now the two things that I think will be most interesting is sort of applications. So applications that so one application so the two applications I want to discuss is about circuit complexity. One application is a Is concerning what is called noted by the sum of linear threshold functions. So this is a depth 2 circuit. The upper gate is a sum gate. The lower gates are linear threshold functions. Upper gate is an exact sum. Exact sum, yeah. And we don't know how to prove lower bounds for this class, like strong lower bounds for this class. For this colour. But the observation is that if there is a Boolean matrix, then the blocky rank is at most the size of the circuit computing this function times n. So this is a 2 to the n by 2 to the n matrix now. This matrix This matrix represents a function from n-bit strings by n-bit strings to n-bit strings. So if we have such a function and we can prove a lower bound on the blocky rank, we prove a lower bound on the singlet size. So this raises a question to prove any lower bound for explicit matrices. What's the size of m times? N is the, so this is the 2 to the n by 2 to the n. So n can be fed as a function size here is the circuit size of this model. Yes, size of the circuit size of the small. Size in this small. So you get two n-bit inputs here? Sorry? For the circuit, you have two n-bit inputs? Yeah, it's n and n. Now we're saying the blocking angle of half spaces is small. Yeah, this is what space is. That was for greater than, for near. For any greater than specifically for LTFs. Yeah, so this theorem, this we Theorem, this result implies this result. So is the number of it supposed to be a number? 0, 1. It's a Boolean number. So it's like a threshold of the sigma L T? No, no, this is an actual. You actually want to compute 0 in the sum, or 1. Oh, okay, I see. It's strange but even for this, we don't know how to prove the And the last thing I want to tell you is that sorry? Yes, so I have a question. Yes, it seems that it should be true that there's an upper. So if a matrix has blocky rank R, it should have gamma 2 norm at most R. The gamma together, this is true. Ah, no, the other way. So I will mention it. No. So the coefficients are. We have no control over the coefficients. It is. No, but suppose that you want to prove a lower bound on blocky rank, then it should be enough to prove a lower bound on Ganner 2 wrong. No. No, no. It it is wrong. We can talk about it better. Yeah, but We can talk about it better. Yeah, but it is wrong. Okay. But it is what you said is good in the sense that it's related to the last thing I want to say, which is about majority of linear Freshard functions. This is another model which we do know how to prove lower about for. Einal mass in Pudlak and Sega And put like and segregate into one, they prove that the size in this model of the inner product function is something like 2 to the n over 3. This is what they proved. And with this perspective, indeed the connection between Lockyrank, Gamma 2, and this related idea. These related ideas, you can use basically a C, you get a similar proof of a stronger statement. So this is what you can prove with this perspective, which is a the proof is very simple and indeed the point is that the gamma two or I Gamma two or I prefer the nuclear norm of linear threshold functions is at most n. This is one property and the other property is that the L the gamma two of inner product is more than two to the inner product. These two This is related to the block event, to the block event perspective. This is what is called Lindsay's lemma. So this is the standard argument for inner product. And putting these two things together, you get such a new bound. So these are the two applications. So these are the two applications. One is that proving lower bounds for this rank is circuit lower bounds in a very clean form. My deal. So maybe we can prove something. And another is that this perspective allows to clean things in a way. So it gives a better, and the proof is really like very simple. Very sh very simple. So these are the two things. So these are the two things. Two more comments I wanted to make is one, in a way, which I can't make formally, understanding blockhurren is also a cleaner way, like a special case of understanding rigidity. So somehow blocking matrices are either very sparse or have low rank. Low rank and somehow they fit in the same language. You don't need to speak about sparse and rank separately. You just speak about blocking ages. This is one thing. So I think this is another reason to think about it. And another calculate problematic matrix that I am one. Yes, yes. And the one more comment is that so there is a blocky rank, the functional blocky rank, you can consider sine blocky rank, approximate blocky rank, whatever quantum blocky rank you can do, and each is, I think, a good thing to try to understand. So I think I'm done. Thank you for enjoying the hike. So what is the relation with gamma 2? You said you were coming up. What is the relation between Block Ernst and Gamma 2? So it's sort of the connection is that I don't know the general connection, but if a matrix is written as a sum of Is written as the sum of the oxygen set, then its gamma 2 is at most. This is the only... This is a trivial state. I mean, no coefficients. No coefficients. Magga 2 is also an inner button. Can you include coefficients and then the gamma 2 is just the sum of the absolute values of coefficients? Yeah, if you can. Yeah, you can you if you can control the absolute values, but it's I think you you get an upper bound exponential in rand or something, right? Because even if you have real coefficients. Because you can always because m is Boolean and those are Boolean, you can always get this very significant thing. If this matrix is Boolean, And you actually write your n equal to which is the rand, then this is like maybe two, either two to the r or two to the two to the r nature. Because you can look on these block images, there are block imager cells, you think about it as sets. One and zero, then you look on the sigma algebra they generate, so sort of all the cells, all the different cells. Because there is a finite number of cells. Because there is a finite number of cells of the object, the the number of cells is finite. So and the the intersection so this may now there is another issue of intersection with the complement, which I need to but you need some properties, but but this may be true from what comes. Yeah, from what Hamid just said. I had a question about this functional luck here. So if we change the matrices, the rank one matrices, to just be rectangles, does this correspond to anything that we're familiar with? The rectangle case is rank one matrices. Rank Boolean matrices. Yeah, so. The functional rank of rank. Ah, for the functional rank, this is a good one. For the functional rank, this is a good point. For example, inner product has functional rank n, although it is a 2 to the n by 2 to the n matrix, because it's a so of matrices. So of rank one matrices. So the functional rank is a very, very small quantity, like strong, weak, yeah. One minor comment which I think is good to know is these blocky matrices are precisely uh Boolean matrices of gamma to norm at most one. They are exactly those. You were after the hike. Yeah, it's blocking steps. So I'm sorry, I just need to go yeah, go ahead. Is anyone missing a key card? Yeah, we would be current productions to follow you versus. So and there are many growth. Can you talk about this then go blocking methods? No. Complete is usually once the actual scale. You have a complement here and another complement here. Oh, so sort of a few complete. And at the beginning of the joint. It's like this joint with me at all. For real by project. So so this so in the upper control, this is called the head. I guess controlling the power. Is that that's 1027? No, but you only credit this high block. Is that actually most everyday side? Like font toys blow. What point was it drawing? There's a new interchange. Now this operation can ask if the first matrix has some norms. The new matrix doesn't know how much does the new normal knowledge. So we can ask what is the. So now a matrix becomes an operator of a matrix. And the matrix has no. No, the maximum can blow up. No, no, I see that theorem. It's Mark rather than that. This corresponds to Boolean. But if you take a matrix, point to a product to make cell states, this is a Boolean cell. So they wanted to understand, then what that operates? Yeah, I know what happened. People make use of some statement about. Some statement about emotions of the collections are because they have terms. I mean, then you can write it as this complexity. Oh, yeah, that's not the system. So the magnitude of the microphone is the same. So the self-ceremony, if you are approximate, you can write them in approximate. You can rightly approximate the matrix so that you get to define one is small of the quality. This is the classic fine bracket file, you can. Blocking rust, you can. It's only small enough about the latest data structure.  So, what's the entry point? So, staff members in none of the terms. Anything else to ensure that things are being for every function you just want to do? And then basically, I'm just wondering if it was characterizing the side first button. Yeah, the depth to the maybe related to lots of matrices. So so it's actually already So so it's actually covering the I know I know it's a different question. But it it's just that for most of the generates. No, no, no, I mean I mean this is no no no no no the the either one is not this is uh I don't think it's easy. No, I mean the finger. No, just see what's on the topic. And uh and uh this was but I thought this was four years.