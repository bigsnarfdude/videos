And also, well, it's kind of a privilege to be talking about this new passion of mine in front of people who also share common interests. So it's been a pleasure to discuss many things and collaborate with many people on this topic over the years. And today's talk will be focused on three, well, these works, these two. Works. These two are for infinite-dimensional systems, so field theories. And the more recent one is for finite-dimensional systems. And I'm going to start with the more recent one, although historically we understood quite a few things first in infinite dimensions and then sort of evolved to finite dimensions. So kind of reversed order from what you would think is logical to do. The plan is as follows. So first part is just a First part is just a review of the idea of Lagrangian multiforms. Fortunately, many people already talked about that, so I'll be able to go quite quickly over that, but it's to be mainly self-contained and introduce notations. And section two and three are the big ones, and essentially they go hand to hand. So the idea, there are two main problems that I wanted to address is how to construct multiforms, examples or classes of examples. Or classes of examples efficiently, and why is the multi-form theory a good criterion for integrability? Because that's how it was proposed initially: an alternative criterion for integrability, but from a variational point of view. So, in order to contrast from existing criteria for integrability, I will need first to review essentially two of them that are extremely well established and accepted: Laxpares. Lax pairs and the Hamiltonian condition for integrability. And reviewing them will actually and understanding them better from the point of view of multi-forms will actually give us tools to construct multiforms very efficiently and a very large class of them. So, of course, famous people in our area both have names starting with L. So there's Lux and there's Lagrange. And there's Lagrange. So the straight L will be for everything that has to do with Lax stuff, and the curly L will be anything to do that has anything that has to do with Lagrangian stuff. And hopefully you haven't made any typos, otherwise you're going to be very confused. And although multiforms work for discrete, semi-discrete, and continuous theories, as you're probably aware now, today is only about continuous theories. So, of course, well, Frog and Sarah. Well, Frank and Sarah are the reason why we're all here, and hopefully, this is the first workshop, which means that it might inspire more people around the world and not just the leads and Berlin schools, as Yuri put it. So, what's the practical implementation? So, first, I'll review in finite dimensions, finite dimensional system. So, if you know nothing about integrability in multiforms and you just And multiforms, and you just consider an action for a Lagrangian density. Well, essentially, you're integrating a volume form. But the idea put forward by Frog and Sarah is instead to consider a collection of Lagrangians, because in the background you anticipate that you need a hierarchy of commuting flows or multi-dimensional consistency. You arrange them into the one form, but this one form is no longer the volume form. One form is no longer the volume form, it's a one form in the higher space, the multi-space, which, for instance, I'm going to say it's n-dimensional for now if I have in mind n commuting flows. So that's the first idea. And the second idea is to integrate this one form. You can only integrate non-tributing one forms over curves, one-dimensional curves, even though you're in a higher-dimensional space. And the important idea is that now this curve becomes also. Becomes also an object of variation. It's not just fixed once and for all. So then, associated to these new ingredients, you have a new variational principle, which consists of two steps. First of all, you say, well, for fixed but arbitrary curve, you vary as you would do before. You vary and you try to find critical configurations, but for arbitrary. That gives you actually more than just. That gives you actually more than just the standard L-Lagrange equations. The corner equations, or well, the set, the whole set is called multi-time L-Lagrange equations. So that's the first new ingredient. And if you stop there, you have the idea encapsulated by Player Lagrangians. Second step, you say now on solutions of whatever equations you find here, we're going to require a little bit more is that the minimum, or let's say the extrema, actually is also. The extrema actually is also independent of the curve you consider it, and that's what leads to the closure relation on share. So that's what I summarize here. You have a collection of multi-time allayer Lagrange equations, the standard ones for each Lagrangian, and then some real ones which control the structure that you can have for your Lagrangians. Here I've only written them for first chat Lagrangians. For first-chat Lagrangians, simpler situation that you can imagine, but it's actually enough for what I want to say today, which is quite good. More general forms were presented, for instance, by Matz. And the second consequence, as I said, is that on shell you would have the closure relation, which takes this form here. The same story works for field theories. In one plus one and even two plus one, we have an example of even two plus one we have an example of kp and also uh more recently uh this um uh generous darbu slash chan simon's example is in higher dimensions so today i'm only going to consider ones plus one dimensions and the same idea if you don't do any multi-forms you have an action which is a volume form with a lagrangian density but now the idea is to consider a two-form in a much bigger space turns out infinite in that Turns out infinite in that case, because for field theories, you have an infinite number of commuting flows, but I won't dwell too much on that. With the same idea that now the underlying two-dimensional surface becomes part of your variational principle, so you do the same thing. Oh, before I do that, yes. So if I anticipate, you do the same thing. You have two consequences, multi-time Lagrange equations and closure. I want to make a remark here that just came to me recently. That just came to me recently, but what multiform, Lagrange multiform theory tells you is that the correct object is a two-form and not the naive extension of a one-form where you would just include an infinite number of Lagrangians. You get the wrong picture if you think about standard Hamiltonians as being Hamiltonians, standard Hamiltonians integrated, Hamiltonian densities integrated over a special variable. Variable. So, this is actually quite important because I think it teaches us that the proper Hamiltonian counterpart of Lagrangian multiforms is actually a Hamiltonian multiform, where now you have to consider what's called covariant Hamiltonians and not your usual standard Hamiltonians, which sort of break the symmetry between the independent variables. So, we studied this object where we introduced So we studied these objects, well, we introduced them and studied them superficially, but there's a lot more that should be said about that, I think. Going back to the proposal of generalized variational principle, two consequences, multi-time Legend Lagrange equations, which have been shown since that they can be written compactly and in coordinate independent form using what Linux reminded us of this morning, the variational biocomplex. That's the vertical derivative and that's the horizontal. Derivative and that's the horizontal sorry, vertical differential and horizontal differential. And then you have the closure relation, which now for a two-form has three-tailed terms with cyclic pernotations. So two main problems I want to address today. Why is it a good criterion for integrability? Maybe that's the wrong crowd to ask that, because hopefully you think it's a good criterion. But still, if you want to convince more people to use those criterion, maybe we should try. Those criterion, maybe we should try and spend some time convincing them. You could say, Well, is it just a matter of taste? We like Ligandians more than Hamiltonians, or is there a real added value, like Yuri tried to convince us? Perhaps that's the case. And an intimately related question is how you construct all the Lagrangian coefficients efficiently. So in complete generality, that's the classification problem using the multi-timer Le Lagrange equations. Using the multi-timer Le Lagange equations. Say, well, can I solve them? Because the philosophy put forward by Frank is that actually part of the new vision is that these extra equations that this partational principle gives you are constraints on the Lagrangian themselves. So you don't guess the Lagrangian or find it from symmetry considerations like Peter told us this morning. They should come out of Should be, they should come out of the structural equations. So, in principle, you can classify. I am going to be much more modest today, not going to tackle the classification question, but I want to mention that, of course, that's one of the first things that we will, well, all people working in this area have tried to do to construct all the possible coefficients you can come up with. So, you've got the brute force approach, which is actually trying to solve. Approach, which is actually trying to solve. So, part of the idea of the classification problem. A fruitful idea is the use of variational symmetries to construct the higher Lagrangians from, let's say, a starting one. And something also discrete to continue limits have been used efficiently, but it means you need to know something from the discrete side of things. Actually, Pavlos yesterday mentioned the idea of master symmetries, which I didn't know that, construct. I didn't know about construct the Lagrangian coefficients. But so it seems to me that either these ideas give a finite number, well, a selected number of Lagrangian coefficients, or there is indeed a procedure to get the Morgan principle, but it's quite recursive. And what we did is actually try and find formulas that give you, in principle, any Lagrangian coefficient you want without having to calculate the lower. Having to calculate the lower ones. So, not recursive. So, that's the most modest approach I was talking about. Say, okay, I'm not going to tackle the classification problem, but I'm going to try and learn from what's already known for 50 years of hard work from people who've developed magnificent structures on the Hamiltonian side of the story and the Laxpa side of the story, and see if I can use this knowledge. I can use this knowledge and inject it into the Lagrangian multiform story to get some answers to how I construct this efficiently. So I need to review these two aspects to be able to use it. And first of all, sorry, and in fact, what I need to understand or what has turned out to be very fruitful is to see how they are deeply connected. It is to see how they are deeply connected. That's really the interplay between the two that's going to be useful for us. So that's actually the first and maybe the most famous approach to integrity is the notion of a Lax pair. It contains your dynamical degrees of freedom, and M is what's going to control the dynamical evolution via this equation. And the important thing is that M depends on L, that's why it's non-linear and difficult to solve. But an immediate consequence, that's the signature of integrability in that language, is that the trace, the powers of the A, are conserved. That's immediate from the commutative form. What's a lot less obvious, but actually the thing that is the most impact or influence on the theory is that this. Theory is that this equation is Hamiltonian. So you can find a well-chosen Prussian bracket, which I'm going to tell more about in a minute, and you can find a function h such that this commutative form and this m are completely determined by the Hamiltonian and the Prussian bracket. That's the key. So how do you do that? Well, before I tell you how to do it, an observation is that an important idea is these integrable equations don't come alone. Equations they don't come alone, they come in hierarchies. So, again, to simplify things, that comes from the fact that these can all be used as Hamiltonians, they're all Poisson commutes. So, you can impose a certain amount of flow simultaneously on the same function from a Hamiltonian point of view. And if your Poisson bracket and your Hamiltonians are chosen well enough, all these guys actually take a Lax pair form or different Lax matrices controlling the front. Controlling the flows. So, how do you realize this connection for an entire hierarchy efficiently? Well, to my knowledge, the best way to do that, the quicker way to do, I mean, one quick way to do that is to use the idea of an R matrix and a special Poisson bracket related to it, a special Lie Poisson bracket related to it, such that when we calculate When we calculate this Poisson bracket for well-chosen Hamiltonian and this particular Poisson bracket, you can rewrite it in this fashion. And the bonus is that you know what this matrix is. It's completely determined by the knowledge of your Hamiltonian and the knowledge of the R matrix. You have everything you need. So I think the most complete approach to this systematic derivation. Systematic derivation is Seminov-Chenchensky-Lied algebra theory. Of course, it has a long history. You may know, for instance, the Ad-Layer constant scheme, Ad-Layer constant sign scheme, which is a special case of that. But let me remind you of the key ingredients. Don't worry if that looks a bit overwhelming. Think of it as a recipe. So I'm going to give you ingredients. I'm not going to give you too much of the recipe, and I'm just going to give you the final. And I'm just going to give you the final dish. All right? So, what are the ingredients? Some linear algebra. That's just think matrices, because you want a lax matrix. The famous objects of interest, the R matrix. Think of it as your object that controls the classroom structure. It has to satisfy a certain condition for everything to work, which is the modifi called the modified classical. Called the modified classical Young-Baxter equation. You may know it in another form with what it's called, the kernel, which I'll come back to later on. This condition implies that if you define this bracket, it's actually also a Lie bracket on the same Lie algebra. That's where the the notion of Lie die algebra comes from. You have two Lie brackets on the same vector space, and it's the interplay between the two that provides integrability for you. That provides integrability for you. And now, thanks to the second Lie bracket, I can define the Poisson bracket of interest on the dual of the Liege. So I have all my ingredients. I have my phase space, almost. There's a subtlety. Let's say that's the phase space. In fact, I will want the coagular orbit, but I'll come back in a minute. And I have the Poisson bracket. So essentially, think of L, the Lax matrix, as leaving. The lax matrix as living within the dual, and then as being this being the coisson bracket that's going to do all the hard work for you. The last thing I want as an important ingredient, essentially, so that I don't work abstractly in a dual, but I can bring everything back to just nice matrices, is a bilinear form. And think of this as just the abstraction of the trace of the The abstraction of the trace of the matrix. So every time you see the binary form, think trace. Every time you see a Lie algebra or is dual, think matrices, because that's at the end of the day in the example, that's how they are realized. At least for what I'm talking about today. Okay, so these are the ingredients, and this is the main theorem, which summarizes why this is important, essentially because it realizes what I announced. So if you take So, if you take well-chosen Hamiltonian functions, so they are the invariant, but Benoit mentioned them yesterday. So, essentially, they are trace of powers of your Lax matrix, so invariant under conjugation. If you take such functions, then automatically your Hamiltonian flow takes a Lax form, and the Lux matrices are computable explicitly. And as a bonus, automatically, as well. And as a bonus, automatically as well, because the functions are invariant and because the R bracket is what it is, you have flows in involution automatic. The last ingredient, I said, well, we almost know what the phase space is, so that's also a bit of a technicality. I'm going to give you the formula, but I just want you to think of it as roughly the following idea. You fix a fixed matrix non-dynamical, and then you conjugate it by And then we conjugate it by a group-valued field, and your dynamical variables are there. If you're familiar with the dressing ideas, that's dressing. It means in more fancy language that your Lux matrix is a certain coercion orbit of a fixed element, and all the dynamical variables, so the fields of your theory are in this group element phi. And this is what allows you to write Lagrangians, this parameterization. In the background of all these stories, there is automatically for free zero curvature. So, I'm not going to mention them explicitly in the rest of the talk, but they're always there for free. So, in particular, it's important because for finite dimensional systems, people are quite happy to just work with the hierarchy of Lux equations and not really bother about those. But usually, when you talk about people with people about field theories, With people about field theories, all they care about is the zero curvatures. And what I want to convince you of today is actually the important equations also for field theories are the Lax equations. Zero curvatures, they're there anyway. They're just a way of rewriting the equation contents. And that's also key to constructing multipoles. So, taking all this on board, all this preliminary and this knowledge that we gleaned from the That we gleaned from the lead algebra theory, then it's a guess. So you just say, okay, I want to write a Lagrangian multiform, which reproduces all this set of commuting flows as its multitime Ele Lagrange equations. So there was a bit of trial and error, but the final answer is this. And the easy part is just to put your Hamiltonians where the Hamiltonian is. Hamiltonians, where the Hamiltonian should be in the Lagrangian, so as minus the energy. The difficult part was to guess the kinetic part. That's where all this information I told you about is encoded. So you have your Lax matrix, which is this coagent orbit element. You have the group value field, which contains the dynamical variables. This multiplication means that I am in a group associated to the group associated to the second. The group associated to the second bracket, not the original one. And these braces are essentially the pairing, which translates into taking a trace. So if you think about how you compute things in practice, you've got to take the trace of a matrix times a matrix. That's all you have to do. And then you have one of such Lagrangian for each of the times of the hierarchy. The hierarchy, you arrange them in your one form. It looks complicated, but in fact, they are very beautiful because they are just of the form, the usual form you would expect. As I said, everything is hidden in the kinetic part, all the difficulty. And then you calculate your multi-time Ray Lagrange equations, and the miracle happens because you've just chosen your Lagrangians appropriately, and you can show that the And you can show that these are the equations of motion for the standard Le Lagrange equations for each Lago engine. The remaining multi-time Le Lagrange equations, so let's say the corner are triggerly satisfied in that case. And essentially, to qualify as a multiple form, the closure relation holds. So when you compute this quantity, taking these equations into account, you find zero. Find zero. So that's the main result in finite dimensions. What does that mean? Let me see what I've got means. Yeah, before I turn to examples, if I summarize what this means, it means I have a very algorithmic way of constructing a large class of Lagrangian multiforms, because all you have to do is to give me a Lie algebra, an R matrix, and a family of ad invariant functions. Family of ad-invariant functions, independent ones. You write down what you get, you have your Lagrangian multiple. That's all you have to do. And there are loads of examples known in the literature that people already did from the Hamiltonian point of view. So all these examples, they are contained in this construction. So we didn't cover all the examples. We chose two particular ones that are very famous. The first one is the open to the chain because Is the open to the chain because if you want to convince people that you have epicable systems, that seemed like a good one to do. So, I'm not going to give you the details, I'm just going to give you some salient features because, well, you probably all know or all heard of Toda before, so there's no point repeating the equations. But what's interesting is that we obtained it in two completely different ways. So, choosing a different R matrix, one being excuse memory, the other one being not. Excusymmetric, the other one being not. So, if you're not too familiar with classical R matrices, you might not get the point of what this means, but it's actually quite important and was a thing we couldn't do in field theory because R matrices not being symmetric, for instance, upon quantization, pose a serious problem because then you don't really know what to do. You don't know how to quantize such things in general, especially in field theory. So, we were quite pleased that the So we were quite pleased that with this linear algebra setting, we don't care. We can do skew symmetric, no skew symmetric. It doesn't matter. The other comment is that because of the parameterization on coagent orbit, the natural variables you have to work with are the variables contained in the group element phi. You have no choice, you work with them. And in fact, they give you a set of canonical variables that are neither the original QP variables nor the flashed colour variables. Nor the flushed car variables. Of course, you can relate them, you can do the transformation, but they give you a completely new set of variables, and they turn out to be canonical as well. And they're the key to actually writing the kinetic part of the Lagrangian multiform in a nice way for Toda. If you write, if you try, I think Mats has told us he tried. So, if you try, for instance, to reverse engineer Lagrangian multiforms with Pashka coordinates, it becomes quickly. Coordinates, it becomes quickly, it becomes very difficult in general, but here it just comes out free because you're working somehow with the right coordinates from the beginning. Another example that we looked at in detail, and that's the object of a talk later on this afternoon, is the Goda model, which was already mentioned yesterday in Benoit Vicedo's talk. The motivation for this was as a preparation in the final. Operation in the finite case of perhaps trying to construct a Lagrangian multiform for affine Goda models, field theories, which was also what Bernois mentioned yesterday. So that's kind of why we chose this one. It's because it's a preparation for the bigger task of doing the field theory today. So you could say, okay, so what? We have lots of Lagrangian multiples now. Lots of Lagrangian multiforms now. But in fact, now that we have a concrete class of Lagrangian multiforms and an understanding of their underlying structure coming from the Hamiltonian language, we can look deeper into the theory so we can learn something about Lagrangian multiform theory itself. And that's the content of this result. We have an identity which is at the basis of the Is at the basis of the closure relation. So I'm going to spend some time on this identity because I think it's an important result. Yeah, an important result for the theory of Lagrangian multiforms. So the term in blue are the components of dl and here I can see that it's an identity. I'm not working modular equations of motion. So this is an identity. So this is an identity in the sense that, well, in the sense that it's an identity, so I'm not working modulo any equation of motion. So that's just DL. Probably you've recognized this guy on the right. So let's spend some time on the red bit because it turned out to be already of discussion in this conference. It's a double zero bit, which we discovered in this example and apparently seems to be playing. And apparently, it seems to be playing an important role because Mamma's convinced us that now it appears in other contexts as well. So that's the double zero term. So these epsilons, what are they? So that's the Euler-Lagrange operator for the time k and the coordinate m on the office space. And that's just another one of them. So that's where the double zero comes from. You have Euler-Lagrange times Euler Lagrange. So on equations, it's zero times zero. But now we've identified the But now we've identified the object that links that is behind the structure of double zero, and it turns out to be the Poisson tensor related to these R brackets. More precisely, is the Poisson tensor reduced to the of the Poisson, sorry, of the Poisson bracket reduced on the cohesion orbit. So it's really the constant Kirillov Poisson tensor. I think that's the terminology that Lenoir used yesterday. So it's the inverse. Used yesterday. So it's the inverse of the constant Kirillov form on the collagen orbit. If you're familiar with that, it's just a comment. If you're not, don't worry about it. We know what it is. And it has some interpretation in terms of the propulsive structure. And of course, the last term is the term that controls the involution of the corresponding Hamiltonians. So that's the identity. So it means it's always true, irrespective of whether you're. Of whether you're on shell or not. So, what does it mean? As a corollary, now we have the equivalence between the closure relation and the familiar Leuville criterion for integrability, because what does the closure relation tell us? It tells us that on shell, so when this is zero, that should be zero. So, left-hand side is zero, implying the Hamiltonians are in evolution. Hamiltonians are in evolution. Conversely, what does Hamiltonian involution tell us? It tells us that this is zero, hence on shell, we must have closure. So that's the interpretation of this identity. And we see now the importance of the closure relation that was put forward by Frank originally from this very nice observation on multi-dimensional consistent equations. Dimensional consistent equations. So now there's an understanding in finite dimensional continuous systems as an identity. Right. So now, how to treat cases in infinite dimensions, field theories. I'm only going to do one plus one dimension here. So PDEs in one plus one. In fact, it turns out that the It turns out that the idea that everything I've said has a counterpart in infinite dimension. All you have to do is to work with infinite dimensional algebras, so loop algebras, instead of finite dimensional algebras. And in fact, there was already a hint of what you should do in this very rich paper by Flashka, Neuril, and Rathu in 83, where they only did the ablovitz called Neural Segura hierarchy. So it's the hierarchy that contains the non-linear Shanu. That contains a non-linear Schr√∂dinger. And they actually, I don't think they didn't have actually the right interpretation for it at the time because they were wondering what the meaning of the construction was. And in fact, that's the meaning that I'm going to explain now. So as I said, let's start from scratch. So what should be the LAX, the equivalent of the Lux matrix in that case? Well, you're working in infinite dimensions, so naively you're packaging your Lax. Your Lax matrices, if you want, using a spectral parameter in an infinite series. So for AKNS, you only need SL2-valued coefficients. And the meaning of this element, L0, being taking this value, it means that I choose the coagent orbit around this element. The hierarchy of Hamiltonian flows or Lax equations, that's what they show. So, Lax equations, that's what they showed, can be put in this form. So, you write the hierarchy of commuting flows on this object. The Hamiltonians are known. I don't write them down, but we know what they are. The Poisson bracket is known, and it can be reinterpreted as an R Poisson bracket, although at the time they didn't know anything about that. It's actually a special case of the general theory. And then you can repackage everything as a Lax matrix. The only difference, so with the finite-dimensional case, is that you have an infinite. case is that you have an infinite number of k so k now can be any integer so these hamiltonians they have to be infinite number of them and these uh lax matrices they're your family if you're familiar with the ak and s hierarchy they're just your family polynomial valued uh matrices of degree k and lambda okay so if you you see superficially i just look at that and i'm like well i'm just doing the finite dimensional case again but Case again. But of course, there's a deep difference, there's a spectral parameter, and there's an infinite number of time flows instead of just a finite number. So, yeah, just to show you, if you were not familiar with the Ablovitz hierarchy, if you look at the first three flows, the set of equations that you obtain, you obtain nonlinear Schrodinger system and reduced, so for two fields, QNR or the modified KDB system, similarly. KDB system similarly unreduced. Okay, so now you want to write down or find a Lagrangian multiform that produces all these equations as variational equations. So that's what we did, but we had some inspiration. First inspiration, Frank's idea that compounding the hierarchy is better than looking at individual elements of the hierarchy. Than looking at individual elements of the hierarchy. And one efficient way to do that is to package all your vector fields, all the time derivatives, into a generating vector field by introducing another formal spectral parameter. So you package all your vector fields in one object in this way. And similarly, this lambda k here, which controls the evolution, you package it using the same trick by multiplying one of them. By some by multiplying one of them mu to the k plus one and summing, and then you recognize a geometric series, so you resum it, but you remember that you expand with around mu to infinity. That's the meaning of this little map, Yota. I don't want to insist too much, but it's just to remember that you do the expansion on mu around infinity. Because if I just give you the partial fraction, you could, you don't know if you expand in lambda or in mu. So that's just a So that's just a gadget to remember what we do first in the formal series. And then using this trick of compounding, you just apply this operator on both sides, resum, and the entire set of infinite, so you have an infinite number of equations here for they're just written in generating form, but with two spectral parameters. So that's this one is the one that tells you where you are in the high-like. So that's the first. So that's the first trick that allows you to write, well, that not allows you, but that we found convenient to. The second idea is that, well, if using generating ideas is powerful, then we should do the same for the Lagrangian multiple. So why not package all the Lagrangian coefficients that we're after also in a double generating series? Same idea. Series, same idea. Then knowing this object or knowing all these objects is the same thing. It's the idea of a generating series or generating function. So then all you have to do is to guess L, L lambda mu. Okay? Well, here's the guess. So it also has a kinetic and a potential part, but the guess is not completely we were not completely in the dark. Completely, we were not completely in the dark. We also had some inspiration. So, an important, oh, maybe I said it before, or I'll say it after. But an important source of inspiration was a paper that is not so well known by Zakharov Mihailov in 1980, for which they derived a single Lagrangian for zero curvature equations with Lax matrices with spectral parameter. So, exactly the type of. Parameter. So, exactly the type of things that we're looking at. So, that was an important source of inspiration, in addition to the compounding hierarchy inspiration. So, let me now give you a little bit more information about the various bits. So, what's happening in the kinetic part? Well, in blue, you just have these compounded vector fields. So, one in lambda, one in mu. That's what I explained before. This L0 is This L0 is the reminder of which Korangian orbit you're considering. And as before, the key idea is to write your Lagrangian, sorry, your straight matrix of interest as a coagent orbit. So here it's actually just really an eigen orbit in that particular case. So all the fields of interest, so all the dynamical variables are in this group-valued here. So, with this understanding, this is the formula for the kinetic term, this is the formula for the potential term, quadratic in L. And then you just have to take the variation of that and check that it is indeed a multiple once you have the answer. So, that's what we did. This is the result. So, you calculate all your multi-time Lagrange equations, and you find that you have exactly what you wanted, and you can check that. And you can check that dl is zero on these equations. And again, it's actually very useful to write the closure in generating call. So now you package everything with spectral parameters. Yes? It's just a bracket of matrices. Is that your question? That your question? Yeah, yeah. So it looks more complicated than it is. So at this level, it's realized. And in fact, for KNS, it's even simpler than that. Oh, I have to go back quite a lot. So it's just two by two matrices. So you can use the underlying SL2 bracket. Oops. Yeah, a command. And actually, Uh, yeah, a comment. And actually, as I said, the zero curvature are always in the background. And in fact, now we have the generating function for the Lagrangian multiform. If I extract any Lagrangian I want, so as I said, I can do that just by picking which j and which k I want, by computing some residue. So there's no need for a recursive operation. And the Lagrange equations just give you the corresponding zero curvature equations. Any questions? Emmett, yeah. So, this is the corresponding wax matrix. It's it's this one. It's your polynomial. No, no, so this one just depends on lambda. Think of it as a LAX, as your standard Lux matrix that you would do. This is a component of your gauge connection, if you like, of the Lax connection. It's just that I have, and each of them is a polynomial in lambda with two by two entries of this form. And the field, if you want, and the fields of interest are the off-diagonal in that case. Turns out the A is the H. Okay, so it turns out the A's, they trace out, they're not relevant. Does that make sense? Maybe I can tell you more. Okay, so for sanity check, you say, okay, I know I have NLS and modify KDB as the first few flows, compute the corresponding Liquid engines, you can extract them. Actually, these were known before, so we just verify that probably is the right thing. That's probably like it's the right thing. And in principle, you can do all the otherwise actually four, etc. Okay, so I'm supposed to finish in five minutes. Right, okay, so fasten your seatbelt. I just want to, because this is the most exciting part. Well, the finite dimensional was exciting because we can do non-SQC. Was exciting because we can do non-skew symmetry. This is exciting for the following reason: is that we only did AKNS, but actually, with simple observations, you can do a lot, lot, lot, lot more. And in particular, you can create or discover new hierarchies in theories. First observation, the potential term that we sort of guessed, you know, educated guess, if you know a little bit about our matrix theory, it turns out that it's a special case of something like that. Something like that. And before I mention this question, this is perhaps the R matrix you know more from the work of Sklianin. And it's related to the linear operator I mentioned in the first part of the talk through the usual formula. So that's the kernel for this linear operator. That's the link between the two pictures. And if you choose the first discovered and simplest R matrix, which is called the rational R matrix. R matrix, which is called the rational R matrix. You plug in, this is just the AK and SK. Can look back to what I said. But then you say, well, now I have that. What if I plug in other R matrices, the trigonometric one, the elliptic one, or other? In principle, you build other models. So you already have access to other models beyond AKNS. Second observation: we were dealing with series in one over lambda or one over mu. It means you're One of a mu, it means you're looking at Taylor series around infinity. Infinity has not got nothing special on the Roman sphere. So, what about looking at other points? And that's a very natural idea in multiple systems where you have Lux matrices which have poles at fixed points. So, let's include these poles in the story. And final observation: we made a special choice of quagging orbits. Of quagging orbit and even a special choice of Lie algebra. Take another semi-simple Lie algebra and other points in such semi-simple Lie algebras, sorry, loop algebra of a semi-simple Lie algebra, and then you construct completely different phase spaces. So now we have three ways of changing, well, yeah, three new ingredients. We can change the R matrix, we control the puzzle bracket. We can insert C. Insert singularities, which gives access to other modules, and we can change the equation orbit, so we can change the phase space. So, you modulate all these things, and you come up with your favorite example. So, we should do that. We should be able to reproduce known examples that we had before. But then, if you start digging, maybe you can construct new ones. And indeed, you can. So, let me quickly show you that we can reproduce old ones by specific. By specific changes. So, yeah, let me skip that. So, that was actually Benoit's observation that to take these observations more rigorously into account, you have to work with the Lie algebra of G value Adel. I'm not going to skip the details, but essentially it's to implement what I told you, to have more freedom in what your object contains. Then you Then you have a much richer structure. You discover that all these Lax equations, so now written in this much more general setting. So in there, you have potentially an infinite number of integrable hierarchies lurking. You just need to fix all this data I was talking about. But one beautiful result is that the compatibility of these flows is ensured by the The classical Yam-Maxstor equation written in terms of this eta r. That's obviously that was known before, not in this language, but as a general idea. Now, what's really interesting is that now we can cast or find the meaning of the classical Yon-Baxter equation in variational form because it implies the closure. So now we have also an understanding in this general setup of the closure and how it relates to much older. And how it relates to much older and important notions of integrity. Okay. How do we fix, find examples? You fix all the data. You fix the points on the Riemann sphere, the order of the pole, you fix your Lie algebra, and you fix your co-agen orbit. With this data and you fix your R matrix, I'll recover the AKMX example, which was our starting point. But now you can do Was our starting point. But now you can do other things. So now take two poles: zero and infinity, semi-algebra, change your coager orbit, and change your R matrix. You get the sign of Gordon hierarchy. And you can compute everything explicitly and recover the results that Yuri had found a bit more brute force before. So that's how that's to recover all results. Now, new things, we have access to. Things we have access to the entire hierarchy of things called Zakharov-Mihailov hierarchies. I'm not gonna do the details of that, but already a new result there is that we have a trigonometric version of these models, which was not known before. And finally, so that's a well-known example in this case, which is the principal Kaura model, but in the Feyersticking convention. And really, one interesting thing. And really, one interesting thing is that you can couple hierarchies. All you have to do, so I'm going to do something a bit silly, I'm going to couple the AKNS hierarchy with the Falayer-Frashitekin model. And you could do that. You just take a linear combination of the two elements that would describe your two phase spaces. You put them together, couple them with a coupling constant. When this is zero, you have pure FR. This is your you have pure FR when this tends to infinity. You can show you have pure AKNS, and then we compute everything. You can compute the Lagrangians, you can compute the equations of motion, and you see clearly that's the AKNS model, the first, well, actually, sorry, that's the non-Schringer model, that's the principal Carol model, and these are the terms that couple them together. And it's very easy to do that, so you can create as many new integrable hierarchies as you are. Integrable hierarchies as your heart wants to do. Okay, and I think the rest I'm going to say for the sandpit. I prepared lots of ideas and suggestions, but I think my time is running out. So I'll stop there and I will thank you for your attention.