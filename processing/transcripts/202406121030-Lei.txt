Okay. Yeah, so the title is Data Reconstruction Attack, and I'm going to talk about different attacks and defenses and the way we understand it. This is work with my students, Han, and a lot of other collaborators. Okay. Okay. So almost all the talks start with how amazing deep learning is. So, and then they talk about the challenges. But instead, let's talk about challenge first. So there is potentially data leakage problem in different phases of machine learning. For example, when we want to train, like we want to do distributed learning, which means the data and the model. The data and the model are not located together. So, one of the goals for, for example, federated learning or multi-set collaboration is for the data to never leave each edge user or each industry. And they will collaboratively train a model. But the question is: even though the data has not leaved that user, does the local update still reveal the information about the data? Review the information about the data. And imagine, like, since we believe this framework is relatively safe because we haven't given out our data, we only use it to train the model. We probably just type some like very private things in our head users, like in our phones. Then, what how like frightening it is if the local update using the data can actually review the data? And it also, for example, And it also, for example, happens in fine-tuned models. For example, in the white box regime, when we see different versions of the models, they are fine-tuned with some potentially licensed or private data, and that's like the difference of the model review, the training data that is being used. So today, the question is: when and how does our observation. When and how does our observation, which is for example the difference of the model, will reveal the training data and potentially involve sensitive data. So when I say when and how, I want to understand what condition or what properties of the model makes the procedure more sensitive or more vulnerable to reconstruction attack and how I want to understand whether there is an efficient algorithm to actually reconstruct the data. Actually, we construct the data, and also how should we defend against it. Okay, so let me talk about the general framework. So, basically, we have potentially the data, and perhaps it is mapping through a model. So, that is potentially some features. And these are the protective knowledge. So, why do I mention the feature? Because originally, the data may be very high-dimensional. High-dimensional, include a lot of information. But after going through several layers, maybe they have like removed some redundant information, and there are few bits of information in the feature, which makes it more vulnerable to attack. And in federated learning or in the other procedures, we observe some matrix or vectors that depends on the data. For example, in the federated learning procedure, we observe the gradient that is computed from the That is computed from the objective function that depends on the data. And of course, also the model we used. And I would characterize the procedure of reconstruction attack as an inverse problem. Why is that? Because like our observation, you can see it's a function on the data, and the function is known to us because we know the model. So the adversary, they access this observation, and they also know the model, and they want to reconstruct the data. And they want to reconstruct the training data and perhaps some features from this observation. And similar as the inverse problem, some prior knowledge of the data may help the procedure. And to conquer this problem, potentially people will do some defense on the observation. So for example, they do not give out the actual gradient, they give out some perturbed version of the gradient. Down some perturbed version of the gradient, like they drop some less useful coordinates or they add noise to it. And so the defense procedure will hurt the reconstruction procedure and prior knowledge. And also some features we may reconstruct from the Widian will help us to reconstruct our original data. Yeah, so in the entire talk, I will also emphasize why this. Also, emphasize why this framework is useful. So, for one thing, when we frame it as inverse problem, it gives us some more tools from inverse problem that can help us understand the problem. And that's exactly what we will do. Yeah, question, clarification question. So I would imagine that one example, maybe the easiest of leakage, would be I am able to reconstruct one of the trainings or all of them, more generally. And so when one has access to the gradient, it's going to be aggregated over the mini batch or training exams. Yeah. So could you speak more about what precise Think more about what precise form of data leakage: do you really mean that the goal is to reconstruct each and every training example in the private set, or you're just trying to extract the distribution and then you can sample? Yeah, or yeah, thank you for the question. Question is about, so from the gradient, what kind of like what form of data leakage it is. So in reconstruction attack, usually we mean that like we want to reconstruct X1. Like we want to reconstruct x1 through x b as a batch size, like almost precisely. I will talk about like prior work, what target they are doing. So there is work that tries to reconstruct the distribution of the data, but that is actually less frightening because knowing, for example, the distribution of credit card numbers is not that frightening. And you said I have access to the gradient at every step of training. Step of training, but do I also have access to the mini batch on which that gradient was computed or just the value of the gradient? Oh, I only have access to this gradient that is computed from the minute desktop data. Yeah, and of course, we don't know the data, and our goal is to reconstruct the data. I was just wondering why. I'm just wondering why you're assuming that the attack happens at the level of the aggregated mini-batch? Yeah, because that's the only thing they observe. Yeah, but I don't understand the attacker when it does intervene in this process, just more intuitively. So you could also think that the attack may happen at the communication level of a single individual with when they're sharing not just like mini batch scripts, but individual creativity. So, you can assume the batch size is one. That's okay. But the batch size is actually what is happening. Yeah, but you only observe like the aggregated version. So, in each edge user, we generate some batch of data. We send the gradient update based on the data to this parameter server. And I'm wondering if this parameter server can reconstruct the data. Yeah, so in any case, So, in any case, we don't control what is the batch size. So, we also don't control which data it is. We only observe this, and we know it depends on the data, and we want to construct the data. I mean, we eventually don't want to construct the data. We want to prevent that from happening. But the goal is to understand when this is vulnerable. Okay, so more formally, the problem is. More formally, the problem is we have a batch of data and we have like a prediction function and the private learner have access to that. And the adversary, because they receive the model update, and they also they are optimizing over the parameter, right? So they, of course, they know the function, the node mode update, and we want to know whether they can reconstruct the data and to what degree. And to what degree of precision can they reconstruct the data? And as I said, this is kind of an inverse problem because we want to reconstruct the data set from the gradient we observe and the parameter for the function they are known. And sometimes the gradient has been added some noise, which is also common setting. Which is also a common setting in inverse problems. Okay, so now let me talk about prior work. So there are prior work on the attacking method. The most common prior work is on gradient matching or gradient inversion. So that is very straightforward because you know the gradient observation and you know the mapping that comes from the data to the gradient. Of course, you can just treat XIYI. Uh, treat XIYI, the data set, as the unknown parameters and optimize over them to match the gradient, your observed gradient, and the actual gradient computed. And of course, people also have some, added some prior knowledge of the data to aid this procedure. But this is completely heuristic because this is very non-convex and it's almost impossible to resolve. And there are other methods based on feature reconstruction through linear algebra tactics. That includes our work that we will mention and some other work. Like if we observe the gradient on like a two-layer neural network, we can use tensor method or like a principal component analysis to reconstruct the data. And there is also. And there is also some partial data reconstruction. So there are some work they can reconstruct part of the data, but they don't know in prior which data they reconstruct. They use some fishing parameter. You can imagine if your parameter is set at something that, for example, the model is like an invertible function, that maybe this can ease the procedure. Yeah, so these are almost all heuristics. Are almost all heuristics, and on the defend methods, these are not like new, those are like very standard methods. But I don't think we have fully understand like how useful they are and what is the utility privacy trade-off. There are ways like quantizing or pruning the gradient, which makes our observational dimension smaller and also draw part. The difference is jarpod, we uniformly randomly drop some of the coordinates. Randomly drop some of the coordinates in the observation, and the pruning will just remove those with the smallest magnitude. The mechanism or the reason behind is to reduce the observational dimension. And there are secure aggregation, which means like the local worker, they will communicate with each other and they aggregate the gradient securely themselves and then send the aggregate. themselves and then send the aggregated gradient to the central worker. So that the difference is just now the central worker are not like observing the gradient on the mini batch of only one site. It's observing the gradient calculated from multiple sites. So the batch size increases then which potentially make the problem harder. And multiple local aggregation, it's kind of similar, which means like in Which means, like, in one worker, we do multiple gradient steps and send out the gradient. Then, in the multiple steps, they might be using different batch sites. So, potentially, the data involved, the size of data involved has increased. So, these two ideas is like to increase unknown signals dimension. So, all these four methods, what they are trying to do is like reduce the observation dimension to the Dimension to the unknown signal ratio. And also, of course, add noise. That's reduce the observation to noise ratio. Okay. So there are some theoretical work, but I wouldn't say that they are tailored to this kind of problem. So when people mention privacy, every time I tell people I work on privacy, they will ask me. People, I work on privacy, they will ask me, differential privacy? Yeah, and I say no. Then say, oh, then what kind of privacy? And it's hard to explain. So I won't like list the definition of different privacy here, but what it tells you relatively it means is just the condition that you cannot distinguish any two neighboring data sets. So which means you cannot do better than epsilon better than random guesses. Epsilon better than random guessing. The definition of differential privacy is that. So, this is more tailored for membership inference attack, because what it's doing is it won't even reveal that one bit of information, whether the data comes from this data set or that data set. So, yeah, so this you can imagine this stronger than what we need, because we want to know whether we can actually control the data, which potentially will increase. Which potentially will increase, will include much richer information. And there is work that noticed that DP, the original DP is not really tailored for constructive attack. So they did not like introduce, but they emphasized that they can use Renee differential privacy. And the change is just the DP was measured, difference measured. DP was measured, difference measured in max divergence, and they changed to a more relaxed choice of divergence. But essentially, what they can guarantee is you cannot reconstruct the last sample where the other samples are known. But that is also not the setting we are interested in because we have no, why do I have the knowledge of the majority of samples and I do not know one of them. And also, it can be proved that with DP or Renee DP, they only have a constant conversion rate. So, if DP doesn't, it's not a really good tool for the problem, and I will explain why, then Renee DP is also not. They only have constant conversion rate. And now I will explain why DP is not a really good tool for this. So, there are some problems. So, there are some problems. So, I think the problem is just DP is too strong. Of course, DP is a very good tool, and we cannot deny its usefulness in the privacy regime, in the privacy problem. But for this problem, probably it's not very practical. So, most prior work when they mention DP and when they do perturbation to the gradient and say that it has will help give you a better DP guarantee of what they mean. DP guarantee are what they mean. So the most traditional DP guarantee is like if a model F satisfies F SF sensitivity. SF sensitivity just means when you change one data point in the input sample, the output will change at most SF. Then adding Gaussian noise with this variance will satisfy epsilon dp. So even though epsilon is very Epsilon is very is moderately large. Like, let's see how large this variance is. So, majority of the world just leave it here, saying that, okay, so this is the situation. So, we will need to make at a larger variance or we'll make SF smaller. But taking a closer look at it, you will know that how impractical this is. For example, in a two-layer For example, in a two-layer M with neural network, and I'm not doing super weird scaling, if it's just in a standard midfield scaling, then SF will scale with the number of hidden notes, which is huge. So I can imagine we add constant noise to every coordinate. But if you want to add order m squared of epsilon squared noise to every coordinate, it completely destroys any information you are transmitting. Transmitted. So it's not really practical. The reason it's not practical is too strong. And also, in some scenarios, it's not necessary because sometimes just due to the structure and one important thing you mentioned is what we observe is not a gradient calculated from individual data. What we observe is not what we can control. It's the gradient calculated. It's the gradient calculated from the aggregated data. And as a very simple example, if the model is, for example, linear and the loss is quadratic, then the gradient we observe is of this form, is summation of the data. Then, in this case, if we don't add any noise, there's no DP guarantee at all. Because when we change one sample, like the changing the output is the same. The same. It's not like we cannot distinguish these two data sets. But it's not possible to reconstruct individual samples unless there is prior information. But that is not the point of our discussion here. The point of our discussion is because of the structure of the problem, which means our observation is the applicated version, then it's possible that we cannot reconstruct any data even though it's. Reconstruct any data even though without a legit DP guarantee. So we want to study the problem, understand the problems in some other perspective. Okay, so instead, how we tackle the problem, we want to seek a very common trajectory in security, which means we find out stronger attack, which will trigger stronger defense, and then stronger attack again. Attack again. Because when we understand whether a defense method is strong enough, we want to understand whether it works on the strongest attack. And also theoretically, we want to establish algorithmic upper bound for the reconstruction error, which is aligned with the attack. So when we design some attack, we want to understand whether there's upper bound on the reconstruction error. So with the algorithm, The reconstruction error. So, with the algorithm, can we make sure that the reconstruction error is small? And when we try to understand defense, we want to establish information theoretical lower bound on the reconstruction error. So, which means like where is this defense? The information is not enough for us to reconstruct the data. And hopefully, we hope they can match in the key parameters. So yeah, there are ways to do that. Yeah, so the encryption. Oh, so when you encrypt the model, so encrypt the gradient and send it to the parameter server, they need to also decipher it. Also, decipher it in order to update the model. So they also still observe the lean gradient. Yeah, the question is whether the server can reconstruct the data. There is adversarial case and there is a curious, honest, but curious parameter server. Or parameter server. And also, sometimes this is passive. Like, for example, if they release the model and the different versions of model, can we like use that to reconstruct the data? And also, for example, in the project we are working on, there are multi-site collaborations. multi-site collaboration whether the worker they act they interact with those industry we have private data whether they can reconstruct data that is the question yeah thank you for the question uh so now let's move on to part one of the results so this is uh mathematics for deep learning so uh of course we also see two layer neural network results yeah and uh Yeah, and don't worry, this is part one. So we have some other things that go beyond that. Okay, so on two-layer neural networks, so you've seen that so many times. It's just the notation might be different. So two-layer neural network, the first layer is parametrized by WJ and wrapped after activation function. It can be general, and I will explain. And then the second layer is AJ. The second layer is AJ. So the observation of G is just consists of the gradient of the first layer and the last layer. It doesn't matter the form right now. So I'm just trying to explain the setting. And the parametrization is like Aj will scale with like 1 over m and w j will scale with, for example, unit random Gaussian. And let's And let's see some examples in this setting. I would say this is a bad example, which makes the reconstruction almost impossible. So for linear activation, let's see, when we calculate the gradient, the gradient of A, of the second layer, is of this form. It's W times a weighted average of the data. And the gradient of the first layer. And the gradient of the first layer is A times that same weighted average transpose. So W and A doesn't depend on data. And what we can observe, what we can reconstruct is at most this vector, which is a linear interpolation of the X. So we cannot possible to reconstruct individual X, but we can. Question? WNA? Do not depend on data? Those are the. They don't depend on data, those are the weights of the effort, right? Yeah. But while you're training, I mean, they will depend on the data. Yeah, so that's we don't consider it. I will tell you why we don't consider that. It's because, for example, founded learning, they don't control what batch size, what batch of data they are training. It's very possible W and A, they are trained on historical data. It doesn't depend on the current A. And the current data set, and you cannot tell that. And you cannot tell that. That's just too complicated. Question? Why not consider full gradient set instead of a five? And now the algorithm is being run in multiple iterations. So at different iterations, I get different gradients on the same day. So in fact, we're learning, they almost never get the same data. And they don't control what data. Yes, but perspective of analysis, one could first. Of analysis, one could first try to attempt to analyze full gradient settings. We are not really interested in full gradient. That's two harder problems. Because the batch size, yeah. No, I'm advocating is easier than in the batch case. Why? Because when you have the full gradient, you can have now multiple, you can have access to multiple gradients, right? Every iteration of gradient descent, you get a different gradient for the same data. So you get multiple observations for the same. So you get multiple observations for the same data that you're trying to. Yeah, but that's not actually what we observed, and we are not analyzing. We are analyzing the small batch size regime. Yeah, and in real federal learning setting, we cannot control which data we access. And actually, what's important here, we cannot query two times on the same data. Yeah, so yeah, that is a different problem than, but yeah, I think. Yeah, I think so. It's harder in one query, but I think Renee emphasized it's easier because we can observe multiple times of the gradient. Yeah, and in the quadratic activation, we can see that the gradient is of this form, Wj times like weighted second moment of the data. So similar thing here, although the observation contains a rich information. Observation contains a rich information that depends on the data, we still cannot reconstruct individual x, but only the span of x. And that will actually resonate when I tell you the analysis of lower bound. Yeah, so this is a message to you that, okay, in some kind of activations, it's kind of impossible to reconstruct the data. Sorry, so before you move on. Sorry, so before you move on, when you say you cannot reconstruct, you mean that if X followed any arbitrary distribution, you cannot reconstruct. But if you have some reasonable prior on X, then I'm guessing that the situation would be better, right? Because you can't, if we have additional information on the data, we can reconstruct it. Yeah, I will mention that. I will mention that. And as I also mentioned in the beginning, that a lot of things change if we like we know the distribution of the data. But that is not the point of the analysis. We can only analyze one problem at a time. So in the future, we can also analyze how prior knowledge of the data affects the procedure. Yeah, so our goal is to establish algorithms. Establish algorithmic upper bound and also information theoretical lower bound. So, let me first introduce the upper bound. The upper bound is among the choices of the data satisfying some like a good conditioning and some regularity condition, like the distance between the actual data set and the reconstructed data set. And the distance is measured in the like the best case point. The best case permutation because it's aggregated, we cannot know the ordering of the data. So, as long as after the proper permutation, we can match some of the reconstructed data with the actual data, that's fine. As is the data set. Yeah, so basically, we measure the average L2 norm between the reconstructed data and the actual data up to permutation. And then, when there's no defense, the observation. And when there's no defense, the observation is just a gradient. With defense, the observation is some defense function applied on the gradient. And this is also relevant to the question. Our focus is on the property of the model architecture and the weight. The properties include, for example, the number of hidden nodes and also the data dimension and also some property of the defense method. For example, how strong is Methods, for example, how strong is the dropout? Like, what is dropout ratio? When you do clipping, what is the threshold? This kind of problem, not on the data. So, on the data, probably on the data, this is a different problem. We'll probably investigate it in the future. Yeah, so on the arithmetic upper bound, this is what we get. So, with no defense, we can request. Defense, we can reconstruct the data with like reconstruction error scaling like V times square root of D over M. So I don't think the best size dependence is optimal, but I will talk about like the dependence on D and M matches lower bound. But what is the message? What it's saying that, so if we don't consider prior knowledge of the data, it's arbitrarily d-dimensional information. D-dimensional information. Then we need the number of hidden nodes to scale with D and also B squared. That is not true. Yeah, so for a long time, I also wondered whether this dependence of M scaling with D is necessary or not. Because if you do parameter counting, the number of dimensions of the data is D times P, batch size. And number of parameters we observe, like in the gradient, is m times d. So, if you do parameter counting, it seems we only need m scale with d. We don't know whether this dependence is optimal or not. But our lower bound will tell you an answer. And with local aggregation, as we said, this is essentially sort of like you increase the batch size. Increase the batch size to k size. If you do k times of local aggregation, so the upper bound becomes this. If you do sigma squared gradient noise at individual coordinate of the observation with gradient noise, with varying sigma squared, then this is the upper bound. So interestingly, this doesn't affect theory contraction error that much. Yeah, it only has like a constant effect. Like a constant effect. That's also why, like, kind of why in VP you need to add such large noise to make a difference. And with DPSGD, which means you first clip the gradient. If the gradient magnitude is too large, you project it to smaller. And then as noise, then this will, your reconstruction error will scale with something like this. And p drop out, which means you randomly drop a proportion of p coordinates, and then the essential number of hidden nodes scales with one minus p times m. So reconstruction error becomes like this. And interestingly, gradient pruning, we don't know the result. At least with our attack, it doesn't work. I mean. Attack, it doesn't work. I mean, in theory. And in practice, we also observe that gradient pruning is the most effective defense method. So let me tell you a little bit about how we get the upper bound. So this idea is from recover the third moment of the data. So this is relevant to what So, this is relevant to what I gave you, the two examples. With linear activation, you observe some linear interpretation of the data. With quadratic activation, you observe some version or some weighted sum of the second moment. But you know the second moment is not enough to identify individual data. You only know the span of the data. But interesting fact in tensor is like if you observe higher order moments of the data. Moments of the data, then the reconstruction of individual size of the data is unique. So by looking at this, as I said, we want to estimate a tensor, TP. Yeah, it doesn't work. Tensor TP. Well, P is larger or equal to 3. And it doesn't really matter what this looks like. What's important is this is only a scalar. Scalar. And this is a summation of a weighted sum of the higher moment of the data. If we can reconstruct some form of this, then it is provable that we can uniquely identify individual xi up to the direction when p is greater or equal to 3. And as a matter of fact, we can like approximate this tensor. approximate this tensor for any p. Just when p is higher, the approximation will be worse. But we only need p equals 3. That is enough. So we observe the key the key technical part that makes it happen is with things lemma. Because if we have some form of a function g that depends That depends on some vector A, transpose unit Gaussian vector, times the Ermit polynomial of it. Then this is in expectation is equivalent to taking the peace derivative of G. And in our problem, our observation of the gradient is actually of the form that is how it interacts. How it interacts with each Xi is through Wj transpose Xi. So if we view it as a function of Wj, if we can take derivative of each Wj, then this Xi will be pulled out. So if we take the derivative of this J multiple times, for example, three times, then this Xi will be pulled out three times and form the third order tensor. Third order tensor. And that is the key part. Maybe let me finish because and the key thing here is we observe that our gradient is of the form that interacts with each data point in the way of like xi transpose w. And we can set each w as random Gaussian. And then we Random Gaussian, and then with stains lemma, if we have a lot of beta nodes, we can approximate this expectation. We'll be able to pull out each xi from the function. So as a matter of fact, we will set a tensor which of the form g wj, which is just a j's coordinate of the of our gradient times the earmute function of w j. function of wj which we can compute and which will approximately equal to its expectation. And with Deng's lemma, this will be the p derivative of this j. And this g will pull out ej xi for p times, which will equal to the tensor we are interested in. So now you can ask questions. Oh, we we don't like prove we can get an efficient yeah yeah that is uh possible. Yeah, actually that is like uh yeah, that's essentially the problem because we can actually prove that we're constructing data is MP hard. So why do we have an algorithm? That that is mostly about like a statistical result. Statistical results. Yeah, so that's good to have a tensor decomposition, but it doesn't guarantee an algorithm. Yeah, thanks. Question? So isn't it only true that your W's and your A's are gaps? Your W's and your A's are Gaussian at initialization? Yeah, so we're interested in what kind of weights make it vulnerable. So in the upper bound, this is a specific setting of the weights. In the lower bound, it will be general. Yeah. So I would say if it's like not a random weight, then we have algorithm or we have like gradient inversion algorithms, but it's almost not possible to do analysis. Possible to do analysis. Yeah, thanks. And also, so I think you could probably do the tensor decomposition if there's enough like signal in your underlying tensor, because the tensors also have statistical and computational gaps. And as long as your signal is above the computational gap, then you can approximately recover it. Yeah, yeah. For example, if it's orthogonal data, then yeah, it's computational trackable. Yeah, but I'm saying yeah, in general it's not. Yeah, in general, it's not. Yeah, so it takes time to digest, but the point is, we have ways to estimate the third moment of the data, and which gives us a unique reconstruction of the data. And what's important is in the procedure, we can prove this procedure gives you a reconstruction error of orders where you do Error of order square root d over m. So, of course, this is an example of no defense. So, with defense, this will be a similar analysis. And we have got like what I showed you, those reconstruction error that depends on the key parameters of the model, like M, and also the defense methods. And also, I want to mention that because in the beginning, I talked. But because in the beginning I talked about that linear or quadratic activation won't work, what is the property making it work? It applies to the case where the third derivative of the activation is not trivial. Or the fourth derivative of the activation is not trivial. So this applies to sigmoid, 10H value, Liquid, all the common activations. Activations. Yeah, and actually, our lower bound will also coincide with why linear and quadratic won't work. Question? It was not clear what is the role of P. So are you saying that P equals 3 is enough? I'm saying P equals 3 is enough, but sometimes if the third activation is zero, then we will use P equals four. Yeah, but we only use these two cases. These two cases. And could you explain again exactly how you can compute the moments if you only have access to so? Who computes the moments? It's done by the server. And how does the server compute the moments based on the gradient only? Yeah, what I'm saying is we computed this. This is coordinates from the gradient we observe. And this is a function on the weights we also. This function on the weights, we also know. And what we say is this approximate to the third moment of the data. Yeah, so then we can reconstruct the data point. And the error comes from the approximation because this is not exactly equal. Thank you for the question. The weights I have access to might have a distribution that is very far from the Gaussian for which that approximation is done. Yeah, indeed. That's what I mentioned, that when we calculate the upper bound, we are interested in like we will assign the weight because actually for Google, they can carry whatever weight they do. Yeah. And next, we will talk about lower bounds. Next, we will talk about lower bounds. The distance metric is the same as before, but the lower bound again is worst case among the data. So, again, our focus is not on the property of data, only the data dimension. And it's minimum over all the estimators, or in other words, minimum over all the attacks. And in this case, because we, you see, our observation is a deterministic function of the data, we don't want the lower bound to be pure. Don't want the lower bound to be pure computational combinatorics. So, we are interested in the case where we have some noise in the observation so that we can have information theoretically go about. So, when no defense, we actually mean we added some noise to the observation. And with defense, we mean that observation is the specific defense method on the gradient plus some noise. And again, And again, our upper bound will also depend on the properties of model architecture or the weight and the defense method, not the prior knowledge of data. So this is the upper bound we get with no defense. You can see that it scales with square root d over m. So the dependence of d and m is optimal. This also answered the question that I've been wondering for almost. Question that I've been wondering for almost one year because you see the number of if you do parameter counting, the gradient is of dimension roughly m times d. So it seems like we only need m to scale with a b, but that's not. Oh, we have the lower bound here. I think that is because of the structure of the aggregation. So that is not that easy to reconstruct the data. And with local aggregation, so our lower So our lower bound, maybe it's because the caveat of our analysis, we don't have a good dependence on the batch size. They only have the proper scaling of the D and M. And the properties like with DPSDD, we clip the data with threshold C. So we have the proper dependence on C. And with P job out, we have P here. So this dependence on the like strengths of Dependence on the like strings of the defense method and the data dimension, and what he didn't know they are optimal. Yeah, don't worry. I will definitely honestly talk about like the limitation of our like analysis. But first, let me talk about how we get the lower bound. So, this is from the Bayesian Chromarl. So the minimax risk is lower bounded by this form. Sigma squared is the noise variance and j is the Jacobian of the forward function. So our forward function is the gradient of the objective. So it's the Jacobian of this. If after defense and this Jacobian on the defense after the gradient. So, the key factor that determines this lower bound will be like how is J modified. Imagine that if like the J, if we are doing dropout, that some columns of J will be a zero or pruning similarly. Yeah, and with connection to the linear and quadratic case, that is when J is singular. That is when J is singular. So when you have a linear or quadratic activation, the Jacobian will be singular. So that it will be worse. Okay, so some takeaways on the theoretical results. So I believe this is a promising framework. Why? Because at least with DP, we got something very unrealistic. And now we at least have some match. At least have some mesh dependence on like dm, the dropout ratio, and the threshold, a c, the threshold of gradient clipping. So I want to say that we may call for more analysis under this framework, for more general model architecture, etc. Yeah, and again, the analysis focuses on the properties of model architecture and weight. So no prior information of the data is assumed. Is assumed. And our lower bound analysis technique is general, but the upper bound is more restricted on the weights we set. Yeah, so I would say we need new tools to go beyond two-layer neural networks and to go beyond the random weights if we want to have a different upper bound for a general case. And this method, even though it's Method, even though it gets a scaling within each type of defense, it cannot properly help you to compare across different defense methods. Because if you look at this, how would you compare these two? Not that clear. The reason is what you actually want to see is the utility privacy trade-off. And that's why we have the part two to go beyond that. Go beyond that. Yeah, to go beyond two-linear neural networks, we actually have a more practical method than apply for general neural networks. Then we have this strong attack method that also utilizes tensor method we described, but also combine it with screen inversion. And the key idea is: if the last two layers are fully connected, then we can treat. Connected, then we can treat the intermediate layer before the last two layers as input, and we can reconstruct those two layers. And we can use that as the feature that helps us to reconstruct the input data. Yeah, so it's not that important to understand every component, but let's say we have a strong attack method and apply for general neural networks. And what we observe is indeed it did it can beat the other attacking method on different defense and in the original new defense case yeah and the reason we want to propose stronger attack is to properly evaluate the defense method uh let's see here in order to compare across various defense types we want to explore the utility We want to explore the utility privacy trade-off. So, I want to see comparing those defense with similar utility loss, meaning that after they perturb the gradient, that gradient also helps you optimize the objective to some extent. And if the utility is similar, we want to compare the strengths of this defense based on this, which is the reconstruction error. Which is the reconstruction error on the strongest attack possible? Yeah, because we don't want the defense to work on a weak attack. We want it to work on all the attack methods. So might as well on the strongest attack. Yeah, with that, we have our analysis. So that's also why we want to propose stronger attacks so that we can have proper evaluation on the defense. Yeah, so indeed, so we can draw some figures like this. So the x-axis is the training loss value at convergence. So this roughly measures the utility that is kept after you perturbed the gradient. Because the smaller the loss it is, the better the utility of the gradient you observe it is. Yeah, and the y-axis is relatively related. Relatively related to the privacy because this is the reconstruction error. The larger it is, the more private it is. So we can compare different types of defense methods. And the more left it is, the better it is. Because on the left-hand side, it means it has good utility because the the pain loss is small and it has also good privacy because the reconstruct error is high. The recontract error is high. So you can see that if I compare the different defense methods, creepy is not doing anything because it is one step. And the pruning is on the left-hand side. And adding noise is relatively clear. And drop out is here. So and so this is a plot about the utility privacy trade. About the utility privacy trade-off on our method. We also plot it for comparing all the attacks, which means each dot will be the strongest attack on that defense method. But that just looks a little messier, but the information will be similar. So I present this image. Question? Yeah, thank you for the question. The question is. Thank you for the question. The question is: What does the different dots mean? So, because there are different strings for the same defense method, for cliff, this is the same because you just cut it to the threshold. Now, for the others, it's like pruning, you have different pruning ratio. And for adding noise, you have different variance in the noise. So, there are multiple rounds with multiple different. Multiple rounds with multiple different strings, different strings. Okay, so also we can go beyond images. We can go beyond continuous data. We can do discrete data like text. We can do dynamic data like time series. And actually, we worked on the large language model as well. So, basically, because you see the idea of feature reconstruction, as long as we get a good embedding, we can also like. Embedding, we can also like do some proper way to reconstruct the data whether it's discrete or not. If it's text, maybe we can use like some prior perplexity, but it's more for the text data. And as a matter of fact, our method also beats state of the art in reconstruct the text. I can compare. So we think we use BERT, and these are some examples. These are some examples of the reconstruction. And we also have some, we also do some study on some time series data in medical images, but it doesn't work so well. Okay, so finally, this is the discussion. So I explained why DP was not an ideal tool to study data reconstruction attack. Reconstruction attack. So, I call for more theoretical analysis under the inverse problem framework. And we can also like for the lower bound, we only studied the information theoretic lower bound. Maybe we can study the computational barrier. And also, we need new tools to go beyond to linear neural network for upper bound. Also, as I mentioned today, the discussion is focusing. The discussion is focusing not on the data, but on the properties of the model. So, to study how data properties will affect the vulnerability of privacy attack is also important. So, that maybe the edge user knows when, oh, so for this set of data, it is more vulnerable to be expected. So, I may want to add more to perturb more of the gradient. And also, based on And also, based on the strengths we defined, which is the worst-case scenario about the attack, this means when a new and stronger attack method comes, I hope they can also evaluate properly. Yeah, so at least based on the current attack methods we compared with all the different types of attack methods I mentioned in the prior work, our attack is the strongest. Attack is the strongest, and on our strongest attack, we notice gradient pruning is the most effective way. Okay, thank you. That's all of it. Yeah. Yeah, because of the time, so I didn't answer every question very long. So now feel free to ask questions. Maybe I can start with a very high-level question. So grouting descent is just one way of learning from data. And this kind of in-context learning is emerging as kind of a new way of learning from data. So I'm just wondering if. Data. So, I'm just wondering if your result applies to in-context learning, like learning from examples, training examples in context. Yeah, yeah, that's a good question. So, the question is, like, gradient design is only one way to process the data. What about in context learning, et cetera? Yeah, so one of the main messages here is that we want to study the problem as an inverse problem where we know the forward function, et cetera. And of course, in this concrete example, we have the gradient descent. The gradient is end, and we calculate the gradient, etc. In a different method, we should also study it in a different way with the forward function that depends on encounter learning exactly. Yeah. So I wouldn't say it's directly applied, but I think the high-level idea applies. Yeah. The high-level idea is also one of the most important thing I want to tell you, because I don't think people studied the test inverse problem before. Yeah, actually, the theoretical Yeah, actually the theoretical understanding was just very scarce.