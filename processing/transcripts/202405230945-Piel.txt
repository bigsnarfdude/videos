Okay, so let me still just turn this off, maybe so you can see better the screen. And so, Robert, you will speak to us about adaptive structure present finite elements. So, it's finite elemental. Yes, correct. Thank you very much for the introduction. And thank you, everyone, for having me here. I'm really enjoying my time. Here. I'm really enjoying my time. And I want to talk to you about what Martin just introduced. And the first thing I want to do is kind of use the motivation to unpack the title. Then I want to quickly cover what subdivision is, because we haven't had a talk about that yet. And then after that, I'll show you the construction that I use to get. That I use to get the spaces that I use for approximation. So, motivation-wise, I'm interested in quite feasible problems in the sense that I want to simulate physics. The community that I kind of come from in this context is the one of geophysical fluid dynamics. And so, we want to basically simulate climate more or less. So, we want to do long-term forecasts of weather and Of weather. And to do that, all the conserved quantities of our physical system are super important. I guess you could all imagine if the energy isn't conserved and it just keeps increasing, like in 2000 years, the result doesn't make any sense anymore. It might not make sense in two weeks from now, if the dissipation or energy accumulation is very bad. So we want something that's exact in that regard. And there are plenty more conserved quantities that we want to respect as well as we can. Can and the next thing is the full 3D Navio-Stokes thing on like the stratosphere, for example, is too complicated. It's a very large domain. It's like a lot of effects that are coupled. It's not just Navier Stokes, it's coupled with all the thermodynamics, everything. So people don't really attempt that. What they do is they take 2D kind of layers that they like concentric. That they like concentric CS more or less, and they couple the different layers somehow. So, air that new moves between layers is modeled kind of outside of that. And we solve the 2D systems more or less on the surface of the Earth. And the whole system, by its very nature, has multiple scales and desired resolutions, right? So, if we are interested in whether we want to, I don't know, have it, we want to be very precise over. We want to be very precise over some large like town or whether it matters for farmers, things like that. We don't really care about the middle of the ocean, whether it be whichever weather. And just to kind of have an example for that, this is supposed to be a kind of like a tsunami wave that just travels across the ocean. And we have a rather coarse mesh everywhere where nothing happens really. We refine the wave front rather finely, and we do a bit, we are a bit more, we have a bit. Are a bit more, we have a bit better resolution in the wake, but other than that, we want to keep it as coarse as we possibly can or refine only when necessary. With that being said, let me talk about the construction we use to get to basis functions that have desirable properties for that. And what I use is something called subdivision. Before I actually talk about what is on that tonight, I want to quickly just I want to quickly just. Oh, anyways, I want to quickly just make a 1D example so that we understand what the next few things mean. So suppose we just have the line, and we have a few control points, we have a few pairs of X and We have a few pairs of x and y values, right? So just five points here. So we can do something. I'll just show the algorithm for now. So we do something, which is first we insert, we have the original thing. And in the middle of every interval, we insert one more point. Kind of. We only insert it in a We only insert it in a kind of topological way. We don't associate a y value yet. We just make the measure resolution bigger. And then we need to still find the values for these points. Everyone associates some y value. And what we do is we take some local neighborhood of the newly inserted point and we just wave the initial points and take a linear combination of their function values. So a very common rule in 1D is this one, where it just says if I One where it just says if I insert a point in the middle of an edge, right in the center, I'll just take the average of the two values, which is what I did here. And if I insert a vertex where the vertex used to be, for example, this one, I take, what is it? Three-fourths of this value, an eighth of this, and an eighth of this. So, what happens is now I assign new positions to these vertices. This one moves down a little, right? This one kind of moves up a little because it takes a one over eight of this value. And then what we have is something that is like a little more spread out, it's got a wider support if we did some piecewise linear interpolation of these things. Now we can apply the same algorithm over and over again, and what we'll eventually end up with is a spine. A spline. In this case, it's a d spline. This is a d spline subdivision row. And so this is a genuinely smooth object. This is how subdivision is a bit of an umbrella term in that sense. You can think of it as a spline in subdivision form, which is like a genuine spline. It's got the smoothness and all of that. And what I call subdivision is the algorithm. And the algorithm is this whole refinement process that eventually. Whole refinement process that eventually yields one of these generalized slides in the limit. And the whole purpose of this idea is to kind of construct splines around irregular vertices. So when I generalize this thing to 2D, which is made here, this is the idea here. It's called loop subdivision. It's the thing that I use for zero forms. You can see that for every vertex that used to be a vertex, we weigh it with. We weigh it with kind of its vertex one star, if that's what you want to call it. And for every vertex that I insert at an at a like previous edge, I have some other averaging rules. Just repeated averaging of things. And in this case, around regular vertices, this whole thing converges to a box plane. And around irregular vertices, interesting things happen. I'm not going to go into much detail there, but they are still like somewhat smooth. So this scheme. Still, like somewhat smooth. So, this scheme, I think, in the limit, the box plan is C2, and irregular vertices, it's C1. But that's at the very limit of the whole process. It's not what I use. I use these pieces versions of the algorithm. All right. So, what we have here then is, I'm still interested in the whole structure preserving kind of aspect, right? In my sense, in my case, that's the ground complex, which. In my case, that's the ground complex, which we have discussed already quite a bit. And what happens here is I just drew some basis, plotted some basis functions as examples. This is like the lowest, the top row is the lowest order complex you can choose in 2D because that's what we're interested in. So this is the gradient. This is the curl. This is the standard hat function. This is Nedeleg one. So the lowest order. So the lowest order H diff, sorry, H curl space. And this right here is a piecewise constant. It's DG0 for everyone who's familiar. And then there is a construction that where I have, oh, let me just mention one more thing. When I do this weighing here, I can assume, I can assemble all these weights into a matrix, right? I can just go take five values, make it nine values. So, take five values, make it nine values, and then put some weights in a matrix. And this matrix is what I mean by A. So, A0 is a matrix that acts on vertices to get refined vertices. A1 is a matrix that acts on edges and gets refined edges, because we're looking for corresponding one-forms, so we want to subdivide edges ideally. And A2 similarly does the same thing for faces to eventually give me two forms. And then there is a construction that exists in the literature. Exists in the literature that has a commuting relationship between the exterior derivative, that's put along this way, and subdivision, which means I preserve my Durham complex under subdivision. I go, I kind of smooth out my basis functions. They're not genuinely smooth, as I said, but I kind of make a support a little larger. And I still have a complex between all of these spaces. So, no matter to what level I subdivide, how often I repeat this process, I'll. I'll have a complex, which is quite a nice property. We care about that a lot in GFD. So, this is, I just wanted to write it down one time. This is the relation we were looking at, just the commutation of the subdivision acting with the subdivision operator and the exterior derivative. I just want to make a short little computation. I want to understand basis functions in this space. So in this space. So we think of the space as just the linear combination of the span of all these basis functions per edge, per vertex I have one. So the basis function here is a zero of a hat function, right? This is a hat function. I take a zero of it, I refine it down. And then I take weak the derivative by our commuting relation. We know that's the same as taking the derivative first and then refining. So this is what I wrote here. So, this is what I wrote here. And then we already know it's the same thing for Whitney forms for the lowest order, for this lowest order complex. We can express because they form a complex, we can express the derivative of this function in terms of this basis. This is what I did here. I have some coefficients that are associated to some sort of simplex because I'm lowest order. And then I want to formalize this set a little. So, I want to describe this set of. Set of simplicities I end up with. So if I take the derivative of the zero form, which is centered around like a subdivision of some hatch function associated to this vertex, I just get all its surrounding edges. This is how you express that in the one-form basis, the exterior derivative of this function. And similarly, I can do that for one forms, where this derivative of some basis function associated to this simplex will give me. Will give me, can be expressed as two basis functions for the faces that are colored in green. So, this is one thing. This is called what I call the derivatives then. So, it just associates to some k-vad, some k-simplex, a set of k plus one simplices. And if all of them are present in whatever function space I choose, I know that I can express the derivative of all the refined basis functions as well, because they commute. As well, because they commute. So, from there, I want to. So, this was the derivative part of it. Now, I want to kind of do the adaptiveness part of it, and then I want to fuse the two together. So, here what we do is this is a hierarchical approach. So, we can go ahead. What I did here, basically, I refined a head function, right? I created a basis function with the support that is a little larger, like the beast line, the same support as the beast line has in the limit. I have in the limit. But I could do the same thing for this. It's going to also give me a basis function. It's just a head function on a mesh with a final resolution, basically. And this is what I do here. I say, I start at some initial level, but I call L0 through L2. I take hat functions on that mesh and I refine them down to some capital L, which is just. Some capital L, which is just the amount of steps that I choose. And these basis functions have a nice few properties. In particular, they are refinable, meaning the basis function here on the final mesh, I can express the core basis function as a linear combination of fine basis functions on this mesh. And this relationship carries through to these basis functions. So it's still true for those kind of smooth. Kind of smoothed out. I don't want to say the word smooth, but you know what I mean, I think. The kind of widened out support basis functions. And I will use that to, as I say, mix and match basis functions. So whichever resolution I desire, I just choose like some mesh resolution around it. If I don't care about if nothing much happens in the vicinity of some point, I'll choose the big basis function. It's the same idea as for hierarchical pieceplacks, right? If some things happen. If some things happen, I'll go like medium resolution. And if there's really a lot of dynamics going on, I'll go finest resolution. But I've got like, yeah, other problems that require the high resolution. So now I want to once again look at what this implies for the simplices that are involved. I call this the refinement sensor this time, and I seek to express my associates to each k simplex a set of. K-simplex, a set of k-simplexes on a lower level on a higher refinement level, sorry, on a lower resolution. So, this vertex would be associated to all these vertices, this edge to all of these edges, and then this phase to this set of phases down there. And what it tells me is if all of these are in my function space, then this function is linearly dependent on all of them, right? I can refine, I can express this basis function as a linear combination of all of the ones associated. Combination of all of the ones associated to these vertices, and similarly for the other k-forms. So, now putting that all together, let's start with a course domain mesh. And I kind of partition this mesh into just different patches. For these faces right here, I don't define at all. These two patches, I define one by one level, and then here I go two levels deep. And I kind of Um and I kind of say I formally stitch them together because um we want to think of these meshes only as indicating where they put degrees of freedom. It's not that the basis functions are only supported on these faces, but it's just to indicate kind of where they are, how many they are, and so forth. And yeah, so that's what I mean here. We indicate what basis functions we choose to use, and I call them the active basis functions. So now, if we go ahead and just actually look at what we include in the function space, we can see that I'll just look at the patch on level zero for now. We've got these six vertices, right? We've got a few edges or the edges of this patch and a few faces, which are the faces of the patch. And I can do that for all the levels I just had, right? I will have this on level one, this on level two, and so forth. Same thing. So, this is a space for now. Now, I have to kind of put that together with the aspect of structure preservation. So, first of all, let's start from the lowest level up. If we look at some interior vertex here, we can see that it's derivative stensor, right? All the edges surrounding it here, they are all included in the space. So we're good. The exterior derivative of this is some is. If the derivative of this is in the space of one forms. If I look at things on the boundary, though, I notice that a few edges are missing. So here, all of these four, they are not contained in this set of orange edges. So I have a problem here. We see the derivative of this does not map into this space. Similarly, for two forms, right, if I look at the derivative stencil for this edge, for example, I don't have both of its adjacent faces in the adjacent faces in the in my green domain here, but only one. So for all of these spaces functions, I can't express them in the spaces. I can't express their derivative in the k plus one form spaces. So starting from here, what we can do, the only thing we can do really is add all of these edges as well. So we take every edge that is adjacent to one of these vertices and we add it to our function. And we add it to our function space. Same thing we do here. We also add the one ring of spaces to everything. This is on the lowest level, we don't have any other options because we can't exploit the refinability yet. There's no lower level. And on level one, things become a little more interesting. If we look at this vertex, for example, and I want to draw this vertex. We're looking at this vertex. So all these also matter, I guess. So first we want to check first derivatives then so, right? So we get all the surrounding edges here and we find these two are already included in this set of one. Were already included in the set of one form degrees of freedom, right? These two edges. So now, whoops, sorry, that leaves us with these two edges and these two. So these edges, once again, they kind of, they are on a course of patch, the patch, right? These two edges, and they correspond to faces that weren't refined at all in the beginning, the pattern level L0. So also there's no hope to ever express them. So, also, there's no hope to ever express them through final basis functions because there just aren't any here. So, these two edges here we have to add to the space. Similarly for these vertices as well. However, for these two edges here, they are on level one. They correspond, these two, they correspond to these two, right? And these two we can explain. And these two we can express through refinement. So, for these two edges, we already find they are in the function space, just because we have the right basis functions there anyways. So, they are in the space of that. After I've added them, correct. So, that's what I was going to do next. So, now we kind of see the topic, like the common theme here, right? We either include the basis function if it's not included, or we. function if it's not included or we include the simplex if the associated basis function is not included or we need to kind of show that we can get it through the refinement relation that it is there already so now we update our function spaces right we added all the one rings here and here which means this vertex is good similarly we can check that as well for these all the edges that aren't included here they can be refined either through edges here or here so we can always piece together the things yeah Piece together the things, yeah. I get it, this is now fine from here to here, but from here to here, are you still missing these? Um, this is fine because these edges here, they're not included. I only include the ones that touch one of these vertices, whereas this edge here is this edge, so it doesn't touch any of the vertices here. Does that make sense? Yeah, so you're asking about the derivative of this edge here, right? But this edge isn't even in our. This edge isn't even in our space in the first place. It's not, we don't. The boundary one is not included. Edge, right? We don't need to worry about it because it's just simply not in the space. This is what we do here. And yeah, we can see now for this edge, for example, it's the one I've been talking about previously, corresponds to kind of these two, and they have the full refinement stencil included in the degrees of freedom now. And also, that's something that I did because that comes up in geophysical fluid dynamics. Geophysical fluid dynamics here, we've got an interface here between things on level two and level zero, right? It's not adjacent levels anymore. We jump more than one level. So I kind of insert the ones on the boundary back. Because otherwise, let me paint the picture this way. Let's say we've got the boundary here. And this part here has all the core spaces functions. And then from here, we start inserting only fine ones that are two levels final, not just one. Levels final, not just one. So, we've got a lot of transport problems in geophysical fluid dynamics, which I guess makes sense. We transport something along like water or we transport along wind or something like that. And then every time you transport something across this boundary this way, you kind of have some artificial rigidity, if that makes sense in your discretization. So, you would kind of have. So, you would kind of have reflections here, meaning if we think about energy, for example, the little part of the transported energy will be reflected back because this is, in a sense, it's like mechanical rigidity almost. And in particular, because we preserve energy, we know that everything that doesn't go here will be reflected back, which is something we don't want to have. And we mitigate that a little by adding the intermediate. By adding the intermediate basis functions here to kind of can't really put it rigorously here, but to kind of make the communication easier, to make the transport across the boundary easier before across this interface. So this is why we add those. And then also we have to add kind of V's and V's, right? This is just the consequence of adding those. Yeah, and with that, this is the basic construction already. It's quite visual in the sense that you can just look, you can see what the stanzas you have. you can you can see what the stancils you have this is because you've got this commutation right that always that always helps you figuring things out on very small stancils and then you just yeah by construction you're you're you're good to go as you as you refine as you use subdivision actually so now with that i want to just quickly go over uh one numerical experiment which is not related to geophysical fluid dynamics this time this is the test case for structured preservation Test case for structured preservation, for the round complex preservation in this case. I think we've had it mentioned a few times. It's the standard Maxwell eigenvalue test case. And yeah, so we look at curl U dotted with curl V equals lambda U V. And so V spaces V1 are the ones that I just spanned. Due to lack of proper notation, I haven't introduced any, I just call it V1. I haven't introduced any. I just call it V1. Of course, we can write that down precisely, but yeah. And the analytical solution, that's the interesting part, has these eigenvalues. M and N are like wave numbers that come up in the solution of this eigenvalue problem. And what we care about is reproducing this spectrum precisely. So, what happens if we don't have the round complex preserving discretizations is we just add like screw. Is we just add like spurious modes here, or in the worst case, we can't even distinguish the eigenvalues really. So, we really want to meet those. This is our benchmark for saying we've got a nice one form and we've got a nice complex here. And so, first of all, I chose this mesh. It's a mesh of the domain zero pi squared. And you can see I just kind of randomly refined regions. We go from this. We go from this is level zero all the way down to like level four here. And also, no, this is a partition of the mesh. Of course, it induces some degrees of freedom that I haven't drawn on here, but yeah, so the ones, right? The fine vertices, sorry, yeah, the fine vertices need some like extra fine edges here and so forth, but I haven't drawn those. But we can see that the spectrum, at least for the 10 first eigenvalues, and I've checked more, of course. More, of course. Look pretty good. So we kind of conclude from the simulations as well that we can observe the structure preservation, we can observe the preservation of structure we were hoping for. And we can use that for adaptive refinement. And just the very last thing, I want to just show you the eigen modes that I compute for eigenvalue two and fifty eight, because I don't know if I found it looked nice. I don't know if I found it looked nice. And you can see that they don't like to the eye, at least. Also, if you zoom in and everything, they don't exhibit any artifacts. They just look nice and smooth, to the eye, at least. Of course, there's a lot of rigorous analysis missing. This is very much a prototype and a hundred of open questions and a lot of things are probably, I don't know, very much to be determined. But yeah, this is a preliminary result. And And this is the discrimination that I've been working on and now want to kind of continue working on and see where I can take this whole thing. Thank you very much. Yeah, thank you very much. Very nice presentation. Just two questions for you. So you have to have Uh, you have to have a uniform finding point for this, right? Right, this is a this is a very very good point. So the answer is no, because the whole subdivision kind of framework is built to treat irregular disease, right? The problem with it is, the problem that I have during my simulations is that actually the test case requires that I have the exact boundaries so that my analytical solution works. And this is something that like exactly. Like exactly getting the boundaries right isn't super easy with like if I interpolate the domain with slides as well, with subdivision in that case. So I need to kind of work around that. It does work. I want to eventually move to the sphere, right? So I won't have boundary issues because there's none. So in that case, I'm kind of fine. And here this is, yeah, this is a restriction due to the boundary that I mean. Okay. I mean, okay. All right. So you can basically there's no requirements of the match regular layers. Maybe I should have, I should have drawn that differently then. So these things, the derivative really, it just picks out all the edges that touch the vertex. It doesn't have to, there's no need for it to be six. It's any, it's like the exterior derivative for the standard lowest order CG1. You can just take out all the edges next to it and then, and then the other. And then the only other question was: so, this operator that you have, that the subdivision, and then you have averaging that you do. I mean, this seems like some sort of like geometric Laplacian that you've implemented here, where it's like it's smooth. Can you reinterpret this in some more abstract framework? I'm not too sure about that. I'd like to redirect the question. I don't know, do you know really if this is anything that you can relate to the Laplace? Anything that you can relate to the Laplacian? Yeah? Yeah. Sorry? So why do you use as a parameter domain 0 pi squared under the domain? One thing. Oh, sorry. I mean the domain is squared. It's like a 1D thing, but I just have a tensor product. Oh, because the analytical solution then is like the eigenvalues, you just scale with it. So I could also go like 2π and So I could also go like 2π and 4 pi and so forth to get as the eigenvalues. Just a scaling thing to make these nice. It's easier to look for that than for like 17 pi. Yeah, yeah, I would just need to rescale. That's perfectly fine. Can I ask you the numbering of the number in this table? Oh, sorry. This is not the numbering. This is like the first eigenvalue is one, second is one, third eigenvalue is two. This is not a numbering. Sorry, this is what this is supposed to be. Supposed to be? Four and five is five minutes. Oh, sorry, there's only one four eigenvalue. The next four corresponds to the site. Yes, yes, yes. This one is a five. Sorry. The eigenvalues, I copy-pasted wrong. The eigenvalues are these. Yeah. No, no, no, there's only one. Yeah, yeah. Sorry, there's one thing is only once. So there's two fives and two fours. Right. We've got two fourths and two fives. Five and this tree doesn't exist, it's supposed to be four. Sorry, yes, yes, yes, yes. This is an enumeration error. Yeah, the tree doesn't supposed to exist. Yeah, refused on the spot. Nice, yes. I have two questions. So when you do the hierarchical replay, you have to add these functions. If you look at the supports, instead of looking at the functions, because in your meshes, the meshes are related to the functions, right? To the patch, try to see whether the supports are you refining the same supports for every yes, yes. So, what I do is when I look at the, let me just quickly. So, this I also drew the things on the mesh, on the support basically. So, the support of this basis function is like the two ring more or less. And the same is true for these, but now it's the two ring of the small mesh, right? So, you can see the two ring of this, like if I take the two rings of all of This, like, if I take the two rings of all of those, it exactly adds up to the same kind of thing. And my other question is: we have this paper and we're now working with this patient, the fact that with theoretical these plans with a tensor product, if we refine arbitrarily, we may get the scroll seigen values. Sorry? We may get the scroll second values. There are some images where you refine a square and then another square. If they overlap, you have to be careful with the overlap because we may have square second values. Okay. And it's because. Per second value. Okay. And it's because what we remove from the cost space and what we add from the final space have different topologies. Do you know if this can happen in your case? I mean, situations where you find like two squares in a diagonal and then you add functions. Have you tried this kind of mesh? No, no, I haven't tried it yet, but I'm very happy to try it. I don't know. I need to think about that. Maybe you can tell me exactly what you did later. You can tell me exactly what you did later, and then yeah, thank you. You said in the eye file norm, it works, but you have the exact solution in that case. Yes, yes, I check like the L2 errors compared to the yes, yes. The question kind of is what sort of norm do you choose? Because so I can obtain a solution here. The question is, I can keep refining, right? Because in the limit, I get some spline. I in the limit I get some spline. Like, do I measure it like a spline? Do I measure it like a that's the kind of question? I mean, they all are kind of similar at some at some point, like it converges, but at some point it also converges rather quickly. So the differences between the like seventh and nine seventh and eighth steps is very, very small, but it's still like questions like, well, how do I want to think about it basically? But yeah, maybe arrows to do converge to the analytical solution. Analytical solution. I have a very nice question. Sure, no worries. Can you go to your mesh, please? Which one? Yeah, the computational example second. Yeah, sure. So if all the guys were triangles, then this would be the finite element in this. Yes, if I so the space that I have is, I can just include it in the nether-like space that is globally the finest level. No, I mean, just very, very simply. No, no refinement whatsoever. No refinement whatsoever. Yes, then it's in the Nedlek finite. Right. But you have to be careful about what mesh. So if you look at the Netherlands on the finest mesh that I consider here, then the answer is yes. It's not in the Netherlands space of like level one. If I have had everything on level one, like on a coarser level than here, then it is not, not in not a neterlake. No. Okay. It's going to be my next question. So kind of that's that's what what all of these approaches you described. So it's kind of even on the It's kind of even on the coarsest mesh, you kind of see the finest one, that's it, and then you re-interpret the coarse things with that procedure, and that's what it would be on the coarse mesh. Sorry, what's the question? So, you're saying on the very finest mesh, this could be just another finite element. Correct, then you said on the coarse mesh is not the case, okay? Right, because kind of the coarse mesh C is the very finest mesh that you have in the background. Yes, yes, so for quadrature, because because. So, for quadrature, because I still take piecewise linear interpolations of these refined things. So, the whole quadrature works on the finest mesh, which is something I want to change because it's not strictly necessary. But it was easy to implement and good for like a prototype to see what happens. But yeah, the quadrature is exactly done by including everything in the finest Netlex space and then just brute force computing it, which is okay for toys. Which is okay for toys like this, but it's certainly not okay once I move to the to like the scales of uh geophysical fluid dynamics, then that's not okay. Yeah, so it's again I have a question for myself. So on the coarse mesh, the C is the finest one, you would kind of start from the coarse data like basis functions, but you would apply to them the fishes on the board, which kind of speaks of them, and then this would be the result of your computer. And then this would be the result of your computation on the curst. Oh, yes. So, yeah, I keep thinking, I think about it like as dimensions. So, this doesn't increase the dimension of the space, right? Yeah. So, yes, yes, certainly. But it changes the functions, right? Yeah, it changes the functions, but not the dimension. So, yes, so the actual result I would get after solving a linear system are coefficients on this phase, right? Because this is the matrix will be different, it'll be a different method. Different would be a different method, yeah, because you don't work with the lowers or the forces mesh thing, correct? I do some, I make the supports a little wider, I get some like I increase the bandwidth of a matrix, I get some more kind of communication in between the elements. And so this is my last question. So what is on this coarse metric as it kind of means that you kind of increase the smoothness of your test functions, right? Of your test functions, right? Something like so. So the kind of the difference of the difference of in refinement levels of the coarsest mesh and the finest mesh, they tell you how far into the process I get. If that makes sense. They'll never be genuinely smooth, unless I do it infinitely often. And also, depending on what level I start, this determines the support of my basis function, right? So if I start defining a basis function here. Basis function here, I will get something with support, I don't know, like this and the adjacent two elements. Whereas if I start from a basis function on the next level, one finer, I get a smaller basis function in the end after doing all the refinement. Does that make any sense? Yeah, okay. But what you just told me with me helping to understand what you're doing. Any further points, please? Announcement for your side? Coffee paper. Okay, so thank you again. I think so. Just believe that the schedule is alive. And that is pinky. That's a great thing. Everyone got a television for a long time. Vision for 122, yeah. I want to eventually take it to the actual smooth thing. I don't have to because I mean, I honestly think that only I saw you got substitutions. That's the really yeah, you just take it today from before. I just got it from Turkish jewelry. I really like it. Oh, over the. I really think it's memorized.