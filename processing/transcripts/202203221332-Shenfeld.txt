Uh yeah Schenfeld from MIT present the sequel to The Last Talk of the Morning and he's going to tell us about transportation along Elange Mountain Senior. Thank you Robert and thank you the organizers for inviting me. It's great to meet many of you for the first time and I hope to do it again in the future. So this talk is on the one hand is completely self-contained and I tried to design it in such a way but on the other hand it's In such a way, but on the other hand, it's also very much related to the previous talk that Dan gave. And I will hint the connections as we speak. Maybe just say the motivation. So, as Dan explained, our interest in transportation maps comes from the study of functional inequalities, the idea that if you can, if you have, we know many nice functional inequalities which hold true for the Gaussian, and then if we want to put True for the Gaussian. And then, if we want to prove them for other measures, all you need to do is have a transport map which goes from the Gaussian into your target measure with Litchitz properties. And the obvious choice is the optimal transport map, but for it, a lot of time we don't know the desired Litch properties, so that's why we designed the Brownian Champ Sport Map, which indeed gave us a lot of what we wanted. However, something, the Brownian Champ Sport Map, Thing the Brown Transport map is not great for, and that are for things which have to do with maps from the same dimension to itself. Meaning, the Brown Transport map was a map from infinite dimension into RD because it just pushed forward the wiener measure into the target measure. But sometimes you need, and I will show the application for which you need transport map from the space itself. So you need a measure from the Gaussian measure on Rd into your target measure on Rd. And we were looking for something like that that still has nice properties, and eventually we found it, and this is what this talk is about. Okay, so how do you transport a measure mu into a measure mu? So one way of doing it, you can just use Langevin dynamics. So what does that mean? You sample your initial position from the Position from the measure μ, that's your source measure, and then you just run the Langevin dynamics, so it's a stochastic differential equation where we have a Bt term, which is just the Brownian motion, and then we also have a drift term, and the drift term is the gradient of the log of the density of this target measure nu with respect to the big measure, and then we evaluate it at yt. And this measure had a step. And this measure has a stationary, this process has a stationary measure, which is just the measure μ. It doesn't matter where you start. It would always converge to the measure μ. So in some sense, you can think about it as a process which takes mu into mu. Okay, so let's take a Qt to be the Langevin semigroup. This is a semigroup associated with this process. So what is exactly the definition? If you have a function eta, any test function, then qt of eta of x. And then Qt of eta of x, it's just the conditional expectation of eta at yt condition on y0 equal to x. This is the definition of the semigroup. So once we have the semi-group, we can define an interpolation between the measure mu and u as follows. How do we do it? So let's define the measure rho t as the semi-group Langevin, semi-group qt applied to the relative density of mu with respect to mu. Of mu with respect to mu, and then all of that is multiplied by mu. So the path of measure, rho t, will interpolate between rho 0, which is mu, and rho infinity, which is nu, and why is that? Just to double check. So at t equals 0, q0 is just the identity map. It doesn't do anything. And this we can, of course, see from the definition of a semigroup. At time 0, it's just the expected value of eta at y 0. Condition on y 0 equal to x, so it's just eta of x. And y0 equal to x, so it's just eta of x. So it's just the identity map. On the other hand, at q, at t equal to infinity, q t of eta is just the expected value of y t, the expected value of eta of y t. But now in the limit, time goes to infinity, yt will be distributed like nu. They're just going to be at time infinity, qt of eta, is just the expected value of eta with respect to nu, because we forget the initial condition. So because So because Qt at time 0, it's just the identity map. So when you apply it to d mu d nu, it just you get back d mu d nu, then you multiply by d nu, you just get the initial measure, mu, that's why rho 0 is mu. On the other hand, as time goes to infinity, this will just become a constant, qt applied to this relative density, so just a constant which will be 1. So therefore, in the limit, you will get mu. So by taking So by taking this Langevin semigroup, you get a path of measures which interpolates between your initial measure mu and your target measure mu. Okay, so hopefully this should be clear. If there's any questions. Okay, so this is just our path of measure, but so far we haven't defined any transport now. This is just the path of measure which interpolates between two measures. So how are we going to sorry? Maybe this is trivial, isn't it? Maybe this is trivial. Isn't there a half missing somewhere? I think it's just defined whether how you define the Laplacian like a half times Laplacian or the Laplacian. So we can have it with Laplacian, not a half times Laplacian. But yes, it's anyway just a normalization. Okay, so how are we going to get a transport map out of it? So it starts with a continuity equation. So this Langevin path satisfy the following continuity equation. Satisfy the following continuity equation for these measures rho, where the derivative in time of the measure is equal to, well, I guess minus if you move it to the other side, and then the divergence of some vector field, vt, times rho t. So this is, I mean, here we're not saying anything, but the point is that you can actually write out what this vector field is equal to. So vtx is just equal to minus the gradient of the log of the relative density of rho. Of the relative density of rho t with respect to mu. And another way of writing it, and this is crucial, you can write as minus the gradient of the log of the Langevin semigroup applied to the relative density between u and u. And this is just a consequence of the fact that the Langevin dynamics, so the Javin semigroup just satisfied. This is what we have in the right here, it's just a generator. The right here is just the generator of the Langevin semi-group here on the right. So, just by definition, it satisfies the equation. Then, if you do a little bit of algebra, you will see that with this particular choice of Vt, it will satisfy the path rho t will satisfy this continuity equation. So, actually, so far, you can even do it a bit more generically. So, maybe I'll move one more slide and then I'll explain what I mean. I'll move one more slide and then I'll explain what I mean by that. So, how do you now get a transport map out of it? So, given the vector field Vt that we have, you can just define a family of diffeomorphisms, which we'll just think of them as a transport map, as follows. You start at time zero, you just take a map, which is just the identity map, and then you evolve these maps. These are maps from R D to R D, and you evolve them according to the vector field coming from your continuity equation. So, actually, you can. Equation. So actually, you can even view this whole thing as there's something more generic going on. Whenever you have a path that measures rho t, so usually you can write some kind of continuity equation for that path. And if you take the vector field in that continuity equation, you automatically get a transportation map of this form. So, of course, you need your vector field to have certain properties, or else you won't be able to do much. Able to do much. So, what kind of transportation do we have here? Well, what you can show is that if you take this measure, if you take this map st and you evolve it, then at time t, it will just transport the initial measure mu into rho t, so rho 0 to rho t. And let's just define by capital T time T just the inverse of the map. So that will transport rho t back to rho 0, which in our case is mu. New. So this is basically where our definition comes from. So I write here L Vn for lange of n. You just take the limits as t goes to infinity. So the first one, st transported rho 0 to rho t, meaning it transported mu into rho t, but we already said that rho t in the limit is equal to nu, which means that S L V M, which is the limit of S T, transport mu to nu. And of course the inverse of it. Nu. And of course, the inverse of it will transport new to mu. So this is how you can generate transport maps from the Rangevin dynamics. Now, this idea, I think it goes back all the way to Molsaire, maybe even before that. Certainly in Otto and Bilani, you can find this construction. But as we will see, the people who brought it into the context of showing literature properties are Kim. Lich properties are Kim and Milma. But I don't really know what is the first place that constructed this kind of transport map based on the Langevin semi-group. But certainly in Auto-Villanya, you can find it. Okay, so any questions about just the construction of the map before I show some results? Is the limiting space? Go back to the previous slide while you're answering the question. Yeah. So no, and sometimes it won't exist. No, and sometimes it won't exist, and indeed you need some assumption on your method. Other questions? Okay. Well, if not, then I'll continue. I mean, but please do ask, because this is kind of a construction, so it's important to make sure that we're on the same page. Can you go back one more slide as well, sorry? Yes, of course. Yes, of course. Okay. Okay, so let's start maybe with a warm-up. By that I mean some results that maybe are to be expected already from the theory of optimal transport. So let's say we take our tortz measure to be gamma d. This is just the Gaussian. This is just the Gaussian, and then we'll take μ to be kappa log concave. And by loconcave, what we mean is, what Dan mentioned before, if you look at the minus of the Hessian of the log of the density of mu with respect to Lebesgue, it should be greater than kappa times ethanol. So if you look at the optimal transport map, we know that it's one over square root of k Lipschitz. This is basically Caforel's result. But the map here, this transported long-lane Javin semigroup, also satisfies exactly the same bound, and it's also sharp. This result is not new. This already follows from the result of Kim and Mealman, and I will mention a bit later more about the result. So this is not really new, but the reason why I want to mention it is because I want to show this result, which is actually new. So if you know a little bit about Caffarl contraction theorem, then you know that there is also kind of a then you know that there is also kind of a reverse direction. What does that mean? Suppose now that, so you know, you kind of have to keep in mind what is the source and what is the target. So in the first problem we went from the Gaussian into mu. In the second part we want to go from the Gaussian, I'm sorry, from mu into the Gaussian. So the question is when is a transport map from mu into the Gaussian is a contraction? Now this is like Casimovian. Now, this is like asking about the inverse of the previous question. This is exactly the inverse, which is why T and S are always inverses of one another. So, you know, for the context of functional inequalities, it's a little less interesting because usually we know a lot of good things for the Gaussian. So you want to go from the Gaussian to another measure. It's not so clear what are you going to do with a measure going into the Gaussian. But nonetheless, it captures the structure of the problem. So, what exactly is the result? So, if μ is So, if μ is low concave, so we need this assumption, though it's not because it's completely necessary, but so more importantly, you should focus on this beta log convex. So, what does it mean to be beta log convex? It's exactly the opposite of the above condition, where we upper bound by beta times identity. And maybe the example to think of is like a Gaussian with a different variance. That's the best example. So then the map is square root of beta-lipsch. Square root of beta-lips. And the point is that this completely parallels the analogous result for the optimal transport map. Because for optimal transport, this result is, I think it's maybe fair to say due to Kolesnikov, it basically follows from the same argument as Caffarelli. So this is basically Caffarelli, but maybe with a little bit more work. So already we see that in this setting, this is kind of the basic. And we, this is kind of the basic of our early result. You can completely get it exactly the same result. Well, maybe not with low-concave condition here, but for this transport map along the Langevin cycle. Okay. But we can do more, of course, or else this would not be too interesting. And let me state that. This is the type of a measure that Dan. So this is the type of a measure that Dan already talked about. So here again, our source measure is the Gaussian, and our target measure is kappa low concave with a bounded support. And the diameter of the support is upper bounded by r. And now we also have this assumption that kappa times r squared is smaller than 1. So the scaling, this is somehow the correct scaling, the fact that you have 1, just the way things scale, support and concavity, this is kind of the right threshold. I'm not claiming that this is. I'm not claiming that this is sharp somehow, but it does make sense that you will need an extra condition. But the result is that if you go now from a Gaussian into a measure which has bounded support and is also kappa locum k, then it is Lipschitz with what constant, so you get the constant r, and then you also get e to the 1 minus kappa r squared over 2. So maybe just to get a better sense of this result, let's take a simpler example. Result, let's take a simpler example. What if we just take a measure which is low-concave? So that would just mean that kappa is equal to zero. Then the question is: what do you get for a measure which is low-concave and has a diameter which is upper bounded by r? Then you just get that the map is basically r ellipsis. So of course there's some constant. And the order of the ellipsoid is the order when I say it's sharp, I mean the dependence on r is completely sharp. Whether the constant is sharp Constant is sharp, that's not clear. It's also not clear what should be the right constant. But so, you know, just to mention what was said before, so the question Kolesnikov asked that, is this true if you take a transport map of optimal transport, if you take the transport map from the Gaussian into mu, where mu is low concave and has a bounded support, is it true that the map is Lipschitz over? The map is Lipschitz of order O of R, this question is still open. But for this transportation map along the Langevin semi-group, we can in fact prove that and show that it is true. And moreover, we, as this above results show, we can actually capture this interplay between convexity and the support, because you can see that this is what's happening up there. So, of course, if kappa is positive, this exponential is gonna get smaller, as you expect. Smaller as you expect. If kappa is negative, which doesn't really give you much information, then of course e to the 1 minus kappa r squared is going to get large as exactly as you would expect. Okay. Almost less than one level. Yes, exactly. So that would be like a true. Yeah, so maybe, okay, maybe this is um. Yeah, so maybe I should say actually, this result is kind of formulated in a particular way. You can basically recover already the previous result from it. It kind of covers a bunch of cases, and we just found this condition to kind of cleanly separate between the cases. That's right. So if kappa is negative, then you're always smaller than 1. This is automatically satisfied. And that has to be that way, probably, because then you will incur the cost of this exponential. Because if kappa is negative, Because if kappa is negative, there's just no reason to think that you can get away without this exponential cost. But when kappa is positive, then the exponential can even help you, and you will get even smaller. But still, you know, taking kappa equals zero as a good case, you see that it did capture the bounded support. Now, yeah, whether the question whether this is the optimal dependence on kappa and r, that is, I that I don't know. That's perhaps too much to ask. That's perhaps too much to ask. But certainly when kappa is equal to zero, this is the optimal dependence on the size of the support. Okay, here's another example that was also mentioned by Dan. So if now we transport the Gaussian into a mixture of Gaussian, so mixture of Gaussian we mean the convolution of Gaussian with some measure M, or another way to think about it, you basically, you first sample. Basically, you first sample a point from M, and that's the new mean of your Gaussian. So now we make the assumption that the diameter of the support of M is upper bounded by R. So this does not mean, of course, that the measure itself has a bounded support. It does not. But just the means are contained within ball of radius r. Then the map is e to the r squared over two Lipschitz. And again, the order of this Lipschitz constant is sharp. And when I say sharp, I mean And when I say sharp, I mean, you know, there could be some exponent here, which is a little better, but up to there it's a sharp result. For example, this theorem leads to improved Loksovolev inequality for mixture of Gaussian solar capability. People knew how to... So if you have measures all of which would satisfy, let's say, Poincaré inequality, you know how to take a mixture of them, and you would like to say the mixture also satisfies a Poincaré inequality, and you would like the constant. Inequality, and you would like the constant of the mixture to be somewhat related to the constant of the Poincaré inequality for the measures containing the mixture. So, people knew how to do it for Poincaré inequality. For Luxabole, it was actually a harder problem, and people proved some recent result only, I think, a year ago or two years ago. But we can actually improve on their result for the case of a mixture of Gaussian. We can actually improve on their results. I I should say the other result in the literature have to do with general mixture. But for the case of mixture of Gaussian, we can do But for the anti-mixture of gaussian, we can get uh better results. Um okay, so really the first people to have to look at this uh transport map in the context of uh functional inequalities and prove uh Litsch properties are Kim and Emmanuel Milman. So what they did, they took a measure, so they wanted to generalize Gaffare's result. Gaffarello's result tells us that if we go from a Gaussian to a measure which is more local. A Gaussian to a measure which is more low-concave than the Gaussian, then the map is literate with constant one. They wanted to improve on that. So, what they said was that maybe the measure that you start with does not have to be a Gaussian. It should just be a measure which has enough symmetries, in particular a Gaussian. And then they show that if you go from that measure mu into a measure which is more low-concave than u, then that map will be one-mitch. So this is the result of Kinan-Mehrill map. Then there are some other results that result. Then there are some other results by Clartarg and Putnam. They show that if you go from a mixture of Gaussian into a low concave measure, then the transport map is one Lipschitz. This is not really comparable with our result. And then there is another result, very recent due to Joe Nieman. And this has to do with bounded perturbation. So we know that if a measure, so the Gaussian, of course, satisfies many functional inequalities, and then And of course, satisfy many functional inequalities, and then we know that if you do small perturbation of it, the small perturbation would also satisfy functional inequalities. So it is natural to ask whether a transport map can do that. So here's the result that if you start with a Gaussian and then you look at a measure, so it's of the form e to the minus u times the Gaussian, and your measure, and this potential u is upper and lower bounded by, this should be used farther, by a convex function. By a convex function, then the measure is e to the c ellipsid. So, this is probably not the optimal result, but I just wanted to show how using this transport map along the line of n semigroup, you can get a lot of new Lipschitz properties that are not known for the optimal transport. Okay, so the next thing that I want to do, I want to compare That I want to do, I want to compare this transport map to the one that Dan spoke about, the Brownian transport map. So, first, I should say the constructions are completely different. So, the Brownian transport map was a map that took the Wiener measure, living on the infinite-dimensional Wiener space, and mapped it into Rd to some finite measure mu. Here, we are taking the Gaussian measure on Rd and mapping it to another measure mu on Rd. On RD. And also, it's not clear that there's any connection. The only connection is somehow the techniques of the proof themselves, and that's what we realize, that we can use the techniques of the proof for completely different maps. But it's not clear whether there's any connection between the two maps. If there is, it would be very interesting to learn about. So, okay, first you might wonder: okay, in both cases, in both talks, we talked about target measure, which are Which are low-concave with bounded support, and we talked about mixture of Gaussian. Which result gives you the best result? Well, roughly you get the same thing with some small differences in constant. And sometimes the Brownian transport map does better, sometimes the transported Langevin semigroup does better, but the rough order is basically the same. Here's the big advantage that running transport map has that the Langevin transport map. map has that the Langevin transport map does not have and that is this property which is Lipschitz on average. So as Dan mentioned if you want to prove the KLS conjecture one way of going about it is a conjecture saying that one way of saying it that isotropic low concave measures satisfy a Poincaré inequality with a constant which is independent of dimension and of the measure. If you want to prove that, then one way of going about it is to show that you have a transport map which is Lipschitz on average, meaning in expectation. On average, meaning in expectation. So the Brownian transport map does indeed have that property, but it is not known for any finite-dimensional transport map that has this property, including for the transported long-long Javan semigroups. This is not known. So this is a big advantage of the Brownian transported map. On the other hand, because the transport map along on the Java Vincent semigroup is a finite dimensional map going from R D to R D, you can transport functional inequalities which Functional inequalities which depend on the dimension itself. So, for example, the Lux-Sobolev inequality or the Punker inequality, one reason why people are so interested in them, it's because they're dimension-free. This means that the constant appearing in front of the inequality does not depend on the dimension. And indeed, that was actually the motivation, for example, for Luxovolev inequality. It comes from quantum field theory. The whole point is that you want to take limit when the number of particles is infinite, and then you do not want the dimension to appear, or nothing else would make sense. That's the origin of the sub-limit inequality. The origin of the sub-life inequality. However, even though these inequalities are dimension-free in the sense that the constant do not depend on dimension, you can actually get better results which do depend on the dimension. Now, for something like that, the Barnes transport map is not very useful because it goes from a space which is infinite dimension. So it cannot transport anything which depends on the dimension into your target measure. If you want to do something like that, then you really do need a measure. That then you really do need a measure from Rd to Rd. Another thing that you cannot do with a barn transport map, and that is what I've shown before, we've seen this caffored thing where you can go from the Gaussian into a measure or from the measure back to the Gaussian. This inverse map does not exist for the Brownian transport map because you go obviously from a much bigger dimension into a smaller one. And really, the reason why we thought of it, we had a particular application in mind, which we really liked, and I will show you that one in a second. I will show you that one in a second. That's eigenvalue comparison. That has to do with comparing the eigenvalues of the operator corresponding to the Gaussian with the operator corresponding to the measure that you're interested in. Again, the first eigenvalue just has to do with the Poincaré inequality. So the first eigenvalue you can control using the Brownian transport map. But the higher order eigenvalues, you cannot do that because of issues about multiplicities and things like that. So for that, you really do need a finite dimensional map. But, however, like I said, there are similarities in the proof techniques in both for both maps. Okay, so let me just recall again the idea of how do you transport functional inequalities using Lipschitz map. So, say you have a Lipschitz map T from R D to R D, which is L Lipschitz, which takes a Gaussian into mu. So, then let's see how that mu satisfy a concurrent equality with a constant L squared. This is something that Dan showed before. So, let me just show it again. Showed before, so let me just show it again. It takes the variance of some point inequality, just mean that the variance of f, as any test function with respect to u, is upper bounded by some constant times the expected value of the gradient, of the norm of the gradient squared. So how do you do it? You start with the variance of the test function with respect to mu, and then you write it as a variance with respect to the Gaussian once you compose your test function with your transport map. Then you use the fact that Transport map, then you use the fact that the Gaussian satisfied Poincaré inequality with constant one. This is this inequality here. Okay, so now the test function, new test function is a composition of f with a transport map. Now you do the chain rule. So the derivative of a transport map comes out. But now because you assume that it's ellipsoid, you upper bound by L squared, you get the expected value with respect to the Gaussian of the gradient, and then you just change again the measure, and you get a Poincaré inequality. Again, the measure, and you get a Poincaré inequality for mu as constant L squared. This is how you transport inequalities. So, here is one application of this idea, and so this is due to Immanuel Milman, the idea, it's a very simple idea that you can use the transport map also to have comparison between eigenvalues. So, that's lambda i of mu and lambda i of gamma d. These are just going to be the eigenvalues of the Langevin semigroups operator associated. Semigroups operators associated to both measures, respectively. So, here's a corollary which follows from our result together with Emmanuel's comparison principle. Again, so the idea is that if you have a transport map, you can just transport the eigenvalues, so to speak. So, if you start with a Gaussian and you go into a measure mu, which is kappa local concave, which is bound on the diameter, and again we have this condition that kappa r squared is smaller than one, then you can lower. Is smaller than one, then you can lower bound value of mu by the eigenvalue of the Gaussian times the constant, which of course depends on your diameter and on kappa. And so again, if you just look at lambda equal to one, then this is nothing but concurrent equality. But if you look at lambda i for i larger than one, then this is something completely different. And this, of course, holds for all i. And this is exactly the type of result that we cannot do with a Brownian transport. We thought that we could not do with a Brownian transport map because of the issue of multiplicities. Basically, the eigenvalues for the Unstein-Ulingbeck operator on Wiener space have infinite multiplicities. This just does not work. So that was really kind of our motivation. But once we have that, we were actually able to use this transport map for many different things. I'm just showing here a few applications. And here is also the analog this similar type result for the case of Gaussian mixtures. And mixtures. Yes, maybe before I move to that parametric inequality, maybe I'll pause for questions. Sorry, can you move back to the eigenvalue comparison? Yes. So here the eigenvalues are all negative eigenvalues, right? And so they are countably discrete and they are all negative, right? The way you have defined them? They're generators, right? Yes, although I think. Yes, although I I think I think they will actually be positive in the negative of the generators. Yes. Okay, so then they're in the positive half-line. Yes. Yes, yes, I believe. I believe so. I might be mixing the signs as well. And this I, the index, is going in the increasing order. Increasing order, increasing order, which So yeah, you start with the with the top eigenvalue and um sorry sorry you start with the with the bottom eigenvalue which is zero and then you Which is zero, and then you well, um yes, the very top one is zero, then you have the point caré, and then you go up. Yeah, so that's how behind the. Yeah, that's how they are. That's right. That's right. Yes, the four. And there's issue with multiplicities and things like that, but the legislature of mu, this is dominating the eigenvalue. Yeah. Yeah, and this isn't, um, it's really, I mean, it's, it isn't. It's really, I mean, it is a surprising result, I think, but actually the proof, what Emmanuel did, it's really just a transport map proof. You just have to think a little bit how to do it, but it really does, exactly in the same way that you transport the Poincaré inequality, you transport this. Really what you use, you use the fact that you can write the eigenvalue as a variational problem. So the min-max, the min-max principle. So what you have on the top, the numerator is just a gradient of something, the denominator is the L2 norm. Is the L2 norm, but we saw exactly how to control this gradient. And the L2 norm is preserved because. And the other, exactly. So you just work it out, you have to do a little bit of work, but that's the basic. Yes? That's lots of fun questions. Yeah, okay. That's not fair. Let me show you an application which is maybe a little bit of a different nature, more coming from comic geometry, and then the next one will be even, I guess, more different. So let me remind you: what is a Gaussian isoparametric inequality? So it has to do with the measure of sets once you expand them a little bit by epsilon. So maybe, let's start maybe with something more basic. Let's start with the Euclidean isopermatic inequality. So the Euclidean isopermatic inequality tells us that of all shapes of equal volume, the ball minimizes surface area. This is the isopermetic inequality. The Gaussian isoparamic inequality. The Gaussian anti-parametric inequality, you're asking the same question, but now you're measuring things in terms of Gaussian volume. So you're asking of all sets of equal Gaussian volume, meaning Gaussian measure, which one will minimize the Gaussian surface area? Or Gaussian surface area has kind of the obvious definition. And in that case, the optimal solutions are half spaces. So this inequality is at least seemingly stronger, and it tells you that if you look at the And it tells you that if you look at the Gaussian measure of some set K, where you expand it a little bit by epsilon, so you're looking at the epsilon neighborhood of the set K, this Gaussian measure is always lower bounded by some expression, but this expression on the right is exactly the Gaussian measure of a hash space that has the same volume as your set K when you expanded this one by epsilon. So here B D is just a unit ball. Here Bd is just a unit ball, so K plus epsilon B D just the epsilon expansion, epsilon neighborhood of your set K. And phi is the C D F of just a regular Gaussian in R, and the phi inverse is just the inverse of it. But really, what it says on the right-hand side is that this is the measure, the Gaussian measure of the epsilon neighborhood of a half space that has the same measure as the original set K. So in that sense, half. Set k. So in that sense, half spaces are optimal. So it turned out that you can also transfer, you can transport this kind of inequalities as well. So here is one result. If mu is low concave, I'm just stating it here in simplicity, just for the case where kappa is equal to zero, so low concave, and the diameter of the support is upper bounded by r, then you have a lower bound on the expansion of the set in distance. In distance epsilon, and you get the same expression as for the Gaussian, but instead of getting epsilon here, you get the constant coming from the transport map. And that too is a consequence of the fact that the transport map is Lipschitz. So the idea is that you can also transport many functional inequalities, even though they might look different than something like Poincaré. It's in fact much more versatile. So let me go even a bit further. So, let me go even a bit further. This one, the result itself is not so interesting. It's more, I just want to show the power of transport maps that it can really take you far if you know how to use them. Well, this has to do with something called waste inequalities. So, this result by Gromov, in some sense, you can think of it as an extension of the isoparametric inequality, kind of. Let me say it exactly. So, you take L, which is your working dimension. Which is your work in dimension D, and L is some lower dimension, and you have a function which goes from Rd to RL, which is continuous. And the statement is that you can always find a point T in your image space such that when you look at the fiber of that point and you expand it by epsilon and you look at the Gaussian measure of it, it's always going to be lower bounded by the Gaussian measure on the image space R L of um a ball of uh radius epsilon in R L. Radius epsilon in RL. So this is a bit to take large. You have you need some time to think about a result like that. I should say that again, L equals one somehow corresponds to the hydroparametric inequality. In fact, in that case, here the result says that there exists a t. But in L equal 1, there are actually many, many, many T's will behave like that. But the reason why this result is hard is because as soon as L is greater than 1, it is no longer true that this will be true for many T's. Will be true for many T. You have to find one special T, and that's really the result of Gromo. Okay, so then what Clartard showed using localization techniques. Sorry, I mean presumably the interesting thing would be to look at the difference in divide by epsilon ticker limit, right? That's. You mean as epsilon goes to zero? Yeah. Yes, yes. We will get, well, I guess actually it's. It's almost here. So exactly. What you really want to look at is you really divide by epsilon to the L, and then you look at the limit. And then you look at the limit, so you subtract from it on the other side the measure of F inverse T, and then you divide by epsilon to the L, and you take epsilon to the L. Yes. So this is actually this result. This is roughly this quantity, what we're looking at here. So what people are interested in is lower bounds on this kind of things. And what the idea Clartex showed that using Gromov and localization, which is a technique in Karmic geometry. You know, a technique in combat geometry, which then led to stochastic localization, which was already mentioned here. So he found some kind of a lower bound. I mean, there's no need to understand it too much. All I wanted to say was that actually using transport method, which is also due to Clartic, we got a result, which is weaker, which is why, you know, it's not even included in the paper because I don't think it's worth it. But I think it's worth mentioning just to show the power of transport map. So really what here we took advantage of is of the fact that we have a transport map from the galaxy. From the Gaussian into low-concave measures on convex sets. So, in particular, we took the Gaussian and we just transported it into the uniform measure on convex body, which is an example, of course, of a low-concave measure with bounded support. But there were no maps that had good Lipschitz properties for this particular setting up until now. So that's why you can get something like that. But this result is really a bit weaker. The proof is, I think the proof is actually simple because the localization is. Is actually simpler because localization is not an easy result. It requires work. So, this is somehow a very easy proof, but it's weaker, so I don't think one should read too much into it. But the point is just to show that transport methods can really be applied in many, many different settings. And in particular, this is an example of why you want a transport map which goes from R D to R D rather than from the wiener measure into R D. I mean, maybe you could do something like that with the wiener measure, but You could do something like that with a wiener manager, but it will require more. Okay, so let me say a little bit about a high-level idea of the proofs that are involved. So let's recall again, how do we define our transport map? We define it through the differential equation where Vt was this vector field that came from the continuity equation for the Langevin path. That's how we defined it. That's how we defined it. So, if you want to control the Lipschitz properties of st, especially as you take t to infinity, well, you're just going to differentiate on both sides. If you differentiate on both sides, you just do the chain rule, and you get the gradient of v applied to st and then times the gradient of s. So, here is a very basic lemma, which basically follows immediately from this equation. So, of course, if you want to control the size of the gradient of st, you better control the size of the gradient of vt. Better control the size of the gradient of V. If you think about this lemma in one dimension, you can just solve it. This is just an easy solution, and you get something like that. So the statement is that the Lipschitz constant of both T and S, these are the maps that we got in the limit as T went to infinity and we took the inverses of one another. The statement in that these maps are basically controlled by the maximum and minimum eigenvalues of this gradient of Vt. And you have to take the supreme aim for all x in. The superborn aim for all x in your space. So you could sharpen, I mean, there's a sharpened root of this lemma that involves the time-ordered exponential of these operators, right? So the gradient of Vt at different times may not commute with each other. Yes. And so can you take some advantage of this sort of additional smoothing that goes on as time goes on? That's a good question. This is somehow, you know, this is a kind of a very rough bound. And the fact, actually, and we will come back to the issue. Actually, and we will come back to the issue of the commutation of the thing, but yes, I don't know the answer to that, and that's actually a good if there are like kind of clear formulas that tell you exactly how to do it, that we'll be interested in hearing. But you will have to control them somehow. There are clear formulas that tell you what the solution to the Zodi are. Yes, in terms of time-order experience. Yes, but then the question is: how do you control? Yeah, so that might be a bit complicated, but maybe it's worth looking into. Okay, so once we wrote that, let me actually mention something that I think is quite natural. You could say, well, maybe this is just optimal transport. I mean, in disguise. So actually, Kim and Millman, they left it as an open question, and it was solved, and I will mention that. But let me just show you the argument. So this goes back to the idea of the commutation. Goes back to the idea of the commutation. So again, we start with this equation. Now let's pretend that the matrices did commute, or the gradient of VTS did commute. Then this would be an actual solution of the equation. But now you notice something nice. If you look at this equation, you realize that gradient of s is always going to be positive, there should be a gradient of st. It's always going to be positive semi-definite. So, which means that st is a gradient of a convex function, and then the limit. And then the limit, you will get that it's something great of a convex function. Just by uniqueness, you know, there's only one, you would get that it's optimal transport. So basically, whenever these matrices commute, these two maps coincide, optimal transport and this map. So, for example, in dimension one, this map is exactly optimal transport, because in dimension one, of course, everything commutes. What happens in higher dimension? This was left open. This was left open by Kim and Milma, but then Tanana she answered it and she showed that in general these two maps are not the same. And the example is not even, you don't have to go that far. You just take the two measures to be two Gaussian measure with non-inth covariance matrix. If one of them, both of them should have non-inthance matrix. And of course this has to do with a commutation. Probably what you need is their covariance matrix as well as to you, right? Yes, that's right it's all about. Yes, that's what it's all about. But there's a bit of an argument, but it's basically about the non-commutativity, because if one of them was the identity, then they would commute, of course. You wouldn't get anything. Okay, so this, I just wanted to kind of make this clear, this connection between optimal transport. So let's go back to our setting. Like we said, if we want to control the Lipschitz constant of this map, what this lemma shows us is that we better control the gradient of V. Control the gradient of Vt. Okay. So we need upper bound and we need lower bound because we have a result both for the forward map and the inverse map. So that's why we need upper and lower bounds. So for the upper bound we need to control the maximum of this one. But the gradient of Vt, if you recall, Vt was nothing but the gradient of the log of the Langevin semigroup applied to the relative density. So really you're looking at Relative density. So, really, you're looking at the Hessian of the log. But in this particular case, you're even working with the Unstein-Uhlenbeck semigroup. So, the type of object that you're looking at are just the Hessian of the log of the Unstein-Ullenbeck semigroup applied to a function. And this thing we know how to show. And if you remember from Dan's talk, these are exactly the type of object that we controlled while showing the Lipschitz properties of the Brownian transport map. So, the idea is that, again, Again, like the proof, the construction is completely different, but in both cases, controlling the Hessian of the log of the heat flow or the Unsegolding Beck, which are of course related in a trivial way to each other, is kind of the key to showing Lipschitz properties. So, in particular, what we do is we want to control this by some function, let's say in the case where the diameter is upper-bounded by r. So, then So then you combine basically two estimates. So you use one estimate that upper bounds, the maximum eigenvalue, just because the support is bounded. While the other estimate, you use the fact that the measure is capital con k. And then you just combine the two. You take the best depending on the time where you're at. So when T is small, F, this one coming from the support. F, this one coming from support is bad, but G is good. When T is large, F is good, but G is bad. So you just take the best one at every point, and that's how you get the bound. Now, how do you actually prove this type of result? You just write the grain of Vt of X as a covariance matrix, which is again something that Dan showed, and then you use cross-complete inequality. So I'm not getting into the details too much. Let me just say the lower bound, how do you do that? How do you do that? Well, so for that, we have this lemma, which is probably known to people, but we have it found in the literature, so we proved it. And that has to do with if you have a measure which is beta semiloconvex, then you can lower bound the eigenvalues by the corresponding term if the measure mu was just a Gaussian with a covariance 1 over beta. So I told you that kind of the canonical example of beta semi-local convex, just a Gaussian with covariance. Just a Gaussian with covariance 1 over beta, but the result is that under the heat flow, under the Winter-Ulgenbeck flow, any measure will be lower bounded by the corresponding term with the Gaussian. So that's the general result. Okay, so I think this is it. So I'll stop here. Thank you. So let me ask if there are any questions. I can't really see who has their hands up at the moment, so out there and want to ask a question, please go ahead. So let me see. Are these maps connected with? I know you worked also on extremals in the Alexander Fenkel inequality. Extremals in the Alexander Fenkel and all the G use these maps in that context? Not really. Other questions, anyone? So the last one dynamics here come through the specific form of the semi-present so whenever if you can control how do functions and their derivatives evolve under the Langevin semi-group, this will work. Semigroup, this will work. It just, you know, for the heat semigroup or impeccable semigroup, because we have explicit formulas, like the Mahler formula that you can just write out, it's kind of easier to control. But it's a basic principle. And I mean, in fact, I should say, we have some work in progress in this direction. There's more to be done for sure. Any other questions, comments? But so I think if not we have about a ten minute break before the units.