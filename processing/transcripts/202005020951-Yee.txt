Me to speak at the conference. So, as Jack has already mentioned, I'm going to be talking about unconditional computations for fundamental units in number fields. And so, I guess just sort of as a quick overview of the organization of this talk, I'll mostly spend a little bit of time talking about the motivation and the goals of this project, followed by a quick A quick overview of the relevant previous work. And then, following that, will be a description of the algorithm and the two main pieces that go into making this algorithm, which are a method for expanding a lattice along a prime, and then the baby step, giant step method as it pertains to number fields. And then, following that, I'll talk a little bit about the work that remains to be done in this project, as well as Be done in this project as well as some of the future applications. I'm sure most of you in number theory are familiar that one of the major computational problems involved with number fields is the computation of sets of fundamental units. And the reason why fundamental units are important is because they play an important role in computing class groups, and they also can be used to help. Also, can be used to help, or rather, to solve certain kinds of Diophantian equations. And so, there's interest in developing fast algorithms for their computation just because they're useful for expanding evidence for certain kinds of conjectures in number fields, such as the Cohen-Martinet heuristics and things like that. So, in terms of actually computing In terms of actually computing fundamental units, the state of the art currently for number fields is the index calculus algorithm, which is due to Buchman in the 90s. And so this algorithm has a sub-exponential running time in the bit length of the field discriminant, and its output is going to be a basis for the corresponding logarithm lattice to the unit group. But it's worth noting that the correctness of this algorithm is going to be guaranteed only if you assume that the generalized Riemann hypothesis is true. And so it may be that for some applications, you can't necessarily make that assumption, and you need to verify that the output is actually correct. And so that would lead you to think about: are there any ways for us to take the output of the indexed capitalist algorithm? Take the output of the index calculus algorithm and make sure that it actually does give the unit group. And so there are two main ways that you can certify the output of that algorithm. Asymptotically, the fastest way to do this is the baby step, giant step method for number fields, which is also attributed to Bookman. And it has a complexity that is on the old On the order of the discriminant raised to the one-fourth, plus some small factor epsilon. Some issues with this method, though, is that it does have a very large storage requirement, and it also involves the enumeration of lattice points. And so what that has the effect of doing is that as the degree of the field increases or the unit rate increases, then this becomes more and more costly. The other method that is The other method that is more commonly seen, and it's implemented in algebra systems like magma and pre, is a method due to post. And if I computed this correctly, the complexity of that algorithm is the discriminant raise to the half times some multiple r, where r is the unit rank, divided by the logarithm of the discriminant raise to the half. And so Post method It deals with the units explicitly. And unfortunately, when you're doing computations with a very large discriminant, that means that the units themselves can get extremely large. And so this method may become quite unwieldy when the discriminant gets too big. And so I guess I've kind of laid out the framework for the problem. And given the way that I've set it up, one natural question might be: well, is it possible? might be well is it possible for us to combine the two methods in order to obtain a better complexity and is it possible for us to confine the computations strictly to the logarithms so that we can avoid dealing with size issues with the units so moving on to the work that's been done in that front this was actually proved to be possible in 2007 by dahan et al In 2007, by De Haan et al., and they showed that in the case of real quadratic fields, which is a unit rank one case, this is indeed possible. And so, specifically, if you're given a full rank sub-lattice of lambda prime, in this particular case, that happens to be a multiple of the regulator S. But I'm going to pretend that I'm going to treat it as the full rank sublattice and denote it by lambda prime. You can show that you. You can show that you can compute the actual logarithm lattice corresponding to the units in time s to the 1/3 times delta to the epsilon. And so in the case that you're taking an output of the index calculus algorithm, you would probably expect that lambda prime actually is the regulator itself. And so what that ends up giving you is if you plug plug. You expect the so the expected size of the regulator is around delta raised to the half. And so if you plug that in for the value s, that gives you this complexity here, which is delta to the 1 sixth, where delta is the discriminant. And that is indeed better than the previous best method for this type of field. And so building off of that, Fontaine and Jacobson generalized this algorithm to work for higher degree number fields. To work for higher-degree number fields and for larger unit rank, arbitrary unit rank, I should say. And so the current project aims to sort of improve essentially the space and time costs of the work that's sort of already been laid down by Fontaine and Jacobson, as well as provide a rigorous analysis of the runtime, as well as the precision needed to obtain a correct result. And lastly, we're hoping to implement. And lastly, we're hoping to implement this algorithm and provide numerical results as they pertain to larger unit ranks and different degrees. And so this is joint work with Mike Jacobson and Hot Tran, who are here today. So let's see. So I guess that's sort of all the background that I need. And so following that, I should talk about some of the notation that I'll use for the rest of this talk. Talk. And so throughout, k will denote the number field and n will be its degree. Delta sub k, I may sometimes accidentally just put delta, but that will be the absolute discriminant of the field k. Rk will denote the regulator, ok, the ring of integers. And then the lattice lambda will denote the logarithm lattice that corresponds to the units. And then I'll use lambda. The units, and then I'll use lambda prime to denote some sub-lattice that you may be given. For those of you who aren't familiar, the way you obtain the logarithm lattice corresponding to the units is you basically take elements from OK star, which is the unit group, and you map them into some vector in Rn, where each coordinate is essentially the image of that unit under one of the distinct non-conjugate embeddings. Non-conjugate embeddings, you'll take the absolute value of that and then take the log. And so that equation is just given right there. Oh, and also important, I'm going to use r to denote the unit rank. Okay, so before getting into any of the details of the algorithm, I'll just give a very superficial overview just so we know where we're going. You can think of the generalized algorithm. You can think of the generalized algorithm as a hybrid of two separate techniques. The first one is the method of POST, which basically seeks to expand the subgroup lambda prime by trying to find different prime numbers that divide the index of that subgroup inside of the larger group. I'm using the word group here, but you can think of this to mean lattices. And the second method, And the second method is the baby step giant step method, which will search a region inside of a fundamental parallel pipet. And the main goal of that is essentially to improve the regulator, or rather to improve the lower bound on the regulator. And what that does in effect is it allows you to not have to look at as many primes in the first step. And so these combine in a fairly nice way. In a fairly nice way, and the idea is that if you can balance the two runtimes between those two steps, then you can achieve a minimal or some sort of optimal balance between these two. Cool. So I guess I'll talk a little bit about the first method, which I've called lattice expansion along a prime. And so this is described in the work of Post, but keep in mind that his description is strictly working. Is strictly working with the units and not the logarithms. So in this case, you're given some full rank subgroup G, which is generated by these values epsilon 1 through epsilon r. And the idea is that you should consider every single prime that divides the index of G inside of the unit group. And POST essentially devised some way to test whether or not it's possible to enlarge a group G. possible to enlarge a group G to obtain some new G prime where the index of that new group is exactly smaller than the original index by a factor of P. And so the main crux of that method rests in this theorem here. So you basically, or rather the setup for this theorem is that you assume you have some set of generators. You assume you have some set of generators for some group G, as well as a bound on the number of primes that you need to look at. And so the idea is that if for every prime up to that bound, there is no solution to the equation that I've listed here, where these exponents m0 through mr range between 0 and p minus 1, then you can guarantee that epsilon 1 through epsilon r is actually a system of fundamental units. And so the precise way that this theorem gets applied is that if you can actually find a solution to this equation, then you've essentially identified an element that is inside of the unit group, but is not currently inside of the group that's generated by these epsilons. And so if you add that unit in, that'll have the effect of expanding your group and reducing the Group and reducing the index by a factor of P. Cool. So that covers the first method, which is not too bad. So I should talk about the second method, which is the baby step, giant step method. And the idea here is that you use this baby step, giant step search to look over a small region inside the fundamental parallel pipette. The fundamental parallel piped of your given lattice lambda prime. And what you're trying to find as you search is you're trying to find elements of lambda, the full logarithm lattice or the unit lattice, that are not already inside of lambda prime. And the way you deal with, if you search and you find no new points, then what that has the effect of doing is Is getting you a better lower bound on the regulator, and that will decrease the number of the amount of work that you have to do in step one. But if you do happen to find a new element inside of your search region, then you can basically just add it to lambda prime, and that will have the effect of enlarging the lattice. The key idea that is required in order to do Is required in order to do baby step, child step inside of a number field is this idea of an ideals infrastructure. And so here's just a slide sort of explaining the basic idea of that. But ideals in number fields have these special elements which we refer to as minima. And their definition essentially states that if you define this set that I've written here, then the only element that can be contained inside of it is zero. That can be contained inside of it is zero. And so, just sort of a visualization of how that works: these absolute value sub i's are defined as the image of an element under one of the embeddings, and then you take the absolute value. So what this condition has the effect of doing is it essentially defines some sort of convex volume inside of a real vector space. A real vector space, and the idea is that no element inside of the ideal should have an image that sits inside the volume. It should be empty except for zero. And so if you consider the set of all the minima, they actually possess some group-like properties, which are referred to as the infrastructure. And what baby step, giant step will do is you try to compute the set of all Is you try to compute the set of all minima that are contained inside some relatively small region compared to what you're actually trying to search. These are the so-called baby steps. And then you can basically leverage that computation in some way in order to more quickly cover a larger region. And this is called the giant step. So that's a little bit vague, but I've included some picture that will hopefully give some intuition about what's happening. This picture contains This picture contains a small set B as well as a larger set G, which is the area that we want to search. And so the idea is that you have to compute the set of all these minima that sit inside of B. And then now, in order to search G, these non-filled out circles in G, these points are all known. And the idea is that Points are all known. And the idea is that you use some sort of jumping algorithm to target these unfilled elements. And what you can do is you can find a minima that is reasonably close to each of these. And once you find one of those minima, you essentially compare it to everything that's sitting inside of B. And if you can find a comparison, If you can find a comparison that basically shares a particular property, so the property needs to be shared between one of the elements in B and your target minima, then you essentially get an explicit way to compute an element inside of lambda that you didn't already have. Okay, so Okay, so now that the two techniques are described, I should probably talk roughly about the complexity of the algorithm as a whole. And so assuming you've chosen your bound B, which determines the number of primes you need to search in the first method, as well as how large of a volume that you need to search with the second method, then you end up getting a runtime expression that looks like this. It's some exponent in. This. It's some exponent in the determinant of lambda prime times a small negligible power, I suppose, of the discriminant. And what happens is that if you assume the typical case, which will be that the lattice you were given actually is the unit lattice, you can expect that to be that the determinant of that lattice is on the order of delta k to the half. And so again, if you And so again, if you plug that into the expression up here, that will give you a slightly nicer expression strictly in the discriminant of k. And you'll notice that the exponent here, r over 4r plus 2, that will always be better than 1 over 4. And what that means is that this method should asymptotically always perform better than the baby step giant step on its own. So, and just for a more concrete illustration, I suppose, if you set the unit rank to be either one or two, then you can plug that in and you get delta to the one-sixth and delta the one-fifth respectively, which is a fair improvement over the baby step giant step. Okay. Okay, so in terms of what remains to be done, we do have preliminary implementations of the algorithm, but they still need to be compared with the existing methods that are contained inside computational systems such as MEGMA and Perry. A second thing that needs to be done is concrete precision analysis of the algorithm because It does involve computations with logs, and therefore we have to use real approximations. And so, it's important to make sure that we have enough precision in order to correctly compute the logarithm lattice. And so, one other thing that needs to be done is within the post method, there was an improvement that I believe is attributed to a rents. However, the translation into the logarithm setting isn't very Algorithm setting isn't very straightforward, but if we could implement that, then it would improve the overall complexity of the algorithm as a whole. And I guess for the last thing, this is an application for this algorithm in the future, but the hope is that this algorithm will speed up a particular method for computing class groups and fundamental units. Fundamental units in the case of complex cubic number fields. And the idea is that hopefully this algorithm will be able to be leveraged in an effort to increase the tabulation data on cubic number fields. Okay, so that's it. These are my references. Thanks for listening. Okay, thanks, Randy. Are there any questions? If not, let's thank Randy again. Okay, perfect. Thank you very much. So we now.