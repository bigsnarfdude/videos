Hi everyone, I'm Rodell. Today is my great pleasure to share my work on functional random effects inference with applications to accelerometry data. And thank you Rahu for just giving you a very nice introduction on the functional data analysis and for the accelerometers I don't want to talk about anymore. So what I'm going to do is I will start by talking about the data set. We have been discussing about this NH data set. Specifically, it's a very large data set. The data are publicly available on their website. Are available on their website. And this is a very large data set with over 5,000 individuals for every two years across four weeks, 2003 to 2006, which consist of two weeks, and 2011 to 2014, which also consists of two weeks. It's also a very rich data set with both the interview and the examination information. So specifically, here on the right, I'm showing you the example of three of participants from AHA 2011-2014. Each row represents one day of the week, each column represents One day of the week. Each column represents the data from one participant, and here I list the data from Sunday to Saturday. You will notice that I splitted the data for certain days of the participant because of the low quality, but that's the general data structure. So for each panel here, the data was collected in high resolution in the raw data, but released in the main level. So here we have 14-40 observations from midnight to midnight. For each individual, For each individual MIDI study, we have seven consecutive days, and the data was released in MINI-Level Means Unit. So, there are many ways to model this data, but one of them is to think of this as a multi-level functional data. And I should say that the device is selected in this way, so it's a restable accelerometer. And yeah. Now, let's jump into the specific data structure. So, here I will model it as Here, I will model it as a multi-level functional data because, in addition to the standard functional data analysis, here we have the multi-level structure. We have more than 10,000 individuals, and for each of the individuals, we have the data from seven days. And for each day of each subject, the data was collected in the mini level from midnight to midnight. And I should say, if you want to try to explore your approach on this data set, this is available on the book website. Available on the book website, and the book was just advertised by Jeff Listing. Please check out our website, which is also very nice. Specifically, here for each day of each subject, we have the data from the minute level. So a natural way to model this is to use the Yijs as a notation, where I indicates the subject, J indicates the day of the week, and S denotes a time. So here you will notice this is a function over the time of the day, and this is why we call it multi-level functional data. So the specific Data. So the specific question we are interested in in this topic is: can we actually predict the subject and subject today's specific trajectories? Before that, let me first introduce some idea of how to first approach this problem. We have been working on it since several years ago. So the first thing that we were trying to address is to do the fixed effect inference. Essentially, here we are regressing our observed field activity data on a bunch of Field activity data on a bunch of different outcomes, sorry, coverage. And the model shown over here is called the functional mixed model, which extends the mixed model to the functional setting. There are many different ways to fit this functional mixed model. So the approach that we come up with and proposed, and we named it fast-fit-merit inference, is essentially you can fit massively various pointwise mixed models along each location of the functional domain. So in this example, we have 1414 locations. You can start by just fitting those pointwise. You can start by just fitting those pointwise models. And the intuition is: although in functional scatter regression model, we assume the correlation over time of the residual. So, here the epsilon Igs is assumed to be correlated over time, at each location, it still follows a normal distribution. So, that's the intuition. And then, after fitting those massive devices models, we will smooth the estimated coefficient along the functional domain, and then we will obtain the confidence band. So, compared with the basis expansion approach, we showed in our simulation that this marginal approach works very well, but can be much faster in different simulation settings. What I want to share, you know, the paper is already published, but we developed a software called the FastFM. Specifically, we have a function FUI to implement our approach, which is essentially using the same syntax as that in the LMUR. All to do is to instead All you do is to, instead of having a scatter on the outcome, you put a matrix here and you just run the whole process smoothly and give you the estimates. You also have a nice function to realize your coefficient, so this can be not. However, some limitations of FUI is, first, it fits point-wise mixed-effects models, which is great, but for the dense data, especially for the ultra-dense data, that means you probably want to fit thousands of the models, which can still be very slow. At the same time, Be very slow. At the same time, for the sparse data, we may have the length of observations. And for the point-to-ex models, especially for the non-Gaussian outcome, we might be able to see some non-convergence issues. And in addition, in the original work, we focused on the functional fixed effects, that is, you know, for the population level, how would age affect feed activity? However, for each subject, at each widget, can we actually leverage random effects for the personalized predictions? Predictions, which motivates our work. So, in this project, we proposed an algorithm to do the prediction inference for each subject at each with it in this structure. And it consists of three important components, including the binning, the fast MMPCA, and the Bayesian hierarchical model. So, I will just go ahead and introduce each of those components. The first is about the binning. So, the idea of binning was not new and it was discussed. Not new, and it was discussed in some paper on the generalized MPCA, generalized.mpca. So essentially, here it just offers a solution to actually organize the data in a more principled way to have more observations within each location. We will just specify a certain number of centers along the functional domain instead of a model at each location, we fit the model using all observations within the being of each with Within each beam. And the number of beings is finite and can be much smaller than the dimension of the functional domain, which means the computation will be much faster. Another advantage is if we fix the local GLM, especially for the download chain data, that's the only way that I'm aware that we can estimate this subject with specific random effects. And from this meaning, we will obtain estimated fixed linear predictors for each with it, as each subject, as each location. With it, as each subject, as each location. So, the thing is, what to do next? Here, we just follow the natural idea of now you have the fixed effect, and you know your linear predictor consists both the fixed and the random part. So, let's just get some adjusted random component, which is shown here. And after that, since it's all about the random, we will apply a false MFPCA to decompose a variation at multiple levels. So, for those of you who are not familiar with the concept of MFPCA, just imagine. Of MIPCA, just imagine PCA is a powerful approach for dimensional reduction in the multivariate data. In the functional data setting, we call it FPCA. And here I have the multi-level functional structure, so I just name it MFPCA. Essentially, it will decompose the variation into the between subject and retin subject component. And here, I also want to highlight that we have a software to run this multi-level functional PCA. It's currently a function mapca.face in the package before. Face in the package a font, and it's a very powerful function. I highly encourage you to try that, and it's very fast. Okay, so the last step in our pipeline is to fit a Bayesian hierarchical model. Here, the idea is we want to construct a credible band for random effects. And the existing solutions in the MMTCA, including the block by the linear unbiased prediction in the original paper, as well as the mixed model equation in our review paper, both just generate the prediction. Both just generate a prediction result. So, you know, one nature solution is you can extract those eigenfunctions and do some sampling on those scores and use those scores to generate a credible bank. So that's a general idea and we will just show you some simulation results. So here what we are doing is we are plotting the mean, we are plotting the median coverage probability across all widgets. Across all widgets for each subject. So, here, what you are showing is actually the coverage probability, sorry, what you are seeing is the coverage probability of our prediction interval for all those random effects. So it's subject weight-specific. And this is different from the coverage probability of the functional fixed effect. And here we take the average across all the subjects. So what we can say is for our pipeline, it works well when we have a large number of observations. Well, when we have a large number of observations, for example, if we have 1,000 individuals, the average coverage probability of our confidence spans is close to the nominal level. You will notice some decrease of the coverage probability when we have small sample, which is actually because of the potential item probability issue between the functional fixed effects and the eigenfunctions that we introduced in the functional random effects. The functional random effect. So we talk about this issue and we name it the linkage of the random effect into the fixed effect. And this suggests, you know, this approach can work well for the functional random effects inference, and especially when we have a large number of observations. Now let me show you what it will give us. Essentially, what we have here is each panel represents a predictive trajectory for each subject in enhance at each specific date of the week. Specific date of the weight. So, this is different from the MFPCA in the way that you not only have the prediction result, but also have the credible math for each of them. So, for each day of each subject, you can actually do some hypothesis testing and draw inferential conclusions. And I should also notice that here we start with a multi-level functional structure because that's the data set we have, but this approach can generally apply to the single-level structure, which is a special case. Special case. And as we can see, our approach is able to identify those specific time periods of the day where we see a significantly increase or decrease of your physical activity trajectory for each subject at each day of the week. So that's the resolution that is personalized prediction. In addition, one question you may of interest is why do I care about this problem? So here we show another example where in the end hands the data was not complete. Data was not complete. Like, you still have some missing observations at certain locations of your functional domain. So, what we find is the pattern of the missingness generally follows this interval sensoring in a way that you miss the whole information for one hour or two hour, which makes perfect sense just thinking of how your wheeling works. So, in our approach, we actually can accommodate this situation and give you the prediction intervals for those picking ranges. Here, for each panel, those boom. Here, for each panel, those blue areas are those time parts where the participant didn't wear the device. And as we can see, we are still able to generate the prediction interval during this time period with the confidence band. And the good thing here is the confidence band doesn't expand or shrink dramatically for those missing regions. And the reason is it still borders the information across days for that subject at different and also across the subjects. And also across the subjects. So, this is pretty stable. And that's pretty much what I want to show. Just want to share, you know, these are several papers. The first two are K components in our framework. And the second paper is also our archive. Please feel free to check it out. So that's it. Thank you. Yes. Great talk. I have a question about Oh, great talk. I have a question about the last part with the sort of missingness.