The solutions to these problems using dimension reduction. I'm going to motivate this using some measured transport approaches to solve these Bayesian increase problems. But a lot of the things that I'm going to discuss today can also be applied with other inference algorithms. And so before I continue, so this is joint work with Yusuf and our collaborator at RIA Olivia. Okay, so first for some high-level motivation. So, first, for some high-level motivation, so we're interested in characterizing the posterior distribution of some parameters x in some statistical model given some observations y. And we're particularly interested in doing this when both the parameters and the observations are very high-dimensional. And this appears in many inverse pumps that we've already seen today, but in particular in applications in geophysics, in imaging, in material science, and many other domains. Any other domains. Now, the challenge is that, of course, for these high-dimensional spaces, as Lucas was talking about, sampling methods are often very challenging. And so the one approach that we've been focusing on to characterize these distributions is using measured transport techniques. So the idea here is that we have some target distribution which corresponds to the posterior distribution condition. Condition in the description for the parameter condition on some realization of the data that we've collected. For the talk today, we'll denote that by y star. And in transport, what we look for is a map T, a transport map, that pushes forward some canonical reference distribution, something like a standard Gaussian, to this target distribution. If we have such a map, in particular, if this map is an invertible transform map, then we can use it to cheat. Then we can use it to cheaply generate many independent samples, an infinite number of them if we wanted to, from the target distribution by simply applying this map to samples drawn from our reference. And we can also evaluate explicitly the posterior density using the change of variables formula as a function of both the map and the reference density. Now, for this, one approach to really find these maps for this setting. Find these maps for this setting of when we're interested in conditional distributions, these posteriors, is to look for a map between slightly larger spaces. So we're going to consider a map that takes a reference to the joint density of parameters and data. And in particular, we're going to look for a map that has a block triangular structure. What I mean by that is this is a map that is a function whose inputs are the dimension of. Whose inputs are the dimension of x and y to something of the same dimension. But that can be partitioned into two blocks, like we see here, where the first block only depends on something of the dimension of the data, and the second block depends on something of both the dimension of x and y. The reason for doing this is that this first block of the map will characterize a part of the joint density, in particular the marginal density of the data, and then the second block will characterize the remaining. Block will characterize the remaining part when you disintegrate this measure, this joint measure, which corresponds to the conditional we're interested in for the parameter given the data. More specifically, what this map does is that for any value of the input y, it pushes forward part of the reference to the conditional of x given that realization of y. And so this is something, a tool that's really been used throughout this. Used throughout the statistics and machine learning community for something that's often referred to as amortized inference, where we want to sample any posterior for any realization of the data. And the recipe for doing this is basically to collect joint samples of x and y, which we can do by sampling from the prior, and then sampling from the likelihood model given every prior sample. Using these joint samples to estimate this map, and then once we have this map, we can then use it to. And then, once we have this map, we can then use it to sample from our posterior density. So, just to see how this works, for instance, if we're interested in some parameters in some HODE model, let's say, we can collect joint samples, we can learn these transformations, we can then push forward our samples and look at these posterior samples to give them some realization of the data. We can also use these samples to then look at the predictive distribution for the states of this OEE and see that they for. And see that they, for instance, here on the top from this transport approach match classical sampling methods. Now, one nice feature, of course, is that we did this just based on these joint samples, so we never had to really evaluate the likelihood or the prior density for both y and x. And so this makes this method also amenable to problems where either of these are intractable or simply unavailable, which appears in likelihood of free inference. Which appears in likely a free inference and other things like this. But what I want to talk about today is really trying to take this a step further and trying to use this approach to solve problems where we have very high dimensional parameters and observations. And to motivate that, I want to consider a problem of estimating turbulent flow. So this was done in collaboration with some folks at UCLA. And here the problem is that we have an aircraft wing, an air force. An aircraft wing, an airfoil that's represented with this flat plate. It's moving downstream in this domain, and it's subject to some wind disturbances or some gusts, and as a result, there's all this vortex shedding that appears around the leading and trailing edge of this airflow. And what we'd like to do is to, as this aircraft is flying, we'd like to estimate these flow structures so that we can develop. So, that we can develop, for instance, control systems that can maintain this aircraft stable while it's flying. Now, the challenge here is that we have to do this given just some limited sensors or observations that we have, which correspond to some pressure sensors on the surface of the airfoil. And maybe most challenging aspect is that we have to do this online in real time as this aircraft is flying. As its aircraft is flying. So, in practice, we might have, for instance, less than 100 milliseconds to do the inference of this turbulent flow. And because this flow is very complex, what we do to kind of solve this is to use a particular vortex-based model that is used to represent this flow. But in order to really represent complex. Two, in order to really represent complex flow structures, we have to use a lot of vortices. And so this leads to problems where the state or the parameter that we're trying to infer is on the order of about 180 dimensions. We have 50 observations, so that leads to, if we wanted to use these inference techniques that we just described, we'd have to estimate maps that are about 230 dimensional, 230 dimensional inputs and outputs. And we have to do this within 100 milliseconds. Within 100 milliseconds, which means we can only afford to evaluate the forward model for this problem, maybe about 50 to 100 times. So to tackle this problem, what we're going to do is the approach that we've taken is to, instead of trying to build these maps as functions of these two hundred and thirty inputs, we wanted to reduce the number of inputs, the the the dimension of these parameters and these observations. Of these parameters and these observations. And what I really want to get across is that in order to do this, we take advantage of some structure that this inference problem and many other inference problems actually have, which is that even though we have all these parameters or states that we're trying to infer, many of the parameters actually we can't really learn a lot about them from the observations that we have, partly because there's a lot of noise in the observations. In the observations, the forward operator might be very smoothing, and so we can't really recover very high frequency components of the parameter space. And this leads to this phenomenon that Yusuf described yesterday, where typically if we look at the posterior density for the parameters, it actually only changes relative to the prior in a very limited number of directions that correspond to the directions of the parameter space that we can really learn. Parameter space that we can really learn something about from our observations. Now, the second aspect that we're going to leverage here is that the observations, even though we have potentially very high-dimensional observations, a lot of them might actually be very redundant. In the case of this airfoil, we have all these sensors that are next to each other, and we can imagine that sensors that are very close to each other actually contain very similar signals. And so we can probably compress. Signals. And so we can probably compress the observations without affecting our posterior inference too much. So to kind of formalize how we then do this dimension reduction, what we considered is to look for decompositions of both the parameters and the observations into two orthogonal subspaces for both of these variables. First, for the parameters, we decompose it into a subspace that's A subspace that's spanned by the columns of this unit, the first R columns of this unitary matrix U. And this is a subspace that contains the part of the parameter which we consider to be informed by the observations and whose coordinates are described by XR. And then a remainder part X per. And similarly, for the observation space, we decompose it into two parts. One, spanned by the first S columns of V, and that's First, S columns of V, and that contains the part of the observations that are informative of the parameter, and then the remainder part, y per. And what we do is we consider a class of posterior approximations, pi hat, which can be written as the product of two terms. One is a density over this reduced part of the parameter xr, so the informed part of the parameter space, conditioned only on the relevant part. Conditioned only on the relevant part of the observations, YS, instead of the full observations, Y, and a remainder of the, then the remainder part of the parameters, X per, which in this case is we consider to not be informed by the observations, and so it reverts back to the prior for that part of the parameter space. By Bayes Real, this can also be written as the product of the prior times actually a likelihood, which is only a function of this reduced data like. Of this reduced data ys and reduced parameter XR. And our goal is: can we find such a decomposition, in particular with very low-dimensional subspaces for the informed part of the parameter and informative data, relative to the ambient dimensions, such that there exists a posterior density within this class that's close to the true posterior with respect to some measure. In this case, we look at. With respect to some measure. In this case, we look at the KL divergence between these two in expectation over data, and we want this to be small. So we want this to be less than some small error tolerance epsilon. If we can do this, then the way that we would then approximately sample from our posterior distribution is to, using this transport approach that we discussed earlier, is to construct maps that are only functions of these reduced variables. are only functions of these reduced variables XR and YS. Use it to sample this reduced posterior, what we call the reduced posterior, the density for XR given YS. These are maps that are functions of very lower dimensional inputs. And then we would sample from the conditional prior for X perp and then assemble these two parts to get a sample from the product's contents. So just to quickly go over how we identified these decompositions, what we consider is the error between the posterior Consider is the error between the posterior and the closest posterior density within the class that we introduced. What we show is that this error can actually be related to some information theoretic quantities. These are what's something known as conditional mutual information that measures the independence of two random variables conditioned on some other random variable. And what we can show is that this error in the posterior is upper bounded by two conditional mutual information terms. Conditional mutual information terms. One that intuitively tells us how much information there is in the data about the part of the parameter that we don't update, expert. And this is only a function of this decomposition of the parameter space. And then another term that tells us how much information is still contained in the part of the data that we neglect whippurp about the parameter. So, what we'd like to do is to, in order to control this left-hand side, this posterior approximation error, we're going to try to minimize this upper bound over these decompositions in order to identify something that will best minimize the error in the posterior approximation. Now, to do this, in general, conditional mutual information, even though we can We can write these upper bounds down. They're difficult to actually numerically work with in practice outside of the case where we're working with Gaussian distribution and that they have closed forms. But what we can rely on is some earlier work that showed that if the joint density of x and y satisfies certain conditions, in particular certain functional inequalities, then we can upper bound these conditional mutual information terms. Conditional mutual information terms by terms that depend only on mixed partial derivatives of the log likelihood function. This is something that we can typically evaluate in practice. But most importantly, it's terms that are actually quadratic in these matrices that define the decomposition that we're interested in looking for. And so, because these are quadratic functions, we can explicitly minimize these. Minimize these upper bounds and get a closed-form solution to identify these decompositions of the parameters and the observations. To see how that looks like for one setting where the observations are a nonlinear function of the parameter with some additive Gaussian noise, minimizing those upper bounds can be done by computing two matrices. So for the parameter space, it corresponds to assembling a matrix. A matrix HX given here, which we call a diagnostic matrix for the parameter, which is based on taking Jacobians of the forward model, these inner products of the Jacobian, integrating them over the prior, which we can typically do by sampling from the prior and estimating this matrix, and then taking the leading eigen directions of this diagnostic matrix matrix. For this case of Gaussian likelihood models, it actually reverts to some earlier work. Reverts to some earlier work in 2020. And now, what we can actually do with this formalism is also consider the observation, the reduction in the observation space, which can be found from the eigendirections of a similar diagnostic matrix, in this case given by outer products of the forward model integrated over the fire. Furthermore, the eigenvalues of these two diagnostics define a nice upper bound for A nice upper bound for the posterior approximation error. And so, if we want to control this error, make it less than some tolerance, we can choose these reduced dimensions R and S such that these trailing, the sum of these trailing eigenvalues is small and below that tolerance. One thing I wanted to highlight is that when the forward model is actually linear, these diagnostic matrices are just, we don't have to integrate over the prior because the Jacobi. Prior because the Jacobians are constant. And the eigendirections of these diagnostics actually match something that's a form of dimension reduction that's popular and well studied, which is the solution of a phenomenon of canonical correlation analysis, which is based on just looking at covariances or correlations between the parameter and the observation and solving some eigenvalue problems as well. Value problems as well. But what's kind of known is that CCA is really just based on these correlations between the random variables. And so this is not really a very good form of dimension reduction when we're dealing with very non-linear forward models. And this gradient-based method to identify the relevant parts of the two variables tends to work a little bit better, as we'll see in that setting. So to see what So, to see what these eigendirections look like, what we first looked at is a conditioned diffusion problem. Here we have this problem motivated from molecular dynamics, where we have a particle that's moving around in space and is moving according to this SDE with this nonlinear drift and some Brownian motion term. The drift is chosen such that the particle tends at time one to either end up in one End up in one of two wells with high probability, either a well at plus one or minus one. And when we compute the eigenvectors for the observation space given here on the right, what we see is that I should first actually mention that the observations for this model are the position of the particle as it's moving in time. And the inverse problem is to infer the driving force on the particle to that light given a Lighted given a particular realization of its path. And if we consider this inverse problem, look at the observations and compute this diagnostic matrix and its eigenvectors, what the relevant part of the observations correspond to are trying to extract the position of the particle at the final time. And this intuitively makes sense because for this model, knowing whether the particle is at plus one or minus one. Knowing whether the particle is at plus one or minus one tells us a lot about whether the biasing, the forcing, tended to be more positive or negative. How frequently do you make the observation? Yeah, so here the observations, there's 100 observations between 0 and 1, so we just discretize uniformly. But you can consider the inverse problem with fewer observations if you'd like. Just these eigenvectors will become a little less smooth, but they'll have the same behavior. In comparison, one thing I want to show is that methods like PCA or these other forms of dimension reduction just try to extract the overall paths of the observations and just try to minimize the variance in. And just try to minimize the variance in the reconstruction of the observations. They're not really tailored to solving the inference problem. We can also look at this more quantitatively, looking at the posterior approximation errors in terms of these mutual information terms and show that for any value of the reduced dimensions, these methods based on this gradient-based approach, which we labeled by CMI, outperforms both PCA, CCA methods. But just in the last few minutes, I want to go back to But just in the last few minutes, I want to go back to this problem of turbulent flows. And one thing, and this problem can really, as we mentioned, is a problem that we have to solve online as we're collecting more and more observations. And so this is a problem that can be formalized using not just as a static inverse problem, but as a sequential inference problem, where we're trying to estimate the state of the system as the state is evolving. As the state is evolving in time and according to some dynamics. Here, the dynamics for these vortices are given by the Nielsavar law, and the observations that we collect are related, are these pressure observations, which are related to the vortices by an elliptic equation with some additive Gaussian noise. And so, our goal here is to characterize what's known as the filtering distribution, the state of the system at any time given the. System at any time given the observations up until that point in time. This is typically done with a recursive approach where essentially at every point in time we have to solve an inverse problem given the new observation that we have. Now, thankfully, if we look at these diagnostic matrices for this problem, we tend, if we look at the eigen spectrums of these matrices, their eigenvalues, at many different Their eigenvalues at many different points in time, what we observe is that the eigenvalues actually decay very rapidly, and so this gives us good confidence that this form of dimension eruption actually is very suitable for this turbulent flow problem. And what we observe in practice is that if we look at how well we can estimate the underlying flow for a prototypical version of this problem, where we measure Where we measure the error based on this root mean squared error metric as a function of the number of samples that we have to estimate these transports that are being used to sample from the posterior density. As we reduce the number of samples that we have going from right to left on this plot, the approach based on reducing the dimension, which corresponds to these yellow, red, green curves, is actually very simple. Green curves is actually very stable as a function of the number of samples that we have. And in comparison, these methods that actually operate on the full-dimensional space, so this algorithm doesn't do any form of dimension reduction, actually becomes quite unstable as you reduce the number of samples as we would expect. We can also look at this qualitatively for this, for recovering the pressure distribution on the airfoil. And here we used some, we saw. We solved the inverse problem given some true data from a computational fluid dynamics model. So, a model that had a lot of viscosity in it, even though we're using a model to do the inference that doesn't account for this. And we looked at how well we can recover this true pressure distribution on the airfoil, this blue line. In green, we have what we would get from just sampling from the prior at one particular point in time. In red, we have In red, we have this predictive distribution given by this method that operates on a full-dimensional space without dimension reduction. And what we can see is that here at the leading edge, there's quite a lot of variance represented with these error bars in estimating the pressure, as well as some error relative to the blue line. And then using this algorithm that does the dimension production on the parameter space, we can reduce this bias relative to that blue line. Relative to that blue line, and reduce the spread in the posterior predictive distribution for the pressure. So, just to conclude, so today we looked at performing dimension reduction for both parameters and observations in inverse problems in this Bayesian setting. And we did this by detecting informative part of the parameters and informative part of the data based on. Part of the data based on gradients of the forward model. We provided error guarantees on the posterior approximation and used those to determine what are the dimensions of these subspaces. And we showed out the application of this to turbulent flow estimation. In some future directions, we're looking at ways of identifying these subspaces without, in a setting where we don't have gradients, which is often the case in some models, as Lucas was describing earlier. This was describing earlier, and as well as other sources of structure that can be useful to accelerate the solutions of these methods, in particular, combinations of conditional dependence and these low-rank properties that we talked about today. And with that, I'd like to put up some references, and I'd like to thank you all for your attention and having to take any questions. I think a slight foil of the thing is that you have this curves here, but here the upper bound is in the middle of the left. Can you explain the one? Yeah, yeah, that's a good question. So the upper bound, here we're actually only plotting the sum of the trailing eigenvalues for the upper bound. It also has constants. Values for the upper bound. It also a constant in the upper bound, which depends on the joint density. And that's what accounts for this gap. Okay, and the curves on top are... The curves are actually the true value for these conditional mutual information terms, which are the The true bounds for the error and expected KL divergence. Here we're just trying to show that there are that in particular these CMI-based method kind of closely tracks this the CMI-based method, the true error for this method closely tracks the upper bound given by these trail and eigenvalues. Yeah, you decompose, I mean, both the parameter space and the uh the data space. So if you know the model uh in advance, you know how you know, like just let's assume you parameterize everything in freedom and you're looking at you know low frequency, high frequency, right? So there's a coupling between you know the low frequency of the input of the X. Input of the X to the low frequency of the Y, and also, you know, the high frequency of X concerns to low frequency X, etc., etc. Like, if you know the model in advance, is there a way to simplify this process of doing this, like using grading instead? Yeah, so if I see if I understood the question, please read correct me if I did. Correct me if I didn't. So, if you have some previous knowledge of the model, so that you know that the solution tends to live in the span of some particular basis vectors or some as a linear combination of some functions already. So, essentially, you can think of this like first part of the parameter space. And similarly, you know something about the observations. About the observations. And so you could, if you know that, so there's maybe two points here. One is the bounds that we construct, if you were to use that particular decomposition of the parameter in the observation space, you can still assess how well an approximate posterior performs given that particular. Performs given that particular decomposition. So you can plug in this like UR and VS into these bounds and get an idea of how well the approximate posterior to this, the one defined here for that particular decomposition, how close that is to the true posterior. And furthermore, then you could compare it to maybe the decomposition that really minimizes the right-hand side of that upper bound. The upper bound. The one thing I would stress is, and that might also be the setting that you mentioned, but one thing we're interested in here is not just kind of identifying a decomposition of the parameter space that's good for, let's say, reconstructing the dynamics. It's really the part that we're trying to find this decomposition that is of interest for solving the inverse problem. The inverse problem. So these two kind of decompositions are sort of like coupled, and we're trying to find them in this coupled way. I hope that answered your point. Okay, thank you. Very nice. A quick question. So since you use gradient-based method, are you worrying about going to some local minimum? So we're not so like we're not doing like a gradient descent to kind of To kind of minimize some functional of like the forward model, we're just using evaluations of these gradients of the forward model to define this diagnostic matrix that we looked at, to construct this matrix that tells us about the decomposition of the primary space. Like SVD, you always will find the best approximation. Will this go into the battle? Approximation. Will this try to define the path decalization? For the up for the upper bounds that we have for the posterior approximation. Yes. Alright, maybe we can leave the discussion for the coffee drink. Yes, thanks Carlo Elene.