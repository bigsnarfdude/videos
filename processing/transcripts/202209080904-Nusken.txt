I've got 20 minutes, right? Something like this. It's 25 pressures. Okay, okay. Yeah, so this is joint work with Michele and Torsen and also the Bastian Reich from the University of Potsdam. So the general setting is stochastic filtering, which many of you might know. So there is a hidden signal over here, X, which we can't observe, and then there is an observed process Y. There is an observed process y, and the idea of this is to draw conclusions about the hidden process x given information from y. And maybe one slightly unusual feature of this is that the noises here in the signal and the observation are correlated. This is somewhat important for this project. Okay. Yeah, so more precisely, given an observation. More precisely, given an observation path y. So, the idea is to find a posterior distribution, or even more precisely, we are after this conditional measure here denoted by pi. Okay, this is a Bayesian way of doing statistics. So, the output of any such procedure or algorithm, in fact, is going to be a probability distribution. And this means that, for instance, we can That, for instance, we can compute an estimate for x at a given time using the mean, for instance, of pi. And also the spread of the variance of pi will tell us something about the uncertainty associated to this procedure. Okay. Now, the next question is: how can we solve this bittering problem? This filtering problem. And it is a classical result that this filtering measure pi shows an SPDE and the Kushnas-Kartonovich SPDE. So this is a well-known fact. And this SPDE consists of a part which accounts for the dynamics of X. Essentially, this is a part which is due to the Fokker-Planck equation associated to the hidden signal. To the hidden signal. And then there is this part, which is driven by the data. Why? Ah yeah, please do interrupt me at any point in time when something is unclear. So this is essentially Bayes' formula for computing conditional expectations or conditional distributions. And then there is, as I said, a slightly non-standard term, but which is also well known, but maybe slightly less well-known. But maybe slightly less well-known, which accounts for the correlation between the noises. Okay, so theoretically, the Kusha-Tortunovich SPDE provides a quite complete theoretical description of the filtering problem. But however, numerically, this is quite difficult, especially in high dimensions, because SPDEs are very difficult to solve. To solve okay now, yeah, was there a question, or was it just some noise? No, it's very much noise. Okay, um, okay, so now fairly recently in the 2010s, um, there is an approach towards filtering which has been developed actually first in the engineering community. Actually, first in the engineering community, let's say, or yeah, in different communities, maybe. And I'll refer to it here as the feedback particle filter formulation in terms of McKinnell SDEs. So SDEs where the coefficients or one of the coefficients depends on the law of the process. And this feedback particle filter SDE is one. Is this one? So, I'm going to mainly talk about. So, as you can see, you can think of this McInrasov SDE as a correction to the hidden signal. So, this is just a copy of the hidden signal. And then there is a correction or a nudging term that drives the X process into the direction of the data. Or maybe more precisely, there is a term that measures the misfit between the observed data. Between the observed data and what would maybe be expected on the basis of the simulation. So, on the basis of this, and then there is a Schlatonovis correction, which is less important for now. Okay, so the Machine Vlasov character of this STE comes through this coefficient K, which is supposed to solve this PDE. This PDE. So B and C are just matrices. Maybe I wasn't, so I don't have so much time. Just see that this is a divergence type PDE, which doesn't have a unique solution because this divergence or weighted divergence type operator does have a kernel. And we can always add divergence-free vector fields to any solution K. And this is the right-hand side computed from data of the problem. From data of the problem. And the point of this is that if we manage to solve this Makimlassov STE, then the corresponding conditional law will coincide with the conditional law of the solution to the Kushnas-Shartonovich SPD and hence solve the filtering problem. This is a little bit, so this is Is appealing because this formulation is, in some sense, much closer to algorithms that are actually or algorithmic approaches that are successfully used in high dimensions for filtering problems, because it's based, of course, after going to the particles, particle representation or associated particle system for the McKinley SDE is associated. Is associated to a particle system, and particle-based algorithms for filtering, like particle filters, are very common and in principle successful. And so these algorithms can then neatly be connected to this formulation in a more seamless manner, let's say. Still, numerically, the problem is, of course, to solve this high-dimensional PDE. To solve this high-dimensional PDE, and certainly there have to be some approximations made in actual implementations. What is interesting is the observation that this PDE can be linked to the Montjam-Père equations from optimal transport. So a linearization of Montjam-Père will give this equation. And so you can think of this term appearing here in the Makin-Lassov STE as, in some sense, optimally nudging the particles into the direction. The particles into the direction of the data. Yeah, finally, I'd like to stress that well posedness for this coupled system between the PDE and the Makimlassov SDE is in fact open. So this is only so the correspondence between solutions to the Makim Lassov SDE and the Kushna-Shotunovich SPD is assuming that is assuming, so is in some sense formal and assuming well-positeness of this equation. Equation. Okay. Now, further on the topic of connecting this formulation to actual algorithmic approaches, we can make a very crude approximation to the solution to this PDE and find the best constant approximation to the solution. And this will give the following McInlassov SDE, where now this coefficient. Now, this coefficient p doesn't depend on x anymore, but does still depend on the law. And the appealing observation about this is that this is, in fact, a somewhat classical algorithm. So it's an ensemble-Kalmann filter formulation, which we can then, in some sense, retrospectively recover from this Mackin-Rassov formulation. And in the rest of the talk, I'm going to talk about this particular formulation, which is an approximation. Which is an approximation to, or which provides an approximation to the solution of the filtering problem, but anecdotally a successful one in applications. And yeah, there are many interesting questions about it, including, for example, like what is the error and how this can be improved. And maybe one comment. I'm also interested in, I think it would also be really interesting to see how this can be related possibly to Joseph's talk or similar approaches. Approaches because the data in some sense drives the SDE. Okay, only that here we are in the Bayesian setting where we are actually interested in probability measures. Okay, so this then we can write down an interacting particle system for this, and this can in fact be implemented on the computer because the covariance can straightforwardly be estimated from particles. From particles. Okay. Yeah, then maybe the main point of this talk or of the work is to address the issue of modern mispelification. So by this, I mean that oftentimes in practice, we don't have access to actual data y that is assumed in the mathematical. Doomed in the mathematical model, but only to an approximate version thereof y epsilon. So, this can be, for instance, because the model is in some sense not the right one or the data is in some other way perturbed. And of course, then in this scenario, we would really like to obtain accurate estimates anyway. So, we would like to employ the filtering methodology that I showed. Showcased in the preceding slides and hope that it will still approximately work. Now, there's a major obstacle to this, which I think, like most of the audience, will be well aware of, namely that integrals of this form are not classically continuous. So this map is not classically continuous. And similarly, the solutions to ODEs or SD easterf differential equations are not continuous in the driver. Continues in the driver. And so, I mean, in classical topologies. And so, if we have a situation like this, then it might still be the case that the solution to the Machim Lussov SDE for the filter on the previous slide gives a completely different output in terms of the final measure, for example, when applied to y epsilon, or indeed when applied to y. And this is the problem that we'd like to. The problem that we'd like to address. Okay, this is an example. I don't think I have so much time to actually go. How much? How many? 13. 13? Yeah. Oh, that's actually. Okay. That's actually quite a lot. You can finish quicker, it's quite quicker. Lots of matches to kill the time. Oh, yeah. Okay, that's all right. So, this is an example just to showcase a few other features that we had in mind when doing this project. So, as you can see, there is an epsilon here. And we do have a convergence in epsilon as epsilon goes to zero. So, more specifically, this is an example taken from, I think, Peter Fritz's. From I think Peter Fritz's and co-authors' work, which is called physical Brownian motion. So the W epsilon process approximates standard Brownian motion in the limit as epsilon goes to zero in supreme norm, for instance. But there will be a caveat to this convergence, which I will show you in the next slide. Yeah. What is interesting, so this looks a little bit different to what I showed you before, because here the goal is now to estimate the parameter theta in a dynamic like this, where we assume that this Z dynamics is driven by physical or indeed mathematical standard Brownian motion, and we observe a noisy version of this process Z. Version of this process Z. So we observe Y, which is a noisy version of increments of Z, so perturbed by randomness. And the aim is to recover theta. This looks a little bit different, but in fact, this can be written as a filtering problem by promoting theta to a stochastic process itself. So this is a type, all right? This is sort of meant to be theta t is a stochastic process and t runs between zero and t. and t runs between zero and t and capital t and for any time point in time t theta t represents our belief about the parameter theta based on the data which is available until time t there's an epsilon dependence in this goal and no i see the above one but in the set is this Above one, but in the set, is this the second problem down below? The one where you're estimating theta, um, this is this is this is in some sense, uh, so this is in some sense hierarchical. I see. I see the w epsilon is the one from up there. Yes, sorry. Yeah, yeah, yeah. So we are basically considering this parameter estimation problem where, so for both cases, either epsilon is positive but small, or epsilon is zero when this is really a standard point in motion. Standard Brain motion, right? Um, does it answer the question? Yes, that's okay. Yeah, thanks for the question. So, so this can be, so this is some sort of so this is an interesting example for us because this leads to a filtering problem with color with correlated noises, as I showed on the first slide. And the point about considering filtering problems with correlated noises is that if noises are correlated, it is well known that the Well, known that the map that sends the path to the measure of interest, to the posterior measure, is only measurable and not continuous for fixed test functions. And this is in contrast to the case where the noises are uncorrelated. So, in order to showcase this problem with the non-continuity of this map, we need to consider a case where this shows up, right? Shows up, right? And yeah, I don't know if you can see this very well here. These are the graphs. What I want to show here in the left-hand side, there's two graphs, one green one, one black one. And you can, and this is sort of the, this is the observation y. And as you can see, by eye metric or in supreme norm, they are almost the same. And so according to this result. Result. But here in the middle and on the left-hand side are the outputs of a filtering algorithm. So the mean of the associated McKin-Lazov equation. And so in the middle, we've got the solution for the dynamics when this is actually driven by actual mathematical Brownian motion. So this is the case where the filter was designed. Case where the filter was designed for, right? Where the mathematics was done for, and you can see that. So, the blue line is the correct value for theta, and it gets recovered by the method as time increases. So, here on the x-axis, there is time, and on the y-axis, there's theta. And as I said, this is now a stochastic process itself. On the other hand, for physical Brownian motion, there is really the problem that it does. Is really the problem that it doesn't work at all. Okay, so this is the point. And okay, so now how can we correct for this? And I think I should almost finish. First of all, why is it so? As you can, I guess, imagine that this convergence here is in this classical. In this classical space, but in terms of iterated integrals, so if you look at these at this convergence in terms of rough paths, they don't converge or they converge if you add the correction to it. So this can be seen also numerically. So you can actually compute numerically just the area. So this kind of expression. And we see that there is a discrepancy here. So this is, so on the on the On the bottom left, there is a area plotted over time, and we can see the discrepancy between the green and the black path, which could not be seen in the graph here above. And this is just because the convergence is in this sort of whirly manner, to speak, speaking intuitively. So as W epsilon converges towards the Brownian notion, it Um, the Brownian motion, it accumulates area while doing so. And this situation is, in fact, generic. So, in this paper, the author showed that if the for sufficiently chaotic ODEs, under fairly mild assumptions, if there is a central limit type scaling, the solution will converge to Brownian motion, but in this weird sense, which requires a rough path correction. Okay, so here's the Okay, so here's then our main, or one of our main results. Namely, we promote this formulation in the Machinasov to a rough path, Machinasov system driven by this rough path Y. So the data is now considered as a rough path. And the main result is what you've got results, which in particular includes a continuity statement from the driving rough path, the data to the Driving rough path the data to the output IT. Okay, and this theorem, although not stated here, also includes a statement for the Gubinelli derivative in the construction, and we can use this to construct a numerical scheme here. Now then, so this Gubinelli derivative is, we didn't really address the convergence of this numerical scheme, but this is a Davies type scheme. But this is a Davies-type scheme, which is in some sense natural for this rough path equation. In the last two minutes, maybe I'm going to say something about this part, because of course it's a problem to know what to put in here, actually, right? This is the remaining question. And here's an answer. So we are considering the following. Considering the following situation. So let's say that Y is a semi-martingale and consider it's autonomous lift. So this would be the actual data after which the situation is modeled. So yeah, so the one that we had before the observation. And then let's say that there is a convergent random family of rough paths y epsilon. That so this family does converge. In such a manner that actually the limiting rough path lies above y, but with a possibly different lift. So that's the problem that we had on the previous slide, that the Shartunovich lift of y doesn't coincide with this second order contribution here, double dash y bar. And so now the question will be: how can we recover the difference between Difference between dash y bar and the Shvartonovich lift and use it to correct in the numerical scheme. So the point here is the idea here is to consider piecewise linear interpolations on a grid of some mesh size delta. So this can really be done numerically, right? This is why this assumption is useful. And we also construct the canonical. Construct the canonical lifts that are then possible after piecewise linearly interpolating. This can also be done conveniently numerically. And we furthermore assume that these rough paths are so that the lifts can be recovered from these piecewise interpolations. So this is, for instance, the case if The case if, in fact, y epsilon here is smooth enough so that these iterated integrals can be defined canonically. So, in this situation, which is, for instance, the case in this example that I showed with the physical Browning motion, the discrepancy between y bar and y strad can be recovered from this commutator of limits. So we can. Commutator of limits. So we can compute, like on the last slide, the iterated integrals computed along a grid of mesh size delta. And then first, let delta go to zero first. And on the other hand, let epsilon go to zero first. And the commutator of these limits will give exactly the correction that we get. Correction that we get that we need to put into the scheme. Yeah, finally, numerically, this can actually be done. I mean, in an actual application, it will be the case that we don't have, so that y epsilon is only available for one particular value of epsilon, so for epsilon fixed. But then we can choose a large, so a grid with large step size and another grid with a small step size. And another grid with a small step size, so that these limits are in some sense, or these different orders of the limits are in some sense realized by this choice of deltas and compute this numerically. And yeah, I think I should really finish. So, this is a plot, or maybe this is a plot where I will just say one last word about these two plots here. Worried about these two plots here on the um the different colors um show different values of this delta or equivalently sub-sampling of the data. And you can see that the area curve moves in this case, accumulating or rather building up this area correction A. And you can then actually graphically use this really in a procedure to compute this this correction and put it into the scheme. Correction and put it into the scheme. On the left-hand side here, this is a case where, so this is the case of mathematical Brownian motion where there is no correction, and you see that, and you can see that all the different colored curves lie above each other. And so the correction will, in fact, be zero and can be sort of seen purely from the data. Okay, yeah, this is the main paper on archive. And I think I should really stop. And thank you very much for your attention. Stop, and thank you very much for your attention.