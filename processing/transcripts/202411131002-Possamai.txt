So what I want to tell you about today, this is a joint work with Davier Guo, who's at Saint-Fran√ßois in Paris right now, Sam Oison and Christoph Heisinger, who are both in Oxford. So the whole thing started from discussions with Gauyue about this page that some of you may know. So this is on Argus's website. He has a bunch of open problems that he finds. Open problems that he finds interesting, and then among those, there was this one that's phrased like this: what's the max entropy wind probability marking, and so I'll tell you a little bit more precisely what this is about. And so, that's the one we were started to be a little bit interested into. So, what's the idea? The idea is that you consider somehow a game, which for simplicity is between two teams. Between two teams, and they are supposed to be as good as each other. If they're supposed to be as good as each other, it's reasonable to assume that at least at time zero when the game starts, the probability that each of the team wins is going to be one half. Okay? You just don't know. And the main question is: how do you formalize the idea about what does make this game exciting? So, one way of thinking about it is that you can't get it. One way of thinking about this is to say, well, the game is exciting if this win probability for one team is not too high or too close to zero before the end of the game. Because otherwise, if very early on, you realize that, oh yeah, one team is going to win with 95% probability, it's not so great, the game is a bit boring. So you're kind of thinking, you want this to remain uncertain until towards the end, where for sure one of the two teams is. Where for sure one of the two teams is going to win, and then the probability is either going to one or is going to go to zero. So, Aldus has a very simple model that we can start by looking at and see whether we're happy with this or not. So, instead of specifying win probability, you just work with point differences. Okay? So, it's actually there's an argument that tells you that it's probability. Argument that tells you that it's completely equivalent to work with these guys or to work with the win probability, but I'll tell you about that afterwards. So this is the point difference between the two teams during the first half of the game, and that's the point difference in the second half. Let's make the assumption for simplicity. I don't think it's too realistic, but let's make the assumption that those guys are IID. Okay? Clearly, you would expect. Okay? Clearly, you would expect that. There's a large point difference in the first half. There may be a large point difference in the second half as well, but it's not always the case. But just do this for simplicity. And then also for simplicity, assume those guys are symmetric and continuous. Then in this case, there is something very simple that's going to happen and that's going to answer the following intuition you may have. So we said we So we said we don't want the probability of one of the two teams winning to be too large. If we look at what's happening at the middle of the game, so we don't want this to be too large, but also you don't want in principle that probability to be too close to one half. Because if in the middle of the game the probability is still at one half, which it was in the beginning, it's like why did I watch the first half of the game? I just skip it and then I stop watching this. Skip it and then I start watching this. This is what lots of people seem to do with. But you wouldn't know in the beginning, right? But what's happening in Caprice Road? You wouldn't know that. No, you wouldn't know. But if somehow, somehow, if you're thinking about how are you going to describe a game that is exciting, you're thinking that you don't want to have something like this. So is this middle point in times important, significant? Afterwards, it's not going to be. But so if you're thinking like this, But so if you're thinking like this and you're saying like I don't really want the probability at the middle point to be too close to one-half you should be thinking probably I want it to be spread out and if I want it to be spread out then somehow you can anticipate that the win probability in this model at half-time is actually uniform over zero one and the proof is super easy so this is the point difference right so Difference, right? So let's say that this is for team one. So team one is going to win given what has happened at half-time if that sum is positive. So this is the point difference in first half plus the point difference in second half. If this is positive, then Tim 1 wins. Those two random variables are independent. They are symmetric. So what you get here is exactly the distribution function. But now it's clear that here. But now it's clear that if you take the distribution function and then you put a random variable that's continuous in it, then we all know that this random variable here is uniformly distributed. Okay? So in this simple model, you get exactly this. Things are uniformly distributed at half-time, and then you may say that you're happy with this. Obviously, the point is that Aldus was not so happy with this model because you You could also say that having things too spread out at half-time is not super exciting. Maybe you want the distribution at half-time to be a bit more biased towards one-half. And also with a simple model like this, you have no idea what is the distribution of the wing probability at any other time besides one-half. So it's a very simple model, but you may want to go beyond that. Beyond that. So, somehow, if you want to go beyond this, you need something that is going to measure what you will call excitement. So, there you have to start making a bunch of assumptions. One assumption is that we are going to equal excitement with randomness. It's like if a game is very random, then you're like, it's exciting, because I don't know what's going on. We can all like that's going to be the math afterwards, right? Don't make me say that this is necessarily what makes a game. Said that this is necessarily what makes a game exciting, but that is the criteria that is chosen. So, if you do this, let's play now if this win probability is easy to see that that thing should be a marking aim when the teams are of equal strength. This starts at one half and it has to end at 0.1. Okay? And that's the question we want to answer. And what ALDUS does is saying that Does is saying that, well, somehow this should be linked to X having a large entropy. Okay, because entropy is kind of measuring how random things are. So I'm going to do something like this. I'm going to do something like this, and I'm going to simplify my life also. I'm going to be working with diffusions. So here, I don't need this to be like a solution to some Markovian SD. You could put like a more general process here, it's just at the end of the More general process here is just at the end of the day, the best one is going to have this form, so that's the one we're going to get. So you start with those guys and you say, okay, among all the martingales that are given by diffusions like this, and starts at one half and end at either zero or one, which one has the maximum entropy? Good question, which does not make sense. Doesn't make sense because the thing you can actually compute is not really entropy, it's relative entropy. Is not really entropy, it's relative entropy, and relative entropy is with reference to a reference measure. And when you start playing with changing the relativity here, I mean, this is always plus infinity, because those probability measures are typically singular with respect to each other, so the question doesn't make much sense. So what ALDAS does is saying, okay, it's too complicated here. I am going to discretize everything. Now I have a finite state space, finite time, everything is equivalent. Everything is equivalent to each other, and I can do some arguments, wave hands, and you can show that somehow this guy that's supposed to be the best choice of a function is related to this, which is a second-order derivative of a function E here, which solves the PD that you have here. So those arguments are completely formal. Formal. We actually don't know how much, I mean, how you can exactly make them correct, but that leads to anyway the actual point of the whole talk is answering those questions. Can you solve the PD that I just gave you before, which then gives you a value for sigma? Can you then show that this S D here is going to have a solution? Is going to have a solution, and can you say some things about that mortality? What is mainly by maximum entropy? So, for each gene x, yes, you want to maximize somehow relative entropy, yes. But I'm going to show you in a second, if you go to continuous time directly, what it is that you're kind of doing. So, it's going to be a process. So, you what you're going to be maximizing is something that's more related to relative specific approaches. Specific entropy. So that's not exactly the entropy itself. So that's the whole bunch of questions, and we kind of answered as much as we could all of them. So the first one, well, there is no explicit solution, but there is a smooth one. And you can say a bunch of things about it. You know how it behaves. You have a bunch of properties that you can reasonably obtain. You can do numerics, obviously, and then have an idea about how Then have an idea about how the half-time distribution is looking like, and of course, it's not going to be uniform like in the simple example, but yeah, we will see in a second what it looks like. So, before I tell you all of this, there is a natural connection between what I'm telling you and martingale optimal transport. Okay? So, the point is that you can recast the problem that I The problem that I've shown you before into finding the maximum, the best martingale in a certain set. So, those are called usually win martingales because they are martingales. They start at one half and they end up here. And the choice that you will make for this function f is exactly what is going to your choice of describing how attractive or how random or how exciting the game is. Or how random or how exciting the game is. So that's a very generic problem. That's not the one we're solving. We're actually solving one where F is very specific and choice is the following one. So this is in another paper by Julio Bakhov and Matthias Beitelberg that came out pretty much at the same time. So we talked for a while with them to know whether we were doing the same thing or not. And it turns out that we're not doing the same thing. And it tells us that we're not doing the same thing. It looks similar, but we are not. So, what they are doing is the problem I showed you before. Okay, maximizing this. And their choice is what's called specific relative entropy, which is always well defined, and which in this setting is given just by this. So it's one plus the log of sigma squared, where sigma squared is key. And so they just say that's my criterion. I am going to try to find the best parting of this form. Partingle of this form so as to maximize this here, and they show snot. I mean, you do have to be a bit careful, but they can get something that's completely explicit, and they get this here of the following form. So, sinus pi x divided by pi, and then you get square root capital T minus T. And they make the link saying that, well, this is an exciting game. Now, does that answer ALDOC? Does that answer Aldus's questions about the PDE here? Well, yes and no. Yes, because once you have sigma here, remember the link here between sigma and the entropy E. So you compute it, you get this slightly ugly function. It does satisfy the PD. It satisfies one of the boundary conditions, but it does not satisfy the other boundary conditions that I have. Like the other boundary conditions that I have. So it actually does not solve exactly L2C PDE, it solves a similar PDE, but with different boundary conditions. So what's the relative activity? Specific relative activity can be. That I have no idea. I just know that this is this, and that, like. I know that this is this, and that's the one that they said was going to make sense for them. So, what's the difference between what they're doing and what we are doing? Well, the difference just stands in what type of games you're looking at. The fact that they get rid of one of the boundary conditions means that the game they look at is a game that has to last until capital T. So it's a game that cannot finish early. Cannot finish early. It's not a boxing match. In boxing match, it can happen that somebody gets knocked out and then the game is over. It's more like football game. In football game, typically, unless something extremely weird happens, the game is not supposed to be interrupted in the middle, though that can happen. And so that's the one that they are doing. But what Aldus is proposing originally is a game where there is a positive probability that the game will end early. That the game will end early. So you realize this, and then you say, okay, so what I will do instead of doing this criterion that they have here, I'm going to do this relative entropy, but I'm not going to integrate this until the end of the game. I'm going to do this until the minimum between t and tau, where tau is the first time that my win probability exists. That my win probability exists 0, 1, meaning the first time that it is 0 or 1, meaning that the game stops. Okay? So that's the criterion in terms of martingale optimal transport that is associated to what ALDUS does, and what they are doing is a variation of this. Okay, so just you're looking at two different names. So if you uh so sorry, check. Okay, so here I plotted the densities. I plotted the densities that you get for here. So, this is the sub-probability density of X because the game can end early at different times. Maybe the most interesting one is the T0, which is the half-time of the game here. So I'm taking capital T equal 1. So no early stopping. So that's what Julio and Matthias are doing. So T equal T0, that is this one. So that's what Is this one? So that's what their distribution looks like. Here, and with early stopping, that's ours. That's what it looks like at half-time. So you can see that those are completely different games. They really don't have much to do with each other in what you have here. Okay? So, this is just the presentation at the beginning to relate what Aldus was explaining in continuous terms. Was explaining in continuous time to matting and optimal transport and to just make the difference between those two descriptions of the game. Okay, so now if you are looking at what they've done, then they're pretty much done with all the questions because they have something completely explicit, it's smooth, they can analyze all the behavior, everything is fine. Now, of course, the fact that you find a smooth solution is top-rising. I mean, it's weird. I mean, it's weird, we're not supposed to find nice solutions. And indeed, the moment you add the second boundary condition, everything breaks down. So, what do you do? So, okay, we're here. So, now we'll get into a little bit more of the math, where I want to prove that the solution to this problem actually does the job, solve the problem that I want, and leads to uh at least uh an entropy function E, which is smooth enough. Function E, which is smooth enough for me to be able to do things. In particular, I would like for it to be C2 so that I can define sigma, and then I would like to have at least some regularity on sigma, because remember, at the end of the day, I want to define this process. Okay? In the best possible world, I would like to get that this thing here has a strong solution, hopefully unique. It has a weak solution, and it's unique, and we're going to be happy as well. But so, all of this tells me that. But so all of this tells me that I need to understand a bunch of things about the behavior of sigma. So, how do you do this? Okay, so then, I mean, I don't do matical optimal transport, I do optimal control. You show me a PDE, I look at it, and I say, ah, this is a PDE with a non-linearity where the dependence in the action is convex, so it's a control problem. So, control problem, I do So control problem, I do French transform, and then I get to this control problem here, which actually is completely inspired by what I showed you here. But somehow the way we did it is that we just did this first and then realized the link with Martin Galoptimal Transport afterwards. Okay, so that's another way, if you want, of seeing this, is just taking the original PD and realizing that it is there. And realizing that it is the Hamilton-Jacobi-Bellman equation of a certain control problem. The control problem is the following one. You get this nice process here, it's controlled by alpha, it's just in the volatility, and you're looking at this exit time problem where you're trying to maximize this criterion that depends on alpha, and you do this up to the first time where your state process X exits the domain that you. Okay, so now you want to do this and you're wondering, okay, is that being nice? Well, it's not nice. I mean, you should expect what's going to happen here. A priori, alpha here can get close, as close as I want to zero, which means that I can potentially kill the diffusion, which means that the Habilton Jacobi Bellman equation that I have is not a priori uniformly elliptic and therefore um may run into issues May run into issues with regularity. But you can think also a bit and tell yourself: look, it doesn't make too much sense to let alpha become too close to zero. Because when alpha gets close to zero, that thing here becomes negative. And I want to maximize. So it doesn't seem like it makes sense. Actually, the whole thing, I think the best thing you can prove, which Can prove, which I don't think we managed to do it, but we strongly suspect that it is always optimal to take alpha greater than one. So you don't make the log negative. But it's not so clear because at the same time, when you let alpha become smaller, you integrate something here that is smaller, but for a longer time. Because if alpha is smaller, this is less volatile and it's going to take up. And it's going to take a priori more time to exit the domain. So it's a little bit unclear what is the correct balance, but something that should be clear is that you should not let this here become negative, meaning that you should not let alpha become smaller than exponential minus. And this is not too hard to prove. So that's the AGB equation and so on, but what you can prove is that this control problem has the same value. This prob control problem has the same value as the one where you bound controls from the one over here. So it's a bit of computations, but now you're very happy because that is a very nice uniformly elliptic equation. And then you just open one of the books by Philo for events, just look whichever one you like the most. It is going to tell you that this thing has a unique classical solution. And because it's equal to the previous one, and you're in business here, you know exactly how all of this is going to work. All of this is going to work. So you know a bit more, but I think I'm going to tell you about that afterwards. If you want to approximate things, that's easy, you just do finite differences. There is a comparison result for viscosity solutions to your equation, so Paul Sudelidis tells you that this is going to converge to your solution, everything is fine. And then you want to understand a bit more, because Understand a bit more because I mean it's great if I smooth things, but I want to understand what's the behavior. And one thing here is that if you invert time and you look at the second derivative, which recall this is linked to sigma because sigma squared is minus one over the second derivative of the entropy. And you give it a name, then you just plug this here and you try. You just plug this here and you try to find the PDE that this second derivative is supposed to satisfy. So the PDE part here is straightforward to get. The boundary conditions, they are not clear because if you differentiate the boundary conditions here, you will get zero everywhere. Just believe me, those are the correct ones. Those are the ones that you need to have for things to make sense. But this PD here, it's a normal one. If you do physics, at least, it's a normal. If you do physics, at least it's a known one, that's called logarithmic diffusion. And there are plenty of results out there that give you information about this. So why am I interested in this? I'm interested in this because now I can define sigma, because I know that E is twice differentiable, but I want to solve the S D, and that's driven by sigma, so I need to have X more than just continuity for C prime. Okay, so if you look at this guy here, then you have a bunch of people that worked on this, again, Richie flows, gas kinetics. There is well-known theory for solutions of this in like appropriate symbol F spaces. And you can relate E to this one here, just integrating twice. The nice thing with this result is that it gives us information. Is that it gives us information about long-time behavior of the solution? So when t becomes large, you can so there is a clear stationary solution here, which should correspond in some sense to the limit as the length of the game goes to infinity. It's just this simple quadratic function here. And thanks to this representation, you can prove that the distance between The distance between the solution and the stationary one is controlled like this. So it's controlled with an exponential, I mean, yeah, it decreases exponentially fast to zero as capital T goes to infinity. So you take information about how close that thing will be to this nice parabola, which if you remember the pictures here, it's not too surprising that you have something like this. Surprising that you have something like this, but it's not exactly the parabola, but if t is large, it's very close to it. Again, information. Then we were wondering if we cannot get an explicit solution, which we're pretty sure we cannot. Can we get explicit approximations? So, there, this is where Sam Olson came in. Uh Sam Owenson came in and did some black magic with things with PDEs, but I'm still uh quite surprised. I mean, you can do the math, it works, it's great, but understanding why if he thought about this, that's more mysterious to me. But the point is that you can do a bunch of things. So there is this thing called the outer region here, which gives you asymptotics when you get close to capital T. So in this region, what you do is So, in this region, what you do is you forget some of the boundary conditions, you look for separable solutions, you find something, of course, that's going to look like what Matthias and Julio got, but you can do this. Then, there is an inner region where you're closer to zero, where now you will forget the boundary condition at one. So then you solve this, and then you do a bunch of things, and then you're looking for. Things and then you're looking for you do a bunch of computations, and in the end, you get this. So the function p, okay, which again is related to the second order derivative of e, is well approximated by this function. So this guy here, f, solves a first-order ODE that you can find. And then you have this one here, and those are modifications of the solution that Julio and Matthias got. But basically, if you do all of this, But basically, if you do all of this, I think here I'm showing you what is the error you're making between the real solution and the approximated one for when time increases here, and you can see that it's reasonably good. When you're very close to the beginning, it's great. As you increase time, it's not as good, but it's still doing a pretty good job. It's still doing a pretty good job here. And I think what you have here is the volatility. So, for these two times, you can't tell any difference between the two curves. And for the other ones, you can see that it's still pretty close to the other one. So, it's a reasonably good approximation if you want to do computations. Okay, so that's answering Aldus's questions. Is there an explicit solution? No. Can you approximate it well? Yes. Can you approximate it well? Yes. And then I think, yeah, so this is just the somehow the inequalities you deduce from the ones you have with P to E and to sigma. So yeah, you have a bunch of things like that. But then the last question that you want to answer for the game, as I said, is that is the marking ill defined? Because I've defined sigma, it's the best. Because I've defined sigma, it's the best one. I still need to know that I can make sense of this here. And you can, because you're, as usual, very lucky. As I told you before, there was this lower bound on the controls alpha, which translates into a lower bound to sigma. That's great. Sigma is uniformly bonded away from zero. That will always give me uniqueness of at least weak solutions. And actually, I have better than this, at least if I Better than this, at least if I stay away from capital T, I can prove that sigma is one-half agree. So this you get from, again, looking in the right spot in one of Krilov's results. You can prove that you have this, and then now you're done. I mean, I give you this SDE. Sigma is uniformly bounded away from zero. Faldor continues, this has a unique strong solution. Um just again look wherever. Ah, so where did we find that one? It's probably in one of the papers by Rochner. In one of the papers by Rochner that gives you this here. So, this has a unique solution when you stay away from capital T and then you can extend it by continuity at capital T. And you can check, as you should have, that the probability that with this guy here that the game ends before capital T is okay, so I mean, you know that the game is, of course. Write it? Okay, so I mean, you know that the game is, of course, going to end before capital T, but it can end strictly before. And so it does happen that the probability that tau star is strictly less than T is positive. Okay? So I think that's pretty much everything I wanted to tell you. Obviously, all of this is like a very specific model. If you don't like specific relative entropy, because at the end of the day this is what we are doing, there may We are doing. There may be other criteria that you want to investigate. And now, if you do this, I have no idea if any of the things we're doing here is going to work. It's just in this one. If you combine in the proper way some probabilistic and PD arguments, you kind of get the full story at the end of the day. So yeah, that's it. Thank you. Any question? Yeah. So what if instead of boxing you purchase a finite time? Well there is a finite time, which is. I mean I get what you're asking, but just to purchase there is a finite time. Yes, because they have to press the thing, so it cannot last forever. But if you have a game that never stops, then I mean, at least here, you will get the Here, you will get the stationary solution to the equation, I think. So, I'm not sure if this left on this to be very sort of hard to conceal. Yeah, because the elliptic PDE has a this unique solution and then your problem. Yeah, so the the stationary version of the problem is too simple somehow. One quick question. Do you have these criteria so you can change the criteria? Because it could be easy. Are you looking for macro foundation of this thing? For a game to be exciting, it's the interaction between the games and the people who are there. So are you looking for modeling of this type of thing between? So we haven't tried to microphone this at all. I don't know if anybody has done that. But I mean, it's more behavioral stuff than anything here. So the I mean, I don't know, when you watch a game, what makes it exciting? For me, if it's too uncertain, and it's my team, then I feel bad during the whole game. I'm more anxious than anything. And then just except. You want your team to bring the team. But then you can like. But then you can like, come on, if you watch sports, you're not, you're going to just like take some discipline or there are studies that say that even if you don't care about any of the teams, if you look long enough at the game, you will find reasons that are going to push you towards one team rather than the other. That's, I guess, kind of how we react as human beings. But yeah, like it's really ideal, like I want to do math and then there is a cute problem behind this. There is a cute problem behind this, that's nice. But if you really want to do what you're saying, then it's a completely different story. What about election? I think it's the same, right? You want your team to win, and then you just look at everything, and you are feeling very bad, and then the next day you're feeling even worse. So, what if we put a sound sort of betting mechanism these days that we need odds and thoughts on dollar values associated with it, right? Associated, right? And then you can incorporate like a TV function, whatever. Yeah, I mean, that's a reasonable reasonable thing. Just like a Twangle versus Scarvis, there are several actual markets with all the board and everything. So at some point you introduce some lower bound and you control it. So what happens when it goes to zero? When it goes to C. So the optimal one is which the optimal one is always above 1 over E. So that we know. From the original problem, it's always above that. I think numerically we observed that it's always above 1, but this we could not prove. So it never visits the other part close to zero, so that's why regularity is nice. So the intuition was really that I mean, I'm trying to maximize this. I should never make that quantity negative. So that's how you get the one. I mean, that's. That's intuition. That's the intuition, and then you have to prove it. I think tokens. Yeah, so here, like, you really, like, it's the same control problem whether I take every non-negative alpha or if I take only the alphas that are above one. Or if I take only the alphas that are above one over, the two have the same value.