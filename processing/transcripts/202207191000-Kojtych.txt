First, thanks for the invitation. Actually, I've just finished my PhD in mechanical engineering, so I hope this will be still interesting for you. There are some still some optimization in it. I will talk about how to escape unknown discontinuous regions in black box optimization. And this is a joint work with Professor Shallow Day. So the first motivation for this work was actually the design of aircraft engines. So basically, an engine consists of rotating parts called the blade. Rotating parts called the blade here in blue, and a fixed part called the casing. And because of the blade vibrations in operations, you may have contact between the tip of the blade and the casing. And this leads to non-linear vibrations, which may be dangerous. So during the design process, we want to predict these vibrations very accurately. That's why we use non-linear simulations. So a design site. So, a design cycle may be seen as an optimization process to go from an initial configuration of the system to an optimized one. And today, the non-linear simulations are used at the end of the cycle. So that's not very effective. And the idea behind my PhD was to develop tools to account for these non-linear simulations in the main optimization loop. So that's just the motivation for the project. The first thing we need to know is what know is what are the mathematical characteristics of these nonlinear simulations. So here you have an example of results from such simulations. You may see the blade vibrations with respect to two engine parameters. And what is interesting is that we have lots of discontinuities, we have sharp gradients, regions, and this means that we don't know the, we don't, the derivative does not exist for some. The derivative does not exist for some points, and although, because of the complexity of the simulation, most of the times we don't have access to the derivatives. So, that's why we choose to model these simulations as black boxes, where only input and output are known. And although, in addition to that, in our context, these simulations are very time consuming. They may have several local minima, and finally, they may simply fail for some points in the space of the variables. In the space of the variables. Finally, the problem we want to solve is a black box optimization problem. We want to minimize a function f with respect to some variables x contained in the space of the variable capital X. So here you have an example of the space of variables for a very easy problem with just two dimensions. We consider some constraints modeled by the C vector. And here you have only one single constraint, which is Only one single constraint which is satisfied in the feasible domain omega. And in our context, as I said, we have discontinuity. And from a mechanical engineering standpoint, these discontinuities betray unsafe regions of the space of the variables. So we want to avoid this region. But if we solve the problem this way, we may have an optimal solution very close to a discontinuity. And that's what we don't want. So to avoid Want. So, to avoid that, we add a remoteness constraint D, which basically says that X should be far from discontinuities of some output function chosen by the user. So, at the end, in this problem, the function f and c are deterministic. I'm not going to talk about any stochastic stuff. And they are deterministic functions. They may be discontinuous. They may be discontinuous, and the D constraint is an infinite constraint. So, theoretically, it's not possible to be sure that a point X is far from discontinuities without an infinite number of evaluations. So, the challenge here is to tackle this constraint in a black box optimization context. And we developed a version of the Mesh Adaptive Direct Search to solve this specific problem. So, that was for the contest. First, I will describe the way we model the discontinuities. After that, I will just remind some basics on the MADS algorithm and then present our algorithm, just give a brief comment about the convergence analysis, and finally show you some results. So, the first thing necessary is to model the discontinuity a bit more. Models of discontinuity a bit more accurately than just explore from discontinuities. Numerically, we can't really distinguish between true discontinuities and sharp gradients. That's why we introduced a region of weak discontinuities here in orange, so the region D, in which the rate of change of some function between two points at distance atmosphere exceeds a limit rate to. So this region is characterized by two parameters. Characterized by two parameters, a limit rate of change to and a radius RD. As a consequence, we can't be exactly sure of the position of the discontinuity in this region. That's why we have to take into account a safety margin around this region to be sure that the solution will be far enough from the discontinuity. And this safety margin is basically a region of a ball of radius Re around the region D. So, to sum up with this model, So to sum up, with this modeling, the remoteness constraint D is satisfied outside the colored area and the optimized solution of the problem is in the right area. That's just, of course, an example. So as I said, we solved this problem with a version of math. So I need to recall some things about the measure-adaptive value research. This was developed in 2006 by Ode and Dennis. In 2006, by Ode and Dennis, and I will explain some details first for an unconstrained problem. Math is an iterative algorithm, and it requires a starting point at which the black box is evaluated, so here x0, and then the aim of each iteration is to find a better solution than the current incumbent on a mesh of size delta k. So each iteration begins with the search step. With the search step, in which the user may evaluate the black box in a finite number of points on the mesh very freely. So, here in this example, we suppose that neither S1 or S2 are better than X0. So, the algorithm continues to the pole step in which pole points are generated in a frame of size delta K around the best solution. And this time we suppose that T three is better than X zero. 3 is better than its zero, so the iteration is qualified as a success. And as a consequence, we go to the update step and we want to update the best known solution at this point for the following iteration. The following iteration begins at x1. I just skip the search step. We have already, we have again the poll step, and this time we suppose that x1 is still the better point. So in this case, the iteration is called a failure, and the mesh size. Failure and the mesh size and pulse size parameters will be reduced after for the following iteration, and it continues this way. So, that was the version of math for unconstrained problems. In the case of constraint problems, there are several ways to address the relaxable constraints contained in the vector C. And I just want to talk briefly about the progressive bi approach that Joseph mentioned earlier. This approach is based on the This approach is based on the constraint violation function H, which quantifies how much a point violates the constraint. And if the value of H for some points exceeds the threshold HKmax, then the point is simply rejected. And the idea behind this is to authorize the violation of constraints for some points and progressively decrease the threshold to converge, to try to converge towards the feasible domain. Domain. So, with this approach, the points are compared during the run of the algorithm based on both f and h values with the dominance relation. And practically, in the algorithm, what are the changes? So, first, we have another type of success because we consider the iteration where both f and h value are improved, and iterations where only h values are improved. H values are improved, and we also have to deal with the update of the threshold to be sure we will decrease it. I skipped a lot of details about this algorithm. I just want to give you the main points to be able to understand my algorithm after this one. So that was for the review. Now I will talk about the algorithm we developed to address the discontinuity problem. It is called disc commands. Okay, so here we want to solve this specific problem and treat the constraint D. The aim of an iteration is still to find a better solution than the current incumbent, and the algorithm is based on math with the progressive barrier, so we have the same structure for the iteration, but with some differences. The first difference is that after each evaluation, either in the search Evaluation: either in the search or the poll step, we try to reveal discontinuities in the space of the variable because we don't know the position of the discontinuities without running the simulation. So, for example, just after the evaluation of the point T3, all points in the board of RD around T3 will be considered. So, here are T1 and X1. And if we suppose the user is only interested in the discontinuity of F, then Continuity of f, then we will just compare the rate of change of the function between a couple of points and compare it with the limit rate to. So, in this case, the rate of change is less than to, so nothing happens. The algorithm continues. But this case between X1 and T3, the rate of change exceeds the limit rate to, so a discontinuity is revealed, a weak discontinuity. And as a consequence, we consider x1. We consider X1 and T3 as revealing points. This means that these points belong to the discontinuity region we want to avoid, the one from the beginning of this presentation. So to be sure, we get solution away from them. We introduce an exclusion constraint to penalize the ball of radius RE around this point. And the idea is to escape from the safety margin. I will detail some of this aspect just after. So to take So, to take into account this exclusion constraint, we need to add another type of iteration, which is a revealing iteration. And finally, for the need of the convergence analysis, we need an additional pole before the usual pole of mass, which we call a revealing pole. And I will also explain it, give a few words about that after. So, about the revelation and exclusion, here And exclusion. Here are the layouts of the domain for an example problem. But remember that when you run the algorithm, you don't know where these domains actually are. And the only things we know are the revealing points here in red. So we have to keep track of the revealing points during the run of the algorithm. So we introduce a set of revealing points big decay, and as a consequence, we define the exclusion constraint with respect to this set. With respect to this set to penalize the ball of radius RE around this point. This means that at each iteration, we solve a different problem because the constraint, the exclusion constraint depends on the iteration. So that's the idea of the algorithm. And to solve this, if we want to use the progressive barrier, we have to redefine the function, the constraint violation function, to make it dependent on k. So when we add the On k. So, when we add the term with the exclusion constraint dk, the constraint variation function became dependent on k, and it will be a challenge for the convergence analysis. Now, finally, when during the run of the algorithm, if a revealing point is found, then the iteration is considered as revealing, and we stop the evaluations at this iteration. We update the set of revealing points. We update the set of revealing points, accounting for the new information we have on the problem. As a consequence, we update also the exclusion constraint and the constraint violation functions. And this equality is very important because it means that the feasibility of points may change during the run of the algorithm. Because at the beginning, we may have points which seem feasible, but they may be after some evaluation close to discontinuity. So they may become. Continuity, so they may become invisible during the run of the algorithm. This means that we have to deal with the update of the bio threshold very carefully. I won't detail that, but there are some things to think about here. And finally, we update the mesh and pole parameters at the end of a revealing iteration. So that was the third ingredient of this algorithm. And finally, the last difference with the Last difference with the mass with the progressive barrier is that we need a revealing pole. Why? Because when we have a series of unsuccessful iterations, the pole size parameter is decreased. So at a certain point, the pole is not sufficient to detect discontinuities at a sufficient distance from the best known solution. So we need to have another poll to ensure that we detect discontinuities sufficiently far. That's why we add this revealing poll. Why we add this revealing pole, and it's basically just we just evaluate one random point in a ball of radius or m, this doesn't matter, around the best solution. Of course, this point should be on the niche. So that was for the description of the algorithm. Now I'd like to just give a few words about the convergence analysis, but very briefly. So, the aim is to draw some optimality conditions related to the original problem. Remember, the problem with the remoteness constraint D, which is actually unknown. So, we explicitly define this constraint in the same form as the exclusion constraint, except that instead of the set of the revealing point decay, we have the true discontinuous region D, which appears in the expression. This constraint is bounded. This constraint is bounded by 0 and 1. And we also define for the need of the convergence analysis the ideal constraint violation functions, which accounts for this remoteness constraint D. This function is called hat H. And the difficulty here is that we want to have optimality conditions about both F and hat H, depending on the feasibility of points. But the only things we know are DK and HK. we know are dk and hk. This function hat of h is not accessible at all. So that's the difficulty in the convergence analysis. About the steps of the convergence analysis, it's based on the analysis done for math. So first we show that under the same assumptions we show that the mesh gets infinitely fine. It is sufficient to show that there exists at least one refining subsequence converging to a refining point at hex. Point at hex. And then, depending on the position of this point in the space of variables, we may derive some conditions. For example, in the case where the algorithm converges towards a point, a feasible point far away from the discontinuous regions, then we preserve the results from math because, in this case, the discontinuity does not modify anything to the original algorithm. In case B, that's In case B, that's actually quite the same, except that the refining point in this case is infeasible. And finally, the new results is about, it's when the algorithm converges to a point in the discontinuous regions. And in this case, we have a stronger result than MAS, but it's not something revolutionary at all that just because we consider additional assumptions. Indeed, the idea behind the proof is to make a link between what we know, which shows the value of HK, and what we don't know, the hat H value. And to do this link, we need some piecewise continuity assumptions and also the revealing poll I've mentioned just before. That was all about the convergence analysis. Now I will move to some numerical results. I'd like to show you two. I'd like to show you two applications of the algorithm. The first one was to detect weak discontinuities, and in the second one, we tried to do something a bit different. And we wanted to escape regions where hidden constraints are violated, because we can use the algorithm to do that, finally. So, just to illustrate the behavior of the algorithm, I take my first example with blade vibration. So, we are interested in the vibration. In the vibration at the tip of the blade. It's a very simplified problem. And here you can see the result of 45,000 simulations of these blade vibrations with respect to two parameters. On the x-axis, that's the clearance, the space between the tip of the blade and the casing, depicted by the orange arrow. And on the y-axis, the rotation speed. The black region indicates high vibrations, high deep High vibrations, high displacements, whereas the white region indicates low displacements. And of course, we want to have low displacement for safety reasons. And what is interesting is that optimal configurations are very close to dangerous configurations. So we are going to look at this small rotation speed range. On this space, we may approximate the position of the discontinuity by the dash. Position of the discontinuity by the dashed line, and we are going to solve a very simple optimization problem with only two variables: the clearance and the rotation speed. We simply want to minimize the S function, the clearance, so go left on the graph. As a constraint, we just want to limit the displacements. This means that the feasible domain is actually the top right corner of the graph. And finally, we have our remoteness constraint. We have our remoteness constraint to be far from the discontinuity. So that's the problem. The optimal solution is the point x star, and we start the algorithm from the point x0. A solution returned by this command is the point x prime. As you can see, it's close to the optimized solution, but still it's not at the optimum. It's in the safety margin. It's in the safety margin, and that's something we observe a lot in numerical tests. That's not a problem with respect to the convergence analysis because the convergence analysis is valid for an infinite number of evaluations. So there's no problem with that. What is interesting to look at is the history of all points evaluated during the run of the algorithm. In particular, in red, you have the revealing points and the stretched circles or the exclusion ball around these points. Ball around these points. So, what we can see is that these exclusion balls superimposed well on the margin. So, the algorithm is able to detect discontinuities and to escape them during the run. So, that's what we wanted. Of course, that was just a simple example. Now, the other applications were suggested by Sebastian, and it was to use the algorithm to escape regions of hidden constraints. I think the maybe the potential. The maybe the potential of the algorithm is in this application. So, first, we consider a Tyran production problem. This is actually the black box Tyran, which is available online. It has eight variables, 11 constraints, and for some X, the simulation simply fails. So, we have hidden constraints, constraints that are not explicitly given in the expression of the problem. We want to use these commas to escape this region. Has to escape this region. And to do this, we need to model these hidden constraints in a certain way. So each time the evaluation fails, we artificially increase the value of the objective function to a very huge number so that it creates a weak discontinuity of the function. And this way, we can detect points where hidden constraints are violated. We need also an indicator to Also, an indicator to assess the performance of the algorithm on this problem. So, we need to be to know if the remoteness constraint D is satisfied for some point X. To do that, we use a very simple indicator H, which depends on a point X and a radius sigma. And to compute it, we just generate a thousand of points in a ball of radius sigma around X, and we count the number of times the simulation fails. The simulation fails. So that's a rough idea of the severity of the constraints around X. I fix the problem parameters and as initial point, we choose one of the best known solutions at the time I ran the test. And for this solution, we know that lots of hidden constraints are violated for the ball of radius 15 around this point. Just to give you an idea, Just to give you an idea, the number 15, sorry, this corresponds roughly to the radius of all the colored areas because it's the sum of Rd plus Re. So this means that the point X0 would be actually in the orange area in such graph. We went 100 runs of the algorithm with different random seeds because in the revealing poll we have some randomness. Randomness. And if we first look at the F values, here you can see in blue the F values for the solution written by this command, and in orange the F values of the initial solution. So we deteriorate the value of the optimum. So that's because actually as the initial point is close to hidden constraints, it becomes infeasible. So we moved from this solution. And if you want to qualify how much Qualify how much the hidden constraints are violated around the written solution. We can just draw kind of histograms. So the x-axis is the h value, our indicator, and the y-axis is the number of occurrences. We have the value for x0. And if I had the values for the solutions returned by these commands, what I can say is that generally the solution returned by these commands are via. Commands or the points around these solutions violate less hidden constraints, but that's not really striking, I would say. So, if we look at the same indicator, but this time with a radius of five, which corresponds to this time to the radius of the discontinuity region, so in dark orange on the picture, this time we see that most of the solutions are very far from hidden constraints. Constraints. So this gives us an indication that the algorithm managed to find solutions away from hidden constraints. Finally, I will just sum up all of this. So this command is an algorithm to escape unknown discontinuous regions, but we may model different things with discontinuities, including hidden constraints. Actually, we build inner approximation of the safety margin. Approximation of the safety margin to treat the infinite constraint D. It is based on maths with the progressive barrier, and we preserve the convergence properties, and it was validated on some analytical and engineering problems. This led to a publication in SIOT. It is not available yet, but there is a preference if you are interested in. And about the perspectives, as I said, I think one interesting Said, I think one interesting thing would be to use it on other hidden constraint problems just to see the performance and maybe improve it. There also, on a short term, I will implement this algorithm in the fourth version of NOMAD to make it available with a focus on hidden constraints. So there is some work to do about the parameters to choose them very carefully. And also, I think the performance of the algorithm could be really improved, for example, by Could be really improved, for example, by scaling the in the H function the relaxable constraint in C and the exclusion constraint decay, maybe modifying the revealing pool to make it a bit smarter, or maybe use surrogates for this continuous function. We could also compare the algorithm with other methods, but we need to adapt them to treat this problem because we don't know any other methods to do that. And to finish, And to finish, maybe could be used for other kinds of infinite constraints. And in any case, I will be very happy to hear your suggestions for the use of this algorithm. So thank you for attention.