Referencing of this system, or you know, sometimes maybe that might be called okay. This is just the minimum, it's like win max thing, so it's the minimum over x maximum. Okay. Alright, so what are we doing here? We are coloring each element of the ground set with plus one and minus one. And then we are seeing what's the maximum imbalance in any set. So equivalently, this is the same as this is also equal to the smallest that the infinity norm of Ax can be as A x and b as x changes over the vertices of the hyphen. So this would happen. Okay, right. And now sorry, unfortunately, usually I'm going to get rid of this. Okay, and you know, famous theorem due to Spencer. From I think 1985 says that the discrepancy of any set system is at most root of n log, I would say E plus M of n. Right, and again, of course, this should be contrasted with what you get by doing Trusted with what you get by doing Chernoff and Union bound, which would give root n log n. So this is the saving. And I mean, if you don't like all of this log business, what you should really keep in mind, so that the case to keep in mind, think of 20%. So in this case, Spencer shows the discrepancy. Is at most root n and turn off posterior bar which shows that the discrepancy is at nice. Right, so this is the theorem. Now, uh, Spencer's proof. Now, Spencer's proof was existential, so he used this entropy method. And there was a question of Spencer's work. So there was a question which was, can we actually find low discrepancy coloring? I'll just 15 new stones of stone. Algorithm. And I think maybe there was some version of Alon and Spencer where they even suggested that this might not be possible. But famously, Bunsell in 2010 showed that he could do this. Show that we can do this. And after that, there were quite a few different algorithms built for this. So there's a very well-known work of Lovett and Mecca. I forgot the years, but it's between 2010 and 2022, right? So Rochbos has a very nice paper. Very nice paper, I'll draw the sink, and many other things. Okay, so there are like a lot of algorithms for finding the load discrepancy coloring in the sense of Spencer's theorem, but the running time of all of these, right, or the running time of all of these are really. Well, if all of these algorithms okay, so it's a little bit complicated, but the running time is. So it's like n cubed. So assuming assuming As you know like no fast matrix modification that matrix multiplication takes time and cubed, which you know it doesn't, but if you don't use fast matrix multiplication, then all of these algorithms, these are fairly different, they take time and cubed. And even under the moon And even under the most optimistic assumption that matrix multiplication can be done in n squared time, right? So even as even this order n squared, right, so this is the most optimistic assumption. The fastest thing is really due to l down and sn and it's something like 2 plus 1. 2 plus 1 over t. Of course, of course, you know, we are very far from showing that mutual multiplication can be done in time n squared. But even if you could do this thing, you know, it doesn't really give you like n squared and nine squared. Okay. So, yeah. Okay, so people who work in algorithmic discrepancy, I think I think it was Kenny Didi, she asked, okay, you know, can you actually do anything faster than these things? And I mean, the reason somehow you have all of these bottlenecks is that the algorithms are like sort of really linear algebra basic. So you know, matrix multiplication is sort of a bottleneck. But even like raw matrix multiplication is not enough. You need to solve linear programs or you need to do QRD evaluation or something. A QID convolution or something. So the question we ask is: okay, can you actually do faster than this thing? And of course, the thing that you would want is a linear decide of the input. And that's sort of what we show with Action Metal. So the theorem is that you can actually find and find. You know, in sort of inputs possible, right? So in time, which is, let's say, like, you know. So in the sex system notation, right? So remember I have this notation, this is slightly less, or in the time this is fair. And partisan multi-factors. Okay. Right, so this is the result. And now what I want to do is I want to give you some idea of the proof. So there are two components. One of them is a question about geometry, and then one of them is a question about optimization. Given the theme of the workshop and the time constraints, probably I'll spend time talking about the geometric aspects. But before I do that, are there any questions about Before I do that, I have only questions that I'll escape into the theorem itself. So it's n squared polygon? So in the dense case, this would be n squared polygon, but you don't need to have n squared elements. Yeah, but of course. So sorry, right-hand side, sharp for this part? Well, so I mean, so this also includes the Geys case, and that is sharp. No, I don't think it would be. Assuming maybe it's improved, you know, improved. improve right you know improve yeah but you know yeah so i mean you you could have something like you know like like like a small yeah i mean you could have like some heavy rows and then some light rows and yeah i mean in principle in principle you could have an improvement and you could have an instance depending on we didn't really try to do this and i agree that you know i mean like i think really maybe maybe the the thing that you should think of is is what you were saying that you know maybe you can think of this as m equals n and n squared polyparagno but yeah i agree that you know for like But yeah, I agree that for different instances, you could have an instance depending on the bound. And certainly, this is not sharp in general. Absolutely. Yeah, so let's say the most important case. So here, let's say the most important case m equals n and you know Okay, so let me let me say something about the proof of this. So, I'm going to go ahead and get a little bit of a little bit of a little bit of a little bit of a little bit of a colour. So let me say something about the proof. Some proofs catch. Okay, so in pretty much every proof of Spencer's theorem, I think including Spencer's proof. I think including Spencer's proof, like Spencer's inertial proof. So, like, one idea which is which is very useful here is what's called a partial covering. And let me not actually write down what I was, let me just say it. So, the idea is that: okay, here we want something which is, you know, sort of on the vertices of the cube, which has no discrepancy. But suppose you could find something. But suppose you could find something. Oh, so maybe I can. So suppose you could find x in the continuous cube with omega n minus coordinates and discrepancy. So let me even say pretty n equals size. I'll get the block file. any cosine. So I'll make you not that cosine of the time. I just map and see. Alright, so suppose you could find a point not on the vertices of the cube, but in the continuous cube with linearly many plus 1, minus 1 coordinates and discrepancy square at n, then you could just keep on iterating this procedure, right? Because let's say you have like 1% of the coordinates, now you would zoom into the remaining 99% of the coordinates. The remaining 99% of the coordinates, and so on, and so forth. And then, you know, you will get some geometric sum, which will still be squared down. So, is that good? Right? So, you could just sit with this, thank you. Then you have square root of 10, and then 0.99 times square root of 10, and so on. And the second thing in And the second thing, and this is certainly not what Spencer did, but this is kind of in the work of Rochfort Hessing, which is that you can actually do partial colouring using linear programming and the skipping. And the statement is the following. Okay, so let me actually define a convex polytope here. So let's say notation. Omega C is the set of points in the cube such that Right, so this is really in this form G2I drama is same, but also kind of G2R was very similar. So what they showed is that if you look at the following thing, right, so sample, sample the random Gaussian. Let's say this is just like a standard Gaussian in the dimensions. And if you look at so let's say X dot so why did you So, what you do, you take a random direction, and then you see how far you can go until you hit the body here, gamma C. So, what they show is that x star has omega and many coordinates, plus minus one coordinates. Because there is some statement here that exists a C such that this one is a hyperlink. That this one is like water. Right? So there is like a C parameter here, which controls how good the discrepancy can be. And the theorem is that there exists a C such that with high probability over the choice of the Gaussian, if you look at the point in grammar C, which is sort of like, you know, maximally in the direction of G, then it's going to have linearly many plus one, minus one coordinates, and then you can keep one to it. Minus one quartx, and then you can keep minus two different things because it gives the quasi colour. Okay. Any questions about this? So, so once you know this, then you're basically done with question colour, right? Okay. So, so what do we do? So so what do we do? Right, so yeah, so I don't know like how many people know how to solve a linear program. Certainly I don't really know how to do it like in a good way. But I would say that you know so the thing is what this is saying is something like if you look at the actual mathematics If you look at the actual maximizer of the linear program, then it has these good properties, right? And of course, you can do some very trivial argument, and you see that you don't really need the maximizer, you need something which is a maximizer up to polynomial relative error, and this is still where they have all the good problems. But the only way to solve a linear program to polynomial error is like a hub thing. So what we do is we actually still use this linear program. Still, use this linear program, but what we do is we prove some sort of stability version of this program, and we show that it's actually fine to solve this program to within logarithmic error, right, instead of like polynomial error. And now, you know, since you're trying to solve a linear program to within logarithmic error, you can try to use methods like gradient design. And those can be faster. I mean, that ends up being actually fairly non-trivial to use gradient design to a faster. You use grading design to the fastware here. But somehow that's the idea. First, sure, that you can actually do this in a, like, it's fine to solve this up to like a logarithmic error. And then, you know, you can try to do great. It's fine. Okay. So I think I have 10 minutes. And let me actually give you a proof showing that it's fine to solve this logarithmic error. And the benefit of that is that you will actually see like a nice proof of Spencer's theorem. Right? Because once we know how to do this thing, then at least you can do this. At least you can do this, and linear programming is what I should let me not say anything about that. Okay, so yeah, I mean, like, the good thing is that this sort of thing can actually do things more general than Spencer's theorem. I mean, there's like a version into Gernopoulos, and it gives you like an algorithmic version of this thing. But let me not say it too much. Okay, so let me sort of get, so okay, so the first thing is, what we show is that basically any is that basically any any any point x star any point of x such that x car g is bigger than max gx up to maxing one minus x Has oh my god, quite nice without singing value at least one minus c2 over one, right? And then, you know, the second thing is actually solved, actually. So. Okay, cool. Right, and I should say here I said something like you want to show that x squared has omega n coordinates which are plus 1 and minus 1. Here I'm saying Here I'm saying that I have omega and coordinates with absolute value bigger than 1 minus 1 over log n. But this is fine because if you can actually get points which are so close to 1, you can do randomized rounding, and then this is going to be fine. So it's not so much of an issue. Sorry, but the statement was, okay, repeat the statement, the C, what is the statement is that exists a C such that with high probability over the Such that with high probability over the choice of g, the maximizer of this linear program has omega n coordinates which are plus or minus one. And again, you know, this is like some point. So there exists a constant c and like big c and small c, right? So this is exactly the same sort of statement here. Except now it's saying that, you know, even if you only have sort of, even if you solve the program to within logarithmic error, it's good enough. So yeah, so this this this has quite a cute proof actually. Let me let me uh do it in five minutes. Okay. Uh okay, yeah, sorry, this was a diagnosis. Of diposition in the law. Right, so yeah, so we have this program, right, that we're trying to maximize. This objective that we're trying to maximize. Let me just normalize things by square 10. The norm of g is on the square. The norm of G is on the square ten. And now let me look at the expectation of this thing. Expectation of a G. Let me call this object C because there is a parameter C here. For those of you who know it, this is the mean width of the set, galva C, up to this factor of square gram. So you can ask, okay, you know, firstly, what does this look like? Typically, Look like, you know, typically, how does op C behave? And, you know, this is like sharply concentrated about the expectations when we can get some sense of this. Okay. So an easy upper bound is that opt C is less than the expectation of the one or no G, right? Because G X. Because gx is less than the one norm of g, right? This is just like Cauchy-Schwartz. And this is propile times. Okay. Okay, so this is fine. On the other hand, so now you can ask: okay, how good is this lower bound? So the claim is that. So the thing is that opt C, so there exists delta C such that delta C goes to 0 as C goes to infinity such that opt C is bigger than or equal to The claim is that this is actually a good opportunity. The reason for this is like some sort of super saturation version of Spencer's theorem. So I said that Spencer showed that there is like, you know, you can find one vertex of the cube with no discrepancy. But because he used this entropy method, what he actually shows is that you have many vertices of the cube with no discrepancy. And somehow if you combine that with Dauchy, And somehow, if you combine that with Gaussian concentration, you get a result of this form, right? So, I mean, it also sort of makes sense, right? Because C is getting bigger and bigger and bigger. And this polyglot is also getting bigger and bigger. So somehow we're capturing the photo. So we know that the optimal value is like basically root 2 over pi n, right? So morally, what that tells you. And we actually don't know how to prove this exact statement, but that's fine. Actually, it would be nice if somebody could prove this. So Mark, what this shows is that if you look at the match C to match D, then the derivative is going to zero, right? Sorry, because I'm just getting like closer and closer and closer to repi. Okay. So now let's sort of like think about for the sake of contradiction. So suppose, so now let's take C to be large enough, right? So now. So now for a large enough C suppose we had a point we had x star which is 1 minus C 1 over log n optimizer. With labeling the required with absolute value. Look at that. Right, so suppose, suppose, uh suppose I had like uh uh suppose my my near optimizer uh had like Had like very few points which were close to the vertices. So, what do I do in this case? So, I have my point x card, and then let's say s is the set of large coordinates. In absolute value and S complement is, you know, so So what I could do is I could take x star and I could add something which is supported only in s complement. So I could add to it some z which is supported only in s complement such that x dot is z. x to the plus z is still in minus 1, 1 to the n, right? So I can do this thing just by selecting it off. Because on S complement, you know, my coordinates are larger than the arrays, 159, like some small multiple. And moreover, so actually, if I look at this point here, So if I look at this click here, this also shows me that if I think about, okay, what's the best way to do this thing? If I think about max of text Let me just say what this is saying. What this thing is saying is that if I look at points not in the full continuous cube, but only at points which are supporting that. But only at points which are supported in S complement, then you know by doing some concentration and union bound statement, I can show that, okay, you know, like if I only remove a few coordinates in the cube, right, if I only remove a few coordinates in the cube, in the complement I can still get things for which the objective here has value at least square root of. Let's actually put a specific constant here, which is square root of negative 4. Okay. So now what I can do is I can take z to be a point which is coming from To be a point which is coming from here, right? But then x squared plus z is going to have value which is too much. So that's why this cannot happen. Okay. So the basic idea is that somehow going from gamma c to almost all of the coordinates does not decrease the value too much. But because the derivative of c to op c is going to 0, if I can actually add a vector, A vector with like a large support, then I can make this objective value to be large. So, this is why you cannot do this, and this sort of gives you like a stability version of this. Now, how you solve the linear program is like a separate thing, and yeah, I certainly don't have time for that. Minus two minutes. Minus two minutes, yeah. There were like a couple of questions I wanted to mention, but I also guess I don't have time for that either. But you know, if you have questions, too many. Two minutes? Yeah, okay. So let me say, you know, this is sort of the usual thing where you want to solve one problem, but you end up solving another problem. So here is like a very nice beautiful question. Let me just sort of quickly mention it. I bet there is the problem. So here is the problem. I really want to see. So there is a generalization of Spanish throne. This is called. This is called the Combus conjecture. This is called the Combus conjecture. But the statement is that if you have V1 through Vn which are unit vectors, Then there exists assignment. So these are in our sign such that epsilon will be one right so. Right, so if you take the incidence vectors in your set and you scale them by 1 over square root 1, then you get exactly this thing. So this is a conjecture. And the theorem here gives you like an ordered loop like that. And the sort of very interesting question here, and I can say a little bit more offline, is: so, okay, so this. Is, so okay, so this thing is also sort of known in a constructive way, right? Like, there are like many other things now defined signs like this. But sort of a very nice question is that suppose you had some adversary who picked these vectors for you, right, and also decided the order in which you're going to see these vectors. So this is called an object-based adversary in the sense that the adversary decides the instance once and for all, and then it's gone. And what happens is you see these vectors one by one. You see these vectors one by one, and as soon as you see the vector, you have to assign the sign to it. The question is: in such a model, can you actually get this root log n? And it's known with like log n instead of root log n. It's a very nice paper of Rani was Yangi and Matav Sani. And somehow I think, you know, uh of course, you know, if if if you do this in a correct way, this also gives you some inputs parts of your time algorithm. But yeah, that's that's that's that's somehow like, you know, how how That's somehow how I started thinking of that. And if you look at their paper, it's like this sort of self-balancing random walk, and it's a very sort of nice work, I guess. So anyway, I just want to add this question in case you're not anyone just. Oh, so let me add. One of the questions to the