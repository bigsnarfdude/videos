All this together. So, my talk comes more from the production side of things. So, I work on a code called Marvel, and Marvel is the next generation high-order multi-physics code at Livermore. So, Marvel builds on modular physics and computer science packages. So, there's things like the Axem library for computer science infrastructure, MPEM for high-order discretizations, and then Raj and Umpire are used to. And then Raja and Umpire are used to essentially put together a single source code that you can run on various platforms. And I'll dive more into this as I go into my talk. But one neat capability of this code here is its ability to do arbitrary Lagrangian Eulerian multimaterial radiation hydrodynamics using high order finite elements. And Marvel is a fairly busy code. It's an integrated physics code, meaning that it employs various external. Meaning that it employs various external physics packages. There's equation of state strength models. There's a variety of math libraries and tools for processing results. And as a production code, we're expected to perform at scale on the world's largest and fastest supercomputers. So a little bit about the L framework in Marble. So we follow a three-phase framework using higher order finite elements. At the beginning, we essentially solve a At the beginning, we essentially solve hydrodynamics conservation laws within the Lagrange phase. And since we're doing this within the Lagrangian frame of reference, the mesh is moving along with the simulation. But then at some point, it gets decided that the mesh is of poor quality. You know, we want to prevent tangling. So then we relash the mesh using a target matrix optimization paradigm or team op for short. So with this relaxed mesh, we then transfer the data from the initial mesh onto this relaxed mesh. So, this remap procedure is really what this talk is about. And in doing so, we want to make sure we're monotonic, we're not introducing new extrema, we're conservative, and we're accurate, right? So, we do remap by considering the advection equation, right? It's advection-based remap. So, introducing the concept of pseudo-time, you can consider a transfer function such that at time zero, you're at the end. Such that at time zero, you're at the initial mesh, but then at time one, you're at the relaxed mesh. And then you can track the movement of particles essentially in the pseudo-time. If you take the material derivative, you can use it to compute the flow field. And since we're propagating the solution without changing it with respect to the Eulerian frame, that partial derivative just goes to zero. So putting that together, the equations for the Lagrangian and REMAP phases look And remap phases look like this. So, in Lagrange phase, we're solving hydrodynamics conservation law. So, we have things like conservation momentum, material volume, mass, and energy. And then on the remap side, we essentially just cast the fields we're interested in transferring over into the action equation. So we'll transfer momentum, the indicator values, the product of the indicator values, and the product of the indicator values, and the energy. So, for discretizations, we use finite elements, in particular high-order finite elements. We use two discretizations. So, for the term in blue, that's just continuous Galurkin. And then for the rest in black, that's discontinuous Galurkin. And in our framework, both the mass and advection matrices depend on pseudo-time. So every time we do a right-hand side evaluation, we have to assemble new. We have to assemble new geometric factors or assemble the matrices, that sort of thing. So, for DG, for the phase terms, we're just using an upwind flux. So, we're taking the data from wherever it's being propagated in. And then putting it together, the semi-discrete form for the Lagrange and remap phases look like this. So, big caveat on the remap phase, right? So, for the equations in black, right? For the equations in black, right, I have these little stars because if you just apply a higher DG method onto it, we don't satisfy monotonicity requirements. So for that reason, we use the algebraic FCT DG method. So I'll describe this in a little bit. So the idea behind the algebraic FCT method is that we're going to use this method to maintain conservation. method to maintain conservation accuracy and not introduce new extrema. And the basic idea or recipe is that you're going to maintain monotonicity by appealing to the local extremum diminishing property, or we'll call it LED. The basic idea is you start with some initial bounds and then you keep your solution within those bounds. And then ingredients to make this work is that you start off with a low order DG scheme, which is guaranteed to be monotonic. You combine it with Bernstein basis functions. combine it with Bernstein basis functions, explicit RK schemes. And with this, you take your low order solution and then you apply, you introduce your high-order solution in a controlled manner to maintain that monotonicity and improve the accuracy as well. So being a production code, we care as much about the accuracy and the properties of our algorithms as much as the performance. Algorithms, as much as the performance of our algorithms, and how we can implement those on various programming models. So, like I said, we have to run on a variety of supercomputers. So, we have to run on CPU-based machines and GPU-based machines. So, we have to run on commodity-based hardware. Here, we'll just use MPI everywhere. Our code is C based. We've been running on Sira, which has NVIDIA GPUs, so we'll use CUDA. GPUs, so we'll use CUDA for there, and then for the upcoming L Capitan machine, we'll use AMD, we'll be targeting AMD GPUs. So we've done a lot of work on our code, and we've gotten some really good performance. So here on the left, we have an example of a 3D multimaterial shape charge problem running on 96 GPUs. And then here, a simplified ICF implosion benchmark problem. So this was run on 256 GPUs. And typically, we see about like a 15x performance speedup when you're doing like node-to-node comparisons. So, performance is very important for us. And with the introduction of these GPUs, we're facing, you know, we have to start rethinking the algorithms that we incorporate. What seems to work well is having algorithms with fairly regular axis patterns with massive parallelism. So, a lot of work from So, a lot of work from the Center for Efficient Exascale Discretization or CEED has demonstrated that you can get better performance by switching from full assembly to this partial assembly based or matrix-free approach. So on these plots, this is essentially the throughput or how many degrees of freedom per second I'm processing when you're doing like a conjugate gradient soft with the mass matrix. In fact, if you store things in this In fact, if you store things in a CSR format, your performance actually goes down as you increase the order. But it's the opposite case when you do things matrix-free, right? And of course, there's proper implementation that has to go along with this. So matrix-free is awesome, but it's difficult to do with the algebraic FCT DG method. Another note about these GPUs is that they typically have less memory than the CPU counterparts. So matrix-free. So, matrix free methods are again a natural candidate. And of course, you want high compute-to-communication ratio, right? You don't want to be bounded by MPI communication. So, high-order methods have this particular feature. So, if we can do high-order, that's also great for scaling. So, a key ingredient for performance in Marvel has been the adoption of these partial assembly-based algorithms. So, the idea behind partial assembly is that if you consider a finite element operator, Consider a finite element operator, you can decompose it into a sequence of operators with cascading scope, right? So these operators within the cascading scope is really what you want to assemble and you want to operate with. So you can have an operator that takes you from the global representation down to like a processor local, down to the element level, down to the quadrature point data. So basically the data sampled at the quadrature points. And that data is really the only thing. That data is really the only thing that's specific to each of the elements. So, there's various advantages to doing this. There's fewer operations for quad and hex elements. There's reduced memory storage and transfer cost. Since the remap algorithm has time-dependent mass and convection matrices, this is a huge saver in terms of storage and flops for assembly. However, you know, the disadvantage here is that it's not really suitable for the algebraic FCT. Suitable for the algebraic FCT DG method because part of the algorithm requires you to actually look at the values of the matrix. But one area where we can apply this approach is the momentum remap. So you can consider this particular equation here. And if you want to solve it, we can solve it. We can invert that mass matrix, say, using conjugate gradient. So we need to be able to apply the action of the mass matrix. Able to apply the action of the mass matrix fast. So, the way we do that through partial assembly is we go down to the element level. And if we write out the definition of the entries of the mass matrix for each element, we just start recognizing that the basis functions under quads and hexes are just tensor products of 1D versions. And if we did a little bit of algebra and rearranging, you can express the entries of the mass matrix as the product of. matrix as the product of these one dimensional operators. So whenever you apply the action of it, you essentially are just doing small tensor contractions. And this works extremely well for GPUs. And we actually also get performance improvements on the CPU as well when we switch to partial assembly. So with proper implementation, we've been able to demonstrate that high order methods perform better than low order methods. Than low-order methods, and especially on the GPU, right? And these are code, these are performance numbers on the actual production code. So we've done a lot of profiling to understand our code. And in gray here, we have the CPU, the performance on CPU-based systems. And even on CPU-based systems, going from linear to quadratic to cubic, we improve performance. And then huge performance jump when we go over GPUs from linear to quadratic cubic. For linear to quadratic cubic, we improve performance. So, higher throughput, a higher order. And then you can also improve performance on GPU platforms by increasing the problem size. So matrix-free algorithms are definitely a key enabler of this performance. And we've been able to carry out the Lagrange and mesh optimization portions of the code completely matrix-free through partial assembly. Free through partial assembly. And if we look at the throughput performance, higher is better here. You know, going from order one to two, we improve performance. And that's true for two to three as well for Lagrange and mesh optimization phases. But then for the remap, that actually ends up not being true for going to two or three, right? We actually go down in performance. And the reason for that is that the remap still has. The remap still has full assembly components, and we see that drop at order three. So, in our algorithm, we're assembling the DG convection plus trace sparse matrices, which can be slow, especially at high orders. And then, because we store things in CSR format, traversing the data, it leads to indirection access, which can be slow. So, you know, because of this, we recognize this, and because of these emerging architectures, we're These emerging architectures, we're rethinking the type of algorithms that we use, right? And we're really looking for something that's a balance of accuracy. You know, we want fast convergence, high accuracy with a reasonable number of degree of freedoms, robustness. We want the method to be stable and work for a variety of problems. And we want speed, right? And you can look at speed in terms of computational complexity, but we also want to look at how we're going to be able to We're going to be able to express the algorithm with parallel programming language, such as CUDA or HIP, right? How we express this in our code becomes increasingly important when we want performance. So what we've been doing is we've been looking at different algorithms using this Ramos Mini app. So the Ramos Mini app is available on GitHub. Anybody can just grab it. The idea is that this is a platform for exploring methods. Platform for exploring methods for marble. It's supposed to be a representation of what we use in the actual code. So there's things like support for high order curve meshes. There's explicit time integration. It's based on MPM. It's a quick to prototype. It's based on C ⁇. And I definitely want to recognize Henness Ajuk for the initial work. So the initial work he did essentially evolved into REML. Work he did essentially evolved into Ramos. So, Remos formulates the L remap problem as a blending of low-order and high-order DG solutions using Bernstein basis functions. This is, so it also includes the approach that's used in marble, which requires full matrix assembly. So, what we use in marble is, you know, our monotone low-order solution is constructed using discrete upwending, and then our blending strategy is algebraic FCT. Blending strategy is algebraic FCT, right? And the reference for this is this paper by Anderson and co-authors. So there's also a matrix-free approach that we started investigating and we're interested in using. And here for the low-order solution, we use the sub-cell residual distribution method. And then, so that's worked by Hajuk and co-authors. And then we're using the clip-scale blending strategy, right? And this is a Strategy, right? And this is also work by Manuel Quesada de Luna. For the high-order solution, that's just standard discontinuous Galorkin. We use the same approach for both. You can implement that matrix for you, no problem. So just a little bit of theory. So to main monetanicity, the methods in Remos and Marble use Bernstein basis functions and explicit strong stability preserves. Strong stability-preserving methods. So, Bernstein basis functions have some nice properties. They form a partition of unity. And then, like, you can also show that with the solution with an element can be bounded by the local degrees of freedom. But combining this with these Runga-Kutta methods, you can put together a method that will appeal to the local extreme diminishing principle, which essentially just says. Principle, which essentially just says if you have some initial bounds, we can keep your solution within those bounds. So, in terms of where those bounds come from, there's various ways to do that. And Remos has a couple of options for that. So, what's used in Marvel is this approach of zone plus never elements. So, essentially, this is the sincer on the left. This is the sincerity that we use to figure. Stencil that we use to figure out what the bounds are. And then another option that we have in Remlis, and now we have in Marble, is looking at overlap bounds. So pick a degree of freedom. And when you're doing your search, include all of the elements that touch your degree of freedom. So, you know, here we're interested in this black degree of freedom. We look at the elements highlighted in yellow. And of course, I want to point out there's additional stencils that have been explored in the work. Sensils that have been explored in the work of Manuel Casal Una in this paper here. So, just an overview of the different approaches. So, starting with the discrete upwind plus algebraic FCT. The starting point for this framework begins with the discrete upwinding for the low order solution. So, you can construct a monotone low-order solution by considering the discretization of your Discretization of your PDE and then lumping the mass matrix and the advection matrices. So once you do these lumpings, you end up with a monotone, a bounds preserving scheme. However, the drawbacks of this approach is that the accuracy goes down for increased orders, and this requires full matrix assembly. So we want to try to get away from this approach. And then the idea behind The idea behind algebraic FCT is to you start off with your low, you look at your low and your high order solution, and then you recognize that your high order solution can be expressed as the low order solution plus these two correction terms. So with these two correction terms, you can do a little bit of reformulating to express it in what they call the anti-symmetric flux form. And if you have it in this form, And if you have it in this form, then if you do symmetric scaling, you can maintain conservation. But then the idea of the scaling is that you want to scale the value. So you want to scale back the terms or set to zero any terms that can generate bounds violation, right? And then you choose your alpha values based on this kind of worst case scenario of looking at all the positive and negative fluxes coming together. Fluxes coming together, right? And this is where recognizing that the range of values is determined by the Bernstein coefficients can come into play. So for the matrix-free approach, this is different. We want to use the sub-cell residual distribution and clip scale methods. So the way the residual distribution method works is that you consider the upwind DG scheme. The up one DG scheme at the element level. And then the idea, the underlying idea here is to replace this blue term. So this is the volumetric integral of the convection term. And you want to replace it with a term that has the same total fluctuation. And this is going to be part of your ingredient to construct this LED scheme. So the basic recipe, and I'll defer for finer details and more theory to the paper by Haju. More theory to the paper by Hajuk is you take the sum of the integrals and then you split them into positive and negative values or contributions. And then you can come up with an approximation for the low order integral by taking this summation here, essentially. So if you take this approach, this can be a little difficult. This can be a little diffusive. So, what's recommended is to apply this procedure at the sub-cell level. So, consider a zone from your mesh, and then you can partition it into different sub-cells by constructing this Bezier net. And then you can perform this procedure at the sub-cell level. So, you end up with more compact stencils. And then, to transfer the data from the sub-cells to the zone, essentially it's multilinear interpolation. Linear interpolation. And again, I refer to this paper for further details. So, this is how you construct the lower solution to blend it with, oh, sorry, I forgot about the flux term. For the flux term, it's essentially just analogous to mass lumping, as you would do for the mass matrix. It's actually pretty straightforward. So, yeah, you just lump together the terms in the face. The terms in the face integral. And then you can put together a conservative LED scheme. And you would do mass slumping as well for the mass matrix that comes up. So that's how you would come up with your low order solution. Now, to do the blending using this clip scale approach, basic idea is that with clip scale, you consider first a candidate update, and your candidate update is going to be a combination of your high and low order solution. Lower solution. To prevent for overshoots or undershoots, you essentially just do a clipping where you pick the max of the lower solution, your candidate, and then the min of your max and your candidate solution. However, if you do this, yes, this gives you a solution that's within bounds, but then you don't necessarily get mass conservation. So what needs to happen is we also have to perform an additional scaling to preserve mass conservation. And currently, we do uniform limiting. And currently, we do uniform limiting. So this is already kind of a big difference than the algebraic FCT method, which these alpha values are chosen, are not necessarily uniform. So in order to make this work for marble, we have to extend this for products, fields, and multi-materials. Like I mentioned, when we do remap, we have to remap the indicator function. The indicator function, the product of the indicator function, and the product of the indicator function, density, and energy. And we follow the work of Anderson and co-authors in this paper, High Order Multimaterial L-hydrodynamics, where we do the remap procedure in a sequential fashion. So, first we'll remap the indicator, then the following products. So, if you want to remap a product of fields, call it XY. Call it xy. The goal is then to keep the product conservative, but then the bounds are going to be coming from y, right? So you want to then keep y within some bounds. And then you can also assume that x is non-negative. This allows for zones with empty material. And this procedure is applied to x being the indicator function and then the product being the indicator and density. And then you can do the same thing for the other variables. So the extension. So, the extension to multimaterial is straightforward as each material is treated independently of each other as well. So, like the algebraic FCT scheme, we've adapted that methodology to the ClipScale solver. And we do this in, we do the remapping in a sequential fashion. So, after evolving the indicator function, we set x to be the indicator and then the product to be indicator times density. And then, what we have to then And then, what we have to then do is build a conservative low-order monotone solution for the product, right? So, here's the catch: the catch is that if we apply the low-order scheme, say sub-cell residual distribution on the product, it may not necessarily guarantee satisfying the bounds for y. The bounds for y are just given on top. So, you just take the product and divide by x. In this case, it's zero if x is. It's zero if x is zero. So we do a bit of an adjustment and we construct a mass conservative update for the low order solution by taking essentially a combination of the updated x value doing like a mass average, if you will. And you can show that this is mass conservative and this is going to satisfy the bounds above in the work of Anderson and Of Anderson and co-authors. So, in moving towards a matrix tree framework for Marble, one of the things we've also done is that we've moved away from assembling matrices to compute the time step, right? So with this under the discrete upwind in algebraic FCT method, since we have the matrices, we can calculate a time step estimate for our remap procedure by looking at the mathematics. Or remap procedure by looking at the diagonals of the mass of the must lumped and upwinded advection matrix. So, with the introduction of the subset of residual distribution and clip-scale framework, what we've been doing is looking at selecting our time step based on computations from the low-order solution. And the basic idea is that we start off with an initial guess, we take a few remap steps to find the right time step. Steps to find the right time step estimate, and this is based on doing a small analysis of whether we're within bounds, right? But we also have to be careful for in regions where the solution isn't changing. And again, credit to Henness for pioneering this work. We've been using it and it works pretty well. So, with these two algorithms, we really want them to be performant on GPUs. And implementing this within our code requires being able to express our kernels with Raja, MFM, and umpire. And since we build our code from MPEM, MFEM is really there to build the foundation for BLAST. And it was required that MFEM was GPU-aware. So with MPEM 4.0, they introduced an intermediate layer. They introduced an intermediate layer between memory allocations and kernels. So now, through MPM, anything we run through MFM can be offloaded to a device. We can also use MFM's memory manager to move data between the different platforms as necessary. And then we've enhanced the memory manager with umpire memory pools to share device memory between packages. So in moving to this matrix-free framework, we actually used Ramos as our starting point, and we started sketching out what the kernel should look like. point and we started sketching out what the kernel should look like and making sure that we had a matrix-free kernels for the different parts of the algorithm. And then once we had that, we transferred things to Marble. So a little bit more about Raja and Umpire. So these tools here is essentially what enables us to write a single code that you can run on different platforms. So umpire is there for managing different memory resources. Different memory resources. One key capability that it has is you can construct these memory pools. So when different parts of the code are running, there's a pool that it can draw memory to quickly and from. And it can also release it really fast once it's done. And then Raja is the way we express our code within Marvel. So Raja is the way we write our, it's essentially our unifying language, if you will, where we can write. Of you will, where we can write our kernels in one way but then dispatch them with different programming models. So this can be sequentially CUDA with CUDA or with HIP. And of course, only C ⁇ 11 is required. So making GPUs work with umpire and mpem and really getting that performance really required a tight integration with umpire. So if you look at how mpem. So if you look at how MPM objects are designed, you have a memory object under each of the mpm objects. So under an array or a vector, you have a memory object which will point to host or device data. And then one of the enhancements we did is we created this, we added the capability of drawing memory from umpire allocators. So our umpire can set up a pool beforehand and we can draw. Beforehand, and we can draw memory to and from that pool fairly quickly. And this was very important for us in order to be able to do large-scale calculations. So again, to achieve good performance, you want to solve as big of a problem on a GPU as you can. So umpire really enabled us to do this. So on the right, we have an example of a year ago, essentially. Of a year ago, essentially, where we took a simulation, but we required 12 GPUs. By doing by moving how we're allocating memory around, we were able to run the same calculation on six GPUs. And over time, we've also done a lot of optimization, so our performance has been improving as well. Okay, so I really want to go over the results of the clip scale and The clip scale and the algebraic FCT method, and kind of describe where things are going well and where we really would like to figure out how to improve our existing framework. So one of the problems that we looked at is the 2D triple point problem. So these matrix-free methods perform really well on the CPU, right? So if we look at the runtime for the total remap for the map for the discrete upwind plus algebraic FCT and we compare it to subscale plus clip scale. We're getting this 3x performance in the loop where we're cycling through all the different materials and doing that DG remap where we're hitting close to five on the CPU. We do do more remap steps, but I think that's because maybe the time selection might be a little too conservative, leading to a high number of remap steps. But it's a clear performance win on this E. But it's a clear performance win on the CPU. In our code, we use a sub-cell residual distribution method. It's a little more costly than the non-sub-cell version, but if you go with the sub-residual distribution approach, you get a more diffusive low-order solution. So if we start comparing how the quality of the solution, we actually kind of see that around the swirl here, there are some different. The swirl here. There are some differences in the discrete upwind plus FCT approach. There tends to be a little more detail around the swirl here. I'm not sure if you guys can see my mouse, but around this particular region, there's definitely more detail compared to the sub-cell plus clip scale. The work on the GPU for getting the sub-cell plus clip scale framework is still work in progress. I've done a very modest port. There's still definitely a lot of room for optimizations, but we have it essentially running around the same runtime on the GPU. But we do have a nice performance improvement for that FCT loop where we're processing all the materials. The one thing that is missing from the GPU version of the sub-cell clip scale is setting up the sub-cell mesh on the GPU. Once that gets GPU. Once that gets implemented on the GPU, we should see a nice performance improvement. And then there's a lot of just smaller things that can be a performance bottleneck. For inverting the DG mass matrix, I actually just use LU factorization. This ends up being faster since you can just reuse the mass matrix for all the different materials. But I expect that with some careful tuning, we can get the GPU. We can get the GPU version of subcell to be 10 times faster than the CPU version, 10 times at least. So, a more difficult problem that we've been trying to make work is the 2D shape charge problem. So, here, this is where we see the biggest difference between the existing approach of discrete upwinding and FCT versus subsill versus with cliff scale. So, here I essentially zoom. Here, I essentially zoomed in where the jet is forming. And if you look here, you know, it's clearly much more diffusive with the sub-cell plus clip scale procedure, right? And we believe it might be because of the clip scale blending. I mean, we've done some tests where we look at just the lower solution, and the sub-cell is an improvement over discrete up wending. So, you know, we believe it's the clip-scale blending that might not be. Blending that might not be choosing the corrections optimally. We also think it's because when we're doing the limiting, it's at a per zone basis. So it's less localized compared to algebraic FCT. So, you know, this is clearly an area for improvements. And we're definitely going in the right direction in terms of performance. We just need to figure out exactly how we can improve. Need to figure out exactly how we can improve the quality of the solution here to be as good, if not better, than the Screed Upwind plus FCT framework. But even here, even for the CPUs, right, the new method, there's a clear speed of we're doing well. Again, the GPU port of this, there's still some work in progress. It's a work in progress, but I'm more than confident it'll get there. So just with that. So, just with that, I want to kind of conclude and point out some future research directions. We're definitely close to a matrix-free remap framework. You know, I think if the clip scale strategy can be enhanced a little bit, it could be a clear winner. But in general, there are questions. You know, this is why I'm very excited to give this presentation here. Can we apply like a sub-cell type of approach to the Klipscill blending method? But then, if we do that, how do we maintain conservation that the element? That? How do we maintain conservation at the element level? Are there different blending strategies we can try? I do want to point out that Ramos is a great platform for collaboration. It's a pretty easy code to get around with and be productive with. And with that, I conclude. Thank you, Anturo. Can you hear me? Yes. Thank you. Well, I have to say. Well, I have to say sorry because you are showing this computational resource and I don't even have my Wi-Fi working, so I cannot put my camera on. Sorry for that. I don't know if someone in the audience has questions. Yeah, yeah, I do have a question. Yeah, so in the beginning, just give me one second. In the beginning, just give me one second. Yeah, okay, sorry about that. So, in the beginning, you mentioned some comparison between using high-order spaces versus low-order spaces with respect to the performance. And basically, you said that it's when you run simulations in parallel, you observe an improvement in performance by using these harder spaces. This is something that I was aware about. This is something that I was aware about, but with more like more standard cluster, more standard clusters, like processor-based simulations. So I wonder if using GPUs, you've seen like a more dramatic or a less dramatic improvement versus when you use like standard processors. Thank you. I'm sorry, I'm not sure if I fully understand your question. Are you asking, like, do you get better performance when using GPUs versus CPUs? Better boost in performance from low to high order. Yeah, so there's a boost of performance from going to low order to high order, right? And there's a bigger boost going from CPU to GPU. I don't think I quite understand your question. Basically, I just meant that Basically I just meant that in these slides we can see that when you use high order space you basically take advantage of the performance of the supercomputer when you do the parallel runs. So basically I was just wondering if the improvement that you see when you use high order spaces is more dramatic or less dramatic when you use standard clusters than when than versus when you use GPUs. Oh, it's more dramatic. Oh, it's more dramatic. I understand. I understand. Good. That's a, I don't know. It would be like whatever the difference between this from here to here to here to here, right? I don't know if I have that quantity. I don't have that off the top of my head, but in general, as you go up, the performance goes up. But you know, implementation is a big part of this, right? Like maybe I can. Maybe I can chime in here. In our experience, Manuel, it is about two times better on GPUs than on multi-core CPUs. The benefit from going to higher order. But you do see significant benefit on CPUs too. Oh, no, it's interesting that even with GPUs, that with GPUs, it's even better than with CPUs. So yeah, I was just wondering about that. Just wondering about that. Thank you. I'll just add to that that that does require quite a bit of tuning, though. You have to do this very carefully to get that. Okay. So just to carry on with that, I guess, line of discussion, the can you hear me okay? I just connected my mic. Yes. So So, with this project, I mean, you're doing a lot of custom CUDA programming, and you know, and it does require tuning. I just quickly was looking at the seed web page, and I noticed there was a mention of co-design. And I'm just curious as to sort of are you guys in contact with chip makers? Are we talking about eventual designs? Of accelerators that are even better than this? And then, two, I guess, in terms of getting students involved with this type of development, how much of these frameworks have good entry points for researchers without having to develop an expertise in optimization on the latest and greatest CUDA or NVIDIA projects. Greatest CUDA for NVIDIA processors. You know, I think Remmos is a great starting point for both methods and performance tuning. A lot of the initial work for this project here started with Remos. We explored what was available, how to write the code, and then we transferred it to the Bitcode. And then, in terms of talking to people. Talking to people in terms of talking to the vendor as well, you know, we do we routinely provide feedback. So when, yeah, we routinely provide feedback to NVIDIA or AMD and we say, hey, these are the properties of our kernels. You know, how do we make them fast? You know, how much we influence, well, it's hard to say, but our experience has been very positive. They always work with us. Been very positive. They always work with us and they help us make our code better. And I'm sure they relay that information to the compiler team. And there's considerations for future design, but it's mostly through collaboration feedback. And mostly on the compiler side, I guess. It seems like AI is now driving the train, you know. Yeah. And so that's now we start to. Now we start to optimize for whatever they produce for the AI community. Well, I'll add that we do have some in common with the HPC community and the AI community. Where we differ is in the precision requirements. So as long as they continue to make double precision units, we can actually benefit from a lot of the other optimizations. And maybe just to add a little bit to that regarding C. Regarding seed, we are absolutely talking to the vendors and AMD in particular with the provider for two of the access scale machines in the US. They've been extremely, I'd say, helpful and engaged with us. And the mentioning of tensor cores, that's one example where how the methods are really good because if you can fit the local dense tensor contractions that you need. Local dense tensor contractions that you need on the on the tensor course. That's exactly, you know, in double precision, of course. That's those are the operations that are optimized for convolutional neural networks, for example. So we just take advantage of that. And regarding entry points, that's what we are trying to do with MFAM. We're trying to provide a tool that's both useful for research and production, and building mini-apps like Kramos and Lagos is the way we try to engage the community. Great, thank you. Is there any other question? If not, we maybe can introduce the next speaker. Thank you. Thank you, Attora. So the next speaker is The next speaker is Chris Nies from the Luciana State University. So, if you want to share your screen. Can you hear me, Chris? I can hear you. Sorry. Hear me, Chris? I can hear you. Sorry, I'm just slow with my adjustments. Can you see it now? Because I'm having so many problems with the connection, but I don't know if. Oh, that's right. You can't see. Sorry. So, well, this is going to present one percent of the discretization.