So, this will be also another talk on the MaxLinear model, in particular on detecting MaxLinear structural equation models in extremes. And this is a joint work with Anthony Davison and Claudia Klupperberg. And so, this is an overview of the talk. In particular, I'll start with some motivation on Max linear structural equation models. And then I'll add in a touch of regular variation to the problem, and then I'll say something about our findings on detecting these. Something about our findings on detecting these models. So, the idea here is that the main goal is that of modeling the joint tail behavior of high-dimensional extremes in the context of causal dependence. And the natural framework for studying causal dependence is that of graphical models, and more particularly that of directed acyclic graphs. And I don't have here for you an example with the climate data, but one with food data, which I suppose is not that much. Food data, which I suppose is not that much related to climate, but in any case. So these are food nutrient intake data from the NHANES survey. And applying our methodology, we came up with this deck. So here we have alpha-carotene, beta-carotene, and vitamin A, and another component. And here, right beside we have these bivariate dependence between two of these components. So these are all standardized observations. So, these are all standardized observations beyond a certain radial threshold. And this is the simulated data from the fitting model. And we see that here, the Max Linear model actually captures the dependent structure quite nicely, apart from the fact that there seems to be a bit of noise in here. Okay, but so why max linear Bayesian networks? So, the idea is that the max linearity captures the common experience that the largest shock effects are the ones to influence. Largest shock effects are the ones to influence the nodes in the network. And this is done via this winner-takes-it-all mechanism. But again, the problem is that I suppose many of you don't like the Max Thinner models because of the discrete spectral measure, and which leads to this ray-like behavior in extremes that we can see in this plot here. But again, the spectral measure is discrete only in the limit. And because in the real world, we have only a finite number of observations, one often assumes. Number of observations, one often assumes, one often deals only with extremes and penultimate settings rather than the limit. And this allows for the presence of the noise in the model, as long as things in the limit do not change, which allows us also to use the theory developed specifically for the Max linear model. And this is one example from the previous simulated from the previously fitted model that I showed you. The previously fitted model that I showed you, but instead of a regularly varying index alpha equal to three, here I have alpha equal to three, so the tail is slightly lighter. And this is without noise. And here we have additive noise of 0.5 times T10 distribution. And here we see that actually it becomes quite difficult to see this ray-like behavior. And here one can even think that instead of having a discrete spectral measure, maybe... Of having a discrete spectral measure, maybe there is a density which can also model this dependent structure. Okay, so now moving on to graphical models. Okay, a directed graph is an ordered pair of a set of nodes and a set of directed edges. And the idea is that of identifying nodes with random variables from a joint probability distribution and the edges with dependencies. And this is usually done via structural equation models. Equation models where one writes each of these variables as a measurable function of the parent nodes in addition to some exogenous innovation term. And again, the underlying graph structure is directed and acyclic and also called the Bayesian network, like Claudia mentioned before. And the ordering here is a bit different from the one used by Claudia because we say that i is less than j for all j which are parents of i. So here we go in a decreasing instead of increasing order. Increasing instead of increasing order. But again, the problem here is not why to use the max linear model, but rather when to use it. And so when using graphical models and directed acyclic graphs, one often assumes the presence of all node variables. And I think Limbo also mentioned this on the first day. This also goes along with this no-unmeasured confounding assumption, which in a way makes it. Which, in a way, makes it unrealistic as long as one cannot really verify it. And this is a small three-dimensional example where we assume that we observe only x1 and x2 from this structural equation model with this deck. And in reality, we don't really know whether node 3 is present or not. But in case it is present, but hidden, then it may be incorrect to express the relation between x1 and x2 via this structural equation model because we will. this structural equation model because we would have x1 as a function of z1 x2 and x3 and x2 as a function of z2 and x3 and okay z1 and z2 are both exogenous innovation terms but not x3 so the question now is whether we can find whether it's possible to actually write this simply in terms of as one and x2 under specific models and the right model for that is the max linear model Model for that is the MaxLinear model. And okay, defined the following way due to Gie Siebel and Kluperberg. So the idea is that again that one starts recursively by writing every node, every node variable as a max linear combination of the parent nodes in addition to the exogenous variable terms. And this is a three-dimensional example when one starts building the model recursively, starting with the parent nodes and then going on with the children. And now, because we want to make all And now, because we want to make also some distributional assumptions on the innovation terms, we are also interested in dealing with specifically by expanding this representation in terms of innovations. And again, this is done recursively in this fashion. And again, analogously to linear structural equation models, one can also write, represent x by applying this max linear matrix A to the vector of innovations by using this. Of innovations by using this times max operator defined similar to matrix multiplication, but by replacing the summation term with this maximum. And then one gets this. So for instance, for this three-dimensional example, one gets this max linear coefficient matrix. And what is interesting here is, like Claudia mentioned, this A only carries information about the max weighted paths which are present. So for instance, in case I would have that from this data. In case I would have that from this DAG here, that the path is the path from three to one moving along node two is max weighted, then I could essentially disregard this edge between three and one, because the only visible information would essentially be given by the second, the entry to the right-hand side of the maximum. And so, certain, so the idea is that this matrix A carries only certain information. This matrix A carries only certain information. And again, this is generalized in this theorem, which tells us that all the entries of this max linear matrix are essentially taken such that are defined to be the maximum over all path weights for all paths PJI from node I to node J to node I and the path PJI such that Aij is equal to these coefficients dij is also called the max weighted path. Is also called the max weighted path. And yeah, then we can also write x in this way. And also important for us because we will make some standardization assumption on due to regular variation is this standardized Max linear coefficient matrix defined the following way by essentially normalizing the raw norms of A. And I'm sorry. Excuse me. Sorry, one. Sorry, one minute. One is four minutes. Okay, so now about multivariate regular variation. So we let x be regularly varying, denoted by this. And then we know that there exists a finite measure h of x on the positive unit sphere for any choice of the norm. And a function vn going to infinity, such that whenever we consider the polar decomposition of the vector x, then we have the this. X, then we have this joint vague convergence result with the limiting product measure being the cross product of mu A, which is mu alpha, which is the measure which controls the magnitude of events. And this measure h of x, which controls the size, the share that each margin contributes to this event. And we call this h of x. And we call this h of x, this measure also the spectral measure. And in case that is, okay, in case of the for the Max Linear model, if we let our innovations be regularly varying, then this spectral measure has this explicit form with support given by these normalized columns of this max linear coefficient matrix of A. And okay, so some more ingredients that we're going to need. So, some more ingredients that we're going to need are scalings. So, these were, or more specifically, cross-scalings, which were first defined by Coulee and Thibault in this paper from Biometrica. And so, this allows us to study multivariate extremes in a covariance-like fashion, analogous to linear models, as one can also see from this expression here. But again, there is quite a loss of information. Quite a loss of information by simply looking at these structures. And so we also need to take something else given by these maximas over specific tuples over this subset H. And well, one can also consider these to be max projections. And the idea is that if X is max linear, then whenever we take this maximum and we compute the scaling of this maximum, the squared scaling, then this maximum will essentially translate inside. Essentially translates inside these columns of these max linear coefficient matrix A. So this allows us to hide or reveal specific information carried inside this matrix. And again, we assume standardized scalings, which would correspond to A being to this A bar matrix. And okay, so one of our main findings is that of identifying source. Is that of identifying source nodes? And the idea, as I said before, here, we don't assume that what we see is the full recursive max linear model, but rather we assume that for some given little d in the set, we observe only a specific subset from a arbitrarily ordered recursive max linear model. And now, if this is satisfied, that is, there exists some node L such that for all pair. For all pair I L, where I is an observer observed descendant of L, and for all common hidden ancestors of node L and I, there is a max weighted path from U to I passing through L. In addition, L has no ancestors in O and no common ancestors so it has no common observed ancestors and in addition it has no common ancestors with any nodes which are not observed descendants. not observed descendants. Then we can denote this set by V0 and then we say that this V0 is a set of observed is a set of source nodes in this reduced recursive max linear model consisting of only of this observed set. And okay, just to show this with these examples, so here we have that d is equal to 8 and little d is equal to 2. So we observe only node 2 and node 1. And the rest of the nodes are hidden to us. So we would like to see whether it's So, we would like to see whether it's possible to model this via 211 via a bivariate DAG or bivariate Max Linear structural equation model. And now under this condition that we have here in I, then we would have that all paths, so we know that all nodes from three up to eight are common ancestors of two to one, and therefore if all If all paths from eight to one, from these hidden nodes from going to one are max weighted paths, that is those going from node two, then it would actually be possible to write, to express this small graph consisting of nodes two to one as a two-dimensional recursive Maxina model. And therefore, we can disregard all. And therefore, we can disregard all information carried from the hidden nodes. Now, in order to provide some verifiable conditions, then we also need to, under the assumption of regular variation, we're going to need this extended version of Breiman's lemma due to Basak et al., which in particular tells us that if we apply a linear transformation to this Linear transformation to this regularly varying vector x, then this will be reflected inside the spectral measure. And in particular, if x is max linear, then the new atoms of the spectral measure will essentially be given by the normalized columns of the matrix A S applied to A. So the idea here is to find linear transformation, to apply linear transformation to these bivariate vectors. Transformation to these bivariate vector xi and xm so that they disproportionately affect the columns of these max linear coefficient matrix. And this will happen only if the max weighted path condition does not hold, which is an interesting result per se. And for that, and in order to keep things positive, we also define this maximum over the rescaled bivariate vector. And we know that then That then, the way that these linear transformations are defined is by essentially by using a linear combination of xi and xm in addition to both this rescaled maximum and the original maximum. And then we also know that this m minus vector is regularly varying with the same regular variation index. And in addition, we're also going to need that this standardized version of this m- vector, which we M minus vector, which we denote by this tilde. Now, one condition which we found in this paper, which Claudia mentioned before, is for node M in this case to be a source node to node I is given by the following. But again, this is under the assumption that the DAG consists specifically of node I and node M. And so, in a way, this is not sufficient to disregard all. Way this is not sufficient to disregard all the remaining nodes as I showed before in this DAG with eight nodes. But then we have this proposition which tells us that if again if I let X be regularly varying with arbitrarily with a Max Linux coefficient matrix A, and if I compute this M tilde, as I showed in the slide before, under the assumption that this Under the assumption that this condition C holds, then we have that max weighted path from the common ancestors of both I and M to I pass through node M if and only if this equality is observed. And in case they only have common ancestors but no causal link, then we have that this is strictly less than one. And so the idea here is going back to the example that I showed you before, then this tells us that if If in case if I let my m be equal to two and I select i be equal to one, then if I compute this m1 and m2 tilde, and I compute this squared cross-scale in between the two, whenever this equals one, then I can essentially disregard all edges going from these hidden ancestors to node one. And the idea is again, because these the max linear. the the max linear coefficient matrix A carries only information for the provided by the via the max weighted path. And why is this nice? It's nice because under regular variation, because we know that undertaking the regularly varying distributions are close undertaking the maximum. So we can always write this as merge all these innovation terms into a single term. turns into a single term. So say Z two star. And then this simply becomes a simple two-dimensional structural max linear max linear structural equation model. And there's also an equivalent criteria which is more interesting theoretically but which in practice doesn't work so nicely which I'll skip but maybe I can if there are questions I can tell something about it later. Tell something about it later. And about the estimation, so here we for finding the scalings over these maxima, here we use simply the empirical spectral measure. But whenever we want to compute this, we want to apply this linear transformation to the vectors x, what we do is first we use an intermediate threshold. So we only select those observations L which are for which the radial For which the radius is larger than a specific threshold. And the reason we do so is because observations with a high threshold are more likely to provide a better approximation to the spectral measure. And once we standardize this vector M minus and we compute the polar representation, then again we compute this cross-scaling between m1 minus tilde and m2 minus tilde. Minus tilde and m2 minus tilde in this way again by using the empirical spectral measure, but you end up with a random number of observations. And but again, we're also able to prove under appropriate technical assumptions on this k1, k2, and n that these consistency results and also asymptotic normality. And so we also conduct simulation study over 50 randomly generated. Over 50 randomly generated directed acyclic graphs over different number of nodes, sample sizes, sparsity levels, that is, probability of having edge connections, and regular variation index. And here we will just provide some reports for these D equal to 40 and then equal to 100. So we assume only two observable nodes, I and M at a time, and we check whether these max weighted parts. The this max weighted path property is satisfied. That is this equality of the cross-scaling equaling one. And because the discrete dependence measure makes the max linear model difficult to use in practice, we also add noise to the model. So additive noise. So the idea is that we let that be in the domain for attraction and not max table. So we take ZI to be the absolute value of a t-distributed random variable with. Distributed random variable with alpha equal to two or three, and then depending on the alpha, we select these noise terms to be 0.5 times T5 or 0.5 times the T10 random variable. And then the margins are standardized empirically to Foucher 2. And here are some of the results. So these are all the first entries, the positive rate over all. Entry is the positive rate over all the true positives, and the next two columns, so to speak, are positive rates but over the false positives. And the remaining columns are essentially false discovery rates. But here, the idea is that we want to distinguish between cases when there is a causal link but no max rated path and the case when there is only a confounder relation but no causal link between the two. Causaling between the two. And as we can see here, because we also add the noise to the model, it's in some cases it's slightly difficult. It's more difficult to distinguish between the case when there is a causal link between the two nodes. But again, we see that it is able to differentiate between the three cases. And in particular, here, for instance, where the sparsity level is equal to 0.2, we have the ratio of. We have the ratio of nodes with causal links, but no max weighted path can be three to two or four to two. And still, it is able to discriminate or distinguish between the two categories. And more so, it is able to penalize cases when there is a confounder-only relation between the two nodes. And also, in terms of false discovery rates, we see. Also in terms of false discovery rates, we see that it also does a somewhat nice job in terms of specifically in terms of these the cases when there is a common confounder but no causal link and okay it does quite well in terms of never almost never detecting the wrong ancestral relations and okay and this is a the food data example that I Food data example that I started with at the beginning, but here, rather than studying only those four components, which I must say we were lucky to find in the first paper, here we study all 39 components. And we are interested in checking those pairs which satisfy this max-weighted path property to see whether two-dimensional recursive max linear models are feasible. And the way we come up with this matrix is after applying some additional conditions. Conditions. We estimate this sigma, this cross-scaling between the standardized m minus vector. And then based on a thresholding procedure, then we only select set to one only those entries which are greater to, I think, 0.93. That is a value which is close to one. And so interesting to see is that we also have these three components here, which are from the DAG which I showed you. Here, which are from the DAG which I showed you at the beginning. So, in a way, maybe we would have been able to come up with that DAG also from using this. Okay, so to summarize, we consider regularly varying recursive Max Lina models. And based on a scaling technique, we provide new criteria for detecting whether the observed extremes can be modeled via a recursive Max Lina model. And the empirical estimates are consistent and asymptotically normal. And I mean, there are also criterion for essentially building the whole DAG, not simply the source nodes, but those are a bit more complicated and I don't really have the time to talk about it. But thank you for your attention. And these are some references. Okay, thanks very much, Mario.