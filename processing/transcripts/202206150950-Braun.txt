Is it perfect? Perfect. Yeah. Okay. Thank you. Yeah. So thanks, Chris O'Maris, for having me here, giving a talk, and thanks for organizing. And so, yes, I'll be talking about the stochastic emulation of quantum algorithms. And so, yes, okay. Now, if you compare classical mechanics with quantum mechanics, there are a few. With quantum mechanics, there are a few fundamental differences that make that in the end simulating quantum mechanics is quite difficult. So, maybe the most fundamental difference, if we talk about probability theories, that is that in classical mechanics, we just have access to some probability distribution p, whereas in quantum mechanics, we have access to a quantum state psi. And often it is quoted the problem of the dimension of the Hilbert space. The problem of the dimension of the Hibalt space, if you're in particles, well, the Hubble space dimension grows exponentially. And of course, this makes it very difficult to even represent the state. But the same is actually true in classical mechanics, as long as we talk about stochastic simulations and really propagate just a probability vector. Also, that probability vector lives in an exponentially large Hilbert space with exactly the same dimension. In fact, if we talk, for example, about bits or qubits, it's just that the Hilbert space is zero. Just that the Hilbert spaces here are our real numbers, and here it's our complex numbers. So, maybe this is not the fundamental difficulty that is involved here in simulating quantum mechanics. But then there is another property of psi which is really absent for P, which is just the possibility of interference. Interference happens, of course, also on the level of single particle quantum states, but it is really used very profitably in quantum computers for In quantum computers, for many particle states. So we have many particle interference in quantum mechanics, and this is just completely absent for a probability vector. And of course, for the quantum computer, it's also essential that psi represents some physical reality. What exactly it is is not quite clear, but it's clear that psi will steer actually a probability distribution through the bond rule. And of course, you might think of many different And of course, you might think of many different objects mathematically that have similar properties like this psi, but they don't have necessarily physical reality. Now, of course, the translation from quantum mechanics to our classical world where we have measurement results and probability distributions is through this bond rule. So we just take p is absolute value of psi square. Or the other way around, we can say that psi is the square root of p and then we add some phase factor. P and then we add some phase factor. Now, the question I would like to ask, and to some extent, answer today: is there another object other than the quantum mechanical psi that shares the same essential properties? So psi should again be a vector in some large-dimensional hybrid space, it's a tensor product structure, it should allow for interference and it should represent the sort of physical reality. And well, the answer for short is yes, there is. Shot is yes, there is. And so, in fact, in this talk, I will be speaking about a new type of wave function, you might say, which is just defined as the nth partial derivative of a probability distribution. So it's not p itself, but the nth derivative with respect to all its argument that we want to consider in the following. And so, I will often abbreviate this just as the nth derivative of p here. Now, at this point, you might object immediately and say, yeah. At this point, you might object immediately and say, Yeah, but okay, psi is known, it must be complex, or quantum mechanics needs complex wave functions. And the short answer to this is yes, I mean, this psi that we define here will always be real, but we can nevertheless cover the full quantum mechanics by just splitting psi into real and imaginary parts. Okay, sorry. And so, to do this, it's quite well known, we just have to add an additional qubit in the case of quantum computers. The case of quantum computers, a qubit that will distinguish between the real and imaginary part. So I will call this the ray in qubit, and then we can actually also cover complex quantum mechanical wave functions. So let's look at the simplest case first. So n equals one. And so we just have a probability distribution on the real line, maybe an interval ab on the real line. And so p is a probability distribution on that line. And it's And it's in the vector space of the once integrable functions, and so, of course, ψ of x is then element of a vector space of twice integrable functions on that interval. And in the following, we'll actually discretize this derivative. So we approximate the derivative with just the difference quotient x plus delta x here and just x here divided by delta x, possibly at different positions, xi. And then I will just set delta x equal to. And then I will just set delta x equal to 1 and look at i equals 0 or 1, define d of xi equals p i for short. And so this leads to this object here. It's a vector of two entities, just defined as p0 minus p1 or p3 minus p2. Actually, switch the sign here, but it's irrelevant. And so more compactly, we can just write psi i as this combination of probabilities. probabilities. So 2i plus sigma takes on four different values. i is 0, 1, sigma is 0, 1. And then there is just this minus sign that depends on sigma. And so this object will be called a gradient bit or grab it for short. Okay, so you have one grab it and it's a state vector that lives in R2. So it's not C2 like the typical quantum bit, but it's R2. But it's clear that the components of psi. That the components of psi, that is psi 0, ψ 1, they can be positive or negative, and hence allow for interference, the most basic form of interference, in particular destructive interference. And this is illustrated directly with this simple equation. We would like to have that the state 0 plus the state minus 0, of course, gives just 0. And of course, this works. So I represent this state 0 here with a probability distribution over these four values. Distribution over these four values one zero zero zero and this minus zero will be represented by zero one zero zero and now if we take the convex combination of these two probability distributions well we just get this thing here and obviously the difference between these two guys and these two guys down here is is always zero by the way do you see my my mouse pointer yes yeah okay good Okay, good. All right, so we learn here that convex combinations of the probability distributions lead to interference of this new gradient wave function. For the following, let's introduce the following notation. So there is this capital I here, which is an index which for a single graph it takes on the values 0, 1, 2, 3. So we call this the byte 4 values for the four values that it will take on. I is the byte logical value, so just 0, 1. Values, so just 0 or 1, abbreviated E L V and sigma we call the gradient value, also elements 0 and 1. Now, in quantum mechanics, we know there is a certain gauge degree of freedom. So the states are actually not states in the Hughel space, really, but in a projective Hugo space, meaning that you identify all states that are multiplied with some complex prefector. You might argue is it just a phase, or it's really any CLM and C. I think it's not so relevant. We can always normalize. So relevant, we can always normalize the state and then it's just the phase, but we don't necessarily have to do so. So we'll just say that all states that are equivalent up to a global complex factors are equivalent. And for the gravitas, this means that only the ratio between psi zero and psi one is relevant. So here I assume psi one different from zero. If not, you take the other way around. And this motivates a parametrization of this probability vector of this form here. Of this form here. So you have a constant vector here, which is equally distributed in it's just zero for zero and one, and then one half for two and three. Then you have this values r, which are also real numbers, and this addition here. And all the information is really just in this last part with q and minus q and one and minus one. So it gives the correct ratio q equals psi zero of psi one and the prefect of b here will actually. And the prefactor B here will actually determine the amplitude or really of psi zero and psi one. Now the r here is linked to what one might call the physical probabilities, meaning marginalized over the gradient value. So p0 tilde here is p0 plus 1, then it's just little r, and p1 tilde is 1 minus r then. And so these would be the probabilities for finding 0 or 1 independently of this gradient value sigma. Now, as I said, Now, as I said, this B determines the amplitude of psi 0 and psi 1, so we would like to maximize this. And if you do so, well, then you find that p has this form here, q 0, 1, 0, and normalize. And so now you see that actually these physical probabilities, pi tilde, just become the absolute value of psi i. And I will call this equation the Bond 1 rule. Obviously, in quantum mechanics, you would have a square here. Obviously, in quantum mechanics, we would have a square here. We would have pi tilde equals psi absolute value square. Interestingly enough, Born actually advocated at the very beginning this Bonn one rule, as I call it here. So there are two motivations for calling this the Bon 1 rule first because it's the first one proposed by Bonn after switching to the normal Bon rule. And the other reason is, of course, that the power is equal to one and not two. But this in But this in particular makes once more evident that the gradient wave function that we define here really has a clear physical interpretation. So if you make this gauge choice here, we really have directly a translation of the amplitudes of this gradient wave function by their absolute value. They give here the physical probabilities Pi tilde. So here is a short graphical representation of an example. So there is this gradient value here by logical value 0, 1. Value 0, 1, and then the I, capital I here, by fall value 0, 1, 2, 3. And for example, if you want to represent this state here, well, we can do it with this probability distribution, which I also write often in this vector notation. So these double kets here mean always a probability distribution. So there are four real numbers between 0 and 1, P0, P1, and P2. P3 is 1 minus the sum of those, and then we get a tetrahedron. And then we get a tetrahedron. That's the state space of our gravit. And so if p0 equals to 1, well, it just gets 0. If p1 equals 1, then get minus 0, and so on. And of course, now again, by convex combinations, moving on in this line, for example, we get 0 minus 1 with varying pre-factors, or along this line, we get 0 plus 1 with arbitrary relative weights. Okay, so this was the simplest case, and equals one. Now, of course, we want to move on to n gravitas representing n qubits. And so it's very simply generalized. So all the indices become vectors, so I1 up to In. Again, each little i is 0, 1, and the same is true for each little sigma i. And then we have again these combined indices, which are to be understood in vector notation. So you just add for each component the 2i and the sigma i. The 2i and the sigma i. And the phase that really determines the overall sign and hence the interference possibility is given by the sum of the sigma i's here. So minus 1 to the sum sigma i's. So with this, we can represent, for example, Bell states very easily. So psi plus is this Bell state here, and it directly translates to a probability distribution like this, 0, 0 plus 2, 2. If we change the sign here, well, then The sign here, well, then we can have the choice. Also, when mechanics put the minus sign in the first state or in the second one, and depending on that order, we then also get two different probability distributions with the 3, 2 or 2, 3. Now, again, we can check the two-particle interference. So the sum of these two guys obviously just gives 0, 0 in quantum mechanics. And this translates directly to the convex combination again, where you see that the sum of these two. See that the sum of these two probably distributions will give zero zero. That's fine, and then the sum of those two guys here will give a zero zero one one. And so the state corresponding to that part just gives zero. So in the end, you can have complete destructive interference and get the same result as in quantum mechanics. All right, so why did we take the ends derivative? This is a question you might ask, and in effect, it's to natural. Ask and in effect, it's to naturally represent product states. So, if you have a quantum mechanical product state like this, then well this means that we represent that state with an uncorrelated probability distribution. So if you take this probability distribution and calculate the nth partial derivative, you can pull in all these derivatives to the corresponding factor, and you just get the product of this single particle gradient wave functions, and so you've got. Functions and so you've got the product state that you would like to represent. If you want to represent any pure state, well, it's a superposition of such computational basis states. And here I take C sigma and R. As I said, if they are complex, we can always extend the system by one qubit and then have again a real C sigma. And so this can be represented, for example, by this probability distribution. Again, there is. Probability distribution. Again, there is the freedom of putting the sign in whatever factor you want. I put in the first factor, well, I just get here the exponent value of the C sigmas. And then in each component, there is two sigma i, but in the one with the sine, we have to add one plus sine of c sigma over two. So if the sine is plus, this means, well, we get one plus one divided by two. So we shift by one. If it's minus one, then it just goes away and we get actually. We get actually up to the total sign, we get the same status in quantum mechanics. So you can really represent any pure multi-qubit state as a multi-gravit state. That's great. Now, we would like to run quantum algorithms with this, emulate the quantum algorithms. And so the idea is that the unitary gates that we use for propagating our quantum mechanical state correspond to stochastic matrices that propagate our probability distributions. Our probability distributions. So we'll call these stochastic matrices S. And so they should be chosen such that the corresponding map of the gradient wave function is the one that we get from quantum mechanics. Again, possibly up to a different prefector here. And yeah, normalization. So for example, an X gate, well, it exchanges 0 and 1. So it also exchanges minus 0 with minus 1. And this amounts just to a simple permutation of our byte forward. Permutation of our byte 4 values. So 0 will be exchanged with 2 and 1 will be exchanged with 3. So it's this simple permutation matrix that implements for us the x-gate. Here it's defined really for the computational basis states, but because of linearity also of a stochastic propagation, this works for all probability distributions. The Haramar gate is slightly more evolved. And in fact, that's a gate where you really need a stochastic process. It's not just a pendulum. Stochastic process. It's not just a permutation. So we know that zero should become a plastic, so zero plus one up to normalization. And so this again amounts to a convex combination of two probability vectors. And for our byte four values, it means we should choose with probability one half to stay with a zero if you start with zero, but with probability one half we should flip to byte four value two. And correspondingly for other byte four values, they will always be flipped with a probability one half. They will always be flipped with the probability one-half, either stay or go to another value that flips the sign, or in case it leads to the correct combination of zero and one with the corresponding sign. So this leads to this stochastic matrix. You see, it's always two different possibilities that come out here with probability one-half. Now, if you want to represent a more general phase scale, let's say, so this one with one and equal to the i phi, well, this is now really complex. I phi. Well, this is now really complex 8, so we have to do this what I call realification, meaning to represent also the u with an additional qubit. So this took quantum mechanics here. We can always do this. It means that each complex matrix element uij in the original matrix has to be replaced by a 2 by 2 matrix, where this additional index new here really tells you whether you have the real or imaginary part. And then this becomes the real part of UIJ. becomes the real part of uij on the diagonal and plus or minus the imaginary part of uij on the off diagonals. So this u phi here translates into this u phi tilde, which now acts on two qubits. So there's identity up here, two-dimensional Hilbert space, and this rotation matrix down there. And so I would call this the controlled R phi gate working on this verified quantum mechanical state. Mechanical state. And so just this R gate is now again represented by a stochastic matrix where the ratio between this cosine and sine phi will enter. And so yes, this is Q equals cotangent phi. And for the moment, this is written for the first quadrant. So both sine and cosine are positive. If you're in another quadrant, the position of the Q or the one will change according to the sign. Now there is one issue with. Now, there is one issue with this, which is that this stochastic matrix will not preserve the two-norm in the sequence one subspace. So, see control qubit in this controlled R phi gate. So, it corresponds to this lower right corner. And of course, this stochastic matrix will preserve the one norm, but not the two norm. And that means that if you have a state which has weights in both s equals zero and sequence one subspaces, the relative Spaces, the relative weights will change if you do this. And now, unfortunately, it's not possible to increase the two-norm beyond what you get here. So, a way out of this is to decrease the two-norm also in the SQL zero subspace by some amount. And so this leads to this, what we call the amplitude reduction gate. And you see what enters here is just the ratio between these two norms, so the one norm and the two norm. And so, this leads to a total stochastic matrix in the first quarter. Matrix in the first quadrant of this form here. So amplitude reduction gates, tensor identity four, and direct sum with here this stochastic matrix that we wrote before with the nothing on the control gravity. All right, so these were all single cubic gates. In the case of the U5 here, we had to translate it to stochastic matrix on two gravitas because of this complex value. Because of these complex values, we also need the C naught, of course. And well, this is again just a simple permutation, it turns out. I don't write it down here because it's a large matrix, 16 by 60 matrix, or if you include a re-in qubit, it will be 64 by 64 matrix. But it's very straightforward. So, for example, a state minus 1, 1 should be flipped to a minus 0, 1. Actually, it's this right actually is using the second qubit as a control, but never mind. So, yeah, and this just translates then into an exchange of pairs of byte four values. And so, with this now, we have a universal gate set of stochastic matrices, you might say. So, for all the universal gates for quantum computation, Hadamar pi over eight, which in fact corresponds to having phi in this phase gate equals pi over four. Phase gate equals pi over four and the C not gate, we have our stochastic matrices, and so we can just replace in any quantum algorithm those gates, universal quantum gates, by the corresponding stochastic matrices and propagate the probability vector. And then this leads to this theorem, the emulation of a quantum mechanical state evolution, which in short says that up to a brief factor that depends on the quantum algorithm, the quantum mechanical state evolution is emulated exactly with the Stofes. Is related exactly with the stochastic algorithm. In more detail, well, we write the Pyrrho state quantum algorithm as the sequence of unitary transformations on single or two qubit states. Psi zero is the input state, psi of ng the output state, related like this. And phi zero and phi ng, they are realifications. And yes, so we have to spend one more bit here, and so let. Here. And so let P0 be the probability distribution over by four values for this n-bit gravit, such that the initial state is represented by this nth partial derivative through the probability distribution. And so you want to identify that state with initial state up again to a pre-factor. And then if you calculate the final state as again the nth derivative of P and G, which is just a propagated probability distribution. probability distribution, then this psi ng is again up to a pre-factor the final quantum mechanical state of the quantum algorithm. Right, so that's a theorem and we can actually check it numerically of course. And so here are three examples. So the first one is from a stochastic emulation of the Bernstein-Mazirani algorithm, which is just a different version really of the Deutsche-Josha algorithm that probably most of you know. The idea is that No. The idea is that you're given some function, a linear function of x, and the function is defined by a dot x really modulo 2. So there's some binary multiplication, binary scalar product in modulo 2. And your task is to find out a. And this Benstein-Mazirani algorithm actually does it in one shot. So you have your oracle that calculates f of x, and you call it only once. And then in the end, you get a single peak with probability one on your output. With probability one on your output A. And so this is indeed the output here. So the endpoint with psi i from the gravit simulation, and indeed you find it's a perfect peak on the desired state A, the search, the searched output A. Here are two other examples from the quantum field transform. So here it's five bits. n-ball is what I call the number of realizations. So really samples that you Really, samples that you draw and then propagate with a stochastic algorithm. We started here with the period three state on the first four qubits, and so you don't get some sharp peaks, but you find some funny quantum mechanical wave function here from 0 to 31. That's actually shifted by 1, so 1 to 32. And so, orange is the exact quantum mechanical propagation, and the blue dots are from the stochastic emulation. If you really take a period four states, well, so a state is commensurate with the period of your entire number of states, then you get very sharp peaks quantum mechanically. And also, this is very nicely reproduced by this stochastic emulation. Right, so now probably you would say, okay, so can we just factorize numbers, you know, run chorus algorithms and so on? And well, at this level, not yet. And actually, if you think about it, what you have so far is really not good enough. Think about it, what we have so far is really not good enough. What we showed you so far is that we can really emulate the propagation of the state, and now I plot the state and it agrees nicely, but of course you have a thousand qubits, I cannot even plot the state, right? And actually, quantum mechanics does much more for us. It propagates the state, but in the end, it steals the physical probability distribution. So, in a typical quantum algorithm, you really have something like this. You have very few peaks, like when you do factoring, yeah. Like when you do factoring, you have some phase estimation, and the phases that you find contain information about the factors you're looking for. So, very few peaks, and these peaks really make that in the end. If you run your algorithm, you end up in the corresponding positions with very high probability. So, the psi really steals the physical probabilities to the corresponding peaks. And now, if you look at the stochastic emulation, unfortunately, it does not do that, at least, not directly. So, in fact, if you have destructive interference, it turns out that always leaves some socket in your physical probability distribution. What do I mean with this? Well, if you look, for example, at this very simple quantum algorithm, it's just Hadamard gate squared acting on input state zero. So, Hadamard squared is, of course, identity. So, you just get back the identity, the initial state zero. Now, this works as we have seen in theorem. So, the probability this. And so the probability distribution at the end of this algorithm will be 2, 0, 1, 1. And of course, yes, you take the difference here, you get psi equals 1 and 0. Okay, but if you calculate the physical probability distributions, you marginalize over sigma. So you actually add these two values, 2 plus 0 and 1 plus 1, and you have the preflector 1/4 here. So you get actually a completely flat distribution for your physical probabilities. For your physical probabilities. Okay, so this is terrible. And in fact, if you keep iterating this, h to the power of 2m, well, it's still identity, so it should get back the same state. And yes, you do up to a pre-factor. But the pre-factor turns out decays exponentially. And what does it mean? Well, if you simulate this with a certain number of realizations, then the number of realizations that still contribute from which you can estimate your psi decays exponentially. Exponentially. And this is disastrous. It leads to an exponential increase of error. So, just for this example, h to the power nh acting on zero, if I compare the exact state psi here, capital of psi with the estimate of psi just from the histograms. Well, in this Lin log plot, you see the error really increases exponentially with an h. Of course, the increase is slow if you start with a large number of realizations, but it's essentially. Start with a large number of realizations, but it's essentially always the same exponential increase. So, what to do? Well, a partial remedy is in what we call refreshments. So, refreshments are essentially a way of removing this socket that I talked about. So, it's a map from some distribution P to another distribution P prime, where, of course, the wave function should not be changed up to a prefector. Ideally, the prefector will increase, but in any case. Ideally, the prefect will increase, but in any case, these two things should be proportional if I vary i. So we don't want to change the state. On the other hand, I want that my physical probability distribution is exactly given by something proportional to absolute value of psi i. It's ideally the bond one rule, but even if not, I mean, at least it should be proportional so that peaks of the psi i prime translate to peaks of pi tilde. Okay, so let's look at this example we just had: the h square. So, again, here's the probability distribution. But if I realize this with a stochastic emulation, I might end up with a histogram of this form. So, number of realizations and number of builder balls, let's say, is 100. I might end up with this distribution here, 49 there, zero there, of course, because produced zero, and then maybe 27 and 24 here. And now, what the refreshment does is to take. The refreshment does is to take out this socket here. So we still have to keep the same difference because we want to keep the same state size. And then I just remove 24 of these realizations of these blue bolts, right? So you get three and then zero. And then in this particular example, I really have a state already that for the histogram obeys already the bond one rule, right? So I really have a zero here and zero there. It's interference free. And the And the bad thing is, of course, I reduce the number of samples I have now. But this can be remedied if I have this histogram here. I just multiply by two, right? So I just copy essentially my histogram once more. I double the number of realizations. So I get 98 here and six there. It's still the same state. And I just have to renormalize my histogram. And so it's nice. The bottom one rule is enforced. All things are coherence-free. The difficult thing is that it's. The difficult thing is that it's a non-linear map. In fact, it's a non-linear because we have to base our mapping of our samples on an estimate of the state. The state is estimated from the histogram. That's what I call here this H here, just by the same rule as what we calculate, is that we calculate the state psi. We take the estimate of the state psi from the histogram instead of the probability is to normalize. Normalized, and so the problem is that the estimate becomes exact only for n-ball going to infinity. So, that begs, of course, the question: how big n-ball does to have so that this works? And then we'll come back to the question. What we get here is also some diffusive process of the total number of realizations that makes it mathematically maybe a bit interesting. We didn't really follow up on this, but there's some diffusive process on n-ball varying between n-ball and to n-ball. And to n ball. But let me first of you show that this actually works quite nicely. So, the same example once more: so h to the power nh, single qubits. And so, this was what we had before without refreshments, exponential increase of the error. And now I plot this on the log-log scale and I go with nh to much larger number. So, actually, lnh, ln nh equals six corresponds to nh equals 200. So, go up to Hanamarkate to the power two. Panama gate to the power 200. I would bet that no actually existing quantum computer right now can do that without error that is actually larger than about the 10% that we are left with here. So it's a slow increase. If you fit it, you get roughly a square root of n age scaling. And so, indeed, one would say this is something we can live with. It's not exponential, it's really not even polynomial, it's a mild square root of an increase. And so this leads us to theorem number two, which again roughly says with regular refreshments, the physical probability flow for n ball to infinity is the one of quantum mechanics up to the use of the bond one rule instead of the bond two rule. And so once more, in a bit more detail, so we have this U unitary quantum algorithm of finite depth. I should have added here, also down there. Also, down there. Finite depth ng is the number of gates, and phi zero and phi ng are the ralifications of these two states, initial state and final state, and s is our stochastic emulation of the algorithm. Then we modify that algorithm by intercepting it with this refreshment gates. In fact, we need them only after each interference generating stochastic matrix. So, like after the Hadamard or the phase gates, we don't need them after. We don't need them after pure permutations. So C not is fine, X is fine, and so on. But if you really generate interference, in particular destructive interference, that's where it's most important. You need these refreshments. And so if you do that, after each interference generating gate, you add such a refreshment gate, then it turns out the physical probabilities pi tilde in the end will in the limit of n ball to infinity satisfied upon one rule. satisfied upon one rule. So it's really a peak of phi i will translate to a peak of the physical probability distribution pi and yes this will also be the limit of the estimator for m ball going to infinity for he study histograms right so that's nice again and indeed let me show you that this this works and then at least improves the situation so what we have plotted here is the minimum number of realizations needed for a successful inverse Q. Need for a successful inverse QFT. So, the idea is again we start with some periodic initial state and we'd like the inverse QFT to find the period. And so, here we look at the number and more, number of realizations needed to be successful in at least, let's say, in 10% of the trials. And this is a function of number of qubits and bit. You say, okay, if in 10% you're successful, you can still factorize numbers. Just have to try maybe 10 times, and then you're relatively sure to find. And then you're relatively sure to find the correct phase and corresponding who is processing will then find the factor. And so, once more, without refreshments, that number really grows very quickly or exponentially, that minimum number of realizations. So, on a log scale, actually, we cannot even go to a large number of bits because it just slows down so tremendously the simulation that we cannot go very far. And in fact, the scales roughly like exponential of 2.9 n bit. Of 2.9 n bit. One should also know that the number of gates in the QFT and inverse QFT scales roughly like n-bit square. And now, if you do the refreshments, well, the good news is the exponential increase has slowed down substantially. So the exponent goes down from 2.9 to 0.7 in bit. And in fact, if you write it in the power 2, sorry. Then this is roughly two to the end bit. So the good news is, well, the exponential increase has slowed down, but it still is an exponential increase. And when made R, it's possibly the slowest one possible. So to the NP, this will number of. So, to the end with this number of realizations that you need to sample the full space of classical states. So, that's of course bad news. And so, not to conclude on a too pessimistic note, I would also talk a little bit about a fundamental aspect. After all, the workshop is also about fundamental aspects of quantum mechanics. And so, one lesson learned here is, I think. Here is, I think, there is a certain reality, a physical reality in the quantum ensemble. And what do I mean with this? You probably have read this famous quote by Schrodinger before, that we never experiment with just one electron or one atom, loss, or molecule. And if you do so in thought experiments, this invariably entails ridiculous consequences, he writes. Now, the first naive modern answer to this would be, yeah, that was at those times, but now, of course, we manipulate. Times, but now, of course, we manipulate single atoms, like you do, quantum corals from IBM or an ion trap where you see really a single ion, or you can detect single photons. Often really manipulate single quantum mechanical systems. But nevertheless, there's actually quite a difference in the way how the probability distribution is manipulated. So, classically, we can really manipulate the probability distribution only by acting on individual samples. So, we must physically. Samples. So we must physically construct an orthoma from many, many different samples. That's what we did. I mean, this n-ball is really the number of realizations, the number of samples that we need in an ensemble that in the end is described by P. I admit naively, I was somehow expecting that this P is propagated just the same way as the psi is propagated somehow in the background without representing it somewhere. And of course, it's not. And of course, it's not really true, at least, not from what we know. We really have to manipulate our individual samples. And doing so, we might abstractly describe this with a probability distribution that is propagated. But this is really for an ensemble in the limit probably of n ball going to infinity. But in quantum mechanics, it's very different. We can really manipulate psi by acting on a single quantum system. We can really manipulate a single atom and still think the state of the atom of that single atom is represented by psi. That single atom is represented by psi. But in the end, psi is really about an osomo. We get statistical information, we get different measurement results every time we do the measurement, and all that information is coherent in psi. And so there must be some physical reality to this ensemble. And the question is, what is it? What is this quantum ensemble really? Of course, it's somehow linked to what is psi really, but I think it's a more fundamental question: what are your ensembles in quantum mechanics? What are your source models in quantum mechanics? Is there maybe really physical reality, maybe in some other dimensions? What do we know? But in any case, it makes it really clear what we did here with a new type of wave function that is based on classical probability distributions, that there must be some fundamental difference between classical and quantum probability distributions, respectively, size, that translate to a probability distribution. I would say this still does not endorse a many world interpretation. Does not endorse a many-world interpretation. David Deutsch famously says that the success of the quantum computer is really a proof of the many-world interpretation of quantum mechanics and firmly believes this. I don't quite agree with this because many worlds really says there's a bifurcation whenever you have a measurement or maybe at least an interaction with the environment that leads to a sufficient amount of coherence. Now, here we don't have that at all. I mean, we have just a unitary propagation and the state. Propagation and the state propagates. And we really think that psi is the state of an ensolve that might be hidden, but it gives us stochastic information every time we do a measurement. All right, so this actually points to a last thing that I would like to mention here, which is the importance of sampling and sampling complexity. We talked so far about quantum algorithms that ideally end in a peak. Ideally, end in a peak, one peak, or at least few peaks compared to the dimension of hubless space, so that you can extract useful information out of it. But there are other algorithms like this QAOA that you might have thought about, it's quantum approximate optimization algorithm. And essentially, this is a variational answer to find the ground state of some complex Hamiltonian. So, some easing Hamiltonian with many different interactions and maybe even a transverse field. Field. And so you make an ansatz by actually propagating an initial state here, it's a proposition of all computational basis states, with this proper Hamilton Hc and interspersed with a simple mix of Hamiltonian. So this HB is a very simple integrable and well-known Hamiltonian that you can implement. And then you just iterate this many times and you have variational parameters, which are these angles gamma one, beta one up to gamma p and beta p. P and beta p. And so the algorithm actually contains a classical step where you suppose you're supposed to minimize the expectation value of the energy. So you have to estimate here this energy in most cases from, well, measuring something. So measure your qubits. And then from these measurements, you get some statistics. And then you estimate the expectation value of HC in that state depending on gamma and beta. State depending on gamma and beta. Then you run your classical gradient algorithms and try to adjust the gamma and beta so you find the minimum. Now, for estimating this expectation value, you need many samples. You need to really sample your state many, many times. And so we thought it would be interesting to compare the number of samples you need to reach a certain, let's say, ground state probability with the quantum algorithm. And I mean, compare that to the number of realizations in our stochastic in the. Realizations in our stochastic emulation of the identical quantum circuit. And so, what we see here is: well, okay, the quantum mechanical simulation that is run with the Quasm simulator on Qiskit in an IBM quantum computer environment is slightly better than what we get with our Stockholm simulation for p equals one and then b t equals five here, but not much. And it's quite comparable, right? It's not the log scale or something, it's really like, okay, maybe like 10. Like, okay, you're maybe like 10-20% better. And we should also say that the stochastic emulation really is based on the same quantum algorithm that the transpiler of IBM spits out. So that transpiler actually has to take care of hardware constraints. So you cannot do a C naught, for example, between all qubits because you need any nearest name interaction in the actual physical circuit. So you have to propagate your C naughts, for example, over many qubits. Propagate your C naughts, for example, over many qubits. Nevertheless, we just translated that output quantum algorithm into the stochastic emulation. I think if we did not constrain ourselves with the same hardware constraints that after all are there for our stochastic emulation, we would probably still do a little bit better. So all of this just to say that, well, this sampling issue also exists if we do not have the algorithms that we mostly think of with single peaks. Think of with single peaks like in Delta Mazirani or Deutsche Yosha, or few peaks like in Schaus's algorithm. So, whenever the sampling comes in, we have to estimate some expectation value quantum mechanically. It would be quite interesting to see whether we do at least comparably well with the Stofasty emulation. And with this, I come to my conclusions. So, first take-home message is there is, besides the quantum mechanical wave function. There is, besides the quantum mechanical wave function, another physical object, namely this nth partial derivative of a probability distribution that shares essential properties of the quantum mechanical wave function, namely those that are really necessary for running a quantum algorithm. So in particular, it lives in a tensor product level space. We can manipulate it by acting on only single or two gravitons at the same time. It allows many particle interference and it has physical reality. Interference and it has physical reality. If you do the right gauge choice, you really get upon one rule. And this leads to an automated translation of any quantum circuit to a classical stochastic algorithm. The number of gravits units, or number of classical bits, so yeah, is 2n bit plus 1 if n bit is the number of quantum bits. So you add one for the real imaginary part and then multiply by 2 to represent each qubit by 2 gravits. I would like to mention this can actually be relaxed, it turns out, to need only nb plus 2. I can say more about if you wish. The problem is that we see an exponential decay of the number of contributing samples if we don't do any refreshments, the number of gates that create destructive interference. So, actually, it's also, I think, an interesting take-home message here. What really is problematic in the simulation of quantum Of quantum propagation, quantum dynamics is destructive interference. You really see very clearly here. This is where the number of samples you have to do something. And in our case, well, we have to estimate the state and so on. And so that requires a large number of samples. Still, this is one way of doing it. Maybe there are better ways. So far, we don't know of any. And I think also from the fundamental perspective, it is a clear hint there is really a difference between Really difference between psi and p. Again, we can manipulate psi with a single quantum system, and we cannot do that classically. We need the whole assembly that we have to construct physically. That's all. I just want to point out this is work done together with Bonnie Müller, who worked on his master thesis on it. Now he's doing PhD in Copenhagen. And if you want to read more, it's published in this paper in the journal Physics. Thank you very much. Thank you. Thank you. Any questions? Yes. I don't hear any questions so far. Hi. Hi, Daniel. Nice to meet you again. Yeah. I have a question. With these gravitas, one can emulate Bell's inequalities? Bell's inequalities? That's actually a very good question. Yes. Not directly. Actually, I've calculated it, but I think I know what happens. The thing is, because of this BOM1 rule, I mean, the Bell inequality really is based heavily on the BOM2 rule. And with this BOM1 rule, I, as I said, I haven't calculated it, but I'm almost sure it will not violate those inequalities. But there is a way around it, which I Is a way around it, which I yeah, I have to calculate and try it out, but I think there's a possibility that one can do it if you still extend the system, but not in this form I showed it here, not this upon one, I don't think so. Okay, thank you. Yeah, you're welcome. Good to hear you again. Any other question? Well, I have one by representing this. By representing this C as the nth derivative of a probability distribution, you introduce some sort of gauge freedom. Yes. Many different pieces will give you the same C. So is this feature a bug or you don't care? Well, I think it's very similar to standard quantum mechanics. I mean, there is also this gauge degree of freedom. It looks a bit different here and we use it definitely. I mean, in this sense, Use it definitely. I mean, in this sense, it's a feature. We use it for doing the refreshments. So there it's really maximum contrast, meaning that, well, we only have, let's say, in the positive part, we have some probability, but not in the part that contributes to the negative amplitude. So yeah, I think it's more of a feature than of a bug. It certainly makes the whole thing more closely connected. Certainly makes the whole thing more closely connected to standard quantum mechanics and it helps for doing these refreshments. Okay, so you say that somehow it represents the freedom of scaling the wave function or something. It corresponds to that gauge freedom in standard quantum mechanics? Yes. I mean, again, all the information in this Q here. And so with B, we can scale up and down the amplitudes. And R in the end determines this reduced probability. Determines these reduced probabilities, but which in principle have nothing to do with the quantum mechanical state, first of all, without the refreshments. They do once you do the refreshments and go really to the fully, I mean, the maximum amplitudes of the states. You maximize psi zero, then of course you also maximize psi one because they're related by a factor q. And this leads automatically to this form where you have q0, one, zero. And then you get automatically also the bond one row. We asked upon one rule. Okay. And the second question in your last transparency, I think there was this comment that instead of two n plus one, you can go to almost half of it. Yes. How do you do that? Yes. So actually, it can be easier seen at this point where I looked at the two bell stains. Let me go back to that. We cannot see your transparency anymore. Okay. Yeah, I have to reshare it. Sorry for that. Somebody flipped out of this presentation. Just to see things Yeah, I don't know what can you see it again, or at least the screen here. Okay, so yeah, so here we are. So for example, for the spell states, right? So there's a minus one, one an aesthetic can put the minus sign in the first qubit or the second qubit, even in quantum mechanics. And that corresponds to having two different states of three, two or two, three. States are 3, 2, or 2, 3. And so this means also here we put the sign in the first or the second gravit. And in fact, in this refreshment, I didn't point this out, but the first step is that you actually concentrate the sign in always the same gravit. So actually, I wrote it here, concentrate signs means you put the sign, if you have several gravitates, you always put in the same gravit. And that means, well, if you really run a stochastic emulation on many gravitons, there is no point in actually keeping the sign for each. So you actually have just. Each. So you could actually just have a single stochastic bit 0, 1, for each qubit and put all the signs in a single additional qubit, right? Actually, a single grab it. So that would give the overall sign. So your stochastic emulation would be non-local in the sense that you always put the sign in a single graph independently of where these signs come from. But that's no problem. You can do that. And then you. Can do that, and then you really need only well, one more bit for the real imaginary part, and you need design essentially, which is another bit, and then you end up with this much smaller number. Okay. Yep. Any other question? Well, then let's thank our speaker again. Thank you. Thank you, Daniel. Bye-bye. Thank you, Daniel. Bye-bye. Yeah, bye-bye.