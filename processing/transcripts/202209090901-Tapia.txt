Uh, what we call generalized iterators, uh, some signatures, uh, which is joint work with Yosha and Gurush. And so, let me start by giving you a brief outline of the talk. So, I'm gonna be speaking about first how this iterated some signature comes about as some sort of feature map for sequential data. And then I'm going to concentrate on some algebraic properties of this map. Of this map. So, this requires me to introduce what's called the quasi-shuffle algebra. You might have seen this before, but anyway, I'm just going to briefly go over it again. And then I'm going to show you some properties of this map in terms of this quasi-shuffle. So, this is section, well, should be three and four. And then at the end, I'm going to explain what I mean by generalized when actually on top of doing this. On top of doing this feature map, we want to apply some transformation to the data for reasons I'm going to explain later. Okay, so first, what is a feature map? Just a quick reminder. It's just a way of taking data points. Suppose you are in a machine learning setting, so you have some data that you want to learn, but it doesn't have any structure, so it's just living in a set. Living in a set that I've called here X. And so one way of going about this task is just taking your set and mapping it to some feature space denoted here by F that has more structure. It could be a Hilbert space, a reproducing kernel Hilbert space, what have you. And the idea is to kind of extract some numbers or some quantities out of your data in order to have more structure and work with it. Save them, put it on a On a neural network, and so on. And so, how you pick this, it's somehow very important for certain applications. And so, if I'm not mistaken, two years ago, Patrick Bonnier, Harold, and Chaba have proposed that if you have, you are dealing with sequences, what you can do is take your favorite feature map acting on one time point and then build another feature map going. Build another feature map going into the tensor algebra over your feature space here. And then just do an ordered product of one. So one is here is just the unit in the tensor algebra. So it's the unit vector in the real direction. Apply your feature map that you've chosen to each time slice and then concatenate them in this order. So I would first do one plus phi of x0, then concatenate Phi of x0, then concatenated with 1 plus phi of x1, and so on. And this gives me a new feature map that has a very good structure. Of course, this depends on the choice of phi, but given that we choose phi, the small phi here, in an appropriate way, then we also get nice properties for the capital phi here. And so, one common choice, and it's the one I'm going to use in this talk, is what's called the polynomial. Is what's called the polynomial augmentation, which is just basically taking powers of the data. So you take all polynomials in your data points for each time slice. So you think of each j as a vector in Rd, and then you take all possible polynomials in these entries. And so, in terms of the in a basis-independent way, if you want, this writes like this. Right, and then when you insert this into this capital phi here, you obtain what we call the iterated sum signature. And so actually, you end up by taking values in the tensor algebra over the symmetric algebra, because you see here, when you take these tensor powers of x, you are actually just computing symmetric tensors, right? So x, then x tensor x, x tensor x tensor x, and so on. X tensor X tensor X and so on. These are all symmetric tensors. So the feature space here is just a symmetric algebra over R. And so if you just replace this definition here of the big phi, you obtain the map sig of x. That's this order product here. You can actually be a bit more precise and write it as a geometric series here. And this has the flavor of somehow an exponential, right? So if you remember. Exponential, right? So if you remember Euler's formula for the exponential as the limit of 1 minus 1 over n to the minus 1, this would converge to 1 over E. This has somehow the same flavor. This is related to implicit Euler schemes. But I don't have time to go over it. And the nice thing is that you can actually organize this big product. This big product here in terms of the tensor powers in the tensor algebra over the symmetric algebra. So, here, keep in mind that we have two tensor products. So, we have the symmetric product that we are taking here on X, and then we have the concatenation product that's going somehow in the time direction. Right? And so, if you do a very quick computation, you can just show, and if you're used to dealing with symmetric functions, this should come as no surprise. Come as no surprise, the nth tensor power, the component of the signature in the nth tensor power, is an iterated sum of these polynomials here. Right? So again, just to be completely clear here, we have two tensor products. So we have the tensor product in Rd, which is the one that I've used to build the augmentation. And then I have this other tensor product here in the middle, which is just the concatenation in this abstract. Which is just the concatenation in this abstract tensor algebra that we built, no matter what the feature space is. Okay, so this is somehow telling you why this signature is called iterated some signature, but you can actually be a bit more precise about what it is. And so for this, you're happy to fix a basis. So you fix a basis E1 up to ED of Rd. Up to Ed of Rd, and then you expand this tensor power. And what you end up is for each tensor level with a sum over some index set of some real coefficients, which I've called here m i of x in some tensor basis, which is related to the original basis of our d you picked. And so the index set you obtain are n-tuples of multi-indices. So you have to think that. Multi-indices. So you have to think that here we have sort of, according to the fact that we have two tensor products here, we also end up with somehow multi-indices of multi-indices. Okay, but this is still manageable. And so the basis elements look as follows. So if you take an n-tuple of indices, then all you do is just take the symmetric power of the inner indices, if you want, in each I1 up to. In each I1 up to I, and then concatenate these results. So here you have to think about each I1 up to I n here in this multi-index as a letter in some alphabet. And therefore, I here is like a word. But this word also contains commutative words inside, if you want. And so, and the real coefficient you obtain, which is this is also. You obtain, which is also a very quick computation, is given for each word now as this iterated sum, where you see here I have repeated time indices. So I have J1, K1 times, and I'm taking the product of X in the direction I11 up to IK1. Then I multiply this with XJ2. So second time slide, and I take the monomial that's just the product of all. Polynomial, that's just the product of all the directions indicated by the multi-index I2, and so on. So I end up with this huge sum here. And so since this notation, it's getting very unwisely quite quickly, I'm just going to use some shorthands. So whenever I want to express a symmetric tensor product between two elementary tensors here, I'm just going to denote it by eyebrow. just going to denote it by i bracket j so here i and j are is a commuting if you want commuting multi-index index corresponding to one of these products here whenever i take the outer tensor product so in the tensor algebra of basis vectors ei and ej i'm just going to denote it as usual by a word ij and with two with these two notations then a generic basis like here on the left is Like here on the left, it's just identify with a word on these brackets. And so there should be a one here, right? So now I have a word that has commuting words inside. And as is usual, I'm just going to denote this coefficient here in these spaces by the components of the signature of x in the direction ei. And now the fact that we have this somehow inner. The fact that we have this somehow inner product structure on top of the outer concatenation product allows us to build a modified version of the shuffle product that's called the quasi-shuffle, or sometimes the physicists call it stuffle or sticky shuffle, and it has many names. Here, I'm just going to call it quasi-shuffle. And it's defined recursively as follows. So you take two words, and then you either Two words, and then you either extract the last letter of the first word, put it at the end, and recursively do the quasi-shuffle of these two smaller words. You do the same for the second word, you extract the last letter and concatenate and then the shuffle product. And or you make use of this additional associative product, this commuting product that you have here, and then use it to contract the last letters. So, a quick example, just to give you an idea how this differs from the user shuffle, let me take this word with two letters here. So, remember that you need to think of these bracket things as a single letter in your alphabet. Then quasi-shuffle, just another single letter. You get this shuffle part here. So, you see here, J1 goes either at the end, at the middle, or at the front. This is just the standard shuffle part. But then I have the contraction part. But then I have the contraction part that comes recursively from each of these three terms, where I've contracted I1 and J1. So this is coming from this first one here. So you see I've extracted the last letter, put it at the end, contraction here. Or I can take this letter and contract it with J1. And now I get this symmetric three tensor. Right, so it has additional terms. Right, and this somehow reflects the fact that we are dealing with sums here. And whenever I multiply two sums, I can get diagonal terms, which doesn't happen for iterated integers. That's why we don't see this structure in the iterated integral signature. Right, so we can quickly go over some of the properties. So this iterated sum signature satisfies change. Satisfies chance identity. This is quite easy to see from this product decomposition I showed you in the first slide. So if I take the product from N to M and then I put on the right, the product from M to L, I just get the product from N to L, right? So considering those time slices. So this is really no surprise. And what's more interesting is that whenever I multiply two of these iterated sums, this horrible iterated sums with all these. Of horrible iterated sums with all these small t-indices here. I can rewrite it as a linear combination of iterated sums, and this linear combination is coordinated by this quasi-shuffle product now. And having this quasi-shuffle identity is actually really important for applications because it allows you to prove what's called the universality of this map. Meaning, that given that I assume that my data space is compact, for example, then every function. Compact, for example, then every functional that I might want to learn from the data space into R say, we can be approximated by a linear functionality of the signature, just because you see, I can approximate polynomials in the entries by just linear maps. So it would be given by this quote-chechel products, and then I can apply on biostras, for example, and get every function. Get every function as just a linear combination. And so, for example, I could, which means that if I want to make f learnable, I just have to somehow pick out linear combinations of the signature are enough. So this makes it easier to work numerically. Right, so just as in the shuffle case, this quasi-shuffle product has some finer structure. So if you remember in the shuffle So, if you remember in the shuffle case, the shuffle product can be split into two half-shuffles. So, for the quasi-shuffle, something similar is true, where now I need to include a third term, which was the term that I had in red before. And it's just given by taking this contraction. So the half-shuffle looks the same as in the shuffle case, where I would use the shuffle product here. Where I would use the shuffle product here instead of quasi-shuffle. And now we get the contraction part that is just the third term. And you can show that these two operations now on these words satisfy these three relations, which are called a commutative treatment reform algebra, or CTD algebra for short. So in the shuffle. So in the shuffle case, again, just to give you an idea, these two last identities would not be there, just essentially because I'm setting this contraction part to zero. So I would get just this identity, and this is what's called a Symbian algebra. But if I have this contraction term here, this dot product, then I get these two additional identities, and this is what's known as a CTD algebra. As a CTD algebra. And in fact, this very special algebra works here, it's the free CTD algebra, and we can leverage this to give some characterization of the iterated sum signature. So in fact, we show that the iterated sum signature is the unique commutative tradendrophomorphism mapping each single index to the time series in the direction indicated by that index. Indicated by that index. And in particular, we get the following formula. So if I take now any two words, whatever length, doesn't have to be single letters, it can be any length, and I do this half shuffle, and then I look at the coefficient associated with this linear combination of words. This is just, if you want, a single iterated integral. So on the left, I might have something very complicated as a linear combination of things, but in the end, Of things, but in the end, this is just a double sum, and this also gives me a way of computing more efficiently these iterated sums. So, for example, if I wanted to compute just a subset of the entries because there are too many of them, then I can just choose somehow particular words. And then, if I can split them in this way, then I can just reduce my complicated computation to a single iterated sum. Okay, so that's that's sort of the properties of the iterated sum signature by itself. And now we want to see how far we can push these properties in terms of applying additional transformations to the data. And we're going to do this in two different ways. And so the first way is And so, the first way is by applying what we call a formal dyspiomorphism. And the reason you might want to do this is because once you do the polynomial lift, you might be interested in mapping these features, say, by a neural network. And so you make this map F here learnable, right? And then you're interested, for example, in knowing that, I mean, knowing if applying this extra term. If applying this extra transformation will ruin, for example, your universality, right? Because this is what the property you want to keep. And so just to be a little bit more precise, what we do is take a function that's expressible as a power series with a zero constant coefficient and just for simplicity, unit linear coefficient. And we use it to induce And we use it to induce a map on the tensor algebra by just taking the same coefficients and replacing the t power n here by a tensor product. So this is the tensor power now of s, which can be an arbitrary series. So this induces a map, a new feature map now, going into the same space by just taking the polynomial map and then composing with this. This capital F mapping. And what we end up is what I will call now the iterated sum signature of the first kind, a generalized iterated sum signature of the first kind, which is just obtained by doing the polynomial lift and then applying some function which can be again a neural network, etc. And so another reason why you might be interested in doing this is taking Is taking the specific case of somehow a truncated exponential, in which case you expect this transformation now to approximate the iterated integral signature, which we know, at least in the piecewise linear case, is a product of exponentials, right? This is somehow a truncation of it. And so we are interested in knowing whether this is still a good map. Is still a good map. Right. And so what we can show is the following. Okay, so you do this F2 map, so which meaning that you replace the polynomial lift by itself plus its square, and you start looking at the entries. So the first entry, so the single direction, there's So, the single direction, there's no surprise, right? So, our function has a t here, so it does nothing to say single indices. But when I look at the iterated sums and I go into this pencil product, I can see that I still have the same term as before, but now I get this new term. Right? So, if you recall from the definition, if I didn't have this F2 here, the signature entry in The signature entry in the direction I1, I2 would be just this simple iterated sum. And now, since I apply this extra transformation, I get this new term here, which is somehow a diagonal term with a one-half. And now, since we want to see if this still satisfies some sort of linearization property, we just multiply these two entries here. So, for I1 and I2. And you can see. And I2. And you can see from these two expressions that actually you get a shuffle property. So it's not quasi-shuffled anymore because it would need to have a third term here. But it turns out that at least for these two guys, this actually becomes shuffle. However, you can check that when you do the same with three indices now, it's not shuffle anymore. So this identity with what would be the shuffle of these two words here is not. Of these two words here, it's not satisfying. You get some extra terms which are kind of a bit hard to describe. And so, but it turns out that you can transpose this operation to the words. And this was done by Hoffman and Ihara, where they introduced associated to the same F a linear map now on words that's coordinated through compositions of Through compositions of integers. So, say I take a word of length n, and you give me a composition of the length of the word. If you don't know, a composition is just a sequence of numbers that add up to n. And I define an action of this composition on the word i by just doing symmetric tensor products of each single letter. Remember that. The each single letter, remember that each of these guys is a symmetric tensor product. So I take the first alpha one letters and I contract them together in a symmetric tensor product and I concatenate this with the following alpha two letters. I do symmetric tensors and so on and so forth until I do all of them. And the map And the map is just obtained by taking the coefficients of the original function associated to the size of each part in the composition, and the composition itself acts on the word. So this looks a bit complicated, but it's actually quite simple. So if you give, if, for example, I take a word of length three, which is just three single indices, and I act on it with the composition to one, which is a composition. With the composition to one, which is a composition of three, right? Two plus one is three. This is just telling me: take the first two letters, contract them with a symmetric tensor. This becomes now a single tensor. In the tensor algebra, it becomes a single letter and concatenate it with the last one. So this one just says leave I3 alone. Nicolas, three more minutes. Sorry, how many? Three more minutes. Okay. And so we can use this sign map here. We can use this psi map here to define what we call the twisted quasi-shuffle by mapping the original quasi-shuffle like this. And this is also a CTD algebra, meaning that there is a splitting into three parts satisfying the same relations as before. And so what we can show then is that if you pick out some entry in this generalized iterated sum signature where I've applied this tensor transformation on top of the polynomial leaf. On top of the polynomial leaf, you can pick it out from the original signature by applying this sign map to the word. So, in other words, this doing one plus f of polynomial lift is just the transpose of this Hoffmaniharama. And in particular, now the generalized signature of the first kind will satisfy a quasi-school identity with this twisted by f. So, just by the definition of the star F product, this would hold. The star f product, this will hold true. And so, if we come back to our example, I can invert the f2 function. I get square root of 2t plus 1 minus 1. I expand it as a series, and I get some coefficients. And now I apply this sign map. And I can see that the product of these two entries now, it's almost a shuffle, except for this triple contraction here, which it's not so easy to see if you just multiply the sums. And you also start to see that if you go to longer words, you start. Start to see that if you go to longer words, you start getting non-trivial coefficients like this 5 over 8 here. And in the end, when you take p to infinity, so you take just the exponential map, this is what's called Hoffmann's exponential, and what you recover as a twisted quasi-shuffle is the shuffle product. So, just to finish, let me just briefly give you another type of transformation where instead of applying the polynomial The polynomial extension first and then doing a transformation, you do the opposite. So you apply some polynomial transformation and then do the polynomial lift. So, for example, here you can take a time series and then take, for example, the Euclidean norm. And if you look at the entries in the signature, you see that you can also pick them out from the original signature. And so it turns out that this polynomial map that you apply to the signature induces a corresponding Induces a corresponding map on the corresponding tensor algebras. And you have a similar formula. So if you take the signature of the transformed path by this polynomial transformation, you can pick it out as well from the original signature by using this phi p map now. Okay, so this is the example which I don't have time to go over, but it's actually rather simple. And Simple. And what's interesting is that these two maps satisfy what we call the generalized shurweil duality, meaning that they commute. So it really, you can do whichever first you want, and then you can pick everything out from the original signature in whichever order you want. So this is sum up actually intertwined in this way. Okay, so that's all I wanted to say. Thank you for your attention. Thanks a lot, Nicholas. Any questions? I have a question, Harvard. Yeah, go ahead. Nicholas, thank you very much for this talk. I have two short questions. So the first one, so you gave us this expose on algebraic properties of these generalized signatures. So do you know any analytic properties? In the sense, can you define some norm on the space of time series so that you Time series, so that you can observe something much like factorial decay or something like that, so that you can say that these terms contain maybe much more information than going further into the sequence. And my second question is, in practical situations, why would you, let's say, favor one generalized signature over the other? So choosing one F instead of some other F or the standard arbitrated sign. Okay, so to answer your first question, yes, you can somehow think of these sums in terms of p-variation, and then you can get