I think we uh there's a better reason for being the other one. Let me try to grab one. Yeah. Saturated. I wouldn't use that or it's a question. Yeah, fine. Can I shop only a Gamoro or a PS? It's a cube mod, but yeah. Not the one you fortify. Well, I know it, but I know it's good to look at two posts. Unless you're on very good terms. Not really. Okay, remember to push the latter, we should not be selected. I crashed the last one. Also, we should not make jokes. You can make jokes if you want. Can I so I certainly want to call for bigger? Certainly more for bigger twelve but next time I ask for my question about the screen adoption. That's your diagram to use on that. Yeah, I was going to give it. Yeah, I was going to give it one extra random. Yeah, sorry, I'm not sure. The random detection matter and read the scores. I'm not trying to read the scores. Sure, it's even better. Well, it's outside is so well. So well, yeah, so she's doing what it's like. Oh, she ties it. Let's say if we're let's stick with our model. Let's get started. Thanks for coming back. David, go for it. Thank you, David. Thank you. So we'll talk about private algorithms for block models. The primary algorithms for block models, for block model and graphon estimation, it's trying to work with current and four members of microDTH. Some build or are shock markets that can try to dial. Now, okay, so private diagrams, what does it mean for algorithm to be private? So what you want is that you have some kind of sensitive piece of information in your data, then it should be the case that, the input data, it should be the case that. Distribution of outputs that the algorithm computes are almost the same as if this, if you would imagine applying the algorithm to the input data with that private piece of information removed or altered. And so this experiment that you sort of think about the input contains this private piece of information versus the input without that private piece of information are altered. This is called, we will refer to this as neighboring inputs. Neighboring inputs. Then there's this definition of differential privacy that I feel obligated to write down. So if you have a randomized algorithm A, then we say this epsilon differentially private. If the probability of the outputs is almost the same. It's almost the same for any pair of neighboring inputs. So, A, think of it as the randomized algorithm. So, it's like the distribution of outputs, given input X, and probably for every output Y, it should be up to 1 plus epsilon factor the same as if you had a neighboring input, X prime and Z. Or outputs. Outputs y and neighboring inputs x and x prime. So good. Now in our case, we will think about block model as graph estimation. So that means that the input data will be graph data. And there are two reasonable notions of what constitute neighboring inputs for graph data. And so one, so maybe this, like for example, the mathematically simplistic is that you have to consider two graphs to be neighboring if they differ by a single edge. This is referred to then as an edge-pribe diagram. Diagram. So you can add or remove the single edge. And then you know that the existence of that edge is sort of the private piece of information that you're protecting with this kind of algorithm. And then, like in terms of these kinds of private algorithms for some applications, maybe the more net. Algorithms or some applications, maybe the more natural notion of neighboring would be that you consider two graphs to be neighboring if they differ for a single vertex. And so the algorithms that respect that note that respect that notion of neighboring are called the node-private. So here what you do is you take a single vertex in your in your graph and you are allowed to rewire arbitrarily. Arbitrarily. So it means that you can add or remove any number of edges from that vertex. And you know, like if this case here, if you think about the relationship of these notions, so if you have algorithm that is epsilon edge private, sorry, epsilon node private. And you think about, okay, for what, you know, is it also edge-private? Then, you know, what you have to think about is: okay, if you want to rewire a single vertex, or if you want to re-whether this algorithm is edge-private, you have to think about, okay, how many vertices do I have to rewire in order to add or remove a single edge? And of course, like, you know, just one vertex is enough, right? And you so if you think about uh sort of adding or removing a single edge, then Adding or removing single hash, then any of the two endpoints of that hash you can use for this rebiring operation. So it means that any epsilon-note private algorithm is also epsilon-hash private. However, if you think about the other way around, so if you have an epsilon-ash private algorithm and you try to think about what's the node privacy that you can guarantee, then is it? Then, you know, if you want to rewire a single vertex, how many edges might you have to remove to simulate this rewiring operation? And the answer is that it can be as great as n minus 1. But you might remove all the edges that are currently incident to the vertex, and then you have to add all other edges to all other vertices. So that would be n minus 1. One edges that you're adding or removing. So it means that this works in have multiple neighboring operations and then it sort of adds up in the exponent. So it means that you have an algorithm that is n minus one hatch private, n minus one epsilon not private. And you know like this kind of privacy parameter is pretty bad, especially because like Especially because if you sometimes want this to be more some constant, and for this to be a constant, it would mean that epsilon would have to be one over n. And if you have that kind of privacy parameter, then there are a few things that you can do, even if you just want edge privacy. So the question is really: okay, what can you do if you want algorithms for knowing graphs that are not private? Graphs that are not private. It turns out that there are very sort of few results that, or very few known results about that. And typically it's about computing global statistics of graphs, like counting a number of edges and things like that, counting a number of certain subgraphs. And many of the results also specialize for random graph models. Random graph models is something that expect to play a role in the workshop. Play a role in the workshop. And this particular result for a node-private algorithm for a very general graph model that I want to talk about. So let me first introduce the model. I mean, it will be something that people are familiar with, but I just want to standardize. Standardize some terminology and notation and then show you what is known for, you know, what kind of private guarantees are known for that model. So in this model, we have a block connectivity matrix, and we will refer to this as V0. And this is an unknown matrix. And eventually, our goal will be to estimate that matrix. So it's a K by K matrix. So K is the number of. K matrix, so k is the number of blocks. And we will normalize it, this is a non-equal entries, we will normalize it so that the average of the entries in this matrix is one. And you also don't want that crazy large entries in this matrix. So the and here, just for simplicity, let me require that the maximum entries at more. Require that the maximum entry is at most 100. And of course, like they can do everything also with 100 replaces some parameter, and then the parameter will mess up all the other ones that you will get. But 100 is already pretty interesting because the canonical thing to think about would be maybe a block matrix of the following shape. You have epsilon will be usually what you call epsilon at this point, but epsilon will Epson is point, but epsilon is already taken. So I can use another Greek letter. So suppose you have 1 minus eta on the diagonal. And on the orthodon entries, we want the entries to average to 1. So to ensure that, the orthodon entries should be minimized eta, I guess, divided by k minus 1. That's the number, that's all the other entries. The number that all the other entries see, and it's a symmetric matrix. So then, if we look at this, and this would correspond then to the graph model where you have a certain average degree and you partition the vertex set into pieces, let's say of equal size, and you then when you create edges, you ah, sorry, you get plus. Plus, yes. You will have your final. your by an epsilon term more likely to connect to you know connect have edges connecting to the same block and a little less likely to connect to a different box good now another piece of notation so we will we will try to we will partition things into these k pieces and we will And we will give you other normal, like less than 100. Ah, yes, yes. This is maximum entry of the matrix. Maximum entry. Maximent of the matrix. So this average entry and maximum entry. Good. So we will use dissertation to denote sort of partitions of the numbers from 1 to n into k pieces of equal size. And more precisely, I will use so the way I will encode these partitions is as a vertex part incidence matrix of a K epipartition. Of numbers from model n. Here's what does it mean? So it's a matrix, it has n rows and k columns, and it's edges are 0 and 1. And it tells you, if you look at the ith row and the jth column, it tells you whether vertex i belongs to part j or not. So in this script Z will know all matrix. Z will know all matrices that correspond to such K equipartitions. Good. And so now I can define a model. So, you know, I refer to it as zero, as a degree parameter and as vertex parameter n. And this model, the way it works, is that you generate a graph and do in two steps. So, first is that you sample uniformly at random from Uniformly random from the set of these agri-partitions. And then you draw a matrix. So you use this double underline to denote objects that have a distribution. And you subtract a graph. And the way you do this is that you do this is that you first construct a matrix. And so the matrix will be multiplied by D of n, so that will be the average entry of the matrix. And so what you do is you take B0, okay, and you create out of B0 an n-by-n block matrix, and you will choose the blocks according to the acupartition Z that you have chosen before. And so the linear algebraic operation that does that for you is that you multiply B and so. Is that you multiply B from the left with C and then B0 and from the right with C transpose? And so this matrix here will be an N by N block matrix, you know, N by N matrix that has blocks, and every block corresponding to an entry of zero, and like the partition of the blocks comes from this matrix Z. Z. And then, so this will be a matrix with average entry one. Just from here. And you scale it by E or N. And I guess maybe, sorry, the average entry. It doesn't matter as much. So you scale it by this, and then it gives you probabilities, and you. And you use those probabilities to draw Bernoullis. And you have n-squared Bernoullis that you draw, and that gives you an adjacency matrix of a graph. But yeah, it's just a standard stochastic block model. And just a notation that we can refer to later. And maybe the interesting thing is that they have this sort of general connectivity matrix V0 that tells you what Uh um sort of what what's the what are the properties between um with parts? Um which um and you have to go with you to estimate B0 given G. And so if you think about this kind of thing And so if you think about this kind of estimation problem, so one so first thing that one has to think about is that this B0 is clearly not identifiable from G. But because, I mean, for example, if you would permute the rows and columns of B0, you give rise to the same distribution of graphs. So clearly, there's no hope to recover that kind of permutation. And so we now want to have some way of factoring. To have some way of factoring out this kind of symmetry in the model, so that we can talk about identifiability after factoring out these symmetries. And a convenient way to do this is define a distance that does this factors out of these symmetries for us. And it turns out that there are several choices of distances. So this will be something that we discuss. So the maybe most natural. the maybe most uh natural or uh uh choice of distance, which turns out to be um uh not the one we end up using, is as follows. So what you do is you want to define a distance between two matrices E1 and E2. And basically, you know, like what this distance should tell should allow us to do is to tell, should, you know, if this is small, if If this is a small, if B is a small distance to B0, it should mean that it's like a useful estimate. And the distance, we use this notation delta sub k. So sub k will be clear later on. So what you do is you just go over all permutation matrices, pi one and pi two, and you see, okay, how can I get the closest fit in Fubinis norm distance? In Fubinius norm, distance after using pi one and pi two to permute the rows and columns of one and t two. Okay, so what this means is that and let's let's say if you look at the average entry you use pi one to commute the columns and rows of e1 and pi two of e2 and then you see how close can you get in for b the sum that would be one. That would be one reasonable notion of distance. Again, it would have the property that even if you don't get the permutation of blocks correct, you could still have a small distance. That's definitely a reasonable choice. However, it's not clear whether this notion of distance really captures all the symmetries that you have in your model. Especially sort of also once you care about. Especially sort of also once you care about maybe some kind of approximate symmetries. And therefore, I want to suggest that the following notion might be a better notion of distance. And on some level, I mean, our goal is really to think about algorithms. And so the algorithms should just work in the most reasonable notion of distance. And so the following might be a more appropriate notion of distance, and delta so n. Okay, and it would actually be like a proper generalization of the previous notion of distance. And what we're doing instead, instead of having here these k by k notation matrices, what we'll do is we will blow the matrix E1, the matrices E1, B2 up to n by n matrices that have this box structure, exactly like we had for the definition of the model. Some level that's the most Can you take what is variation difference between the distributions of the graph? I mean, so at I mean, so I will look at the, you know, in this parameter space, I will not look at the distribution, so we get again at the Fubinis norm of the two matrices. So this would be minimum, Z1, and Z2. They're chosen from the set of repetitions in N items. And then because you have our larger matrices, we normalize it by the same amount. And then you have here Z1. And then you have here a Z one one Z one Z two Z two I forgot transposes here and so this is clearly sort of capturing more more of the symmetries than this previous notion of distance. However, you know, like this notion of distance is sort of if you think about it, I notice that it generalizes because if you would choose here Because if you would choose here, you know, if the general is a freely sort of distance, if you choose, you know, because if you think about ZKK, you know, it would just give you the computational matrix. So the difference is that the second one allows you to kind of fractionally match. Yes, that's it. I think I didn't get that. Can somebody you say that in more words that are information? I mean, so you know, there are some blocks that. So you know there are some blocks that that some choices of these block matrices that correspond just to permutations of the of things, but you have here more you have you have more degrees of freedom where you can sort of to you know try to match some fraction of one block to to you know to another fraction of if I were working just in the like SPM setting which has a special structure for the maybe two. Ah, if you're like this ismetric phase, probably would not make a difference. Okay. Let's say I didn't check what. Okay, let's say I didn't check, but it could be that it it doesn't make but but here like the I mean the goal would be to really do this estimation for arbitrary choices. Sure, sure, sure. I'm just trying to understand the difference between this. We actually yeah, we will get to this. I mean we know some examples where these things are not equal. We also know that this thing cannot be ma larger than k times k or k squared times. So it cannot be I mean the difference here, the multiplicative difference, it cannot be larger than Be larger than k squared sector. If I had to describe the information theoretically correct notion of distance, would it be the total variation distance between the distribution graphs with like B0 and B1 and B2 as the, yeah? Like in some sense, that's a, like, you know, you're just trying to decide whether they're different by one sample. It just seems like maybe the information theoretic book. The information-theoretic motion that would capture it would be the total variation? Right, I mean, but I guess it's a difference of where you want to learn the model in intensity or whether you want to learn the parameters rare ones. At some level, it's like, I mean, here's Gaussians, and you ask maybe how close can I get in at the square distance to the true means. But you could also ask how close can I get into total relation. Going to get into variation distance through the original distribution. I'm not, you know, because it's just a single sample, I'm not sure if. So then you need like distance close to one in some sense in order to be able to distinguish. Because you want to have a high probability distinguisher, then you need like, you know, the distance we might just say close. I mean, if you care about distinguishing, then it's a different question. But I mean, our goal will be to estimate zero. So our goal will be to have the sun be head that is as close as possible. We had that is as close as possible to the record, correct B0. And yes, I mean, you could ask that two descriptions or graphs should be close, but I'm not sure. I think in that context, it seems... Yeah, it's a good question. It might be a harder question then. But this is E1 and Z2 is kind of defining a coupling between those two distributions. Yes. So, you know, of some sort. Yes. This being small requires a chi-square small tiny distribution. Yes, yes, exactly. So on some level, if there's an example where this is much smaller than this, then probably it means that this is really the correct notion that you should use to estimate. I don't know what it means for the closest of distributions if you have this closest. Of distributions, if you have this closest of the parameters. Right, so, however, you know, there's something not so nice about this notion of distance. That's because it's like, okay, so some of you chose here some, but if you just think about defining distances on A biker matrices, and now there's also this weird parameter n that comes in. And so, you know, the final distance between U is just a limit of this for n to infinity. So, delta, the distance that we will end up using, will be, you know, choose this limit of capital N to infinity of this previously defined distance where you have these partitions. Yeah, we'll talk about it. I mean, actually, so one thing, I mean, so you know that there are some examples where there's a difference. We also know that this kind of We also know that this cannot be smaller by a k-squared factor than this. And it also turns out that if you choose n here, let's say significantly larger, if you choose n to be at least k to the 10 or something like that, then for that choice of n, you'll be very close to the limit. So it's simply the case that once n is significant, is large enough compared to k, then there's sort of not much more that happens. So this definition seems to like kind of use the fact that we're sampling uniformly from echo partitions. But if you wanted to like generalize this to unequal blocks, what would be the right way to yes, yeah, that's a that's a good question. I mean um so I mean okay so so this equipartition I mean is I mean one thing is weird is sort of like there's some dependence. There's some dependence, right? And usually, I mean, often you just say that for every vertex, you should choose independently what block it goes to, and uniformly, and independently what block it goes to. And then you will not really get acrylic partitions. However, in these contexts, you can show that unless K and N are very strangely related, this acrylic partition model will be equivalent in terms of this estimation task to the model where you have to dependent labels. To the model where you have independent labels. If you ask about unequal, like having partitions of unequal sizes, I think you can simulate it to a certain extent by sort of using multiple blocks to encode a single block. Of course, then there will be some losses because our errors will depend on k. So, yeah, it's a good question if what are the right generalizations. The right generalizations for that. The better distance version seems like actually generalizes. The naive one, probably, not so much. Yes, that's true. Good. So now, what do we know? If there are no private algorithms in this model, so this is a result by Christian Borges, Jennifer Chase, and Adam Smith. And Adam Smith, and there's like a follow-up work by together with Zarik. I think 15 and 18. What they show is that there exists some no-private estimate, epsilon no-private estimate B hat. But so this is some random object, you know, sort of Sort of depending on the input G or the observation G that you have. And what it satisfies is that with high probability, the distance the estimate and B zero is bounded to a constant factor by K over D plus K squared times k squared times log n divided by epsilon n. So let's say two more things here. So it's really thought about it as like an information theoretic mechanism. And in particular the running time that you would get would be not so great. So it would be exponential in n times k. In n times k. So, David, this is delta k or delta n? Sorry, here? Yeah. No, this is just delta. Just the bottom. The limiting limiting things. And so they do this for routes with logarithmic degree. So let's talk about. So, one thing that you can observe here is. So one thing that you can observe here is so this here epsilon is a par privacy parameter, so so this affects only one of the two terms. And you know like here you divide by n and here you divide by d. So you know typically the way we think about these parameters is that n is much larger than d. And so it means that actually like this term here will be dominated by k over d for a pretty large range of values of epsilon. And what it means is that you get privacy for free. So you like you know you don't incur the error estimate. The error guarantees don't deteriorate. The error wants to deteriorate. It's to choose absolutely smaller. Unless, of course, this term starts dominating. So this term K over D, it turns out to be the kind of term that you would expect from the Cassin-Strikenbound in the context of the block model. To see precise calculations. And there's also, like in the context of this estimation task, there's evidence that based on these low-degree low bounds, that K over D is the best thing that you can hope for, even without privacy concerns. You're running in exponential time and still paying this K over D. Yes, yes, exactly. So that's. Yes, exactly. So that's maybe they could have, if they were willing to stick with the same time, maybe they could do it. Yes, yes, actually, so that's a good point. So I'm not sure if you end up writing it in the paper, but yeah, you could get you could remove this k and replace this k by a log k a log k factor and get the same phase. In which case the first terms might not dominate anymore. Well, I mean, it depends on how you think about K and anyway only depends bigger on K than this guy. Good. But yeah, I mean, usually we think of N as very, very large, but K and D is much smaller compared to compared to that. Good. Good. Um see. Right, and so so and uh now you can ask, okay, what this privacy term. So sometimes, you know, even for non-private algorithms, this K over D I know uh has to be there. Um and now this term here also turns out to be necessary in the sense that you know if you didn't have like up to this log factor you can't hope to remove this term. Like it could be that you could prove this kind of a bound, but you can't prove something that is strictly smaller than that. Okay, so that's a sense in which also this low bond in our paper. We have a low one in our paper where it shows that this is unavoidable. Good. Now the question is: okay, what do we do in this work? So, I'm not sure if I have a colour, that's not is the sh say that you can get some of these colors here for the color child. Oh, yes, but kinda So we will be able to reduce the time from n expansion in n times k to something like n to the polynomial polyk. So still expansion in k, but now only polynomial in n. And we can also remove this restriction on D. That's it. We notice that if D is super small, then this error bond is not interesting. So I guess, I mean, for this error bond to be interesting, the D should be larger than K. What we're doing this work. So let's see. So maybe let me very briefly talk about how you can think about getting private algorithms. Think about getting private algorithms for this kind of problem. So, there's one very general approach which is subsample and aggregate. So, the idea here would be that if you apply that approach to this context, what you would do is you would take your graph and split it into an aggregate. So, if you apply it to this problem, what you would do is you would have your Problem: What you would do is you would have the big graph, then you would randomly partition it into a number of pieces, let's say t pieces, and this is like a random partition, so it's completely unrelated to the block partition. And then you would independently run some non-private algorithm on each piece. It would give you some estimate for B0, and then that means that you have now T estimates for B0, and you would try to aggregate them in some way. And you would try to aggregate them in some way. And so, one observation here is that if you want to do this kind of aggregation, the fact that you have the symmetry in the block matrix, in the connectivity matrix, creates some problems. Because you can imagine that if you want to meaningfully aggregate these K okay, so maybe let me just say what's the connection to privacy. So, what's the connection to privacy? So, now if you do this thing about node privacy, what it means is that only one of these blocks will contain the node that has been rewired. So it means that if you do this thought experiment, the way you want to aggregate is you know that all of these blocks contain the correct information, and only one might be affected by this rewiring operation. This reviring operation. So that means that we want to aggregate these estimates in a way that slope us to one of these things being changed. Good. So now, but if you want to think about aggregating these estimates, then let's say you would expect that you at least need to be able to tell whether two of the estimates are close. Because you do want to detect whether one of them is bogus. One of them is like bogus. So it seems that you at least would have to be able to test whether two of these estimates are close. Now, what kind of task is that? Well, you have two k by k matrices, okay, and you ask whether they are close in this notion of distance. So this seems like some kind of approximate graph isomorphism that you have to solve on a k-vertex graph. And so it means that's a task that we expect to require time expansion in k. Exponential in K. It means that the fact that this year requires time exponential in K is probably unavoidable in this report. Yes? So the last best distance is. Can't that be written as some kind of optimization over doubly stochastic matrix? Yes, yes, very good. Yes, so this would be, yeah, thanks a lot. So this would be one of the important observation, exactly. Yes, so you can solve it as you can. Yes, so you can solve it as you can view this as an optimization, project optimization over double-stochastic matrices, where the double-stochastic matrix keeps track of how, like, what's the fraction of overlaps between the two partitions that you mentioned. Yeah, but that also is not clear how you can do this in time better than exponential. Because it's aquatic over convex sets, yeah, something really. Yes? So here it just, you know, you split the graph independently at random into T pieces. So for every vertex you put it in one of the T pieces at random. So if one of the vertex is connected to every other node, then it's negative? Ah yes, exactly. So you only keep the edges that stay within the pieces. So you remove edges that go across two pieces. You remove edges that go across two pieces. Or remove edges that go across pieces. So you also cut down the depth of the pressure. Exactly. Right. So here, one observation was that you will probably need to spend time exponential in K. And the other thing, as you said, you will remove a large fraction of the edges. Only one of a t fraction of the edges will remain. So it means that your, you know, the quality of your estimate will be affected by this t factor. Essentially, it's like it means that the average degree decrease from d. means that the average degree decreases from D to D divided by D. And so it means that your error, even for these non-private algorithms, could be at least T times K divided by T. And that means that actually, with this approach, you won't be able to get. So is it fair to think of T as one or epsilon? Right, so that's a good question. So T you actually like there's a packing argument that shows that T has to be at least the K squared uh divided by epsilon. Divided by epsilon. And so it means that you pay sort of like your quality of your error won't be worse off by t factor. And because you have this epsilon also here, it also means that you don't have this free privacy property anymore because the first order term will be hit by the epsilon. So this doesn't work. Doesn't work too, you know. It gives you some guarantees, and the running time will be years of this kind, but it will not be. Of this kind, but it will not give you this bound. And so now, in the remaining five minutes, let me tell you how you can get sort of this improved running time and I missed it, but is there the reason why this doesn't work is because it's too sensitive to the perturbations? For regulations because of the T is bigger implication. Yes, I mean, okay, so I mean, it doesn't work, it's also like maybe a strong word. I mean, here, like, in the sub-sample and aggregate, the nicety is that it provides you an abstraction where some level you reduce a problem where some vertex might be corrupted to the problem where you have T very good estimates and only one of them might be problematic. And now, if you sort of abstract the problem to this, And now, if you sort of abstract the property to this, like you have, you know, given these t estimates, you want to aggregate them in a way that is robust to one outlier, then Packing Markman shows that t has to be bigger than k squared over x. Okay, but so the way the I mean actually the approach will be to start with their proof because that gives the right error bounds and then just think about okay, what do we have to change to get the better running time. So the starting point is if you look at what So, the starting point is, so if you look at what we get here, this adjacency matrix of this graph, you can think about it as a noisy version of this matrix. And this matrix contains essentially the information that you are trying to get. So you can think about trying to fit a matrix of this shape to the adjacency matrix that you have received. And so, this is some kind of optimization problem. And now we want a private algorithm that corresponds to this optimization problem. And the way we'll do this is that we will define the distribution that the private algorithm will sample from, and we specify the density of that distribution in terms of this kind of optimization. In terms of this kind of optimization problem. In this case, it's called a score function. So, what you can do is you since a score, you decide, you know, for every matrix B, we define some score, depending on the input matrix. So, this is our input matrix. And think of it as maybe a scaled version of this adjacency matrix. Let's not worry about this as much. Not worry about this much. And maybe this is maximize the correlation between matrix of the form that our model postulates and this input matrix, Y. And here the maximization of overall choice is Z from this set of equipment partitions. And there's an additional term that doesn't matter for the optimization, but plays a role for defining the density, which is the norm of Z transpose in the Venus map. There's a one-half factor to make this local grid. Okay, and then you will sample from a distribution, you will sample from the following distribution. From the following distribution, where the probability of matrix B given some input matrix is proportional to the exponential of say lambda times this score function. So in the quantities of this private algorithm, this is called the exponential mechanism, and it's like a standard construction for, you know, Construction for designing private mechanisms. And so, this is what they do in this work. And they show that if you sample from this distribution, it satisfies the privacy guarantee. So for neighboring inputs, this corresponds to removing or changing one row and one column in this matrix. For neighboring inputs, you will get the distribution will be multiplicatively changed only multiplicatively by e to the epsilon factor. By e to the epsilon factor. And it has this property, its high probability. If you sample from this distribution, you will output a matrix B that satisfies this error bond. Okay. Now, the question is, if you want to think about algorithms, the question is, how do you sample from this distribution? Can you sample from this distribution efficiently? And so, one thing that you can think about. So, one thing that you can think about is that if you want to discretize this probability space, what's the probability? The probability is over these matrices B, the potential outputs of the algorithm. And these are K by K matrices. So you can discretize them so that you have, okay, if you discretize, you have epsilon net, and the epsilon has to be fine enough. And so the size of this probability. And so the size of this probability space can now be exponential in k squared, that's the number of degrees of freedom. And because you want to describe the situation as to be fine enough, the size of this property space will be something like n to the k-square. It means that you could try to sample distribution by just explicitly discretizing the properties space and then summing up these kinds of functions in order to compute the properties and then. Probabilities and then this this works fine. The only problem is that even evaluating this unscaled density function for a single matrix P requires you to solve this optimization problem. And this optimization problem, it's this optimization problem of all partitions. So this is like an NP-hard optimization problem. Naively, this takes time exponential in This takes time exponential in the number of variables and the number of variables here is endance. Good. Now, to just five minutes for I can show you what else it is, say what we use. So not to you know so this this function is problematic because evaluating it as in the B is takes time, it's mentioned in state. Takes time, it's mentioned in state. So the idea is, and this is not our idea, to use some space relaxation as a score function. Essentially, you think about this optimization problem, and for every optimization problem, every reasonable optimization problem, you can define some squares relaxation. And so let's denote this by Square S V of N. And we use that as a score function. The new probability distribution will be defined so that the probability for P is proportional to the exponential of this sum of squares. And so there's an interesting something interesting going on here. Okay, good. Going on here. Okay, I should state where this comes from. This coming from two papers by Sam Hopkins and Hamad Majid and Sapoda worked with Darrellan, 23. And then he sort of explored what you can do with this idea of using some stress relaxation as the flow function. And so, one question here is that for these kinds of estimation problems, like if you want to just non-privately, given G, estimate V0, there are very good spectral algorithms that can do this. So, it means that there's a spectral relaxation that works for non-prioritical things as well. So, one question is why can't you use that? It seems like a much simpler thing to use, and maybe that should work just as well, and don't have to use this. Work just as well. You don't have to use this somewhere square machinery. And I think it's like, so here is why it doesn't seem to work. And the main reason is that, you know, for spectral extension, what you show is that if your input matrix comes from the model, then it will output some estimate we had that is close to the correct v zero. And you show, you know, okay, so this is what you show. But here, I mean, one thing is that. But here, I mean, one thing is that here we have to define a reasonable probability for every choice of B. The probabilities have to be consistent in a certain way. And so that means that even for a worst case choice of B, it has to give you some consistent probability. And this is something that spectralizations can't really do. Right, so now how do you analyze this? How do you analyze this? So, you know, if you have these four functions, they are for these private algorithms. You first have to do, I mean, there are two things that you have to do. You have to do some privacy analysis that it satisfies the definition of differential privacy. And you have to show that it satisfies this zero bond. Now, for the privacy analysis, this boils down to proving that the score function has low sensitivity in a sense that if you do some noise, You do some node in the input matrix that the value of the score function doesn't change by much. And it turns out that that analysis for the sum of squares relaxation looks exactly like the analysis for the non-sum squares. So it's sort of for free. You don't have to change anything, you just have to use that. The previous proof that this has low sensitivity, you used only some of Swares arguments, and so it's fine. Good. Now for the utility analysis. So we want to show that the score function assigns large scores only to matrices that satisfy this kind of error bound. And here's something interesting happens, which is that almost everything works out in the same way as a default non-SS version. The only difference is that in the non-SSS version, the equation was. In the non-SS version, the conclusion was that if you have a high score, then you're close in this delta distance to B0. Now, if you do the same analysis for the sum of squares version, you get the same conclusion, however, not for this delta function, but for the sum of squares flexation of this estimation problem, of this distance problem. So you only get the sum of squares flexation of this distance, and actually the version where you have these n. The version where you have these n embeddings is small. When you get that this year is