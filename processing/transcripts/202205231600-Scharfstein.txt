Well, thank you very much. I'm glad to be here. I wish I could be there in person. Unfortunately, I'm here in Berkeley for the competing meeting. Thanks to all who stayed around for this last session. I appreciate your attention. So I was at Johns Hopkins for many years, but in 2020, I actually moved over to the University of Utah, where I'm currently. Where I'm currently chief of the Division of Biostatistics and in the Department of Population Health Sciences. And I'll just put in a plug for anybody who's on the job market looking for a job, send me an email. My email is right here. We're hiring and would love to talk to you about our division. So, this is joint work with Bonnie Smith, who was a PhD student of mine at A PhD student of mine at Johns Hopkins, and our very own Shu Yong, who's with us today from NC State, and then our clinical collaborator, Andrew Apter from the University of Pennsylvania. So we all know that in randomized trials, we often design them to collect outcomes at specific points in time after randomization. But in practice, Optimization. But in practice, there can often be a lot of variability in the times at which people are actually assessed in those trials. And these irregular assessments times can be due to a number of reasons. And these reasons could be related to the actual outcome of the participant. And when they are related, the assessment times are actually related to the outcome of interest. We say that the assessment times are informative. And so, for example, informative. And so, for example, if you're studying patients in asthma trial, a participant may be less likely to attend a data collection appointment when they're actually having an asthma exacerbation. And we've actually interviewed patients in asthma trials and have found that, in fact, people who had delayed assessments, you ask them, hey, why didn't you show up for your assessment? And they say, well, I was having an asthma exacerbation. I was in the hospital. Asthma exacerbation, I was in the hospital. So these assessment times can actually be informative. So here's an example of a study that was conducted by our colleague, Dr. Apter, called the Asthma Research for the Community Study. And what you see here is each panel. So this is the first assessment time, which was scheduled to be occur at three months. Here's the second assessment time, which was scheduled to take place at six months. Time, which was scheduled to take place at six months. Here's the third assessment time, which was scheduled to take place at nine months. And here's the fourth assessment time, which was scheduled to take place at 12 months. And you can see in each of these panels, we have the histogram of the actual first assessment time. And you can see that it varies dramatically from the targeted assessment time of three months. Similarly, at the second assessment, again, varies dramatically from Again, varies dramatically from the sixth month, and the third assessment varies dramatically from the ninth month, and the fourth assessment varies dramatically from the 12th month. And so, this is very typical of clinical trials. And so the question is, how do we handle this when we come to the data analysis? Well, what people typically do is they create windows. Windows. They create windows against around each targeted assessment time. And then assessments that occur within a window are treated as though they occurred at the targeted time itself, while observations that fall outside the window are either excluded or placed into a different window. And this approach of creating windows can actually lead to biased estimation of treatment effects. And so in this And so, in this next slide, I'm just creating a little toy illustration to demonstrate the kind of bias that can occur. So here's an example just of one assessment time that's scheduled to take place at 12 months. And higher levels of asthma control are considered to be worse, more severe. And so Here. And so these solid lines here is the blue, is the true mean outcome for the control group. And this green one is the true curve for the mean outcome as a function of time for the intervention arm. And so you see here, this red, the length of this red line represents the actual true treatment effect. Actual true treatment effect had everybody shown up exactly at 12 months. And so the treatment effect at that point is minus 0.25 on this asthma control scale. And now if I create, and if I were to just use all the data here and basically just compute a naive mean of the blue dots for the control group and a naive mean for the intervention arm, what you'd see is that the Is that the actual estimate using all the data would be minus 0.35 or about 40% biased. And then if I were to create a window around 12 months and just took the observations that fall in the window to represent the treatment effect at 12 months, what we would have is that instead of minus 0.25, we get minus 0.29, and we have about 16% bias. 16% bias. And, you know, this bias is driven in part by the fact that the sort of differential shape of these curves between the two treatment arms. Here's another example where there's actually the true mean curve for the control arm and the true mean curve for the intervention arm are completely flat. They don't depend upon time. The true treatment of Upon time. The true treatment effect of 12 months is represented by the length of this red line here, which is about minus 0.6. If we use all the data here in this example, we actually get an unbiased estimate of the treatment effect at 12 months. But if we create a window around 12 months, what you see is that you're losing, particularly for this, for the control group. For particularly for the control group, you're losing a lot of these high values or more severe cases of asthma. And so, what you see is that instead of minus 0.6, you get minus 0.53 or 12% bias. And so, the idea here is just to just demonstrate that using Windows to solve this problem or to address this problem does not can introduce bias. Can introduce bias depending upon the actual shape of the treatment curves and the control curves, and who is being left out of the windows. Sorry. So there had been a lot of work done on addressing assessment times. There's a thread of work that was called joint model. That was called joint modeling, where they jointly model the distribution of the assessment and outcome processes. And so here's a list of some papers that I've identified that starting with Lin and Ying in 2001. And there's a whole suite of papers working in the joint modeling space. And then there's a branch of work that does sort of inverse weighting approaches and And under the assumption of assessment at random, here's a paper by Lin et al. in 2004. And here's a suite of papers that have used this kind of inverse weighting approach. So just to formalize the problem and set up some notation, so here tau is going to be the end of follow-up. Okay, so it's going to be sort of towards the end of the end. So it's going to be sort of towards the end of the follow-up assessment time. The A and B will refer to the time period over which we want to draw inference about treatment effects. L is going to be this outcome process where, so Y of little t is the outcome at time little t. And then it's a process. And so you can think about defining the outcome over the interval zero to tau. The interval zero to tau. Then we have an assessment time process, n of t, which is a counting process that ranges that's going to be defined over the interval a to tau. And so that's going to be the time when this will jump when people show up for their assessments. I'll define a of t to be the indicator of being assessed exactly at time t. And we assume that everybody is assessed at time zero. assessed at time zero. And then A of t comma t plus epsilon is going to be the indicator of being assessed in the window t to t plus epsilon. And so the observed data for any industry, this is all at the individual level for a random individual. So the observed data is the union of the counting process for that person and then the observed outcomes. So it's going to be Observed outcomes. So it's going to be the Y's when the assessment at the assessments where the A process takes on the value one. And then O bar T I'll define as the observed data up to, but not including time t. And we'll refer to O bar T as the observed past. So we're going to define two different types of intensity functions. So when you're Of intensity functions. So when you're modeling counting process data, that's typically modeled using what's called the intensity function. And so we'll have an intensity function, which is like, is the rate of visiting at time t conditional on some information? And so lambda t, depending upon just O bore t, is going to be the conditional rate of visiting at time t given the At time t, given the observed past history, O bar t. We can also define another intensity function, which is conditional not only of the observed past, but the entire outcome process. So we define that with a row here. And so that's going to be the rate of visiting at time t, conditional on the observed past and the entire outcome process. So, one of the typical assumptions that's made in this literature is something called assessment at random, which is just a generalization of the missing at random assumption that is typically made in the missing data literature. And one way of expressing the assessment at random assumption, it basically says that if you take two subgroups of people who share Subgroups of people who share the same observed pass through time t. So, if you look on both sides of the equation, what we're doing is we are conditioning on the observed pass through time t on both sides. So if you take people who have the same observed past through time t, and then you break them into two subgroups, subgroup zero, which is are people who are who are not assessed at time t, and And a subgroup of people who are assessed at time t. What assessment at random says is that the distribution of the outcome at time, little t for people not assessed at that time is equal to the distribution of the outcome at that time among people who are assessed at that time. And it all conditioned on the same past history. And so these two subgroups, again, share the same past. These two subgroups, again, share the same past history up to T minus, but they differ with respect to who gets assessed. And the critical thing about the assessment of random assumption is that it's not testable from the observed data, just like missing at random is not testable. So because of this untestability, right, there may be other assumptions that are equally tenable. And so what And so, what we can do is define a class of assessment not at random assumptions or ANR assumptions. And basically, this follows the same literature in the missing data about doing exponential tilting. And it says, well, this subgroup zero distribution, the left-hand side of the previous equation, may be different than the right-hand side, that these might not be equal to each other. And so, what one can do is use an exponential tilting device that basically governs. Device that basically governs how this distribution differs from this distribution. And so that's what we do here: we multiply this by an exponential tilting factor, and that is properly normalized so that the guy on the left here is a proper conditional distribution function. And if you set this sensitivity, so there's a parameter here, alpha, that governs the deviations from these two distributions. These two distributions. And if you set alpha equal to zero, then that what we get back is the assessment at random assumption. And so alpha really governs deviations away from assessment at random. Alpha is not identifiable. That means you can't evaluate it from the data. And what we propose to do is just burpha in a sensitivity analysis. So here's a So here's a schematic representation. So if this is the again, so these, let's just assume that we have a specific past history, okay? And this is the distribution of outcomes for people who are assessed at time t. Imagine you had infinite data. And so this represents the distribution of the asthma control store for people actually assessed at time t. And then the exponential tilting device then either shifts this. Tilting device then either shifts this distribution to the left or to the right based upon the value of alpha. So if you let it be a negative number, what you're doing is you're shifting, you're saying that people who are not assessed at time little t are shifted towards lower values of the asthma control scale relative to people who are assessed. So the green is people who are assessed. By setting alpha equal to minus 0.6, let's say, then what we're doing is we're shifting the distribution. What we're doing is we're shifting the distribution for people who are not assessed to lower values of the asthma control score. Similarly, if you set the purple one here 0.6, you're shifting it towards higher values of the asthma control score. So that's what the exponential tilting device does. And if you set it equal to zero, then you get right back to this distribution here. So in order to So, in order to make progress, we need to make some additional assumptions. So, the first assumption that we're going to make to make progress here is to assume that the conditional intensity function, so the rate of visiting at time t, conditioned on the past history and the entire outcome process, doesn't depend upon all the outcomes. What it depends on is the observed past and the outcome. And the outcome at time t. Okay, so we're going to assume that this conditional intensity doesn't depend on all outcomes, it just depends upon the observed past time and the outcome that you're scheduled to be at time t. We're also going to try to borrow information across time. So, we're going to assume a model for the mean of yt, where say it's going to be some. say it's going to be some um we'll have some link function uh s which is invertible uh we'll have some spline basis b and uh we're going to assume that the expected value of y t is an invertible link function times beta times this spline basis. So we're going to make some smoothing assumptions about the mean of yt. Okay, and so if we do that, what we can do is write down an identification formula for the mean of yt as a function of things, features of the distribution of the observed data. So here's a formula. I'm not going to work out all the mathematical details, but the mean of yt can be written as with respect to this integral. And what you'll see on the right-hand side are only quantities. Are only quantities that are identifiable from the distribution of the observed data. So you can see here, for example, this is the conditional distribution of yt among people who show up at time t and the past history. And then this is just the distribution of the past history. And so this shows that the mean of yt is identifiable. And since that's identifiable, we can then write beta, the parameter of this. Parameter of this model up here, we can express it in this form as this integral. And so, what appears in here, so all these quantities here are you can, you know, and then we just plug in the amine of yt into the inverse of the link function, and you can see that we have that beta is identifiable. So, um we went used the semi-parametric. used the semi-parametric theory and we divide we derived an influence function for beta and so we have a theorem in our paper that shows that the influence function for beta looks a lot like it's like a it's like a continuous version of a typical augmented inverse probability of weighting assumption that you that we have in regular missing data problems. Regular missing data problems. So the first bit, you can see your inverse weighting by this conditional intensity. Here's your outcome, your integrating with respect to the counting process. And then you've got this conditional mean of yt given the past. So that's kind of the inverse weighting term. And then there's an augmentation term that appears in the influence function. We do We do in a special case when S is the identity link function, this influence function reduces down to a slightly simpler form. Again, looks a lot like an inverse weighted bit plus an augmentation term. So, when you look at these formulas for, let's say, for the influence function, you see here that you've got this. And you see here that you've got this conditional intensity here, which depends upon the observed past and the outcome at time t. And then you see it also depends upon this conditional expectation. It's not immediately obvious when I look at this influence function that how to express these things as a function of the distribution of the observed data under our modeling assumptions. And so we can show is that we can write down formula. Is that we can write down formula for these quantities in terms of the distribution of the observed data. And so the conditional intensity function that depends upon the observed past and yt depends upon the conditional intensity at time t given the observed past alone. There's an exponential term, and then this depends upon the conditional distribution of yt among people who show up at time t in the observed past. So I can express this guy here. So, I can express this guy here in terms of identifiable quantities from the distribution of observed data. And I can similarly express the conditional mean of yt given the observed past in terms of conditional expectations that depend upon distributions that are identifiable from the observed data. And I should say that all of these expressions involve the tilting parameter, the sensitivity parameter alpha. Parameter alpha. So, as you can see, there are two bits here that depend upon the distribution of observed data. It's the conditional intensity given the observed past and the conditional distribution of yt given at equals one and the observed past. And so, what we do is we model those guys. And so, this conditional intensity, we'll use a stratified Anderson-Gill model, which is a semi-parametric model. Model, which is a semi-parametric model, and then in our application, we actually used a fully parametric model for this conditional density. But one can use more flexible modeling, let's say like a single index model to model this conditional distribution. And then, in terms of estimation, all we do, we go back to, we can write down, let's say in the case where S is. Let's say in the case where S is the identity link, we can just plug in estimators here for these quantities, and we can sum over individuals, and we get a closed form expression for beta hat in the case where we use the identity link. And here's a formula for beta hat. And you can see here we have the estimated conditional means here and the estimated conditional intensity. Conditional intensity. So, in the paper, we established conditions under which we get root n asymptotics for beta. The estimators for the conditional intensity that depends upon the observed data and this conditional distribution can converge at slower than root n rates, but they can't converge slower than n to the quarter rates. So, we need to have. So we need to have, so we can use flexible semi-parametric models so long as they converge it into the quarter rates. And we're using a fully parametric model for this. When we use this, we have to do some kernel smoothing of the baseline intensity in the Anderson-Gill model, the stratified Anderson-Gill model. So, a big question with doing any kind of sensitivity analysis is: how do we select the sensitivity parameters? This is the big issue with sensitivity analysis. And so, domain expertise should be used to decide what's the range of reasonable values for alpha. And there's been a few papers recently that have been published in the college. That have been published in the causal inference literature that have explored how to select the range of sensitivity parameters in the context of unmeasured confounding in observational studies. So there's a paper by Sennelli and Hazlitt in 2020 where they make a couple of statements that I think are relevant. So the first statement they say is perhaps the most fundamental obstacle to the use of sensitivity analysis is Sensitivity analysis is the difficulty in connecting the formal results to the researcher's substantive understanding about the object under study. So I totally agree that these sensitivity approaches are there are big obstacles to them because people don't really know how to set the values of the sensitivity parameters and we don't do a very good job of connecting it to what researchers really understand. And that the bounding procedure we should use depends. We should use depends upon which quantities the investigator prefers and can most soundly reason about in their own research. So that's really important. So this is the paper by Sennelli and Hazlitt, and there's a paper by Franks in JAZA, and then a paper by Veich and Zaveri, all in 2020. And they proposed ways for using the strength of the impact of a key COVID. Strength of the impact of a key covariate or group of covariates, xj, giving the remaining covariates. And they try to reason. So they take measured covariates, they throw a bunch out, and then they look at the strength of the other covariates to create a bound on the strength of unmeasured confounding. So they're trying to use the observed data by discarding certain covariates and seeing what they're. Certain covariates and seeing what the remaining influence is to create bounds on the sensitivity parameters. And so I think there's a number of issues with this approach, but I think most importantly for the setting that we're dealing with, the impact of any group of variables in the observed past on the assessment of time t may actually be weaker than the impact of yt on the assessment of time t. On the assessment at time t. Like, I don't think their approach in the causal inference setting will extend well to our setting. In fact, the very reason people might not be showing up in a study is exactly because of the outcome under investigation. That is, the person's not showing up at time T because they have an asthma exacerbation in their hospital and they can't undergo the survey under the survey, right? So, the actual impact of YT on assessment. Of YT on the assessment at time T actually can be stronger than anything in the observed past. So instead, what we do is we propose to query domain experts about what do they think like a reasonable value for the mean of YT, the actual object of interest, is at any time. And, you know, are there certain values of these guys that are completely Completely implausible. And we treat any value of alpha that generates a mean that's outside the min and the max of these possible values, we would exclude those from the sensitivity analysis. And we can do this separately by treatment arm. And we believe that this is aligned with the recommendations of Sennelli and Hazlitt, because it is a quantity that they can reason about. They can think about what. About. They can think about what, you know, the mean asthma score in a population. And so I think this is more connected to what investigators can reason about. It's not perfect, but we think it's a reasonable way of moving forward. So here's some results of the analysis of the ARC study, the ASHMA research community study. So here, what I'm showing on the top. What I'm showing on the top is so these are the estimated mean values of the of the outcome, so asthma control score for the this is the treatment group and this is the control group. And these are various values of alpha ranging from minus 0.6 to 0.6. And remember that higher values of asthma are more severe. Severe. And so these dashed lines show us what are values of the mean in the population that are reasonably plausible to the expert. So it's three and it's, I can't remember, this is like 1.2 or something like that. And so then these are my, so this is the estimated mean as a function of time for under assessment at random. Assessment at random. And then these are for various values of alpha. And so you can see that, except for this extreme one around 0.6, all of the estimated means fall within the regions that they consider reasonable. And the regions they considered reasonable didn't vary by treatment group. So here's, so this is for the So here's for the control group. This is the estimated means. So again, this is the assessment random. This is 0.6. And this is minus 0.6. And so clearly, a value for the control group of 0.6 really doesn't align with their what they considered reasonable. So down here, I'm just choosing those specific times, which were the Times, which were the targeted assessment times for the trial. And then I have estimates and I have 95% confidence intervals for each of these specific values of alpha. Now that just tells you the estimates and measures of uncertainty separately by treatment arm. But of course, we can compare across treatment arms. And so what I have here is I let the sensitivity parameter Let the sensitivity parameter for the control group can take on a different value than the sensitivity parameter for the treatment group. And then we can look at every combination of sensitivity parameter for the control group and the treatment group separately by treatment arm. And we can, for each combination, we can get an estimate of the treatment effect. And so here's the estimate of the treatment effect at At um, at assessment at random in both arms. So, this is these, this is this panel shows the estimated treatment effect as a function of these sensitivity parameters. This is assessment random, and then these are just estimates, and then this looks at the confidence intervals, so the lower end edge of the confidence interval, and so in this region here. And so in this region here, we would conclude, I think, that the treatment was better in this region. In this region, we can conclude that the control group is better. Under assessment at random, we would say we don't have enough evidence to conclude that there's a difference in this white region. And this is for six months, and then you can do the same thing at 12 months. And again, we don't have enough evidence, certainly. We don't have enough evidence, certainly at assessment at random, to conclude that there's an appreciable treatment effect. And then we'd have to have actually quite differential bias going on or differential deviations from assessment at random in order for us to conclude, right? The signs have to take different values. And so I would say this is kind of a robust find. Is kind of a robust finding that there's nothing going, you know, we don't have enough evidence in this particular study to say that one treatment is better than the other. So just in terms of discussion, I know I'm finishing early and I'm sure you all appreciate getting out earlier. Just in terms of discussion, I think the idea that irregularity of assessment times is largely ignored in applied work. We don't see many applied papers really. Applied papers really using the actual assessment time in the analysis of the data. We do see lots of examples of creation of visit windows and then just using standard missing data methods. I would argue that there's absolutely no reason to be creating these artificial reasons in the analysis. I get like from a logistical perspective, when you're running a trial, you can't get everybody to come in and Trial, you can't get everybody to come in exactly in time, so you do need to have some, you know, measure of creation of some kind of logistical windows for the collection of data. But then when it comes to the analysis, I don't see any reason why one needs to use those windows. Methods have been developed for analyzing trials with irregular and potentially informative assessment times that have been around for 20 years, you know, starting 20 years. For 20 years, you know, starting 20 years ago. And like missing data methods, they rely on untestable assumptions. And as with any method that requires untenable testable assumptions, you need some kind of sensitivity analysis or some other assumptions need to be brought to the table to make progress. So we worked out a lot of the details for the identity link in this paper, but the This paper, but the ideas naturally extend to other invertible link functions that one might want to use. When we did the kernel smoothing of the intensity functions, we just chose the bandwidth focused just on the intensity functions, you know, based on intensity itself rather than the target parameter of interest. And so there may be better ways of choosing. Well, there are just human beings. There might be better ways. There might be better ways of choosing the bandwidth that actually target the parameter of interest. We've noticed this a lot in a lot of our work that when people use sort of standard influence function based variance estimates that they don't necessarily in complex problems, they don't necessarily work that well in reasonable sample sizes. So we have tended to use So, we have tended to use symmetric bootstrap T confidence intervals. And these confidence intervals tended to work well, but they did have some slight undercoverage at larger values of the sensitivity parameter. And so I think there needs to be more work in that space. And we might want to investigate using double bootstrap or infinitesimal bootstrap procedures. And so that's research that's undergoing. Research that's undergoing. Anyway, I'll stop here and I will look to see. So Shu, see if there's any comments. If we do not consider a sensitivity analysis first by setting alpha, will alpha be identifiable and estimable? I'm not sure what you are asking, Shu. I don't think alpha is identifiable. Is identifiable from the data. Okay. Then do you think there are some conditions that can make our file identifiable? Well, maybe if you impose some additional assumptions or bring in, if you found some other, you know, you work in this area, you know, some other variables that where you impose some independence assumptions. Independence assumptions, or there's some assumptions, possibly an instrument of some type or a negative control or negative might be brought to bear, but that would be an interesting to look at. Do you have any?