Good morning everyone. My name is Emma from Emory University. So before I start, I really want to thank organizers for putting together this wonderful workshop. I really learned a lot. I would characterize myself as a newcomer to the field. I work more in statistics, but through my collaboration with my wonderful clever Honu and his talented lab members, I've come to really appreciate the complexity of the biological problems that we're trying to look at. So today's talk, I'm not looking at anything too fancy. Looking at anything too fancy, we're just estimating code expression networks. I know Haiyan already commented that there are many issues with estimating code expressions or just correlations from CNSR and SIG data. But under the premise that if this is something that we're trying to do, how do we go about it? And it seems like a rather straightforward problem. You're just estimating correlations, but we actually realized this problem is not as simple as we thought it would be. So hopefully through this talk I can point to some issues already into us, some solutions and we're looking forward to address those. So, click expression networks probably don't need any more introduction. They just characterize this correlation of check expressions across different biological samples and unique opportunities that open up with single cell RNA seq data because you allow you to look at these co-expression relationships in a cell type-specific way. And this assume in the simpler setting where the cell cells have already been annotated, so the task is rather simple. You have the cells, you have the genes, and you're trying to just get a core. You have the genes, and you're trying to just get a correlation network. And the first thing you can think of: okay, you have the cells and the genes, just get the correlation network, right? Just get the correlation matrices. You can use whatever correlation matrix that you like, metric you like, like Pearson, Spearman, or some of the fancier ones. But it turned out to be not so simple because the different cells are sequenced at different sequencing depth. So here, just a simple example where using those single-cell RNC data from the brain, we randomly sampled a bunch of cells with very simple. Randomly sampled a bunch of cells with very significant depth, and we also randomly sampled four genes. And you notice here: for cells with high sign depths, genes would have higher counts. For cells with low signaling depths, genes would have lower counts. So even though two genes could have been independent, just because they have higher counts in the cells that have high signaling depths and lower counts in the cells that have low signal depth, so it would have appeared that they have correlations in their counts, but these correlations might just be due to the confounding factor. Might just be due to the confounding factor of the signal depth variations across all these cells. For example, for these genes, we have a correlation. Tristan correlates are very high, but what does this correlate really mean? Because it could just be due to the variations of the depths across the cells when these two genes could be in fact not very correlated at all, where it could be independent of each other. So it seems like directly calculating the correlation of counts is not a good solution. Is not a good solution. And then think, okay, maybe we can just do some normalization to remove the effect of variant season depth. And that is also not so straightforward. So we first look at some marginal normalization. And this, again, this is in Norway what we were doing, but it's just simple to example to kind of help gain some insights into the challenge in this problem. So what we're doing here is that we just simulated an independent gene pair and these are. Gene pair. And these are the plots of UI accounts of gene one against gene two. Again, they're independent with a simulation setting. And you'll notice that data is not continuous, they're actually discrete because they're count data. And all these points actually have a lot of cells populated there because, again, these are counts. So each one of these stars actually have a lot of assimilated cells there. So let's say for one of these bivariate integer locations A and B. Integer locations A and B, you have a bunch of cells with varying sequencing depths S1, S2 to Sn. So if you divide each of the all the counts in one cell by the corresponding sequencing depth, what you have is that for these endpoints, for end cells that are all populating this location, they will turn into endpoints. And each one of them is scaled by the corresponding sequence in depth. So here I'm just scaling by just doing a simple standardization by just dividing. Standardization by just dividing the cell that counts by the sequencing depth. And so each point will turn to endpoints, and you will see these endpoints actually stretch out to a line with slope B over A and intercept A, intercept 0. So what this says is that this point, for example, 1, 1, would actually just turn into a line of cells with intercept 1 and slope 0. So after you scale these counts by sequencing depth, these independent UMI counts will stretch. UMI counts will stretch out to many of those lines pointing in this direction, and this will actually artificially inflate the correlations. And when you calculate the appearance of correlation of these scaled counts, actually, they will be significantly correlated. And this issue is not addressed, but if you look at a different way of normalization, if you look at the log normalized counts again, a particular point where you have in cells populated there after the log transformation, they would again stretch out to align with a different intercepted slope. To align with a different intercepted slope, and again, would give you artificially inflated correlations. So, again, these are kind of just simple steps that we took and realized there are challenges with this problem. And we also look at a lot of the existing solutions, including just apply generic correlation to log normalized data. And here we have selection methods that aim to estimate co-expressions from single-cell RNA sec data. We're estimating expressions from single-cell RA-C data. Expressions from single-cell RSC data. For example, like SD transform or add Pearson residuals. For these methods, we'll just take the estimated expression levels and then take a Pearson correlation to get the estimates. So we'll put a row there because the original methods inflex estimate expression and we take a correlation of those expressions to give the co-expression that we're estimates. And lastly, there's this wonderful method developed by Zendus and her student, Sandus and her student Shan Lu, but we didn't really have time enough time to add it to our comparison, but that's something that working on right now. So, in today's talk, we'll focus on comparison with the methods, all of these other methods. So, here we kind of compare all these methods in the example where we created null data. So, what do I mean by null data? Null data is just a data set where genes are not expected to co-express. So, to create To co-express. So, to create this null data, what we do is that we take real single-cell RA-seq data and we do a permutation. What we do is that we have all these cells and you have the genes and you have their single-seam depth. So for each cell, we get the relative UMI counts of each gene. And then for each gene, we just permute these relative counts across all cells. And we do this independently for each gene. So previously, two genes could be correlated, but if we have to do this independent permutation across all cells, Independent permutation across all cells, they will be decorrelated and their expressions are expected to be zero. So after we do this permutation for HG independently, we would then multiply back the sequencing depth to give the counts. So here we consider two settings where we keep the sequencing depths constant when you multiply back to these permutated relative counts, or you have the sequence depths varying across the cells when you multiply back to the relative. Multiply back to the relative counts, the permutated relative counts. And you can see for all these methods, when the sequencing depth is kept constant across the cells, estimated for expression are all pretty accurate and zero. So the color is not too well, but it's actually blue, and a lot of them are actually very correct. The overlap is black line, which shows brown truth. And when you have very exciting depth across cells, you can see for these methods, it gave you. For these methods, it gave vary degrees of bias estimates, and these biases change with the expression level. So, the x is the expression level. So, when the expression level is low, for example, experiment would have very high bias, but when the expression level is high, it actually performs quite well. So, again, whether the sequence depth is very constant, these genes are not supposed to have co-expressions, but you do see the estimated co-expression to be quite high when you have these sequencing depths. When you have this variation profounding issue, so this issue is not really well addressed in these methods that we're looking at. And so, this is more like when the genes are not co-expressed, you ask them to be so. So, this is like false positives. On the other hand, when the genes are truly expressed, when you measure their correlation, there's also this issue of attenuation. Because what happens here is you have the underlying expression for the genes. When I say underlying expression levels, I mean like the relative cancer. Expression levels, I mean, like the relative counts. And then UMI counts is actually measuring the underlying expression levels, and these are the counts. So when you measure the underlying expression level using a count, using UMI counts, you actually inject measurement errors for noises due to this measurement using counts of underlying expression levels. So if two genes, for example, have a correlation that's very high, 0.9, but then if you measure these expression levels, get some counts, then the correlation. Level is given some counts, then the correlation will be deflated, much lower. So there is this attenuation by these measurements due to the UMA counts. And here we just generated co-expressed gene pairs where the true co-expression level is 0.5. And you can see for these methods, they all have varying degrees of bias. So the true value is about 0.5. And when the genes are lowly expressed, the bias is particularly high. Particularly high, but as expression level increases, the messes seem to improve in their performance in terms of estimating the true co-expression level. So, these kind of points to two issues. One is reduced power in detecting true co-expressions, and also you have false positives. When the genes are not co-expressed, you ask them to be so. So, in our kind of work, we try to kind of find what possible solution that can kind of elevate some of these issues that we saw. That we saw. So, what we considered is like when we have this problem, we first want to read out mathematical model. So, I think we think like a quite sensitive model, a sensible model is the expression measurement model. So, here you have N cells and you have p genes, and you assume expression levels of the p-gens are from a multivariate distribution. We don't specify what a distribution is, it just generates non-negative p-variant distribution, and this very P-variant distribution, and these were summing up to one. And then, given the underlying expression level Z, the UMI counts are given by a Poisson measurement model that's subject to the sequencing depth. So, these XIJ are the UMA counts we observe. These Z's are the latent expression levels that we're trying to measure, but we don't know what they are. And X measures Z via a quick measurement model. So, this is what we call expression measurement model. And when we talk about correlations of quickness, When we talk about correlations or co-expressions, we're actually talking about the correlation of the underlying expression level that's not subject to the variations of supersonic depth across cells and also not subject to the Poisson measurement error or measurement points. So this gives you kind of a more a better view of the true correlations co-expressions across different biological samples. So we want to estimate this correlation matrix. There are many things we can do. We can put on a current matrix. Do we can put on a parametric assumption on f and then we can estimate using like this foot that it's going to be very slow. And also, we don't really want to put any parametric assumptions on f, we want to make it more flexible. But I do want to add, if you assume f is gamma, and then you measure a gamma with Poisson, then the marginal distribution of x is actually negative binomial, which is something that we will use a lot to measure counts. So, what we do is that, okay, we're now trying to really model the distribution. What we're interested in actually a much We're interested in actually a much simpler summarization of distribution, which would just need the moments. We just need the first and second moments to get this. So we'll just look at the moments. And that seems to provide an easier solution than characterizing the whole distribution. So here, we're just defining the quantities we're interested in. So these are the underlying expression levels that we don't observe. And we have the mean, the variance, and co-variance. So these together give us the co-expression matrices across the PGs. Other PGs. And then, once you have these first and second moments of the underlying expression level, you can calculate because we're just measuring this with the Poisson. So you can calculate what is the first and second moments of the count. So this is vector value of the counts. These are variance and covariance of the counts. And you can see these are actually functions of the CBS in depth, which vary from cell to cell. And then you also have these parameters that are actually coming from the underlying expression level. eronal expression level. So, what does this really mean? This means we can estimate these quantities that we're interested in via counts. And what is that? How we do that? You just write out the regression equations, right? On the left-hand side, these are the counts, these are the variances of the counts, covariance of the counts, all of which we calculate. On the right-hand side, these are the parameters that we're trying to estimate. It summarizes the mean of the expression levels, the variance of the expression levels. The variance of the expression levels and the covariance of the expression levels. These are just mean-zero random errors that are heterostochastic. And this S, this is in that something we know. So this kind of acts as covariance for what goes into the design matrix in your linear regressions. So you can easily estimate mu, get that in here, estimate variances, and get those in here, and just get the covariances, and you can put together your co-expression matrix. And this estimation can be done very fast. And this estimation can be done very fast, even though these are done pair by pair. But you can see these design matrices actually are the same across all these regressions, so can be computed rather efficiently. And we also want to advocate using weights in these estimates because different samples are very different in their variance. So by choosing a properly defined weight, that really reduces the variability of your estimates. And that will be the detail to the paper. And we also develop a test that's We also develop a test that's valid and NRML is a syntactically normally distributed. So we have this method. First, we want to see how fast we can do it. And it turns out the method is very fast. So this is based on 500 genes and 5,000 cells. And you can see the computing time of our moment-based methods comparable to just doing Pearson experiment of the log-normalized data. So it's very, very fast, where it takes about ten seconds to compute. Whereas some of the other more sophisticated methods Some of the other more sophisticated methods take much longer, and these are computed on 0.2% of the cells. And we show that the estimate is not confounded by various depth because we explicitly model that. And we also gave, say, unbiased estimates, even though the expression level is low, the variability is high. But we also saw that across all the methods, you have the smallest MICE. And then the students did. And then the students did extensive validations using existing single-cell RNA-C data. So for the sake of time, I will probably not go over this. For those of you who are interested, I'll be happy to talk about it or refer you to the paper. What they found is that the CS4 gives more biological interpretable results. The results are more reproducible against independent single-star RNA-seq data. They also have a higher overall. We also have a higher overlap with known TF target pairs. And we also found interesting kind of up or down regulated genes for AD Ozma P samples and COVID-19 blood samples. And a lot of work is done by Chang, who is going to start as an assistant professor at Emory University, and a couple other lab members from Home East Lab. And the paper is on Bar iPad from last year. And we have our packages and Python for implementing this. How do you explore the effect of dropouts? Perhaps you can show up with data model.