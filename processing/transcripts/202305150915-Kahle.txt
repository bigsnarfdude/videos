Yeah, I was just giving the welcome and yeah, saying that it was unfortunate that some people cannot join us, including you, Thomas, but we're still happy to hear what you had to say. And hopefully, you can inspire us to have discussions here and yeah, also then talk to you. And yeah, so I think we can. We can get ready to start. So, I think. So, I want to call out. I see Anthea already there. She's a co-organizer. So, she cannot be here, but she's going to be helping us with the online part, as well as Elena. I don't know if she's there, but Elena too. And Barnd is, as I understand, right now flying to Mexico City, and he should join us in the afternoon if he manages. Afternoon, if he manages, um, yes, so I think uh, we have everything set up, uh, Jose. Uh, one thing, so I want to reiterate, so if you are speaking, uh, please send your slides to Jose at this email. And we also have in the schedule some impromptu talks and discussion sessions where we invite you to, if you want. To, if you want to say something, so we like have like a 20-minute slot where you can tell us about the research you're doing that connects to the workshop or propose new problems or things that you have been thinking based on the talks that have happened in the morning. So, if you want to, so please, we're going to have in one of the next sessions, we're going to maybe pass around a little sheet of paper. Pass around a little sheet of paper and you can put your name if you're interested. Okay. Maybe, Carlos, maybe I just add the clarification that this is for in-person participants only. Although everybody joining online is, of course, very welcome to dial in and listen to these. Yes, that's correct. Yeah. So. Okay. So I think everybody, everything is set up. And maybe we can then start. And maybe we can then start with the first talk by Thomas. And he will tell us about the classification of five gauss and coming with fairness, combinatorics, and reality. So thank you, Thomas. And we can hear you, I think. Okay. Hi, everybody. Also in the room. First, let me apologize for not coming. I was already at the gate and the first lag was cancelled. And then I was faced with the decision of, you know, Was faced with the decision of going home or 48-hour trip with multiple stops in the US, going to that flight that some people are planning to take, and that's overbooked now. So yeah, okay, it's unfortunate, but that's the way it is. Okay, so speaking about five Gaussoids, so I want to talk to you first about Gaussoids, these combinatorial structures, and then what the classification of those might mean. Classification of those might mean, and why I'm interested in. And this is not a talk about research that has been done, it's more of an outlook of what I plan to do. And I try to convince you that it's something worthwhile. So one thing that I like to do these days is to talk to ChatGPT about mathematics. So when I, maybe five years ago or so, when this AI and robotics revolution started, I think Revolution started. I think bus drivers and tram drivers were the people who are worried that their jobs might be automated away. But, you know, these days I feel like teaching computer science one, mathematics for computer science one might also be one of the jobs that's automated away very soon. But ChatGPT doesn't know what a Gaussoid is. So that's what this slide is for. I ask it, do you know what a Gaussoid is? And according to its knowledge, it's some tool. To its knowledge, it's some tool in quantum mechanics, and that's not the case. So you have to listen to the talk and find the right answer. However, if this concept has been significantly expanded or redefined after 2021, it wouldn't know, but to my knowledge, it's older. Okay, so let's learn what a Gaussoid is. So the Gauss in Gaussoid, of course, refers to Gauss, and we like the Gaussian distribution. You know it all. Know it all. It's a multivariate normal distribution, describes vectors in Rn. And in the former currency in Germany, the Deutsche Mark, this was even honored by putting, so maybe if I zoom in here, you can see it, the univariate shape of the univariate normal distribution on this bill along a picture of Gauss. Okay, so wait, what's the best? That's the best. Okay, so you all. Okay, so you all probably also remember the density of the Gaussian distribution. It's a parametrized family by the mean, which is a real vector, which will have no significance for this talk. And then a positive definite covariance matrix, which will have a lot of significance for this talk. And so it enters here in this quadratic form. More precisely, the inverse enters in this quadratic form. So this is of the form e to the minus the quadratic form, which is also why it's integrable. Y it's integrable, and that's the inverse of the covariance matrix defines this quadratic form. Okay, and so here's an example for a multivariate distribution with structure. Let's say you have three random variables and the first is just independent. Well, the first is just normally distributed with some variance, let's say epsilon one. And the second is the sum of x1 and some. Sum of x1 and some independent noise independent of the first noise. And let's say the third is of the same structure. It's also x1 plus some further independent noise, completely independent of the two other noises. Then, as you might know, we have the whole vector is multivariately normally distributed, and its covariance matrix has a particular structure that we can see here. And the quadratic form has two zeros that are. Has two zeros that are marked in red here. If you remember linear algebra, then you know that these two zeros come from two vanishing two by two minors, namely the one that's formed from those four entries and the one that's formed from those four entries. That's probably known as Kramer's rule in most linear algebra classes or the adjoint formula for the inverse. Inverse. And in algebraic statistics, we know that this tells us these zeros in the inverse covariance matrix tell us something about conditional independence, and zeros in the covariance matrix tell us something about marginal independence. And here, for instance, in these zeros, we can see that x2 and x3 are conditionally independent given x1, as we might expect. Yes, so what goes into here is So, what goes into here is x1, and we have this conditional independence for all distributions in this family. Okay, so there's a general theorem, but the motto is that the covariance matrix encodes the independence in its minors, so in the subdeterminants. And here's how the theorem looks: it's in the well-known book by Seth Sullivan. So, let's say now. So let's say now you have n random variables, okay, and you pick two of them, two indices i and j, and then some other set k of the remaining ones to condition on, then we can learn from this theorem that xi is independent of xj given this collection k, if and only if some determinant in the covariance matrix vanishes. And I call it an almost principle. And I call it an almost principle minor because it is, well, almost a principle minor. A principal minor is a subdeterminant where we have the same rows and same columns. And here we take the difference of one. So we take the rows and columns corresponding to the set K. And then we take the row I and the column J. So that's almost principle, but not quite, because there's this one index different between the rows and columns we. Different between the rows and columns we take. And if I go back to this slide, you see here that this minor, for instance, it corresponds to rows one and two and columns one and three. So one is shared and two and three are the i and j in this case. And the corresponding statement is x2 and x3 are conditionally independent given x1. X1. One is the shared and two and three are the I and J for this example. Okay, also note that off-diagonal entries of the matrix are also almost principal minors. If k is empty, which is allowed, I get sigma ij. And that would be a marginal independence, a non-conditional. A non-conditional independence. Aij give nothing, give an empty set equal to zero, means an off-diagonal entry in position ij vanishes. Okay, so that's where algebra comes into play, and we can use algebra and also combinatorics of these symbols to learn something about conditional inner. So, let's do some counting. How many of these symbols are? How many of these symbols are there? How many almost principal minors do I have? So I need to count the, I have two choices for i and j. And then I need to pick this set k out of the remaining n minus 2. So I get n choose 2 for the i and j and 2 to the n minus 2 for the subsets of the rest. And note that this is exactly the same as the number of two-dimensional phases of an n-dimensional cube. Of an n-dimensional cube. Why? So, if you want to specify a two-dimensional phase, let's call it a square in the n-dimensional cube, you have to tell me in which directions does the two-dimensional phase extend. That's two directions. You need to pick two directions, like this. And then once you have picked these two directions, you can place your face in your square on the front. Square on the front or the back of the cube, if it's a three-dimensional cube. But if it's a four-dimensional cube, you can put it like in four different positions. So you can put it in two to the n minus two different positions once you've specified the two directions. So the number of almost principal minors or conditional possible conditional independent statements agrees with the number of squares in the n-dimensional cube. And that's a useful combinatorial analogy. A useful combinatorial analogy. I did the math for you. So for n equals three, there are six almost principal minors. As you know, the three-dimensional cube has six squares. For n equals four, there are 24. And for n equals five, they're 80. And now we're going to talk about subsets of those, because I want to talk about collections of independent statements. Not only one independent statement, I want to combine them. I want to say one is independent of two given three and three is independent of two. three and three is independent of four given five. So I need to talk about collections of those. So we're going to unfortunately work with power sets of like an AT element set. So that's kind of a big data, maybe not the big data that they have in mind at OpenAI, but it's the big. Okay, so if you don't like statistics, well, everybody likes statistics, then you can see this as a fundamental linear algebra problem also. Linear algebra problem also. So you have a symmetric n by n matrix, and let's say its principal minus do not vanish. That's by. So if we have a covariance matrix, we assume that it's positive definite. But for most of what I do here, it's also interesting to consider just that the principal minors do not vanish. Yes, positive definite, principal minors are all positive. And principal minors do not vanish, you can divine over any field. So if you want to do this. Any field. So, if you want to do this more algebraically, you might want to consider this condition. Condition which we call principally regular. It's a phrase that Tobias Berger coined. Is Tobias in the room? Maybe Tobias is in the room. So he worked really hard to find the right word, and I'm very happy with this principally regular definition. Okay, so are there any conditions on which almost principal miners offers? Which almost principal minors of a matrix can vanish at the same time? Maybe any combination is possible. And not any combination is possible. Here, let me show you an example why there are some conditions. Oh, oh. Am I still online? We see you, but we don't see the slide anymore. Okay, I hear you, and nothing is moving on my screen. Nothing is moving on my screen. Maybe you can reshare screen. Yeah, yeah. Yeah, the screen is just black, but. Okay. Also, no Zoom control does anything anymore. Okay. Okay, you still hear me? I mean, yes, it seems my computer has crashed. I mean, it seems my computer has crashed. I mean, nothing moves anymore. Interesting. Okay, we still see you on camera. I'll fix it somehow. Well, I don't know how. Is it just Zoom that's crashed or the mobile computer? Zoom that craft or the whole computer? I can move the cursor but not do anything else. Oh, you can try to restart. Yeah, I guess that will be the  Okay, so I think he's restarting. We can wait. So, Tobias, do you have any comments on how you came up with principles? How you came up with principally regular already? It should be. Can you hear Tobias? Okay, Lohamas is back. No, no? You can kind of hear her. I can hear you, and I'll be right back with the screen share. Okay, great. Okay, so I just found, you know, there are multiple people calling this principally regular multiple things. There's principally non-singular and Non-singular, and I don't like the non-singular part. And recently, I met somebody history of principally regular. Yeah, recently, I met somebody who calls it totally non-singular. I don't like this either. But maybe totally regular would be my favorite now. Lots of combinations. I believe totally. Yes, yeah, so yeah, but just giving a bit of context on the principal irregular. Okay, so the screen share doesn't work anymore. Uh-huh. What is this? Do you need to be allowed to share something? No, it's before that. I am allowed to share. I'm transmitting the iPad screen somehow to the computer in a different way. Okay. In a different way. Okay. Yeah, we see your second camera. See your second camera? I try this now. It's different. Let me see if I can get it to work. Okay, we see something. Okay. Let me get rid of the How do people get rid of this preview of the Zoom meeting you're in? Preview of the Zoom meeting you're in? You swipe it to the right. Oh, you mean the picture? Yeah, okay, got it. Thanks. Okay. Oh, sorry. Okay, so I wanted to explain why there are relations. Yes, why cannot all sets of minors vanish? And the one reason is very simple because you have like many, you already know many relations among the almost principal miners because I already told you that entries are. Because I already told you that entries are also almost principal minors. For instance, this almost principal minor 2 by 2 that I've wrote here is a polynomial in the entries. And from this relation, that just the 2 by 2 determinant, it is what it is, you can make some conclusions. For instance, if A31 is equal to 0, then this term would vanish. And at the same time, AT23 is equal to 0. At 23 is equal to zero, then you get that their difference, the difference of these two terms, this determinant, must vanish. So that's an implication, that's a logical implication for the vanishing of these almost principal minors of different sizes. Okay, so Laplace, that's similar to Laplace expansion, just for symmetric matrices. So there are many of these relations. So Laplace expansion would be a relation where you express the n by n determine. Where you express the n by n determinant as a polynomial in one by one determinants and n minus one times n minus one determinants. Yes, entry times the subdeterminant plus entry times the subdeterminant. You could view that as a polynomial relation among the minors of a matrix. And so when it comes to these relations among almost principal and principal minors of symmetric matrices, it's not entirely clear what, for example, the ideal is that contains all these. That contains all these relations, but we're relatively close to that in a paper. Okay, let me also point out one other conclusion. So these implications, they can also be of a different type. So if you have A3, 1 vanishing and A13 vanishing, then you can infer that this product vanishes. So you get an OR condition. So that can also happen. The product vanishes if and only if. Product vanishes if and only if one of its factors vanishes. So the types of conclusions you can get can be of this form that they contain the logical or. And you can also do it with signs. Maybe let's skip the sign analysis for now. All right. So you write down, and not you, but Radin Deniska and Fevo Matic did it. Radien Deniska and February Matic did it in 2007: a bunch of obvious necessary conditions of this fall that come from three-term relations, not only the one for two by two determinants, but also for higher determinants, but those relations with three terms. And this becomes, this exercise, if this vanishes and this vanishes, a third thing has to vanish, becomes an entirely combinatorial structure. It says if this conditional independence holds, Conditional independence holds, and this, then a third must hold, or if this symbol is zero and this symbol is zero, then this symbol is zero. Or if you do it with two phases of the cube, it says if this two phases is there or is in and this other two phases in, then a third or maybe one of two must be in. And so this led them to the definition of a Gaussoid, which is a combinatorial abstraction of the possible independent condition. Independent conditional independences that can hold for multivariate normal distribution and a necessary condition for the set of symbols that are zero. So the symbols that are in the Gausset always correspond to the conditional independencies that hold or for the almost principal minus that are zero. And here's a reformulation in terms of combinatorics that I think Tobias developed. So a Gauss is a set of two phases of the n-dimensional cube such that for Dimensional cube such that for each three-dimensional cube, you have the following implications. If G contains two faces that, can you see me, meet at an edge, we call that a knee, then it also contains the other two that are needed to wrap once around the three-dimensional cube. We call that a belt. Yes, if you have two that meet at an edge, you will get the entire belt that goes around the cube. Around the cube. And the other one is the one that contains the OR. If you have two faces that are opposing to each other in the three-dimensional cube, so that's an entirely three-dimensional definition that we're making. So it's easy. You have one of the two belts that contains the two opposing faces. So if you can see my hands, you have these two opposing faces. Either you go around front and bottom, or top and front and back, or top and bottom. One of the two belts must be contained. Uh, must be contained, and that's the OR condition. Okay, so that's a Gauss. A Gauss is a set of two phases of the n-dimensional cube that for each three-phase satisfies this. Or it's a set of conditional independent statements that satisfies whatever these things. If something is zero, then this other thing that corresponds to these axioms must also be zero. And this holds. If you have a positive definite matrix or principally regular matrix over any field that's symmetric. Matrix over any field that's symmetric, and you consider all the sets, all the statements that hold. I wrote this down here. If you consider all the symbols such that the minor is actually zero, then it satisfies these axioms, but not the other way around. So the whole action is about the other way around. So it's a necessary condition that to characterize, if you want to characterize these sets, the sets of simultaneously vanishing almost. Simultaneously vanishing almost principal minus if you go through all matrices, then a necessary condition that it be a Gauss. And note that it's built from this three-dimensional. It looks only at the three minors, like we would say in Gauss theory. And so the oid in Gaussoid refers to, reminds us of matroid theory, because it's a similar combinatorial abstraction of independence, in this case, conditional independence, to a In this case, conditional independence to a combinatorial structure, like linear independence and independence in graph theory, are abstracted to matroid theory. So together with Alessio Dali and Bernd, we had a paper a couple of years ago that's called the geometry of Gauss. It picks up on this 2007 work of Linischka and Matos and shows that these Gaussoid axioms that they developed actually Gaussoid axioms that they developed actually come from three-term relations between the principal and almost principal minors. So, the high-tech way to say that would be on the Lagrangian-Gauss minor. And as in matroid theory, you can use that to define variants like oriented and valuated Gaussoids. And there are many more parallels to matroid theory. For instance, there's a nice theory of duality and minus. That's entirely combinatorial, but it also relates to taking sub-matrices. It also relates to taking sub-matrices and show complement on positive definite covariance matrices. So it descends from symmetries on the Lagrangian-Grassmannian that were known due to work of Olga Holtz and Bern Schopel. So it's also the case, as in matrix theory, that there are non-realizable Gauss. So you can satisfy these axioms, but there's no positive definite matrix that realizes this structure so that it's all. This structure, so that its almost principal vanishing minus are exactly your given structure, although it's a Gaussoid. It can happen even that no complex, principally regular matrix is possible. So they're non-realizable Gauss, even if you're over the complex numbers. And the complex numbers are, in fact, the most flexible ones. So there are actually non-realizable Gauss holds over any field. And Gaussian graphical models, I want to. Graphical models are one-to-one to positive Gauss holes, but I didn't want to talk about orientations in this case, but they appear, they appear rarely, but Gaussian graphical models are very useful to constructions. And we used SAT solvers, so computer programs from computer science that solve satisfiability problems of Boolean formulas, and we found them very useful to enumerate and count Gauss. Count Gaussoids. So we have this website gaussoids.de where you can download your Gaussoids. For instance, on the right, you see the three Gaussoids, one per line, and there's some encoding, which so these six digits in each line are the six almost principal minors of a three by three matrix and zero means either they are there or not there. I forgot. I can infer from this that it means they are. It means there zero means what you think it means. Yeah, zero means that the conjugate independence holds and the minor is zero. Okay. So we you might now ask the questions like how many Gaussoids are there? And together with Tobias, we also studied this question. So this whole business is doubly exponential. So the number of subsets of Subsets of two phases of the n-cubes is doubly exponential, and the number of Gaussoids is also doubly exponential. You can somehow see the order of magnitudes. So I took the logarithm so that there's only one exponential, and then it's somewhere between n times 2 to the n and n squared times 2 to the n. So that's the order of magnitude. So there are many, many Gauss. That's unfortunately. And the realizable ones are. Realizably realizable ones are only exponentially many. So there's a big zoo of Gaussoids that have nothing to do with covariance matrix, one could say. So in this paper, we also, so using these minors is also interesting. If you remember the definition, it was something that had looked only at the three cubes. So you could say, so you can rephrase the definition. So, you can rephrase the definition once you know what the three Gaussoid is. You could rephrase the definition. Well, an n-Gaussoid is something that in each three-cube looks like a three-Gaussoid. And then you can restrict this to say there are some special types of Gaussoids that in each three-cube, not every Gaussoid is possible, but maybe only some of them. And we found some quite mysterious classes, the LUBF Gaussoids. So, let me only show one. So let me only show one table from this paper. So there are different classes of Gauss that you can get from restricting what is possible in each three-dimensional cube. If you allow everything, you get Gauss, of course. And then you can go try to find the smallest subclass that still has exponential growth, a doubly exponential growth. You can find some combinatorial puzzles. Combinatorial puzzles that are unsolvable except by trivial solutions. So, for instance, if you require that in every three-dimensional cube, you either have like no two-phase or all of the two-phases, then the only solutions are the entirely empty Gaussoid and the completely full Gaussoids. And then, either complete independence, everything is independent of nothing, or no independence at all. And there are some other things that are only. And there are some other things that are only possible in n equals 4 and not n equals 5, and so on. Various combinatorial structures appear here among the graphical models, but that's kind of expected. Once you're down to graphical models corresponding to undirected graphs, you find like substructures that correspond to forests or similar constructions. Okay, and there's one exceptional class that we might want to study further. To study further in the future, where we don't even know the growth of this class. So, that's something for our combinatorics, yes. So, things remain to be investigated there. Okay, so now back to reality, as I would like to call it. So, let's say you have one of these. Let's say you have one Gauss, and you ask, Is there a covariance matrix which realizes it? So, to say it differently, so you have a wish list. So you have a wish list of independencies. So we say A12 should be not zero, but A13 should be zero, meaning that one is independent of three. And at the same time, one should be independent of three given two, and so on. You have a complete wish list for each possible conditional independent statement. You say, I want it or I don't want it. And you ask, is there a covariance matrix that realizes this? So that's a fundamental problem. If you can solve that problem, you can also solve the CI implication. Solve the CI implication problem, the condition independence implication problem, which asks: Do some subset of these imply some other subset of these? So the implication problem is, if I have these conditional independence, do these other conditional independencies also hold? And this is equivalent because this implication holds. I mean, maybe the flavor of this you can appreciate or find. Find easy to understand. The implication holds if there exists no counterexample. So if there doesn't exist the realization of G and at the same time, not G prime, like we do with basic implications. And so the problem of does there exist a realization is a semi-algebraic problem. So our friends from optimization and real algebra ought to help us with it because we ask, does there exist a positive definite matrix such A positive definite matrix such that some inequations and equations hold. And remember, these things are polynomial equations in the entries of the matrix. So it's a semi-algebraic problem. There are tools to deal with this, both exact tools and numerical tools. And very nicely, there are these certificates for unsolvability. So if this set is empty, there always exists an algebraic proof that it's empty. Prove that it's empty. A certificate that basically tells you how to combine these inequalities, sigma is positive definite, polynomial inequalities on the principle minus, with these equations and inequations to form a contradiction like one is smaller than zero. So, whenever we want to store in a computer for the eternity for mankind to never forget that some goals that is not realizable, we have this method. Realizable, we have this method. We can store this certificate. It's not a, it might be numerical, but we, of course, we want like certificates that are rational. So we can store them exactly. Okay, so Tobias wrote a very nice thesis where he developed this over any field and proved a number of things. The most exciting thing, in my opinion, were these universality results. These universality results, something in the form of you, if you give me an algebraic number over q, then Tobias can make a list of conditional independent statements. It's a long list and n is really big, but he can make a list of conditional independent statements that forces any realization of a Gaussian distribution with this independent statements to have an entry. Statements to have an entry that's the algebraic number or one of its Galois conjugates. So that means that if you can realize, you might think that if you can realize something with a covariance matrix, you might also realize it with a rational covariance matrix. And that's far from true. And so the separation of combinatorics in algebra was really useful. So by separation, I mean once you do Gausset theory, which is an entirely combinatorial theory, you count them with such solvers and you do this logical. With such solvers, and you do these logical rules. And then, on the other hand, you do real algebra, which concerns studying these semi-algebraic realization spaces and their properties. Okay, so now you know what a Gaussoid is, and let's talk about the classification. So, the classification is what are all Gaussoids for growing N and what properties do they have? Properties do they have? So, the most interesting property in my taste is this realization problem. Which of the many, many Gaussoids actually come from matrices? Which almost principal minors of a five by five matrix can simultaneously vanish. And it can be shown, it has been shown already in 2010, that no axiomatization for the realizable Gausso can exist. Gaussoids can exist that works for all n simultaneously and is finite. So you cannot take Gaussoid axioms and add some more axioms so that the resulting structures are only the realizable ones. Okay, and that's the same as in matriarch theory. That's also the case in matriarch theory. So that's familiar in a sense. Okay, so the classification of three Gauss is just an example, fits on the bottom of this slide. There are 11. There are 11 possible configurations, and we can check them. They're all realizable over Q. So this non-realizability doesn't occur. And let's look at them. So there's the empty set. The empty set is a Gaussette. Remember, the Gaussett axioms are something like if two phases are there, then some belt has to be also there. And so if you have a singleton set, only one minor vanishing, these are also vacuously true because. Also, vacuously true because the premises of the axioms are always: if two things are there, then something else has to be there. And they're precisely of the form: if two things are there, then an entire belt has to be there, four things have to be there. So that's why the three belts that wrap around the three-dimensional cube are Gaussoids. And then we have, if you have everything, that's a Gausso too. And that makes 11. And your exercise is to find. is to find for each of those 11 sets a positive definite rational matrix so that exactly the minors that are in one of these sets vanish and the other ones don't. For instance, if you want all of these principles minus to vanish, then you just take a diagonal matrix. Okay? And if you want none to vanish, you take a generic. none to vanish you take a generic matrix and so on okay so now what we want to do is to solve this problem for n equals four and n equals five actually we don't need for n equals four because it's already solved that's the next slide for n equals four it was solved by Linschker and Matos in the original paper that defined Gauss okay so there are 679 Gauss remember that's in two to the 24 The 24. So it doesn't look like so. These two numbers grow at this both grow doubly exponentially. You have two to the 24 and 679 are on the same order of magnitude among friends. But I mean, for these small numbers, it's already drastically different. The difference between, well, somewhere between n 2 to the n times 2 to the n and 2 to the n squared times 2 to the n. Okay, so it's always useful to sort them. So, it's always useful to sort them according to symmetry. There's lots of symmetry in this. So, in the three-dimensional example, you already saw there are these singletons and then other singletons that look the same. So, for instance, these are all, there's only one off-diagonal entry up to permutation of the rows and columns. And same for others. So, there's a natural SN symmetry, S4 in this case. And symmetry as four in this case. There's more symmetry because there's also duality, which is on matrices taking the inverse. So if a positive definite matrix inverse is also positive definite. So there's more symmetry. But up to the S4 symmetry, you have 58 orbits here. And they consider the 58 orbits in that paper. And they find that 53 of the orbits are realizable and they give realizations. Turns out there is 620. Turns out that it's 629 Gauss. And for each of those, they give either a matrix that realizes it or one of these semi-algebraic certificates of non-realizability. So they bring the conditions to a contradiction. So now if you restrict to only n equals 4, you can axiomatize the realizability. Basically, you make one axiom that excludes each orbit that's non-realizable. So you get five new axioms. You get five new axioms for the five orbits, but they're more complicated. So the Gaussian axioms, they have two antecedents, meaning if these two phases are there, then something else. And these new axioms, they are in the four-dimensional cube. So, and they have three or four antecedents. There's a paper of Matthias with, I think, his former student, who derived these axioms again with computer algebra. So that's interesting for me. Algebra. So that's interesting for me that. Well, in computer science, things are moving very quickly. So there is software that basically, I think, uses SAT solvers to also work in logic and find axiom systems for something like this. And in our four-author paper that I mentioned before, we show that if you go away from positive definite matrices and allow real matrices, real symmetric matrices. Real matrices, real symmetric matrices, just with negative principle minus two, then they all become realizable. All four goes outs are realizable if you allow negative principal minus. Okay, here's a table that's copied from their paper, and that's the part where they give realizations of the ones that are realizable. And I mean, it's all done by hand, it's a beautiful table. Done by hand. It's a beautiful table. It's a beautiful paper. So when you see epsilons, you see a bunch of epsilons. So these are the four by four matrices that realize something. And for epsilon small enough, for instance, the top left here, this matrix will be positive definite and have the right independent structure. And in this case, if you have just these epsilons in the table, you will see that a realization not only exists. Not only exists, but by the structure of this matrix, a realization also exists close to the identity matrix. So you just move an epsilon away from the identity matrix along this ray. And of course, you have rational points on this if you pick rational epsilon and you find a realization. But not all of them are of this form. So there are also sums that have g's and f's. And so it can get more complicated. It's also no. Okay, it's also known that if not everything that's realizable is realizable close to the identity. So there are some things that are realizable, but you have to pick specific matrices that have a positive distance from the identity matrix. So we kind of have some intuition about the geometry of this, but other than that, it's like really wild. Okay, I love this paper. It's an amazing paper. An amazing paper, but it's not computer readable. So, my vision for the computer for the future is that we make our papers readable for both, for humans and for computers. That would be, I think, useful papers in the future will be human readable and computer readable. And this paper is a beautiful paper and it's human readable. So, for instance, it has this table of realizations. And I don't know how many people type this into a computer. This is into a computer, but it has been. So they derive this table, they typeset it, they put it into a computer, and then they send their version to the publisher. The publisher put it in again, another source of error, and now we want to work with it. We put it into our computers again, and that's 30 matrices. So errors might occur. So it would be good if this data was available electronically. But if you need it, I'm sure to PSS. I'm sure Tripi S has it. I probably also have it somewhere, but the paper doesn't come with a computer-readable version of this. They even have computations. So in this paper, they have computations that are encoded in proofs. So deriving these certificates for non-realizability, they write, well, you take this inequality, you combine it with this inequality, plug in, and then one is more than zero. So this is a computer algebra, one of these certificates that's. These certificates that's changed into text to be human readable when it was originally computer readable. Of course, there's no source code whatsoever, or at least not with the original source, and then the authors are unavailable. Yes, you can't ask the authors about this paper anymore because one left academia and long busy with other things, and one, Fero Matos, unfortunately, passed a couple of years ago. A couple of years ago. So it's not ideal. And now let me come to the fairness, which was this strange acronym that I had in the beginning of my presentation on the title page. So I think this is research data and it's valuable research data. And the name of this workshop is data and computation. So this is very much data that results from computation. And here's a definition of the German Science Foundation, the DFP. Of the German Science Foundation, the DFG, that research data is data generated in the course of a scientific project, e.g., through source research. That's maybe what we're seeing here, source research experiments. Also, maybe what we're seeing here, measurements, maybe not so much surveys or interviews, maybe also not so relevant in mathematics. But I think I claim, and that's part of this talk here, that we have research data, especially when it comes to computer experiments. comes to computer experiments and doing exhaustive computations. So we could discuss if our theorems maybe are research data. Does this, I mean, you just think about it. Does this definition apply to our theorems? I think it certainly applies to computational results and maybe also to the code that used to produce these results. So the best data is FAIR, which is an Is FAIR, which is an acronym standing for findable, accessible, interoperable, and reusable. And that means, well, you can find elaborate definitions of this on the internet. So you need to be able to discover that some data exists. So maybe for our theorems, we are in good shape. We have the social network. We tell each other about theorems. And if we put our papers on the archive, the theorems are also accessible, meaning without barriers. Accessible, meaning without barriers, without paying for them. And when it comes to interoperability, I mean, the interoperability means that somebody with a different operating system maybe could also use them. So we have the PDF format, which is relatively portable and interoperable. And we have the paper format, the print it on paper and put it in the library, read with your eyes format, which is also relatively interoperable. But whenever I Relatively interoperable. But whenever it comes to data or computations or source code, it gets less and less so. And in particular, the reusability of code can be very bad if the code is two or three years old and people have moved on. So in a not too far future, we want our treat our data more fairly. And that's also part of the project that I'm proposing here. So this slide is somehow about So, this slide is somehow about the infrastructure in Germany. So, this is the page of MADI. You may have seen Bern running around with t-shirts that had the logo on it. And this is the Mathematics Research Data Initiative, which is part of the German National Science Foundation Research Data Initiative. So, in Germany, there is some thinking about this. What is research data and mathematics and how do we deal with it? And this organization is all out there to help us. Organization is all out there to help us. Okay, so let me finish by talking about the classification of five Gaussoids. So I'm solving, if you don't like statistics, I'm solving fundamental linear algebra problem, which almost principal minors of a real symmetric PD five by five matrix can vanish simultaneously. We have seen that there are two to the 80 possibilities going through all the 5x5 PD matrices. Fortunately, there are only 60,200 something thousand. 60,200 something thousand Gaussoids, which you can compute in, well, you can compute it on a laptop. And if you print it in text files, a couple of gigabytes, but you can compress it and it's available on Causo HDE. You can also sort it into orbits. The reduction is pretty good. So S5 has 120 elements. And if you divide this number by 120, you get approximately this number. 20, you get approximately this number. So that is a good reduction. So, if you want to solve this realizability problem, you only need to solve it once for each orbit, yes, because this S5 symmetry, it just permutes the rows and the columns. So, a realization for one gives you a realization for the entire orbit by permuting rows and columns. Now, with five Gauss, we have this problem that there exist Gauss that are not realizable over any field, not even over the complex numbers. But realizing them over the complex numbers, But realizing them over the complex numbers note is not a semi-algebraic problem, it's an algebraic problem. So that ought to be slightly easier because we can do rational arithmetic computation with McCally 2 or singular or other computer algebra systems. So that would be, if it's not realizable over C, then it's certainly not realizable over R and certainly not realizable with positive definite matrices. So there exists some quicker checks. So that's good. So that's good. So now you might ask why? Why does he want to do why does he want to do this madness of understanding 508,000 orbits of something? So I think this project has something to offer for many different areas of mathematics. For instance, we talked today about combinatorial commutative algebra because we started the Gausser definition from relations among the minors of Of a symmetric matrix. So that's something people in commutative algebra study, and we're not done understanding these relations. So there's real algebra. It has appeared all over the place. There are these theorems of the alternative, like the positive standards, and there's cylindrical algebraic decomposition, cut, which is an algorithm to decide whether one of these realization spaces is empty exactly. So we touched upon computer science. So, we touched upon computer science, or at least we used the SAT solvers, and not everything is done there. So, that's a business that has many competitions. So, people working on SAT servers, they engage in competitions. So, SAT servers are A, very good, and B, very many, because for each year there's a new competition and they make new SAT servers, and they get ever better and ever more. And to my knowledge, there's none that includes the symmetry reduction in the computer. Reduction in the computation, or would exploit the symmetry directly from the start. I think that, so not, I think I know the numbers that we computed of orbits. We did by symmetry reduction after we had the results. So there's some number theory if you want to study this realizability over the many fields that lie between Q and R. There's some topology to be studied. So if you look at all these realization spaces, one of the results of to be. One of the results of Tobias from his thesis says that they can be really bad if n is like really large, but what if n is five? Yes, how do these realization spaces, some topological space, do they have holes? Maybe you want to navigate in this space. We want to optimize overall covariance matrices with the given independent structures. There's something info statistics. So, graphical models and their variance give, there are many, they're just models in statistics that give. They're just models in statistics that give interesting covariance matrices with interesting independent structures. So, maybe there's something there. I think it offers something for software development. So, when I did my naive experiments with the classification of five Gausser, just like let's see what I can do like this afternoon, I can get software. Some of you might know Bertini, the numerical algebraic geometry software. So, it's very easy to make it just crash on some of these realization spaces. So, tell it, find me a. Spaces. So you tell it, find me a complex point in this complex realization space, and it will just crash. So I think it's a database of very challenging examples. And of course, it's a test case for this treating the data fairly, because it's a massive amount of data. And so it's not really clear how we want to present this data. How would we publish this data so that it is useful? So at the moment, there's this Gauss-DE web. So at the moment, there's this Gauss DE web page. You can download a text file which contains 60 million lines or something. But is this the best way? Is this a way the computer wants to consume this data? Maybe not. And popular databases like the OEIS, the online encyclopedia of integer sequences, or PolyDubDB, the database of lattice polytopes, show by example that it's worthwhile because once the data is there and it's machine accessible, people do all sorts of things with it. People do all sorts of things with it. So, once it's published, people have new ideas. They use machine learning techniques on the database. They use it for things you cannot imagine now. Once the data is there, people will come with creative uses for the data, as long as it's accessible and findable and so on. Okay, so there is some start, but it's not awesome. So, I want to make it awesome. So, we have the enumeration, we know the Gaussoids. We know the Gaussoids. There's no API, no computer access. You can download the text file and write your own parsing tools. To Biaz has written some of them. And he's working on a more general conditional independence database for conditional independent structures independent of this normality assumption, but then restricted to four variables, maybe, or certain subclasses there. And one thing that has also And one thing that has also been done is if you study the realizability over C as a first check, then due to further symmetries, let me not explain too much why, you can reduce the number to 16,981 orbits. So that's 16,981 Neulstensat certificates you might look for. And I gave this to a bachelor student, and he did something, and it resulted in a very unfair zip file that's somewhere. File that's somewhere in the zip file, it says like zip file of the content of the previous computer, and in there there's a zip file of the previous computer, and in there there's this unfair zip file. And so, yes, I mean, I'm also guilty. This was a couple of years ago that I didn't present or preserve the data that was generated in this thesis in a way that would be useful for me now. But I think just a couple of years later, it can be useless because I don't trust the data anymore. I don't trust the data anymore, and the documentation is not good enough, and it's hidden in layers of zip files. So, the next time I want to do it right, and that's my message. And in Germany, we have this new priority program on combinatorics called combinatorial synergies that's all to start next year. And I hope that in four years from now, after the first phase of this priority program, we might be in a much better shape with the classification. Shape with the classification of five Gauss. So, thanks a lot. That was great. Thank you very much, Thomas. So, I think we can Jose, can we use this one for questions? Yes. Okay, so are there any questions? Um are there any questions for Thomas? Who wants to start? Well, maybe I can start with a question. So Thomas, in the table of the realization where you had this epsilons, you also showed a delta. So what's the difference with, do you know what it is in this 20? I think a delta, oh, there's a two minus delta. Oh, there's a two minus delta inverse squared. A delta might actually be just an independent. Um, no, there's I see a matrix. I see a matrix with only deltas. So delta is not just an independent version of epsilon. So Tobia says that it's a number very close to three quarters. Yeah, okay. Thanks, Tobiah. And f epsilon is a polynomial, is a quadratic polynomial in epsilon, I think. is quadratic polynomial in epsilon i think wait if it's close to three quarters then the two by two minor of top one won't be positive um anyways here yeah this one won't be positive you say uh yeah i'm not sure so maybe maybe it's four thirds but it's one of these two numbers and there's a you know delta to the minus two and it's subtracted from oh it's to the minus two sorry Subtracted from the minus two, sorry. Delta to the minus two. Yeah, it's very small, but yeah. Okay, so it may be three quarters. So instead of delta, you can write like three quarters minus epsilon, I think. That's the idea. Three quarters minus epsilon. Okay. Thanks. Okay. Thank you. There's a question there. So, have you thought at all about, well, I guess you mentioned some of the parallels with like matroid theory with like duality and minors and stuff. Are there any other combinatorial objects that you can associate Gaussoids to? Like, if you have a graph and you take it's like Laplacian or something, like, I don't. I don't know, like, that's a symmetric matrix. I guess it has like a Gaussoid associated with it. Like, is this at all interesting? Is that at all? I mean, whenever you have a mechanism that takes a combinatorial object and constructs a PD matrix or matrix with non-vanishing principle minors, you can look at the class of Gaussoids you get. I mean, you also can look at the class of statistical models of multivariate normal distributions. Multivariate normal distributions, you get. Yes, that's so. If you go combinatorics to matrix to Gaussoid, that's interesting. But it only gives you realizable Gauss, which in this whole classification business are very few. What would also be interesting is to study in the class of non-realizable Gaussoids subclasses. I mean, there's like, for instance, there's a notion of orientability that's like also similar to what. Like, also similar to what's happening in matrix theory, and it's um, so realizable um, by PD matrix implies orientable, um, but not the other way around, for instance. And cryptomorphism. So, I mean, of course, we would like to have more axiom systems, but there's no the parallels are not so obvious in this sense. So, there's no really, there's not really a hundred different definitions for. 100 different definitions for Gaussoids, and there's also no Gaussoid polytope, so that part of that part, that linear optimization part of major theory is also not clear at the moment. And while you were talking about that, how important is the like your principally regular condition? Like, what if you relax that and just allow principal minors to vanish? Like, not a complete break. It's not a Gaussian. Okay. Yeah. Yeah. The Gauss axioms won't hold for matrices. So if you have these implications need to be true. Thanks. Anthe Alina, is there anything online? No, no questions in the chat or anything. Any online participants who would like to ask questions, you can just raise your hand or feel free to unmute yourself and just speak up. Maybe I can ask a quick question. So there's this paper by Liam and Caroline, and one of Caroline's students about counting Markov equivalence classes for directed graphical models. Classes for directed graphical models. And so it seems like if you can count, I mean, for directed graphical models, maybe you need more axioms or something, but if you can count, if you can count realizable Gaussoids that corresponds to directed graphical models, somehow maybe you can count market equivalence classes or something like this. Oh, the other way around. Use this to count equivalence classes. Yeah. Yeah, and because they only have it up to like, I forget, 12. Yeah, yeah, yeah, I mean, okay. So, first of all, so n equals 12 is like out of the question here, so don't ask about n equals 6, please. But then the other way around, yes. So, the um it gives some subclass of realizable, so it could be used to give an. Realizable. So it could be used to give an lower bound, for instance. If you know there are at least that many covariates, different types of coordination independent structures coming from this kind of models, some kind of models, you, for instance, get a lower bound on the number of realizable ones if you can ensure that their Gaussoites are really different. And that's often possible. Yeah. This is some of the part where I said like statistics has many constructs. Like statistics has many constructions for models that give lower bounds in the sense. Or, I mean, approaching this classification would be from two sides. One side is like showing that many are not even realizable over the complex numbers. So that should exclude many Gaussoids. And then from the other side, you come and say, okay, these are all realizable because they're graphical models. These are all realizable because they're directed graphical models. And these are all realizable because they come from this construction. And these are all realizable because they come from this construction, and then narrowing the gap between the two, and hopefully, to a manageable amount that you would or we would brute force with real algebra tools or so. So, speaking of this non-realizable, and so in the four case, do you know which ones are realizable over C, Q, and R? Um, all are real in the four case, all are real as. In the four case, all are realizable over C and R if you allow negative principle minus. So in the n equals four, everything is realizable with real matrices if they need not be positive definite. And rational numbers, you can also get. I think all of them should be okay. Rational with negative. I say yes, but I need to do a quick check, but I think yes. All right. Ben? Yeah. Hi, Thomas. So I was going to maybe ask you a question about fairness in the case of like when you're going to brute force, because I think one problem that kind of exists that Tobias and I know have talked about a lot recently is there's not really great. Is there's not really great open source software for doing something like CAD, right? And then if you're going to try to brute force your remaining examples, I mean, are you just like, do you have a plan? Or like, are you just going to use find instance? And then who knows, whatever magic thing Mathematicus fits out? So in an ideal world, so I talked to Chris Brown about it. He's the developer of the open source tool that might be used for CAD and his opinion. That might be used for cut, in his opinion. So it's called CapCut, but it's also like software archaeology, yes. So it relies on the library, it's not really working anymore on modern computers, and it's kind of in a bad state. But he said that all the engineering that, so, I mean, what Ben is talking about is that Mathematica is so much better than the open source tools. And the reason is that they have the manpower to do a lot of heuristics. So you input the problem. So, you input the problem and it will massage the problem so that the cylindrical algebraic decomposition works more efficiently. And these problems are very sensitive to how the input is formatted. Sometimes you have three inequalities and you replace two of them by the sum and the difference, and it can make a huge difference for how these computations go. So, that's not really understood. So, they implemented lots of in mathematical. They implemented lots of, in Mathematica, they implemented lots of heuristics. And they're pretty good at it. So that's one answer to the question. We could expend lots of time to develop the open source software to a different degree, but maybe let's not do that. Maybe we are also in the fortunate situation that once Mathematica has given us the certificate, we can check it with the open source software. So that's like a human brain, it's like creativity. So once you have the proof, you write it down and it's checkable. You write it down and it's checkable. Yes. And it doesn't matter how you came up with the proof. So, if you have a mathematical computation that, in the end, where the verification that the result of the computation is correct, which is either a matrix, I just check the minus of the matrix, or a certificate where I just check the certificate. I'm kind of okay with using Mathematica. Yeah, that's a great point. That I like the idea of checking with an open source tool. That's very good. Checking with like an open source tool, that's a very good idea. Can I comment on this? So, when you call find instance in Mathematica and you and there is no solution, Mathematica will just give you like an opening brace and a closing brace, right? So, there's no certificate you get out of Mathematica in the unrealizable case. And I really don't know if there's any software to spit out this positifer certificate. Aha, you mean, um, yes, okay. You mean, um, yes, okay. Also, if I do quantifier elimination of a formula, it will just say false without telling me how it derived false. Yeah, okay, so that's so that's a real problem that that's not open source because it had the proof at some point. It did derive, I mean, this computation did derive from the cylindrical algebraic decomposition at some point. I mean, so then the proof existed in memory, but it didn't tell you the proof. So, maybe we can get it to tell us the proof. Maybe we can get it to tell us the proof, or yes, you're right. I mean, then that's okay. You're saying that Mathematica does not actually give certificates in the non-realizable case. And well, I think you can ask it for the CAD, and then you can check that the projections are empty. But for example, these final polynomials, you will never get them from any software that I have ever encountered. You will get a big mess. Okay, but there's something to be done, I think. Yep. Thank you. So I think maybe we'll end the questions here and have later more discussion to go to a coffee break. But let's thank Thomas again for a great talk. Yeah, thanks. And sorry for not being there. And see you all soon. I try to jump into the Zoom, but. I try to jump into the Zoom, but the time zones are also different. Yes, no problem. No, thank you for putting it very on topic with computations and data. I'm sure we'll have a lot. See you. Okay, bye. So I think, so we'll do a coffee break now and then we'll return, I think, 10:45 for Anna's talk. 