And she's, you're currently a postdoc, right? Or are you a faculty? I'm at the end of my current instructorship. Yeah. Okay, good. Excellent. And Yunan is going to talk about optimal transport-based objective function for physical inverse problems. Go ahead, Yana. Yeah, thank you very much for the invitation. And it's my pleasure to talk about the overlap between inverse problem and optimal transport. Optimal transport. So that's an area that I have been working on since 2013, so for quite some time. And I hope to share some insights on the benefit and probably potential limitations of using Marshall Stand Decent for inverse problems. So first of all, I just want to give a brief overview of inverse problem. And inverse problem has a counterpart of the forward problem, which we always bring up hand in hand. So when it comes to forward problem, one could think about modeling a simulation or new market. In a simulation or numerical solution to PBEs, usually you are given all the parameters like initial condition, boundary condition, variable coefficient. You already know everything, but you want to simulate the physical phenomenon by the solutions of PD or ODE as accurate as possible. So that's what we refer as forward problems. While inverse problem is the other way around, I do have some physical information, like I know the pressure of the field, I know the velocity field, or certain things I can measure. Can measure, then I want to use those information to infer probably what is the source, what is the inertial condition, boundary condition, what is the variable coefficient, etc. So that's exactly the inverse process. And the common inverse problems include earthquake location, locating where the earthquake source or detect material artifact without breaking the material, and also common medical imaging problem like X-ray CT. They all belong to the category of inverse problem. To the category of the inverse problem. So I will talk about mainly about the wavy inversion, but I think a lot of the insight why Washes and Distance works for UV inversion apply to a sequence of inverse problems. So for inverse, V inversion is very similar to ultrasound like CT is the fact that we have an artificial source and this source can be in the form of vibrator truck or in some machine like a boat that Machine, like a boat that has this ability to generate a very large signal, and the signal is so the shock is like so strong and it acts like a source and the way propagate. As a wave propagate, some of the wave can go back to our surface. And for those waves that go back to our surface, they went through some refraction or transmission or reflection. So those physical phenomena turn those waves go back to our surface. We go back to our surface and with the orange dots here record the vase. And what's information available for us at the source that we designed, which is in the form of a truck, we know the frequency, we probably also know the energy. And we also have those sensors on the ground that we were able to record the wave fields. But what we want to know in this inverse problem is what's going on underneath the ground. So that's like material properties such as density or wave speed in the very Or wave speed in the variable coefficient manner. So, the application setup is what data available to you, you can consider that just waveforms. So, they can be just a wave solution at a particular location, like particular depths. And the data we recorded usually looks like the left-hand side, like a lot of waveforms. Well, what we want to know is what's underneath the ground that we can now just detect automatically. And they are variable coefficients, they can depend. Coefficient, they can depend on the depth and depends on the horizontal location. So, and if we discretize it, that's a very high-dimensional problem. So, this is the application background. And if we turn back into a PDE formulation, and we can also find the correspondence. So, the forward problem is we already know the variable coefficient, we know the source, we know the initial condition, the boundary condition. The forward problem is. The forward problem is you tell me everything, I would be able to simulate the wave solution as accurate as possible by all kinds of high-accuracy numerical methods. And then I will obtain you every use that restricts on the boundary those data that we have. While in inverse problem, we are interested in the other very once, which is we have the solution, we have some data recorded on the surface, which is part of the boundary, and then we want to reconstruct the velocity m. The velocity m. So, this is the inverse problem a lot of the time that we are dealing with. And sometimes, if it's a physical, usually, there's a PDE or ODE model related to it. So, one thing I want to emphasize is the inverse problem we are interested in, since it's a variable coefficient in the Rev equation on the left-hand side. So, both the forward map from the coefficient to U and the inverse problem from U to the coefficient are highly nonlinear. Although this is a scalar ready equation, although it's linear, but it's linear. Equation, although it's linear, but it's linear with respect to the right-hand side. While the map, the inverse problem we care about, that particular map is non-linear. And that's the source of all the difficulty, because I won't be able to write it down analytically what is M, even if I have U. So in terms of this part, we will get stuck if we want to proceed just looking at this wave equation. So, but application-wise, there's a lot of like we would still want to continue to go to do a To go to do all kinds of applications related to geophysics. So, people in geophysics reformulated a little bit into a PDE constraint optimization, which means I'm going to regard my optimal solution M as the one that minimize the true data G that I can record from the field, they are true data versus the synthetic data I can probably simulate with my current guess. And I know what I simulate depends on. Know what I simulate depends on the variable coefficient M. So I will try to minimize the difference between the synthetic data that based on my current prediction versus the true data I recorded from the field. And the one that minimizes the differences is what we regard as the true solution. This is definitely one relaxation of the original inverse problem presented in the previous slides, but that give us a potential way to find one solution, and then it will be based on the geologists to do. Then it will be based on the geologists to decide whether the solution is trustworthy or not. Okay, so once we formulate as the optimization framework, usually it's solved as a lot of those non-linear or non-convex optimization problem since it's large scale. So we highly rely on the gradient descent. The reason is large scale because each time we need to solve the wave equation and the wave equation is about part of the Earth. So the scale of the domain is in terms of kilometers. In terms of kilometers. Once we discretize, it takes a lot of memory and the computational time just to solve the wavy equation once. And another source of the large scale is M, since usually it's a variable coefficient, it belongs to a functional space. But once we discretize it, we usually discretize it as far as possible. So M can have millions of variables. So this become a very large-scale optimization problem. And that's why it's not affordable. Why we it's not affordable to do global optimization, and the common way to do that is to really rely on the gradient because if I can compute the objective function, I would have one way to compute the gradient and that give me a direction. So in order to compute the gradient, we only need to solve a forward problem, so the equation once and solve the hydrogen equation once. So only two PD solve, I will be able to get the gradient. So that's more like computational feasible. Okay, and then because the problem is highly non-convex as a result of the non-linearity of the map, so earlier I tried to emphasize the fact that this is a non-linear inverse problem because the map from M to the solution with solution U and vice versa highly nonlinear. So, if you put them into a quadratic formulation like the traditional least score method, the optimization landscape with respect to M is not going to be. Respect to M is not going to be quadratic. It's actually highly and highly non-convex as a result of the high non-linearity of the forward map. So when we are in this situation using a least square formulation, the objective function is highly non-convex while you can only afford to do gradient descent. It's not really avoidable to encounter one of those problems. And the number one problem we encounter in this community, since Tarantola proposed this formulation, Tola proposed this formulation, which is called a Fourier inversion, this 3D constraint formulation of solving such inverse problems. We start to realize most of the time I get stuck at local minimum. I won't be able to find the global truth. I was only able to find a local minimum that is not far away from fitting the data between F and G. And another challenge is a common source of a problem for least square formulation. Once we formulate it like this, if G has Once we formulate it like this, if g has noise in it, our objective function can easily, easily capture that amount of noise. Therefore, if I successfully minimize the objective function to zero, I will overfit that amount of noise and I have an incorrect reconstruction. So one way to modify or to mitigate the second challenge is to add a regularization. That's like the so-called TikTok regularization. So this two-man challenge has been accompanying us all the way. Has been accompanying us all the way for almost three to four decades, and so there have been a lot of methods proposed, and one of the methods is to replace this objective function by the Vashes and distance from our optimal transport theory. And the reason is what I will present the following. So here, the inverse problem or many, many like machine learning-based problems, we always formulate the problem as a data feeding approach. Fitting approach that we want to minimize the differences in the data, but our ultimate goal is actually find the good parameter. So, in order to propose a good objective function, we first have to understand what is the connection between the data and the parameter. And that's what we did back in 2015 and 2014 to understand for this raby equation, what is exactly the phenomenon that caused me to have this continued dependence. To have this continued dependency, it's usually impossible to write it down the dependency between the solution wave solution u and the parameter m. And one of the few exceptions is constant wave solution, constant wave speed. When the wave speed is constant, and if additionally it is a 1v wave equation, we all know there's a Dallenbar solution. So we can write it down the solution analytically just based on its boundary condition. And that's what I present here. You can see. That's what I present here. You can see exactly how the M will determine what a perturbation in M, what type of perturbation you can see in U. And this is what I refer as the continuous dependence. So here we can see that if there's some small perturbation in M, which means I didn't guess it correctly, I didn't predict M correctly, what I see on the data side will be pure translation and the dilation in X and in T, which tells me when I have a Which tells me when I have a scalar perturbation, even if I have a scalar perturbation in M, what I observe, and not scalar perturbation in U, what I observe is actually very global change, such as translation and the dilation. And then back to the original question, if that's what the continuous dependency gives you, why you need to use the L2, what the failure of using L2 norm is this piecewise comparison of the class of LP norm. Of the class of IOP norms, they don't capture such global change, such as translation and dilation. And that's why, for decades, we observe such a failure. Like, this is the optimization landscape when you compare two wavelets. And clearly, these two wavelets differences is purely a translation. And if we think translation as the variable that determines the optimization landscape, you can see how non-convex it is when you use a least square formulation. And that's, that's a simple. And that's a simple example basically represents the difficulty when people in this community use the L2 norm. And on the right-hand side, what you observe is quadratic Russian distance as the objective function. And for this simple case, we obtain a global convex optimization landscape. And that's one strong motivation for us to focus on using the Vash-Strand distance as the objective function because it offers me global convexity in this particular. In this particular case, it offers me global convexity, which means I can just use gradient-based methods and I will be able to find the ground truth. So that actually brings me back to the optimal transport part. So one way to think about non-convex or convex optimization is to look at optimization landscape. But here, M is a variable coefficient in the wave equation, and usually we assume M belong to a functional space such as L2. Space such as L2. So, from another perspective, it's impossible to prove there will be a global convex optimization landscape. And I don't think that's also, that's not the case, even if you use the Vashes and distance. You use the Vashes and distance as the objective function for the variable coefficient M, it's never going to be globally convex. But what we can do is to make the global baseline of attraction, which is really the strongly convex. Which is really the strongly convex region of the optimization landscape to be a lot wider than what the L2 will be. And that's basically what we hope for in this case. If we think about as the global strongly convex region around the global truth, I want the Rajasthan landscape is a lot wider, which means if I start random initialization, initial starting point, I have a bigger chance of convert to a global truth. So that's actually what we. Local truth, so that's actually what we aim for. And another way to interpret when we get stuck at local minima is when we get us stuck at local minima, usually gradient is zero. So gradient is zero, but the gradient is zero at unfavorable places. Gradient is zero not as a global truth, but it's just like local minimum. So that's one reason, like your optimization landscape is not great. And the second reason is, based on your starting point, the constraint. Starting point, the constraint gradient flow actually visited one of them. So that's one why we finally get stuck at a particular location. Because for this non-convex optimization, the entire path from the starting point to the final is a constraint gradient flow in the continuous setup and we visited one of them. So if we look at the gradient for the constraint, that's why I emphasize since because it's constrained, so the gradient contains two parts. J is my objective function. J is my objective function and j is determined by m, but j is actually measuring the difference between the synthetic data f and the observed data g. So the gradient by chain rule contains two parts. One part is a data feature derivative, is a feature derivative of the objective function with respect to f. And another is the derivative of f with respect to m. So dj dx is usually non-zero when the gradient is zero. Zero when the gradient is zero. Because the choice of the objective function are usually very convex metric. Like no matter it's L2 norm with the Vashes and distance, they are very convex comparison between F and G. So if DJ DF equal to zero, that usually already means you have F equal to G. So you feed the data. And that's now the case. If DJ DF equal to zero, it means you feel the data perfectly. So you already feel the truth. That's that's you already feed the truth, you achieve the data fitting purpose. Therefore, most of the time, when we get stuck local minimum by definition, that's when DJDF is non-zero. Okay, let's look at the other side, DFDM. DFDM purely come from my forward problem, from my physical system, and that's there's not too much I can change about it. And usually, when you solve an inverse problem, you will assume the inverse problem has a solution. So, most of the time, we So, most of the time, we also assume DFDM is non-zero, which means there is a solution. Okay, so if both of them are non-zero, but my gradient DJDM is zero, that only give us one possibility left, is that these two are orthogonal, which means the orthogonal projection of DJDF onto the subspace spanned by DFDM is zero. So it means they are orthogonal. And by our And by our empirical observation, that's usually where when the non-convexity happens. So we can think about the wave, the wave propagate in a horizontal sense, like a phase will change. In 1D at least, the phase will change, amplitude will remain the same. However, if you compare things in vertical direction, like what L2 or any L P norm will do, they compare in fit like horizontal in the y-axis direction. So the projection will be. Projection will be orthogonal and adjusted when we get that local minimum, which I showed in the previous case. So then it really motivated us that if I want to change the objective function, I want to change J in the sense that DJ DF will have a non-zero projection onto the subspace of DFDM. And that's the motivation and also why potentially Russia's anti-sen will be more applicable than more than this hyperbolic value equation. So for optimal So, for optimal transports, since I will focus on the applied side, I want to say that there have been a very rising application in terms of inverse problem and machine learning, which also related to kind of data fitting phenomena. And another thing which I want to share is recently when I was searching this AMS MSC mass class subject classification, in the past, when I want to search for some optimal transport, I get Some optimal transport, I get no like nothing. But recently, when I apply, when I search for optimal transport, I actually found a particular subject classification in the MSC 2020 category. So it's my first time to realize actually we have our unique MSSC right now. And I also, because this MSSC only gets updated every 10 years, so you can search 2010, you can also search 2020. 2010, you can also search 2020, but when you search 2010, you get nothing. But even you search 2020, you actually get this classification under the category of calculus evaluation. So now you can put that in your paper is 49Q22, if you don't know that yet. Yeah, that's another topic. Okay, so earlier, what I was trying to talk about is I want to make DJDF to be aligned with DFDM. And that's actually why I want to make sense, why you need. What why I want to make sense why you need one you can use washes and distance for hyperbolic equation inversion? So, as we all know, the definition of the washers and distance, when you compare things, the cost is always split into two parts. One part is the amount you moved, the vertical direction differences, and another part is horizontal direction differences. And I regard this as a fundamental difference between the WP class of metrics versus the LP class of norms. LP class of norms because LP won't have things like the second part, it only has first part. And it's exactly the second part. Make sure our dj df, if j is a variation distance, that actually our counter-avage potential for one of the density functions, that part has a very nice property and it aligned with my application's need, which is DFDM. So because of this particular definite, because of But this particular definition, because of the second part, we will actually be able to prove out a convexity issue, convexity properties. So, here is the formal definition, and the formal definition is exactly two parts, right? One part is the amount removed, the density, and another part is the distance removed. So, this is the two parts that play the key role for this particular application. So, property one of using variation distance in applications is what I've been stressing so far in my talk. Been stressing so far in my talk is a better convexity. So there's not a lot of we can prove, but for a particular very simple setup is if you are two density functions, one is one that you can think is fixed, it's just observed data and another you can change. So let's say G is fixed, but now I define F as a pure translation and dilation of G. So that's the dilation and translation I can. That's the dilation and translation. I can have a lot of parameter in it. I can make this translation and dilation to be dimension dependent. So every dimension has their own translation and dilation parameter. That's all fine. So let's think about that F and G depends on each other in distance. Then the Varsha stand distance, at least for the Varsha two distance, I was able to prove that it's going to become globally convex in terms of translation and the dilation. And if we combine this with the earlier, like the wave With the earlier, like the wave continuous dependency I mentioned, for wave application, the variable coefficient change it gives me exactly translation and dilation. And if a variation distance was will be convex in translation and dilation, that means BJDF is so perfectly aligned with my application's EFDM. And that explains a lot of success we have seen in this area in the past five to ten years. So this is just a numerical. So, this is just a numerical illustration. The left-hand side is the inversion using the L2 norm as objective function, and the right-hand side is the Vashes and distance as objective function, and they start with the same initial starting point. The only difference between the left-hand side illustration and the right-hand side is a change of objective function, and the correspondingly there are different gradients. So you can see at the beginning, they demonstrate a very different, what I refer to as a constraint. What I refer to as a constrained gradient flow path. Like they visit different iterations along the path, and then finally they get stuck or they stopped at the different locations. And one of them is the Truman-Musi model that we saw at the beginning of the slide briefly. Even if you don't know the truth, let's say we can, in universe problem, we can say realistically, you never know what's the truth. But let's say take one step back. Even if I don't know the truth, I can just look at the data residual. So data residual here is purely. So data residual here is purely the subtracted difference between f and g, since I use my objective function to measure the differences. So as long as the difference is zero, I basically already achieved what I can do. But if you look at the residual of L2 norm, it's still a lot out there, even when it gets stuck. But if you look at the Vashes and distance residual F minus G, it's nothing, almost nothing visually, which means it fits the great amount of the data. So that's one thing of the success. So that's one thing of the success. And another aspect, so yeah, sorry, continue the fact. So this is what I want to extend a little bit. So what I presented was the hyperbolic wave equation inversion. But in a recent paper with Kujian and Bjorn Christ, we were able to extend to a few other areas. For example, other inverse problems, like inversion for transport in homogeneous flow, for reconstruction from projections, or kernel source deconvolution, or deconvolution from... Source deconvolution or deconvolution from diffusive environments. In those cases, we're able to show that using fashion stand distance, you will still have globally convex landscape for certain simple setups. And the reason is nothing surprise or nothing secretive. It's because for those applications, for those inverse problems, the DFDM you are interested in that aligned with the counterview potential. And because it's aligned with the counterpart potential, the gradient is The gradient is hardly to be zero because they are orthogonal to each other. So the gradient is zero probably because we feed the data or because the inverse problem is your posed by itself. There's nothing we can do about it. So that's one extension. And hopefully we are exploring in those directions. And for the last 10 minutes, I want to express another good property. It's a better stability. I think it's an interesting add up. Get added up on top of the global convexity or better convexity by the Vashesan distance. So, the better stability is in terms of the connection or the linearization or the asymptotic connection, which H minus 1 semi-known. And that's some known result. So the first one was by Felix Auto and Sedra Vinani in their paper in 2000. They proved that the linearization of the Barshasan distance is weighted H minus 1 seminar. Weighted H minus one semi-moon. And the second result is more recent by Ramy Puri. He proved that even if I don't have to be in the linearization regime, as long as my density function f and g are bounded above and below by two constants, I can still show the very simple equivalence between the h minus one semi-norm on the Lebesgue measure versus the wash-sen distance. So both of these two are for me who work on inverse problems, it tells me. Would work on inverse problems, it tells me in terms of spectral bias, W2 is going to be the same order of H minus one norm or H minus one semi-norm. We all know H negative norms are weaker norms. So at the weaker norms, it advises more towards the lower frequency just by its definition. There's several definitions we can go for for HS norm. One of them is the Fourier transform. You take the Fourier transform of what you compare, but the H negative norm is that you wait. negative norm is that you weight the different frequency differently. If s equal to zero, that's L2 and we have possible identity, every frequency content will treat them exactly the same. But if s is smaller than zero, then this weight will be monotone decreasing as z gets larger. So we have much bigger weight on the lower frequency, and the vice versa for higher frequency. And the fact that W2 is equivalent or some equivalent or linearization is almost Equivalent or linearization is almost the same, tells me that Russia and Decent also inherit this frequency bias and it's about a negative one order of that. And what's the benefit you can gain by using an active norm is exactly you have a lot of emphasis on the low wave number components, and you will be blind for good to the higher frequency noise. So if the higher frequency components of your data contain a lot of noise, you won't over. A lot of noise, you won't overfeed that. We won't overfeed that because that component of the data has a very, very small weight on top of that. So, the majority emphasize is on the low frequency content, on this like low wave number component of the content. Well, there's also a negative side of it, because in terms of resolution, we won't achieve as good as stronger norms. So, we may gain better stability, but what we lose potentially is We lose potentially is because we focus too much on the lower frequency, we will lose the high frequency content that contains the physical information. So, the reconstruction result in terms of inverse problem using weaker norms, we won't get that much high accuracy. And I want to just illustrate both quantities by this numerical example. So, earlier, sorry, the first show is the illustration when there's absolutely no noise in the data at all. Absolutely no noise in the data at all. And the left column is the L2 reconstruction in this very simple kernel deconvolution case. So you can see it matches a lot of the truth, this black one. While the H minus one and the W2 reconstruction is reasonable, like we match the low-women component of the unknown, but we fail to match the edges. And that's what I tried to say earlier that because this H minus one, the bias on the lower frequency. On the lower frequency, so the high frequency gets less weight, so we won't be able to match it high resolution. But this is a case when this is absolutely no noise, and that's very idealistic in realistic situation. And the second row, what we present here, is what if the data has noise? And if the data has noise, and if there's no any regularization on top of that, the L2 reconstruction easily overfeed that 2% of the noise. And you can see the reconstruction. Of the noise, and you can see the reconstruction become very wild away from the ground truth. While even in terms of 10% of noise for both the H-1 and the W2 reconstruction, it's still the same as before. So the 10% of noise is not affecting the reconstruction and the low-wind number component are still correct. So this is what I want to emphasize as the gains. The gains is using weaker norm, we get better stability, but the potential lose is we may not. But the potential lose is we may not have as high resolution as the stronger norms. So, fortunately, Vashan 2 distance is only h minus 1. It's kind of between L2 and H minus 1 since it's a weighted H minus 1. Therefore, it's not a lot of resolution lose, but in the end, you can always improve the image quality with the reconstruction by switching to stronger norms. That's also what people do in practice. And you can see this type of phenomenon, even just from. see this type of phenomenon even just from the Bayesian perspective and that's what another thing I explored with McDonald from Cron. So in deterministic inversion, we have objective function we want to minimize sometimes we add regularization on top of that and then we use a gradient descent, Haitian, based method or conjugate gradient descent, all kind of optimization algorithm to minimize that. So that's the standard deterministic framework. But for Bayesian framework, we basically have a one-to-one. For Bayesian framework, we basically have a one-to-one correspondence. We have in Bayesian inversion, we have likelihood function, we have prior distribution, which is the counterpart of regularization. And after that, we have a polynomial, we have a posterior, then we want to sample. So all those components are one-to-one incorrespondence. So in this work, we basically explore what does it mean to use the Russian instance as the likelihood function. So first of all, in Bayesian inference, all your likelihood functions. All your likelihood functions, there's an underlying noise assumption associated with that. So, here we prove that if you use the Bachelor's and distance as the likelihood function, it means the noise is assumed to be multiplicative noise with a state-dependent covariance operator. So that's one thing that we really, and that's also based upon the fact that the linearization of W2 is H minus one, weighted H minus one, and that comes naturally. So the second thing. So, the second thing that we actually, the main contribution from that paper was to say, even when you use Vashes and distance as a likelihood function based on this assumption of your data noise, the better stability phenomenon that we talk about earlier in the deterministic framework still applies in the Bayesian inference case. So, for Bayesian inference, your true solution is not a point anymore. The true solution or the inverse, the solution to the inverse. Or the inverse, the solution to the inverse problem is the posterior distribution. So here we were able to bound the perturbation in the posterior distribution. So pi here is my posterior distribution. And y and y prime means I'm using either the true data or the perturbed data. And there's some perturbation in y and y prime. While the lower subscript means I'm using which type of likelihood function. So when you use wash-stand instance as a likelihood function, use one y prime as your data. and use one vari' as your data so you get different likelihood function and we bound the heilinger distance between those likelihood distributions and the right hand side we have some stability bound based on certain assumptions and similarly for l2 we have some stability bounds but what i really want to emphasize is the bound for these two are very different on the right hand side the right hand side for the w2 posterior is y and y primes h minus 11. minus one norm. And while for the L2 is y and y prime L2 norm. So why that makes the difference is again the weaker norm property. So if we consider that y and y prime is different from each other by a very high frequency noise in the form of some kx, for example, so the h minus one norm will be order of one over k, which means the larger, the higher the frequency, the better for us for weaker norms, because the higher the frequency, the more obvious we will be with respect to the Be with respect to the noise, and the more stable will be. While if the difference between y and y prime is purely L2 norm, then it's always going to be all the run emphasized. So this two inequality basically demonstrates that the Russian distance as a likelihood function, it gives you a better stability in the inverse problem with respect to the data perturbation. Okay, so this also can be reflected in our numerical examples all the time. Examples all the time. So, for example, you are using very noisy data, the blue one that I'm using. The blue data is the noise wave, and I'm using the very noisy wave data as my reference data. And I want to do the reconstruction. So the noise power is about 0.76 dB. It's a lot of noise. But if I still use the vector sand distance and do gradient descent after about 90 iterations, I still reconstruct the Mamusi model that you saw earlier in this presentation. Saw earlier in this presentation, it's almost the same, except the resolution we lose a little bit of resolution because of the noise. But in terms of where are the discontinuities, it's much, much better than what L2 reconstructed, even without any noise. So here, this kind of stability with respect to noise is also very handy when we are dealing with very noisy data. And finally, I just want to briefly mention actually recently we push it a little bit and try to see other inverse problems. To see other inverse problems, how the performance with respect to the largest and distance, and one of the case is to doing parameter identification for chaotic dynamical systems. So the idea in this work is that chaotic dynamical systems are very sensitive to initial condition, but if I accumulate the dynamical system time trajectories long enough, I will approximating the invariant measure, and I can compare the invariant measure and try to reconstruct the right-hand side. To reconstruct the right-hand side, while the environment measure won't depend on where you start with, so even if you don't know the initial condition, you can still stably reconstruct the right-hand side. There's a lot of details, but I just want to show you the final reconstruction result. So the red butterfly Lorentz attractor is what we start with, and the truth is the blue one, while the green one is what we reconstruct. You can see the change from the initial data versus the From the initial data versus the final data, it's not purely point-wise difference because there's a lot of shape expansion. And those type of shape expansion or some global changes like this, it will be really favorable to be compared under the Vashesan distance. And that's also one reason we were able to reconstruct this dynamics successfully in this very simple setup at this point. So finally, I think it's time for me to draw the conclusion. It's time for me to draw the conclusion. A lot of the time, in non-linear inverse problems, we encounter local minimum trapping. And that's the number one thing in terms of practice in applications. And for a lot of the inverse problems, if the FDM or your continuous dependence between the parameter and the data have those global changes like translation and dilation with the shift event, etc., that's exactly perfect for various distance because the geometric information in Bash. Geometric information invited in the way that we compare things. And the second challenge, which is, I think, is a good sweet surprise for me to realize that is a connection between version distance with the V current norms. So the benefit of the V current norms, like being stable with respect to the high frequency data or being robust with high frequency noise, we also inherit that. And that's also the benefit we observe in many applications. So, and in the end, a lot of the case, And a lot of the case, if we want to go for imaging, maybe a combination of the stronger norms on top of W-2 will be actually the best practice. So, this is the end of my presentation. Thank you very much for your attention. Thank you very much, Yunnan. Let's thank Yunnan once for her very nice talk. Thank you. All right, okay. So, now the floor is open to questions. Please go ahead. Please go ahead and speak up if you have questions for you now. So, you had a lemma about dilations. You were doing different dilations in different directions. Where can I find that lemma? I think it's in the paper with Bjorn Chris called Beyond the Cycle Skipping. Beyond Cycle Sweeping? Yeah, Beyond Cycle Skipping. Yeah, it's beyond psychoski, some beyond the cycle skipping. Yeah, thanks. You know, is p equals to two? Is it special? Are you using the auto-calculus or other p's will also work fine? So, yeah, that's actually a good question. So, in this particular web community, there's a group in France. Some of you may know Ludovic Mativir and the Grand Noble group, they actually use W1. Actually, I use W1 norm, a W1 distance for a while, and it's equally effective. I think W1 norm, I think it's just like not quadratic, it's just absolutely that kind of landscape with respect to translation. That's expected. And for dilation, I think it's similar. I think one still have convexity. Then, in the end, which one to choose is a lot to do with the numerical aspect. Like W1, you can solve it by linear programming. Solve it by linear programming. And for W2, there's also a lot of things we can choose to solve for it. I mean, this connection with this inverse of levinum kind of weakens as you leave W2, go to WP's, for example, the connection is not that strong anymore. So I was wondering if that has an effect, I don't know, like W4 or something like that, if that will have an effect. I think for W1, actually, it's in all the books for optimal transfer for applied mathematicians. For climate mathematicians, I remember there's some comment there saying W1 is equivalent to the sober of W minus one, one. Yeah, I think it's so one can probably generalize that for the entire W pieces. Yeah, but the W1, W2 are the most darby ones. So that's the result I have known so far. Yeah. All right. Okay. Any other questions for you now? Okay. Okay, if not, let's thank her. Please don't go away. There is an announcement to be made, but let's thank Yunan for her talk. Yes, thanks very much. All right, okay. So