Okay, thanks everyone for joining here. I have roughly 20 minutes, if that's correct, right? Just to double-check one more time. Okay, I don't see any negative feedback, so 20 minutes it is, hopefully. So I guess most of you might have heard about this application of graph neural networks, right? Like molecular modeling. So what do we want to do? So we have like a molecule, which means like usually the positions of the atoms and the atomic numbers, and we want to predict properties. Atomic numbers, and we want to predict properties like forces or the energy of the atom. And this is what I want to talk about here today. So, why is that important? I mean, we have different applications for that, right? We can predict like energy bindings, molecular dynamics, reactions. So at the end of the day, what we want is to design new drugs, catalysts, materials, and so on and so forth. So, hopefully, having a better future with nicer batteries, for example. So, what is the traditional approach for that? It's like doing simulations, right? So, you have like a very So, you have like a very expensive simulation which is trying to model what is going on in this quantum chemistry world. And then, at the end of the day, we get these kind of properties. And since this is so expensive, people are wasting a lot of energy or compute time on this, right? And this is where machine learning comes to the rescue, right? So what we essentially want, sorry, what we actually want is we want to use, for example, graph neural networks as surrogate models, as a surrogate for the simulation to predict these properties. These properties, and since all of you in the room and online are familiar with graph neural networks, of course, graph neural networks are a good surrogate model for that because graph neural networks cover this kind of atomic structure, this underlying graph structure of an atom. So this is what I just said. So the machine learning models are used as surrogate, as proxies, so to say, in a supervised fashion, usually. So we have some training data where we have like existing molecules and their corresponding energies, let's say. Corresponding energies, let's say. And this is what we then train in a classic superwise fashion, essentially like a regression task. And as I said, like G and N's are highly suited for that because they cover in this graph structure, which we see in atoms. And indeed, in the last couple of, yeah, it's not always, it's not decades yet, but in the last couple of years, I would say, we have seen that indeed GNNs have outperformed not only like the simulation tasks, but traditional machine learning models specifically. So like handcrafting features like these sort of fingerprints, this has been outperformed by This has been outperformed by these GNN models in the last couple of years. So they're really, really powerful for predicting molecular properties. What I quickly want to talk about here, actually the talk is split in two halves. So the first part is about like these limitations of traditional GMNs and what can we do better for molecular modeling before I switch slightly the focus at the end of the day. So I first want to talk about these limitations, which again, I guess most of you might be familiar with. Because if you look at like counting Like quantum chemistry, you can essentially, or at least the chemists know that we can more or less decompose the actual energy we want to predict for a molecule into four pieces. On one hand, we have the so-called bond energy, which is from a graph perspective, the energy which is determined by these underlying edges. So the connections between two atoms, so to say. Then we have the angular energy, which is determined how these triplets are arranged. We have the torsion, and we have the non-bonded energy. And we have the non-bonded energy. And what you now can easily see is that a traditional GNN or a traditional graph model only captures directly the bond energy, so to say, right? Because what do we do in a traditional GNN? We essentially exchange messages on the edges, right? So we have like our nodes and we are exchanging information between the node pairs. And this is more or less directly only modeling the bond energy, but it's specifically ignoring, of course, the angular information, which is extremely important for molecular modeling. Model and the second point, which is a limitation of traditional GNNs, and this is also well known here, I guess, in this community, is that with traditional GNNs, at least we cannot distinguish every possible graph, or we cannot distinguish even simple molecules in this setting. So, for example, you see here traditional examples from the graph community. So, these two graphs, which are of course very different, cannot be distinguished by traditional GMNs. But if you go to the molecular world here on the right-hand side, you see different molecules. Inside, you see different molecules, they cannot be distinguished by a traditional genus, which, of course, is a strong limitation, right? Because, of course, these molecules are different. We want to predict different properties. They have different forces, they have different energies. So, how can we make our GNNs more expressive to capture this more real-world settings? So, what I specifically quickly want to talk about here is how can we recover this angular information. So, this term here I mentioned before. The this term here I mentioned before. So, how can we recover that? And there are various ways of doing this. One some are better than the others. So, you can first of all think about let's use the raw atom coordinates. So, think about like you have your atom in living in your 3D space, right? It's living somewhere in your space here. You might want to use the coordinates, x, y, z coordinates. Of course, this is not a good idea at all, right? Because we want that the properties we have that they are invariant to translation or rotations, for example, right? If you shift around your Example, right? If you shift around your atom in space, of course, it doesn't change its underlying properties. However, for a neural network, of course, it might look completely different, right? If you're changing like your X, Y, Z positions, it's a completely different input to your neural network, and accordingly, the properties might change. So you don't have any guarantees that your model is invariant to these translations or rotations or reflection properties. So this is definitely not a good idea. What is an alternative? You can use so-called equivariant models, which are specific. Equivariant models which are specifically baked in these kinds of invariances or equivariances. So, here specifically regarding this so-called SO3 group, so rotation translation. And there are many, many popular architectures nowadays in the graph community, but also in traditional neural networks, so to say, which are proposing these equivariant models. But they are, I would say, rather expensive. I mean, this is now debatable. There's, of course, lots of research going on, but back then, they were quite expensive computers. Then they were quite expensive computation, it also had limited flexibility, so that you cannot use, for example, non-linearities and so on and so forth. So, what we were thinking about back then when we proposed this first work, I quickly show you here is can we find an efficient yet expressive solution? So, how can we recover this angular information? But we want to do it in a very efficient and scalable way. And the solution actually is more or less straightforward. So, what we do is we essentially are not encoding now any longer information. Now, any longer information about nodes, what you usually do in a GMN, but we are encoding essentially information about directions in real space. So essentially, we are encoding directed bonds, or if you want to use the graph language, simply directed edges, right? So we are using essentially this directional information. So we have here a starting point and an end point, right? So we have a direction, right? And this is what we want to represent in our graph neural network. So technically, we're essentially operating on the line graph, simply speaking. So the operation So, the operation we are then doing is no longer the standard GNN message passing update, but it's simply operating on the edges or directed edges. So, we have here an embedding, and this embedding, for example, is updated by taking into account the neighboring edge embeddings, right? The neighboring directions, so to say. And then, of course, once you have the neighboring directions, you of course can also use the angular information, right? I mean, that's trivial. If you know the directions, you can compute the angle. And this can also be fed in into your graph neural network architecture. In into your graph neural network architecture. So, by going from the simple, changing simply the perspective from the nodes to the directed bonds, you can straightforwardly encode this Angular information and you still have a very simple message pattern update, but just in a different perspective. So, there's a hand in the Zoom call. So, yeah, feel free to ask a question. Just a very, very basic question because I don't know that much about bond angles and things like that. So, from what I can see here, you're taking. From what I can see here, you're taking three vertices or two edges and defining this kind of an angle. So if there are other edges coming out from the vertex J, would there be other angles? Or is it between every triple? Or what is it? Yes, yes, yes, yes. Technically, as you see it here, the update you are doing, so for this message or the edge embedding, so to say here, this pair of JI here, right? It depends on all triplets, so to say, right, all neighbors of this. Right, all neighbors of this guy here except this node. So, at least in this case, it means you take this angular information into account and this embedding, and you take this embedding and this angle into account. If there would be another edge here incoming, right, you would also take this angle. And then you're just taking the sum of them or no? Yeah, I mean, yes, that's what we did, but you can mean like in any graph neural network or any machine learning model, you can plug and play essentially. So, this is like some aggregation, right? It can be a sum, it can be whatever. Right, it can be a sum, it can be whatever you want. Right, a mean pooling, a max pooling. And here's this is a learnable function, right? This is important, of course, to note this function here. This is like in any graph neural network, it's some learnable function, so which takes the neighboring embeddings, so to say, of the edges. It takes also the distance actually into account. So, here this distance and that distance, so the lengths, right? And it takes the angular information. Then it's somehow kind of aggregating this information. Exactly. Okay, thank you. So, in a classic GNN, right? So in a classic GNN, right, this would be a node embedding. And yeah, that's more or less it, right? So in a classic GNN, you have your node, and then you're updating based on your neighboring nodes. And you're updating based on your neighboring edges. Yes, edges. Exactly. And I mean, as I said, this is super straightforward to see, right? It's almost trivial, right? If you see it, right? But of course, it also preserves the important properties, right? It's invariant to these rotations, right? Because if we rotate this object, this atom in space, and if we translate it, of course, this information. Translated, of course, this information doesn't change, right? Because it's relative information which is preserved, but it's then invariant to these global properties, right? So it's a very simple and different view compared to these SO3 equivariant models, but it's straightforward to implement. And that was a nice insight. And it's still today, it was and is still one of the best techniques you can use for molecular modeling. So here's like a very classic benchmark data set. Sets yeah now better benchmark data sets, um, but on that data set, this dime net model hub was called. So, directional message passing was a state of the art in 11 out of 12 targets. So, target means properties you want to predict for a specific molecule, right? So, different properties, so to say, it's a molecule. And it had a significant improvement to all the previous approaches on there. So, you see, this direction information is extremely important, and you can easily encode that, right? It's straightforward. So, here, just as like So, here, just as like an advertisement. But if you're interested in this model, you can also check out an improved version of Dymet, which also takes some scalability issues into account. So, it makes it much faster. And it's still extremely good in these predictions. So, is this all? That's the question you can ask now, right? Is this direction information everything you can encode? So, here, if you look at this image, for example, right, we have this angular information. Now we have the bond, of course. So, can we do better? And of course, you can do better. Better, right? And of course, you can do better. You can add more power, so to say, to your model. You can, for example, look at so-called dihedral angles, which we did in our geometric message parsing framework. And this figure might look a bit confusing, but simply speaking, is what you also take into account are different angular information. And probably the best is actually to look at this figure here. So if you have now four atoms, right, no longer like these three we had before, where we had this angle, right? We now have this dihedral angle, which is these. We now have this dihedral angle, which is essentially the angle which is spanned by these planes. Right, you have here these three atoms which span up a plane, you have here these three atoms which have span up here a plane, and then you have the angle between these planes, right? This is like your dihedral angle, the angle, which is essentially modeling this torsion effects, so to say. And you can equivalently incorporate that, right? Of course, your message passing, your equation looks now a bit more tricky, right? And you need now a two-hop neighborhood, right? So it's not only just the neighbors, but the neighbors of the neighbor. Just the neighbors, but the neighbors of the neighbors, but with this, you can include even further information, right? So, this is what we coined the geometric message passing. So, you can see you can add more and more of this information to make your model even more powerful. And I don't want to bore you all with these numbers, but indeed, if you compare it now to our previous model here, the DIMENET model, you can see that indeed you get even more significant improvement here, predicting so-called forces, right? Not just the energy, but the forces. So, essentially, where's your molecule? The forces. So essentially, where molecule push to get a lower energy state, and you see again, this was a significant improvement in a 40% error reduction rate in this first prediction for molecular dynamics. So again, encouraging this higher order angular information, so to say, again, improves the model. So is this the end, you might ask, right? So do we want to add even more angles? Do we want to go to three hop, four hop, five hop neighborhoods? So what should we do? When should we stop? And this was indeed. And this was indeed a question we were asking ourselves. So, when do we have enough expressivity, so to say, right? Or simply speaking, can we show some kind of universal approximation results for what we call directional GMNs? So, you might know this from classic GNN literature, right? So, like any kind of MLP, you have universal approximation results that you can essentially more or less represent any function you are interested in, right, with a standard GNN. Right with the standard GM GNN, with a standard MIP multi-lapperceptron, right? You can simply speak and represent any function. So, what we were asking ourselves: what can we do with the directional GMN if we have kind of this directional information only, this relative information? So, what is their power actually? And of course, the question is now not, can we represent every function, right? Because we do not want to represent every function. We only want to represent functions which are translation invariant, rotation invariant. Invariant, rotation invariant, or equivariant, and so on and so forth. So, can we represent this class of functions which we are interested in? And the question actually is, or the answer to this question is actually not straightforward, because what I said just before is there are, or there is this class of equivariant models using this so-called SO3 representations, and indeed they are universal approximators. So, that has been shown that this kind of was a design for that, this class of representations which are This class of representations, which are translation or rotation and permutation invariant, and they are universal approximators. So, this kind of representations. But the question is: are ours, right, our models are also universally powerful, so to say? And why is it not trivial? Because these directions we are looking at, they are connected to this so-called two-sphere. So this is like the surface, right? Think about it, you have like a three in a 3D space, right? A 3D space, right? Directions only mean you're operating on the surface, right? You're not operating on every point, but directions are essentially points pointing to the surface, right? That's enough to determine what is the direction, right? Which means this is or seems to be less powerful, right? Because these here are essentially 3D representation. This is simply speaking only on the surface of a 3D field, right? Which, of course, makes them cheaper, as I have shown you before. Cheaper means simpler and more efficient. Simpler and more efficient, right? But are they less expressive? And the answer is, and that was somewhat surprising: no, they are not. So, what we could show in our work very last year already, time is running very fast, that these representations, which we call spherical representations, so these directions, they indeed ensure universal approximation guarantees for the invariant case. So, if you're interested in invariances, translation, rotation, invariance, this model guarantees that you can represent every function. Guarantees that you can represent every function, so you don't need more complex models than that. It's enough to use this kind of model class. Okay, and indeed, you can even extend it to the equivalent case by some other results which appeared at the same conference. So indeed, it's universal also for the equivalent case. So long story short, directional GMNs are enough to represent all functions you are interested in. So theoretically, you don't need even more complex models than this. So it's all what you need to do more like. This, so it's all what you need to do molecular modeling, which does not mean, of course, that from a practical point of view, it's always the best class, right? Just to be clear on that, right? Justin's theoretically, you can represent everything you want, but practically, of course, it's a different story. But this is what we have seen in the previous results, right? So, also practically, these models work extremely well. Okay, so a very short intermediate summary then. So, what did I show you? I showed you that you can use these so-called surrogate GNNs as a These so-called surrogate GNNs as a proxy for simulation, right? And they're extremely good. And specifically, if you incorporate like the knowledge about your domain, that, for example, directions or angles are very important. And indeed, you then even get universal approximation results if you design your architecture right. So these models are super fast, as you can imagine, right? Because you get a molecule and then you predict it, just one forward pass. They do it super accurately. So nowadays, even more accurate than the data you have, right? Because the data is also not perfect. Right, because the data is also not perfect, the simulation data we can do even more accurately, which is also somehow nice. But this also one strong limitation: you need the draining data, right? You need your training data to train these models in a supervised way, right? Otherwise, you cannot use them at all, right? And what is also unclear, if you have some training data, of course, the question is unclear, do these models also generalize to new molecules? Let's say you train on a specific set of molecules, right? I don't know, water molecules. Water molecules, let's say, right? How do these models generalize to other kinds of molecules outside your obvious training domain? This is also unclear. And this then brings me very quickly to the second part of my presentation. So can we get rid of the training data actually? So do we need these large amounts of training data to train machine learning models for molecules, for molecular systems? Or can we even design some unsupervised approach to do that? We don't need training data. Need training data, and indeed, the answer is yes. And we can use only design so-called we can use it in the context of up initio methods. And here, just very quickly on some high-level background on quantum mechanics, otherwise it's not really understanding what I will say here. So, how can we do this with our training data? And the core idea is essentially that you can model the so-called wave function. So, essentially, you can describe your molecule, simply speaking. Your molecule, simply speaking, by the wave function, and having this wave function, you can get the energy. So, the wave function is a function which operates on the electrons and then more or less, like, let's say, spits out the energy, simply speaking. So, this is a simplified view of that, right? And so you see here on the right-hand side, like in a specific molecule, right, you see here the atoms from before, right? This is what we used so far, right? We have our gemnet model, which is operating on this graph structure. Operating on this graph structure, and then you have the electrons flying around here, right? So, probably that's what you remember from your high school chemistry studies, and the wave function is operating on these electrons, okay? So, once we have the wave function, we can get our energy as you see it here, right? It's a rather simplistic function of the wave function, so to get our energy. The tricky part here is finding this wave function is super hard. So, it's actually almost impossible to do this exactly. Almost impossible to do this exactly for non-trivial molecules, finding this wave function. So, how can we do this? Or how can we do this with the help of machine learning? How can we find this wave function in a clever way? And indeed, the core idea is that we use neural network as function approximators for our wave function. So I actually said it before, right? Neural networks can approximate any function. So they should also be able to approximate our wave function, right? So we essentially say the neural network is our wave function, right? And how can we now use this? Right, and how can we now use this training of a neural network to find the energy? So, essentially, we do some kind of variational approach. So, what does it mean? It means we are trying to find the best parameters of our neural network. So, this wave function is now our neural network, right? So, some parameters. And we are trying to minimize the energy we are getting because that's the ground state energy of our molecule. Okay, so simply speaking, you have some function which gives you the energy, and now we are minimizing this energy by optimizing over the parameters our new background. Parameters or neural network. Okay, and that's this gives us a non-state energy of our molecule. So, again, the point is here: the wave function is approximated by our neural network. And it cannot be an arbitrary neural network, has to fulfill some specific property. So it has to be anti-symmetric. That's actually the only property. So you see the property here. If you switch to position, so to say, the sign has to fit. So this can be done. I don't tell you how in detail, but this is the only constraint our neural network has to fulfill. And then once we have that, Network has to fulfill, and then once we have that, we can train our neural network to find the optimal energy. So, how does it link actually to graph neural networks? Right, this is what I wanted to talk about here today, because this has nothing to do with graph neural networks, right? These are just arbitrary neural networks. And by the way, some limitations of that. So, this works extremely well. This is extremely accurate. So, this has been done in 2020, but it only works for a single geometry. So, only for a single atom back. Single atom back there, and this actually is then the bridge to the graph neural network field, right? Because we do not just want to do it for a single atom, for a single geometry, but we want to do this for multiple geometries, as you can imagine, right? So, for example, we have here the so-called potential energy surface, where we have a specific atom, for example, change the distance between two atoms, right? Or we might even change the complete geometry of this object, right? And then, of course, the energy of the molecule also changes. So, we want to adapt. So, we want to adapt the wave function to the underlying geometry. We do not just want to stick to a specific fixed geometry. And as I said, this is where the graph neural networks come into play, right? Graph neural networks capture well geometry, as we all know, or they are designed to capture geometry. And the core idea we had in this work is: let's use a graph neural network then to predict the wave function parameters, right? Plus the additional physical constraints we have to take into account. So, this model we So, this model we call PestNet here, just to almost wrap up my short presentation here, is more or less really straightforward. So, we have a, we call it the emeta GMN. It takes the geometry into account, so it takes the nuclei into account, which includes the geometry. And then what we get out is the parameters of our wave function model, which represent the wave function, as I said before. This wave function then can be optimized with this variation approach I mentioned at the beginning. Variation approach I mentioned at the beginning. So we're using a GNN to see essentially how the wave function has to look like, and which then allows us to cover different geometries in this kind of variation monte colour approach. And this entire architecture then is enter into differentiable. So we are training the G and N and we're training the wave function model, of course, simultaneously in a differentiable way. And then it works out of the box for multiple geometries. So without doing this expensive approach, multiple times for different geometries. Approach multiple times in different challenges. So, one quick last experiment, and I don't want to discuss this in too much detail. So, what you see here is essentially the baseline, which is the experiment, the real numbers, so to say, we cannot reach that, of course, but here, this line here and this line here, which is almost the same, right? This is our approach, which has multiple geometries, and this is a competitor which only can use one geometry at a time. So, for every dotty, essentially, this approach. Dotty essentially, this approach has to start from scratch, and here ours is doing it in one shot. And you see, they have almost the same accuracy and much better than many, many very expensive simulation approaches. So, no loss in accuracy. And this, of course, is the selling argument much faster. So up to 42 times faster than competing approaches. So if you look at the Spermi net, 4,000 hours on a very simplistic molecule, only 90 hours. Only 90 hours for these different geometries of this molecule. So indeed speeds up the training significantly by taking account the different geometries at the same time. Okay. And this is actually already the end of my presentation. So what did I show you here? And that's the take-home message. So you can use GNNs in two different ways for molecular modeling. On one hand, you can use the more established, I would say, the more famous so far. Famous sofa GNS as a surrogate model. So, as a replacement for a standard simulation by requiring some training signal, right? You have some supervised model and then you're predicting and training and predicting these energies, which is super fast and accurate. Or you can do even more accurate, but also slightly slower. But you don't need training data if you do some kind of up initiative calculations using, for example, the wave function, as I showed you before. But in both cases, GNNs help you to model. Cases, GNNs help you to model the different geometries of your molecules. With this, I want to say thanks for your attention. Of course, a big thank you also to all the collaborators who helped along the way with all these works. Yeah, thanks for your attention, and I'm very happy to answer all of your questions.