I work in the area of detective characterization. Sounds like I'm going to be covering some of the areas that Gabriele was alluding to just previously, specifically the glitch problem here. I have a lot of very similar thoughts and questions and concerns with Gabriele. So very similarly, I want to just kind of talk about some of the key problems in this area. People here, I'm sure, are very familiar with some of these aspects because it really has a big visual impact on some of the astrophysics. Visual impact on some of the astrophysics we do, but then concluding with a lot of questions or maybe more of a wish list on my part of what I'm looking for. So, I think one thing that actually has been a big challenge in Grabbish Pike Astronomy is that we've been too blessed by the universe and had too much luck in some of our early discoveries and set unrealistic expectations of what our astrophysics is actually going to look like. So, 15914 here is an example. Our spectrogram data, it's absolutely beautiful in both Hanford. Absolutely beautiful in both Hanford and Livingston, and this is what we kind of thought would be our expectation of what future things might look like. But in reality, if you look at most of our gravitational wave events, they look much more like this. So there's a lot of things going on here. This is showing here Livingston data. In addition to the really beautiful gravitational wave signal right here, we actually have two other separate things happening within half of a second of this gravitational wave event. One of them, here's Wave event. One of them here is a camera shutter glitch, which was someone at the site was doing a time-lapse photograph of some install work. He was taking a picture every two minutes. They left the camera on after they left. And basically this right here, the same power and amplitude as this gravitational wave signal, is a Nikon camera clicking every two minutes. And that actually ruined two weeks of data for the CWU search in particular because of how precise they needed. Particular because of how precise they need those types of things. At the same time, we also have a fast scattering glitch right here, which is coming from the same type of physical source that I got earlier was talking about with scattering. That's actually something I'll talk about more as well, because that's a really, really key problem for us in detector keratization. And the other thing about this that I wanted to highlight is this empty box here for Hanford. This was actually one of our single detector candidates. This has greatly increased some of the challenges of doing this analysis. Increase some of the challenges of doing this analysis. You might ask, how do I know this is a gravitational wave signal if I don't have any counterpart and there's all this mess happening? I think it's relying on the fact that we have very good modeling of these signals and understanding of our detector. It usually doesn't produce shirks like that, up to a few 1 per thousand US types of projections we have. So we can be fairly confident it's a real signal, but all of these complications make it very, very difficult to do our downstream data analysis. So these glitch problems. Data analysis. So, these glitch problems, this glitch problem comes in many, many different types of shapes, different morphologies. We give them really kind of cute names, like we have glips, extremely loud, slow scattering, fast scattering. But the main thing here is the diversity of the different glitch categories. And this has been one of the biggest challenges that we've encountered in trying to develop robust algorithms to address all of these problems. Even some of the more generic algorithms. Even some of the more generic algorithms have had issues spanning this wide array of different morphologies. So we have basically millisecond-long broadband things like these blips to these very, very structured glitches like slow scattering that are multiple seconds long. Fast scattering here is a repetitive feature that's happening for many, many seconds along the, I guess, in this, going beyond the spectrogram here, as well as extremely loud here. This is actually about a SNR 10,000 glitch as well. SNR 10,000 glitch as well. And so there's also that wide variety of different amplitudes. So this type of generality problem is going to be a huge issue for anyone that's trying to develop robust ways to address this problem. How this comes in to a lot of our astrophysical analyses right now is with glitch overlaps. So these are a problem for, let's say, discovery, trying to confuse them for real signals in some cases. But now that we have hundreds of CPC events, I think one of the biggest problems on the CPCC. I think one of the biggest problems on the CBC side is on the effect of our parameter estimation and the downstream analyses, and this comes in through our overlaps. So, this is a pretty significant issue as well. About 20% of our candidates in the recent catalogs have had overlapping glitches. This, it's not as bleak as that number suggests. This is including some extra time padding to account the full duration of parameter estimation. So, it's not necessarily overlapping the 2-2 mode of the signal. The 2-2 mode of the signal, but still, with the very short duration of signals that we usually see in binary black holes, this is quite high. So, we're very concerned about biases on our resulting PE since a lot of the parameter estimation is assuming Gaussianity, non-stationarity. So, one of our main steps is going through and trying to model and subtract these glitches before we ramp parameter summation. A really fantastic example was 17 by 17, which I think really kick-started a lot. Which I think really kick-started a lot of this work and really highlighted to everybody how important this would be going forward. So, this right here is showing an example. This is 170817 in the background here, and the very, very loud glitch overlapping at Livingston. And then also zooming in, showing the actual strain data. And then in orange is a model that we used to subtract it before we did all of our parameter estimation in that case. Another example here of a particularly terrible overlap is this is the 2-2 mode of a. Is this the 2-2 mode of a gravitational wave event, and this is a fast scattering glitch, and it just threaded the needle in between the two of the successive bursts. But this is something that definitely needed subtraction or some type of data mitigation before we could proceed further actually doing our analysis. This is something that's highly human involved, even when we're using more generic approaches to modeling these glitches. It often takes a couple times to get this right, multiple steps of review. So, definitely. Multiple steps of review. So, definitely something that we can have a lot of improvement going forward. Some of the tools we have in our toolkit to try to address these things can be split into a couple of categories. I have some examples here, so there's a wide variety of additional things I'm not going to just highlight here, but kind of splitting them into some of the analytic and machine learning approaches. First, in the identifying glitches category, just knowing what our enemy is. We have some analytic approaches, like basically looking through. Analytic approaches like basically looking through spectrograms. This is something myself and a student developed, where basically we look at individual Q tiles, basically the wavelet matches that get interpolated into the spectrogram to actually see bits of excess power. And then there's also machine learning implementations. Gravity SPY is one of the main classification algorithms we use, which is taking these images and basically doing image recognition issue. We have a wide variety of different, in this case, specific glitch classes that we Specific glitch classes that we know about. We present it with images and it classifies it into one of the many options it has. There's also a citizen science component to this, which I won't talk about today, but if you want to help out and do some own classifications, you can on Zooniverse. The other area is tracking glitches, going into some more of the things that Gabriel was talking about, actually trying to address the problem at its source. Some of the analytic options include these wide variety of sensors, which we have too many and too. Of sensors, which we have too many and too few of. One area where we particularly have a good handle is PEM, the physical environmental monitoring. This is something where it's a very labor-intensive process of actually going in and doing physical environmental injections, which involve physically making sounds, shaking the ground, driving cars by, a wide variety of different things to actually try to get a measurement of that coupling between our witness sensor and strain data. And with that, we're able to actually project our sensor. Actually, project our sensor motion into our strained data and try to estimate if there's actually noise from that physical source in our trained data. Another option on the machine learning side, IDQ, is a tool that's trying to find correlations between glitches and auxiliary channels, these other witness sensors, and our strain channel. Based on the likelihood of the correlation or the significance of the correlation, it can try to predict if there's actually a glitch in our strain data. So, this is an example here of a whistle glitch in, I see. Of a whistle glitch, and this is the strain data here, and it's showing that there's a very, very clear correlation in quite a few channels at the exact time of this glitch. So, a pretty good handle. And this is something we've also been using quite a bit now, especially in 04. And then on the glitch subtraction front, this is something where we still have mostly analytic methods. So Bayes Wave is this key generalistic model that we use for a lot of these cases. What this is doing is a wavelet decomposition of glitches. Decomposition of glitches. In theory, you should have enough wavelets. You can model almost anything. The problem is the wide range of different morphologies. It might be two wavelets, it might be a thousand wavelets. On modeling glitches on the machine learning side, this is an area where there has been quite a bit of literature, but not as much, you know, basically not enough to get competitive with some of our more analytic techniques or have us effectively use it in our analyses in the LBK. Use it in our analyses in the LBK. This is an example of one of the denoising pipelines. Sometimes denoising is thought of basically trying to remove the background noise to isolate louder signals. But from my perspective, if you're removing the background noise to isolate the excess power, you can do the opposite. You can model the excess power and then you subtract that to have your clean background noise plus signal for parameter estimation. So we do have a number of these options, but as we go forward, there's going to be. But as we go forward, there's going to be some additional challenges that we're going to be facing with next-generation detectors. So, ultimately, just like some of the problems we talked about yesterday, more signals is going to be a key issue, but it also is going to mean more glitches that we're going to have to deal with. If we're going to have hundreds of thousands, a million gravitational wave detections with next-generation detectors, even projecting the 10-20% number that I was quoting before, that's hundreds of millions. Number that I was quoting before, that's hundreds of thousands of different events that we'll have to de-glitch, subtract, analyze, make sure we're actually doing that to high precision to make sure it's not contaminating our catalog. So this is a very significant issue. How do we scale up our very current human-intensive processes to deal with the large population of signals apprenticeship in the future? But it's actually even worse than this. One of the big reasons for developing our next-generation technology is our low frequency. Our next-generation detectors, our low-frequency sensitivity actually makes things quite a bit worse in terms of glitch overlaps. Basically, our signals are going to be much, much longer, and therefore, that's going to significantly increase the number of overlaps you see. So, this is some projections we've made of basically extrapolating the low frequency sensitivity to see what the chance of, let's say, impacts on our search sensitivity and glitch rate for a range of different or overlap of glitches with different glitch rates. Overlap of glitches with different glitch rates. The shaded region here is indicating the range of glitch rates we've seen in any of our advanced gravitational wave detectors in the past five years. So this is ranging from one every few minutes to tens per minute. I think actually we're starting to get a little bit lower in some of our detectors. It's been back and forth, I think. It hasn't been a uniform climb down to lower La Traits. To lower blitz rates. But this, I think, is a decent expectation of what we might expect to see in future detectors. This is going to, one, have an impact on our overall sensitivity, but more concerningly is the glitch overlap probability. So this is direct overlap. So basically, basically overlapping the 2.2 mode. And for our low-mass signals here, like 1.4 by neutron star, at current glitch rates, there's a 100% chance of There's a 100% chance of there being a glitch overlapping a signal we see. This is actually something we've seen in our current detectors. Basically, I believe all of our low-mass candidates, ENS, BH, or BNS, have had data quality issues overlapping them due to their length. If it's not all, it's maybe all but one. But this is a very significant issue for these types of signals. One thing that I've added here to scare some of the burst people in the room is this dotted line. People in the room is this dotted line actually of a supernova. So, basically, thinking of a tenth of a second-long supernova, what's the probability of that really exceptional signal having a glitch overlap? The good news is it's much shorter, so it's a much lower probability, but the fact it's not zero here is quite a cause for alarm. So I think we do need to do a lot of instrumentation work to try to prevent this for these types of exciting signals, no matter what type of analytic techniques we have. So, from my perspective, similar to Gabrielle. So, from my perspective, similar to Gabriarelli, I think we're still building the tools we need for precision analyses in this real data. We're still in the first couple years of actually dealing with this since 17017. So, there's a lot of things I think we still need to build to be ready for these types of next generation, large number of detections that we're going to be anticipating. So, some of the roadblocks, first off, is it's somewhat hard to actually do the glitch modeling subtraction. I have a couple cases studies here to demonstrate this. The first one is 2001-29. This is the This is the most highly processing signal we've detected so far based on our primate destination. So, this procession shows up in our signal as basically modulations in the amplitude of the gravitational wave signal. This is also particularly exceptional because it has lots and lots of data quality issues. So, not only is there, so Hanford, I think, is pretty clean. Livingston has some issues that you can see right here. Virgo has quite a few things going on here. Livingston, at the time, was the most sensitive. Livingston at the time was the most sensitive of the three, so it's driving the measurement of this procession. So that's what we'll focus on. But in addition to having a glitch right here, we were able to use one of our mini sensors at the site and figure out where this was coming from, get a decent understanding of what was going on. We were able to actually figure out that there was not only glitch power right here, but directly overlapping the signal around this region, which was a huge issue, particularly since we're looking for these modules. Huge issue, particularly since we're looking for these modulations in the amplitude. And the time-frequency overlap of this excess power and the signal is exactly where we're going to expect to see these additional modulations. So, one thing we did with a group of myself and others at Caltech, we tried out some additional data modeling techniques using Bayes wave. So, first, we used a witness sensor to do the subtraction. That was what was originally used to make the precession measurement. We tried out multiple different ways of approaching a Bayes wave subtraction. Ways of approaching phase wave subtraction. And what we found was that depending on what model we use to subtract the glitch, we can have very different results between basically a vanilla black hole to a maximally processing signal here. So we don't have any conclusive statements about whether this is or is not processing, but the key takeaway here is that your modeling has a very significant impact on the astrophysical parameters that you record. Similarly, another exceptional event, thank you. Similarly, another exceptional event, 191109, this is the most negative spinning event in our catalog so far. It also has data quality issues in both detectors as well. We can see issues right here at Hanford. It's disjoint from the signal, and also Livingston is more sensitive, so we'll focus again on Livingston. But this actually, this tower down here, directly overlapping the time of the signal, is another scattered light clutch. So in this case, rather than having a witness sensor, since it's very difficult. Having a witness sensor since it's very difficult for us to actually do exact projections of a witness. We use modeling basically as making some basic assumptions about the behavior of the scattering source. It's some type of linear oscillator. We knew this was actually coming somewhere near suspension system. So that was a decent approximation. We tried some additional approaches to modeling the glitch, and we found that basically similar to 20129, the exact time frequent police see placed. The exact time frequently placed that we expect to see the imprints of this additional spin measurement are where we have the glitch overlaps. This was a case where, again, we found that the modeling was an important part of this measurement. And if we actually throw out this data that's corrupted completely, basically we have unconstrained spin measurements. So it is very important that we actually can well separate our signal and glitch to actually make this type of a measurement. So I think these are actually so far, at least. So I think these are actually so far, at least with the events that have released the most extreme spin events, and they have extreme issues with these data quality problems. Other things that haven't been highlighted as much include the cases of weak glitches. So almost all of the energy we've spent is dealing with our high SNR, tens to hundreds of SNR glitches, things we can really see in the spectrograms. But this is a plot here of a distribution of access, basically glitches we see at our detector on a daily basis. Daily basis. And if we just project our power law qualitatively backwards, this is suggesting, in addition to having this high SNR population of glitches, there should be a very large population of low SNR glitches that may potentially add additional bias. At the very least, this is violating our non-Gaussianity assumption. There's some Gaussian fluctuations and non-Gaussian fluctuations on the same scale. And this is something I don't know if we've really addressed or understood how this might actually bias our analyses. How this might actually bias our analyses. Particularly concerning is that these types of weak glitches you might not be able to distinguish on their own from Gaussian fluctuations. So it's unclear how many actual gravitational wave events could be affected by this type of a problem. And finally, the biggest issue is the new glitch behaviors. So this is again a generality issue. For scattered light, we actually did, it was one of our biggest problems as I've highlighted. It's overlapping a lot of our gravitational wave events. A lot of recreational wave events. We've done a lot of work to try to understand where it's coming from, of starting to model it, but ultimately, we were able to actually do a lot of successes. But when we go move from O3 to 04, it's changed completely in its character. So this is kind of a scattered light glitch here in O3. This is our O4 example of something that's light scattering, but not necessarily from the same physical source in the detector. This we've been calling Karens. Here's an example of a rock Karen from Band. Example of a Rob Karen from Banff, if anyone's not familiar with that term. So, this has really set us back a bit because we've done a lot of work on O3 and it's hard to extrapolate this down to O4. So, are the most exciting astrophysics discoveries just noise? It's hard to tell at this moment, but for everybody who's excited about machine learning problems, in terms of the wish list that I hope we could try to build in the future to address this problem, there's a couple of different categories I think we could use to address this. First is just better. To address this, first is just better harness machine learning for Dechar, taking the types of things I've talked about already, and just applying some additional machine learning techniques, identifying glitches, making this faster and less human involved. It's going to be a key part of any other solution we've developed. More quick glitch subtraction with machine learning, try to address those hundreds of thousands of cases we might have to deal with. That's something that people should have some interest in, as well as improving the detector performance. We're going to have to try to cut down the glitch rate to have. To cut down the glitch rate to have a more pristine detector in the future, but this isn't something that we're going to be able to get rid of completely. There'll always be new glitches. So, this I think is going to be a part of any other solution as well. So, these are kind of the core principles. Next up is trying to train our algorithm on glitches. So, this can include things like joint modeling of glitches and CPCs. This is something that I know analytic pipelines right now are starting to be able to do. But if there's machine learning tools as well that can handle both of these at the same time, both cases. Both of these at the same time, both cases of excess power, that would be the ideal solution, as well as also some additional generic modeling approaches. I think some of these types of joint modeling may have issues with the more subtle effects like weak glitches or just low SNR overlaps. So trying to train model on real data that's relaxing this requirement of Gaussianity, I think, is one way that we could actually improve upon our current approach. It's something that I'm not sure we could actually do analytically. One other option we have. One other option we have that's a bit heavy-handed is let's just throw out a bunch of data. I'm actually including this as a technique because it's not that simple. The first one is that if you're going to be throwing out a large fraction of your population, you want to do that in a controlled manner. You don't want, if you're going to be throwing out 10%, you're going to have significant biases in your downstream measurements, particularly since there's a bias for low-mass signals to have these types of overlaps. So, any type of rejection process that you're developing needs Process that you're developing needs to be a part of integrated into your full pipeline to make sure it's part of your selection function. At the same time, unless we have extremely good ways of identifying all of the potential non-Gaussian features, you're not going to get all of these features. So you may get a large fraction of the worst cases, but the really subtle cases, and like some of the things that I've actually talked about is those examples might actually get through your data selection process and still lead to significant bias. And still lead to significant biases. So then finally, I know Deshaun's coming up here. So I'll just note that one other big picture approach that I would advocate people to start thinking about is instead of trying to address a lot of these problems directly, you know, of trying to put into a parameter estimation, remove the glitches, modeling the glitches, trying to lean into this and think bigger and have much more complex, large-scale analysis pipelines. So one example here. Analysis pipelines. So, one example here would be trying to account for these biases and how we run our population estimates. Rather than ingesting individual set of NPE that we assume is bias-free, we could try to do large population studies with the entire strain data stream to get understanding of what's the range of effects that glitches would have on a population. This, I think, has some benefits. There's many, many ways that a single glitch might overlap with a single signal. So, this is very hard. Single signal, so this is very hard to deal with on an event-by-event basis. We do have fixed observing runs with fixed data, so it may be easier actually if we have the computational resources and the tools available to understand what is the range of possibilities or real data in this observing run could impact on our population rather than looking underneath, for example, individual events to try to pull things out. So, to conclude, just another thing I wanted to highlight: key decisions. I wanted to highlight key decision making, which was something that we talked about yesterday, is something that machine learning can be really helpful with. So, one of the most important parts of glitch characterization is naming these glitches. So, just in a comparison here with some of our examples, from the humans, we have koi fish, raindrop glitches, tomte, these are Norwegian, Danish gnome-type creatures, and then fringy, the sea monster, that's a really fantastic name. I also asked ChatGPT, what are we gonna be naming our glitches in the future? It has some good examples. Feature. It has some good examples. We have our vortexes, flicker shifts was the name it came up with: Sparks and Hiccups. So look out for those in the future. This is what we're going to have to be dealing with come 20 years from now. So thanks, everybody. Question. One of the strategies you mentioned is reducing the number of users is that actually Is that actually possible in general? Because I thought as our noise floor gets lower and lower, we are just going to get more. We're going to get more. We're pretty successful, as Gabrieli noted, at doing this by hand with humans. That's actually one of the problems is there's a selection function and the types of things that actually impact our events. The things we understand the most and are the easiest to address, we solve in the instrument. So the things that are left are the really, really complex. The things that are left are the really, really complex, challenging types of glitches that end up affecting the most signals. But if we can try to address those more challenging issues and reduce their likelihood, that's always going to be a way. Anyone online? Let's thank Derek. We have Jay. I was just going to say that a tenth of a second is like when too shortly. 