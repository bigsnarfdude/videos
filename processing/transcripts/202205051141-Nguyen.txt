Okay, so yeah, we're a little bit late on schedule so I will try to be as brief as possible so Savannah can talk as well. But if you have more questions we can talk at lunch or you can contact me either on those addresses or also on Instagram if you want my Instagram or my dog's Instagram. I don't know. We will see later. So I will talk today about constrain neural networks for increased transparency and we will see later what I And we will see later what I mean exactly about a four-a-constrained neural network. But before we go into the details, I will give you a quick glance of how taxonomically organized all the models, just to give you a better idea on how my research is positioned in the landscape of interpretability. So let's say that the first two groups that you can find in interpretability are underhealed method. In interpretability, are under hockey method and post-hoc methods. And these are just the two groups where you can divide all the methods depending on where you design in the interpretability. And most of the models that we have seen in this workshop are probably anthropomets. So the models from Ben and Rich, or the models from Lintia. And while in the postdoc group, we While in the post hoc group we have all the stuff like feature attribution, usually a post hoc and stuff like this. So methods that we apply after we train a model. Then in terms of information that we want to show, we have three big groups of interpretability methods. The first one is mechanistic transparent where we can actually see how the model reasons and does the And that's the decision process. The size of our sample is predicted, right? And usually this corresponds to anti-hoc methods, but it's not always true. For example, LIME is a linear model, but it's usually used in a surrogate method, so it's not an anti-hoc, but it's more a post-hoc. And then feature attributions are usually post-hoc methods, but for example. Post-hoc methods, but for example, attention models are feature attribution methods that are used in an anti-hoc way. And then, like, we have example-based methods, but like, I'm not going to go into the details because we haven't seen any of them. And another important thing to say about this taxonomic subdivision is that they are very different ways of visualizing the information, and actually, the amount of The information and actually, the amount of information that we get from these groups are very different, so that's why we cannot really compare them between each other. So, after, like I told you about all this taxonomic classification, like the reason why I wanted to give you this language is to better understand where the state of research recently was. So, lately, So lately, like the trend of research in interpreted machine learning was let's first train a black box model and then we apply a postdoc method on top of it. And for example, here we can talk about dog and cats like we like to do in this week, right? And then we have, okay, we have the image, we pass it through a neural network and then we get the probability distribution over the docker cats. Distribution over the blocker cats, and then like we apply a post-hop method, and then we ask the model what is it focusing on if we want to explain the prediction on cat, right? And so this is the usual process. And why most of the initial trend in interpreted machine learning was like this, was that there was this general belief, like you probably heard it a couple of times during this week, that basically. That basically accuracy and interpretability are in a trade-off. So, basically, the higher the accuracy, the less interpretable the model will be. But as multiple times we repeated in this workshop, this is not always true. And this was like one of the catchy titles of the papers of Cynthia, where she basically asked us as a practitioner. Ask us as a practitioner to not use black box machine learning models but use directly interpretable models. And this was reiterated by Rich at the beginning of this week with his EBMS models. But this is, like as they said, this is applicable only on tabular data. And this actually doesn't really scale to a more complex domain. Maybe text in some cases in some particular tasks. In some cases, in some particular tasks, if the text classification is particularly easy, but for example, on images where your single feature usually, the single features that you pass through the network are the pixels, then like there, these models are not usually applicable. I mean, at least I don't see how to use EVMs or sparse precision trees on images. And if, Ben, you have a different idea on how to use EVMs, you can interrupt me whenever. You can interrupt me whenever you want. So, yeah, so that's why, like, recently, I mean, I don't know if this was exactly the reason why these people did their research, but I think the research fits fairly well in this context in the sense that lately we found a more line of research in interoperability which focuses on neural nets. Which focuses on neural nets with some kind of constraints to make them more interpretable. So the first one is attention-based models. Okay, like this one, we know for sure that it was not for interpretability, but they wanted to demonstrate that actually attention mechanisms are good enough for learning functions. But we can actually locate them in this group of models. And then we have self-explaining neural networks. I will show you a little bit later some details. A little bit later, some details on ProtopNet from Cynthia's lab, and yeah, but I will not go into the details of this one. So, to my model. So, my model is called FLAN, we will see later why. And the basic motivation of my model is basically so the way I designed this model was basically inspired by linear models, right? And by answering one. By answering one basic question, why do we usually think linear models as interpretable? And okay, like there are some caveats because linear models are not always interpretable, but we can discuss those later. So in my reasoning, and you are free to disagree and we can discuss that, but there are two main reasons why linear models are interpretable. The first one is the separability, meaning the separability of the features. The severability of the features, in the sense that, like, when we have a trained linear model, then we know exactly how we can separate the effects of each of the features, right? So, this basically gives us a modularity property to interpretability. Because if there are a lot of interactions, it's going to be difficult to understand the model itself. And the second one is predictability. And the second one is predictability, which is a little bit related to separability, but what I actually mean with predictability is that we know if we basically because we know the functional form of one feature on the output, we know basically how to modify the feature and then which effect it's going to have on the output. And this gives us a lot of understanding about the model, and then we can do many things like editing the model, like reach the model. At the model, like Rich demonstrated on Monday, right? And maybe if the model is actually representative of what's happening in the real world, then the model is going to actually be actionable in the sense that we know what to modify in the real world to actually obtain the outcome that we want. Now that we have these two elements, we can see how the model that we created. Created extends this idea, these two components of interpretable models, of linear models. So, the first one is to apply a non-linear functor to each feature, and this is where we see like this is the main thing that makes my model comparable to EVMs and neural article models, and pretty much it. And the second And the second component is that we actually apply another non-linear function on top of the sum of this processed single features. Just to have maybe a more visual understanding, I don't know if it actually helps, but we have our sample, we divide it in our single features, and then each single feature is processed and it's gonna be projected in actually embedded in a service. Certain latent space, and this is this happens for all of the features, and then we're gonna sum them all together, and then we're gonna apply another nonlinear function on top of that. So, and this is, and yes, I forgot to say that, like, the way how we implement all these functions are neural networks. So, all these functions are neural networks, and then we can train everything just via backpropagations, to castigrade the descent, and all the variables. The gradient descent and all the variants and whatever. So, yeah, I'm just gonna skip that. So, since we already talked about explainable boosting machines, like so EBMs, which on Monday and here are the models, I will draw some parallels between the two models. So, the main commonalities that, as I said, they also they they also generalize linear models by like this is typical of generalized additive models and what we have is that each feature like we have like a non-linear function of each feature and then these functions the output of these functions are summed together to to not obtain the output so in EBMs if I'm not mistaken there they're decision trees correct right yes thank you Right? Yes, thank you. And then in their latest work on neural additive models, these are neural networks. So what they obtain here is they maintain actually exact separability between the features and they actually can do, they have exact predictability as well because they actually have the functional form and as Rich show, you have the whole graph. The whole graph of each of the features along the range of that feature. The only thing that makes them maybe arguably less interpretable than linear models is that to actually understand the effect of the feature, you have to plot the whole thing or look at the actual formula. While linear models, you just need to, the coefficients. So basically, from logo, Need to the coefficients. So basically, from local information, you have global information. So this is like the only thing that I would say, from my interpretability perspective, differentiates them from linear models. But the problem, as I said before, is that this is not applicable to complex stats. At least I didn't find a way to do this. And if they actually have If they actually have interactions in the data itself, they have to model it directly by adding, as mentioned, they will add like second order interaction or sometimes third order. And in more complex data, maybe there are higher order interactions. So as a summary, and just to continue on my present just to have you have clearer ideas, so the three steps of AFLA model. Steps of AFLAM model. Oh, I just realized I didn't put anywhere what the acronym means. Well, it means feature-latent additive neural networks. And yeah, the name is a little bit retrofitted, so maybe it doesn't really mean anything. Anyway, so the three steps of a flat model are, as I said, like, okay, like we divide the sample in its single features. Each single feature is then mapped to a latent space by different. Latent space by different neural networks, then we sum all the latent representations, and then we apply a final neural network for the prediction. And actually, it can be somehow proven, so okay, there are some caveats, but actually our model is a universal approximator, so it means that basically can represent any function. And this is based on. And this is based on the Kolmogorov-Arnold representation theorem, but okay, let's not go into the details, but this is not an arcane thing. This is the same theorem that is at the basis of the universal approximation of neural networks. And the very important thing to take away from this theorem is that actually what what it says is that to represent any function we don't actually need to uh uh to uh model the the interactions, but we The interactions, but we can just process each feature separately and then sum everything and then apply a function outside. So, exactly how the flunts are modeled. Now, now that we have this background on our model, so how it works, and that it can actually represent any kind of function, the question might be: how do we actually interpret these kinds of things? And the first thing is, so usually when we So, usually, when we want to interpret a linear model, we take a feature and we see what's the effect of that feature on the output. And with our construction, we can do this simply by, let's say, if we want to see the effect of this feature, what we actually do, since this is a simple sum, we just don't process any of the other features, we pass only a single feature to its corresponding. only a single feature to its corresponding to its corresponding function and then we just pass it through. And then in this way we have an understanding of what kind of evidence the model takes out from that single feature. But we have to be fair careful because that is just an approximate interpretation of the feature because the formula is not important but Um the formula is not important, but the thing is that uh these words like uh the perfect interpretation could work if the if the function afterwards is uh is exactly linear. So basically how it works in the EBMs and nums. But in our case, since we want to work on more complex data, we actually have this trade-off between accuracy and interpretability. So and this is So, and this is localized in the how, let's say, how linear is the function there. So, if it's not linear, the interpretation is not going to be very good. And then, like, we actually have a second way of interpreting this model, and it relates to feature attribution. So, basically, if we, since here we have a sum, right, then, of course, like. Then, of course, like in the sum, the most the what is going to contribute to the sum are those vectors that have the highest norms. So, if it has a very small norms, then like that's not going to be important. This means that if we look at the norms of the projected features in the latent space, then those are going to be indicative of the importance of that feature. And we're going to see some concrete examples, right? Concrete examples rate later. And I want to point out that this is just indicative because the thing is that, like, if it's almost zero, then it's not going to be important. But if it's higher than zero, since the function afterwards is nonlinear, like we don't really know if it's actually used or not. And then before going to the result, we have a couple of remarks. So I shown that the basic way to use The basic way to use our model is just by dividing the sample in all the separate features, right? But this doesn't have to be the way to use it. You can actually import some hand-engineer features. But for very high-dimensional data sets where for example, image data set where you have tons of pixels, what you can do is actually divide the sample in different groups of pictures, right? So basically, the way So basically, the way a transformer does it is by dividing the image in different patches. Like the important thing is to have patches that are not overlapping so we can maintain the separability of the interpretation afterwards. Okay, so let's see a couple of results. So the tabular data is just a signage check, so we don't really have much to say like it's a Much to say, like, it's a they're usually very easy data sets. In some cases, even the logistical regression is doing fine. EBM is doing like a we're comparable to EBMs and SENS and neural networks. And as I said, like this is, for example, here we gain maybe 0.009, but like it's really not relevant because VPNs are much more interpretable that are flat on temporal data, so there's no Tabular data, so there's no reason of like for using FLAN in tabular data. So this is really just for some check. Then a little bit more interesting are images and text data set. And okay, like we have NIST again for sanity check, we reached 99%, okay, 99.05 have a higher, but like we also didn't spend too much time optimizing this. Time optimizing this. Like a very interesting thing is if we look at the SVHN data set, which is like a slightly more complex MISC dataset, where we actually do better than a visual transformer, although the visual transformer, so the result is taken from the author's paper and they have some reasoning about why he's performing. Is performing solo and among those, like it's probably an issue of resolution and stuff like something similar. But like an important thing to notice is that ICAPS is another interpretable machine learning model based on CAPS networks from Hinton, and they're like we're doing better than them. And the CAP dataset is a subset of image net mostly focused on birds. And they're of birds and they're like they're we actually much lower than the current the current models and then we will discuss later why and yeah in the text the text data set we have more or less the same trend so we see that like okay like one important thing to note is that in the curve data set we have 200 classes so 70% is actually not that bad but still we could do better there is as a Still, we could do better. There is, as you can see, there is still a lot of room for improvement. So, yes, key takeaways. So, tabular data is just sanity check. Then, on more complex data set, we're doing pretty much similar to other interpretable models, but we, as I said, we have a very low accuracy compared to other unconstrained neural networks. So, I didn't really have time to dig into this. I didn't really have time to dig into this, but there might be a couple of reasons. It might be just a pure learning problem in the sense that, like, maybe we need to do better architectural search and better learning dynamic. So, it might be that for some reason our model is more difficult to train. And this actually brings us to the question: do we actually maybe need to model interactions? So, we know that the model itself can represent any function. The model itself can represent any function, but the problem is that maybe it needs to actually train it better, it needs a little bit of inductive bias. So maybe we do need to input some kind of interaction in the model itself. This is future work, I will repeat it later, but I haven't done it yet, but it's something that we should look into. Okay, for the sake of time, I will just take the most visually appealing results on the interpretability. Results on the interpretability side. Here is just all qualitative. But this is a sample from the CAP data set. So it's an albatross, a correctly predicted albatross, and highlighted in red. I think I fresh showed it then, but it doesn't matter. So highlighted in red are the top 10 or 15 patches for this sample. And we can see that the flam is doing reasonable. The uh flan is doing uh reasonable thing, the sense that like it's uh focusing on the eye, on the head, on the beak, to uh to classify a bird as a particular breed of bird. And this is the feature importance analysis part of the interpretability. But if we do the more algorithmic one, then like what we can do is that, for example, we take one of the most important patches of the image and then we pass it through the network without. And then we pass it through the network without all the other information, and we can see that, for example, close to the eye, we have some evidence to say that it's an albatross. Okay, like it's the second highest probability for that patch, but still we can see that that part gives some kind of evidence towards the albatross. And the same thing goes for the top of the head, although the albatross is not the right size. The right subspecies of albatross, but it still gives some evidence towards albatross. For something more medical, we have a data set that classifies skin lesions where the story is the same. So the food sample is predicted as melanocytic navi. And if we visualize the three top patches, each patch gives Each patch gives some actually some strong evidence towards melanocytic neva. And the other interesting thing is that the same patches give also a strong evidence towards melanoma. And as far as I understood, those two classes are pretty similar and are not always easy to discern between each other. And this is reflected in the evidence and in the actual classification itself. Vision itself. So, okay, so yes. So I'm just mentioning that this is not the only model that we have with this idea. And we have a mononet as well that we're going to submit at some point, maybe soon. And here the idea is that we also extend linearity here, but in a different way, just by instead of being linear, we're being monotonic and we maintain some kind of separability. The problem is that they're a little bit more difficult to interpret. You actually need some kind of exploratory data analysis to interpret them. So it's not so we don't gain that much by applying monotonic networks on complex data sets. So if you want to remember about me, just remember me with flats, not with monotonics. Okay, summary. So summary. So, summary for complex data and tasks, actually, we need a trade-off between accuracy and interpretability while in tabular data. This is not really true. Yeah, this is not really true. And then, like here, we presented a model to approximate that is interpretable, but the important thing to note is that the interpretability is going to be approximately linear. The function that we're actually learning is not linear. Like, I'm pointing this out because Tor. I'm pointing this out because talking to different people, there was this slight mismatch or this slight misunderstanding. But of course, the model is not perfect. No model is perfect. So among the future directions, we have that. As I said, we need to train them better. So possible ways is to just tweak. Ways is like to just tweak in the model itself a little bit. Maybe the sum makes the learning a little bit ill-conditioned, let's say, like difficult to do. So we could try to use a different kind of aggregation like a mean or max. But we will see about that. Maybe we have actually to reintroduce some more hand-engineer interactions or features. Interactions or features to give a better inductive bias for learning, or we just need to blow up the architecture of search or the learning with more GPUs and GPUs. Who knows? And the other important point actually is that that is all based on the fact that I think that linear models are interpretable. But does this mean that all people are gonna? Are gonna think it the same way. So, yes, linear interpretability seems appealing, but is it gonna be really useful? Who knows? So, actually, I'm not a strong believer of, I already discussed this with few of you, I'm not a strong believer of researching interpretability detached from completely detached from applications, so like a very important thing to do update. A very important thing to do that I would like to do is to identify which complex scenarios we can actually apply plans, and this will need like a lot of user studies. That's it from my side, and you can answer any questions. Are there please go ahead? Um yeah, thank you. That was a really nice presentation. Um I was wondering a bit about so you I was wondering a bit about so you have because you're looking at complex data, which I guess is complex because complex matters, right? So for image data, for language data, context is really important. Yes. So you split your data out into features, which you then process independently. So basically there you remove context. So aren't you then afraid that the most important learning kind of happens in the step afterwards, which is exactly the part that you cannot interpret? Yes, I see. Yes, I see. Okay, yes. So, as I understood correctly, what you mean is, okay, the model itself can learn the interactions, but like when we interpret the model, then we're not really looking at the interactions. I guess that's more or less the question. Like, we're not looking at the value of the feature in the context. You split the problem up in a part where the features cannot interact with each other, because that's the first part of your feature mapping, and then a part where Feature mapping and then hardware began, which is the non-linear function that you have, right? So, in the extreme case, your features would be individual, and you could only transform them, but they're still independent of context, and then only afterwards you can learn some features. Yes. So, wouldn't that mean that basically anything that's current is happening in the part that's not interpretable? That's definitely possibly true. So, there are two points. From an interpretability point of view, yes. Yes, there are. Yes, the interpreting of each of the features is gonna be quite shaky if the function afterwards is non-linear. Like here, you're completely right. I agree with you. The other thing is that, yes, so what I believe is that like, okay, you applied, so you first see the first order evidence. And I think in terms of interpretability, you do need some kind of iterative. Of iterative part, like interactive part, where, okay, this is some kind of evidence, but we want to see if there are some context things. And in that case, then this process becomes iterative in the sense that, like, okay, this is more important. So, what we want to do with this, we can add another feature on top of that. And so, we analyze how the linear functions look at two features at the same time, right? Features at the same time, right? So basically, you start with one and then you add other important features and see how the interactions work in that way. Does it reply your question more or less? Yeah, so it's not about the model itself, maybe, but the ability to manipulate the model. Exactly, yes. Yeah, I didn't think it this way, but like I will use that for my next. Actually, we apply this model to the December sector binding program, and first we did it. Binding program. And first we did it in a PC. Each amino acid was an independent feature. And what we got was basically that each amino acid had a weight. So the model predicted correctly, but it was not very interpretable because each amino acid was the important. But then we started going coding interactions manually, so we decided to define features as triplets of amino acids, and then we got a much more sparse prediction than what's actually meaningful as fast. So actually, the thing that you could actually enforce the interaction between features. Enforce the interaction between features, what makes sense to report, although it in a hierarchical manner or something would say. Kind of, I don't know, find out the scale of the interactions, I would say, because you can take patches that are single pixel or bigger, so you can kind of see how far the interactions with the context needs to go, which is kind of interesting. Yes, so you said that they share a common letting space. It means that those three functions are the same, actually in weights? Ah, okay, good question. So, okay, no, what in the most general implementation, what happens is that these are actually just completely different functions that just start, let's say, let's say that these are one-dimensional. Let's say that these are one-dimensional. So these functions will just be functions from R to R D. But the D is the same for all the. So the final dimension is the same for all the features. So this is the general implementation. What can happen is that you actually want to do some parameter sharing and you can do that. This could be, if you think that the feature should be processed in the same way for some reason, this could be actually the same function. Be actually the same function, they just take the feature separately. But another thing that could happen is that if we could do the same thing as with transformers, these functions are the same, but instead of just taking the patch itself, they also take a positional encoding, for example, to qualify for their location in the image. Location in the image. And then, like, uh, you could still parameter share. They have the same function, but they behave differently according to the position in the patch. Like I imagine that most of the complexity is on the later. The the size of the budget that you're using, I'd guess it'll have to feed it for every problem to the size, which is the minimum amount of data that Is it a minimum amount of data that makes it? Yes, yes, I mean, yes, of course, like, uh, yeah, but like here is all feature model engineering in a way, right? Like, so if you make it more complex, like ideally, okay, a pixel is not gonna be informative, so like you can do it with one pixel, and here it's gonna be super complicated, so that's not gonna be useful at all. So, yes, so here, like, you will have to increase the patch, and then like do parameter sharing if you have too much variability in the patch. Sharing, if you have too much variability in the patches, then like you can do some parameter sharing. Yes, you're totally right. So it's a little bit of a trade-off of how much complexity you want here and how much complexity you want here. Presentation. Thank you. I have two quick questions. Building on what was said earlier about interpreting the Interpreting the interpretable part of the model. Would it make sense to think about training this in parallel or on the residuals of some other model so we know that the linear stuff is being learned by a linear model? Where do we think that's already embedded here and we can extract it pretty well? Okay, so you mean like inserting a sort of boosting framework where, okay, like you learn first the linear model and then like the you feed. Okay, you. Okay, you first fit the linear model, then you take the residues and you on that point you just work on interactions and something like this. Yes. Yes, I mean like I think theoretically speaking you can do it in this way. So I guess like there was a different point to your question. I mean you can do it but. No, I mean that was that was the main question was like you know if we're worried about something being learned Something being learned in a way that looks integral, but it actually isn't might make sense to combine this with a model class that gives us interpretation. Yeah, yeah, yeah, definitely true. I'm just thinking, I don't know if you would, if FLAM would be like the best model to do that. I mean, theoretically speaking, what you could do is, okay, you start from a linear model and then you just add on top of the And you just add on top of the just splying models. If you're meaning, maybe on the image data set, maybe, maybe it could be useful, but I'm not sure about that. And then, second question. So, I like this motivation to do more complicated stuff. And it's amazing the experiments you have on the paper that we've read. I wonder, you have imaging experiments that probably makes imaging people want to use filters. Do you think there's a way to filter? Constitutional filters? Do you think there's a natural way to? Filters. Do you think there's a natural way to embed those? Okay, like so. What I didn't say here, those actually are not pure neuron networks, but in okay. So in the CAP data set, actually what we're doing is that the actual functions here are not trained from scratch, but are actually taken from a pre-trained resonance. So here you do have convolutional networks acting on the patches. So like, yes, I mean, like, this is just a, let's say, terrain. like this is just uh let's say theoretical it's not really theoretical but like this is just schematization but like it could be this could be LSTNs or convolution and I think that this is just neural nets they could be LF question for that so then you think you could like just take a black box model to train your features because for medical images you were not using image method oh no no that that was uh like a very Oh no, no, that that was uh like a very simple CNN train from scratch. So it could take whatever black box model to take those features. Yeah, you could do like uh what we were thinking at some point, maybe you can take the first layer of a transformer where we didn't really input the context yet, right? Like, so, and uh do something like that. So it's totally working. I had a second question that was about what you were mentioning. question was about what you were mentioning so here you only did account linear interactions between yeah I mean like in the formula it's a linear it's a yes it's a linear aggregation but again like then like because of this function here then like it's we can model frequency but then when you said like you were thinking of making your model more more complex but still interpretable More complex but still interpretable. I was wondering how were you thinking of doing that, if it was pairwise combinations of the phi is or at the level before, so having one phi that represents x1 and x2. Yes, I haven't really thought about it because so like the main main point of separability is that you shouldn't have overlapping features to still maintain interpretable separability. Separable, not interpretable separability. So I had a couple ideas, so I don't know which one makes sense. So one would be like something that Ingi was pointing at you could you could train in an iterative way in the sense that you start with the first order interactions and if you see that like the training doesn't go very well you look at the you look at what the model has learned and if you see like if if you have like features that are more or less Features that are more or less always together, then, like, you take out the corresponding networks and you put some network.