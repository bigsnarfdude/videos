To talk about some similar global profiles for fluids via physics-informed neural networks. Thanks a lot for the introduction, Eva. And thanks a lot to all the organizers for the invitation and for the possibility to be here and talk about our recent and ongoing work. Let me stress, and let me start by rephrasing this, that this is joint work with. Joint work with Tristan Backmaster, who just moved to NYU, Gonzalo Carlabora, who's currently a graduate student at MIT, Yar Lai, who also recently moved to Stanford, and Yongyi Wong, who is a postdoc at NYU. And I want to advertise and very strongly stress the authors highlighted in bold. These are the young people of the team. People of the team who may be in the near future applying for jobs, and they should definitely be hired at the very best places. So, these people are truly amazing and they have done magnificent work. So, this is just a small commercial and let me also continue with a disclaimer. Okay, so I've given this talk a few. I've given this talk a few times. So, back when I was a child, there used to be a commercial from Pirelli that said, like, power is nothing without control. Okay, so sometimes, especially now, you hear power is computation power. Okay, so right now, the thing is, computation power has become incredibly cheap and you can run very high, very High, very simulations that are using a lot of resources very cheaply. But the problem is: okay, can you really say something about what you get? Like, usually, if you ask a computer to give you a number, unless the computer catches fire, it will return a number. Okay, so this is always going to happen. And the question is: okay, what can I do with this number? And is there really something going on? So, in that sense, machine learning looks Machine learning looks kind of magical, but like a great black box, but it's certainly not well understood. And it really deserves a lot more attention. And just as with any new technology, many things are going to come in the future. But for now, for us, or at least what I feel about how or why we have been successful, is the combination of a mathematician and a computer or a And a computer or a machine learning or computational techniques to really understand and truly dissect what is going on and to tell what is good from what is not good. So I think it's not the machine by itself, but the combination with the human what makes it powerful. So having said that, today's talk I'm going to present mostly numerical results, so there will be very few details. So there will be very few theorems, but some theorems are going to come up pretty soon. And what I'm going to talk about is maybe not so relevant from the point of view of the problem itself, but I would like to highlight more the technique. So I will talk about the specific equations, but the techniques are mostly agnostic to the equation. So this is kind of what you have found out empirically over the years or over the last month. Over the years or over the last month by working on several problems. And yeah, this is maybe important. There are going to be many questions raised and I may not have all the answers. So sometimes the answer is going to be it just works. I don't know why. I wish I knew why, but I just don't know. And we are sort of trying to get a better understanding of how things work. But we definitely don't have the full picture of why certain Full picture of why certain things work versus why some other certain things don't work or don't work as well. Okay, so yeah, lots of things going on, exciting field, many, many open problems, lots of room, great future. So, okay, so yeah, the overarching goal, and I presume everybody here knows about this, is to try to say something about three. is to try to say something about 3D Euler. Okay, so incompressible 3D Euler, I'm going to take no external forces. U is the velocity, P is the pressure, and I want to say something about the singularity problem. So that would be the main goal and the long-term goal of this program or of this way of trying to do things. So we want to answer the question whether for a smooth initial data, is there a finite time? And is there a finite time singularity? Okay, so that's sort of the big question. And yeah, I'm not going to answer this question today. Because you don't have enough time. Exactly. But I wish in the future we are going to be able to say something, or at least somebody will be able to say something. So, this project has attracted a lot of attention very recently with many breakthroughs. Let me here summarize a few of them. So, El Guindi blew So, El Guindi proved that if you start with some non-smooth initial data on the whole space, then you can have blow-up at infinite time. And there is also another paper by Elkindigoloma Smoody proving stability estimates. Chen and Hao constructed a non-smooth self-similar blow-up for 3D Euler whenever the setting is with a boundary. This is in 2021. Now, numerically, there have been many candidates. I'm not going There have been many candidates. I'm not going to review all of them. Perhaps the most compelling one is about ten years ago from Bouruo and from Howe, where they provided very detailed and very compelling numerical evidence for simularity formation in the case again with the boundary. And in their simulations, they report a vorticity growth of a factor of 10 to the 8. And this was beating the previous. Was beating the previous world record in the growth by several orders of magnitude. I think it was something like 10 to the 4th. And moreover, the numerics were very careful and suggested an asymptotic self-similar scaling at the blowout time. So that's kind of what one could infer from their numerics. And this culminated with a very recent 2022-2023 proof of the existence of this scenario by Judge. This scenario by Jaji Chen, who's here, and Anton Tao. So, do you know the initial condition? Sorry. So, this is the setting. I mean, of course, this is a cartoon. And this is sort of how the singularity is forming. The configuration is axisymmetric. You're inside of a cylinder, and the top half is rotating one way, the bottom half is rotating the other way, and then this. Is rotating the other way, and then this induces secondary currents, and there is a loss of smoothness on the plane z is equal to zero, and on the boundary, r is equal to one. Okay, so this is the mechanism for the singularity to form. This is the mechanism constructed by Lu and Hao, and the picture is taken from Quant. Okay, so let me step back, and let me talk a little bit about business. So, I'm going to talk about business and the I'm going to talk about Business Nesque. And the first thing I'm going to do is to try to convince you that 2D Business is a very close caching of 3D oil on the boundary, on the problems with boundaries. So 2D Business, I'm going to set it on the upper half plane. So there is this horizontal boundary. And these are the Bussines equations. So these are 2D. U again, incompressible velocity. Theta is a scalar, P is a pressure, but it's going to disappear pretty soon. To disappear pretty soon. So, the goal now is to say something about globally self-similar solutions for 2D possibilities. So, we're going to see what happens if we construct such an answer. And let's see what we can say about these equations. So, we're going to go to something similar. We scale y. So, y is the scale version of x by this factor, 1 minus t to the power 1 plus lambda. Lambda is 3, and the singularity. And the singularity time is said to be at time equal to 1. So if we do that, then this is what we get, then time is dropped, and then we have some kind of steady equation in the similar frame. So now I claim that if we can prove that there is a nice smooth solution of these equations, then one can prove blow up for Bussy Nesque, and then one can prove Bloaf for 3D Violer in the cylindrical boundary. In the cylindrical boundary. Okay, so I'm going to, as I said, I'm going to drop the pressure. I'm going to work in vorticity formulation. So then I'm going to take a curl of the equations and the pressure will vanish. I will set omega to be the curl of u. And for practical purposes, I'm going to work not with theta, but with derivatives of theta. So I'm going to work with phi and c, which are the horizontal and the vertical derivatives of theta. Now, this is the equation. Now, this is the equation that I get. If you haven't seen it before, it doesn't matter. I mean, forget about the equations because all of this is like the equation is not going to matter, really. I'm going to impose some symmetry conditions. So all uneven symmetry, and I'm going to put some normalization condition here, because there is a one-parameter family of solutions, and I want to move out this one-parameter family by imposing some normalization condition. Normalization condition. And I'm going to assume that graph U, phi and C vanish at infinity. So I'm allowing U to grow at infinity, which is painful for the numerics, as long as it is sublinear. So I can allow U to grow a little at infinity, and then, okay, computers don't like infinities, and computers don't like when things grow. So this is a technical problem, and I'll show how to address it. And I'll show how to address it. So, one benefit of this formulation is that we don't need to go through the pressure, we don't need to recover the pressure, we don't need to invert a Laplacian. And especially in these singularity problems, the phenomenon or the heart of the matter is moving across many, many, many scales, which leads to a very difficult from the technological but also from the mathematical point of view how to deal with this inversion of the Laplace. Inversion of the Laplacian. Okay, so now everything is local, which is great, and there is no inversion of the Laplacian. And the price that we pay is: okay, we have more equations, more variables, and more agnods. So let's see. Okay, so yeah, as I said before, I'm going to quickly try to convince you that blow-off for Business implies blow-off for Euler. implies blow up for Euler with this identification. So now everything is axisymmetric and then if we are close to the boundary of the cylinder which we take R to be one and we map Y to these variables and we take omega identified with the previous omega and phi and C which correspond to these variables in the 3D OLR case and we sort of map it back. Then at the end of the day when we get Then, at the end of the day, what we get is really the same equation as before, other than this term E1 and this term E2. Now, if one does the calculation, I will not show it here, but if one does the calculation, one can see that these terms can be made exponentially small. Assuming a mild condition on lambda, then these are exponentially small perturbations of signal. Of signal, right? So essentially, you can look at Euler as being an exponentially small force version of BussyNet. The previous slide, you had C and T are just partial derivatives of the same function. You don't involve the condition that the cross derivatives are the same. I will get into that. Yes. Yes. So the short answer is yes. I didn't show it here, but we. I didn't show it here, but when I show what the machine learning is actually solving, that's one of the equations that come into play. Okay, so yeah, now machine learning is sort of a keyword and everybody has their own opinion and their own take and their own view on this thing. So let me quickly explain in mine. So you've probably seen this diagram a million times. And essentially what And essentially, what this is saying is that for every layer, I'm going to take the input coming from the previous layer. What I'm going to do is to make a linear combination, well an affine combination of the previous variables and then pass it to the next layer via some activation function, which for us is a hyperbolic tangent. And then we're going to do that for all. Do that for all our variables. You want, you two, phi, c an omega. So we're going to have like this kind of little black box here that is going to tell us a guess or a candidate for, well, for each black box, it's going to tell us one of these. And maybe for the experts, the topology is pretty small. We're doing six layers and 30 neurons per layer, which is, I mean, by now the standards, fairly small. Nowadays, standards are fairly small. Okay, so this is kind of how it works. And now, in words, what they just said, there are some weights, which are the variables of my system, which are the things over which I'm going to minimize. I take a weighted sum of the inputs, and then I add a constant, and I activate the result and pass it to the next layer, and so on. So, essentially, what I'm doing is I'm constructing non-linear. Nonlinear compositions of the activation function. So I'm doing sigma of sigma of sigma of sigma of lots of linear combinations and so on. And then these results, like if you want to think of this in terms of a basis, this is a highly non-linear basis as opposed to modern traditional methods, let's say like a Fourier basis or your favorite orthogonal polynomial family or finite elements or whatever you want. Okay, so the way I see why this. The way I see why this is, I mean, everything in this world is a universal estimator, so if you take enough members of this family, you can approximate whatever you want. And the question here is how many do you really have to take? And this is where machine learning, or where I feel machine learning beats the other methods, because it needs really a lot less, a lot less elements, a lot less computing power, let's say for the same accuracy and for the same computer. And for the same computing time/slash waiting time. So I think this is one of the benefits. And it can lead to, because the landscape is so non-linear, it can really get, for the same amount of time and for the same amount of computation, it can really get much closer to places where a priori using other bases or other methods, it's difficult to get. Of course, a posteriori, once you know that there is a solution, Once you know that there is a solution, you can hit it with any way you want. And if you do things more or less right, you'll obviously get there. But keep in mind that for these problems, it is not evident at all that there is an actual solution. I mean, the story of my life is, okay, I used to work on these things, tried something, didn't work, and then you start wondering, like, well, is there something there? Am I incompetent? What is going on? So you have a little bit of faith in yourself. So you have a little bit of faith in yourself, you try a little bit more, but then eventually you sort of give up. Okay, so I think one of the. Yeah, I mean, it has happened so many times in my life, I couldn't tell. So this is good because it really adds up a lot of value to discovering things. So, especially things that it's not clear that they exist or not. So, yeah, just to So, yeah, just to go back to Boris's question a little bit, what we're actually going to minimize are two things: losses coming from the equation and losses coming from the boundary conditions. So, with certain weights and certain number of points. So, this is essentially, you can also look at this as a minimization problem of some combination of these losses over this weight. Over these weights and biases that appear throughout the neural network. Okay, and just to give a feeling of how these things look like, well, these are the three equations, the divergence-free condition, the compatibility condition between phi and c, and then the compatibility condition between omega and u. Remember that omega is the curve of it. Okay, so this is plus, of course, the boundary conditions which I didn't write here. Okay, so this is what we feed into the neural network, and this is. Into the neural network, and this is what it is reacting to. What it is going to try to find the zero of the sum of the squares or the weighted sum of the squares of these F's plus the boundary conditions and F. Okay, so this is sort of the end of the story. Let me go back to the beginning of the story. So, when we started, I mean, of course, we didn't start with a super big complicated problem. We started with the easiest problem we can think of, which is burgers. Like, burgers is great, we can solve it. Burgers is great. We can solve it. Everything is explicit. And we know that there is a solution. So that's good. Let's go to this is Burgers. Plain, simple, local, 1D, as easy as it can get. And if we substitute for the subsimilar answers, we end up with this problem here. And okay, we can solve it if instead of realizing that if instead of solving u of y, we solve y of u, then we can solve. Then we can solve one implicitly in this way. Okay, and C is a free parameter. So what we can observe here is that for any scaling, for any lambda, there is a solution given by this, but only a handful of ones, and the ones coming from this discrete set are smooth. Okay, so the mental picture is there is always a self-similar solution for every exponent, for every scaling exponent. However, For every scaling exponent. However, only a few distinguished exponents or distinguished scalings are going to lead to a smooth solution. Okay, this is, of course, Burger's, quite easy. Let's see how the machine learning performed. No surprise, the first one, which is extremely easy to find, it found it ten to the minus seven error virtually indistinguishable at the level of U and derivatives of U. Now this is quite easy. This is quite easy. This solution is stable. And the other columns are not so easy. So the second smooth solution, which is one quartel, is unstable. It has an unstable manifold of dimension one. So then it's not so easy. And then, of course, if you understand the problem well, you can make it work, because then you can factor in that information that you have about the unstable manifold and so on. But here we didn't really do anything. But here we didn't really do anything beyond, of course, constraining the lambda, because if we let lambda completely free, it will go to the most stable configuration as expected. So the only thing that we did was to say, well, penalize if your lambda gets to the most stable one. Beyond that, don't do anything. And then it went to the next stable one, which is one quarter. Okay, so that was. So it's somehow imposed a constraint for lambda in the. That's right. Yeah, in the laws. The laws we are penalizing that. The loss we are penalizing that lambda goes above a certain threshold. So we need to know a little bit about information. Yeah, yeah, of course. We ran this one first and then we know that one. If that works, we're done. Yeah, this has been always the case in everything we've seen. So the second line differs quite a bit from 0 to 0.5. Well, 10 to the minus 7? These are all nice. 10297. These are all nines. 299? No, no, 249999996. Okay, yeah, a bit, yeah. But I can take that. Okay, so we were able to find the first unstable solution, and we were also able to find a non-smooth solution. So now there's no constraint on lambda, lambda is fixed, and then the And then the pin was able to find a very good approximation of this non-school solution that has, at the level of the third derivative, a cask here at the origin. Of course, there are some errors, obviously, because we are trying to interpolate or or represent a non-smooth function using smooth functions, but but it definitely does a very, very good job with really low error. With really low errors. Okay? So the non-smooth one virtual stable? Well, I mean, it depends. It's going to be a little bit rain-tested. Yeah. You also have issues depending on how you define things, because then if they are very non-smooth, then you have trouble taking derivatives, and then, okay, then you need to watch out with what is the definition of unstable or stable. Okay, so that's good. That's good, but then this is the simplest as it can get. 1D, local, explicit. Let's try something harder. 1D, non-local, and very few solutions found. Okay, so we started considering the Gregorio or generalized de Gregorio, which has a free parameter A, A is a number, this is the P D, and the non-local relation between U and omega is via Hilbert transform, or here lambda is. Hilbert transform or Hil lambda is the square root of the of the minus of plasma. Okay, so let's again put the self-similar answers, substitute back, and then this is the kind of non-local PVE that we get. Again, we have one degree of freedom, so we're going to normalize in this way. And let me tell you a little bit of what was known before we started. So if A is equal to zero, this is the classic Konstantin Laxmaida. So Constantine Laxmaida, so they proved that there was a self-similar blow-up solutions. Then for A is equal to minus one, this is known as the CCF model, and I will talk a little bit more about this particular case. There is blow-up for negative A. For small A, positive A, El Gindi and Jung constructed self-similar blow-up, perturbing from the case is equal to zero. And Chen, Hao, and Huang solved the case. Hao and Huang solved the case A is equal to 1 using a computer-assisted proof. And very recently, there was also another preprint by Huang, Ching, Wang, and Wei that prove the existence of self-similar solutions for A smaller or equal than 1. So that's pretty good. There are numerical results by Luzhnikov, Silantev, and Siegel, where they gave, they constructed numerically self-similar solutions for the case A between For the case A between minus 1 and 1, and even beyond, and they gave out a lot of data, a huge table, lots of things to compare with, that we use for calibrating our method in the sense that they can get much at one. No, no, no, they can even go beyond one. Yeah, like five, I think, or even ten. I mean, I don't remember the number. Don't remember the number, but they can do very big. So they did the computation, not the yeah, exactly. But they can also go to very negative as well. So this is the comparison for the exponent between their work and our work. So this is A versus lambda, and well, I mean, it's And well, I mean, it's a pretty good match. So the blue circles are our profile, and the black line is the reported self-similar exponent, and it agrees pretty well. And we also had, well, I mean, agreement with the explicit solutions for A is equal to zero, and there were also other checks concerning the place where the exponent gets equal to zero. Okay, so, or I mean, in our case, it's Or, I mean, in our case, it's minus one, the way we wrote it, it's one. So, so that's okay, that's good because now we can deal with non-local equations in one. Okay, now let me talk a little bit about the case A is equal to minus one, which is known as the CCF model. So this equation has been studied by many authors about fifteen to twenty years ago. 15 to 20 years ago, and they were able to prove. So now I'm doing the same thing, but I'm putting some viscosity, some dissipation here in the form of this fractional Laplacian to the power alpha. So the combination of these four papers proved that there is blowout if you put a little bit of dissipation. So if you put a little bit of dissipation, then the this is not strong enough to to to prevent the blow up, which is driven by the nonlinearity. Low-up, which is driven by the non-linearity. And if you put a lot of dissipation or enough dissipation, alpha greater or equal than one, then the dissipation term dominates and then there is global existence. So the hope is that if we go to self-similar coordinates, this is going to be exponentially small. I mean, this is not the hope. One can prove it if this relation holds true. So we want to find some suitable. To find some suitable lambda for which this constraint gives us something non-trivial. So we can say something in the open range as long as lambda is smaller than one. Problem, lambda is bigger than one. So that's too bad. This is like the first thing that we found. But then we found another one. So then we found the first unstable solution to C. solution to CCF which has a lambda equal to 0.6 and then the okay 0.63 dot dot dot dot dot but if I simplify then that leads to a proof of finite time singularities for alpha smaller than three fifths so remember that the open range was the way I wrote it between one half and one so we can improve on So, we can improve on the range, and moreover, these solutions are unstable. Okay, so we can deal with the case where there is a non-trivial unstable manifold. We are writing out the proof and it should come out soon. And yeah, this is just the beginning, right? I mean, this is if you think mentally about what was happening for burgers, there was the ground state, which is like the blue one, then the first Which is like the blue one, then the first excited state, which is the red one, and we expect the second, third, and so on excited states, leading to smaller and smaller lambdas, but of course these are harder and harder to compute. Okay? But this is like a new thing, and it leads to a proof in this improved range of exponents. So this is the first unstable. This is the first unstable, yes. Yes. Smooth, first unstable, dimension of the unstable manifold one. And whenever I say one, I always mod out the trivial solutions coming from the invariance of the equation. Okay, so this is what we found for BussyNesk. Solutions of order 1, of order 10, errors of order 10 to the minus 4, 10 to the minus 5, quite convincing. We put a little bit more, I mean, we put more columns. A little bit more, I mean, we put more collocation points towards the origin because this is where the non-smoothness comes into place. We sample these collocation points uniformly, and then in a big box, which is also stretched by exponential coordinates, we put our collocation points sampled uniformly. We do stochastic gradient descent for a first stage. You can see the error here going up a little bit. This is to prevent from getting stuck at local minimum. We're getting stuck at local minima, and then once we hit what we feel is the global minimum, then we do a quasi-Newton method, which is second order, and it converges a lot quicker. We have done many tests regarding the numerical stability, and also to be able to be sure that we're actually getting an honest solution. And then, like, just let me just summarize sort of. Just summarize sort of where we are heading and what's the plan. And as I said, this is agnostic to the equation itself. So, this is sort of a general program, a general strategy towards proving blow-up for incompressible fluid equations, which essentially is split into these five steps. First, find any way you want, either using machine learning or your favorite method, which doesn't need to be anything as long as it is a good approximation. Anything as long as it is a good approximation of a self-similar solution, then split your linearized operator into some finite that could be very big apart plus a tail that you're going to control by hand, prove linear stability perhaps in a weak sense, so you can tolerate an unstable manifold of finite dimension. And we've already done that in the case of compressible Euler and compressible Navier stocks. Compressible Navier's talks, use interval arithmetic, embed everything into a computer-assisted proof, bound everything rigorously, and then upgrade the linear results to non-linear. And keep in mind, I never said what was the equation here. So as long as you, of course, contingent on finding a good approximation similar solution and contingent on the linearized operator, having at most a finite-dimensional and stable manifold, then everything should in principle. Everything should, in principle, go through. So, this is a universal strategy and works in many cases. Okay, so this is the future. I hope I can tell you more somewhat soon. Thank you. So, questions. So, I was wondering how sensitive is this to the network size? Do you think you need much larger networks and more complicated networks? Larger networks and more complicated non-linearities to find, like, for example, the next unstable solutions? It wasn't very sensitive. I mean, when we tried bigger sizes, it didn't change much for the equations that we've tried. And also different non-singularities. Non-singularities instead of. The activation function? Yeah, I mean, okay, if you put something like a red rule or something like that, which is non-smooth by design, that's a bad idea, right? That's a bad idea, right? You don't want to approximate a smooth function using non-smooth functions. But beyond that, we tried other sorts of activation functions. It didn't really make much difference. So yeah, I mean, now this is starting to fall in the category of these questions. Like, it didn't work for us. We tried other things, other ways of normalizing. It didn't make so much of a difference. So you mean six layers of So you're in uh the last slide, imagine that you can decompose into a finite matrix. So I guess that means that operators in some such components? No, no, we can also I mean this is an overly simplified version of yeah I i i you know what I'm talking about very well. Yeah, that maybe I need to use something about Excel. Yeah, okay. But in general, you can do it if you understand the linearization. You can do it if you understand the linearized operator in a reasonable way. What I mean is that you can do this for many PDEs. It doesn't need to be compact. As long as, I mean, as long as you have to understand the propagator of the linear operator, right? And then that's what you really want to understand. You don't have any instructor indicators in which it didn't work. Well, I mean, this is abstract. Well, I mean this is abstract, no, in the sense that what we have tried so far in the type of equations that we have tried so far, it it did well. I'm confused by the words precise approximate. Replace precise by sufficiently good. Another question is: With regards to the title, where is the physical information of the network? The architecture is just straightforward. It's from the laws. It's from the loss. You're using the equations as the loss. So that's the physical information, because the equations are coming from. I didn't come up with the name. So that's where the physics is coming from, via using the equations as the laws. And the equations come from a physical problem or a physical consideration. It is not really lunchy theorem in machine learning. So I think it's No, no. So I think uh it's better to design the neural network by using the HPD password. We tried to do that. It didn't make a big of an advanced I mean make a big of a difference. That's a good strategy, that's right. And you can certainly take advantage. Sorry? What do you mean we design the network for PD? The network for VD? Did I say VD sound? No, I think he means in terms of the laws. Like you can adjust the losses differently depending on the equation, for example. No, but not the architecture. Or maybe even the architecture. I mean, yeah, that you can do, like, equations behave a little differently depending, like, some equations may, for example, be at a different level of derivatives. Of derivatives. So you can sort of tune your things based on that. Yeah, we did a little bit of that. I didn't explain it here. There is a little bit of that sort of fine-tuning the other ways in the logs. Another question? Do you think there's any chance that you can use similar network-based strategies to do the opposite, to prove long-term existence of solutions? No, that's very different. I mean, here, for better or for worse, I. Here, for better or for worse, I only need one instance of what I'm looking for. Well, maybe there is nothing there. This is the risk I'm taking. But I only need one. Proving that something holds for all data is much more complicated in this way. Of course, you can gain intuition and sort of maybe try to figure out relevant quantities to monitor, but make a general statement about all initial conditions and all solutions. And all solutions is probably going to be very difficult using this technique. I have a question there. So, before neural networks, you tried, I guess, you tried other methods to approximate this kind of self-similar script. What is the main difference between previous methods packaged by neural networks? Is it just non-linear IT approximation or stochastic reduction? But why? I I I don't know exactly. But what happened with your package uh tentative? They either they didn't converge or their residuals were not very good or I mean all sorts of nasty things. Even forget about business nets, even for like the 1D models. They they just would get stuck in local minima many times. I tried every virtually every single algorithm. Virtually every single algorithm that you can think of and fail. I mean, that doesn't mean anything about the algorithms themselves, but more about my inability to do things right. But I was trying for many years to do, I mean, you name it, stochastic gradient descent, simulated annealing, all your favorite bases, even incorporating things from the equation, like doing hard analysis, incorporating asymptotics into the equation. To the equation, none of these seem to work. Why? I don't know. I mean, I wish I knew back then, and I wish I knew now. But the main difference, because the size of the material net is kind of small. Yes. And the main difference is don't have it. Presumably, yes. This is empirical, I don't have an answer to that. I mean, we are the first ones that are also struck by that, you know, in a happy way. In a happy way, but we don't understand really why it outperforms the other methods. At least for us. Good after this exciting talk and discussion. I propose that we take five minutes' break. I think I've already stopped. Oh, yeah, yeah. I think it hurts all the time. Well, it was still showing stop. So then that means I was still recording. Now it's just showing stop. So then, you know, if it was showing stop, it was still recording. 