So, this presentation provides an overview of my research on large-scale inference on heterostodastic units. So, the heterogeneity is a very common characteristic observed in various applications involving multiple study units. So, the heterogeneity can be attributed to Attributed to quite a few factors. For example, you can have like nuisance parameters, you can have site information, and you can have some sensitive attributes like group characteristics. So for instance, when you conduct the tests or estimates on the normal means, then the variances of the study units are the nuisance parameters. And then the heterology. And then the heterogeneity will arise if these variances are different. And this phenomena is referred to as the heteroscedasticity. So that's one of the central topics that I will be focusing on today. And so in this presentation, so my plan is to just give you an overview about the different things. So it's centered around addressing the challenge of heterogeneity in large-scale inference. Larger scale inference. I plan to talk about three or four things. So, first, I will discuss the component estimation situation. We will see that the heterogeneity will aggregate the selection bias in compound estimation problems. And then we will talk about how the heterogeneity will result in information loss. In information loss in standard practice in multiple testing. And next, I will talk a little bit about how heteroscedasticity will create conflicts between statistical significance and the practical relevance in ranking and selection problems. And lastly, if time permits, I will examine how the heterogeneity will contribute to group unfairness and, for example, in classification problems. For example, in classification problems involving sensitive attributes, it will cause some issues. Okay, so and we have developed a series of works to handle the heterogeneity issue, and I would like to present a partial list of related works and acknowledge also my co-authors, all of whom were at USC a few years ago. So, notably, the three students mentioned Students mentioned in the list, they were co-supervised by Professor Garcia Steems and myself. And these three papers share an extremely coherent theme. And in the upcoming talks, in the upcoming slides, I will elaborate the crucial statistical issues and the motivations underpinning this line of research. And also in this presentation, I will Also in this presentation, I will primarily emphasize the high-level ideas because there are a lot of stuff going on. And I want to ensure that everyone has a comprehensive understanding of related issues. And I will skip all the technical details. So if you are interested in any particular issue, you can take a look at our paper, and all these are on the archive. So now I will start with illustration. So let us consider the academic yearly progress study. It's about the California high schools. And so the goal is to identify the schools that exhibit significant gaps in the performances of the students from different social economic backgrounds with a particular focus on the so-called social economic. called social economic status. So basically you have two groups of students, those from socially economically advantaged backgrounds and those from socially economically disadvantaged backgrounds. And then for each school, we can compute the estimated difference. It's called the XI. Okay. And then for also each school, you can compute the standard error. Can compute the standard error, which is given here. And then the very important feature of this study is that there is a substantial variation in the sizes of the schools. So we have some schools only have a few hundred students, but others have like thousands of students. And consequently, because these sample size will play a very important role in the standard. In the standard deviations, and the study units will exhibit highly heteroscedastic behavior. And then why it would be a problem. Let us now just consider three selection strategies. So the first one is based on just the statistical significance. We will utilize the p-value. All right. And the second one is based on The second one is based on like observe the gap just in the passing rates utilizing just the raw observations. And the third one is based on the posterior mean, which is computed based on the QD's formula. I will discuss that shortly. And so the result of our exploratory analysis is presented in this figure. So in panel A, we can see the distribution of the standard errors. Of the standard errors. And just keep this figure in mind. And now let us look at B, C, and D. And they display the distribution of the top selected schools based on the p-value, the posterior mean, and also the raw observation. So these three methods I mentioned previously. And you can see that we are confronted with a dilemma. Why? Because on the one hand, On the one hand, selecting schools solely based on p-values or posterior means tended to result in an overrepresentation of the schools with low standard errors, which are typically larger in size and have a greater number of students. So, for these schools. But if you look at the raw observations, if you choose the schools only based on the raw observations attributed to Down the raw observations attributed to an over-representation of the schools with high standard errors. And then there is no way out, right? So we encounter a conflict between the consideration of statistical significance versus effect size. So basically, if you prioritize your selection based on significance, then you will be selecting the large schools. And if you And if you emphasize the effect size, then your attention would be directed towards the selection of smaller schools with fewer students. And in both situations, the aforementioned three strategies are not desirable. So now, so hydroscedacity is a very bad thing, right? So it has fundamental impacts on many different analysis. Different analysis. So now we will start with the larger scale estimation problems. So suppose you want to estimate like thousands of means together. This is the normal mean problem. And so suppose we focus on, so say, the top 20 selected units. If you do not do any corrections, and you are going to observe a systematic bias in the naive estimate, in the ideal scenario, In the ideal scenario, for example, if you look at the difference between the true value and the estimated value, you want to have an unbiased estimator. And then you will have like, so if you repeat the experiment for maybe 200 times, then you will have a lot of differences. And if you have an unbiased estimator, and you should have a histogram with a center around zero. However, However, if you look at this plot, the uncorrected difference, so which is shown here, is the white histogram on the right-hand side, and it displays obvious bias. And then the good news is that if you can use some advanced tools like Tweedy's formula, and then you can mitigate this selection bias. And this is the Tweety's. And this is the Tweety's formula. And you can see that after you apply this formula, you will have a new estimate. And the new estimate has a corrected difference. And these differences are now centered around zero. And this illustrates that the treatise formula is very effective in removing the selection bias. So that's very good news, right? However, now we wanted to move to the hydroscope. We wanted to move to the heterostatastic case. So, if all the variances are different, then the conventional PD's formula will become problematic. If you look at the top row on the figure here, so on the left panel, you have the naive estimate, and you can see it's highly biased. And the 3D estimate can correct the bias while the 3D's formula is shown in the light-colored histogram. So, the naive estimator is. The naive estimator is the dark-colored histogram. And so the 2D formula is fine. However, if you compare this estimator with our newly proposed, the so-called NEST estimator, I will give more details about that in the next slide. And you can see our corresponding estimate has a much smaller variance. So both estimators are unbiased. However, the TD's formula However, the TDS formula is highly inefficient because it has a very large variance. The MSE is pretty big, right? So the TDS formula is bad because it ignores. So the entire formula has done, so it has assumed that all the standardized statistics have the same distribution, but actually their distributions are different. But if you can take into account the structural information in the analysis, The structural information in the analysis, you will end up with this nest estimator, and it is still unbiased, but it has much smaller variance, so it has a very high efficiency. And this is another scenario we have considered. So now you can see that the 2D formula now is biased under the hydroscedasticity. But if you use our nest estimator, then the method will still be valid. So now. So now I hopefully have convinced you how important the hydroscedacity is in large-scale estimation problems. And this is how we address the problem. I will just use one slide to describe basically what we have done. So this estimator is called the Nest estimator. It stands for non-parametric empirical base structural TD. So the Nest estimator involves two Estimator involves two primary steps. So, in the first step, we have an oracle estimator, which is derived based on the distribution of the theory of it's a two-parameter exponential family, while the formula is huge, so I did not provide it here. So, the initial estimate serves as a benchmark for subsequent analysis. And in our second step, we try to incorporate the use of KST. incorporates the use of QSD, the so-called kernelized stance discrepancy, to construct a convex program. This convex program is very easy to analyze and you can derive very strong theoretical properties of the estimator. And then the goal of this convex program is to create a data-driven estimator that can achieve similar performance to the Oracle estimator. And so basically, this is a very, very interesting result. So you can Very interesting result. So, you can estimate that score function as the solution to this complex problem. And it is very fast and it improves the efficiency and it has very strong theoretical properties. So, this is sort of how you can deal with the hydroscodicity in the component estimation problem. All right. So, now I'll move to very quickly move to the Move to very quickly move to the second part about the multiple testing problem. And this is a very striking story because you know that in conventional practice, we have the so-called data reduction process or data processing. It begins with the full data set. Well, this is not actually the full data set, but I will just call this pair like the full data set. And it includes both the variable x and the corresponding. Variable x and the corresponding standard deviation sigma. And then this data point will be standardized to get the z value, and this z value will be transformed to the two-sided p-value. However, our main point is to argue that you have a very important fundamental principle in statistics called sufficiency principle, which guides all the process of data reduction. And however, in multiple testing problems, this Multiple testing problems, this sufficience principle is violated. And we argue that it should be carefully scrutinized in the conventional practice. So now, someone may argue, all right, so in multiple testing, the standardization seems to be inevitable. It's very important and it has been widely adopted. It has very good reasons to have. The reasons to have this standardization step because you know you have different variances. If you want to compute the significance, you have to standardize. It can take into account the variability in the sampling. And also, if you don't do standardization, how can you compare the test statistics? How can you compare different tests? But the point is that the standard practice leads to the efficiency loss. Leads to the efficiency loss. And there can be significant loss in the structural information from basing hypothesis test on standardized statistics rather than the full data. And this might be a little surprising because people have been doing the z-value, p-value approach all the time. And then to illustrate this crucial point, we are going to consider three possible approaches. And the first one Approaches and the first one is the p-value-based approach, and we are going to reject all p-values below a given threshold. And the second approach is the z-value approach, and so basically you reject or suitably small, a suitably small posterior probability. And the last one is the so-called full data approach, and we still consider this posterior probability, but now it's conditional on both xi and sigma i. And then you are going to end up with three. Are going to end up with three thresholding rules. The first one is the delta P, and we have delta Z, and we have delta 4. And then we also compute the so-called average power, which is the expected percentages of the true discoveries. And you can see if you have this p-value rule, your power is only 5%. And if you swap to the z-value approach, your power will improve a little bit. Power will improve a little bit, and the full data approach has the biggest power. So it's very strange, okay, why standardization would lead to information loss. And we have some intuitive explanations. And actually, there are two stories going on. The first story is in my very first paper in my career, is to illustrate that if you illustrates that if you use the p value then it is not the best building block for multiple testing instead you should use a z value because the z value can capture the shape of the alternative if you look at this particular example if you use the p value based approach then it is always a symmetric rejection region but if you use the z value based approach then you can have some asymmetric regions uh which will increase the power of the Increase the power of the FDR analysis. Well, the second story seems to be more surprising because here we have shown the rejection regions of three methods. They have exhibited notable differences. And the most important pattern is that the rejection regions of both the p-value, which is the black line, or the z-value approach. or the z-value approach, they remain unaffected by the standard deviation. However, if you look at the optimal rule, which is given by the green curve, it demonstrates that the rejection region adapts to the sigma value. And the observation is highly important because it shows why the standardization process would Process would lead to information loss. And let me just summarize the important information here. So the loss of information encountered in the conventional data processing process is very hidden. So actually, you have two steps in the first step and in the second step. And in the second step, and in both steps, you have lost the information. And so the first work shows that if you can leverage the overall asymmetry of the alternative distribution, and then you can improve the power. And the second work, which is much, much later, it acknowledged that the heterogeneity among the individual alternatives is also a positive. Possible to contribute to the loss of information. And if you can appropriately exploit that structural information, you can improve the power of FDR analysis. And so in our work, we have proposed this HAT procedure. It's a heteroscedacity-adjusted ranking thresholding procedure for controlling the FDR. Okay, so I will also skip the details and next I will move quickly to the I will move quickly to the last one, which is like our original question. So, in the context of ranking a selection problem, and one crucial question is, so how can we effectively allocate limited resources among the numerous potential candidates? And this issue becomes particularly challenging in large-scale ranking selection due to the dilemma we highlighted. To the dilemma we highlighted just a few slides ago. So, the conventional approach to the ranking relies on the significance index, such as p-value or z-value. However, this approach tends to overrepresent the larger schools, emphasizing significance over effective size. But on the other hand, ranking based on only the effect size tends to prioritize the small schools, neglecting the importance of the Neglecting the importance of the significance. So, how to address this dilemma? So, in our proposal, we try to incorporate a modified power notion to prioritize the selection of important effects. But also, we have employed a normal ranking index to assess the relative importance, and we also take care of the significance issue. And so, this is what we have done. What we have done. So, we first have defined this multiple testing problem. So, instead of selecting the non-zero effects, we will define this indifference region. And based on this indifference region, you will have a selection rule, and then you can define the corresponding FDR concepts. You can define the corresponding power concepts. So, our concepts for power is the so-called expected number of true power. The expected number of true positives. And you just count, okay, how many true positives that you have found in your procedure. And we have done a very important modification of this equation one. So actually, there are two important modifications from one to two. The first is that we have replaced this indicator by this actual difference. And the second is that the mu i is now replaced. Î¼i is now replaced by xi. And there are various considerations for doing this. You can refer to our paper for more details why we want to make such a modification. The key idea is that, so instead of just looking at this indicator, we will take into account the effect size in our decision-making process. And then the optimal decision rule is quite complicated to derive. We actually spend quite To derive, we actually spent quite a few months to figure this out. So, in order to figure out this optimal rule, you must first divide all the hypotheses into several groups. And also, there is no optimal ranking. You only have optimal ranking within each group, and then you need to do like an iterative process to figure out the optimal path. But, anyways, I will also skip the details. Will also skip the details. Well, this ranking statistic is quite illuminating in the sense that, well, you see, in the past, you either only have XI or you have this conditional local FDR statistic. The conditional local FDR is an extension of the local FDR idea by AFRA. So you can view it as something like a p-value. But this ranking statistic obviously has taken into account both the effect size and the significance. And the significance. And so, this is shown to be the optimal statistic in this situation. And then you can figure out some optimal solution. And also, Professor Gang is very creative and he figured out a very efficient data-driven algorithm to solve this problem and yield asymptotically optimal solution. So, this is the third related projects. So, I just want to highlight. Projects. So, I just want to highlight there is a byproduct of analysis. So, this byproduct is the so-called R-value. We actually can have two R values. The first one fixes the reference level mu zero, and then the r values are generated based on the selection rule by varying the confidence level alpha. And the second one will fix the confidence level alpha, and then you can generate the r-values based on the reference. Based on the reference level mu0. And then the ROI reflects the relative order in which the units are selected in the whole process. And it provides a very practical criteria for assessing the relative importance. So if you are selected earlier, then you are more important, right? And then, for example, in the first definition, the R value corresponds to the minimum FDR level at which the study unit can be selected. Study unit can be selected. Very similar to the Q-value idea, right? So, this ranking metric addresses the inconsistency that some people may want to use alpha one, some may want to use alpha two, but now I will give you a more unified ranking for all these different users. And so, finally, because I mentioned that the dilemma for the California high school data, and now actually, if you look at our selection, At our selection, which are depicted by the red points here. If you look at the p-value-based methods, you still will figure all the local data-based methods, and you will see that they are still problematic because they overemphasized like the smaller variances schools. But if you use our R value, then you can sort of have the selections that will take into account. That will take into account both the significance and the effect size. So, these red points are more sensible selections. At least we feel that it is more useful in practice. Yeah, I want to save some time for your lunch, so I will skip the last part and I will go to the well, just because this is the work with Professor Xin Tong, who is also my colleague at USC. But just sorry, Xin, I don't have time to. Just sorry, seeing I don't have time to talk about our work, but I really want to just mention the last point: heterogeneity is a very important issue. It has fundamental impacts on shrinkage estimation, multiple testing, classification, and ranking selection. And it corresponds to some basic issues and principles in statistical analysis. Thank you very much for your attention. 