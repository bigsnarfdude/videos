Waserstein distance. So it's a pretty different topic from the rest of the talks on the series, I believe. So it'll be mainly about this thing called manifold learning, which I'll explain. So this talk will be about fairly mathematical and abstract points of view of data science. So I wouldn't be dealing with actual data here, but I'll be describing. But I'll be describing a mathematical theorem, two mathematical theorems that are proved in the context of data lying near a manifold. So a common situation in data science is that data is given as points on a Euclidean space. But the typical setup, obviously, is again that the data sets will not span the entire space and rather be of lower dimensional nature. Lower dimensional nature. And if you assume that these lower dimensional points, if these points have good local parameterization where there's no sharp turns and funny apex or that kind of thing, if everything is smooth and nicely distributed everywhere, then that's essentially saying that your data lies on a manifold, where a manifold is like a high-dimensional surface. So assuming that such a manifold exists is called Assuming that such a manifold exists is called a manifold hypothesis, and inferring properties of such manifold could be termed, well, it is the field of manifold learning. So here's a synthetic example where everything like checks out nicely. There are other examples of how one thinks about data lying on manifold, but this one's a particularly easy-to-under case. So suppose you have So suppose you have just a face frozen in time, and you just take grayscale photographs of that face from various angles, just going around at the same distance. Then your photographs are purely parametrized by where you are filming the face from, but you can encode each face into a very high-dimensional space by. A very high-dimensional space by dividing up the photograph into many pixels and then encoding each pixel value into entry in a vector. So, in this case, if you have 64 by 64 image, then you have 64 square dimensional space of images. But then, as I said earlier, you just the only parameters that matter here are just where you film your face from. Your face from. So here's this is an example of an algorithm called local tangent space alignment, not mine. Someone else did it about 15 years ago or so. But you can see that after you perform like a dimensionality reduction algorithm, the true nature of these data set is just two-dimensional. So what I'll be talking about today is using principal component analysis to infer two important properties. Infer two important properties of such a data manifold. So principal component analysis is bread and butter. It's in some sense an optimal linear regression. And local PCA would be optimal local linear regression. And if you look at these schematics that I show here, this is a typical picture you see for PCA where you see these two principal directions and their eigenvalues as indicated by lengths. Indicated by lengths. But if you do local PCA, where you isolate like tiny neighborhoods on this little curvy data set, synthetic curvy data set, then if you do the PCA locally, then you can see that there's this long direction that's kind of essentially where the tangent line is at this point, and this other direction, which is relatively insignificant. So the idea is this. The idea is this. The goal that I want to convey here today is to estimate tangent space and dimension of a manifold. And if you look at this local tiny piece, then the longest direction here just indicates a tangent line. And the other direction is essentially, you can systematically ignore this direction so that you can say that only this direction matters, so that you can say that this data set is essentially one-dimensional. This data set is essentially one-dimensional. So here's another schematic just describing how this is done. If you have data sets distributed on a donut, and then you isolate these local neighborhoods here in these spheres, then if you do things correctly, if you do PCA and then count the top two eigenvectors, then you'll get these tangent disks, well, planes really. Planes really. And since the third eigenvalue will be small, you can say that these estimated dimensions will essentially return two. And how you get these estimated dimensions is, oops, sorry, it looks like I dropped the figure. Anyway, so as I said earlier, this talk will be about a mathematical technique used to prove Um, um, the results that I were hinting at earlier. So, the although I will only briefly mention what the proof is actually like, um, I'm only mainly aiming to convey what the results are because it's kind of cleanly formulated. And that's kind of the punchline of this whole talk. So, how you can do these estimations is to use matrix concentration inequality and use a Wasserstein distance, and well, which is kind of And well, which is kind of similar to saying that you use a transportation plan for moving around your Earth in the sense of Earth movers' distance of the Wasserstein distance. And the matrix concentration inequality matters because we are estimating covariance matrices locally, which is what PCA is about. So, let me briefly recap the technicalities of PCA. We start with a We start with a set of data points in the ambient large d-dimensional space and their small m number of points. Then PCA, principal component analysis, refers to diagonalization of this matrix. So you average over the point count and you take each data point away from its mean, take its transpose, sum it up, and average it. And you get this symmetric matrix. It and you get this symmetric matrix and you diagonalize it. And this matrix U will be orthogonal because of the spectral theorem. And lambda here is a diagonal matrix of real entries, positive real entries. Sorry, no, non-negative real entries. So as I was saying earlier, the main interest is to look at the largest eigenvalues and the corresponding eigenvectors. So in order to estimate tangent space, now suppose Now, suppose we had another point in that ambient space and some local radius parameter. You can also call it a bandwidth parameter, same kind of thing. And local PCA here performs PCA on your data points intersected with the ball of radius r centered at y. So you're looking at only points nearby your Y. So suppose you had a manifold embedded in the Embedded in the ambient D-dimensional space. And suppose it's compact and smooth, because it's easy to do this. And suppose it's a D-dimensional one where we're losing lower D and this is upper D. This is sub-manifold. And now we're taking a probabilistic thing, which is what this conference is all about. We take a sample of size M drawn from the uniform distribution on M and small. Um and small r then we should have that the um d dimensional uh tangent space no dimensional um no span of the top d eigenvectors of the the covariance matrix earlier should approximate the tangent spin of the manifold all right this was the picture I wanted to show you earlier Ah, this was the picture I wanted to show you earlier. And this is how you will be dealing with the estimating the dimension. So this is a rather typical kind of diagram illustrating a typical scenario happening in PCA applications where if you calculate the local, if you calculate the eigenvalues arising from PCA, you're looking for a steep drop in eigenvalue, and then you want to say that only these four components matter. Only these four components matter, that you can say it's four-dimensional. That's what you want to say. But then, you know, if you want to do it correctly, then you need some sort of threshold parameter for how much of these tiny values you want to ignore. So we depend on like a threshold parameter, which I denote by eta here. Then we'll be looking for the largest k such that the tail sum is less than the eta proportion of the total sum. So, okay, finally, I'll get to the theorem today that I'll be talking about today. So, earlier I said uniform sample, but actually, it applies to a more general setting. So, let x1 to xn be independently identically distributed sample from distribution mu, where mu is a law of x plus y. And x here is defined using probability density. Is defined using probability density phi on manifold m and y is any random variable, d-dimensional random variable with the norm bounded by small s. So this y represents noise and x and y need not be independent, actually. So this means that we may have noise that vary throughout your whole manifold. That vary throughout your whole manifold. So now, what we're doing is we supply angular measurement threshold parameter theta and delta for probability. And then we assume that the radius is bigger than some multiplication of the noise parameter and some quantity S1 that will appear below. And we assume that m times this arsenal. m times this r to the power of d kind of quantity here is also big enough. So notice that it's m over log m times r to the power of d, essentially, where s is again the noise parameter. Tau is a quantity called reach of the manifold. Then with probability at least one minus delta, I can ensure that the maximum of the estimation error is within theta. So that's essentially the theorem, but we have to see what the S1 and S. But we have to see what the S1 and S2 are. So, one punchline of the research here is that all of these quantities are completely explicitly computed, so that you can look at all of the dependence on each number. And everything that remains is just C1, C2, C3, where which are just numbers. And in which case, quantification of improvement is easier, that you can just improve these numbers. So here's So, here's an alleged theorem for dimension estimation, where you have the same kind of setup, but you're also supplied with threshold parameter eta. And then you get probably at least one minus delta that dimension estimation is correct at every point, where eta here actually matters in the estimator. I just suppressed it from the notation. Eta is the threshold. Threshold that I mentioned earlier for how much you're going to ignore the small, supposedly small eigenvalues. Right. So I'll just briefly mention the strategy of proof. There won't be too much to say. Well, there is a lot to say, but it's much easier to keep all the details hidden. You can go to archive for the. Go to archive for the details because the whole paper itself spans about 40 pages because a lot of things have to be done really carefully. But the total estimation error happening from these estimation is allocated to two approximations, where one is a probabilistic nature, where you are saying the empirical covariance is close to the true covariance. And the second source of error is a rather geometric source, where you have to take into account the curvature of the You have to take into account the curvature of the manifold and the non-uniformity and the noise. So, part one is a simple modification of a standard but powerful matrix concentration inequality called matrix wifting inequality. And part two here is measured using the Bascherstein distance. And this is translated to the distance is translated to matrix norm using a Lipschitz relation that we also prove. Lipschitz relation that we also prove in the paper. So basically, it's just divided into these two steps. So lots of dots. That's like similar to this curvy Pringles thing. And then that's again similar to this disk thing. So blue is similar to orange, and orange is similar to pink. And the transportation plan here. Plan here is like very multi-stepped. You have to go from this noisy distribution down to the noise version and then apply inverse of an exponential map. Fold in the outer edges that look curvy and then redistribute all the non-uniformities and then rescale, which uses area formula from geometric measure theory. Quite interesting. Theory. Quite interesting. So there's quite a lot of terms involved, but like at the end, we just simplify to something much easier to write. But yeah, that's all I want to talk about today. Thank you. Thank you. Yeah, I'm just going to second it. 