The third talk of the morning by Wendell Xue and she'll be talking about correcting strategic misreporting behavior. So hi everybody, my name is Wendell Xue. I'm a PhD candidate at the University of Washington. I'm going to be a postdoc at UTLC. I'm honored to be advised by Professor Black. And today I'm very excited to share my work correcting strategic misreporting behaviors. Correcting strategic misreporting behavior on outcome variable in estimating treatment effect. So, this work does not aim to advance any OT or DRO theory, but however, it aims to use OT as an application to in-house causal inference methodologies. So, suppose you are a researcher interested in how I can reduce crime and the violence behavior in light area. So, and you are wondering, could the cognitive behavior therapy be an effective way? So, this is the exact research question that Blatman et al. We are interested in to answer in their 2017 AER paper. So, to do so, they hire 3,700 subjects and they randomize into treatment group and control group, where in the treatment group, they receive the eight-weeks cognitive behavior therapies together with a Behavior therapies together with a $200 cash transfer. However, to collect the outcome variable, the user survey data and try to ask the question, how many times have you conducted criminal behavior or criminal-related behavior in the past few weeks? However, their research faced criticism from the audience saying that, okay, because they are criminal-related activities, are your Activities, is your measure of your outcome variable reliable? So, because those respondents always have the incentive to under-report their criminal activities, I think that's a valid concern. In fact, there's a famous Bouhart law saying that when a measure becomes our target, it ceases to be a crime. So, to tackle on this concern, Flatman, they also did a You know, did a second papers, which is published in 2016 in the Journal of Development Econ, which they collect the validation data set in both the treatment group and control group to tackle this concern. So here's this result. So this is the result from the AER paper saying that the treatment is significantly could reduce the criminal activities for those criminal related activities. Criminal-related activities. However, in their 2016 JDE paper, they find that use the validation data set, you know, they're just using the reported outcome, it could underestimate the treatment effect. Seeing that here, using the reported outcome, it's minus 0.4, but however, using the validation data set, it increased to minus 0.5. Okay, so what does that mean? It means that the respondents, there is the Respondents, there is the reporting behavior is not consistent in the treatment and in controls. If they are consistent, then when you do the treatment effect, you subtract from that, subtract the control from the treatment. So if it's consistent, then it does not matter. It will not bias the treatment effect. However, we find that there are inconsistent misreporting behavior in treatment and in control. So our paper tried to propose a method to tackle. Propose a method to tackle this problem. And the scenario, here is our result. And we find that, you know, under this, our proposed method, we could correct the bias. But it seems like it works magically. So what is the insight? We all know the famous no-free lunch theorem. Am I violating this no-free lunch theorem? Okay, so but I know you have lots of questions, but maybe hold on, I will get to that point. Okay, so um there's one more um interesting thing I want to mention. In our proposed method, we only use half of the validation data set in the data set. We only use half of the data set in the treatment group to correct that, to correct the inconsistent misreporting data. But I will get to that with our identification assumption. Okay, so some of you might also. Okay, so some of you might also have a question saying that, you know, this is high-stake reported outcome. So maybe, you know, in terms of criminal related activity, yes, there are inconsistent strategic misreporting behavior. But how about other outcomes? So here I'm saying that if your outcome, your reported outcome, can serve as an incentive to induce your strategic misreporting, then it will bias the treaty effect. Then it will bias the treatment effect. So, another example here is if a researcher is interested in what is the effect of adopt chat GPT in helping the employees' working performance. So, in this case, I summarize that their employees, they could over-report their working performance. I summarize here: there are two kinds of incentives. One is the incentives related to the reported working performance, such as bonus. Performance such as bonus. The second case is not the value of the reported outcome matters, but the rank of the reported outcome, which serves as an incentive. So that promotion decision is usually given to the top two employees with the highest working performance. So I will tackle the strategic and misreporting behavior for those two different kinds of incentives. So our research falls into the So our research falls into the literature of the differential measurement error. So differential measurement error means that even conditional on the observable covariates, the measurement error is different in treatment and in control. So we find some empirical evidence of such differential measurement error exists. There are also some methodological paper tackles on this differential measurement error. So Benderland, their approach is they're trying to use as Their approach is they try to use a sensitivity analysis, but however, they require the knowledge, the prior knowledge of the sensitivity, which is the direction and the magnitude of the differentiation cycle. And we of Lee, and they use validation data sets to provide such information of the prior knowledge. And Huawei Marker, they adopt a partial identification to tackle this. So, our paper is. So our paper is different in the sense that first we tackle all the strategic misreporting behavior, which means I kind of narrow down the approach of the why there's a differential vector because there are strategic misreporting. And the second thing is that we also use a medication data set, but in the treatment. This work is also related to the application of optimal transpose theory in its Optimal transport theory in economics. And yeah, so due to time limit, I'm going to split the second part of the literature. So here, I'm using a simple economic model to first to pin down the notation of the potential outcome framework, and second, try to illustrate that when there's incentives linked to the reported outcome, people could report differently in the treatment. So in this case, I use the y star, y0 star, and y1 star to denote the potential outcome of the potential true outcome. I use y0 and y1 to denote the potential reported outcome. And the difference between the true outcome and the reported outcome are the you know the measurement error. So here I use uh W one and W zero to denote that. W1 and W0 to the So when a respondent deviates from his or her true outcome, he could face some cost. Some cost of misreporting. Here, this is denoted as the cost function C, which is a function of the amount, how much he deviates from the true outcome and also his observable covariance. And he could also, by deviating from reporting his true outcome, From reporting his true outcome, he could receive some reward. So, here, because the reported outcome serves as an incentive, so the reward function is a function of the observable covariant x and the reported outcome. So, a rational respondent will maximize his utility here, defined by the value, difference between the value and the cost, and choose an optimal amount of misreporting. Optimal amount of misreporting. So here we can see that because the value function at a higher value of the reporting outcome, the slope is flatter. So this caused this, you know, the optimal amount of this reporting is going to be different in the treatment and in the control. Okay. So to summarize, we doubt a potential outcome framework and moreover, our data we can observe the Our data, we can observe the reported outcome, the covariance, and the treatment assignment. And also, I need to assume that we have a validation data set in the treatment group. So, we are interested in the average treatment effect on the untreated. For example, in the criminal-related activity case, we want to know what is the treatment effect if such therapy can be applied to a broader population. Population. So then that's the average treatment effect on the untreated. But however, this framework can be easily extended to other treatment effects such as ATE or ATT. Okay, so we assume the strong ignorability assumption are the true outcome, but however, we do not assume the strong ignorability assumption holds for the reported outcome. So, if I denote the gena and gena prime as the misreporting behavior in the treatment and in control group, then we can write the average treatment effect on the untreated to be as the, you know, the second equation. So, this is to say if your misrepresentation part accounts for the misreporting behavior, you know, if the inconsistent misreporting behavior, and the first Misreporting behavior, and the first part accounts for, you know, but that is that could be directly identified from a data set. So, this tells us if you do not have, you know, inconsistent misreporting behavior, if your misreporting behavior is consistent, then you can directly, you know, then your parameter of interest is identified by your data. So, in the baseline model, we are saying that, you know, if your misreporting behavior is consistent. This reporting behavior is consistent, then your treatment effect can be written as this. So, well, here the nuisance parameter mu mu naught is just a conditional expectation function. And estimation is very straightforward. You can replace the conditional expectation function using any non-parametric estimator or machine learning algorithm to estimate this conditional expectation function. Then you just simply Expectation function, then you just simply use a plug-in estimate to get the treatment effect. So then back to the first scenario when there's incentives linked to the level of the reported outcome, we assume this. So that is to say, your strategic, your misreporting behavior, conditional on the covariance, and also the reported outcome should be the same. So what does that mean? So, what does that mean? So, back to our simple economic model. So, this is essentially people are making decisions by marginal benefit and marginal cost. If you have a concave value function and x cost function, you can write the optimal amount of this reporting behavior as a function of the covariant x and the reported outcome. So, this essentially is saying that your optimal reporting behavior. Optimal reporting behavior once conditional on the reporting outcome because it serves as an incentive, it should not be affected by the treatment effect. So, that is to say, in other words, this assumption is saying that, for example, in the employee case, you should compare the employees who, you know, say, report eight hours a week for the working eight hours a day, for the working hours. You should compare that to the people who also report eight hours. People who also report eight hours a day. So, because their incentives are some kind of similar and their strategic reporting behavior are comparable. So, this is essentially what this identification assumption is trying to say. So, under this identification assumption, we can write the treatment effect as the following equation. Following equation, where again there are two users parameters which denotes the conditional expectation. And to estimate them, you can simply use an empirical sorry um sample average of of of this and also you can estimate those Newsen's parameter with the with the parametric estimator. With the parabolic testing too. So, then in the second scenario, we are trying to tackle their incentives not just related to the level of the reported outcome, but also to the rank of the reported outcome. So, here we try to, you know, so here we use the state-of-art definition of the optimal transport to define the multivariate rank, which plays an important role. Variants rank, which plays an important role here. So, yeah, so here this transform map is defined as a push-forward map between the reported outcome in the treatment group and the reported outcome in the control group, so that their rank are comparable. So, we all know that the optimal transfer series are skipped. This means that under this idea. This means that under this identification assumption, our parameter of interest can be written as this. So again, so we need to estimate the conditional expectation function, but here we need to estimate one more thing, the optimal transport map. So we adopt the estimation strategy in Chevang Xie, which we did, you know, we try to estimate the We try to estimate the optimal transfer map in three cases. One is in the univariate case, and second is when the optimal transfer map is in the pathway map, and third is we use a more general case. We use a spline estimator to estimate the potential function, and the optimal transfer map is given by the derivative gradient of the estimator for the potential function. Okay? So So here we give the empirical application, empirical result using BLATM and et else, their data set, and give the two scenarios estimator for these two scenarios. So we find that if we account for the strategic behavior due to incentives on the value of the reported outcome, you know, using the reported outcome could overestimate. But however, if you account for this. However, if you account for the CGT behavior, where the rank of the reported outcome serves as an incentive, using the reported outcome could underestimate your treatment effect. One more thing here is you can find out that the standard arrow of our scenario two is much smaller than the standard arrow in their paper. Although again, we only use half of the data set. This is because This is because we make more assumptions on the, you know, we make more assumptions on people's strategic behavior, so we have more statistical power of that. Okay. So the takeaway is, so first of all, ignoring strategic misreporting behavior can introduce bias to your estimation for the treatment effect. And we study the identification under three cases, a baseline scenario. A baseline scenario when the reporting behavior remains consistent, and that's strategic misreporting behavior due to incentives linked to the level of the reported outcome, and that's strategic misreporting behavior due to incentives linked to the rank of the reported outcome. We use a plug-in estimator, and we show that the estimator is consistent and asymptotic normal in our paper. And we also study the And we also study the performance of the estimator through simulation and empirical applications. That's pretty much for my presentation. Comments are welcome.