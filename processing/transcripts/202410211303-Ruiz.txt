And our next speaker is Luana Rubies to tell us about some graph theories. Thank you. Thanks, first of all. So thank you for the invitation. Second of all, I feel compelled to explain why I'm worried this thing. So I fractured my jaw. And so this, for a few weeks, is better than getting surgery. And so that's why I look weird. So today I'm going to talk about some work that I did with Theme and Stephanie on signal sampling theory for Sampling theory for large graphs using graph limits called graphics. So, especially for these audience, you know, it shouldn't take much to motivate the need for more efficient algorithms for large-scale graphs. We know that the vast majority, if not all, of analysis and processing algorithms for network data scales with the number of vertices and number of edges of. And number of edges of a graph. And so that can become very costly very quickly, right, for modern networks. But the good news is that in most problems, the intrinsic dimensionality or the latent dimension of the problem is typically much smaller than the rough size itself. So you have D much smaller than N. And just to give you a few examples, you know, one is the classical example of community detection. So in well-behaved graphs, if you want to do clear community detection, typically you To do K comet detection, typically you can do pretty well by just looking or doing spectral clustering on the top K adjacency or Laplace and eigenvectors. And just to give an example from classical graph theory, right, we know that we can determine a graph's bipartite structure by just looking at eigenvectors associated with a symmetric eigenvalue pair. So these are just two examples, and there's many more examples where this sort of thing is true. This sort of thing is true. So maybe the way we can think about this is we can think of n as the data resolution, right? So we have all this data, we have the representations that we're trying to learn of the information that we're interested in, has dimension much smaller than the data. So as long as we have n larger than d, we should be able to find solutions. Okay, so this is a machine learning workshop, right? So how does Learning workshop, right? So, how does this relate to machine learning specifically? Well, in machine learning, if you're going to train, for instance, like graph neural network for your graph, the cost of training this graph neural network is, it scales with the graph size, right? So, regardless of whether you represent your graph as an adjacency matrix or as an edge list, the cost of training of that model is going to scale. So, one idea is to sample. is to sample a node subset and train your model on the smaller on the smaller graph induced by this node subset. This is the idea of learning by transference or transferability in GNN training. So you can do this and it works well in practice. And over the last few years there have been many people working on this area by focusing on different types of graph models and ref limits. Graph models and graph limits. And what people have shown is that if you have that, you know, you have two graphs of different sizes, and they both come from the same model, meaning they either converge to the same limit or they can be seen as samples from the same random graph model. If you have a GNN with fixed parameters and you transfer it across these two graphs, provided they both come from the same model, the difference between our outputs is upper bounded by something that decreases with n and f. That decreases with n and f asymplicity. And so, what this implies in practice is that we then have theoretical guarantees to take our GNN model, which has a parameterization that is independent of the graph, train it on the smaller graph, and transfer it to the large graph with performance guarantees given by the full form of this upper bound here. Okay? Now, this seems to solve our problem, right, of doing machine learning at large-scale graphs. Doing machine learning at large-scale graphs? Like, why do you need more than this? One reason for that is that, you know, in practice, how are you gonna do this? You have your large graph, say that you don't have computational resources to train your model on the large graph. So you need to decide how large your small graph has to be in order to get good results on the target graph, right? So in order to do that, you're going to use this upper bound. I don't have the full upper bound here, but there's some constants. Bound here, but there's some constants, and so you're going to look at kind of like the error allowance that you have, and from there, you're going to decide the minimum graph size in which you want to treat your GM. Now, this upper bound, it holds even for graphs that are sampled by sampling the nodes completely at random or a sample space. And we know that that's not a very good way to sample graphs, right? Because in that case, especially if you have a sparse model, you're likely to sample a disconnected Sample, a disconnected subgraph. And so this bound is likely loose, right? If it holds even for these kind of diagram sequences, it's likely loose. And so the question is, can we do better? And we can do better provided that we have more information about the signal. So if we have some priors on the signal, we can do better. And specifically, here we're going to assume that we have. Going to assume that we have priors related to the spectral content of that signal in a way that I would formalize in a little bit of a way. So spectral priors, I mean, are essentially related to having a sampling theory for signals, right? And this goes back to Nyquist. But for graph signals in particular, this problem has been studied now for probably over a decade. Now, for probably over a decade in signal processing and more specifically graph signal processing, the first person to kind of come up with a theory of signal sampling on graphs was Pesenson, who's a mathematician. And he came up with a theory. And then after him, people in the signal processing community came up with algorithms to try to somewhat efficiently or efficiently obtain these sampling sets. In on graphs. And then since then, there have been, you know, people assuming different types of priors for general priors and subspaces for those priors, trying to obtain guarantees when you know something about a reconstruction objective and so on. A lot of the work has focused on trying to make these methods more efficient. And we need these methods to be more efficient because, at the end of the day, signal samples. Signal sampling on the graph, as I'll detail a little bit, it comes down to: you know, you compute your graph Laplacian, you compute the eigencomposition, and if you want your signals that have, you know, spectrum components of the top k eigenvectors of the Laplacian to be able to be reconstructed, you need to do Gaussian elimination on those top k eigenvectors. And so, not only do you have to do a magnetic composition of a large graph, which is expensive, you also have to do Gaussian elimination, which is Also, have to do business lumination, which is also expensive on these brass. And so, clearly, that's not very efficient, right? Especially when you're talking about modern bronze. And so here what we're gonna, what we proposed was to use that idea of transferability in sampling as well, right? So if we can assume that we have our graph, our large graph, come from a random graph model or be part of a sequence that. Model or be part of a sequence that converges to some limit model, then can we perhaps find these sampling sets on the limit and then somehow project them back to the large graph? And the idea of exploiting the limit is because in the limit it's easier to see this kind of low-dimensional structure of your graph. And then that's kind of like the practice, right? But then the theory is, you know, to get guarantees on this, what we need is we need. Guarantees on this, what we need is we need for our sampling sets, our optimal sampling sets on our graphs to converge to optimal sampling sets on the graph. So to motivate things a little bit, I'm going to go back to the real line, signals on the real line, or if you will, like signals in time, continuous time. And for such signals, the classical theorem of The classical theorem of Shannon-Nyquist says that if you have a signal whose Fouliat transfer is then limited between minus B and B, then it can be uniquely determined by uniform discrete samples at rate 2b. So this is exemplified here of this picture, where you have your signal in blue, and then because these samples in red taken at a given sampling frequency satisfy Frequency satisfy this condition here, then what do you know from this theorem is that you can perfectly reconstruct the signal from these samples, right? And just to kind of go back to machine learning again, if you think of this, you know, of trying to learn representations from the signal, right? Say that you have a sample with very fine resolution, which is the sample in blue, and you use it to learn some representation. Learn some representation. What Nyquist essentially says is that as long as the representation only depends on d, the kind of intrinsic dimensionality of your signal here, which would be essentially the number of components in your Boolean transform, then you could get away with just learning your representation from fewer samples as dictated by the Declarist rate. Okay, so what does this look like on round? Okay, so what does this look like on graphs? So, to extend these notions to graphs, we first need to define graph signals, right? Like, what is a graph signal? How does that relate to embeddings and features on nodes? What are graph frequencies? And what are these bases of limited signals on graphs? So, a graph signal is essentially the collection of feature vectors on. The collection of feature vectors on the nodes of your graph. Right here, I'm representing it as a function mapping nodes of your graph to the real numbers, and it's a finite energy function. So it's a function l2, right? And the notion of frequency actually comes from the Laplacian, the graph Laplacian. So we take the Laplacian, we normalize it, and then we calculate its eigencomposition, right? Composition, right? So the eigenvalues of the Laplacian work as, or there's a nice parallel between these eigenvalues and graph frequencies, this notion of graph frequency, because you can define the total variation of these signals F as essentially their inner product with kernel L. And then since eigenvalues are frequencies, then the eigenvectors associated with these eigenvalues are the corresponding oscillation modes in the graph. Models in the graph. And then having this notion of a graph frequency, we can define a graph transform as the projection of these signals onto the corresponding Laplace and eigenvectors. And then we say that a signal is graph benlimited, or alternatively, that it belongs to a baby linear space on graph G if it is such that it only has spectral components associated with. Associated with, I mean, bandwidth at most lambda. Okay, so lambda is essentially the bandwidth of your signal here. And so what Pesinson showed in his signal sampling theory for graphs is that if you have a signal F that is in a pale winner space with n with lambda, then you can uniquely determine it by some uniqueness set, which is a subset of your full node set. Of your full node set. And a uniqueness set is a set such that, you know, if we have F and G belong to this Pale-Wiener space, we bend with lambda, and U is a uniqueness set for this PD-Wiener space, then what this means is that if these signals match on the node set U, right, so if they match here, then they have to match everywhere else in the graph. Okay? So we can essentially recover the entire. We can essentially recover the entire signal from the subset U. Okay, so like I said earlier, you know, you can, you have a bunch of algorithms to obtain these uniqueness sets. They're all heuristics because this problem is combinatorial, but there are algorithms to do so. They are very expensive. And so what we're gonna do is we're gonna try to turn to graph limits. To try to turn to graph limits and find the sampling set on the graph limit. So the specific graph limit that we're going to consider are graphons. Graphons are, you know, just the definition is just as a symmetric measurable function, mapping points from some, you know, sample space, usually the unit squared to the unit interval. And they can be thought of as a graph with an uncountable number of nodes. So you can think of Countable number of nodes. So you can think of the samples U and V as the labels of your nodes, and the value of the graph on evaluated at U and V is like either the edge weight or the probability of sampling an edge between U and V. And I guess more, you know, more useful, we also have two interpretations for graphons. So one is as a graph limit in the sense that the densities of homomorphic. Names of homomorphisms on these graphs converge to the probability of sampling certain motifs on the graph on. And they can also be interpreted as a random graph model in the sense that you can sample your nodes uniformly from your sample space omega, and then you sample your edges randomly with probability given by the graph line evaluated at UIAJ. And here, I just have a figure to kind of try to illustrate these two interpretations. So, if you look at this from left to right, you can see this limit object interpretation of the graph on the right. You can see that we have like a two-imbalance two-community structure here. And as we increase the number of nodes, we see that we can increasingly see that structure. And in the limit, you see that structure very clearly, right? You can see the essentially the lower to. You can see essentially the lower dimensionality, the latent dimension very clearly in the limit. And similarly, you could think of the generative model interpretation as using this model to sample all these graphs. And the way you do that is by sampling points here from the unit interval at random, and then sampling edges for probability given by the values of the graph on at pairs of nodes. This is for an SBM with higher intracommunication. With higher intra-community probability than intra community probability and imbalanced community. Okay, so remember that I showed this sampling theorem for graph signals, right? So the first step to extend this to the graphon limit is to prove a similar result for graphons and signals on graphons. So with this reparte. So, what this required us to do was to first extend all those definitions of signal and frequency and Fourier transform and K-D-Miner spaces to the graphon domain. But it turns out that that's easy to do because, you know, the graph one kind of defines this integral linear operator that's kind of like just, you know, very simplistically, like an infinite dimensional matrix, right? So it has a complete spectrum, a real. A countable spectrum, a real spectrum, and so the only thing that you have to do now is you have to handle these objects that are infinite dimensional, right? So now instead of having a signal which maps a node set to the reals, you have a signal that maps the unit interval to the reals. And now your frequencies are the eigenvalues of your normalized performance, which I don't have on the slide. I have this one. It's a bit of a detail, but This one. It's a bit of a detail, but you can define it relatively easily. And it has a well-defined spectrum. And so the eigenvalues of that operator are now your Grafon frequencies, and the corresponding eigenfunctions are the Grafon oscillation once. And so using this, we can define now Hayley-Wiener spaces, which are the spaces of signals that only have spectral components associated with eigenvalues that hold one that we're levelizing is your threaded. Is our bandwidth, right? And now, with these ingredients in place, the way we define these sampling or uniqueness sets is as these joint unions of intervals of the, of sub-intervals of the unit union, of the unit interval, where if you have two Graphon signals match in the L2 normal, right, so in every set, other in sets of measured zero, in this union of disjoint. In this union of disjoint intervals, u, then they have to match everywhere else. Sorry, I'm going from here to here. Then they have to match everywhere else in the unit interval. Okay? Okay, so I included this here in case I had time. I won't have time. But this was just to show that to obtaining that result is very simple. Is a very simple, it's just a combination of two inequalities. So you have this inequality here, which is pretty simple, right? It's essentially saying, like, if your signal f only has vector components associated with eigenvectors at most lambda, then Lf amplifies F by at most lambda, you know? And then using this Poncage inequality together with this, then we are able to show kind of like the exact conditions under which these unique test sets. These unique listets exist, but I can talk about this more with whoever's interested later. What I wanted to talk about in terms of theoretical results that we derived was actually the core results that we need in order to be able to find our sampling sets on the limit graph on and then use them on the large graph. And that is convergence of sampling sets, right? We need sampling sets on sequences. Sampling sets on sequences of graphs to converge to sampling sets on graphons. And in order to prove that result, what we did was we used some results from the consistency of spectra clustering from this very nice paper that I have cited later by Bimiu and others. And there, what they consider is they consider these kernel mixture models. So your nodes or your samples are sampled from sample space following Space following a mixture, right? And then they are related by some kernel, usually a simple kernel. And in order to use those results, what we did is we related the graphon model to these kernel mixture models. So the way that you would do that is as follows, right? Like assume that your graphon sample space is just a unit interval. And as I mentioned earlier, we sample our nodes uniformly at random from the unit interval, right? Omega interval, right? Now we want to move to a setting or to a sample space, omega prime, where we sample our nodes according to some mixture, some mixture model. So if we want to map these sample spaces, we want to map omega to omega prime, what we have to do is we just have to find a measure-preserving transformation gamma, right? And then using that gamma, we can essentially write our graph on as the Graph on as the composition of a kernel, some kernel with gamma, right? And gamma here will be determined by the mixture mode, right? And in fact, if you know the CDF of the mixture, you can obtain gamma as the inverse of that CDF, right? And this inverse exists as long as the CDF is strictly monotone, which if you had Gaussians here, the components are Gaussians. Had Gaussians here, the components or Gaussians will often be set aside. So how did we use this? So like coming from the realization here, right, that graphons can be represented as these kernel mixture models. What we're able to show, leveraging these spectral clustering results, Spectral clustering results was that if you have mixture components that are sufficiently dissimilar, that are essentially well aligned with the kernel eigenction, so they're all kind of almost orthogonal. And if they are indivisible, and this is that you can't kind of break up one component into more than one component, then if we do Gaussian elimination on the sample of Aussian, we can find. We can find these uniqueness sets, which are essentially sampling sets with at least one node in each certain component with high probability, right? And this is what this figure is showing here. So E2 and E1 here are essentially the true eigenfunctions of your limit kernel mixture model. The points are the samples embedded using spectral clustering on the The end sample Laplacian, and the colors are essentially the mixture components, right? So you can see that the embeddings, right, the spectral clustering embeddings, they kind of form this orthogonal cone structure around the kind of the real orthogonal components of the kernel mixture. And here, I just want to mention that, you know, the reason why you can use these. But the reason why you can use these spectral clustering is that spectral clustering and GAS immunation are very similar. With spectral clustering, what you're trying to do is you're trying to take your samples and classify them into different communities or label them into different communities. With Gaussian immunation, you're trying to find one representative sample from each community. And so spectral clustering is like overkill for Gaussian elimination. And then combining this proposition. Combining this proposition with the fact that the eigenvectors of your graph Laplacian converge to the eigenvectors of the graph on Laplacian, then convergence of these uniqueness or sampling sets on graphs to uniqueness or sampling sets on graphons follows. So we get the desired results. So in the last three minutes, I just want to talk a little bit about how we use this in practice. How we use this in practice. So, in practice, what you have is like you don't have your graph, right? You just have your large graph. And so, what you can, the only thing you can do, especially if you only have one graph, is approximate your graph one using that graph. So, the way that works is that you take your graph, you find its adjacency matrix, and you essentially plot the adjacency matrix on the unit squared, right? That's your approximation of the graph one. The graphon. And now, given this approximation, what we do is we approximate the graphon uniqueness set by this graph's uniqueness set. And then something here is weird because I was essentially saying earlier, like, I don't want to find uniqueness sets on this large graph. But now I'm saying, find uniqueness set on the large graph as an approximation of the uniqueness set on the graph. As an approximation of the uniqueness set on the graph form. So, in practice, we don't really do that. What we do is we first coarse in these induced graph forms. So, say that you have this, your large graph at six nodes, right? So, this is the graphon approximation that you get. What we would do here, because you can kind of see this two-quimity structure, is we would parse in this graph on. We could represent that as a two-node graph, right? This is a very simple example, but it's just to give you an idea of what. To give you an idea of what would happen. And then what we would do is we would find the uniqueness set for this coarsent graph associated with the coarsened graph. And then once we do that, then let's say that we, for the signals that we're trying to represent, we find that just the zero, one-half interval is our unique asset. Then we can project this back into the graph by essentially sampling from this region here. From this region here, if we want to sample sampling sets for the growth. And yeah, so I got a little bit ahead of myself. That was the third step. And there's guarantees for that due to convergence of sampling sets, or graphs to sampling sets on graphs. So, to conclude, let me show you a numerical example where we tried this. So, full disc. We tried this. So full disclosure is that we tried to use this for transferability. So for what I mentioned earlier of sampling subgraphs, training GNNs on these subgraphs and transfer each of the large graphs. It didn't work any better than random sampling in that case, at least for graph ones. And I mean, the reason for that is, one, like a lot of these problems that you have, like these GNM benchmarks, they're kind of, they're not too difficult, but also, you know, if you have. Difficult, but also, you know, if you have a graph that is very dense, it kind of doesn't make a difference how you sample. But where it did make a difference was in calculating graph positional encodings. So graph positional encodings are just a fancy name for eigenvectors. And what people do is when they have these graph classification problems, right, where you have a bunch of graphs and you're trying to classify them, is, you know, you find that a lot of GNN models don't work really well. Work really well. And so, what people have done successfully is they first calculate eigenvectors of these graphs and they feed that as the input of the GNN. And that helps graph classification. But again, you have the issue that if your graphs are large, computing eigenvectors can be expensive. So, what we proposed to do here was to compute eigenvectors on a smaller graph, on a sample. On the sample, and then zero pad these eigenvectors on the large graph. So the results that we got were the following. So without positional encodings on this falmet tiny data set, we got pretty low accuracy. It's better than random, but pretty low. They forget how many classes it has. Then with position encodings, you get a nice boost, right, with these eigenvectors. And then what we did is we replaced. What we did is we replaced those position encodings by these zero-padded eigenvectors that we found on the smaller graph. And we found that even though the performance degrades, it's a little bit better with these graph-on-sample positional encodings than with randomly sampled positional encodings. So there is some improvement, especially when you have like explicit use of spectra information here. And to conclude, so Conclude. So today I talked about transferability techniques for signal sampling on graphs, and again, with the goal of sampling better graphs for learning GNNs via transferability. You know, we saw that the issue with these algorithms is that they're extremely expensive, and so what we tried to do is we tried to find informative subsets using sampling theory, but transfer. Sampling theory, but transferable subsets by using sampling theory on the graph line instead of the large graph itself, with some theoretical guarantees and some empirical results.