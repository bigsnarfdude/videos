Hello everyone, my name is Alex Schetz. I'm a PhD student at CMU first year. I've been working on this project with Ann, Luca, who's another PhD student, and Tomaso, who's in the back corner, and two other collaborators from Italy. So yeah, so we're looking at some of these high energy cosmic rays, and basically, you know, it's Basically, it's pretty well known that the study of these cosmic rays can tell us a lot about what's happening in space. But there's a couple of challenges. Mainly for ground-based detectors, we can never basically directly observe the cosmic ray because once it hits the atmosphere, it disintegrates into the secondary particles. And the other sort of challenge in cosmic ray studies is that not every cosmic ray is. Studies is that not every cosmic ray is performed. And this, the example that kind of we're looking at is protons and other charged cosmic rays that get deflected by intergalactic magnetic fields. So maybe in the future, Francesca's project, the galactic magnetic fields will be mapped out super precisely, but at this point, it's something that remains a challenge. And on the right here is a picture of the proposed Spigo Observatory. It's a gamma-ray observatory that they hope to build in the southern atmosphere, or the southern hemisphere. And yeah, so basically another ground-based detector where we would be studying these secondary showers. And so, really, the point of this project is using the Using this data from the secondary showers that we collect at the ground level, there's two things that we want to do. The first is we want to identify photonic or gamma rates out of this basically collection of cosmic rays that hit the ground. And again, the main concern here is that with photons, if you figure out what direction they're coming from, you can track them back to the source, and the source might be something interesting happening. The source might be something interesting happening. Whereas with charged confident rays like protons, they get deflected by magnetic fields, and even if you figure out their incidence angle as they hit the atmosphere, you don't really know where they came from. And so we really want to be able to separate showers that come from gamma rays and other protons and showers that come from charged cosmic rays like protons. And once we do that separation, Once we do that separation, we want to then estimate various parameters of interest for these cosmic rays. And for this project, we'll be focusing on energy and sort of their orientation or basically where they came from. So the data that we're working with comes from the Corsica Cosmic Ray Simulation, and basically, it's what sounds like it's a simulator for cosmic rays. For cosmic rays. And basically, what goes into, part of what goes into the simulator is you have a primary cosmic ray that hits the atmosphere and explodes into the secondary shower. For the primary cosmic rays, you can specify what kind of cosmic ray it is, its damage radio proton, how much energy it has, and sort of what angle it's hitting the atmosphere at. And basically, what comes up is that it's a very important. And basically, what comes out of the simulation is this entire simulation, like basically the entire trajectory of all these secondary particles. And sort of on the right you can see sort of this projection of these trajectories, kind of looking at it from the side. But what we're mainly interested in in terms of the output of the simulator is what happens at the ground. So at the ground, for each secondary particle that hits the ground, we would be looking at what it is, the identity. Looking at what it is, the identity, how much momentum it has, where it hit the ground, when it hit the ground, and these essentially would constitute our data set for each cosmic ray. So this is what basically the data looks like. So each one of these plots is an individual simulation of a cosmic ray produced by Corsica. And what you're seeing here is And what you're seeing here is on the x and y axis is a location on the ground, and each dot is a secondary particle from that secondary shower as it hits the ground. And so the color represents the timing of when it hits the ground. You see the lighter colors are basically hits that occur later in time versus the darker colors. And the green arrows represent the direction. Represent the direction that the cosmic ray is traveling in. So that's a parameter we can put into the simulation. And you can sort of already see that the kind of timing information lines up with what direction it's going in. There's also another parameter called the zenith angle, which basically controls how steep or shallow the cosmic ray is coming in. And you can see for, I think, this one has a pretty shallow zenith versus this one, which has a pretty zenith. Versus this one, which has a pretty like, or sorry, pretty steep zenith, and this one has a pretty shallow zenith. You can see there's a big spread in the particles across the line. And so this is basically the data that we're working with, and we're interested in doing some inference on the cosmic rays that produce these prints. So, sort of to go back to the likelihood-free inference framework, so what we Framework. So, what we are working with is essentially we have some likelihood, again, complicitly encoded by the simulator, of the observed data. And here we're splitting for this first goal, again, the first goal being identifying which cosmic rays are photons. We split the likelihood or the parameters that go into the likelihood into mu, which is the identity of the cosmic ray, and nu, which is sort of the other parameters that affect. Sort of the other parameters that affect the likelihood. And so then, as a reminder, that's the energy of the cosmic ray, as well as the zenith and azimuth angle, which just say where the cosmic ray is coming from. And so sort of the simulated data would look something like this. You have these simulations, and for each one, you have different identity, different energy, zenith, and all that. And then for the output of the simulator, which was all of those secondary particles being detected at Group. Particles being detected at ground level. And here we're in step one, and we're interested in inference on u, which is the identity of the cosmic ray. And this is sort of like a signal versus background problem. So I learned that the majority of cosmic rays are actually protons, and relatively few gamma rays are detected. And so the ratio is around 1,000 to 10,000. Ratio is around 1 to 10,000. I just learned that anomaly detection is not the best way to phrase this problem because the gamma rays are supposed to appear, sort of in the cosmic ray flux, but just due to the rarity of this, I guess in statistics we can think of this as sort of an anomaly detection problem, but in reality it's just an inference problem on the identity of the cosmic ray. Of the cosmic ray. And in this inference problem, one of our goals is controlling the type 1 error over the entire parameter space of mu, which is the identity of the particle, but also over nu, which is the other parameter. So no matter the energy, no matter the orientation, we want to be able to control type 1 error by doing inference on the identity of the particle. And now you might. And now you might, someone might look at this data set and note that X here is very high-dimensional. So if X is every single one of those particles as it hits the ground, you can even think of it as a very high resolution image. And one of, you know, kind of a naive approach might be to construct a classifier that just tries to predict you from the observed data. And for a variety of reasons, that's not really sufficient. One of the big problems with direct classification is that it depends on the distribution of mu in your training data. And if you train on half protons and half photons, you're going to get a different performance versus training on the actual 1,000 to 10,000. And you also don't really. And you also don't really get touch point error control. And so, hence, we're looking at this using the LF2I framework. So, yeah, so what follows is basically how the LF2I procedure would be performed on this sort of problem. We would define, for a given footprint, we would define the tests that we would like to perform. And so, again, just focusing on the identity. Let's say primary particle, we want to test. Primary particle, we want to test the null that it's a proton or a charged particle versus that it's a photon. And we note that because this is nuisance parameterized, this is actually a composite versus composite hypothesis because we want to do this testing for all values of energy and the angle. So we define a test statistic. So, Anne talked about different test statistics that you can use in LF2I. So, here is an example. So here is an example using the Bayesian frequentist factor, which is defined like this. I won't go too much into the details. And the important thing here, and something that Anne touched upon, is this odds function, which is, again, part of the test statistic, something that can be estimated via a probabilistic classifier. And more importantly, you can throw And more importantly, you can throw a very powerful machine learning method at this problem to estimate the odds as well. And so you can imagine, like, if you have this image of secondary particle detections, you can use something like a CNN or use basically any method that can estimate the odds problem. We estimate the cutoffs at every value in the nuisance parameter space using quantum. In the Newcombs parameter space using quantile regression. And if this is estimated well, we can define a rejection rule that guarantees us type 1 error control regardless of the values of the nuisance parameters. And so basically that gives us a test, a way of discriminating the photons, protons, and continuing our analysis with the photons, which are the things. Our analysis with photons, which are the things that we care about. And so, once we have done that step, and kind of assuming that we apply this to the data and what we have left over is just all the photons, we can basically do LF2i again. And the nuisance parameters that we had in the previous step now become parameters of interest. So the stuff like the energy and the angle are now the stuff that we want to construct confidence intervals for confidence set score. Score or confidence set score. And this is more of a vanilla LF2I approach. So one minus alpha confidence set for these parameters. And that's basically the same as performing this test over all values in the parameter space and finding out what the test is not rejected. We can use exactly the same statistic. And what's nice about this is that this odds function is exactly the same odds function that we have. This is the same knowledge function that we used in sort of the previous step where we were discriminating the identity of the cosmic array. So we can get to reuse this for free. And same idea, we estimate the quantiles using cutoffs using quantile regression. And we can form set confidence set by the inverting the hypothesis test. Running the hypothesis test. And that then gives us a nice confidence set for each of these photons data set. We can then get some idea of where they came from and their energies. And this is useful for a lot of downstream applications in astronomy research, especially if you get a sudden flux of gamma rays from a source in the sky. We can, if we can quickly infer that they're all coming from this point, then we can tell. Coming from this point, then I can tell all the other researchers to go point their instruments at this point in the sky. So, again, just a quick summary. So, we're interested in cosmic ray reconstruction from, again, the ground-based detector data. And so, first step, you identify which ones are photons, which ones aren't protons or other charged particles using the LF2I mixings parameterized. And then we, for those who are telling us, we construct those confidence sets using LF2I. That's it. Thank you. Great. Thank you, Alex. I think we can stop the recording and then go to launch the discussion session. Are we ready?