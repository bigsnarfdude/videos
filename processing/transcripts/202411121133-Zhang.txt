Thank you everyone. Good morning. Thank you for being here for my talk. Thank the organizer for the kind invitation. It's my great pleasure to present my work here. So in today's talk, I'm going to present a new framework called Alpha Potential Game to solve end-player dynamic game. And the talk is based on recent joint work with Singua and Cinevi, both from UCBR. So as we already see, So, as we already see in the several talks this morning, so unplayer game plays an essential role in finance and social science and engineering. So, this type of problem essentially describes the competition or strategic interaction between several players, where each player tries to optimize their objective function based on the strategy of others. So, in many cases, we are interested in Nash equilibrium, which is essentially a set of strategies where given the strategy of others. They are given the strategy of other players, so no player has incentive to deviate from this previous given strategy. And as you can see from this notion of the Nash equilibrium, there is a strong interaction between all players' action at the equilibrium. So that's why it is well known that Nash equilibrium is difficult to analyze and also difficult to compute. So in practice, this homogeneous assumption to facilitate mean field theory appeared to be resolved. Field theory is appear to be restricted. So, in order to analyze n-player game with heterogeneous players in the static setting, so two economists, Modern and Sharpley, introduce the notion of static potential game. So, in this clear case, they consider a general static potential game. There is this I n, which there are n players. Each player, they have their action set, which is given by AI, and they try to optimize their log. And they try to optimize their value function depending on the joint action profile of all the n-players. So, this is just a general formulation of n-player game. And they consider a particular subclass of n-player game, which is more tractable. Which they call this alpha potential game. In a sense, there exists a function called a potential function such that this identity is satisfied. What it means is essentially saying that for any player, say player I, if they want. say player i if they want to deviate a player i unit the if the player i unilateral deviates their strategy the change of player i's value function is exactly given by this potential function okay so this potential function is independent of the player okay so this is a very uh good framework to analyze and hi uh hi uh earn their game with heterogeneous player because at this moment there is no assumption about the homogeneity between AI also with BI. They can be generally different. They can be generally different. So the advantage of this static potential gain is that you can show by this identity the requirement any minimizer of the potential function gives you a Nash equilibrium. So essentially computing the unfair Nash equilibrium becomes minimizing the potential function. So it also approved to be a very powerful to analyze a learning algorithm where many decentralized learning algorithms such as bias response or policy related methods can be Policy gradient method was can be shown to converge to the exact Nash equilibrium if it is a potential game. If it's not for the general player game, this type of algorithm may not converge, but for the potential game, it's actually converging. So given all this powerful framework, then it's very natural to ask what happens if it is a dynamic game. So actually, this has been studied, proposed recently by some computer scientists for a discrete Markov game in the sense that the environment is a discrete time Markov chain, and then the each Time Markov chain, and then each player tries to optimize over some Markov policies. What they try to do is that they just directly extend this concept of the static potential game to this dynamic setting, where they say there exists a potential function such that the change of the value function is the same as given by this potential. And as you can see, naturally any minimizer of this potential function is a natural equilibrium. And also various algorithms has been shown to converge to an exact equilibrium. Converge to exact equilibrium if the dynamic game is a Markov potential game. Of course, there is a serious drawback of this framework in the sense that actually many dynamic games are actually not Markov potential games. So you can actually show that it's a dynamic, even though you consider a very simple dynamic game where at each state you have a static markup game because of this dynamic structure, the overall dynamic game is not a markup potential. Overall dynamic game is not a mark of potential game. It is well known in the community, given a discrete game, to verify it is a mark of potential game is actually very challenging. Because, as I showed, in many cases, it's actually not a mark of potential game. What this essentially shows is that, since there is a more general form of potential gain, it's required in order to analyze dynamic gain. And this is what we are trying to do in this work. So, we consider a more general work without restricting to Markov property. So, we consider a general employee. Property. So we can see the general unplayer game where there are n-players, which is given by this index item. There is a, because it's a dynamic game, there is a state space S, which models the underlying state space of the underlying system. Each player tries to optimize some set, AI. So at this moment, this is just some set can be infinite dimensional. So and then as usual, I denote this A n as all the visible action profile of all the n players. And AI means that the visible profile. means that the musical profile except player I. And each player try to optimize the value function Bi, where BIA means that it is player I's expected cost if the underlying system state starting from a fixed initial state I0 and all players chosen the actual profile value. And in this case, each player tries to minimize the value function over their own strategy AI. So this is a very abstract framework because I didn't specify Abstract framework because I didn't specify how to define this VI. And what is the definition of AI? So we try to keep it general in the sense that it allows for both discrete and continuous state space. It just follows from different definitions of the AI. And it also allows for both open loop and closed loop control. They are just follows from different definitions of the misable set. In the later part, I will going to specialize to a particular setting which I am more familiar. So in this case, So, in this case, what we try to propose is essentially relax the framework about a potential game, and we call it alpha potential gain. It's a very natural relaxation, but it's a very important relaxation. So what we say is that a dynamic game is alpha potential gain if there exists a parameter alpha, such that also a potential function, we call alpha potential function, such that the change of the value function is equal to the change of the potential function up to an error alpha. Okay? Error alpha. Okay? So this is an important relaxation because by allowing for this deviation alpha, most dynamic game is an alpha potential game. For example, you can just simply, for example, one naive choice of the alpha potential function, it just you choose this potential function to zero. Then in this case, as long as your value function is uniformly bounded, then you can at least say alpha is a finite. Of course, this is a very poor choice of alpha, but at least this. Poor choice of alpha, but at least this framework allows you to include many general dynamic games. And as you can imagine, that basically, if again, it's an alpha potential game, and if you minimize this alpha, if you find the minimizer of this corresponding alpha potential function, then this minimizer will be an alpha Nash equilibrium of the original game. Okay? So the key message here is that you don't need to apply this point to solve a Nash equilibrium. As long as you are able to. As long as you are able to find an alpha potential function, finding the mean Nash equilibrium or the cross-mid-Nash equilibrium reduces the optimization problem over alphas. Of course, then, what we are trying to do is that we are trying to push further. We are going to develop an analytical framework so that given a reasonably good game, we are able to construct the corresponding alpha potential function because this is the first step to minimize it. We are going to quantify what is the corresponding alpha in terms of the game structure, for example, the integral. Game structure, for example, the interaction of the players, the number of players, and then a homogeneity of the players. And we are going to derive the technique to minimize the corresponding potential force. So, of course, this cannot be done from the general game. In order to derive an analytical framework, I'm going to introduce, I'm going to have some requirement. So, what I'm going to do is that I'm going to introduce a notion of the linear derivative. You already see that in Jackson's talk. Basically, what I say is that a function f What I say is that a function f, you can think about f function is a value function of each player, is linear differentiable with respect to player i strategy. If there exists a functional, so exist a derivative, such that basically this derived predicts what is the directional derivative, but player i unilateral change their policy. Here I only take derivative over the in terms of player, each single player's deviation, because in the alpha potential game, I only care about the Potential game, I only care about the unilateral deviation. So that's why it's not derivative over all players, it's just one player. So if you are familiar with the calculus on the probability measures, this is very similar to the linear functional derivative of probability measures. Only difference is that it relates to general convex domain where there may not be an integration. It's just an abstract linear function. So assume that this games value function is linearly differentiable, then we are able to construct a corresponding alpha potential. To construct a corresponding alpha potential function, which is given by this phi, what you try to do is essentially for each player, you compute what is the corresponding derivative. For player I, you compute a corresponding linear derivative with respect to their own strategy. You add up all the players' strategy, and then you integrate it back. So, this gives you a function which is independent of the player. And we are able to show this function is an alpha potential function. Is the alpha potential function with alpha, which is essentially bounded by the pairwise asymmetry of the second order degree. So this may not be the optimal potential function, but it is good enough. For example, if it is known that if these two, if this game has a pairwise symmetry, then this r5 will go to zero, then this is the corresponding to the optimal potential function. This is the become a potential game. And I'm going to show you some examples where actually this upper bound r phi. Where actually this upper bound of alpha gives you quite good, already gives you quite a good upper bound in terms of the gain is characteristic. So for now, this is still an abstract framework. Now I'm going to specialize this framework to a continuous time dynamic game where everyone is more familiar with. So I'm going to specify to an open loop stochastic potential, a stochastic differential game, where there is a probability space that's supporting some brownie motion. I'm going to optimize over I'm going to optimize over open loop control, which essentially is some stochastic process adapted to underlying ground emotion. Given this admissible control, the corresponding state space is given by this ID, where this essentially represents layer I's state dynamic, H I values corresponding to a controlled IC, where both the gift and the diffusion are under control, and this gift and diffusion can have general dependence over the whole population state and the whole population. And whole populations control. And then, player, I try to minimize the value function, which is just a general cost, finite time for instance cost function, kind of involving both the state and also control. So basically, it's a really general setting of all the open loops of classic control. All the coefficients are assumed to be sufficiently regular, in particular, twice continuously differentiable. So to apply the previous construction of our potential function, the first step is to compute what is the linear duration. Which is essentially saying I want to. Which is essentially saying I want to compute a value function, player i's value function, with respect to player i's uh own potential. But this is done if you are familiar with the stochastic maximum principle. Okay, what you are doing to do is that you just do a chain rule. Okay, you first differentiate the cost function over the state, and you differentiate the state with respect to player i. Okay, and this is precisely what it is going to be here. The previous alpha potential function with respect to this general linear derivative can be written as Linear derivative can be written as this more concrete form, where this is essentially involved a chain rule. You see, I differentiate the cost function with respect to state, and then this y is essentially the derivative of the state with respect to player i's action. And if you are familiar with the maximum principle of a continuing DSD, this is actually the first step where you derive the BSD. And actually, this y process is going to satisfy a linear SD. So now I have a So now I have a very concrete representation of the alpha potential function. Now I only need to do is that I need to minimize, find the minimizer of this alpha potential function. What I essentially I'm going to do is that in order to derive the corresponding alpha, I just need to minimize this alpha function over all of the miserable potential, which is adapted to the Brownie motion. But there is a slightly technical difficulty, exactly because here in this potential function, it involves this integration over R. It involves this integration over R, which is essentially given in this general definition here. So, what you can think about here is that in this potential function, there have two noise. One noise is actually the original underlying Brownian motion, where I take expectation over here. There is another noise, which is this R, which is cut as a uniform random variable. And I also take expectation over this uniform random prop, as I see this integration. But what I mean, I want to But when I want to find the admissible control, I require that its optimal control or the minimizer can only adapt to the grounding motion because this is the admissible control of the original unplayer state. So what I need to do is that in order to recover this dynamical programming principle, what we try to do is that we essentially embed this optimization problem in terms of a conditional marking plus or control problem, where essentially the state space is the conditional law, conditional on the underlying problem. Conditional law, condition on underlying brawn emotion. And then, by using, by essentially, I derive a verification principle, basically based on the solution of the corresponding HAB equation, which essentially ensures that the optimal control is only adapted to running motion. It's not depending on the realization of R, but it's only depending on the law of the R, okay, which is uniform right there. And C can you explain again how this uh unit came about? Uh integral kilobar. Yes, so and and and also in the simpler case where let's say symmetric game I will going to show you how that is the corresponding uh the symmetric game. How this integral comes about is really coming about this uh general characterization of the potential function. So essentially first in order to construct this potential function, I first compute the first Two ink uh I compute the first uh it's basically like uh like the fundamental theorem of calculus. You want to construct a potential value function, you first compute a corresponding derivative and then you integrate it back. Okay, and then the only difference here is that I need I want to have a potential function which is unified which is independent of the player. That's why I have this summation. So this potential function, if you come to the static potential game, this is exactly a potential function if you want to have a static potential game. To have a static potential name. And here we more or less extend it to a more general sense. And I will comment on this more general case if I have a more further structure of the game, what is a corresponding alpha and what is a corresponding dashboard. But the point of this is not really to study homogeneous player, because in practice it's very difficult to have homogeneous players. It's just good to kind of. Yeah, yeah, yeah. I'm going to show you more about what it looks like. So basically, we show that this corresponds. So basically it will be show that this corresponding GD equation can be solved analytically if it is a linear quadratic by some uh recursive equations. Okay. So the thir the third step, as I said, is to quantify the alpha. Okay. This is more technical because for quantifying alpha you not only deal with the first order derivative, you also need to deal with the second order derivative. So for simplicity we only consider this, we make further assumption. We are saying that here this state space is only the truth is controlled and it's controlled in a linear way. And it's controlled in a linear way. The gift time depends on the individual state xi, and the time depends on the whole population. And then here, this is just some additive noise. There's no further structure about the cost function and also the other cost function. We are able to show the corresponding phi is actually an alpha potential function with alpha given by this quantity. This quantity at this moment is a little bit abstract, but what it's essentially saying is that this constant Ly reflects This constant Ly reflects the coupling of the different layer states. Which is more precisely how this PI depends on this population state. If all the systems are decoupled, in a sense, this PI does not depend on this component. This LP is at least equal to zero. There are some, all the C1, C2, C3 are actually some constants depending only on the upper bound of the difference of a cost function. For example, if all the players Function. For example, if all the players' cost functions are the same, then basically all these at least all the things are equal to zero, that alpha is equal to zero. And then, for example, in the paper, we show that if the game has a mean field interaction, then this gain will be an alpha potential gain with alpha of the order 1 over n. And then, in order to demonstrate the power about this alpha potential gain, I'm going to specialize, I'm going to apply this framework to a particular toy example, which is a linear project. Toy example, which is a linear quadratic game on a graph. So here I can see the graph where the vertex of the graph is just a set of n players. The edge of the graph is essentially reflecting the player's influence on each other. I consider this corresponding value function where a particular player i is going to minimize this conjugate loss. What this game is essentially saying that for each player, their own state space, I satisfied. State space, I satisfy this linear tension, which I will look linearly in terms of their own state space and depending their own tensions. So the player I try to move to the corresponding target, which is di, but on the same time they want to stay close to each other, which is essentially given by this coefficient qij. In this game, we can apply the previous framework. We can quantify this alpha explicitly in terms of the number of planes. Explicitly in terms of the number of players and also in terms of the strength and the heterogeneity between interaction. I want to emphasize here: this Bi and sigma i, they are all independent. Okay, they are all different. There is no homogeneous assumption between all these coefficients. And this gamma i can also be different. There is no homogeneous assumption. So, for example, as I already said, that if this game is pairwise symmetry, in a sense, this qij is equal to qj. This Qij is equal to Qji, so the different players are pairwise interaction is the same. Then this alpha is equal to zero. So the game is actually a potential game. You don't need to, in order to find Nazi equilibrium, you only minimize this corresponding potential function. It gives you the exact Nazi equilibrium. There is no fixed point. So how about the gamma i? The gamma i does not matter. Because what you care about is actually the interaction. So basically the alpha potential cares about it. The alpha potential game cares about how the different players interaction. So here this gamma i is, there is no interaction, it only depends on their own state. So this does not matter. But I think for periods nice, we talk alphabet for alpha, which also the parameter, well the coefficients depend on the difference between L i and J, right? Yes, yes, but they are this depend on the second order derivative. So if you apply this one, this one vanishes. Yes. So if it's not a a symmetric game, but it's a symmetric game, Game, but it's an asymmetric game, then this alpha can be quantified in terms of the number of players, and in particular, it's more precisely quantified by the strength of the interaction. For example, if you assume that they are this weight, they are different, but they decay in terms of the distance of the player, which happens a lot if you do an automatic driving. You only care about the cars nearby, but you really don't care about the people really far away. And in this case, you can show this alpha is the case in terms of its bounded. In terms of its bound by y over alpha. If you minimize the corresponding potential function, it gives you an axial Nash equilibrium, where this axial behaves to be one over there. Again, there is no homogeneous assumption. And if you have a slower decay in terms of the alpha, for example, if it's only a power law decay, then you get that this alpha will decay slower because there is a stronger interaction between different layers. And then if you apply the corresponding, you let out the corresponding potential function, you apply the Potential function, you apply the stochastic control technique I introduced. You can actually solve the corresponding R-final equilibrium exactly. It's a linear quadratic game, so that's why it is a linear feedback strategy. But here, this is a feedback strategy only inside this space. It's not only the state space of your original state space, but also involving the sensitivity. Because for the original control problem, the state, the alcohol function is only a linear derivative of the state space. So basically, you need to Of the state space. So basically, you need to leave the state space a little bit. But it's still a finite dimensional only. And you can solve the corresponding only, you get the corresponding of the national E. So to summarize, so in today's talk I essentially propose the alpha potential game framework for n-player dynamic games. The task is that basically this framework allows you to reduce finding extra Nash equilibrium to minimize the corresponding alpha potential function. So there's no fixed point, there's no homogeneity. So, there's no fixed point, there's no homogeneity assumption. Stochastic gain reduced to a stochastic control code. And you can already see that in the Stevens call, basically the corresponding Nash BP view can be fine by minimizing a central planner's call. So, in that case, it's a particular potential value. So, we characterize this alpha analytically, and then we characterize both the alpha potential function and also the corresponding alpha using the linear derivative of the value function. Of the value function. For stochastic differential gain, the alpha can be, alpha potential, alpha nice equilibrium can be analyzed using a controlled machine blast of framework. And then in the explicit solution for the linear quadratic case. And then you can explicitly quantify the alpha in terms of number of players, the stress, and the degree of heterogeneity of the players. But for the detail, you can see this preprint, which has more examples. Thanks, Karatochi. Actually two questions. The first might be a bit naive. What's the point of doing this for the linear quadratic? You can just solve it. Is it just to get something simpler, which resembles a bit the idea of mean field games and get a one over n equilibrium? And the second question: this general bound for your alpha. Everybody is one over n terms, but There were these one over n terms, but there was also the C by the first term. So your alpha in general is of the order number of players minus one? No. Okay, so it's okay. Maybe I first addressed the first point. Okay. So this one. Yes, I first addressed the first point. Why was the point of doing a linear quadratic? So first this theory, for example, at this point, there's no requirement for linear quadratic. And I only do linear quadratic because I want to really illustrate the whole framework. The advantage of doing even for linear quadratic case. Of doing even for linear quadratic case, there is an advantage in the sense. For linear quadratic, for general employee gain, no homogeneous assumption, then you have a coupled Ricardi equation. The well-posedness of this, the idolatries of this Ricardi equation also requires some fixed point. Because it's a general thing. But therefore, here, the alpha is really become solving again, becomes solving a control problem. So there's no fixed point required. This is the advantage. There's only one value function in terms of repartee. Y value function in terms of Ricardi function. Yeah, yeah, that's simpler, right? So it's a more efficient way to compute Nash. In terms of here, okay. So this C depends on if we make some assumption on the uniform. So at this moment, there is no structural assumption on the cost. First blue C. Yes, this one. Yeah, all these things. They are not, in general, the alpha does not decay in terms of y over n. Because at this moment, there is no assumption on the clause function alpha that. The sumptuous on the cross function i5 and gi. It's very simple. Yes. It's nothing the case, to the contrary. We are summing basically over the set of planes, right? Yes. Then there is this constant C, which is of the order 1. Yes, no, no, that's not true. Not true? No. Because this C depends on the derivative of Fi. If you make further, that's why, okay, if you make further structural assumptions of Fi, you are able, this Ci actually becomes. You are able, this CI actually can decay. But if not, it's only basically, in general, if you have a general gain, it may not be equal to a control problem. That's true. Yes. But if you make further assumptions, this is the point of the potential function, the potential gain, right? You don't consider a general unplayer game, which is very difficult. It's maybe intractable to find large behavior. You only concentrate a Quite much difficult. You only concentrate on something which can be solved. And here, this is more or less what we are trying to do. We didn't claim that for general game, this alpha is going to zero. I also have two, if we do two. One is a more general question, and the other one is more technical. So the first one is: using domain about the convergence results, you can start with Um actually give me um can I go back to solving the uh I think and and the other question is um I I maybe I missed it but I think you said that the controls are um close group? Open group. Open group. But you just get a close uh a feedback representation of the auto group not you get the equation. So how about the first question, no in general uh it's similar to In general, it's similar to what Stephen already had in the first in today's talk. For the game, you can have multiple Nash equilibrium. The corresponding potential function only gives you one equilibrium. So in general, it does not mean that if you have, you give me arbitrary Nash equilibrium of the n-player game, this is equal to a minimizer of the potential function. It may be if you have unique Nash equilibrium, then yes, but in general, it's only not. It only gives you one particular Nash equilibrium, which is more or less cooperative in sense because it's a Cooperative insights because it's minimizing a common function abstract function. But yeah, this is very interesting. Thank you. Right now, the framework is that you construct a current function and then you characterize the corresponding alpha, right? So can you do it in a reverse order? Let's say I give you a particular alpha and you'll be able to construct some potential function correspondingly. I think in general it depends on how large this alpha is. this alpha is like if you said alpha is equal to zero then small number like they're looking for smaller equilibrium so that's what it would be like small yeah i think in general it's it's challenging because here there's nothing to do with the optimality about alpha right i think in the when actually this alpha potential game was first proposed by Xin and Su's previous work on discrete markup game there they can find you can essentially study a minimax game to find optimal alpha what you try to do is that essentially you What you try to do is that essentially you fix the potential function, you find a corresponding alpha, and you optimize overall potential function. This essentially can be formulated in terms of semi-infinite linear programming, and then you can compute what is the corresponding optimal alpha for a given game. But if you have a smaller alpha than this optimal alpha star, then this is not in general not able to find the potential function. So there they're searching a lot of potential functions. Function parameters. Yes, yes, yes, yes, yes. They are searching for the optimal potential function. But for that, because everything is finite, so that's why you can still implement everything for the general dynamic. And maybe another one is, you can also do it for the closed loop, but actually, we have all the things can be extended to the closed loop. There are two technical difficulties. The first is that. is that so here this is the potential you are doing well the stat is that you need to minimize the potential function over all the divisible strategies if it's a closed loop you need to minimize the potential function over all closed loop functions right they are deterministic functions that's why it can be very difficult the first the second is it can show the alpha is difficult because intuitively this alpha depends on the stress of the interaction between different players so if you do a closed look the close the feedback structure also introduce another coupling between Introduce another coupling between the state. So that makes the controlling alpha small geometric. But if you go to the previous version of this paper, then you're going to see a closed loop. And you also applied a closed-loop remark to the linear path. No, we didn't at the end. We are now working on the extension of this to the closed loop things. So I wonder if they will give you the same solution for the LQ problem. Probably not, because you know that in the linear, even for the linear contract, so there is a distinction between open-loop Nashville and closed loop Nashville. Even for the LQ case, they are. Those two actually even for the LQ case, they are not so I guess they are I my first intuition they are not the same, but it works well, I suppose. I think we have done and then we can come back around one thirty. Yeah, so I'll buy. So we'll buy some buttons. Yeah, so it's part of the control.  Yeah, like um Yeah, like um as you can see creation of PR time, time editor, please are Not only try to talk in fact, I also realization for the real world. I also spared you for a while. So they adapt to it. Well, we might not be having your Rajana. No, she's more about our words. What kind of what one of Ralph?