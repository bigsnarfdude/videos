Talk is by Vladimir Koropin. He's also going to be on the panel. So, the incredible Vladimir Koropin. And I'm very happy to actually see his talk. He had a number of different results he could present today, but he decided to talk on quantum search and noisy intermediate scale quantum devices. And Vladimir will also be on the panel, which I invite all of you to attend. It's going to be in four hours. Four hours. Okay. Yeah. So, Vladimir, this is not full screen if you want to make it full screen, although it's perfectly fine if you're comfortable like this. I'm trying, you know. Yeah, so if you go up to view and you press. It is full screen right now. Yeah, it's fine. More or less full screen, yes. And then I can switch from one to another. So, yes, full screen. Sure. Full screen. Sure. Done. All right. Well, thank you so much for agreeing to talk. So take it away. Thank you very much for inviting me. I'll be talking today about different complexity measures, complexity measures. So that's a formal plan of my lecture. I'll be talking about Grover algorithm, quantum search. Of course, everybody knows Grover algorithm. Nevertheless, this will be part of Nevertheless, this will be part of my introduction. I will introduce notation. Everybody uses this notation, it's also the point of view, our point of view to Grover. Then I will argue that oracular measure of complexity is outdated. And better measures of entanglement specifically I will concentrate on depth. So I'll try to optimize quantum search from the depth, reduce the depth. And then I argue this will Argue this will perform better on the real devices. Conclusion outlook. So, here, here is a list of devices which on which. Vladimir, I believe you might be switching slides, but we only see are you still on your I guess you're not on your title slide anymore, right? I'm not on my title slide anymore. I'm on the second outline, right? So, we see actually only your title slide. So, if you want to maybe stop sharing to go. Want to maybe stop sharing to go to the screen page and reshare again? Yes, yes, yes. Okay, wait, then it will resume share. Okay, so I returned back to the first and then now this is the second. It didn't, you didn't stop, so you want to stop, you need to stop sharing because we're still seeing the title page. Stop sharing. Title page. Stop share. Stop share. New share. Okay, this is weird. It didn't go away. I stopped video probably. Stop video. Wait, wait, wait, wait, wait, wait, wait. New shared chat annotate. So I think we had something like this when it was full screen. Ah, there, good. Okay. Uh ah there. Good. Okay. You see the sec the second um transparency, right? Yes. With the outline. Yes, now we do. So uh this should be the third one, another one. Limitation, right? You see the limitation? Yes, we do. It's great. Okay, so this is the beginning again. So the second, let's go through outline again. So outline. So I'll start with Grover. We went through this already. So the third transparency, you see the third transparency? So, the third transparency, you see the third transparency? NISC error. Okay, okay, okay. So, here there is a list of machines which we use to run our modified quantum search. Actually, we used all the machines which have a 5QB database. So, I mean, it's publicly available. And then, so our best, we run quantum search. It's modified Grover. It's modified Grover. Modified Grover on the machines with I will start with three qubits database four and then five. So this is specific IBM with different city names, Regeti, Honeywell, INQ. So I am just moving to the next. So limitation. You see limitation, right? Yes, everything is good, yes. Okay, okay. So noise intermediate quant uh quant no NISC computers. NISC computers already have enough qubits. I mean, right now people are talking about 100 qubits. The problem with the depth, I mean the gates, two qubit gates. We want to reformulate the algorithm to reduce depths. Why we want to do that? We want to do that because we want to reduce the number of two-qubit gates used by the algorithm. Right now, precision of two-qubit gates, like one-tenth of the Two qubit gates like one tenth of a percent. So if I have algorithm with thousand two qubit gates, this will be a problem already. On top of this, two qubit gates has a fixed time to be executed. Like two qubit gates take several nanoseconds. And if I use less two qubit gates, then the algorithm will go fast in real time. So that's the motivation. So I will kind of abandon this idea to reduce the number of queries to the Oracle. Reduce the number of queries to the Oracle. Instead, I will use other, there is also other measures of complexity, but I will concentrate on depth. Alright, alright, so I come. This is just actual beginning of the lecture, the introduction. The introduction, just I'm repeating Grover, which everybody knows, but I probably have some kind of a different notation. So, important part: first of all, I will assume the database have only one target item. Target item also in the Legion. Target item also in the literature called marked item, sometimes called solution. Of course, the important part of the equipment of the database is one-way function. So, this is a function which can be evaluated polynomially fast and then but inverse is exponentially slow. So, I mean this often used in cryptography, but probably I see that all my listeners are expert in quantum computation, so everybody understands that. So, why what way? That. Why do we function somehow in physics departments where I work? It's difficult for physicists to grasp, but computer scientists understand that. So let me move to the next. Yes. Okay, Grover search. I just want to repeat this. So I start with 000, apply Hadamar and get the total average. This S is the S sub n. N is number of qubits in the database. Qubits in the database. There will be other S later, which is equal amplitude. Each and every basis vector. Basis vector corresponds to one of the items in the database. Each and every basis vector has the same amplitude. Okay, okay, so two operators. First operator queried to the oracle. In our days, it's called phase kickback. It's identity minus two projector on that target item. Projector on that target item which we tried to find. The second operation, diffusion operator, I will call it a global diffusion operator because later there will be another diffusion operator which I will call local. So diffusion operator, this is a reflection in the total average in this SN. So two projector on this SN total average minus identity. And then global grower operator is a product of these two, query historical and reflection in. Oracle and reflection in the average diffusion operator. I call it global because later I will have another Grover operator which will be local. Okay, so of course in the textbooks we all learn the Grover algorithm is optimal in depth, minimal, is optimal in numbers of queries to the Oracle. But I try to argue that queries to the Oracle are outdated and not Outdated and non-essential for NISC computers. So we just wanted to modify, and I will explain modification of quantum search. Okay, so this is our modification of quantum search. Sometimes we call it partial search. So new diffusion operator, local diffusion operator. So what we do, we divide the database into several blocks of the equal size. So the number of the blocks will be 2 to the n minus. Be 2 to the n minus m. So n is a total number of the qubits in the whole database, and m will be the number of the qubits in each and every block. All blocks are equal size. So the local diffusion, so we'll make this refraction in the average inside of each and every block. But we do this simultaneously for all blocks. That's a new operation. So local Grover operator. It's this local diffusion. Local diffusion, second factor and the first factor, query to the oracle. That's the same query to the oracle. We did not change query to the oracle or phase kickback. This operator, local query to the operator, GM does, it's in the simplest case. This is element of O3 group. I will have the picture and explanation. And GN, which is normal standard grower, also O3 group. So they do not commute with between. They do not commute between one another. And this will present some problems in optimization. So, in the simplest case, we can represent both of them as three by three matrices, not come using, and then we'll be facing with the problem of ordering them. So, in principle, I mean, later when we shall have access to the larger databases, we can change M, the size of the block during execution of. During execution of the search operator. This will be postponed to the future because currently the database is small enough, so we fix M and then I will explain which the M for. So this circuit. To the left, I have standard normal Grover operator. This is UT query to the Oracle or FaceKickback. And then this is This is a diffusion operator, but it's written in terms of two qubit gates. This X negation and H is Hadamar. So I do this, well, the circuits say. And that's a big Toffoli gate, of course. But Tophali gate is represented as a bunch of two qubit gates. Each of them are noisy and each of them have some fixed time for execution. To the right, I have a local diffusion operator. Local diffusion operator to the left has the same. To the left has the same phase kickback, and then this is totally performed only on the block, so it's kind of a smaller size. Okay, so to the left, that's a picture of the standard Grover algorithm. From the textbooks, Nielsen Chuang, we know that the Grover can be represented as a rotation in two-dimensional plane, like O2 operation. So that's a plane which is spent by two vectors. Spent by two vectors, the target item, that's the one which we're trying to find, and then SM, this total average of all basis vectors. And then the grower gradually rotates from this average towards the target item. So this is rotation O2 group. In our case, with a local search, it's more complicated. Now, instead of O2, we have O3, which is non-commutative group. By now, there is a Group. By now, there is three vectors, three-dimensional space spanned by three vectors, the target item. Then we have a total average of all items in the whole database. And also we have another average which is average inside of every block. So three vectors. And then their linear combination span three-dimensional space. And then the local search is rotation in this three-dimensional space, which is In this three-dimensional space, which is O3-group. As I said, in principle, in the future, we can use blocks of the different sizes or the different stages of algorithms. Then this will be five or seven high groups. Okay, so the strategies which we used in this search. Later, I will present the result on the specific machines, but first of all, the strategies. We have several different strategies. Several different strategies. One of them is the following. So T is a total exact address of the target item. So this is a sequence of bits, right? So we can separate this into T1 prime, which is several first bits in the exact address, which we can guess out of blip. And then the rest is our blog. And in the blog, we shall build this applying. Build this applying Hadamargate, we should be this total average inside of the block and apply this local search. So, success probability will be product of probabilities Pm is coming out of Graver and the left one is just probability of this random guess. So, theta B, well, as you know, in Grover algorithm, the angle is important. So, by now, we got new angle because so. Got new angle because so B will be number of items in the block in each and every block, and the corresponding angle defined by this equation: sine of the angle is equal to 1 divided by square root of B, where B is number of items in the block. So let's see, later I will apply this for the search for some specific machine and we'll see we shall compare this with other We shall compare this with other approaches to search and see which one works better. So, this is just our generic search. So, I generalize Grover a little bit. So, in principle, I can start my search applying GM. This is local search several times. Then, global, local. They don't come used. So, what's the order? Well, we made some numeric, so we have an idea about the ordering. We mean, we just try to. Um, I mean, we're just writing the paper about this right now, but it's not discussed much during this lecture. But in principle, we have a problem because usually in theoretical physics, all everything reduced to Lie algebra of a group, and this is like a Lie group, meaning that those operators seriously don't commute. So, well, the optimization is not that easy. So, if I apply this to the initial state, which is total average of the total data. State, which is total average of the total database, project this on the target items, square magnitude probability, which I'll be talking. I'll be plotting this for the different devices. And then we have a expected depth of the circuit, which is a depth of this, depth of this, divided by this probability. So this is expected depth, and then also will be plotted. And of course, depth optimization, we want to minimize the depths. To minimize the depths, we want to use smaller depths, so we want to use less gates because they're noisier. Okay, so strategy two. So, the first strategy was, I mean, you remember, include, it was like a hybrid. And here, I can subdivide the address, the T, the sequence of the bits, in the first T1 bits and the second T2 bits, and then I can apply growth for T1 and apply. Grove 41 and apply Grow 42. Often is usually interrupted by the measurement in the middle. So the depth is the depth of the first plus depth of the second step divided by the product of probabilities. That's expected depth and we want to minimize the depth because then the algorithm will be less vulnerable to noise. Okay, so now some theoretical device because before we go Device because before we go to the actual machines, we just want to devise some theoretical device. There is an important parameter which, I mean, in our point of view, control everything. We denoted by alpha. So this parameter is actually... I mean, I see there's several charts. I'm not reading the charts, so maybe I should look into this. Feel free. It's a great point, Vladimir. So maybe we can pause for a second. Vladimir, so maybe we can pause for a second and see if anybody has a quick question for you. Is that okay? Yes, beautiful, of course. So, if anybody has a question about the notation, or my pronunciation of English language. Okay, yeah, feel free to ask. Just a quick specifying question: When you have this M, where you subdivide on these blocks, the size of these blocks are they like order one? Order one, uh, well, you know what? Later, it first of all, later it will be sub-specified. So, like, typically, one of the examples which will pop up, which will appear later, will be the following. So, we shall have a database of the size four qubits, and the block will be two qubits. So, typical size of the block is half of the database. Okay, okay, I see, I see. But, but I think about sc scaling even anesthetics. You know what? You know what? You know the M, the size of the block, we choose from optimization. And optimization is just we want to reduce the depths by any price, including size of the block. So far, I expect, well, first, I mean, like, I was trained as a mathematician a long time ago. So, formal mathematical answers, I don't know. But then I can guess. I can guess that. Guess. I can guess that maybe M will be large. I mean, maybe my guess will be that M is a fraction of N. So it will be like quarter of the total size of the half, something like this. But I'm guessing. I mean, all of this should be done with these non-communicating operators. And currently, formally, I can't answer the question, but I can guess. Okay, yeah, thanks, thanks. We'll see later. Thank you. More questions? Thank you. More questions? Comments? Anyone else? Maybe cynical remarks. Remis, I continue? Yes, please go ahead. It seems nobody else has any questions. Okay. I mean, when I'm talking, I'm also reading my check. So, I mean, I don't have time to look into chat. So, if Ramis, if something will appear important in the chat. Ramis, if something will appear important in the chat, please interrupt me. Please don't hesitate to interrupt, okay? Thank you, thank you. I'm sorry that misses the chat because I'm just reading my own transparencies, okay? Okay, so theoretical tool, before we go. So, in our opinion, there is an important parameter which controls everything. The important parameter we denote by alpha. So, what's alpha? Alpha is the depth of phase kickback of this U2 of query to the Oracle divided by the Query to the oracle divided by the depth of the diffusion. So each of them is phase kickback and diffusion is in practical, in practicality, is a sequence of two qubit gates. And then one of them can have a... There's different implementations of phase kickback and diffusion operator. And then this ratio, we argue, is critical parameter which control everything. So theoretically, there is also, there will be some kind of phase transitions with respect. Kind of phase transitions with respect to this alpha. So, first critical alpha, the definition of the critical alpha. So, first of all, when alpha is very large, then Grover is optimal in depth. But on the other hand, when alpha goes down, our partial search will become, will have less depth. And then the critical alpha is that the one. Critical alpha is that the one of the change. So, and then there is like our different estimates for critical alpha, but that's a part of the theoretical work which will be published somewhere. But I just want to emphasize. Sorry, can I interrupt you? Because when you say critical, what are the two phases which are separated? Well, the two phases are the following. So, for large alpha, Grover should be used. Grover should be used because it's optimal in depth and there is no way to improve it. When alpha is smaller than alpha C, then our partial search has lower depth and it should be used for the search. Okay. All right. So later I will have a more dramatical picture of the phase transition, so we shall discuss this again. Okay, so let me let me okay, okay, so depth optimized quantum. So, depth optimized quantum search algorithm. We test different realizations, three and four qubits. In this lecture, we shall go only to four qubits. Five qubits is still in doing. We're just preparing the next paper about five qubits. Here in this lecture, we'll be the database of the size of three qubits and four qubits. So, the main, well, if I just try the circuit how to make Grover search, the main problem is to. Research the main like problem is tofully gate because it has a lot of you know two-qubit gates. So that's our strategy. We use a toy toy oracle. Well, the second is important because when I do the search, I mean practicality shows that doing the search, it's I mean first we dedicate, we just know what's the target item and then we force the algorithm to find it. It's a bad idea to It's a bad idea to designate 000 for the target bait because it's usually the ground state. So the machine goes to the 00 by itself without any algorithm because of the phasing. So we choose the target item randomly. It's like 0101. It's important to do this. Otherwise, you know, there's... Okay, you understand. And then there's like 30 trials in this number of shots. This is like standardization of the machines which we used. Okay, there's notations. Notations are important. There is notations. Notations are important for the further graphs. The next transparency, we shall see the graphs, how different machines perform. So, this is notation to understand those graphs which are about to come, like the next transparencies approximately. So, in the picture, I will have D3M3. This is for the 3-qubit. We shall start with the 3-qubit. 3-qubit, this is standard growth. This is like diffusion. This is like diffusion on the three qubits and the measurement of the three qubits. The other notation, D2M3, this is diffusion of the two qubits. That's a block, right? That's a block of the database and measurement of all three. We shall have the fourth notation. I'm just preparing the notations to understand the pictures of the next transparency. G1, D2, M2. So G1 is a random guess of one of the QBs, I just randomly guess. I just randomly guess. And then diffusion on the two qubits and the measurement of the two qubits. And then the last one, which will appear, is diffusion on the two qubits, measurement of one of them, then diffusion of two other qubits and measurement of the two qubits. So this is like our strategies and we just run all of them on the machine and see which one perform better. So let's go to the next transparency. So metrics. Metrics which we use. So what I'm trying to emphasize. Use so, what I'm trying to emphasize that the algorithms are complicated things, and it is naive to use only one complexity measure, this oracular measure on top of this is like old, outdated measure. So, we can use different measures, metrics, measures of probability. So, of course, the running time is important, but I mean, not number of queries, the oracles, actual time, which is on the Oracle's actual time, which is on the clock. That's an important measure. Then, success probability, of course, because sometimes I reduce the noise, but success probability also goes down. So, I mean, I should avoid this. That's the reason I divide by probability to get expected depth. This is like depth divided by success probability. There is also other measures. So, let me, before I go into the specific machine, what are other measures? Selexivity. What's the selexivity? Selexivity is the measure which we Selectivity is the measure which we use in some of our publications. That's, you know, I use my search, and search is amplitude amplification, meaning that at the result of application of all my operation, the amplitude of the target item increases, but the others, I want them to go down, but because of the noise, I'm not always successful. So, selectivity is the ratio of the amplitude of the target items to the amplitude of all others. That's a selectivity. So, in principle, Selectivity. So, in principle, we use this not in this lecture but in some of our publications. So, let's have a look. So, that's a first experimental result. First experimental results. So, let's consider small database, three qubit, three qubit search. So, the upper one is a grower with one iteration, one iteration of grower. So, this diamond is a theoretical prediction. So, probabilities about, I mean, I mean 0707. That's a theoretical. On the other hand, because of the noise, it was run on VIGO, I think VIGO, that is the IBM machine. The probability because of the noise, the actual probability is very low. This is just actual probability. On the VIGO, it's it's like VIGO. This is actual probability. So Va Vigo is noise is so, first of all, theory differs from experiment because of the noise. Second of all, it's Because of the noise, second of all, it's the actual probability is pretty low, and actually, it's as bad as random, random pick. So, now the Grover with two iterations: I do diffusion, diffusion, and then measure three. So, theoretically, probability should be larger. Noise destroyed almost everything, and then I got this what comes out of the machine: is this probability, which is like about one-half, a little better. One half a little better. Well, but I try harder because I mean I made two iterations of grower. Now, the last one. The last one is what are the arrow bars here? Did you perform this experiment repeatedly? Yes, yes, yes, many times. It's like thousands. That's kind of sort of one sigma or of the Gaussian distribution which you get? I mean, we didn't feel this to Gaussian distribution, just error bars. Error bars. Just a single standard deviation, error bar. So the best one is ours, which is below. So this is diffusion on three qubits. This is the total size of the database. Then we measure one. Then we make diffusion on two other in the block. Measure two. So theoretically it's not so good. It's weaker. Theoretically it's weaker than the total grower. But experimentally it's better. But experimentally, it's better. Experimentally, what comes is high probability. So let's see to the reason. To the right, I had the depth. So the depth also theoretical depth and the depth, well, the green is theoretical, and this is depth on Vigo. I mean, they tell you, the Martian tell you. So the original Grover has very high depth, meaning that a lot of gates, of two-qubit gates, and lot. Gates of two qubit gates and a lot of noise, which is bad. And then, but our partial search has essentially lower depths, so a smaller amount of two qubit gauge and then higher probability. The right graph is supposed to explain the left one. So this was three qubit. So I want to go to the next transparency with the four qubit. I mean, Ramis, unless there are questions, comments, or remarks. No, it's good. Um no, it's good. I just want to exercise that oops. Okay, uh wait. Forkit. For qubit. Okay, four qubit. So this is theoretical. So this is Grover with one iteration. Grover with one iteration. So theoretical probability should be around one half because of the noise, the depths in the right hand side, as you see. So the depths decimate everything and then the actual probability is very low. Probability is very low. I mean, if I do two Grover iterations, then the probability theoretically should be close to one. I mean, but it's also increasing the noise because more gates used. So, what kind of machine? As bad as the previous one. It's really not good. So, the lowest. So, this is our partial search. So, the lowest is we make diffuse. is we make diffusion of two qubits, then we measure two qubits, then make diffusion of two other qubits and measure two of them. So theoretically it's supposed to be low probability than Grover, but in practice it's better. It's better, it's high probability. And the right hand side as usual is explanation because our lowest has the smallest depth, smaller depth, and the Grover has very large depth. So um well it's just the argument, so let's reduce the depths, so then there will be less noise. Then there will be less noise. So I just okay, and then this is just another phase transition. So this is something else. So the previous two was the end of the experiment. So we make a search on three qubits, four qubits. Right now we work on five qubits. The paper is in the preparation. It's not published yet and it's not in this lecture. This transparency is something else. So what we're doing here, we're just considering the Just considering the ratio of the probability which come out of the device divided by theoretical probability, supposed to be close to one, I mean in the ideal world, but because of the noise it's smaller. So this is R capital. So this is this R capital, this ratio, as a function of number to two qubit gates. So if I have a below 20 two qubit gates, the ratio is not so bad. 08, maybe, but at around like 22. But at around like 22, there is like phase transition, and then this ratio goes down to zero two, which is, I mean, the reality is much much smaller than theory. So let me go to the next one. Oops. So set up for the benchmarks. So we run with our quantum search on the different machines, IBMQ, IMQ. IBMQ, INQ and Honeywell they have different hardware that's important because IBM have superconducting and INQ and Honeywell they have trapped ions and we argue that trapped ions essentially are less noisy than superconducting. Superconducting is noisy, it's like many many atoms and molecules involved but trapped ions. I just show some graphs to argue this. So this is a toy oracle. Toy Oracle see it's not 0-0. So Oracle, see, it's not 0, 0. 0, 0, 0, all 0s are bad because the machine will go down to this state by itself. But if I choose this randomly, then this is a better representation. So the circuit runs this much shorts. So it's more than thousands. Trade-off between the circuit, let's go to the next one. Okay, okay. So success probability on different machines. Now, the previous we compared the software. Now we're comparing the hardware. So the hardware. Comparing the hardware. So the hardware, so this is success probability. So success probability on the blue Lagos. Lagos is not so good. So the best is Honeywell. Honeywell is super duper good. And then this is iron traps, trapped ions. So, well, and you have, I sent the file to the organizing committees. So if you're interested, you can meditate. So expected depth. So expected depth. Expected depths, we want depths to be low. So large depth is bad. So Lagos has large depths, Mumbai has large depths, which is not good. But Honeywell has small depths. That's a reason of high probability. That's a comparison of the hardware. So expected depths, somewhat similar. So expected depth is depth divided by the probability. So Lagos is high, Mumbai is high. Is high. Ion Q is also high. Also, this is surprising to us because Iron Q also use trapped ions, but Honeywell much lower expected depths. I guess they have different hardware, different setup, different makers. Okay, so we try to simulate the noise by this depolarizing channel. So sometimes we can choose epsilon to feed the noise more or less. The noise, more or less, it's not perfect, but still useful. So, right now, we're coming to the end of my lecture, and then the outline or the morale. So, the Grover algorithm was optimal in the books. It proved that the Grover is optimal in the number of queries to the oracle. And my point of view, this is outdated measure. So, Grover is not optimal in depth. In depths. So, in the real devices, it can be improved on the algorithmic level. So, current quantum processor can barely run five qubits search. Five qubits, we're done by now. So, I mean, there's two different bottom line consists of two different items. First of all, Grover can be improved. Grover can be improved, but then we will be facing complicated mathematical problems. Facing complicated mathematical problems because these different operations, local and global, they don't commute right now. There's a different non-commuting elements of the group of O3, and then we have to reorder, we can reorder them and optimize the depth, which is not a simple mathematical problem. And then, of course, the hardware, so somehow we like a lot Honeywell. That's a list of publications. We publish some papers in field. Some papers in Ph A, Quantum Information Processing, the last one of the archive. This is actually the end of my lecture. So maybe questions or I don't know, comments or something. I'm done. Thank you. Thank you for this talk. Yeah, we have plenty of time for no, thank you so much. We have plenty of question, plenty of time for questions. So Jeffrey, Jeff Shanker has a question. Yeah, so I'm curious. Yeah, so I'm curious. So you mentioned very quickly sort of one model of the noise using the completely depolarizing channel. So I wondered, could you say a little bit more about that and if there are more sort of noise be characterized in more detail than that? Well, I mean the no well the actual noise The actual noise is the curve. The noise depends. Well, I have a it's in the well, the curves I had in the original version of my lecture, but I put this percentage mark in the LaTeX file, so right now it will take too long time to remove this percentage. So, the meaning the noise is the curve which depends on the number. Which depends on the number of the gates. And this depolarizing channel is a horizontal straight line. So it gives some kind of average of the noise. It's not perfect. It's in one of the papers which I gave to you. So it's useful, but it's not perfect. So I don't know, the number, the dependence on the depth is missing in this model. So I mean, I can remove this sent to you. Sent to you the better version of the leg sheets also in the published version of the paper. Yes. All right, thank you. Of course. Thank you. So next question is by Bruno Nehtegale. Hi. Hi, Vadmi. I mean, nice talk. So maybe this my question is related to the previous one. So do you see any way Do you see any way of extrapolating what you've done and tested on these tiny devices? I don't know whether it's reasonable to call them MISC devices. I mean, four qubits, five qubits. But you have a in part some mathematical model, including the noise, do you see a way of extrapolating and then sort of predicting what you could do with a 100 or 500 qubit device and what the best algorithms would be? What the best algorithms would be in that case? Well, this NISC was introduced by John Presky, and what he has in mind was exactly these small quantum devices which are currently available to public. Why would you call intermediate scale to be G4 or 5 qubits? I don't believe you. I mean, you don't believe Don Prescal, not me, because I repeat after him. This is not my opinion. This is citation. Okay? Oh, no, but I think he was talking about the next deck. Talking about the next five years or the next decade when he intended this story. Regrettably, we have different opinions on the subject. But anyway, I think it's kind of interesting to know whether you see a way of extrapolating. Well, theoretical, theoretically, on this partial search, I work for many years. I have tens of publications, and then the partial search, suppose from a mathematical point of view, as on the piece of the paper, not on the specific device, a partial search, which will work better for like. Search which will work better for larger databases. Asymptotically, it works better. As for the current size of the device, there's 100 qubits is the best, but it's not even publicly available. I mean, theoretically, I publish like tens of papers, and then theoretically, it works better for asymptotically for large database. Partial database. But partial search meaning that I separate the database into the sequence into several blocks of the equal size. Blocks of the equal size. It's on my web page, and the first paper about these partials which I wrote was with Lov Grover. It was actually his idea, Grover's idea. Okay. Sure. Thank you. Of course. Welcome. Thank you for the question and thanks for the answer. So next question comes from Japan. Ryuheimori, please. I'm very interested in the optimization problem. So can you please explain the detail? Can you please explain the detail of the optimization problem on the O3? Yeah, optimization is a headache, I'll tell you. So I'll tell you something which is not published yet. But I mean, we finishing the paper and then, okay, so the first line, you see the strategy too, right? Yes. Okay, okay. So here I have two different Okay, so here I have two different elements of O3 group. I mean, I can write this as explicit matrices to by two. G sub n is the total Grover iteration. Grover is remember diffusion and oracle, product of them. And local is local diffusion and oracle. So they both can be written as a very explicit matrices 3 by 3, don't commute. So working with these small database sizes, we arrive to the conclusion. We arrive to the conclusion so we can cook up an element which is like one global in the middle and then two locals on the side. So it will be local, global, local. And this is the unit. This is the unit. We denote this by J tilde. And then the optimal search can be constructed at the powers of this unit. You understanding what I'm saying? You understand what I'm saying? Here, Here Gn and Gm are rotation in the three-dimensional how to optimize this product loss it's not come using so it will be I mean we're finishing the paper within the months it will be published but we think that it can be solved I mean I don't have an analytical problem proof the one which will certify Professor Bruno Naktihale but what came out of numerics is Came out of numerics is that we can cook one of them, the specific combination of local, global, and local, like local, global, local, and that's a unit, G tilde. And then the optimal for many, many operation should be integer power of the G tilde. That's optimization, which we're guessing. We guess, I mean, conjecture. You understand, right? I get that. Oh, yeah. Okay, thank you for. Okay, thank you for the question. Really might be able to prove it. That will be great. That will be great. Also, my students, I have students in quantum information that told me the proof is the subject of 19th century. Right now, you make numerics, make conjectures. And the computer knows. I hear it. That's a joke. That's a joke. That's a joke. It's related to Throve HTIF algorithm. I hear. Yes, can you give this identity? Yes, can you repeat this? I didn't understand what you said. Can you repeat this, please? Sorry, and I can type area. This is exactly. That is exactly Salavikitaf. Yes. Yeah, thank you. That's the end of the book. I use Salavikita, in my mind, the end of News and Chong book. There's some appendix which I teach, usually in relation to other things. So, Salavikitape, they develop the technique in relation to other things, but they work here. That's exactly true. You're right. You're right. Very interesting. Any other questions? Or comments? So maybe, Vladimir, I can ask you a question. It may not be the best question, but I was wondering, suppose we do have fault tolerance, do you see any practical advantage in using your algorithms over standard Grover? For the perfect hardware, right? Right. Right? Right. In principle, okay, it's also with the answer to the question of Professor Bruno Nartihalle. So if I have absolutely perfect hardware and want to use this and want to use this partial search, then I asymptotically for large database, I can reduce the number of queries to the oracle. Normally, the coefficient in front, there is In front, there is number of queries to the oracle square root of n capital and capital, the total number of queries to the oracle, multiplied by pi divided by 2, right? The coefficient is for square root. So with this massaging, this partial search, we can reduce the coefficient from pi divided by 2 to pi divided by 6. So the coefficient. So a little bit, not much. So you are saying. Well I have one so so you're saying uh if we have perfect hardware and in the presence of fault tolerance there may be some advantages. A little bit. I mean it's not not even the power of number of queries oracle just coefficient in front of the square root. So in your empirical plots you showed that the success probability was a function I mean it it seemed to be a function of architecture whether you have ions or that's very true. So let me talk about this. So with IBM it's This so with IBM, it's very difficult because they have this connectivity. So, different devices like Mumbai or VIGO, VIGO is decommissioned. I know that VIGO is decommissioned, but they continuously decommission half of known to me. So, usually, this problem of connectivity, and then I should miss which of their gates to use because they're connected to different qubits. This is a problem, additional problem, which will be which. Problem which will be which we're facing. We faced this practical, so we tried to use the one which reduced the depth, but this is difficult. On the other hand, on Honeywell, these devices which we use, they have a full connectivity, like full graph. Everybody connects it to everybody. So with Honeywell, all these small devices which are currently available, which some people think non-serious, but they have full connectivity, like full graph. So, you know, well, you understand what I'm saying. I see. I'm saying. I see. So the full connectivity is going to be harder to scale, I suppose. But that's true. I guess if you could scale and we had fault tolerance, I suppose the Honeywell or fully connected would continue giving better performance. It seems like connectivity is driving it. I think, I mean, connectivity is really convenient because when we run our algorithm on the Mumbai or something, we have to try. Mumbai or something. We have to try on this set of qubits, on that set of qubits, which one is connected to which. Usually, there's a little bit of acceleration. I mean, what I can tell you that in the practical use, connectivity creates a lot of problems. And then it's better not to have this problem. I mean, of course, it's the ideal to have a full connectivity, but I agree that for large-scale database 500 qubits, it's difficult to imagine that. Imagine that to build a device with full connectivity. I mean, it's like really difficult. Well, I mean, I know these ion traps. Ion traps, this is a sequence of ions in the complicated configuration of electrical and magnetic fields with some electric fields on the sides. But in principle, this is one-dimensional structure. So, if I will be building these devices like by myself, I will have connectivity problem because there is ions which are neighbors to one another physically in the physical world. Neighbors to one another physically in the physical world. So I don't know how they made the gates between the ions which are far away, they manage somehow. But I mean, with my physical education, it's difficult for me to imagine scaling of the full connectivity. So anyway, I'm talking about... Well, you understand what I'm saying. No, I do, yes. It's interesting. I mean, I don't know if any of these architectures are atomic traps. The atom traps have some advantages because, you know, they don't have the Coulomb repulsion issue. You know, they don't have the Coulomb repulsion issues, but you have to do the coupling in the right-berg state. So, I don't know. Um, maybe it's something worth trying. I think Michelle Lukin's group's group, for example, has such a hardware. Also, in China, they have they built this large-scale quantum computer, which is called Chapter 9. So, in China, 2000 years ago, their mathematics start with some ancient Chinese book, which is called Chapter 9. That's a reason of the name. The name, but it's not publicly available, so we can try this around this on Chinese. So we have we can read something from the news. But as for your statement, I agree. Code atoms are nice, but I don't know which company uses this and how to get a public access to those which you mentioned. I'm not sure there, but you might be able to get access soon, at least through AWS or maybe whichever can scrub. I'm not sure. I'm not sure. Yes, yes, we should try everything because you know the best hardware is not known yet to me. Right, right. So, do you, I mean, this is, do you like to speculate anything, Vladimir? We don't have any further questions. So, let me see. So, our main speculation is the one which I said. So, we assume well, when we were working with this optimization, we had actually two different conjectures. Two different conjectures. I mean, what's the best ordering of this product? This is like really complicated mathematical problem. Key type syllabus has something to do with related directly. So our first conjecture was the one which I mentioned. So we think that we can construct this G tilde. So global, local, global, local. And that's the one which should be repeated. On the other hand, we also. Be repeated. On the other hand, we also tried pentuplets, so I don't know, pentagons. I mean, five of them, so like you know, local, global, local, global, local, global, five of them. Also works nicely. So I don't know, maybe this triplet is not the best. Maybe five of them should form the individual block. On the other hand, I want to tell you something else. So far, what I discussed was the following. So I have a total database. So, I have a total database, which at some moment, roughly speaking, in the middle of my calculation, I subdivide database into halves and then make a search in each half separately. In principle, in the future, when we shall have large databases like 500 qubits, okay, then I can change this. I can start with global diffusion on the whole database. Then I can make subdivide the database into two halves, make local search, then repeat global. Search, then repeat global and then subdivide the database in four in four blocks, and then this will make my space seven, I mean five-dimensional, five-dimensional, and the rotation will be O5 group, and then this problem of non-commuting will become even more complicated. So, I don't know, we need mathematicians like Professor Bruno Nartihalle to look into this mathematical problem. Look into these mathematical problems is like optimization in the group, not in Lie algebra in the group. So, ketype slav is definitely useful, but it should be developed to choose the optimal order of this not come using. And what I'm trying to say is that what I explained was the simplest version, and later with larger database, we shall this problem will become more and more serious. That's my comment. Okay, very nice. Thank you. So, maybe I have. Thank you. So, maybe I have one suggestion that might or might not end up fruitful for you. But many of the provable, so nowadays we want NISC algorithms, of course, and many of almost all provable such algorithms that don't have some kind of a caveat are based on Grover. Now, if some of these algorithms could be looked at more closely, and instead of Grover with the thought. With the thought that they would use your algorithm instead, it would be interesting to see whether these various proposals could be like enhanced or improved. I mean, it would be a nice program for you have a group. So it would be interesting to look in the context of specific problems to see if the structure can be matched with the structure of your proof to give some kind of an idea. That's a beautiful comment. I fully agree. Nevertheless, I want to add something else. So, in principle, there is another approach to search, right? Is another approach to search, right? The one which comes from Scott Aronson and Andres Ambanius, which is, I mean, in classical case, I mean, in Grover, I model the database as a set, right? So according to the Grover model of the database, theoretical. The database is a set, meaning that there is no geometry, no topology. So I don't know which element is next to which, what's the distance, nothing. On the other hand, in praxis, the database can be some. The database can be some square ledges or cubicle edges or something like this. And then in classical case, I would go through the my cursor will go through the database as a random walk. But in quantum cases, it's a quantum walk, which was studied from the point of view of search by Ambanius and Aronson. I wrote also several papers on this, but anyway, I agree with you we should try all possible hardware and software. Should answer yes, of course. Software shall answer yes, of course. I agree. That's all. Thank you. Any other questions? Comments? Comments? Something? Well, you know, I'm always very excited by your talks, Vladimir. It's inspiring. So if there are no other questions, we'll resume in two hours and five minutes with Olushtenko's talk and followed by a panel. So I should stop sharing, right? In order to deal with. In order to release you from poor polls. If you press escape, I think you'll stop sharing. Okay, we do. Thank you all very much. And thank you, Vladimir, for this talk. Thank you. And hope to see you all in two hours, in about two hours, two hours and five minutes, actually. Okay, thank you so much. Thank you. All right. And I'm trying to find. Oh, stop share. Stop share.