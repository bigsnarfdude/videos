First, Simon Di Marino from the Director General will give a talk about curvature and health grains inequalities on many. So, thank you very much for the introduction. Thank you very much to the organizer for the invitation to this nice, very nice workshop and very nice venue. So, yes, we'll talk about these five gradients inequalities and the relation with curvature. So, why five gradients inequalities? So, five gradients is because there are five gradients in this inequality. There are five gradients in this inequality: so gradient of H, which appears two times, then the gradient of Phi, gradient of mu, gradient of psi, gradient of mu. Now I will describe what are those and how they are related to each other. So here there is a resume. So mu and mu are probability measure, which have to have some regularity, otherwise it will take gradient. So that's one, one. Phi and psi are count of average potentials. And I will say in a bit what are the I will say in a bit what are the hypotheses, what is so it's related to optimal transport, we will need some hypothesis. And then we have that this expression is bounded below by something. So we will see what this something is. Most of the time here we will have zero. So in the easy cases, let's say, but then we will see that in manifolds here it will appear equivalent. Okay, so we will see. So we will start from saying what are these Kantarovich potentials. So maybe not everyone knows what Kantarovich potentials are. So we begin with defining in general, so given a cost, so our ambient space will be this M, which is a manipul, but you can think of it as just a metric space at the beginning. Yeah, but two of them are equal. Yeah, but two of them are equal. So it's just five of them. By the way, this was named by Filippo. So if you have a question about the name, you should ask it. So first, so we fix a cost function from m times m to zero plus infinity, and then we will define this. And then we will define this cost associated with this cost function, the ground cost, between two measures, mu and nu, which are probability measure. And this will be a minimum of what in figure in the product space of the ground cost with respect to couplings, right? Now, what are these couplings? Okay, couplings are just probabilities in the Is in the product space whose marginals are new and new. And now this, of course, is the general optimal transport problem. What properties do we ask for C? Well, usually here we will assume in general that C is elementary. But many of But many of the stuff that I say can be generalized also for maybe quadratic cost in RN, but it gets a bit more technical. So let's say for the sake of this seminar, I mean also the paper that we wrote is everything is rich because it's more manageable. Usually when you go to unbounded sets, it gets a bit more complicated. You have to You have to see a bit better what happens at the increment. But so let's say that this is leak sheets and so they are bounded, etc. Okay, so this is the standing assumption and then what we need is the duality for this problem, right? So this problem is a primal problem. We can see the duality and the duality says that this is also equal. That this is also equal to now. You can write that this is a maximum and the maximum of what? Integral of phi d nu on m plus integral of psi d nu always on m and now we have a condition on phi and psi which is phi of x extra plus psi of y less or equal than this cost right and we can assume since c is And we can assume, since C is Lichitz, this regularity property is inherited by potential. So we can assume also that phi and psi are lit. Right? Now I use the same guys here because they are called precisely the optimizers here are the maximizers, the couple that maximizes this, are actually called the counter object. Are actually called the cantorovic potentials, right? Okay, so up to now we have all the ingredients if you want. So once we have these mu and nu which are in probabilities of m, so they are greater or equal than zero and are in w11, then we have the t is greater or equal than something. Now, let me call a bit to be a specific function also. For now, not, but. For now, no, but we will say now what it happens historically, let's say. So, the first case, so first of all, let me say that interesting cases. So, M, which is the Euclidean space or a subspace. Or a subspace and the cost, which is x minus y to the power p. And then for this, we get the vast stain distance. So in this case, C C mu nu is equal to the vast standing distance to the power p between mu and nu. And so and the nicest case is for. The nicest case is for people's, right? People's good is the nicest case. Okay. Other interesting cases which we will be interested in. So, of course, if always m equals to Rn and C of X Y Of xy is a general convex function of x minus y, which convex should be, let's say, is a translation button button so. So, I write it translation invariant because then I will need it to be. So, translation invariant means actually that in Rn it means that it's equal to C of X minus Y 0, right? But now, if we have just a leap sheet, it's just saying, okay, I have this form here, but I assume the H to be only leap sheets. To be only leaf shifts, I don't need the convexity, right? And last case is Miemanya manifold and the cost, which is a convex function of the distance where H is convex. Is convex and increasing. So these are all the cases which we'll hear about. So it will be apparent actually. So this is the cases that will be covered in this seminar. It will be apparent actually that probably with the proofs that we have, we can generalize this a bit, but it gets very complicated to understand what is the To understand what is the right-hand side for a general cost. Okay, so we keep it simple with the right. Okay, so literally, what it is known is, so the first case, so where this inequality was introduced, what it was introduced in 2015. By definite piece to write down what we see, Charles Santa Rogo and Rich Kov. So, what were their assumptions actually is Their assumption actually is M equals omega convex. So this is the subset of our N, omega convex, and only in the case where the cost is the quadratic cost. Right? Okay. So actually, and I will talk a bit about also the bit about also the the at least the the applications that they use here which are uh the the basis of why we use it right so and then more recently there was a paper by kaye where he generalizes the cost part right so it is again omega convex and now the cost function And now the cost function is of this kind, this h of x minus. And in both cases, we have that the right hand side is equal to zero. So that inequality, so that left hand side greater or equal than zero. Okay, so just a remark: why convex? Why omega convex and it's because of boundary terms so if you write it down by doing an integration by parts with the integration by parts it's true whatever omega you take right because it comes from a pointwise inequality that you integrate but then Quality that you integrate. But then, if you do it integration by parts, then you have the divergence of something which is first order, it becomes second order. And for the second order, you are asking second order regularity for the potentials, which is usually not true, especially if you go costs which are not regular enough, because you can have singularities for the maps in particular. We recall that. We recall that the gradient of phi is connected with the maps, but the maps can be discontinuous if omega is not convex, so you can have really discontinuities in those cases, right? Instead, if yes, one question. There's any relationship between small age and big age, capital H? No, no, so the result in those papers is true for every capital H which is even small. true for every capital H which is even convex? Yeah yeah for every for every big H which is even in convex. Now let me go back here and I just say a couple of words about the applications in order to motivate a bit this and in order to understand also thanks for the question to understand for which age this inequality is right. So, of course, whatever question you have, please ask. Okay, what kind of applications do we have? So, from the same paper. 2015, actually, the paper is called BV Estimates, blah blah blah, etc. So, inside there is this five-gradience inequality, but it's in some sense the core of the paper, but the paper is sold as, okay, I can do VPA estimates in some kind of variational problem. And what is this variational problem? Well, you take something which So this is new, maybe new, no, it's the same. New arc mean was maybe something like this. So you do something which is in the spirit of my maybe JKO scheme, right? So you minimize a vast sustained distance plus something else. And now this something else, and now I will do. This is something else, and now I will do just the easiest case, which is just some convex functional of the density. So I get something like this with f which is convex. Now, this function, if this is strictly convex, then this means just Strictly convex, then this admits just a minimizer. And what you can prove actually is that by using that inequality, sorry, this inequality here, so optimality conditions on this. So, the important thing to remember is that the first variation, so the So, the potentials, the contravic potentials, not only are the contravic potentials in the dual, but are also the first variation of the Baselstein distance. So, when I do the first variation here, so optimality condition, here I get phi, and here I get f prime of beta, right? So, I get phi plus f prime of eta equals to constant because uh I had the constraint that this is a probability dimension, phi. Strain that this is a probability dimension, right? But now this implies what? That the gradient of phi is parallel to the gradient of eta and actually is parallel with sine to minus the gradient of eta. Right? Why is that? Because this is convex. So when you do the derivative here, I get gradient phi here, I get gradient theta times the second derivative, which is positive. So it's fine. This is parallel with sine, which is the opposite one, right? Which is the opposite one, right? Now, if I use phi gradients inequality with h of x, which is equal to modulus of x. Now, this is a convex function. It's a stretch, meaning that it has a singularity in zero, right? But still, you can approximate, and actually, it works. Approximate and actually it works. What happens? That the gradient here is just x divided by the normal x. Right? So now what happens if I do gradient of h of gradient of phi? Now gradient of phi has precisely the same. Sorry, here I put alta. Elta is this one. So These ones, so I have to change everything, but it's fine. Uh, this is psi, this is new, this is psi, and this is this is what the admin. So that's okay. So you have something like this. So you have this, and now when I do gradient h of gradient psi, this is equal actually to gradient h gradient nu with the minus. Radiant nu with the minus in front. So it's that's minus radiant nu over radiant nu. Now, what do I get? If I substitute here, these terms here, this term here, becomes exactly minus the grade, right? Because these are the same direction and that's modulus one. So when I multiply them, I obtain precisely this. So in the end, I get that I. So, in the end, I get that I bring it there. I say that here you have a zero actually. So, when you bring this there, what happens? I have that the B V norm of new is actually bounded by this guy here, but now this has modulus which is at least at most one. And so I can bound it with the B V norm of mu. So, what did I prove? They proved actually. I proved actually that the B V norm of nu in omega is less or equal than the B V norm of mu. Right? So in this sense, these are V estimates for variational, this kind of operational problem. Why they are useful? Because if you are doing the JKO scheme, now the JKO scheme is an iterative scheme, so it So iterative it means that you do everything every time like that. So I take new, I take the R mean of this, and then I take this, and then I take the R mean, etc. So it's iterative. Since we have a descending something, we have that this BV estimate is kept, right? So BV estimates are useful why BV estimates we get compactness for frequency. Compactness for free because usually the JTO scheme is a time-discrete scheme where then you want to go to the limit. In order to go to the limit, maybe there are terms for which you cannot just do the weak convergence, which is what usually Wasterstein distance are for, just the weak distances, weak convergence. If you have also BBI estimate, you get also compactness for free, and so you get also that the stronger term. So, you get also that the stronger terms you can convert to import. So, this is the main way in which they have been used. This is the first application. And second application, which has been in the works for four years and then finally came out, I don't know, 2022. I don't remember. You remember? You remember? The one? Yes. I don't remember. Maybe it was January 2020, but which are solid estimates for Ketplant equation. Again, on the JKO scheme, you run the same argument, but But JKO plus H of X, which is this time X to the power P. With that, you get, again, you need to work a bit more because the JKO scheme is not sufficient anymore. You need also LP estimates, etc., etc. But at the core, there is these five variants inequality with these, which is power p. Power piece and then also in the paper, in Kaye paper, you get same V D estimates for now. Let me write it directly at the limit. So it's divergence. So it's divergence, so it's the philaque. Right? Because you can write it as JKL schema, but for the buses time p distance, right? So when you modify the cost, actually you get the same, but with the with the controls potential of a different problem now. Of a different problem now. And now, if you see what is the V gradient flow in this case of the vastest type P distance, then you get actually the evolution equation with the P Laplace operator. Right, right, right, right, yeah, yeah. Or P equals two, of course, we have the same. Do of course, we have the same you obtain the club option, okay? So these are some of the applications. Of course, in general, when you have some kind of some kind of optimization problems of measure where you have a system distance, you can have some more, let's say, first order kind of Estimates with this inequality. Now, I can state what do we do? Maybe I can write there, but we have two cases. So is our contribution. So we have basically two contributions. So the first contribution, which I would say it's most similar to this one and it's the easiest to prove, is the case when M is actually a Lie group, which you can think of. Which you can think of as Rn. But so the Lie group is the right step in the middle, in some sense. And this is again translation embodium. Now it makes sense. Now it makes sense that this is, I call it translation invariant, meaning that I want something like that. So this is the multiplication in the liquid, right? So for example, you can take the sphere and the rotations and then these make sense, right? So it's maybe the distance, the intrinsic distance, or you can take whatever something similar and you. Whatever, something similar, and you get something like this. Okay, so in this case, again, you have something like that. Now, okay, I write it down, M. And now here I use another letter because it's a bit different. And now we say. And now we say what is everything here. Must be horizontal. So why I'm putting all these H because it could be a Lie group which is not a manifold, right? So it could be a subdivision. So, it could be a sub-Riemannian manifold. It could be, for example, the Eisenberg group. The Eisenberg group is still a group, but it's not a manifold. You don't have a tangent space which is the same dimension. So, here you can have only some of the direction in which you can differentiate, right? And so, this is the horizontal directions. So, if you don't know about some So, if you don't know about sub-Riemannian stuff, Heisenberg group, etc., you can think these in a n with the gradient, and it's seen. But I think it's interesting that this holds in general, also because the proof is precisely the same. So, I would say that I was getting confused. Okay, and G, what can we take as G? And g, what can we take as g? Now, g is not really every even and convex function. It's a bit less general. So for us, g of x will be something like, so I take a convex function for every direction, and then I take x times v, so this is the scalar problem, really, and then I do. And then I do this with respect to every direction. So you take any convex even function on R, right? So for every direction, you take something like this, and then you integrate. This is still convex. This is still even. You don't get every even convex function, but the interesting one you can take. So you can do something like this. You can do something like this. The power P, which is what you usually take, it's for granted. Okay. But this is saying also something a bit interesting, which actually you see in the proof. So the base case here is actually taking just a directional derivative in one direction. And this is why it's important to have something which is a group because. Which is a group, because fixing a direction and bringing this direction around, you can do it only if you have a translation invariant group. So only if you can say, okay, I bring in some way this vector here, then I bring it here, and then when I come back, I should have the same, right? And this you can do it only when you have these translations, when you have killing, basically. When you have killing, basically, when you have killing vector fields for every direction, which is basically, once you have killing directions which span mostly everything, more or less you are a regroup or you are a quotient of a regroup. Okay, so this is the first one. Let me tell you also the second one. And then I just tell you just a bit about the proof, which will reveal why. Will reveal why we have curvature. So, in this case, M is a compact Riemannian manifold. Smooth. Well, smooth, it means that it's at least C3 or something like that. So, I need a bit more than curvature. And I assume that there is a And I assume that the Ricci permit is bounded below by K. Of course, since it's compact in minor manifold with CC3, it's already a reach orbital which is bounded from below. But I want to say that it's bounded from below from K, because then I will use this K, and actually, this is the optimal one. Okay, so what do we have? Have similar to here, actually, G will be the same. No, okay, it's not the same. Here I have another name, but I will explain immediately what this is. Here I have the volume measure. Also, there I have the volume measure. Of course, now this is greater or equal. And then here I have minus k integral. Now here I have something which is a bit strange. So it's L prime of H prime of dxy dxy dga. So gamma is an optimal plan and the cost function is h of dxy and here I need h to be convex enough. Right, so this h is different from that big h, let's say. Right, so this h is the is related to the cost function. Is related to the cost function. It's like the little HD which I introduced for the Euclidean case. So you show the summary because if you increase K, you are putting stronger assumptions and you are getting weaker results. Yeah, I always put the wrong one. So probably we are right. I think you are right. I wrote it somewhere. I wrote it somewhere, I'm sure. Yes. Of course, you're right. Yeah, yeah, of course, of course. Okay, so what does it mean if you think at the example that we said about the dv contraction? Actually, this is saying that if k is positive, so you are in a curved space like a sphere, then actually the Then actually, the BV estimate is better, right? You are not only non-increasing, but you are decreasing by something which is bounded by this distance here. Actually, what you get, so for the obligation, you get in the same way as before minus k integral of Integral of you get something like this. Now, I want to comment very much. What I want to say is that if the curvature is positive, this is saying that it's decreasing at a certain rate. It's not linked with the square distance, it's linked with the distance power one. Why? Because. Why? Because we have to take L, which is again L of T, which is modulus of T. But then L prime is one. So when you plot it here, this is one. And so you get just distance. Instead, if you take L, which is the square, right, then here you get gradient of phi, here you get the gradient of psi, and here you get h prime of d. So it's h is the square, here you get the integral of this. Right? squared right so the linear case is uh is the case when l is equal to t squared and in that case you get everything which is uh which is mostly fine right but here you are saying that if again if you are in a positively curved space you are gaining more than just decreasing if you are in a negative space if you are in a negatively curved space what you get is that this may be increasing And this may be increasing, but not too much. So, again, with this, you can get still a compactness and estimate, right? Because, again, this is with respect to the distance. So, maybe as you go along with the JKO scheme, you get the sum of the distances, and so maybe you still get that it's not exploding, right? Okay, so what are the remarks here? So, what are the remarks here? And L is convex. So, here we are losing the general convex, right? So, here it's a general convex function where it's really something which can be anisotropic, right? Here, we get that L, L now is just convex function which goes from zero plus infinity to zero plus infinity and L To zero plus infinity, and L of some vector V is defined as L sorry, L prime, L prime of the modulus of the vector V times its direction. Right? So, this is what this is, a general isotropic convex function. And why is that? Because if you see, I think you can see it. See, and I think you can see it already here. Actually, you're taking a convex gradient of a convex function on what? On the tangent space in some point. And here, you're taking a gradient of the same convex function on another tangent space, right? So if they are anisotropic, you have to find in some sense an identification between the two in order to apply the same function. The same function. And now, this is true in RN, of course, because you can identify one vector space with one another by the translation. You can do the same in Lie groups, because again, there is a canonical identification between one tangent space and the other, because you do the translation, and this is an identification with commutes. In a Riemannian manifold, you cannot do this. Precisely the example that I gave you before. If you have an identification between a transit space and another A tangent space and another function space, it's not function. So, in general, if you have an identification everywhere, then this cannot commute for everything. So, the only thing that you can do is actually the, in general, let's say, is only this one, right? Maybe you can find something if, I don't know, this is a product of some groups times something else, and so for some part of the space. For some part of the space, you can do it, right? Okay, so I want to do this little sketch of the proof in order to, so it's not that it's not super difficult, but I want to highlight where the curvature comes into play and what can be done to generalize this even more, if you want. More if you want. So, again, I don't think, I don't know if I said it already. This is a joint work with Emmanuel Aradici and Simone Muro, which is at the University of Genoa. He's a mathematical physicist. It was our expert in manifolds. Okay, so let me just say the proof in RN. So the key Key estimate is to find the four-gradient inequality. So, four-gradient inequality means that L is just T squared half. So, L prime is the L. Okay, okay. One thing that I didn't say, which I say now, actually, so little remark. The five gradient, the four gradients inequality, as written there, like so for also h t squared over two. So this is the buses time two. This is the vastest time two with the linear one, so the easiest if you want. This is equivalent to the k-convex, actually, the k-contraction of passes time two along the flow. Okay, so why is that? It's it's it's not It's like four lines of proof, not that difficult. You have to use that when you do the first variation of the vases tail distance, you get phi. And then, so when you do the derivative of the vast stain distance along the two heat flows, you get phi times the Laplacian of mu plus psi times the Laplacian of mu. And then you do integration by parts, and you get gradient phi, greater than mu, gradient psi, greater than mu. And so you get this derivative. And so you get that this derivative is greater or equal than k integral of d squared. And so you get Granval, and you get actually this. And this by Otto and what was it? Correlanes K contraction if and only if Richie Kilberg. It's greater than equal to k. So, this is saying what? This is saying that actually our inequality is sharp in the sense that if this is true for every phi and mu, if it is true for every, sorry, mu and mu, then we have that the rich curvature is bounded from below from k. Okay, so it's a characterization of the rich curvature, if you want. Okay, this was the remark. And okay, so about the proof, how much time do you have? Two minutes. Okay. Okay, so I do it directly for Imania. So what do we have? We have that actually, so this is the first. We have that actually, so this is the first order condition. So complementary conditions, sorry. So this is the quality on the support of gamma. Right? So the idea is what now? We want to get basically we need the Laplacian of Phi and the Laplacian of Psi. How can we get the How can we get the Laplacian of Phi and the flaxion of psi? We take the second derivative here, right? But now what do we know? We know that this is true with the equality here and is true with the inequality outside. Right? So we are at the contact set of some inequality and we want some estimate on second derivatives. And this is clear because we are at a maximal point, so we will. We are at a maximal point, so we will have some inequality, right? So, how can we get it? Well, basically, we have to find these what we call a synchronous variation, right? So, we have to find XT YT in such a way So, we will have something like this. So, we are doing variations and we want to do synchronous variation of x and y. This is x t and y t. And we want to understand now this again, well, t is equal to zero. So, suppose there. zero so suppose that we are doing a variation like this and why we are doing evaluation like this at zero in zero we have equality so in terms of zero we have equality right while we have an inequality for every t so again we have a maximum of this function here at t equals zero so we have the second derivative Second derivative of this quantity here, which is less or equal than zero. Now, when we do the second derivative here, we get the second derivative of this guy along the direction. The second derivative of this guy along this direction. And what about this? Now this, it comes down that now here you have to imagine that now this is and they finish here. This is H. Here, this is h of the distance between xt and yt. So, in order to do the second variation of this guy, oh, sorry, of this guy, it's sufficient to have the second variation of the distance. Then this is a convex function, and you can manage to do it. And now, in order to do this, you basically take the energy, which is the along with the density, right? And basically, you have that if you do the derivative of. If you do the derivative of the energy of this gamma t, so now gamma is this curve here, gamma t is the geodesic that you moved along what are called Fermi coordinates, which is a way to propagate a frame, right? And you get that this is less or equal than minus the integral of g of so. So x reach in of x gamma point gamma point from 0 to n. Right? So what is x? xi, sorry, not xi, vi. Vi is this direction here along the geodesic. So once you do this second variation, you get something which is something connected. Something connected with the Ricci curvature. How the Ricci curvature comes up when you sum up this for V1, V2, Vn. So you get the Ricci curvature here, and actually summing up here you get second derivative, summing up over all, you get everything. Okay, and I think I can stop here and thank you for the attention. And sorry, if I go. Do we have some short questions? We need the network to be complete to look this up. Yes, so compact and complete. We don't have to have boundary because, for example, we don't cover omega convex on Rn because we have to play with the boundary. Actually, it's true only for omega convex, the fact that it's inequality, because you need some arrows to be pointed. Some arrows to be pointed inwards. So just for W11, we don't want to deal with the boundary. But yes, you have to deal with the boundary if you are not complete. So if K is negative as a word spaces, then this integral, K, integral of distance x, Y and gamma could be infinity, right? Is it possible to put some possible some negative case? To put some positive, some negative case? Yes, it's possible. So, usually, okay, you are correct. So, if you put, so if K, let's say this is a bound from below, right? So, you can have negative curvature somewhere and then positive curvature otherwise. You cannot have negative curvature everywhere because otherwise you cannot be compact, right? So, you cannot be compact completely. But you can have some positive. Have some positive, yes. So I would say that if you have a hyperbolic space, so negative curvature everywhere, you can still do it, but you have to have mu and mu which are compactly supported. So in that case, it works. So because you are doing everything locally and everything works. So the only thing we have to have is that of boundedness of stuff, in particular, we need the Particular: we need the Lipschitz constant to be bounded, and we need everything to be dealt in a bounded set. So, if you are in a bounded subset, it's fine, but it has to be one W11 globally. Right? So, I've not understood a point in your idea of proof. Are you proving with this picture? The four-grading inequality, or how to deduce the five from the four? No, this is the four-grading. Can you just say that the Can you just say that the forget is inequality is true because of the contraction in W2? I expected you to say forget inequality is true because we have an assumption of the curvature. We know it's true. No, so because, sorry. So the key estimate is the fact that it's inequality 0.1. So the co-contraction is the four-grade inequality integrated, but actually what Integrated, but actually, what you need to generalize is astrometries point-wise, right? And this is actually, you can find. So, we were inspired also by the paper by Ben Andrews, there was a paper of Savare where you do doubling of variables. When you have something like this and something is evolving, usually, you know, it's like you want to estimate, I don't know, the modules of continuity of solutions to the equation. You use this double variable argument and you run something like this. Run something like this. So it's ubiquitous. Not ubiquitous, but I'm not sure.