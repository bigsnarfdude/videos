Today, I would like to present a joint work with my former supervisor Barbara and my two colleagues at the University of Prague, Martin Holler and Christian Archert. The topic would be about parameter identification for PDEs from neural network-based learning to a broader context. To a broader context, which is discretization of inverse problems. So the talk is divided into two parts, clearly from the title. The first part would be about learning informed parameter identification. And then we will see this problem is a special case of a more general problem, which is discretization of inverse. is discretization of inverse problem we will discuss in the second part and finally we we will see some numerical results so now we start with the learning informed parameter identification so let me start with a motivating example so we have here a heat equation in this equation we have several unknown physical parameters Known physical parameter, which could be diffusion coefficient A or potential C, or the source thermal inertia data. We have in addition to in the model a nonlinearity F view and we wish to recover the parameter in blue here as well as the state. And in order to do that, we need some additional measurements. Additional measurement data, which is why here. So, this is a classical parameter identification problem. So, then we raise the question, which is in this model, we assume to know only partial of it, and the nonlinearity F is unknown. Here is unknown, and we wish to cover this non-linearity as well. The situation appears in practice. For instance, in some cases, we only have access to a part of the model, or in some other cases, we use, for instance, a simplified version of the model, and we then wish to include. we wish to include some correction term in order to refine the model. So that is the motivation of our work and we will see in the next slides that we rather look at the problem in a more abstract setting. So how to find the nonlinearity F? Our first attempt is to approximate F via neural network. Neural network due to the excellent representation of the neural network for the continuous function on the compact domain. So F here acts on U, which means the neural network that we always denote by N here, act on the state U as we see here. And remember, U is unknown. So here we present an extended version. Then an extended version with another hidden unknown hidden physical parameter alpha here. So the input to the network would be this unknown parameter alpha and the unknown state U. And they go through a network with several layers and output as F of alpha U in R here. So U. In R here. So UX is in R as well, and alpha is in Rm. A neural network is a concatenation of several layers A, where in each layer the input Z will be multiplied by a weight matrix W, then plus the bias, and finally apply a non-linear activation function sigma. We always use theta. Use theta to denote the unknown hyperparameter of the network, including the weights in the bias. So then that is the first question. The next one is, should we find the neural network first and then insert it into the model and we solve the classical parameter identification or Identification, or should we find a network simultaneously with all the unknowns that we identify by lambda here and the state U? And our attempt is to find all at once. So let's have a quick review on the literature of learning PDEs. So the first very interesting direction is the learning PEDEDEDEDEEEEEEEEEEEE Direction is the learning solution map. That this group is very famous for their well-known algorithm called physics-informed neural networks. So, they derive some neural networks-based solver for PDE. Yes, this is clearly a very interesting direction to look at. Perhaps more closely. More closer to our research here is the direction of a recover PDE model from data. So, actually, this direction has a long history, especially in the past when one was not certain about the physics law that one should follow in order to model a phenomenon. And the recent approach, some of the recent approach. Some of the reason approaches suggest to construct a very rich dictionary of all the bases and then find the corresponding coefficients. And the last which is a paper by Hinter MÃ¼ller, which is a very nice paper that inspired us the most for our study here. So in this paper, the authors suggest the schema. Orders suggest the scheme like this. They have a training step where they learn the non-linearity F first, given the knowledge of all the unknown system parameter as well as a full observation of U. And then with that, they replace this F by a neural network and then run the classical parameter identification. Primary identification. So, this approach, so with this, they need to construct the training data for the training step and how does one do it. So, if we have, we know all the parameter alpha, I'm sorry, A, C, and Phi here, as well as we have a full observation of U. Observation of you. And if it is smooth enough, then we can take the time as well as the space derivative here. Then we move everything to the right-hand side. Then we have the knowledge of F of U. And then we can build the training pair U and F of U. And from that, we run to a learning process and phi F. So that is for the training. This case might be hard to achieve. Might be hard to achieve in practice. For instance, in some cases, we don't know the system parameter, as well as the measurement of u is not smooth enough such that we can take the time and the space derivative of u here. And that inspire our research, where we don't need to know the parameter, lambda, as well. U may not be smooth. May not be smooth. So, with that, we look at our problem in a more general setting. We still have the time derivative of u and f here denotes the known parts of the model. And the unknown part, which is non-linear f, we split it like this, and then we approximate f by neuron network, and with that, we. Work. And with that, we name our problem as learning-informed parameter identification. So, after replacing F by a neural network, we then formulate the problem in an all-at-once setting, namely, we recover all at once all the physical parameters, which is lambda, alpha, and u0 here, together with the solution of the system. Of the system and the neural network characterized by the hyperparameter theta. With that, we formulate the forward operator G like this. So the first component is the learning informed PDF that we see clearly that F is now replaced by N. The second part is still the innovation data and Do the initial data, and the last element is the observation operator M. So G would map the information, would map those unknowns to zero, zero and the measurement y. And then we look at the minimization problem of minimizing the discrepancy between the image of the forward object. Between the image of the forward operator G with the data, some regularization term, a regularizer, over all the unknown physical parameter, the solution of the PDE, as well as the neural network with the hyperparameter theta here. So let's have a look at the advantage of the all-advanced formulation, a classical A classical approach to those inverse problems is via a reduced formulation. So, in the reduced formulation from the PDE, which is in this case a learning from PDE and this is non-linear, we construct the parameter to state map or the solution map, mapping from the unknown q to the solution u and then we insert. And then we insert it. It here, we concatenate it with the measurement operator M, and we form the forward operator G. So G would map directly the unknown Q to the data Y. And in order to solve this inverse problem, we need to take into account several aspects, especially well-defined. Of the parameter to state map, which is unique existing analysis for this non-linear PDE. And later on, there will appear the edge equation that we need to ask the same question. From a particle point of view, for instance, if we solve the problem by an iterative method, we need to solve this non-linear PDE in each iteration. While in the all advanced While in the all-advance setting, we do not construct this parameter to state map. Instead, we have a higher dimension of forward operator G, where the first component, as we saw in the previous slide, it is the PDEs. The second part is initial data, and then we have observation operator M here. And we're supposed to minimize the PDE. To minimize the PDE's residual to zero, and yeah, with the observation data, Y as always. So, by this formulation, we don't construct the solution map. And so we don't need the analysis for us. And in practice, we do not need to solve any non-linear PDE. So, this is an advantage of the older one's formulation. So now our plan is first to look at the so-called learning problem, where we solve the minimization problem here over again, all the parameter, the solution u, as well as the hyperparameter of the network. And if we succeed to do so, we can get theta, which is the network from this step, and we plug into the Into then another parameter identification problem, but we see mainly the difference between those minimization problems is here we have theta and here we don't. And we plan to answer several questions. First of all, the existence of the minimizer to bow problem and then the conversions when the regularization. Regularization parameters tend to zero. As we are dealing with the learning inform PD, we are interested in also the unique existent analysis for this learning inform PD. And then another question that's quite attractive for us, which is if the minimum num solution is unique and we intend to answer it. Uh, intend to answer it via the tangential concondition that we will discuss in detail later. And then, uh, in order to solve these problems, we need some other ingredients. For instance, differentiability, a conversion guarantee for the iterative scheme, and how to compute the analytic adjoy. What remains for us in the list of questions is In the list of questions, is for those, we exist and convergence, we can claim for both problems. While uniqueness of the minimum norm solution, we are able to solve such a claim for the PI problem. We don't know yet with the learning problem. And that leads to also a convergence guarantee for the learning problem. It remains open for us. Open first. So let's discuss first the first result: existence of a minimizer. We can show existence of the minimizer via the direct method, which essentially based on weak closeness of the forward operator G. And we prove it in this way. We remember that G consists of this part of the nonlinear, this part of The nonlinear of this part of the model, this is known, and the second part is unknown, that is now the neural network. And then now we treat F and N as the Nameski operator. And by studying the Nameski operator, we can claim that for the F part, which is the known part, we need F either weekly continuous or pseudomonotone to obtain the weekly clause, while for the neural net. While for the neural network, we need the activation to be localized and we need this embedding of the state space or the solution space, which is an important condition because the neural network offer a good approximation for function on the compact domain. And we remember that N here acts on U, therefore, we need this. On you, therefore, we leave the state you to somehow be informed about it. And so, we look at again the minimization problem. We see that if we send those regularizer parameter, regularization parameter alpha, gamma here to infinity, then we have the convergence of the minimizer up to subsequent in the weak scent to the solution of the PDA. The solution of the PDEs, which means the PDE is residual, must be zero, as well as the measurement error zero. So, in this case. And that if we think of the regularizer as the norm, then this is exactly the notion of the minimum norm solution. Then we ask the next question if the minimum solution is unique or not. So that leads to the use of the Tengjen. To the use of the tangential condition that we introduced before. So let me recall about this condition. We are given an equation, let's say abstract from gx equal y. And the tangential cone condition, which was invented by Schetzer, it defined like this. So defied by this inequality, it says that if It says that if the left-hand side, which is the first-order error of the Taylor expansion, is less than the image at 2 point, then we can obtain several properties here. Graphically, this condition means that the forward operator, the graph of the forward operator, the CG in this case, must lie in between a cone with A cone whose slope is made of the constant, the tangential constant C here, and it is required to be less than one. So if these conditions is achieved, then one can ensure the following property. First, the uniqueness of a minimum norm solution, and of course, the equation must be solvable. Second, if we run, we solve the problem with an iterative method, then all the iterative. Iterative methods, then all the iterates remain in the ball, and moreover, we have a convergence to the attack solution. So we can establish this tangential condition if the activation function is in C1. And we prove that in the following way. So X in this case denotes all the unknowns. And again, the All the unknowns, and again the forward operator G is split into the known part F and the unknown part with the network. Now, which is the network. For the known part, we could follow some results in this paper where we study the contential condition for the A, the C problem and another few nonlinear problem. And for the network part, we study Lipschitz property of it. Property of it. And moreover, we can compute the Lipschitz constant explicitly. And this is quite beneficial because if we know the constant, the tangential constant explicitly, then we can control how large the ball here, how large the ball where we can search for exact solution is. Okay, so yes, so we Yes, so we established the tangential condition for a given learned theta, which means at this point the network is already learned. For the case where the network is to be identified as well, we haven't had the answer yet. So at least for the case of parameter identification when the network is already learned, we can confirm tension. We can confirm tension con condition. And what is left is solvability of the learning-informed PDE. And that is this result presented here, which says that for this model, if we pick the potential C in LQ and A in some space smooth enough, as well as the Smooth enough as well as the source term and the initial data. And here we have some relation between the indices. And we pick the activation in C LipG, when the activation is LipGIS continuous, then we can prove that the learning inform PDE admits a unique solution U in this solution space with some regularity. Regularity, some regularity in space and in time. What is important here is this index for a spatial derivative index must be high enough such that it could be embedded into the space and finally uniformly valid, which is an essential condition that we state in the existing result. So I put here an open questions about time fraction. Questions about time fraction derivative because after listening to the nice talk, very nice talk of Sangaro on Monday, he obtained the unique existence for the time fraction derivative via the gradient flow and obtained as well some nonlinearity, Lipschitz nonlinearity in the model. So I think it fits quite well with the network case. The network case, and that way I put an open question for me to look at next. Okay, so let's have a recap on the first part. We look at two problems, the learning problem and the parameter identification problem in an all-at-once setting. And we could show the several results here. Okay, so now we let's just discuss about the second part. Discuss about the second part. So, so far, we approximate the unknown F by a neural network. So, this could be in fact seen as a discretization scheme. So, then we ask a broader question, which is now if we approximate f by any f n in some smaller set or smaller, smaller space x n in the big space x, so where can we Can we answer something about the regularization method for the discretize problem, discretize level. So the first part is just a recap of the neural network we saw before. So now we can compute the number of the high parameter n, which is this k. And now we look at. And now we look at n as a discretization parameter, and we look at the set Xn in this case containing all the neural network on Rn with n parameters. And then we state a so-called partially discretized regularization approximation, which is a minimization problem where we still look. Problem where we still look at again all the physical parameter lambda and the solution u in some space which is not discretized. However, the unknown non-linearity f is some xn and that we call partially. And we take, we examine the minimizer of this cost functional t so now we we have some We have some parameters to control, which is, of course, the noise level delta and the regularizer gamma and discretization parameter n. So yeah, this is again just a statement of the problem, which consists of lambda parameter u as the solution, f the non-linearity in the model, and some parameter discretization parameter regularization. Discretization parameter, regularization parameter, and the noise level. So for the tick and up regularization, we study when the noise level delta goes to zero and the regularizer parameter to zero, the discretization parameter to infinity, as well as the approximation error must tend to zero. In this case, we obtain some convergence to the exact solution of The exact solution of the inverse problem. But now we will discuss about, so this is presented in the paper, but here we discuss more about the Land Weber iteration. Okay, so now let's first denote all the unknown as an X. And we have an abstract form of the equation, model equation fx equal y. And we know that the Land Weber iteration in the big Landweaver iteration in the big space X reads like this. So now, if we have the smallest V X M, then the forward operator Fn is definite like this with a restriction to the smallest V X M. And we study the projected land vapor iteration in the smallest V X M. So beside the projected part, which is the MPM here, comparing this and this equation. Comparing this and this equation formula, we see that there's different in the forward operate F. Now we have Fn here, and it influences the difference in the edge-roid as well. And we are interested in look at the convergence of the iterates in the discretized level if it converts to the exact solution, provide. Provided some appropriate choice of the stopping group K, stopping index case, and discretization parameter n as well. So our work is inspired by those very nice paper that I include here. Okay, then we have the first result, which is the noise propagation, which says that so XK is Start so xk is the k iteration in the big space x, where xn k is the k iteration in xn, and here it is it has the contamination of the noise level delta. And we can show that the difference of the iteration on different spaces is bounded by some term, which includes, of course, the noise level delta and Sn here. And Sn here shows the approximation error that we see here. Identity minus the projection operator here somehow tells how good approximation is and it must tend to zero. Rho n here is an upper bound for the distance between the two initial value, the starting point, the initial. Starting point is the initial iterate of the iteration. So, x naught is the starting point in x, and again, xn0 is the starting point in xn, and they shouldn't be too far away. Besides that, we have some other constants, MR and LR, which are the upper belt of the F prime, as well as the Liptis constant. So, that was quite a natural assumption when F is non-linear. And then we And then we have the next results on convergence that states like this. So, all the iterates in the big space as well as the small space remains in the ball under this choice of index stopping index case. So, here we have some ingredients. So, k must be chosen appropriately according to, of course, According to, of course, the noise level, the radars of the ball, and some other constants. What is important here is perhaps the constant Kr and Mr, so they originate from the tangential constant. So as we maybe we can recall that in the first part of the talk, we claim that the tangential condition is very important in. Important to show that all the iterates remain in the ball. And then finally, if the noise level noise vanishes and the stopping index is chosen according to those relation, then we can claim convergence. In particular, the index stopping index has some relation with Has some relation with some constant that we already explained and some other row as well as n, which would denote some approximation error on the small space. And with that, we can claim that we have a sub-sequence that converts weakly to the exact solution, a sub-sequence of. A subsequence of the each array in the smaller spaces. And in order to obtain that, we need a further assumption, which is the weak closeness and the lip discontinuity of this map. So this map is basically the land web update in each rate. And the mapping we are studying here is map X to this amount. This amount, and it is clearly nonlinear. So, as we propose those conditions, then we ask: next, is it feasible to confirm those conditions for some examples? As we see here, the tangential con condition is powerful, but it's not so trivial to establish. And for the second condition, we need to fast or write explicit. First of all, write explicit the analytic F-Joy, and then we study this non-linear operator and see if it is really what we assume here or not. So in fact, we can, so we show an example where one can verify those conditions. For instance, if we have the problem with the unknown C potential, C, the source stem. Tensor, see the source stem F and still the nonlinearity, unknown nonlinearity F like this, and we have a full observation of the state U. Then if the unknown C belongs to those space, as well as the unknown source term and initial data, and the state space is chosen like this, and the unknown nonlinearity in HS, where S could be this is greater than. This is greater than five divided by two, like this, then one can verify those conditions. Yeah, but I will not bother you by the detail here. So now we can look at some numerical results that I have in the first picture an unknown nonlinearity F, as always, in this case linear, and we want And we want to recover the unknown F and the state U as well as the source term. We approximate F by a neural network of three hidden layers. In each hidden layer, it has different neurons and with a tan hyperbolic activation function. And so now we look at the reconstruction results for the source term in the panel parameter. In the panel parameter here, we start from the initial guess, which is the green line here, and then we see that the reconstruction in orange matches quite well with the exact one, which is the dashed blue line here. As a consequence, the parameter error shows a decreasing trend. About the reconstruction. About the reconstruction for the state, we have here the exact state and approximated state, and the difference shows that they are not very far away from each other. So, for the nonlinearity F in the model via neuron network, still the initial guess is the green line and the approximation is the orange line in comparison to the exact one in the dash blue line. dash blue light and we see it matches quite well and as an overall effect the pde pde residual is decreasing in this panel in the bottom so in the second example we have a real a real nonlinear f which is quadratic term here and we assume to have only discrete observation of the state Discrete observation of the state U. In particular, we are able to measure a state U at three different time instances. Another feature of this experiment is we run it on three different equation. According to it, we have three different states here, a different equation of different parameter that we assume to be known here. Assume to be known here, and for each state, we observe only the snapshot at three time instances. So, for the first equation, we observe U at the very near time zero value and then at the maximum value and something somewhere in the middle. And so are for the other states in the other two equations. And we could observe. We could observe that the reconstruction nonlinearity in this case, still via a neural network, fits quite well with the exact one. And in this example, the difference is only that we have a little bit of noise in the observation data. But overall, the approximation of the state U as well as the non-linearity in the model is still accepted. Is not still acceptable if we stop the iteration a bit early. So now in the last example, we look at the nonlinearity F in the model, which is now not only the first order or the quadratic order, as we saw in the previous example, but also the cosine form. And we approximate this nonlinear non-linear. This nonlinear not only by a neural network, as we presented previously, but also we tried some other candidates, which are polynomial and trigonometric polynomial. And then we observed that, for instance, for this case, that the polynomial approximation works well for the first and the second order term, why it seems not to be the best candidate. Not to be the best candidate to approximate the cost function. And for the gigometric polynomial, so it's perfect for cosine function. However, for the quadratic term, it shows quadratic nonlinearity, it shows some oxygen here. And look at the first row by neural network. It seems to us, at least from a particle point of view, neural network. Particle point of view, neural network seems to be a universal approach in this problem. Okay, so let me just conclude my talk. So we study a problem where we have a PDE with several unknown system parameter or physical parameter as well as the solutions unknown. As well as the solutions unknown. Apart from that, a part of the model F is unknown as well. And we then approximate the nonlinearity in the model by a neural network. And we solve it in an all-advanced fashion to bypass the parameter to solution map. And we study two problems: the learning problem with all the unknown. The unknown parameter, state U and the neural network, as well as the parameter identification with the learned network in the previous step. And this could be seen as a special case of a broader context, which is discretization of inverse problem, where in this case, for the Lambda iteration, we could confirm that the iterate in the discretized level converge to Level converge to the exact one when the noise level tends to zero. Yes, and with that, I would like to thank you very much for your attention.