Alright, should I start? Okay, I put the URL up here if people want to follow along. We'll be talking about NHANES accelerometry and derivatives. This is going to work down. Oh yeah, QR code for ITED. You don't want to sound all this. Okay, so we've talked a lot about NHANES already this week, so I don't think I need to explain too much. But, NHANES now. But NHANES nationally representative study to assess health and nutritional status of U.S. adults and children. What's great is it's nationally representative and has survey wages and happens continuously. And there's a ton of data. So ideally we could make claims about the U.S. population. In 2011 through 2014, there was physical activity data, so participants Physical activity data, so participants wore this risk-worn actigraph for up to seven cold days. The physical activity data summarized at the minute, day, and minute, hour, and day level was released in 2020, which was great. And we did a lot with this. People did a lot with this. An interesting aspect of this minute level data was, as we talked about, that it was summarized with MIMS, which was a new measure at the time. We did some mapping, not we, Marta and John and some other people did some mapping using BLSA data, the Baltimore Longitudinal Study of Aging, between MIMS and then other measures that we knew about, like NMO. Measures that we knew about, like ENMO and MAD and activity counts. This is an older population, but there was still enough sort of variability in physical activities that we were able to create these mappings. And it looked like there was pretty high correlation between MIPS and activity counts. And then from this, could derive sort of mappings for the cutoffs that people typically use for MVPA. But to do this, we had to take the GT3X files that were processed. Files that were processed already by Actograph basically to get the activity counts because activity counts weren't open source and then also use that to create news. And so five years ago, almost to this day, was this talk for Rick Triano. I was not here, obviously. This is promises, pitfalls, and potentials for progress. Progress from NHANES 2011 to 2014. And so I think John can talk a little more about this. Basically, the promise was that there would be raw data from 2011 to 2014, which would be great because it would allow us to do a lot more than basically just the minute-level data. And so then a few things happened: COVID. John took a leave of absence for Hopkins and For Hopkins and deep learning made a lot of strides. And so then John came back for his leave of absence and I was early in like the second year of my PhD and I've been working with Chipperin on identifying individuals from accelerometry collected during walking. We call this fingerprinting. Here are sort of the fingerprints that we got from walking gates. That we got from walking data. And we wanted to scale this to a large population-based study like NHAIDS. But to do that, we needed to figure out where walking was happening, and we also needed raw, like, sub-second level accelerometry data. And so we thought, okay, if we can get the raw and HANES data, maybe apply JEP counting algorithms to them to identify where walking is happening, then we can do the fingerprinting. And in 2022, And in 2022, a couple things happened. The raw and HANES data was released. The activity count algorithm was released. And then we got some more data sets on walking with labeled step counts. Can I interrupt the screen and mean with fingerprinting? Yeah, so basically the idea here is: can you take someone's walking data from Excel Rometer and train on Like, take, like, train on certain individuals and then have unlabeled, as in, we don't know who the data is coming from, and predict their identity based on that. Yeah, I have this paper. We came up with a method that essentially like transforms the time series into an image and then takes predictors from that image and uses those predictors in regression. And it works really well. So, we have this paper, and then I have seen that block. C mounted with blockchain. Yeah, exactly. And it seems that it also actually works in NHANES, which is really cool. Yeah. So yeah, 2022, a bunch of things happen, and we have the raw NHANES data. So this was released. The data is at 80 hertz, so 80 observations per second. This is a massive amount of data. 80 observations per second, up to seven full days. Per second, up to seven full days of data, almost like 20,000 participants. So it's like, it's crazy. It's a ton of data. Luckily, John is a computational wizard, and he was able to download this raw data on our computing cluster. So, like, the data are in tar balls, basically, got them into CSVs, and then have all the raw data. So where are we? Now the data is publicly available, the raw, truly raw data. We have open source algorithms for activity counts and then also various open source step counting algorithms. And we have the computing power in the storage space to apply these algorithms. So we just did. We just applied all the algorithms. Did we just applied all the algorithms we had to the raw data? Step counting, we had there are five different open source step counting algorithms we found, mins, and activity counts. And then we looked at the results and we thought, what is going on? We need to back up a little bit. We need to see how these algorithms do on data sets with some ground truth, or at least as close to ground truth as possible. And so this is from 2020. Sodmeier said, larger and more diverse training. Said, larger and more diverse training sets probably will be necessary to make transformative progress. And this is still probably true, but the deep learning improvements in the last couple years have made this so maybe we don't need to train exactly. And so this is one of the open source step counting algorithms. This is from the UK BioBank group. Basically, it's a self-supervised learner. Self-supervised learners, where 10-second epochs are classified as either walking or non-walking, and then peak detection is done on the segments that are identified as walking. And so we applied this algorithm and a bunch of other open source algorithms to data sets where we had some ground truth step counts. So we had these three data sets that had either a phone like video recording steps or Recording steps or force sensor like in the shoe to get some ground truth. A limitation is these are pretty small data sets, like around 30 to 40 people, and mostly healthy individuals. But it was great to have some ground truth data and some of these data sets, they weren't like fully in the lab, so they were either simulated free living or like an hour of actual free living with a camera. Free living with a camera. And so we applied these five algorithms. I won't go into too much detail here. And basically, we found StephPound, this self-supervised learner, was pretty good, had the best F1 score across three data sets. See, just plotting some truth versus predicted steps here: step count, SSL is this middle one. SSL is this middle one. So it looks pretty good in general. So after that, we felt a little better about being able to apply these algorithms in NHANES. Nonetheless, we got really, really different estimates in mean daily steps between the algorithms. So looking here and plotting the survey-weighted mean daily steps by age, each facet here is an algorithm. An algorithm, and we see that they're really different. So the step count, this step count that performed the best in the label data is here. So the dotted line is at 10,000 steps. So like mean steps at like 40 to 50. In the U.S. population, it was estimated at 10,000. But we do see a fairly similar pattern, at least, in per year change in mean data. Per year change in mean daily steps. So all of the algorithms show decreasing steps by age, which we would expect. And then we also see a pretty high correlation between the methods. So this box here is all of the step counting methods. So correlation between like 0.7 and 0.96, 97. And then we also see there's somewhat high correlation with activity counseling. Somewhat high correlation with activity counts and MIMS, like between 0.9 and also 0.7. And we also found that the step counting algorithms were more predictive of, very predictive of five-year mortality and even more so than activity counts and minims. So the pink are any step variable. This is cross-validated concordance. We also see a dose of We also see a dose response relationship between steps and mortality. So there's a decrease in your risk of mortality until you hit a certain amount of steps, and then it looks to level up a little bit. So, take-home from applying all these step-counting algorithms to NHANES is that the steps from different algorithms are highly correlated and also highly predictive of mortality, but very different in absolute value. And so, in some And so, in some sense, we think steps might be more interpretable to the general public than activity counts ordinance. I can tell you you should take 10,000 steps, you should increase your step count by 1,000 to reduce risk mortality. But that really is only the case if we can better define or harmonize what a step is. We still need more large open data sets to train on with some ground truth. And I think we still need the And I think we still need this last piece of mapping. Maybe we don't, if we don't have this open training data, we need a mapping between steps from these open source algorithms, true steps, and the Apple or Fitbit steps that people are getting from their consumer devices. And so this is what we're working on now, and that's hopefully what some of you are contributing towards by wearing our devices. And then last, we're really proud that we created this data set. Proud that we created this data set. Again, it doesn't have ground truth, but it has a lot of other things. So we made available the minute-level data from all the different step counting algorithms. We applied also MIMS, activity counts, MIMS, log activity counts, the where predictions from NHANES, and also the flags from NHANES. So any like quality flags. Summary. Summary. And so we hope that this data will be used for functional data analysis. It's cool. The steps are counts, so it can be cool for non-Gaussian data. Physical activity research. Anything else I want to do that? Yep, okay. So over the next one. Yeah, I mean, so I feel like I'm beating a dead horse because I am not a step person. So, do you think that it makes more sense to, you know, hone our resources into figuring out kind of what a step is so we can tailor algorithms because it's closer to what, like, what we can translate to a Jeff Pop than MEMS? Is that like your point? I was just trying to figure out, like, or like just do both in tandems. It just seems like from a like. from a like a like a um you know a regular joe perspective it would make sense to put more of our energy into figuring out like how to quantify steps and like standardize our algorithms for steps i don't know i would love to hear what your opinion is yeah i think so i think i mean also it's interesting that there's not a trade-off as far as prediction of mortality so if like steps are both more interpretable and potentially more predictive And potentially more predictive of mortality. So I do think it makes sense to focus on better harmonizing and defining steps. And I think one of the crown jewels of NHANES is nationally representative versus all of us. And so we need to be, well, the ideal in some regards for NHANES from our perspective is that we want to be able to get a nationally representative thing to map to. And we can't do that, like, we don't have a 5-bit mapping to active right now. Fitbit mapping to ActiveGraph, and that's what we're trying to get at so that we can define you said Fitbit this for this age and maybe age sex kind of breakdown. You're in the blank percentile. Like, you're beating your peers, you're falling behind your peers from an activity perspective. That's what I was thinking. I was like, well, like, because our devices are different and we don't have the algorithm to fit that, we can't even really compare across studies. And so I was just trying to figure out, like, as a community, how. Out, like, as a community, how we do this. Sorry, I'm not asking. That's why we're trying to make a bridge, right? Yeah. How are you making a bridge? Okay. We're like, wear an IDF and try it, see what Apple says on the same routine. Yeah. I think there is a lower-hanging fruit here. Portal Abd. That was never meant to be a step count, right? Right? That was meant to detect walking bouts that are clean enough so we can characterize them for other things than just total volume of water. So if you guys run the attempt over the entire enhance, you should have atop just the label, walking or not, the magnitude of acceleration during walking and cadence, the period of strike. Period of strife. I wonder how that would look like in predictive modality. Yeah, so we are looking at cadence from both ADEPT and the other algorithms. I think we are really interested in seeing what that looks like. And then we also are using the segmentation from ADEPT for the fingerprinting, which seems to be working really well. So cadence is nice because it's steps per second or steps per minute. Steps per second or steps per minute that people understand. What is more elusive is this like the magnitude of acceleration during walking. However, I find it to be more robust than cadence. Can you explain like the magnitude? I'm sorry. Yeah, yeah. So if you have a raw acceleration signal during walking, right? What is the square root of just the energy of the signal that is? Just the energy of the signal, basically, right? So it doesn't translate intuitively to any type of movement, but you can imagine that the faster you walk, the higher the acceleration will be, right? And it seems to work quite well as a proxy for game speed. It's just it has, yeah, the units are still Gs, so it's it's it's less intuitive. Definitely question. Definitely question. Yeah, but upon reflection I think it's better for the baller than for now. So I'm going to move on. This is great. Thank you. For editing the steps at minute level, it is necessary to use the second level data? The sub-second level data. Yeah. So all of these algorithms. Level data. Yeah, so all of these algorithms work on it, it has to be like at least 10 hertz. Yeah. When you guys run a deck, you just use the patterns that were embedded in the work well. Well, you saw there.