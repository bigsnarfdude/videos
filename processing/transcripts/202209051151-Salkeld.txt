Session is William Salkheld, and he will give an introduction on rough mean field equations. Sure, thank you. Thank you to Harold and the other organizers for inviting me here. I don't know that a rainstorm has just started here in Rhode Island, and it feels like it's very loud on the windows and ceiling above me. So I hope there's not too much. So, I hope there's not too much feedback on my microphone. Yeah, I'm going to be talking for about 20 minutes about rough mean field equations. I'm aware that I'm the one thing between everyone in the in-person audience and lunch. So, I'll try to keep sharp on time. And this is joint work with my former supervisor, François Delarue. I am currently based at Brown, as I said, in Rhode Island. So, not too far away from Canada, relative to like Germany, for instance. I'll just click. Yeah, so the central idea of this talk is that I'm going to be describing a regularity structure that relies on That relies on a Taylor expansion using Lyon's derivatives, that is, derivatives developed by Pierre-Louis Lyon. And I just include this slide to make sure that there's no confusion with Terry Lyons. And so the Lyons derivative quantifies an infinitesimal perturbation on the space of measures based on infinitesimal variation. Infinitesimal variation on some linear space of random variables. What does that mean? So let's suppose we've got some functional on the Vassasine space of measures and some measure representing a point at which we would like to take a derivative. Well, this measure can be lifted to a random variable on some probability space. And we can consider the canonical. The canonical lift of our function functional lowercase f to capital F, which is a functional on the space of all square integral random variables that has this property that when we evaluate the random f at the random variable x, we get f at the law of x. This is quite a reasonable object to consider. Now, the thing about this Now, the thing about this canonical lift is that now this is a functional on a Hilbert space, square random variables, this is a Hilbert space. So there's a meaningful sense of Frechet derivative, which we don't have if we're working on Versa-Sein space. And the Frechet derivative we should think of as being a linear operator on this Hilbert. On this Hilbert space, so that actually, because you know, you know, Hilbert spaces are isomorphic to their dual, actually, we can think of the Frechet derivative as being a random variable itself. And the key insight that Leon was able to formalize is that this Frechet derivative is measurable with respect to the sigma algebra generated by the random variable x. Generated by the random variable x. Well, what does that mean? It means that the Frechet derivative can be written as a measurable function of the random variable x. So the Lyons derivative is this measurable function. What that means is that when we take a Lyons derivative, we are creating a new free variable over which we're going to be integrating. That is a free variable that we would substitute. That is a free variable that we would substitute in our random variable x and then consequently integrate over in order to see some functional of our measure mu. So let's look at like a simple example so we can see how the Lyons derivative works. Well, if we've got this simple convolution of some kernel by the measure, then the lift is reasonably simple to state and the Lyons derivative. And the Lyons derivative is just the, we remove the integral over the measure and take a spatial derivative in the variable over which we were integrating. And similarly, if we were to consider some kind of non-linear function on measure space, then we could still get a Leon's derivative, but now A Leon's derivative, but now you can see each of these terms, there's only one integral. So we remove, you know, the first case, we remove the integral over x and then take a derivative. This isn't terribly well written. This should be derivative in the first variable and derivative in the second variable. But yeah, you can see that each variable goes on to become that gets integrated over goes on to become a free variable. So there's a lot of A free variable. So there's a lot of nice symmetry properties going on in terms of this representation for measures on functionals on measures. Now, what we've done is we've probably got quite a nice grasp now on what is the Leon's derivative of some functional. But the question is now, what are the Leon's derivatives of a Lyons derivative? And we could do exactly the same construction that I described earlier in terms of lifting. Described earlier in terms of lifting onto some linear space of random variables, taking a Frechet derivative and projecting down again. And what we would get is that there are two terms that represent the second Frechet derivative. So the first is this sort of two Leon's derivatives, Lyon's derivative followed by Lyon's derivative. And this object has two free variables associated to it. And then we get the second object, which is a derivative in Which is a derivative in the free variable generated by the application of the first, Lyon's derivative. And what we need for the purposes of this talk is a way of representing these higher order Lyons derivatives, which there's been sort of limited research considering. The way that we're going to represent them. The way that we're going to represent those is with these objects that I call partition sequences. So, partition sequences are just sequences of integers that have the property that the first value is one, and the jth value can take values equal to any integers that have previously occurred or one more than the max of all integers that occurred. So, to give some sort of test. So, to give some sort of tangible examples to this, the set of these partition sequences of length two are the sequence one, one and the sequence one, two. And the partition sequences of length three get these five sequences. Now, the reason they're called partition sequences is because there's a natural isomorphism between these and partitions of an ordered set. Set. And what we are going to do is associate to each of these partition sequences a Lyons differential operation. So for the sequence of length one, we just have this single value one, and we call that the Leon's derivative. And then we define these inductively so that for the value aj, if this value, if the value aj has occurred previously. A j has occurred previously in the sequence, then that corresponds to a spatial derivative in the free variable that corresponds to the free variable generated by the first time that that value occurred in this sequence, which will be therefore a Leon's derivative. And if the value aj has not occurred before, then we take a Leon's derivative. So, to visualize this a bit more, we have You know, visualize this a bit more. We had these sequences 1, 1 and 1, 2, and we had the two, you know, the two Lyons derivatives and Lyons derivative followed by derivative in the free variable. And that's exactly what these are identifying. And these just represent a nice, concise way of capturing all of the Lyons derivatives that we're going to have in the associated Taylor expansions that we're going to be considering. That we're going to be considering, and the link to partitions turns out to be very natural. With this in mind, we can define these differential operators. So you can see here, given some partition sequence, what we're doing is taking the Leon's derivative, we then get a collection of free variables. Then we're going to have this collection of increments, and we're integrating over. And we're integrating over a product of the coupling between the measure μ, the point we're evaluating at, and the measure mu, the target measure that we're aiming towards. And we're able to write the Taylor expansion in this form, which is admittedly not complicated if you have to consider any specific term, but this is a Consider any specific term, but this is a concise way of writing down this object. And what we can see is that on the left-hand side, there is no statement about the couplings, but on the right-hand side, the choice of the coupling that we made occurs both in the Taylor expansion and in the remainder term. So typically, the choice of coupling is something that we make, but we pay for that in terms of our remainder term. Term. Now, what we're going to do is apply this Taylor extension using the same ideas as rough paths to our Lakeim-Vlazov equation, which we can see here in the top of the slide. So the first thing we'll do is we'll consider this functional f, which has a spatial variable and a measure variable evaluated at this time r. And we're going to turn this into a tailor. And we're going to turn this into a Taylor expansion evaluated around time s, which is exactly this term here. I've hidden the remainder term inside this approximate equals, which is fine for the moment. And this is just a sum over a specific number of Leon's derivatives that really is too important for the purposes of this talk. And we can see that we've got the increment of our solution x here and the coupling. And the coupling between x at time s and x at time r. Now, if we substitute the approximation that we have based on our increment x, on the time increment s to t into this formula, then we get an expansion that looks a bit like this, where we've got our first term as before, and then we've got this second term, and we're going to have other terms which I've hidden in the remainder just. Other terms which I've hidden in the remainder just to keep the notation simple. So we can see here I've got a collection of probability spaces which is being indexed by my partition sequence. And I've got all of the three variables containing the random variable x on probability spaces that we're integrating over. We've got this product of the diffusion term f evaluated on F evaluated on all these different probability spaces that again are indexed by our partition sequence. And we've got this iterated integral. And this should make us think a little bit about the elementary differentials that occur when one considers heuristic approximation for rough paths. With that in mind, I'm going to try and abstract this process. Try and abstract this process a bit. So, oh, yeah, sorry. So, the first term we're going to associate to a single node, which I'm just labeling i for the ith coordinate for the moment in some sort of d-dimensional setting. For this second term, we're going to have some node and a collection of nodes that are labeled and directly above it and are partitioned according to this. According to this partition sequence A. And then the natural question is that if we kept repeating this to get higher order terms, oh, we would get, excuse me, we'd get something that was more complicated. And the question is, how would we describe that object? Now, the way that we're going to do that in this talk is using something called Leon's trees. This is a definition I know is a lot, so I'll just go through it point-by-point. I know it is a lot, so I'll just go through it point by point so we can understand it. So, what we have here is some labeled non-planar forest with a partial ordering. So, we should already be thinking of something akin to a coned primer type setting. So, we've got a tree with nodes, edges, and labelings. Secondly, we've got some subset of the nodes and a partition of. A partition of all of the other nodes. So, this is the idea that if we include this set H naught, which recall may actually be the empty set, then we have this H prime, which is forming a partition over all of the nodes. And there is a specified tagged partition element, which here we're going to call hyper-edges, which is a reasonable. Reasonable given the literature, which may or may not be empty, which is sort of tagged and specified within the partition. And what we require is that this partition forms a one regular matching hypergraph. So that just means that this is a partition over the nodes that satisfies these three properties. Now, the Properties. Now, the interpretation of these three properties doesn't really matter again for the purposes of this talk. The only thing you really need to understand is that these properties arise naturally as a result of the Leon's calculus that we're going to be working with. And the key insight I think that someone should take away is that if we were working Working with some classical particle system, then there might be the naïve idea that we should work with trees and we're just going to label each of the nodes one to n. We're going to have some sort of limit as n goes to infinity so that the labelings on these Kohnskrimer type trees just taking their values in the integers. And the reason that that is wrong is that. Wrong is that we're going to be making some assumptions about exchangeability within our system, which is necessary to get the mean field limit. And that exchangeability means it doesn't really matter when you're considering some terms in this rough path expansion, which term you're considering. All that matters is that term relative to other terms. And that's why we choose to use. And that's why we choose to use partitions and not labeling everything. Now, given these abstract symbols, we need two objects sort of motivated by the ideas of rough paths. So the first are going to be the elementary differentials. We've got a technical definition on the slide here. And this is just a generalization of the diffeomorphism. The diffeomorphisms that occurred when we were doing the Taylor expansion of our McKee-Blazel equation, because we can see that we've got this single, this is just the rooting of the empty tree. So like a single node, this is the simplest term in our expansion. We can see this is just the diffusion term coming out of our Makim Blazov. And then for some more complicated tree, we're able to define this in terms of the tree. Define this in terms of well, what's going on here is we have the root, and then we're considering all of the trees that need to be coupled together and then rooted in order to get the tree T. And this would be exactly the same if we were doing things in classical elementary differentials, except we've now got this partition sequence that is partitioning up the free variables generated by the Generated by the Lyons derivative and the tagged variables of each of these individual elementary differentials. You can see here there's this function fractor H, which is just coupling up the variables in the correct way based on the partition sequence A that we're choosing here. So this definition is a little bit much, but this is just exactly the same intuitively as elementary differentials. Intuitively, as elementary differentials, but with this extra structure that is going to be corresponding to the hypergraphic structure. And the second thing that we would be thinking about is we've got these iterated integrals. Now, appreciate there's probably a lot of rough path experts here. So to define my terms of my rough path in terms of to be equal to the iterated integrals, I appreciate is horrendous. But just for the moment, let's assume that the driving signal is. That the driving signal is smooth, so that we can give meaning to all of these integrals. Then, what we can see is we've got this, we're considering some tree, and then we've got all the nodes on the first level of the tree somehow, and then they are partitioned up according to this partition sequence. And what we can see here is we've got some iterated integral. We're integrating with respect to the driving signal. The driving signal evaluated on the tagged probability space. And then we've got this collection of sort of subterms of the rough path that are coupled according to the same function here. And so we can see this as a generalization of the construction that we were obtaining heuristically. And And the perspective should be that in general, what we actually want is to be working with a group of characters on some Hopfalgebra. I'm not going to talk about that too much in this talk, but what we additionally require is another property, which means Property which means that this is in general not the group of characters but a subgroup that we call the Makim-Vlazov group of characters. And for each mean field equation, you'll typically get a different subgroup, which is really interesting insight. But yeah, we can see that in general, our rough path is going to be some path on a group of characters of some hopped algebra-like object has this nice chens relation and satisfies this. Relation and satisfies this regularity condition. One, I mean, there's a lot of sort of integrability and regularity details going on here that I would say are what you would reasonably expect them to be in the rough path setting. The key point I would highlight is that what's going on here is we're integrating over all of these additional probability spaces. These are somehow the d-tagged probability spaces, so that when it comes to Spaces, so that when it comes to regularity, we evaluate the tagged probability space pathwise and all other probability spaces mean field in mean square. Now, given the sort of perspective that what we're actually interested in is going to be some mean field rough differential equation, as I've stated here at the top, what we should be thinking is that we're going. What we should be thinking is that we're going to consider an enhanced solution, something that will look a bit like a controlled rough path. And the terms of this controlled rough path are going to be the elementary differentials that I described earlier, where with the solution substituted into them. So we can see here, I'm considering this is my enhanced solution. I've got summing over. I've got summing over a collection of elementary differentials. We had this tagged spatial variable where I've evaluated the solution on the tagged probability space. We've got this measure variable, which is the law. And then we've got the collection of all the d-tagged variables. And then I've substituted in some random variable that is equal in distribution to our solution. At some point T. And the beautiful thing that we would desire to establish is that this enhanced solution satisfies a jet property with respect to the probabilistic rough path. So, what's going on here? Well, we've got our enhanced solution evaluated at time t. So, we should equally, and then this. This is a co-product counting function, which doesn't really matter too much, and this is a sum over all the terms in our abstract Taylor expansion. And then we've got our elementary differentials evaluated for some higher order terms, some larger Lyons tree, multiplied by an increment of our probabilistic rough path plus some regularity term. So this is, this is, you know, formally, this is a jet. Is you know, formally, this is a jet. This has all the same properties that we would desire if this were classical rough path setting. The challenge is, as I've highlighted in red, we've got this expectation over a collection of d-tagged variables, and we've got these coupling functions phi and var phi. What these are doing is if we observe on the left-hand side, for any particular elementary differential that we choose. Elementary differential that we choose, we're now considering an object that is on some product probability space because we're evaluating this pathwise on each of these detagged probability spaces that occur on the left-hand side. And what we need is a language that essentially says the correct probability spaces that occur on the left hand side. Probability spaces that occur on the right-hand side that are not tagged to something on the left-hand side should be integrated over, and anything that is tagged to something on the left-hand side should be evaluated pathwise. And that's exactly what these couplings are doing. And all of the coupling properties come from very nice, well-defined, reasonable operators on the hypergraphic structure. On the hypergraphic structures of the Leon's trees. So, this is very natural to see that all these properties come out of the Leons calculus that we're working with. And I think I am basically out of time at this point. Thank you, everyone, for listening to this admittedly very brief talk where I try to summarize things in a loose fashion. I hope that you understand there's a lot of material I could have talked about here. About here. I'll pass over to the audience if there are any questions. Thank you. Thank you. What is a couple of hop algebra? Yeah, that's a great question. I love talking about this in exactly one minute. So a Hop algebra is a module that's paired. And a module that's paired with a product, a co-product, and an antipode that satisfies some nice commutative diagram property. Now, the central to that definition is that the co-product maps from the Hopf algebra into the tensor product of the Hopf algebra with respect to itself. Now, the tensor product of this. Itself. Now, the tensor product of this of Hopper algebra and itself is a very natural algebraic and analytic object. In this setting, you need to capture all of the extra information that corresponds to the choices of coupling that are going on between all the distributions. That means that you need extra information. So, you need to map into a richer space that carries That carries the capacity for extra information. And so the coupled tensor product is something that satisfies, it is essentially in one sentence, it's just a larger space that captures extra information. The coupled Hopf algebra is one that has the same commutative properties, but when mapping onto this larger space. Is that a classical notion, or did you invent this? I had to develop this, yes, with Francois, yes. Okay, thank you. Yeah. I have not come across this in any other works. And to give an example to sort of motivate it, suppose that you were doing things, Kohn's crimer, but the labelings took their values on the index. But the labelings took their values on the integers, then you could write down like a hop algebraic structure for that whole setting. I mean, it would be complicated, but you could absolutely do it. And then suppose that you constructed some kind of quotient that then took labelings and mapped them onto partitions so that it no longer mattered how things were labeled, just that they were either the same or different. That's, you know, again, That's again quite a reasonable quotient to think about if you're just working with the module. But when it comes to looking at the co-product and the product operation, that quotient gets a bit messy and you no longer get a co-product or and what the object that you get instead is the coupled co-product. And that's because if you And that's because if you think about it for a moment, if you consider some large term and then apply the co-product to it, so you get a sum over smaller terms, it's quite reasonable that if you have all the labelings, maybe two components of this two nodes of this tree might be labeled the same. But when you chop them up into a pair via an admissible cut, suddenly, as a pair, you can't see that they are the same. So you need an extra piece of information. So, you need an extra piece of information that tells you how those labelings are coupled. That extra piece of information requires that the space that the co-product maps into is larger. That's the intuition, anyway. Okay, thank you. So it's like you have to communicate, the left and right-hand side have to communicate across the tender product. Is that it? Yeah. Yeah, yeah. Yeah, thank you. Yeah. Yeah, thank you. All right. I think it's time to thank the speaker again and all the speakers of this morning session. 