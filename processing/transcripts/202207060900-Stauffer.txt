Okay. So this lecture, I think, probably is a less complicated lecture of this series. So there is a fairly nice, so we're going to tweak the details mentioned yesterday, but there is a very nice way to do it for this problem. Before I forget, I forgot about the two days. Forgotten two days. I have lecture notes written about this in my website. Everything I'm talking about here until today is in the lecture notes. It's still in a preliminary form, but I have this optimistic claim of finishing my summer. Because this summer, I'm making a commitment. Speak recorded. To finish in the To finish by the end of this summer. So, hopefully, I'm going to. Okay, so just a quick recap: I wrote the definition of a bad box from last time, just to keep us remember. So, it's composed of two ingredients. The first ingredient is just that a box is bad if it contains too many disjoint bad boxes of smaller scale and too many, for example, maybe two. And then there is another definition that controls how bad bad boxes can be inside a bigger good box. That's the thing Vano didn't like and wanted to be clever and get rid of this, but usually we need some kind of control. And for our problem, even if you manage to get rid of this, it doesn't improve anything. Can you explain this tech on the please? Yeah. Yes. Please. Yeah. Yes. The problem is you know here. Sorry. Do you not have an explanation? Our what? Do you need a do not? Yeah, move more. It's a node. It's a node. Yes. If either of the tools, one of the two events. Tools, one of the two events happen, then the box is better. And here, in some boxes, say okay, and the second event tells me that I have to look at this model box inside, the payment Amazon box inside it. And in each of this box, I have this path that the infection would take. And these paths cannot move more than ballistic. If they do move more than ballistic, then I call it the bad box. The box met. Okay, before entering into the actual dependence, let me just give a little bit of more insight into the way this thing is working. Okay, so I thought that we defined the higher scale in this way, but a little bit out of the box. That's a little bit out of the box. I've got in this recursion and so on. But with this recursion, you could estimate the probability that a box of scale k is bad by just doing the union bound of always you can choose two box inside and the probability that both of them are bad. And these are estimated with the probability as if they were independent plus an error term where the error terms include the dependence and the second events there. The second event is that that's what this error is. And then we just, with this, we completely continue this recursion and we've got an estimate that would be like row one two to the k minus one c two to the k minus one and c didn't depend on l row one depends on l L rho one depends on L. It will take my L inside of the scale one box large enough so that this I'm the sort of space, and I get a double exponential decay on the probability that a box is at. So this whole machinery has to present this and see almost like algebra. I just introduced these things, computed the recursion, and somehow, miraculously, I got a better estimate for the probability that a larger box is better. And I told you that the better the estimate you be, there is lower your lower scale. Because there's lower and lower scale, my estimate doesn't change. But on the other hand, my estimate doesn't change, but LK grows in slower, so it gets more and more similar to 2 scale. For example, if I were able to grow my scale. Able to grow my scale of explanation fast. Able to set Lk like this so that we just see dk minus one l1. But if I were able to put this, then my estimate here to be exponential in something that looks like okay. It's also exponential. That's close bad and bad estimates. That's a smaller and smaller term. But why can't we do these small? The reason is when you go to the speed of a box. When I was doing at a box, same as the origin, let's say I'll stay okay. This is the box of the origin, so zero zero box. Larging zero box. Then we said if this box is good, what happens? Well, the infection spreads linears through good boxes inside. It's a good box, so it can find bad box at smaller scales. Maybe the bad box are smaller scale period. When the infection gets then, it may go back because they don't know what's happening there. What's happening there? But then it keeps moving forward because they only find good box of smaller scale before. And the estimate we got on how far away the infection could go was this one, right? The amount you managed to go to a smaller scale box times the number of smaller scale boxes you have to traverse, which is the ratio of the two time intervals. Of the two time intervals minus a constant which accounts for how much how many bad boxes you can encounter. It's one plus some that intercepts it, which don't give you the speed, and minus again the damage that this black box can make. And if you look at this equation, what you get is a first term here where the s k minus one can. Where the S k minus one cancel and get a term that depends on s k and the other terms will depend on s k minus one. If I grow my scale exponentially, s k and s k minus one have the same order. Right? They are just exponentially. So my bounce on the stay okay for the speed gets a penalty that's a constant. And then at each time And then at each time I'll lose a form step, then I'll end up in Skitzy. So I need to improve my speed and get a panel that decreases with A in a summable way. And that's how we need a scale that grows faster. And intuitively, pictorially, what you're seeing is the number of bed boxes is the same. It's one plus some neighbors. But the amount of space here is so big that I have much. Is so big that have much more wood loss to compensate by the loss of that single red box control. Say you're taking the CO2, so okay. Do to the K. Yeah, so there's a double exponential. Oh yeah, you put it. When you just pay the price. So this model second row is spin because it needs to be sellable in K. So I need to. To be summable in k, so I need to get a term here that's one over k squared so that I can sum it. That's it. So, k is a slower, slowest I can go. I do some more power here because I do other things, but k to actually only k is not enough. You need at least a square. You could grow faster. You could grow faster and have even more and more good box against this single bad box. Against this single bed box. Of course, you cannot go super fast because you start losing the combinatorial term. So imagine now that just to get it right, when you enter the bed box, okay, so this bed means that it's doing something unpredictable, right? So. So it can shoot, you know, in a short time, can shoot to the end. It cannot, because if it does, that's the second event. If it does, a big box is better. This box is better. So the event that a box has to be. It has to control how bad a small box is. Sure, how bad a small box inside can be. It can only shoot according to scale k minus one. Yes, and that's it. That's the worst. No, no, that's okay. I was very pessimistic with this bandistic scale that could do something better, something between power enough and one. That would be enough, but that's an improvement balance. Because I would improve this guy, which the penalty I get from traversing that box, but I still... Travers in that box, but I still have to discount the speed that I don't get, and that's the guy still has that screen. It doesn't matter if I do something small. I always lose this concept. Maybe a quick question. You say several times, you can get better bounds if you do this and that. But are these bounds in any way sort of nice or are they always quite extreme? They they are usually off. They are usually off. It's very rare that you find a Multimodal that deals with the right bound. I have one result, but yes, it's not in this framework. You think it's only one and you have a bound of a million. No, here you lose a lot already because the wave of ruins are still so large that this to the k minus one is. One is very small comparison to LK, which is just the diameter of the box, not even the bottom. So you lose naturally because you're kind of doing some rough control mistakes. But there's no point in saying I want the best possible buttons today. That yes, can be used. So going back to your question. Going back to your question. What if I just take this curve and now I change the way I define this? I do what actually in most papers you see, which is say Lk is equal to Lk minus 1 to some power bigger than 1. So this becomes double explanation that alpha to maybe k minus 1. And here you already see a limitation, but if you go there much, okay. That large. In k, your computer toilet term, we have alpha to k minus one. But the best you can hope for this recursion to give you is, because if you forget the format of time, forget the dependence, the best you can hope for is row plus two to the k minus one. So the idea is alpha sweeps more than one. Any alpha bigger than one blows up the unit function. So you cannot control by the unit balance. Cannot control by the union bound box. So, in some situations, you may need to grow very fast because you need an abundance of wood box to kill the effects of a bed box. It happens. And sometimes people grow like this with an alpha that's even big, like 10. But then I have to go to a definition of bad box and say, you know, I need not two destroyed by the box, but 11. And then we will replace our two by 11. And then we will replace our two by electron. And you hope this doesn't tell you that having grid to grid is not hope that this works at some point. Okay, so I think that gives a little bit of an idea on how things are set. There's another thing I want to say to give you a pictorial image of what's happening with this motor scale. Let me do this in the same drawing because still it looks psychometric, but actually it's really helping to control where bad boxes are. So suppose I have a good box at scale. A k box. And suppose it's good. So inside there are k minus one box. That's a distillation. Let me draw a little bit. We draw a little bit of pain and drawing them not to the left, but a little more. And I say if the QK box is good, then I have at most one of these at the bed. Maybe it's this one. Maybe it includes some of the light box, maybe it does not. That's imaginative. That's the only one that's bad. The other ones are all good. Are all good. But good means what? It means that they also had their own box inside. And since they had schools, they had the most one box here with some intersection that's bad. So we start finding this small piece of bed everywhere. No, this is a good box. And then inside again, we're going to test it again, and we're going to, since they are both, they have one bed box. They are both they have one bed box and so on. So, what this machine is telling you is that by defining the bed blocks properly, what you find is that the bed box has spread out. They start to spread out inside this wood box, if you zoom in. And if you go up to scale one, you'll see one, or scale two, you'll see one bed box inside of scale one in a large region of wood box. But it's always like this, in no scales. Enormous scales. So it really spreads out. And if you look for bed box of larger dimensions and larger scale, they're even better. So that's what this machine is telling you. That's giving you a way to control how spread out these dead boxes are so that when you traverse a region and you find yourself with a dead box, you find a lot of good boxes around to compensate. That's a beautiful hair in mind for the pictorial way to see this. Are we ready for the difference? So now we want to show that if I distribute, or if I take two black bars of the same scale, I want to compute the probability that both are simultaneously split. And I want to show that this. And I want to show that it's close to the square of the problem. This will be done with one period, and I call this a local mixing. That's what it's happening. So we think of the same thing. The setting. So we take a box, and this is just a box in space. It doesn't need to be one of our boxes in the remote scale. So let's give it a label that's not new. And suppose I have a set of particles inside. They are not in stationaries, they are not a Poisson process, just a set of particles distributed in an arbitrary way. About every way, right? Almost every time I tell you how to distribute. So let's call zero set artificial particles. And then we're going to actually follow. I'm going to take a let's give numbers in this here. So this the side left here is going to be big R, and I'm going to my big big r and i'm going to dessellate this box into a smaller box of size a little geometric set and i'm going to ask that my initial set of particles is zero i don't know the distribution it has but That it has, but it is it satisfies a density condition. Suppose I'm given that for each of these sub boxes, for each sub box is 0 and at least some number, which is like against it, after the d particles. Articles so for each sub box here, I'm told that this number excitement for each of them. Yes, they have also, and I don't know how the particle distributes, they could be all here in the corners, for example, that is bad, it could be about. Very bad, it could be well spread. I don't know, but I'm guaranteed that I have at least this density. Better supply. Participants, you don't distinguish susceptible. No, no, just at the moment for me, it's just particles. Here you can even forget about everything else. It's just some particles, they don't have tight, they just place them. They will be our particles because they don't. So, my zero here stands for time, the particle that I start with in this book. What I want to say is: if I run this process for a while, that is, I start time, I let these particles move independently, I simply run them off, they will start move here. The beginning, nothing much happened, but when they manage to go around the whole distance of the box, they will start mixed inside the box. Inside the box, if they go even further, there was something mix with other particles. And I hope that after a while, the configuration file in the book will look like a post-component process. That's the statement I want to make before. So let's go one by one. So how much time do I need to? Well, it depends. Let's fix a parameter first. It's epsilon. Excellent will be the precision. How much close to a point approach? How much close to a point close I need to get? And now we fix time. How much time are we going to run this process called time delta? How much time I need to run this process? I need the particles to run at least inside its own box because since they can be habitually distributed here, I could put all of them in the corner. So they need at least this time to start. So they need at least this time to start forgetting the initial configuration. So I need at least r squared, left. And actually, I also have to pay the price for how much precision you want in your estimate. And there is a constant. We don't care. Okay, so we have epsilon. I'm going to run, let the patterns run for this amount of time. And then I want to say that in the And then I want to say that in the boat something nice happens. But how much in the boat it should be? So I'm going to put a boat here, which we can call K prime. And I don't need to specify the size, I just want to specify the distance to the boundary. So how much far from the boundary does this have to be? Well, essentially, it's the amount of time. Essentially, is the amount of time that a particle from outside would, not the amount of time, but it has to be large enough that a particle from outside in time delta doesn't enter. There is no particle outside, I'm not considering particles outside. So if a particle that could be outside don't enter in time delta, it means I don't feel the effect here that I don't have a particle outside. So that's the distance that I should have here. So that should be like root of delta. Like root of delta at the root of the time. Sorry, I'm confused. So the dynamics is in so the particle inside they can go out and react. Those they can. And outside, you don't know what particle configuration. I just think there is no part outside. But there could be. There could be, but I'm not looking there. I need this distance to be fast. I need this distance to be far enough so that I don't feel that there is no particle outside. So it has to be the amount of distance that a particle cannot traverse the time delta. There should be rules there. Of course, you have to pay something for how much precision you need. And it turns out it's a lot. Okay, now that we said everything, what's the theorem? The theorem is the following. Let psi be a Poisson point process of intensity one minus x one. No matter the steps of that put in the whole spec, it doesn't matter, but I can do in the public. I just throw it, pass over and close it. Then there exists a couple new. It's called new between the point process and let's see if this is clear the motion of the particles zero from zero to delta the coupling between the Between the randomness on the path, the random path the particles will do from zero to delta and the randomness of the pass-off point process, it does not depend on the actual configuration of zero, just the motion. From delta delta, such that probability according to the coupling. probability according to the coupling that the particle let's call the configuration time delta p delta let's call this k prime the configuration of particles at time delta inside k prime this contains psi with probability at least one minus e to the minus c out okay this c depends on delta with no f perhaps Perhaps I have some let's prefer first. So I throw a possible point close in. So I throw a Poisson point cross inside the here. And now I move the particles in a couple way with the Poisson process, such that after time delta, the configuration of particles here will contain the point process. So each point of the point cross will have a path on top of it. And this probability going to run very fast with little arc, the B, which is the size of the small bus. And I lose a little bit on the dense, right? When the point close against one minus absolutely. So I pay a little bit for the next. So in one hand, it's a very strong statement because you really show you can really cut, you can do this even in the other deep, you can still couple the names. So that it contains. So that it contains the point process with very large probability. And this I call this a local mix, it's not exactly a mixing, but at least it shows that the particles inside stochastically dominate the force on point process with a small side small. For independence, don't you need also something in the other direction? I want it because my brand is monotonous. So my event, the probability of. So my depending, the probability box is bad, it's actually monotone. So the more particles I have, the less likely to be the bad. That's a good point. Because you want us to find one there. But the other direction is also true. I could put another Poisson with close of the next one plus epsilon bit, and the couple should dissemblish the path between these two Poisson processes. Another couple between two Poisson processes. And a couple between two possible process first, the motions, and then the couple to the side which is the same couple the same the same couple, but we have the next remnant, so it's the other okay so. So, this kind of argument, as far as I can tell, was first done by myself analysis in Green, 2010, in a paper we never published, but it's in the archive. But it's in the archive. And then we first used it in another paper, and then it was published. The first time it be published is a paper with Perez Anste Perlasi Seloff. But the proof I'm going to present today is not my proof, it's actually a proof by Abust Tesher and. Abust the Shira and Selvehov, who actually generalize this, and then after where we wanted to put this a very nice framework, which I probably will manage to present today. And then several people use this to prove what they call the coupling equalities for the lace limits and levels of calculation and so on. Propulsion, so quite of a part of technique, especially if you if they get some other part here, we look at what I define as percolation time, it's a dynamical percolation process with very strong dependence, and that's the only most character that I think. But that would say. But that boot scale is completely different. So there's no such structure. There's no such mode scale normalization. Okay, so before I prove this, let's try to use it to determine things. So control is actually scale two, but the other scales are actually affected. Manaholt has scale two. And in the compensation of the probability that this box is bad, I say that there exist two bad box of scale one inside that are both simultaneously bad. So there are several possibilities, these two boxes I choose can be organized. The easiest possibility is if they are in the same time interval. So this is time. So they are in the same time. So we are in the same time. Well, because we are the two boxes at the same time, a simultaneous bad. We know by stationary distribution just for some of the particles. But they are far apart. Each box, remember that they only look at particles that start on the base. So these are disjoint regions of my Poisson process, so they are really the galaxies. They have to read the events. I don't know anything in this space. No, but they have these joint boxes. So, actually, I'm not even allowing it to touch face. They have a distance of L. This case is good. Unfortunately, that's just the first case. That's just the first case. The second case is if they have not the same time interval, but time intervals that intersect. Sorry, why the distance is at least siloable and not just continuous? No, but these are counts as overlap, has intersect. It's not this job. They intersect at the face. But they could be another. No, but then the next one is L over the next comp. Comes that's just a technical time just to force them to be actually in this regime that's more important. If the time intervals intersect, but they're not the same, so they intersect here, then I want to do the probability that both of them are bad. Here, in this time interval, I have also. I have Poisson just by stationary, the first time interval of the two box I'm considering, so I can say that here I have Poisson. Here, given this, I cannot say that, but what I can say is that I can take a region at the same time as this one around the first box, a little bit larger than the box, a little bit larger than the box. And I can force the configuration here. I can force the configuration here to come from this region. I'm going to lose density with this. I can just say, look at all parts that arrive here, but which were here before. I'm losing density. But I'm going to lose just a little bit. So what I didn't do yesterday, if I were more sedentary, I should have done, is that the definition of the event, I'm not going to look at the actual density mu, but it's a one minus epsilon mu. But it's a one minus absolute. So that can allow us to lose a little bit of density, and it needs to be important because even my local mix lose density. But for the whole argument, maybe you can ignore this. It's just not super important. But that's the way to treat where the time is similar. You just say, I'll look at this set of particles for this box, and this set of particles. This box and this set of particles for this box. Of course, these are non-independent, right? But I need to show that I don't lose much density here. And that's the delta in the definition of S1 with 3 to 1. This goes back to a question of size. Because now this distance, the distance between these two parts is of size. The distance between these two boxes is of size L1 by 3 at least. So I can make this guy go halfway through and put even all the way, just avoid it, just need to be disjoint from this one. So the particles here move a time that's of its length S1, but move the distance here is of order L1. But S1 is delta L1. This delta of L1 square. Okay, L1 square allows some particles from outside to come inside. But because there is a delta, this time is a little bit shorter, so they pay a price that's like delta to do it. And this delta absorbs in the loss of the debt. But this is still not the hardest case. It's not going to do first about. Because I didn't do anything, I didn't do that. Anything, I didn't use that. Okay, I was convinced that we can treat a stoop with a part of a easy guy. Now the difficult part is when this guy has a timing interval that's really disjoint from the other one. And we can even take the worst case and make it under it. Let's make it happen. Now I have a problem, but the part that arrived here are always influenced by the capacity. There's no way now that nothing will come to something bad. We're going to lose that in a non-controllable way. So, what we do is the following. Let me give the name to the events that. Give the name to the event that this guy is bad and this guy is bad. Let's call this guy B1, the event that this one is bad, and this one's B2. The event that they are bad. And I want to compute probability of P1 and P2. So what I do, I add an event here. I add an event. It can be in the middle, but it can be very close to this guy. I pick a region here. Pick a region here and I'm going to add an event here that I'm going to call V, and D is from the density condition. So I'm going to tessellate this region in space. I map here, let's say, a box in space at this time. I tessellate it, and I'm going to ask that each box inside has at least some number of particles. So each box here has at least which. As at least which number of particles, something close to what I want. They don't take the epsilon over two after the part. And R here for me is going to be the division of this guy. I'd like to write it there. So I'm going to put here a box and I'm going to ask that this event is satisfy. That this event to satisfy form a particle system, and then we're going to just write this using this event. And I say that the most probability of the one intersection, the two intersection B plus easy. This is easy. This is channel. It's a margin, so it doesn't have anything else, just channel process. What set what are you applying turnout on, exactly? Oh, the turnout on what particles are you applying it to? Just the ones at the base. Yes, this doesn't apply anything, but it's for all parameters. It's all part of the idea. Now I'm going to still work with this one because under the density condition, I define the I rate that for the moment that's continuity. Let me write one step before I continue. So this is going to essentially the moment. These were essentially the most probability of B1 times probability of B2 given V1 and B. And I don't need to write the term with B, but then I say it's at the most one. It's going to get a probability of the guidelines and the most function. And there is still a probability of D equal to one. D even v1, but I don't need to do that, just one for me. The important thing is that if I look at this condition, I have an event that depends on this region, obviously, an event that gives me a condition here. But given the particles here, I can just use the Markov property, say that the most that they will do on their way up is independent of. Way up is independent of atomism. That's what we do. So we introduce another event with just the event that the coupling works. With this, I can apply the coupling. Let's call Ln. It's that the coupling succeeds. The company of the local leader. The company of the local mixing. And then with a cross probability of B1, probability of B2 and the coupling, and here we can pay the price of LM of the curve. I'm really not the time to sense too basically. Yes, we have to go to that. That's that is important. But what's the time distance? Well, let's see if we can do the calculator. The magic thing here is that what's R, you choose. That there is no requirement on that. That's the structure we put there. I would choose. So, I here, of course, this model is more still bound in this, but that's where you probably comes from. But funny enough, it's the same bound as the Chernoff Gibson. So, these two terms do not have the same loss. So, the decent. So, the distance you have to traverse is at least S1, which is like L1 squared. So, I could make this distance L1 with a tiny constant in front, but that would be L. So, this S1 is there. This S1 is there. This I cannot play with. This is this distance at the most days. But then I can now backtrack everything and decide what R is. And R can be L1 with a small R or L. Be L1 with a small power or a small constant front, or could be a smaller power of L1 to one maximum. It's not super important, but still it gives me a lot of power. If the two boxes attach themselves for one from the other, there are no districts. So, but then Aris is R is dictated, except for the exile, for the temperature. Yeah, so here I put equalities, right? But these are all inequalities. You could put this could be at least the time is very long. Yes, they pay the price of the distance to the cloud, but this has to be nice. This at least that's one little I can put all the techniques on to I mean you can put anywhere just to do S1 first Okay, and now we are done. These two are done. Done. Then we look at this. I have the problem that the second box is bad and the local mixing works. But under the local mixing, the particles containing the Poisson point process, so the Poisson point process has less particle. And I can ask, what's the probability that this guy is bad if instead of the actual particles, I look at the particles of the point closer? That a second version of this event you can call it. It means I don't get the actual part of the part of the point process that's independent of the configuration. This is really crucial. But if the point process depends on the configuration, you can get information for what happened before. It's independent of the point. It depends on the motion. Then here, just saying that at the most. Just say this is at the most. So we promise the event that the particles of the person controls gives a bad configuration. I can now conveniently. I can now conveniently ignore the LM. It serves the purpose, its purpose is to convert B2 to B2P. And now B2P is interpreted of this. So now that internet conditions appears, and we get the block. You're using this trace of it before there's a spontaneousity with a badness of a block. It feels like a ballistic, you might likely have something ballistic. You might likely have something ballistic if you have more particles. No, no, no. These paths don't depend on the particle system. They have fixed paths, right? Doesn't depend on the number of particles? No, no, no, the construction of this. The difference to the right here. So remember the construction, that's a good point. I said that when you have an effective path inside, each side inside the center has a... Each side inside the center has a path. And spat is peace, doesn't depend on the particle system. Each side has one. And then, depending on where the infection is, I'm going to decide to give the particle from the path from that site to that infected particle. But then when I ask it, all these paths are despite it's either just one particle to me. Yeah, it's it's either these paths do not indicate pathways. Do not look at the pathways. So, with the product, you look at the error term. Notice that the error term, how big is it? It's exponential in this little r to the d. D the deto I said that we can even do it L1 the names of the ball so could be smaller a little bit smaller than L1 but we get something here a little bit less than L1 so one minus something see psi of B the notes that B is Notice that this is much smaller than the estimate we get from the recursion. Remember that when I wrote the recursion, which I like PS, I wrote that rho k is more than a for the color term that we don't care for the moment times rho k minus one squared was an error term and we And we need to show that the error term is smaller than this, so that we can show that this at most lies with things minus one, for example. But this term of the best bound you can ever hope for getting for this is rho one to two to the k since it's k minus one, you get the k minus two. But the bound of the error term is exponential, it's stretched exponential. Stretch the exponential in any one, but two to the k is smaller than any polynomial in any one. Remember that L1 has space at all. This is just the squinish. So my error term is indeed smaller than the work. So I can. Okay, is this clear? Okay, notes that I want to get into this identity bit, but you lost a bit of density in this process. Had you lost this absolute density here? So when I go to my box of scale one, I look the probability they are bad, but according to a particle system with a little bit less density. The same thing will happen if I had scale three to scale two. In each scale, In each scale, I'll lose a little bit of density. But again, it's the same trick as with the speed. I'm going to lose less and less. I have to set my epsilon that depends on k so that this becomes sample. Okay, so at the end, the whole density doesn't go to zero. But there's, I think, all that value that you can. Okay, let's see. I'm happy how we use it. I have 15 minutes to convince you that we can prove this. Before you move to that question, this all seems to rely very much on the stationary. Right. It does rely on. It does rely a lot on stationary, and these types of renormalizations tend to rely on stationary. Now, there are other ways you can put standards that do not rely on stationary. So, in particular, the way we use it here is the other way around. So, here we start for a small scale and we normalize up, right? Price up, right? By defining scale K with respect to pay minus one. That will be the opposite. You start with a large scale and impose a Benson condition for larger scale. Then you say because this holds, after some time we can put Benson condition in a smaller scale and I can keep doing this. And then you don't need station values, just need to be able to start with this condition at some large enough scale. Enough scale, but yeah, and that's more practical. So I can tell you more of why some of the wonders that you presented do not have such analogy when you have the removal. Yes, tomorrow I'm going to tell you a little bit of a different model scale analysis which use a connection. You use stationarity, but stationary is not enough. The position is just part of the process in stationary. For example, you could imagine that the SIR, if I keep removing particles moving and I forget the type of the particles, they are stationary. So that is a stationary component that you can extract with jobs. So the one we'll talk to more will deal really with this. Okay, this one. Okay, this one is maybe stationary. Okay, questions? I'm going to tell you the problem: it's possible to construct any random variable using Poisson processing. So these are the normal variables. It's enough density function G and I can take actually any space. I could take Z B and D, I can take even more abstract space. Let me take R just because I can draw. Even X are not no matter what it's possible to construct. It's possible to construct X from a point process. That is, I say what point process, and then X is a technique from the point process. How do you do this? Well, let's say this is on R. So we take a point process of some points process of intensity one on On R, the original space here, but with an extra dimension, it's a plus. So let me draw. So this is my R, and vertical input is the next dimension of the R. And then put a point process here of the ST1. It's just some set of points. For completeness, I'll give a notation for this. So this is point into x i h i. So my point process is just at this collection collection of points here with the possible then how do I Then, how do I sample X if I D was an Devon process? Well, I'll do the following. Let's draw on this, let's see this as a plot, and let's draw the density function of X. For each value here, we have a value density function. Let's suppose it's a nice function. So, suppose this is the density. This is G is G, I think. This is a density function of my variable x. I see the word cox as just the value of the density. Now, suppose I add here a constant. So I scale the density function. So if the constant phi is more than one, of course, I'm shifting this function down. For example, if phi is zero, I have just the eight. But if I increase phi, at some point there is a value, which is the first value for which this function touch a point at the point process. This could be five bigger than one, as one in my picture was bigger than one, but could be small. But there is a smallest file for which this function taps a point as a point process. Let's call Let's call, let's not call, let's define the scale. So I want to decide this scale is the smallest value such that there exists a point in the control set called J for which phi g of x j is equal to the side. That's my definition of this value. Now, that is a claim. Equalism. The first claim is that psi is nothing else than an exponential variable. The second value is that its j, which is the point here that I'm at, is this point, has density g. So I can construct by doing the smallest value for which my density function touch the controls. The assumption touched the point crosses, and at that point, I assigned to x. I can prove this. One is that is a, so I just need to show that bulletin is equal. So what's the probability? That's the size bigger than z. Well, that's simply the probability that there is no point of the point process under. no point of the point process under z times g that's like the probability no point of the point process density but this is nothing else it's a postal point process so it's exponential of the area under this function of g of Of G of Y dy Z goes out is I mean they get a necessity, it's one so it's explanation for the second to see that it actually has the right density. Actually, it has the right density. Let's do a no refinement on our definition of sign. So take a subset of my space, so a region here, and I'm going to define the same thing, but restricted to this region. I can ask myself what's the smallest value of that. Smallest value that I have to multiply the function so that inside this region dialogue. That's what this definition with the A or with this xi. If I put this, what I get above from one, again, is the same. It's no point of the possible point plus under z, but inside Inside a so the integral gets limited to a and I get the exponential new variable with dense with rate integral just let me replace the a by the whole space and that what I have proved before so how do I prove the next well let's look at the proof Well, let's look at the probability that xj is in A. I won't show that this is just the integral in the A of the decimal. But this probability is equal to the probability that sine of A, at limited to A, is strictly smaller than sine of the complement. Because if the complement is saying that the sign is strictly So say that the sign stifled to A has to occur first, then in the complement of A. If this happens, it means that my xj is in A. But this guy is an exponential variable of some rate. This guy is another exponential variable of some other rate, and they are independent. Why are they independent? Why they disjoint regions of space? It's a possible point process. Process. So the medium of two independent level of L is just the ratio of the radius. So this is the rate of the one I want to be the medium divided by the sum of the rates. But the one below is just one because I understand the whole state. Okay. The whole state of the density, and that's so I get exactly the mass of the density. So it's right the right measure. It's still not late. So, what do we do? So, this you can use for any route of error. You can always use it for some point process to construct. To construct it. So actually, you don't need to decide with only one normal data. You can observe many of them, as many as you want. You can just even have scale is 2k. And the same, take the first one and take one of. First one and take one point process, only one. Take a first pair and say you find the curve that touches a point. It is this one. Sorry, you cannot see anything in this part. Let's take the first one for the third sphere, and this is my Taiwan Ju1. Then, in the region above, I'm going to search for the second density, I take the second density, and then look at the smallest value. Let's suppose this density is really high in the beginning, so it cuts here and. So touch here and go down, but can be very different densities because this is the one so I detect psi 2. The smallest value for Hi 2 times G2 plus Psi 1 times G1 contains a point above the previous one. It's the same thing as if I were removing this bit and redoing it. And I keep doing this, I keep piling up. I keep filing up the densities, finding each time a new point. I'll keep doing here, something, and I keep finding out. Note that if, after I do all my points, all my learning variables, I guess that the sum of all these densities is above a line. A line, say a line here of some variable S. This means that my runoff variables stochastically dominate a Poisson point process of intensity S in this way. But I tied up the densities, and I found out that after pining everything up, That after pining everything up, the last curves completely above the line S. So this means all the points under S have been matched to a rendo variable. So my Renovario contains a Poisson point process of intense test. The points under here, since a Poisson process of intensity one, is a Poisson process on R of intensity. So that's exactly what. So that's exactly what we do here. G1, G2 are the density functions of each of these points. And one by one, we're going to match with the plus all well each point here, depending on their position, they have a density function that tells what their position will be at time delta. That's a density function. That's the density. Yeah. And that will be your G. The possible point possible just to throw here with the extra dimensional. You're just going to have to show that piling up these densities will go above the level one density. Then there's a simple computation to show it. This is officially out of time. It's interactional. It's indirectional. This is from the level. If you have a doubt, you can check it. But it's really a simple computation. The real art is here. This construction. I just want to show afterwards that the sum of these scales densities go above 1 minus epsilon B. It's a random sum because each term of a scale is multiplied by this exponential node vector, but they are I and D exponential novel vector. So it's really easy to control. Easy to control. It's essentially a turn of bounds and a bit of a control on the difference of the density function of a syndrome walk that are close by and it will move for a distance that is square with this distance. That's the answer to this. Maybe I will finish here.