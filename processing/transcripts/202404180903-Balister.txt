Years ago I think it's sort of been in limbo a little bit but hopefully we'll get it onto the archive. Okay so I should say all of this is joint work with Pharaoh, Rob and Eastman. Okay, so if you've seen the BBNS, that's The B B N S, that's that's just us. Okay, so I'm sure you all know what a cellular automat is, but just to fix the notation, you have a K-state cellular automata, which will be on an alphabet, which will be curly A. And it's an evolution of states, and it's always only going to be on either September D or September D or a torus. And it's the important point about selling automators, it's defined by local translation invariant updates. So you fix a neighborhood, which is finite, which could include the origin or not, it doesn't really matter. Then there's an update function that takes all the states in the neighbourhood and updates a point of the origin. And then you just The origin, and then you just extend it by transactional variance. And so I'll use lowercase phi for the sort of local update rule, which determines how a particular letter in the next time set is determined. Okay, now a generalized bootstrap percolation is a special case where you've got two states. Case where you've got two states, which are 0, 1, the unhealthy, inactive, active, antique, whatever you want to call it. And it's monotone in the sense that more infections means if you have infections, you can't have fewer infections later on. So more infections, it's more likely to infect things. So that's monotonicity. And once things are infected, they stay infected forever. I think that was called solidity or something. Or something in the previous lectures. So it's monotone both in the rules and it's also monotone in the time. And an exactly equivalent, if not only of condition for this, is that you can actually represent it by a set of update rules. So there's a finite collection of finite sets of Z to the N, of Z to the D, such that if you just consider the set of infinite sites at time to At 2T, this is AT, and then AT plus 1 is just infect sites when they contain translate, the appropriate translate, one of these updated forms. And we have an initial set A of infected sites. We define the closure, which is everything that gets infected eventually. And we say set pug. And we say a set percolates if the closure is model space, which would be Z for the D or Z D from Z n to the D. Okay, so this, of course, as you know, generalizes many types of bootstrap percolation. So the span of R neighbour bootstrap percolation can be put in this form. You just let the set of rules be all the R element subsets of. Last element subsets of the neighbourhood, and then if our neighbour is infected, one of those sets will be fully infected. That gives you cool. And we're often going to be interested in the critical or the population thresholds. So we include every cycle probability P independently. We define PC to be whenever the probability that probability that the set percolates is say at least a half and of course the half is sort of irrelevant. If you're on Z to the D then of course it's 0, 1 and even on the chlorus it goes from big side on top massive time very quickly. Okay so one of the key sites so there's two ways of looking at it. You think it's these generalized things must be Think it's these generalized things must be incredibly complicated, or could they be incredibly simple? I mean, we don't really know. So, the first thing is perhaps your initial reaction is, oh, it's incredibly complicated. But it turns out that if you just want a rough idea of what's going on, it actually turns out to be much, much simpler than you think. Because all you need to know is what it's, if you just want a rough idea of what the threshold is for. Idea of what the threshold is for percolation. You just need to know what it does on half planes. So I just put it up here. I'm not going to really be using this stuff in this talk, but you have an open half plane whose outward normal is u, and then you say a set is stable if the closure of that is just itself. So it doesn't hope that nothing gets infected. And in fact, it's unstable if and only if one of the Unstable if and only if one of the update rules is inside HU, which you can easily check. And so this thing is some subset of a sphere. It can be easily calculated. It's a finite union of finite intersections of closed half spheres. And you can read it off from the update rules quite easily. Okay, so I'm going to talk about just one slide on subcritom. Just one slide on subcritical, subcritical processes. So if every half sphere contains if every half sphere contains a chunk, sort of something with non-empty and Sort of something with non-empty interior of stable directions, then it's subcritical. For standard bootstrap, that's if R is bigger than D. And then even in the entire set of D, this thing has a strictly positive critical probability. Of course, as with all of these population questions, what are the actual values? You know, ludicrously hard to calculate. We don't expect simple answers from that, but at least it's sort of one-on-one. And this is equivalently saying that if you just do it on the torus, these critical probabilities just stay back away from this to do it. But everything else, we're going to be interested in how. We're going to be interested in how fast this critical probability tends to zero as a function of n, if you look at it on the torus, or equivalently, you can think about on the whole space expected time to infect the origin. I'll do it in terms of critical probabilities. And sort of the main result in this area is that. In this area, is that actually this you can figure out what this rate of decay is. It's a power of one of an iterated logarithm. So this is the sum r between 1 and d. And the value of r, which we call the resistance of the model, is given by some inductive definition. It's given by some inductive definition in terms of the stable set. I won't give it here, but it's again very easy to calculate. And for what's this resistance, this resistance is in the R neighborhood bootstrap percolation. It's actually just the number of neighbours you need to infect. So we need, so this general. So this generalizes the results of its trap proclamation. Just one question. Sorry? When you say that it's easily S, what do you mean I don't know it should be. I mean, I'm sure I'm almost sure it's probably in hand with time. I mean Show it to probably the end of the time. I mean, it's just basically you just have to figure out what the easiest half spheres, and then you look at the points in there, and then you see what the worst difficulty is. I don't know the exact complexity, but I haven't checked it, but it's uh but it but it's certainly computable. Um And so special case is R equals one, which for historical reasons has been given a special name, supercritical. And the supercritical models, which is what I'm going to be mostly interested in here, is we just get a negative power of n as the critical threshold. And it also is quite easy to give a particularly nice. To give a particularly nice description of when something is supercritical, it just means that there's an open half sphere that contains no stable points. And another way of looking at it, which is another equivalent condition, is that there is a finite set whose closure is infinite if you look in set of the D. So there's a small finite set that basically runs away. That the basement runs away. It doesn't necessarily fill up the whole space, but at least it becomes infinite. Right, so in special cases, we can do a lot better. In fact, we can do much better than this with exact constants. But for the R-neighbour bootstrap and for various other models, you can get a more precise form with actually even. More precise form with actually even knowing what the experiment is, and sometimes much better than that. But what can we do if we just have a general bootstrap location? Can we figure out a little bit more about what this probability threshold is? So I'm going to just talk about d equals two. About d equals 2. And in d equals 2, there are just three cases. You've got your supercritical, where it goes n to the minus the power. It's critical, which is the other non-subcritical case, where it goes as a negative power of the logarithm. And there's the subcritical, where it is a constant. And again, amazingly enough, so the subcritical is not much more to say, it's just an. Not much more to say. It's just some horrible constant which we can't calculate. For the critical, you can actually get the exponent. And again, I'm just writing this out in very crude form. I mean, they prove a lot more than this. They can go everything up to a constant factor. But basically, you've got this alpha, which is the difficulty of the process. And And what's more, the proof actually gives a method which you can make explicit in how to even calculate this. It depends a little bit more on, doesn't just depend on the stable directions, but it does depend, but you can still calculate it. Right, so that's the subcritical and the critical. What about the supercritical? And at this point, everything seems to be going wonderfully. Okay. So perhaps these models are actually much easier than we thought. So, and in fact, super critical you think, that must be easy. There's a finite subset that grows to infinity. Figure out what that subset is, and you should be halfway there. I mean, it's a little bit. Should be halfway there. I mean, it's a little bit more complicated because it may not fill up the whole space, and then you might need some extra points to continue. But somehow, that sounds like it's a sort of doable problem. Shouldn't be too hard to figure out what's going on in this case. Unfortunately, it turns out it's a lot harder than you think. In fact, even just finding this sign. In fact, even just finding this size of the smallest set suddenly becomes a big problem. And it's a really big problem because of the following results. So given any Turing machine program, Turing machine, one can assign, you can construct a bootstrap percolation, a supercritical update family, such that if Such that if the program terminates, then you need three sites to be infected before you get an infant infection. And as a result of this, the critical threshold is actually n to the minus two-thirds. And in fact, the three things actually overrelate. And if P does not terminate, then you can do better. You can get something for let me just You can get something of the number of just two elements. It doesn't quite percolate fully, but it is clear enough. And that means that the threshold is, in fact, is significantly lower. It has a different exponent. And as a result of this, if we could calculate, if we had an algorithm for calculating the exponent of an arbitrary two-dimensional supercritical model, just in two dimensions, Just in two dimensions, then we've solved the halting problem, which of course we know you can't do. So there's no algorithm for calculating these things. Update family is finite? The update family is finite. Update family is always finite, yes. When the actor is sitting inside a finite neighborhood. Okay. And I won't talk about this today, but once you've done this case, once you've done the R equals 1, D equals 2 case, you can go from R D up to R plus 1, D plus 1, and you can easily increase the dimension. So in fact, you get that for the exponent of the critical The exponent of the critical probability is uncomputable when r is at most d minus 1. Which leaves the case r equals d is the only one left over. We know we can compute it in two dimensions. My guess is we probably can compute it in higher dimensions, but that's still open. Okay, so let's talk a little bit about the sort of Turi-complete automata. So, you know, if we didn't have a bootstrap potentially, if we just had a cellular automata, it's well, well known that you can build these things and they can be Turing computers, they can actually simulate a Turing machine in some way. So classic example is Conway's Game of Life, which Conway's Game of Life, which is a two-stage solar autometer. And if you are not too worried about your output size and your neighborhood size, real dimensions, it's not too difficult to construct easier examples. The surprising thing is that the very, very restricted monitoring rules, monetary, Monotonic rules, monotony in time, property of bootstrap perpetration is not enough to exclude such behaviour. And I think this is perhaps immediately a little bit surprising. For a stellar totometer, you can build a computer. For a bootstrap process, you've got to build a computer where you can write memory only once, and you've got all your gates are monotonous. There's no not gate. So you can't you can't invert anything. Anything, how do you build a computer with those sort of restrictions? So, well, we can do this. And what we're going to be doing is, we'll do this in, we'll construct first a bootstrap process with a sort of universal Turing machine property that you Machine property that you can feed it a Turing machine as a finite set of infections along the real axis, along the x-axis, so that the closure is infinite if and only if p does not terminate. Okay, so how do we do this? Do this. Okay, and then after this, we're going to modify it slightly to give you the example that was in the in the Okay, so how do we get this construction? So the first thing is to get around the fact that we can only write to things once, we're going to lose a dimension. We're going to have time as one of our Time as one of our coordinates, the y-coordinate would be time, which means that we actually need to start off with a Turing-complete selling optometer in one dimension, which is a little bit harder to construct. And then we'll use the other dimension to represent the evolution of this Turing machine. So we're going to basically code something that we know to be Turing complete inside a bootstrap location. A bootstrap calculation. Yeah. So just one quick question to make sure then. So you bootstrap rules, the order update doesn't matter, right? They're attractive. That is true, right? Because so generally bootstrap you bootstrap those. So that's another way in which our computer is weaker. It's sort of asynchronous. We could sort of asynchronous. Yes, yes. You will be running it synchronously, but yes, you could run it asynchronously, and you would give them the same step. Okay. And as I said, it's not hard to construct these things if you don't really care about alphabet size and neighbourhood size. But just for definite sake, there is actually a two. A two-state neighborhood, just previous site, and the next site, which is actually turning complete. This is Wolfram's Rule 110, which I've written down there. You can ignore that because I'm not going to use it again. But it's explicit to state neighbourhood size of 3 rule which has properties which. Has properties which I'm just going to give on the next slide. But for our purposes, any automata with these properties would be fine. We don't actually need a specific one. So this looks a little bit complicated. Okay, so there's finite strings L, R, H, and an algorithm for encoding an arbitrary Turing machine. An arbitrary Turing machine P into a finite string S P all over the alphabet over some alphabet, which I hope you'd specified, okay, is such that if you start with the S P, which actually codes the program, and then on the left you put a whole bunch of, you pad it out with a different number of L's, and you pad it on the right with an infinite number of R's, and then And then the rule is that H will occur as a substring at some time if and only if the program holds. So if you either you never see an H substring or you do see an H and that tells you whether or not the program is terminated or not. The L's are called the left and right background patterns and if you just have a background pattern Just have a background pattern. The worst it can do, it's a periodic pattern, it might shift, but it basically stays the same pattern. So, you just on background, all the autometer is doing is translating this. Why do I have left and right background patterns? In Conway's Day of Life, you can just make the background empty. For rule 110, you actually need some non- You actually need some non-trivial periodic patterns, and it doesn't really profitable sort of proof at all. So we'll just do it in this generality. Are the farther one-dimensional ones? I don't. I'm not sure. If you extend the neighbourhood, I'm sure there are. Yes, it would take that because we could sort of tell you later. With a small neighbourhood, I think it's pretty strict. So you code your program in an infinite string at only finite amount of it which is relevant, and then you run it and you just look to see whether you see this halting substring. Sorry. I always think of uh the the halting problem as program halts on a certain input or you're thinking of the program. Input is encoded with the program. Yeah, so you can think of it as a program and its input data. Okay, so we're going to start off with something which has these properties. And we can, without posteriority, assume it's rule 110. So here n is 1. So here n is 1, a is just 0 and 1. But in general, it doesn't really matter. Any neighborhood, which we can assume is just from minus n to n, so in one dimension, and some finite alphabet. Okay. So the error here is sort of important. It's the because it determines the speed at which stuff happens. You can only see out. If you can only see out to distance n at every step, then nothing you change can propagate faster than n. So there's a finite speed of light in the system. And we'll be using that. That will be very important. So we now construct, we'll do this in several stages. We'll construct something which is monotone but not on 0, 1. So monotone but on a large alphabet. Alphabet. And the alphabet is not totally ordered, it's only partially ordered. And the monoticity is in the partial order. And that's not a big problem because then we can code those as binary strings. But the first thing is to get it at least monotone. And we also want to force the background to be empty in this model. Okay. Okay, so we extend our alphabet by three new letters with the empty thing being minimal element, full thing being a maximal element, and everything else being incomparable. This thing is for the empty symbol, and it's not going to be the zero of the alphabet. Going to be the zero of the alphabet. I forget what the alphabet was. If it had a zero, ignore it, because we have to make things monotones, so we have to invent our new sort of minimal element. And this is going to be a for error symbol. It's an error symbol because it shouldn't occur. If you run the thing properly on a well-defined Turing program, it should never. Program, it should never occur, but we have to put it in there for technical reasons. And this is a sort of LaTeX symbol for math delimiters. It's a delimiter symbol. And it bounds the region where all the math is happening. So it bounds the regions of the integers where the program is actually running. Outside this region would be empty space and really interesting stuff would be occurring between the dollar signs. And the new automotive will have the following properties. It'll be monotone with this partial order and it will again have this Turing complete property. But it's an algorithm so you can encode it, but now you encode it as empties$ As empties, dollar, interesting sequence, dollar, empty. So, and it halts not when you see a particular substring, that's not going to be terribly useful for us. It halts when the eventual state is all empty. So, if it halts, it sort of completely just dies out. And if it's otherwise, it just grows and rows and rows. Otherwise, it just rows and rows and rows and rows, and you get a linear number of non-trivial letters. And the inside doesn't contain any of the special characters, it's the original alphabet. So the idea is it follows my original inside the dollar signs. And the finite stream grows, so the dormitories move apart at speed n, so as to give you more and more room as the thing in the middle evolves. And because it's going at the speed of light, you can't catch up with it. And then the trick is that if the halting string occurs in my original, I've somehow got to have a way of killing it off. Have a way of killing it off. So, what I do is the next step, empty symbols appear, and then they spread really, really quickly, faster than the speed of light, faster than the original speed of light. So, we're going to have to have a bigger neighborhood. And then eventually, they just take over and wipe out the entire process. So, I'm going to draw a picture of this. So, this is what it looks like. So, this is time. It looks like. So, this is time. You start off with your interesting stuff between dollar signs, and it grows and it grows and it grows. And then, when you hit the halting, when you hit, if you see a halting string, then at the next step, there's a whole bunch of entities suddenly materialize inside the dollar signs. And the point is that these are fatal because at this next step, they grow. Next step, they grow very rapidly, much faster speed, and then eventually, when they get too close to the end, the whole thing is done. So now I have to give you the rules of the new process. We need a bigger neighborhood because the inside has to be faster. Side has to go faster. And then, when we have a little word in the neighbourhood, we have five rules. The first rule is if you only see original letters, do whatever the original did. With one exception. If you see H, don't do anything at all. So we note that this rule is only a partial rule. We'll fill out what we have to do in other cases in a moment. But if we see an h, then we can't apply R1, so it just fails to give a pattern. And that H could be anywhere from minus N tilde to plus N tilde, so it could be quite a long way away from where you actually are. The next two rules are: let's move the delimiters at the right speed. And so, if you see a delimiter with the left upground pattern on the right, and anything you like on the left, we shift the whole process n steps. So the whole thing just moves n. So we read off whatever happened n steps later. Whatever happened, end steps later. There's a slight catch with this. I may have to sort of shift the background pattern so that it matches what it should do in the original. So we need to keep it sort of compatible with what the original would have done on the background pattern. This bit of the word should be a long, long way away from anything interesting going on. So it's just filling out the background pattern and then further in. Background pattern, and then further in, you should be just applying the original rule for the background pattern, and they should match up. Why would you see the left on the right? So, why do I have a star on the left? The reason is I have to make sure that the left, the L symbol. How would it ever appear? If I look at the pitch, L is now to the right of the. The reason I have a store on the left-hand side is I don't want to insist it's zero because I need to make the thing not care about that, because I need the thing to be monotone. So basically, I'm trying to make these rules as monotone as possible because that would be avoiding. And then, similarly, on the right, I've made it until the I've made it until the big enough technical reason so that when I say left background pattern and right background pattern, if you ever need to know what it is, you've always got a long enough string of it that you know how it's aligned. So there's nothing ambiguous there. There is an ambiguity in here. Okay, these rules move the delimiters at speed n, left, or right, and fills in. N left or right and fills in the appropriate background pattern, but only if they have enough L's and R's adjacent to them. If anything else gets too close to the dollar sign, then it collapses. Then these rules just don't apply. And I should point out there's a technicality that both rules could apply. I'll get to that later. We'll fix that. And then the final two rules. And then the final two rules are to fix the monotonicity. If rules wherein the two and three give you incompatible answers, put an error symbol in that. And if you see an error anywhere, it's an error. Right, so as I said, this shouldn't happen, but it just makes everything monotone. And if none of the rules apply, then the answer is 8. And then make it well-defined monotone, because all you have to do is check if you change a letter to a full or an error, everything becomes error. And if you change it to an empty, if you go through those rules and you list them down, changing a cadet to empty will either mean the rule can no longer be applied or it doesn't make any difference. And in R2 and R3, you might be copying it, so You might copy it, so this is actually because of our four and our five, this is well defined, and it is monotoned in this weird partial order. So I guess it's not so obvious that adding an empty can create an error, but it's quicker. I think from those rule, it's I believe, but yeah, you'll like that. It is quite easy to check that. So, the coding of the original program, which was an infinite string of L's and a string and then R's, then we just make it a finite string of L's, S and then a finite string of R's, where we have enough buffering of the background that it won't interfere with the movement of the delimiter. And then the lemma is that it satisfies the two conditions that I set. So, just a quick proof by picture: what's going on? So, we start off with our pattern here. Normally, the dollars move just everywhere around the end is one of these R2, R3, and just everything's just shifting at speed n. If you get an H, then none of the. Then none of the rules apply. We're not near a dollar sign, and we're not, and we don't have all fully letters because the next step will be empty. And then in the middle, none of the rules apply, and so the whole thing just percolates as empty at speed n tilde, which is bigger than n. And then when it gets close enough to the end, the R two and R three don't apply and just none of the rules apply. And just none of the rules apply, and the whole thing just dies. But of course, if H doesn't appear, then this will just grow happily forever. Right. So the next step is: can we make this an actual bootstrap process? And the first thing is we code it in binary. And the first thing is, we code it in binary. The exact details aren't that important, but here's one of my fits. I take all my letters, empty all zeros, full will be all ones. And each of the letters I will code as then with two infected sites distance two apart, and then all the other infections will be at least three apart. At least three apart and at different locations for different letters. So no letter is a subset of any of the other letters. Everything's a subset of full, entities a subset of everything. We're now just considering the set of inductive sites. So this actually corresponds to a partial order. And this is cleverly arranged so that the annoying property that Property that I have a strings binary string and I'm applying these rules and I want to know that it only sees what it should see. It doesn't see some sort of shifted version of the letters. So the 101 at the end is an alignment. So if I ever see a letter in a concatenation of letters, it's got two infected sites, two apart. That must be. Two apart that must be correspond to one of the letters in the actual original word. So I can't end up applying my wrong sort of half a letter shifted along and mess everything up. And so you can, the only way you can get a subset of infected sites that looks like a pattern of letters is if it really is, if that pattern of letters is really infected. If that panel of letters is really there, ignoring full answers. And as I said, the details don't really matter. They're just convenient. And now, how's the bootstrap done? So as I said, there's two things. We code them as these binary strings, with the ones now being infractions and the zeros not being packed. And so for edge. So, for every sequence of letters in this extended alphabet in this neighborhood, we create rules so whenever we see the corresponding infections that would occur in that screen. Is there a question? Okay. So, whenever you see an infection that corresponds to a string, and possibly more infections, that shouldn't happen, but whatever, then you put the coding of the resulting stream on the next rule. So if you think about this, this is one rule for every infected site in SP0. The infected site in SP0. So SP0 is a binary string. It's what, you know, ignore the zeros. There's a one here. You see the pattern of infection sites here and put a one there. That's one rule. See in infected sites, you put the next one up there, that's the next rule, and so on. And you have one rule for every string and every infected site in the SPZilla. And so it basically just reads this string. Just reads this string, anything that looks like a string, it just writes the corresponding thing on the next one. So the choice of coding is that the only infections caused by these rules, if you apply it to a valid string that doesn't contain an error, then the only thing it'll do is exactly that. It'll just copy the string on the row buffer. On the row run. So this takes coding of the teeth state and then copies it on the metric line for the t plus one state. Okay. And the following horrible lemma is then basically immediate if you read it. It's just basically saying what we've constructed. It's just basically saying what we've constructed. So if you have an initial state in this weird monotone process, and you put the coding on one of the x-axis of some state, and then you run this thing, and assuming that you never hit the error simple, assuming that just never happens, then the evolution. Then, the evolution of this bootstrap process, which is now a genuine new bootstrap process, is basically just run the original process and just copy it on the next line, one step at a time. And so, now what we've done is we've constructed a universal tour. A universal Turing machine inside a bootstrap or collection. It's a process that can run an arbitrary Turing machine with this crazy coding. You have some crazy coding in the program, put it along the x-axis and run it, and then either it'll just die out, it holds, or it'll go on forever, it doesn't hold. It either infects an infinite sect, it doesn't hold, and it will infect a finite sect. Hold, and it will be in fact a finite set if it does hold. And we're basically there, more or less. So, just to finish off, we're now converted into the one which I mentioned at the beginning. So, we now fix this Turing program, and now we construct, this is the first time the actual bootstrap process depends on the program by adding a few more rules. A start rule. We notice in what I was doing. We notice in what I was doing before, I never had two adjacent infected sites, unless there was an error, which there won't be. So two infected sites, two neighbouring infected sites, never occurs. So I introduced a new start rule, but if I see two consecutive ones in a row, I build the entire encoded version of P, which is this colour spoller. Is this colour SP colour thing on the next rollock? So, again, that's a whole bunch of rules of the form: two sites and one for every infection along that token. And then we have, oops, sorry, detonation rules. Okay, and then we see three consecutive ones in a row. It infects all the neighbouring states, and you can see what this is going to do. If this detonation rule is ever invoked, we instantly percolate the whole space. And it's this detonation rule that makes it super critical because now we see there is a finite set of. We see there is a finite set of states that you know whose closure is infinite. Okay, and that's it. So let's just check why it works. That's the proof. So as I said, it was super critical because of these detonation rules. Let the initial configuration now be p-random. And let's go through the four cases. Let's assume P terminates and P is little o n to the minus two thirds, then the initial site, the initial infections are very unlikely to have three infected sites within any constant distance. And if there are two sites within constant distance, the worst thing that could happen is. The worst thing that could happen is if they're adjacent, it iterates this start process, and then the program runs for a finite amount of time and then just dies out. So two sites will give you a finite infection within a finite radius, and if there are no third site within that distance, that's it. It just fires. So the worst that can happen is occasionally you get these two neighboring sites that blow up to a little. Two neighbouring sites that blow up to a little region, but it's a finite bounded region. And if they don't see another site, that's it. It'll never work on it. If you just have two sites in any other configuration, that doesn't work. Then it just doesn't work. Yeah, so it just stops. On the other hand, if you have more than n to the minus 2, More than n to the minus 2 to the thirds, then you can get three sites in a row, and it just deconates. And then the whole thing perfects. Okay, now let's suppose P doesn't terminate. Well, if P is a little o n to the minus 1, then it's unlikely any two infected sites will be adjacent, so nothing happens. Nothing boring. And on the other hand, if you're above that threshold, then you're very likely to find a row with two ones in it. This initiates the program. It runs, but it doesn't halt. So it then fills out a constant fraction of all the sites in your Taurus. And if you just want to finish this off, you just notice that it's. Just notice that it's with P being more than one over N, it is exceedingly likely that there will be an extra infected site somewhere that will end up between in the middle of the one zero one in my letters of my process, causing three infections in a row and then the whole thing just decrements. So it gets an infinite infection, but it's ludicrously likely you'll get all. Infection, but it's ludicrously likely you'll get one more cycle that actually causes it to perpetuate the hole. It's a nice twist if your computer actually. If it can't halt, it blows up. So my computer, if it can't halt, then it blows up. So we're done. That's it. Any questions for Paul? Without the detonation rule, is the family you build? Is it sub-critical? Oh, sorry. Oh, without the detonational. My guess it probably is for a different reason. If you have If you have enough ones in a row to form that error site, that would also do the job. So, in fact, actually, technically, I didn't really need the detonation. But it made it nice because I only needed one extra point to cause the whole thing to percolate. So it was either two or three. But if you don't mind not knowing exactly what the exponent is, then yes, it would also effectively deconate if you had enough ones in a row, you would get one of these error symbols. Yeah, I have a question. So I think I understand as the proof, but I'm still a little bit confused about something at a high level because you said you needed monotonicity, but only monotonicity with respect to a partial order. And you just take an empty partial order, so then everything's monotonic. So somehow there's a bit more something than that. And I guess it's something to do with the detonation, and you want to be able to. Well, no, it's the truth that I'm replacing things with binary strain. It's true that I'm replacing things with binary strengths. So, going back to my argument that how can you build a computer with where you can only write once and it has no knock gate? What you do is you don't store 0 and 1 as 0 and 1. What you do is you store 0 as 0, 1 and 1 as 1, 0. So there's the different patterns which are incomparable. And that way you get around the fact that the original ones weren't monitored. Yeah, well you could do that without the extra size, so so so anything can be encoded as something monotone by making everything incomparable. Yeah, which is what we've done. Yes. So by coding the strings, it shows that the monotonicity is not in fact an an obstruction. Uh okay. Why why I called it monotone there was was I I needed uh the empty and full symbol just to make everything work. Just to make everything more decidable. So I guess you also moved some sort of undecidability. Writability and halting are sort of the same thing, because I could take an undecidable proposition or one I think might be undecidable and write a program that will generate a proof or a disproof with the response. Yeah, I mean in some sense of course you know one of the one of the things it's worth pointing out is that the exponent of course is letting n tend to infinity. So you're actually trying to prove a statement about infinitely large tora i. And so yes, so but it seems to me you've also proved there are some root struggles for which it's undecidable whether what the exponent is, yeah. Yeah, what the exponent is. Yeah, yeah. Also, uh, get a construction with a reverse way. So that's that's what could you get a construction with a reverse way so that the terminates then you have the louder exponents? Oh yeah, I haven't thought about that. Uh so could could we get it to yeah, yeah, flip flip the flip the whole thing around that thing for I don't see why it's too big for you. The one-sided uh computation shows that yes, that's a that's a that's a good question. Um uh Does there even exist an experiment? And I don't know. I'm just guessing that it probably is. But yeah, we have meeting home. Anything else? I have a question on the other. I have a question, I don't know if I know how to formulate it, but imagine you have a a rule that for every kind of convex set it expands to another convex set. Then you ruin all your construction on that because you will not be able to put 0, 1, 0, 1, start to be cold. Yeah, and I'm sure if you put more restrictions on what sort of U can be, then yes, that will kill it. Well, it would kill your construction. Um well, it would k kill your construction. Maybe there's something even more subtle. But actually, it's a good question. I'm not sure that convexity will do necessarily too much or I had to be a bit careful. At least in sort of higher dimensions, what you could probably do is you could sort of encode how curved it is. Encode how curved it is and use that information to do something similar. I mean, obviously, if you put enough restrictions on you, then it will be copyright computable. But the point is that it's a bit general. More questions, maybe you're a copy. I'm gonna let you know.