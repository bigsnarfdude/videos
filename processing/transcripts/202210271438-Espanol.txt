Thanks for the invitation to talk about some work that we've been doing in the last two years. Maybe a little bit different from what we have seen so far in the conference, the workshop. This is joint work with Milita Pasha, who was my former postdoc and now is an NSF postdoc at Taft University. Postdoc at Taft University. She's in the HMA. And Sean Brecklin, who is my collaborator at the Nevada National Security Site. So I want to introduce and motivate this work looking at this particular problem. Here we have an object, and we should say object with very high energy x-rays. So here we have So, here we have a scintillator to actually see what's happening, right? To kind of stop the photons and be able to take a picture of what is happening here. So, here we have a checkerboard, so something that we know what should look like. And here we have a cross-section, and what we observe is that, of course, this is very blurry. We like to see very We like to see very sharp edges, and we don't. So, in particular, let me say that the scintillator gets more blur on the edges. So, this is really a specially variant blur, but I'm going to just focus on assuming that it's specially invariant. But there is some work that we have done on publishing. Have done and published it last year where we assumed that this was especially variant and we reconstructed, try to reconstruct the forward or the blurring operator first, separate it from and then use it to de-blur. And what I want to talk today is if we can do both things at the same time. Reconstruct the blur and reconstruct the image at the same time. Image at the same time. Right, just to set some notation. This is a picture of my son. I blurred. So here you can think about this model as this is a convolution. This is my PSF, how each pixel is blurred. So we can think about here I So we can think about here, I got this picture by convolving this PSF with this image, and then we add some nodes. The other way and the way that I'm going to model everything is that I put this picture in a vector. This matrix G is such that if you multiply G with this vector, then we get the blur vector, the vector that represents this blurry image. Represents this very image. But of course, we know that our data is all noisy. So the minute that we try to go back, this is what we get. So we need to do something else. I'm keeping going super fast on these ones. Alright, so this is really what I want to focus on. I want to solve this separable normally. A separable nonlinear problem. So, oops, not really. Okay, so I'm going to assume that the blur is parametraceable. So, imagine that you have a kernel that has some, it is defined by some parameters. And then, of course, from that kernel, we can get the PSF and then construct this matrix. So, I assume that our So I assume that our blurry operator is parameterized by the vector y. And then as we said before, this x is the image that we want to reconstruct. This is our blur and noisy image. And then of course, I want to really have a regularization term later. I'm going to spend some time when p is equal to 2. We know that it's not ideal, especially if we have images with If we have images with edges. And so I want to also consider at some point, you know, p is smaller than 2 and even smaller than 1. Of course, this is not the norm anymore, but you could still do things and get nice results. So several approaches. Alternating approaches consist of keeping one of these parameters One of these parameters fixed and solve for the other. That means, for example, we can keep y fixed and solve for x. Once we have that x, solve for y, right? And we can alternate, solve for x, solve for y, and so on. Gauss-Newton method is a method developed when PCQL2. And I will talk about that a little bit. I think I cut it off. Sorry. But I will quickly give the idea. Give the idea. But the main talk is about variable protection methods. I will take you through why it was developed, what was the problem that was developed for, and some approximations, some new development. Alright, so I really want to focus on minimizing first for p equal to 2. We can put everything under one norm. Norm and we can also take x and y and put it together as an unknown vector. And then I can apply Gauss-Newton, that is a method really developed by solving a function like this, where the f is nonlinear. And the basic idea is we linearize. So we are going to have the f at some value plus the Jacobian plus some vector that is a one-and-oh. Vector that is our unknown, we will solve for it and it will be the update for our vector. In particular, there was some research work by Pez and Rodriguez where they were doing this kind of semi-blind deconvolution. Semi-blindness, because we are assuming that we know there is some current there. It's not totally blind. And there is some nice theoretical result that says. Theoretical result that says certain conditions to ensure convergence. Now, I have to say that in principle, Gauss-Newton is not ideal, right? Trying to update for X and Y, where X and Y have very different roles. You could kind of see that it's not going to be the fastest approach. Okay, so let me just mention the non-linear least square, right? Well, when we think about kind of curve fitting. When we think about kind of curve fitting, because that was the problem where this variable projection method was developed for. So you have a bunch of points, you want to find the curve. Now, this curve is going to be a linear combination of functions that depends also in some parameter. So that's why this problem is going to be non-linear. We would have, you know, if we put everything in metric. Put everything in matrix form. This matrix here will have some parameter that defines these functions, and then these, you know, the linear combination that we are trying to find, and then why is our data? So, yes, so this paper, this, oops, the Golub and Berera developed the method of thinking about that. About that problem. So, of course, no regularization there, but the main idea is the following. Because we have the two norm, this has a closed form. So we can take and basically rewrite the x and replace, instead of having x, we have this closed form there. So we can write everything in terms of y, and we can just sort. And we can just solve for it. So, this is now a non-linear problem. We can apply Gauss-Newton there, but just solve for y. And in the paper, they also show how to find the Jacobian matrix, right? Because you want to solve using Dausenton there where you need the Jacobian. So, they wrote this expression. Furthermore, in this paper, they showed that if you solve for y, and then you define Okay, and then you define x using that y, that is the same solution as minimizing this problem. All right, so now it might, let's see, it's clear here, I don't know if you recognize x there, and in some part here, rewriting this a little bit, what it appears is the residual, gx minus d. Okay, so the paper. So the paper of Kaufman said, well, if the residual is small, we don't really need to, you know, we can discard this part. But then of course, if we discard a part, then it's an approximation of the Jacobian. Later on, in a totally different also problem, here they were looking at this neural network controller, totally different problem, but they also applied the variable projection method and they did an even And they did an even another approximation of the Chacovian. And I'm mentioning this because I'm going to show you some results. Alright, so what we did is pretty simple. We basically tried to apply this variable protection problem to the regularized version. So you can sit down, write everything. It's kind of straightforward, although it's very messy. Forward, although it's very messy. But we can start using other numerical linear algebra tools, for example, the generalized singular value decomposition. So if y is fixed, we do have a matrix G and a matrix L, and we can compute this decomposition that we just say that there is P and this P bar are orthonormal, this C and S are. And the C and S are diagonalized for, and then they share this matrix. So you can write the solution of this problem very nicely using these bases. And you can also write the residual. But furthermore, what we did is also to write the whole Jacobian using this expression. And this was a little bit inspired by this paper of Diana Olerni and Raz, where they look at the unregularized version and using the SD. Version and using the SDD. So basically, kind of following, but of course, now using the corresponding decomposition. Now, in practice, the minute that we start solving large-scale problems, finding this decomposition of plutonium is to be very hard. So the idea is, well, as usual, we can still learn. We can learn and do some theoretical. Learn and do some theoretical development using this decomposition. Now, in image processing, many times if we impose some boundary conditions in the images, we actually have very nice structures. So, we can do some efficient implementation of all this. For example, if we impose periodic boundary conditions in the images, now this G and this S. Now, this G and this L, if it's 1D, not the image, if it's 1D, then these matrices might be circular. If it's like 2D, we have images, might be blocks, circular, circular blocks, of blocks, and still we can have this useful transform. So, same thing as we did before, we can write all these Chacovian using these decompositions, and then every Decompositions, and then you know, every time that we need to do a vector, a matrix vector product, we just can call the FFT or DCT. So, this is very nice. And let me say, maybe here I can comment on that. I usually ski, but here, in particular, we at some point we need to compute the derivative of this blurring operator with respect to every single parameter. But that's actually, in this case, going to be very efficient because To be very efficient because these entries here really depend on the FFT of the PSF. So if you want the derivative, it's really computing the derivative of the kernel and then getting your PSF, computing the FFT of the PSF, and you get this matrix. And everything else is very efficient. Alright, so kind of summary, I've seen how the method goes. Goals, you know, you need to compute x and f, even though, I'm not sure if I say it, but you know, in principle we solve for y, right? We discard it or got rid of x and write everything in terms of y, but when we really want, when we need to find the Jacobian, the x and the residual are there. Okay, so we really can think about this as, well, let's compute the x, let's compute the residual, and then we compute the Jacobian. Compute the Jacobian. And then we do this, the Gauss-Newton update. And I'm mentioning this because I always am thinking ahead. So even though the theory and everything I'm doing, thinking that I can do it in one dimension, I'm looking ahead and thinking, well, but if the problems are very large or they don't have any structure, we still want to do this. So here, instead of, you know, how do we get X? Do we get X? Because we need it for the Chaconian. So here we can use preload methods, other methods. So that's a reason why actually, you know, write it this way here. There is this parameter lambda, it's always an issue, everybody knows it. Now, in general, we don't know it, but we do know what happens. We do know what happens, how to compute. We have some methods that we know when the bladdering operator is known. So what we are going to do is use some of those methods to compute the lambda at each iteration. So, in particular, the GCV. So, and the idea of in particular using The idea of, in particular, using GCV was because there's a paper of Julian Chan and Jin Nei, where they look at this problem, this semi-blind deconvolution problem, where they were looking at L-equal identity and using some pre-log methods, and in the middle they were using particular methods. All right, now we want to extend this to have not just the two norms there. Okay, so we want really to actually have the norm. Actually, we have the norm. But if you think about how, you know, even if for the case where we know the blurring operator, what do we do? Well, we use this iterative reweighting approach or some other approach where basically what we do is we solve this problem by solving a sequence of minimization problems with the two norm. Okay, so we can do the same here. Okay, so of course, well, different approaches. Well, different approaches actually. So you can do this iterative reweighting where you add there a weighted matrix. But of course, if I want to go back and use the GSVD, or well, no, the GSVD wouldn't be a big problem. But if I want to use this idea of using Fourier, where you need a structure for the matrices, incorporating the deep matrix. Incorporating the D matrix there will be a problem. But you can do a primal dual or a split breakman instead of this approach. Alright, but the idea is the same. We are going to approximate the p-norm by some weighted matrix there, get the x, also estimate the residual, and then the Jacobian and so on. All right. All right. Well, I have it this slide just to mention again there is a if these are really large scale problems you might want to use some particular Krylov method and here we use the minimization maximization well some particular Krillov. Sorry. I need to go to the details. Okay, so some example here I'm using a very, you know, I'm something with a 1D. I'm starting with the 1D so I can show you how the GSVD works. This is an example where I have a Gaussian kernel, and here the parameter is sigma. So the true solution is this blue line, and then the blur and noisy signal is the red line. And here in particular, in particular, I'm showing what is the tickle of solution as What is the T con of solution assuming that I know the kernel? Okay, so just to get an idea, what is the best thing that I can get, right? Because actually, we don't know. All right, what do I have here? Here, okay, sorry, many things here going on, but what I want to show is really the behavior of this. So don't try to read anything, just look at these curves there, and I'll tell you what to look at. But here it But here each column is a fixed lambda. So I'm exploring what happens when the lambda is fixed. So I have different lambdas. And then first row here is the relative error with respect to x, the relative error with respect to y, and then how the functional is decreasing. And basically what we can see is that for very particular lambda, actually all the Jacobians behave the same. Behave the same. Now, yeah. So, but here I'm showing what happens if we actually incorporate this GCV, right? So, here we are changing the lambda and finding the parameter lambda at each iteration. And we can see that the Chacovian, the full Chacovian, and this what I call Kaufman, sometimes I call it half Chacovian, right? Because I keep only one part of it, behave more or less the same. Behave more or less the same, and that's not surprising. When I'm trying to find the regularization parameter lambda, actually minimizing a function that has the residual there. So when I impose and I find the lambda that minimizes, it doesn't minimize only the residual, it minimizes something else, but in particular you have the residual in the numerator. So they kind of behave the same. But then the other chapogan is way slower. And most of the people, for And most of the people, in particular for large-scale problems, use the simpler Jacobian. But it is slower. So, you know, if you can go the extra mile and have and compute the Jacobians, then you might have better convergence. It's always a trade-off. A simple Jacobian might be a little bit slower, but taking the time and getting the better Jacobian, you might have a faster convergence. Okay, I can see. Okay, I can skip this. Just another example. Here we have a 2D example. Again, this is the Gaussian kernel. Here, this constant here is normalizing. And here I'm using the L matrix is going to be the Laplacian. So here is the true image, this is the bladder noisy, and this is the reconstruction. Noisy, and this is the reconstruction, assuming that you know the blurring operator. Where else here is something? And again, here this is same kind of experiment where the L, the lambda is fixed, so one column, same lambda, different lambda, different lambda. And we can see here that the Chacovians in particular kind of converge similarly. But let me say that, you know, couldn't put too many examples here, but actually, if you change the initial guess, right, there is an initial guess to start with, you might see that even though here all the Jacobians and the behavior of these convergence goes the same way, they might not be. So, in general, just to tell you, in general, the Kauffman, Jacobian, Kaufman, Jacobian usually converge ways faster. And then, actually, the full Jacobian is in the middle between this circuit approximation and the Kaufman, Chakolem. And in particular, I mean, many, many, many questions, maybe I should move forward to all the open questions that I have, is that they might have, you know, once we have. Once we have this Jacobian part, this square that we want to solve, it's really a matter of how well condition is that matrix. I mean, several questions arise. Why is this working? Those are the open questions. All right, let me skip. Maybe I can show you another example where we do have the p-norm. So these are the images, the blur and noise. The blur and noisy image here, and this is the PSF. And after a few iterations, let me clarify. So here, let me think. No, I forgot. I know it's here. That's good. That's why we put things here. Yeah, so this is the identity matrix, but this is the two-norm versus the 1.1 norm. Okay, so it's hard to. Okay, so it's hard to see that this is way better. So let me show you maybe convergence curves where this blue line is the relative error with respect to X and this is with respect to Y. The blue is the two norm and then there is the, I think this one, the orange is 1.2 and the other one is 0.8. So now we could have these that are smaller than one. Well more examples, but I Well, more examples, but I actually want to go to here to the open questions actually, because we are here looking for collaborations. So in particular, I am in the problem that I show you first, we are actually interested in having several currents. So my examples are... So, my examples, and usually all these papers have the standard example where you have just the Gaussian, right, as an example. So, we want to wonder, because in this particular example that motivated this work, I talk about the scintillator and the influence of the scintillator, but the whole system has different parts where they produce some kind of blur. So, you can describe this different blur. These different blurs in the system by putting a combination of different curves. So, but of course, you want a weighted combination. So, you know, how to impose those conditions, right? So, we have immunization where this, for the y, we want to impose some constraints. For example, like all of them add to one. Ah, yes, so that's my comment for the mixed curve. So that's my comment for the mixed colours, and I was very happy to see that Tristan was here coming to this workshop. He wrote with a collaborator a very nice paper where they look at this more general minimization function, but this is still the variable projection method. But this is written more general using this, I forgot, the I forgot the well, one particular solver. But again, I really would like to understand a little bit more, you know, what happens when you approximate X, then your Jacobin is going to be approximated. So what happens with the whole convergence of this method? And GSBD, you know, large problem, you cannot do it, but you know, what happened if we can use just random IGCB or Just random IGCB or something. So, those are kind of my open questions. I'm also looking for applications. So, if you want to talk more about, let me know. So, I'll stop here. I'm not supposed to go. Sorry, questions. Yeah, so you demonstrate that, for example, key is one. That, for example, P is 1.1, it's better than P equals 2, and also some more complicated Jacobian is better than the most recent spectrum.