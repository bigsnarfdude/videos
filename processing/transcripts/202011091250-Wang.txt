Share the screen yes, we can see, we can hear you loud and clear, so take it away. Okay, great, thank you. Yeah, it's my pleasure to be here to talk about my work on doing the basic inference for parameters in transmission models expressed as ordinary different. Express just ordinary differential equations. So um Okay, so as I remember for the SIR model, so here I'm going to use this SERL model as a motivating example for transmission models. So in this SERR model, so it's a commonly used model for describing transmission disease. And in this model, it divides the population of host into Population of host into four compartments. So the S denotes the susceptible, and E is for exposed, I is for infectious, and R is for removed. So the model describes each of the four classes, how it changes with time. And there are some annual parameters in this model. It is beta, the It is beta the infection rate, omega incubation rate, and gamma the removal rate. So usually these parameters are unknown and has to be inferred based on partially observed or noisy data. So that would be my focus here. So I'm interested in doing the Bayesian inference for the annual parameters from ODE model. ODE models. So, in a more general notation, so I will use this XI of T to denote the dynamic process time period T1 to Tj. And this change of Xi of T, that is the derivative of Xi of T with respect to time, can be expressed as a parametric. be expressed as a parametric form gi of x of t given some unknown parameters theta. So usually we can assume the parametric form Gi is null, but the parameters in the ODE, which is theta here, is unknown. And also we don't know the initial state xi at time t is equal to So we may have some observations. So this can be described as a likelihood model, L of Y, given X of T and some parameter theta Y. For example, we may consider this observation J for the S O D E variable Yij follows. yij follows a normal distribution with mean xi at time tij with the variance parameter sigma i square. So in this case, theta y includes this sigma i square. So that is we have some unknown parameters theta from the ODE and we may also have some unknown parameters from the observation model. From the observation model. So this is denoted by theta y. So I would like to do the basic inference for these anode parameters. I will distinguish two methods. The first method is relies on using the ODE solvers. So recall in the likelihood function, so that's our likelihood function. So we need this X of T. X X of T. However, we don't know them. So one way is to use the ODE servers. That is, given the initial state XLT, we can use some numerical ODE servers to get XLT, and then we can use this likelihood function. So in this case, our target distribution in the Bayesian framework. In the Bayesian framework, is to do the posterior distribution of these parameters θ, θy, and x of t. I will denote this by beta. So that's one method that it relies on the ODE servers. Another method is that we don't want to use ODE servers for some reason. For example, the ODE servers could be Could be expensive to computationally expensive and it may sensitive to some parameters. That is, we can approximate the ODE solutions by a linear combination of basis functions. For example, we can express xi of t. uh i xi of t using a set of basis functions phi k of t times the coefficients c i k so in this case so the basis functions are fixed so after you choose your set of basis functions so they are fixed and no for example you can choose a set of base plan basis functions and in this case we don't And in this case, we don't need to use the ODE servers to get xi of t. Instead, we change the problem of getting solving the differential equations, get x of t to a problem of estimating the parameter, which is the basis function coefficient C here. Of course, here, so we need this x. Here, so we need this x of t to satisfy our differential equations. So, this is obtained using a prior distribution for C. So, the main part is that we would like to take the difference between the two sides of the differential equation. More specifically, so here we take the integration of the square, the difference between the two sides of the differential equation. Two sides of the differential equation. And this is the main part of the prior distribution for C using this. So it's equivalent to have a penalty term for the difference between the two sides of the differential equation. So it can force this x of t satisfy our differential equation. That's another parameter lambda here. So it controls the trade-off. So it can choose a trade-off between fitting to the data and fitting to the differential equation. So in this case, we are interested in getting the posterior distribution for all these anode parameters theta, theta y and c and lambda. And this is proportional to the likelihood. Is proportional to the likelihood times the priors. So we are using this pi to denote the anomalous posterior distribution, and we are using gamma to denote the anomalous posterior distribution. So again, we are using the same notation beta to denote all the anode parameters here. So basically, we are doing the basic inference for some anode parameters. For some annual parameters, the beta. So, the algorithm I'm proposing to use is based on sequential mental colour methods. So, again, here our target distribution is a complicated distribution that is harder to sample from. So, therefore, we can approach this problem using a sequence of intermediate distributions that is go from a simpler distribution. A simpler distribution to a complicated distribution. So, more specifically, so we define our earth intermediate distribution as this target distribution to the power alpha r times the reference distribution rho of beta to the power 1 minus alpha sub r. So, here we have this sequence. And have this sequence of annealing parameters alpha sub R. So if we have the initial annealing parameter, which is equal to zero, so this is equivalent to our reference distribution, rho or beta. And if this annealing parameter is equal to one, so this is equivalent to our target distribution, pi or beta. So, that is, we consider a sequence of intermediate distributions. The first one is a reference distribution. It could be a simpler distribution that we can sample from. And the last distribution is our target distribution, which is the positive real distribution of beta. Excuse me. So, this is our new SMC. a new smc so initially so we can sample our particles from a reference distribution and we set the unnormalized width for each particle to be one and at the following iterations so we can propagate our new particles beta sub r using the previous particles beta sub r minus one beta sub R minus one and a mark of kernel T here. So after we get these samples or we call particles, we update their weights using this formula here. So as you can see, so this weight update involves the anomalous target distribution at region R and at region R minus one. And also it involves this it involves this mark of kernel T and also a backward kernel L here. So after we get this anomalous weights, we normalize them and use the to denote the normalize the weights. So if the effective sample size is too small, and we do a resampling step to resample these particles. Resample these particles. Okay, so there's a Markov kernel in the algorithm. So here we can use the Markov kernel, which is the pi sub R invariant MCMC moves. Basically, here we run an MCMC move with the target distribution is pi sub R. So one example. So one example is that we can do a Gibbs step that is sample one parameter based on the full conditional distribution of this parameter conditional or the other parameters. So if the full conditional distribution doesn't have a closed form, so we implement one step of the metropolis hasting algorithm. Of course, you can also use Of course, you can also use the more advanced JAMCMC moves. For the backward mark for kernel, so we can use convenient kernel as this form. The advantage of using this backward kernel is that it can simplify our weight update. So, as you can see, so when we have with update data as this form, so it only depends. This form, so it only depends on the previous particles at region r minus one, and it also involves the next kneeling parameter. So this can give us the way that we can determine the knitting parameter adaptively. So the criterion that we are using to choose the annealing prime. Choose the annealing parameter adaptively is based on the conditional effective sample size. As you can see, it's a function of the normalized weight W from the last light region and the weight update small w. As I mentioned, this small w involves the particles from last rate region and the new leading parameter are new leading parameter alpha sub r. So if we set this conditional effective sample size to be user specified threshold, for example, 0.99. So we can solve this equation to get the annealing parameter alpha sub r so in this way we can determine the sequence of annealing The sequence of annealing parameters. And therefore, we have the sequence of the intermediate distributions. Then we have our annual SMC method. Okay, so that's the sequential monte colour methods for doing the Bayesian inference for parameters beta. So now I'll show you some simulation. I'll show you some simulation results. So here I consider two differential equations, S1 and S2. There are two parameters, theta1 and theta2. So I purposely created two modes for the posterior distribution. So basically here, theta one can be equal to two or minus two. Minus two. So I consider four algorithms. So the first two relies on the new SMC I just introduced. The difference between them is the one I used the base plan basis function expansion. The other, it relies on the ODE server. And similarly, we can run the MC. we can run the MCMC methods with the base plan basis functions or with the OD servers. So here the MCMC moves are the same as those used in the SMC method. So here I show the particles of theta one and theta two in the The SATA2 in the first row from the SMC measures. The first one is used and the second one used the OD server. As you can see, so the green dots are the initial particles and then I plot several intermediate steps. So that is yellow and light blue. And then dark blue is for And then a dark blue is for the last iteration. So it covers the true value denoted by this black dot. And in contrast for the OD servers, so it's close to the true values, but with some barriers, but the variance is smaller. So currently, we're still doing more comparison between the two. When we compare the SMC method to the MCMC, SMC methods to the MCMC methods with the same computational cost. So the MCMC methods with the same MCMC moves has difficulty with its problems two modes. So we also used our method for some more realistic transformation models. For example, this example this physical distancing model proposed by Caroline Collins group and in this model we have this set of anode parameters and we consider this F which is a distancing parameter and after we do the Bayesian inference so we can make all kinds of plots for example we have the confirmed COVID COVID-19 cases, which is the dots here, and we can estimate the active cases and the 95% credible bands. So as a summary, so our basic measures using the new SMC has several advantages. So it can provide uncertainty estimation for the parameters and it shows some advantages. And it shows some advantage over the MCMC methods with the same moves. And if we don't want to solve the ODE numerically, we can approximate our ODE solution using a linear combination of basis functions. Okay, so thank my co-author, Rani Daig, and my former student, Shujia Wang. And thank you. And here are some references. And here are some references. Thank you. I'll stop here. Great. Thank you very much. We're again right up against the time. So if there's a quick question, put it in the chat or feel free to speak up. Maybe one quick question. Okay, and if there's no, I thought I heard some oh, yeah, was it Jason? Please go ahead. Jenka Wu from York University, Dr. Wong, thank you very much for the talk. What if the age-specific contact mixing is engaged, is used? How complicated of the parameter identification how computational intensive? So, your question is about how complicated the ODE models we can handle. How large a scale, five, ten age groups, for example, in the population. So, right now we haven't tried the age structure the models, but our methodology is more general, so it can apply. So it can apply to more differential, all types of different ODE differential equations and also delay differential equations. So it depends on the complexity of the model and also the number of annual parameters. So the computational cost may be different. If we have a more complicated model, it just needs a larger number of particles. A larger number of particles and more a larger number of SMC iterations. But we can determine the number of iterations adaptively by specifying this criterion for the conditional effective sample. Usually we need it to be a large value so that we can have a good estimate. So the draft. So the yeah, the draft answer is if we want accurate images, so we need to have a larger value for the conditional effective sample size, which means we have a larger number of SMC iterations. But it doesn't involve a lot of tuning, it's just we need to use a larger value for the conditional effective sample size.