          He did a PhD and currently in Melon University where he worked with Tom Michelle and he started working on task learning and transfer learning. Since then, which has made many major contributions to the field of machine learning and more recently interpretability. Interpretability. So, among other awards, we have received many prestigious awards. I would like to mention the NSF Camera Award for Meta Clustering, several best vector awards. I'm not going to name all of them because there are several. And also he serves as ELIA chair in several major conferences like RIPS, ICML, KDE, etc. So his research focuses on learning for medical decision making, transparent modeling. Making transparent modeling and computational ecology. And with that, I would like to give the floor to Rich. Very much, thank you very much for being here with us online, and we are very much looking forward to hearing your thoughts. Thank you. Maria, thank you very much for the introduction. And thank you, everyone, especially those who are attending in person. I apologize that I'm not there in person. I was planning to come. And I did a BAMF four years ago. A banff four years ago, right where you're sitting now, and it was wonderful. So, I really was hoping to be there in person. Okay, so let's get started. Let me know if you can see my slides. Is that a yes? Yes, I can. Great, great, thanks. Hey, so I'm going to talk about the risk of using black box models and the importance of intelligibility in machine learning. Now, I think I'm preaching to the choir. I think we all believe that, or we wouldn't be at this. I think we all believe that, or we wouldn't be at this meeting. What I'm going to do is, because on the first presentation, rather than do too much math and algorithms, what I'm going to do is try to show you a lot of examples where intelligibility really is valuable. Now, this is work we've been doing about 10 years. So many people have collaborated on this. In red, I've got a number of great grad students who have worked on this, done internships, a number of people from Microsoft, doctors, because I do a lot of work in machine learning for healthcare, other faculty from other places, and From other places, and some other people at Microsoft who are researchers like me. I want to single out somebody, Ben. I'm going to embarrass you. Ben is sitting there in the middle of the room. A lot of the work that I'm doing is done with Ben, and you'll see his name pop up multiple times in the presentation. Also, Ben will be giving a really interesting presentation on Thursday. So I hope you get to see Ben's presentation. And Ben knows this stuff as well as I do. So if you have any questions, Well, as I do. So, if you have any questions and it's harder to get in touch with me because I'm online, go talk to Ben and he'll be happy to answer your questions. Okay, you might have heard that there's an accuracy intelligibility trade-off, that black box methods like deep neural nets and random forest-boosted trees, you know, very accurate, but not very interpretable. That's why we call them black box. And glass box methods like linear and logistic regression, things like that, very small trees. Hey, they're intelligible, but unfortunately, they're not very. But unfortunately, they're not very accurate. You know, this is no longer true for tabular data. By tabular data, I mean things where you know what each feature means. For example, if I tell you a person's age or their blood pressure or their income, you know what those features mean without knowing about any other features of the person. Whereas a pixel, if I tell you the red channel on pixel 467 has a value of 12, I really haven't told you anything about the image. I really haven't told you anything about the image. You have to know all the adjacent pixels. So, raw signals, often you need to know all of the adjacent information to make sense of it. Whereas tabular data, and it's been estimated that 60, 70% of the world's machine learning problems are on tabular data. On tabular data, it's a very different property. And this property that you can understand features independently makes intelligibility much easier for tabular data. And the good news is that there is no longer an accuracy intelligibility trade-off on tabular data. Intelligibility trade-off on tabular data. We have a method that we're going to talk about called explainable boosting machines, EBMs, and it is just as accurate on tabular data as deep neural nets boosted trees in random forests, and yet it's even more intelligible than linear and logistic regression and things like that that have been the gold standard for intelligibility for many years. So let me just give you some evidence for that accuracy being so high. In the first column, we've got the AUC accuracy. The AUC accuracy on 10 different classification data sets for EBMs. And notice that the average accuracy is 0.866. And on the right, we've got full complexity models that have been well optimized, like random forests and gradient-boosted trees where we optimize their depth. And notice that the full complexity gradient-boosted trees also have an average AUC of 0.866. So this is one of those cases where the average accuracy of an EBM model is comparable to the average accuracy. Is comparable to the average accuracy of a black box model. And that's great because the glass box model is fully interpretable. Now, I'm a co-author in this paper, so you should take it with a grain of salt. But this is some work from Cynthia Rudin and her students. Cynthia, by the way, will be presenting, I think, tomorrow remotely at the workshop on a different topic. But this is some work they published a few years ago. And the little blue bars and the arrows are pointing to our results. And they compared a number of And they compared a number of glass box and black box methods. And it turns out of the glass box methods, we were the most average, most accurate on average glass box method. And then they observed also in their work that the best interpretable models, and that would be EBMs, perform about as well as the best black box models. So that's completely independent work of ours. And just so you know, I won't talk about this much today, but we now have a way of training these interpreters. Have a way of training these interpretable models using neural nets as opposed to boosted trees. So these are called NAMs, neural additive models. And the neural additive models are similar in accuracy to the EBMs. They're not more accurate. They're not less accurate. They're about the same, which is what we would have hoped for. So we now have a way of training these very interpretable models using neural nets if you prefer using GPUs or need to scale to some very large data set where GPUs are essential. But if you're into boosted. Use are essential. But if you're into boosted trees, we also have a good way of training these with boosted trees. And the accuracy is pretty comparable to things like XGBoost to NDEP neural nets. So that's great. So this is just more evidence that these glass box methods on tabular data are now completely competitive with black box methods. Let me tell you what these things are. They're a form of what's known as generalized additive models, and some of you may already be familiar with these. A linear regression. A linear regression is the original and simplest additive model. Logistic regression is sort of linear regression with a sigmoid wrapped around it so that it generates probabilities. And this is the gold standard for interpretability. Sadly, it's not as accurate as we would like on complex data sets because it can't model nonlinearities. In particular, in healthcare, it can't model things like normal is in the middle and low risk is, say, an intermediate temperature, but if you're hypothermic or if you're feverish, that's high risk. It can't model. Feverish, that's high risk. It can't model that unless you do good feature engineering. And sometimes, by the way, these linear logistic regression models, they just get the sign wrong because they can't fit the data well. They struggle and they end up trading off accuracy on one term versus accuracy on other terms. And they actually get the signs wrong. So even gross things like the sign of the coefficient are not trustworthy in linear logistic regression models. We really should stop using these unless we know they're the right model. The other extreme, we've got black box methods. The other extreme, we've got black box methods. So these, of course, are not very interpretable. That's why we call them black box. They can be very accurate, and that's because they can model nonlinearities. They can easily model things like normal is in the middle, cup-shaped or hat-shaped phenomena. They are, however, a little more likely to learn spurious effects. So if you use black box intelligibility methods like Lyme and SHAP to try to understand what's inside your black box, you'll find that your black box has often learned some things which are high variance aspurious effects. Of spurious effects. So, you do have to be careful about what these models tell you. Now, generalized additive models, of which EBMs are an example, are in the middle. So, they're much more interpretable than linear and logistic regression, and that's because they can represent a more complex shape function that is very interpretable to experts. So, slope is not very, it's easy to understand, but it's not very explanatory of what the data is saying. Whereas it turns out, as you're going to see many examples in the presentation, It turns out, as you're going to see many examples in the presentation, that these nonlinear graphs can be very, very informative. Because they can do nonlinearities, they can be very accurate. They can easily model things like cup-shaped and have-shaped phenomena. Most importantly, because the variance is very controlled on these things and they're a simpler model class than black box methods, these things are less likely to learn spurious effects. And often the things that they learn are real, and you can kind of trust them and learn from them. We didn't invent these. We didn't invent these. GAMs were invented by Trevor Hastie and Rob Tipchirani in the late 80s. This was Trevor Hastie's thesis. So they've been around longer than most machine learning. And our contribution is going to be to put these things on modern machine learning tricks, on modern steroids. And we're going to get an accuracy and intelligibility out of these models that you really couldn't achieve 10, 20, 30 years ago. So we're just going to make these things high flyers and make them comparable in accuracy to black box methods. To black box methods. Let me show you briefly what these look like. A linear model is an intercept plus a slope times a feature plus a slope times a feature. So that's a simple additive model. And then full complexity models are just some function of all the available features. These additive models are somewhere in the middle. So it's a function of the first feature plus another function of another feature. So it's functions of every features. And these functions can be arbitrarily complex. Can be arbitrarily complex. They're not restricted to being linear. So these can be multimodal, non-monotonic. These can be much more complex functions, but it's still a restricted model class. It's a function of a feature plus a function of a feature. Now we can make that model class a little more complex if we want. We can have a sum of functions of individual features. That's the line up above. Or we can do a sum of functions of pairs of features. And now we can do pairwise interactions. Or we can have a sum of functions of triples of features. And now we can do. Triples of features, and now we can do three-way interactions. And what we're going to do is restrict ourselves to main effects, functions of individual features. You're going to see lots of these in a few minutes, and a few functions of the most important pairwise interactions. And we find that if we restrict ourselves to this model class, 98% of the time, we're just matching the accuracy of the black box methods. And in the other 2% of the time, we need to add a three-way interaction. Three-way interactions are a little harder to visually interpret. So if we can avoid them, Interpret. So, if we can avoid them, we like to avoid them. Okay, I'm going to jump into an example. We'll do this pneumonia example for a while. Some of you may have heard this before, but don't worry, there are other results that are going to come later in the presentation that you probably haven't seen before. So this is an older data set. It's got 46 features. And remember, our model class is a function of a feature plus a function of a feature. So we're going to have 46 functions, one for each feature. And then we'll throw in 10 pairwise interactions. So we'll have a total actually of 56. So, we'll have a total actually of 56 functions. And we know things like the patient's age, their gender, if they have cancer or asthma. They've all had a physical exam. We know their heart rate, their body temperature, what their breathing sounds like. They've all had blood tests, so we have laboratory findings. And they've all had a chest x-ray, and we have doctor's notes from the chest x-ray. So remember, it's going to be 46 functions plus 10 pairwise interactions. From a high-level view, that's what this looks like. So here's your first function. Like. So here's your first function. This is risk as a function of age. Here's risk as a function of asthma, a function of glucose, a function of blood, urea, and nitrogen. It's very important to realize that this is not marginalization. We're not learning risk independently for each of these features. We're using a very carefully designed algorithm that is training all of these things in parallel. So if it decides that risk should be on this graph, then that risk will not be on other graphs. So it is figuring out where variance should be moved in the model. Should be moved in the model. It's a fairly sophisticated modern machine learning-based generalized additive model. Now, the way you use the model is you look up, say, the person's age on the age risk graph. So maybe they're here. And then you read off the y value, and that's a contribution to risk. And here we're predicting the probability the patient might die. So we're doing classification. So what this vertical axis gives us is a logit. So that's basically a log. So that's basically a log odds on a log scale. So, and the more positive it is, the higher the risk, the more negative, the lower the risk. So, we get a score from this graph. We get a score from the asthma graph, like minus 0.15. We get a score from the blood urea nitrogen graph, like minus 0.21. And what we do is we sum up all those scores. They're on a log scale, so we can sum them, and that's equivalent to multiplying them in the log odd space. And then we take that score, and we just do the standard logistics. And we just do the standard logistic regression trick: one over one plus e to the minus score. In fact, if this had been logistic regression, all of these graphs would look like straight lines. And because of that, we probably wouldn't even bother to plot the graphs. We would just use the slopes, the coefficients for these things in a table as opposed to plotting graphs. And we would just have for logistic regression, one over one plus e to the minus straight line, you know, curve fit of all of our data. But instead, we've got these graphs. But instead, we've got these graphs so we can have this non-linear effect on each of these features, and our score is going to come from all of these graphs. But you could think of it as if all of these graphs were sitting up here in the exponent of the logistic regression model. And then we have all these nonlinear terms now instead of linear terms in logistic regression. So we take the sum of those scores, and maybe a patient has a score of minus 0.78. A patient who has a score of 0 would have a 50. Who has a score of zero would have a 50-50 probability of death? That would be one over one plus one, so one over two. Here, a score of minus 0.78. These scores are on log scales, so small differences make a big difference. Minus 0.78 is a probability of death, which is still very high of about 30%. You'd really want to be a minus two or a minus three, and then your probability of death would be down around 10%. All right, I'm going to zoom in on one of these graphs. This is risk as a function of age. And remember, Risk as a function of age. And remember, this graph of risk as a function of age is learned in parallel with all the other features. Okay, so that's very, very important. The bottom, we just have a histogram of our patients. Most patients are between 60 and 90 years old, some younger, some older. The top is the interesting graph of risk that's learned by the model. And down is low risk, up is higher risk. And so let's read what the model has learned. So it's learned, first of all, that it's kind of good to be young. You know, kind of good to be young. Everybody under 50 seems to be low risk. The model could distinguish between people in their 20s, 30s, and 40s if it saw evidence for this, but it doesn't see any evidence. So it just predicts they're all equally low risk. Then risk goes up a little bit as you go through your 50s into your 60s. Then risk rises pretty rapidly at around 67, 68, 69. This is retirement in the U.S. And by the way, we often see this in medical data sets in the US, that we see a small increase in risk. A small increase in risk for patients as they go through retirement. Lots of things happen in retirement. Your insurance provider probably changes. Maybe you're on Medicare, Medicaid now. Your health provider might change, possibly because of the insurance. There's a good chance you've moved now that you've retired. Maybe you don't even have a doctor lined up, so it takes you longer to get to care. Anyway, lots of things happen at retirement. And in the aggregate, these things often increase risk a little bit. Then risk goes up pretty rapidly. Little bit. Then risk goes up pretty rapidly as you become elderly, as you go through the 70s into the 80s. Surprising step function rise in risk right at 85, which is a very round number. Surprising flatness in risk in the upper 80s and 90s. And an even more surprising step function drop in risk that's happening right around 100 and 101. This rise in risk at 85, there is nothing in healthcare policy which says you should treat a patient who's 86 differently than a patient who's 84. Who's 84? Doctors explain this. This is an older data set from 1989. Doctors explain that life expectancy at this time in this data is probably around 80. So patients over 85 are definitely considered very elderly. And pneumonia was sometimes called the old man's best friend. The idea is you're very elderly. You have the illnesses, comorbidities associated with being elderly. Because of those other illnesses, you've now got pneumonia, perhaps because you're suddenly bedridden. Perhaps because you're suddenly bedridden, and that's a prime way to get pneumonia is being stuck lying down. And now you get pneumonia. The first or second round of antibiotics hasn't helped you. And at some point, you're elderly, you have a number of other illnesses that perhaps put you at high risk of dying in the next six months or a year anyway. Perhaps what happens is the pneumonia comes along and it takes you out in a couple of weeks. And the doctors call it the old man's best friend because possibly this. Friend, because possibly this more rapid decline and death is a kinder, gentler way to pass away than a sort of lingering illness that is going to take you out in six months or a year. Now, actually, pneumonia is not a friendly way to pass away. So they no longer use this phrase. But many doctors remember thinking this way back in the 80s and 90s. And we think what's happening is that the model has just recognized that the patients who are over 85 are getting loose. Over 85 are getting less aggressive treatment than the patients who are under 85, just because they're so elderly and because of their other conditions. We think the same thing is happening at 100, 101, but in the opposite direction. And what we think is happening here is the patients have made it to being 100 or 101. They're centenarians, which is a really amazing thing to have made it that far. And pneumonia is a treatable illness. And we think what's happening is the doctors do. And we think what's happening is the doctors do not want the patient to die on their watch. They want to go for the record. You know, you've made it to 101. Maybe your birthday is two months away. They want to get you to that next birthday. And they just really do everything they can to fight for the patient. And it's possible that the patient, the patient's family, grandkids, all want you to keep fighting as well. So we think this rise in risk at 85 is a social phenomenon, people just being a little less aggressive for elderly patients. And we think For elderly patients. And we think this drop in risk at 100 is also a social phenomenon. Basically, doctors being very aggressive for patients who have made it to being 100. Now, do we really believe that patients who are 105 are lower risk than patients who are 95? Doctors tell us this is not the case. This is an interpretable model. These graphs really are the model. We can edit this model. And if we edit it the following way, this model will be monitored. Following the way, this model will be monotonically increasing in risk past, say, age of 85. So we can take this kind of model, detect these sorts of anomalies that we don't like in the model, and then we can go ahead and fix them just by editing the model. The accuracy on the test set, held out test set, will slightly go down because this is a real statistic in the data that people over 100 are slightly lower risk. But we believe the explanation for that is because the patients are receiving extra aggressive care, not because they're intrinsically lower risk. Care, not because they're intrinsically lower risk. And we're afraid that if the model predicts they're lower risk, they might not get that aggressive care. So we want to be careful to edit the model to fix it to make this problem go away. And we're never going to get data where these patients did not receive this high-quality care. So we can't actually get data where this problem doesn't exist. It's going to be ubiquitous in many data sets. By the way, the accuracy of this model with and without this edit, it is the most accurate model that anyone can train on this data set. So I'm telling you this very detailed story about. You, this very detailed story about it's good to be young, 50s and 60s, not too bad, retirement's bad for you, getting elderly is bad. Over 85 is an interesting round number, over 100 is a very interesting round number. That nuanced story that I'm telling you is for a model that is as accurate as any other model that can be trained on this data. And not only is this a really accurate model, but we can correct the model when we see problems in it. Do we always want to make this correction? No, if we're an insurance company, Now, if we're an insurance company who's not going to intervene in patients' health care, we just may want to know that patients over 100 are a little more likely to live than patients in their 90s. So we don't want to edit the model. We just want to understand it. That's very, very important. So model correctness and accuracy depends on details of how the model will be used. Insurance company, don't edit the model. If you're going to use it to give care to patients, you should edit the model. That's very, very important. Let me show you. Very important. Let me show you some things the model learned. Jumps at round numbers happen all over. I'll show you a few more of these. These are almost always due to human social policy effects. There are very few phase changes, especially in biology. The model has learned that asthma lowers risk. These are asthmatics on the right. Notice they're lower risk than the non-asmatics. This is real in the data. It turns out the asthmatics are lower risk because they notice the symptoms of pneumonia early. They have a doctor who treats their asthmatics. Early. They have a doctor who treats their asthma. The inhaler didn't work, so they call the doctor. The doctor says, Wow, the inhaler didn't work. Get in here. They get an appointment. The doctor diagnoses they've got pneumonia. They have a history of asthma. The doctor says, ooh, we're going to really hit you hard with treatment. So they get to treatment very quickly and they get very high quality aggressive treatment when they get there. And that it's so important if you've got pneumonia to get care rapidly and high quality care that they actually are lower risk than the non-asmatics, even though doctors. Non-asmatics, even though doctors tell us that they actually are higher risk biologically, but they're lower risk because of the care they get. We see a similar thing for history of chest pain and heart disease. The heart disease patients, they wake up, their breathing feels funny, they're afraid they might be having a heart attack. They get in an ambulance if they're smart. They get to the ER. They get to the front of the line of the ER. They put probes on them. And within minutes, they know they're not having a heart attack. They later diagnose that they've got pneumonia. They have a history of heart disease. They're also considered high risk. They're in the ER. They're also considered high risk. They're in the ER already. It turns out they get care even faster than the asthmatics do. So their risk is even lower. Then there's patients who have strider. That's a kind of wheezing that's a sign of advanced pneumonia. And it turns out these patients, the urban bars are big, but these patients also have reduced risk. And it's because strider is one of those symptoms you can't ignore. Your breathing sounds so bad, you know you have to go get care. And if you have a treatable illness, you would like to have symptoms as early as possible. To have symptoms as early as possible and symptoms that you can't ignore as early as possible so that you get to care as quickly as possible. Now, the doctors assure us that none of these things are biologically true. They're all statistically true in the data, but they are not biologically true. And you got to realize that every machine learning method that can learn complex models will learn these same things. So random forests, boosted trees, deep neural nets, they're going to learn exactly these same effects, but they're just going to be hidden in the model. You'll never know they're there. The model. You'll never know they're there. And in fact, we trained many other models on this data, and we never knew that most of these effects were there. So it really is a tragedy to use black box methods, say in healthcare, where you're going to allow effects like this to be learned by the model, and then it's going to make predictions. But with this model, the doctors can recognize that these things don't make sense, that it's not how they want the model to work. And we can do things like edit the model. If we flatten these graphs, suddenly these effects go away. Suddenly, these effects go away. Or the doctors can tell us: no, I want the asthmatics to be at higher risk, the heart disease patients, even higher risk, and the patients with advanced pneumonia and Strider, the highest risk. And we can do those edits. And you really do want to do those edits. Again, accuracy on the test set will go down slightly because the red line is what the statistics look like in the test set. But doctors assure us that this model will be more accurate and safer to use in the real world. This tells you that accuracy on held-out test data is never. On held-out test data is never a sufficient metric to know if a model is correct or safe to use. You have to stop using accuracy as your main goal in determining whether a model is good or bad. You really need intelligibility to know if a model is good or bad. Now, here's the interesting thing: this is an additive model. So, a model that predicts a patient who's 105 years old, who has a history of asthma, has a history of heart disease, and History of heart disease and has had a heart attack, who now has advanced pneumonia. The model would predict that they are ultra-low risk if you don't correct the model. And that's terrible. Like we don't want to send the 105-year-old asthmatic with heart disease and advanced pneumonia, you know, like skiing on the slopes on vacation because they look like they're low risk. We really have to fix the model. Even worse, this model is rewarded with extra high, like extra human accuracy on the test set for predicting these wrong things. Test set for predicting these wrong things. This is terrible. So, the model is rewarded with looking like it's high accuracy for predicting these crazy things. Intelligibility is very important. You might think you could solve these problems by, say, removing heart disease as a feature from the model. It turns out that doesn't work. The bias is actually in the training signals, the targets, not in the input features. The input features are just facts about the world. So, it turns out there's enough correlation between heart disease and age and diabetes. Disease and age, and diabetes, and blood pressure, that even if you were to remove heart disease as an input to the model, it'll learn to get its heart disease fixed. It'll learn to do the wrong thing as much as possible through correlation with these other features. So what we do now, for example, maybe we're worried about race or gender bias in the model. We now make sure that we include race and gender in the model. We let the model be as biased as it possibly can be. We make sure it learns these bad things. It learns these bad things because then we can see the bias that's in the data. And once we can see it and it's localized on these features, then we can correct it. So we think about this completely differently. When we first worked with this data many years ago, we removed a feature like Azima because we were afraid it was going to learn the wrong thing. We now recognize we should have left it in, and instead we should have corrected the effect in the interpretable model afterwards. The problem is we didn't have high accuracy interpretable models back then, so we didn't know how to do that. Back then, so we didn't know how to do that. But now we can kind of have our cake and eat it too. We can have really high accuracy models that are very interpretable and they're editable. This is another graph for pneumonia. This one, you know, I've been showing you how models can learn the wrong things and you need to edit them. I want to show you how models can learn some beautiful things. So this is your chance of dying from pneumonia as a function of your blood urea nitrogen. By the way, this is a blood test we've all had. This is a very common blood test. And notice that risk is low. And notice that risk is low until you get to about 35 or 40. That's medically correct. Risk goes up, it flattens right around 50, then goes up, peaks at 100, and then comes down. It turns out for blood urea nitrogen, risk just keeps increasing the higher your blood urea nitrogen is. So why does the graph flatten and then peak? Well, it turns out these are treatment effects. So what happens is when you hit 50 in this data, doctors start giving patients a medication for their high BUN. Medication for their high BUN. And that medication is so effective that the risk actually flattens and is flat for a while, say from 50 to 75. So that's great. It's a sign of very effective medication. Notice it happens right at 50 because doctors are using round numbers to decide when to treat patients. Then finally, the medication isn't enough. Your risk continues to go up as your BUN gets higher and it peaks right at 100, another very round number. And then it actually starts to go down, which doesn't make sense. Ben calls these count. Ben calls these counter-causal effects that we see in graphs. Basically, we're seeing something where we know risk is actually increasing with high bun, and yet the model is learning because the statistics are truly saying that risk is going down as BUN increases. This is because at 100, doctors start giving the patients dialysis. So, and by the time you get to 120, all the patients who are eligible for dialysis are probably getting it. And dialysis is very, very effective. So, it's not that the patients are intrinsically lower risk, it's that the Patients are intrinsically lower risk. It's that they're receiving a treatment which makes them lower risk. But doctors were kind of disturbed. Notice that the patients who are healthier with a BUN, say, of 90, are actually higher chance of dying than the patients who have poorer health with a BUN of 120, because these patients are getting a treatment that the patients at 90 are probably not getting. So, one of the things we can do with this model is we can treat it like a spreadsheet, and we can do a sort of what-if analysis. I won't go into the detail of how we do this analysis. Detail of how we do this analysis. But we can conservatively estimate that if we moved the treatment threshold from 100 down to, say, 85, that the risk curve would look no worse than this red line. And the area between the red line and the green line is lives saved. And in the U.S. alone, just for pneumonia, just for this one blood test, just for this one treatment threshold, we can save something like 2,500 to 5,000 lives a year. So it's amazing what you can learn from observational data if you train a very high. Data, if you train a very high accuracy, intelligible model on it. So it's really remarkable. And this immediately led doctors to think about doing a clinical trial where they would actually change the treatment threshold for blood urea and nitrogen and dialysis. The good news is that this is an old data set, and many of the things that this change, this graph suggests, have already come to pass. So healthcare has figured this out without this model. But we can see other effects on. But we can see other effects on other graphs that are things that we think would improve healthcare. And Ben might be able to talk more about this if you're interested. These treatment effects are all over in data sets. This is your systolic blood pressure. Notice that risk goes up and then comes down as your blood pressure gets higher, goes up and comes down as your blood pressure gets higher. That's not true. Your risk never comes down as your blood pressure continues to climb very high. These are patients hitting different treatment thresholds, round number effects, 175, 200, 225. 25, 200, 225. Basically, you know, the doctor here is thinking, wow, your blood pressure is 203. That's over 200. We got to do something. And you're just more likely to get that treatment if you're, you know, 201 than if you're 195, because being over 200 really sounds much worse than 195. So we see these round number treatment effects all over in data. Let me show you what an interaction is. We have 10 interactions in this model. This is risk as a function of age. You've already seen that. Of age. You've already seen that. This is risk as a function of whether you have cancer. And as you might imagine, having cancer does increase your risk. There's something you can't reason about, though, from those two separate graphs. This is the interaction, and yellow is adding the most risk to the patient. So very young patients who have cancer are actually getting this big 0.5 added to their log odds risk. So why is that happening? Especially young patients are usually low risk. Well, it turns Patients are usually low risk. Well, it turns out the young patients are treated more aggressively for their cancer, and that tends to knock out their immune system. So, patients who are very young, say under 25, are treated very aggressively. But even patients who are, say, 25 to 35, 45 are treated more aggressively for their cancer. That knocks out their immune system. They have pneumonia or they wouldn't even be in this data set. And if you have had your immune system knocked out and you've got pneumonia, that does put you at higher risk because you're less likely to be. At higher risk because you're less likely to be able to fight off the pneumonia. So it turns out this model is learning automatically that there's a special category of young patients who are at very high risk. And these are the patients who are young and have cancer. So that's a pairwise interaction between age and cancer. Let me talk briefly about ICU mortality. This is one of the mimic data sets. This is risk as a function of your PF ratio, a measure of how well you're breathing. Low PF is bad. Plus one is very. OPF is bad. Plus one is very high risk. Healthy people are out here near around 1,000. You're at altitude right now, so maybe you don't feel this healthy, but you probably are. And then the funny thing is this little blip that's happening in the graph. And notice that the risk right here is as low as the healthy patients, but it's in an unhealthy region of the graph. So we didn't know what was going on there. We thought it might be a round number effect. It's 323.6, so it's not a round number effect. It turns out it's because. Out, it's because this feature has 60% missing values, uh, and they were all imputed with the mean, and that's where the mean is for this feature. And why are they missing? Well, in healthcare, they're often missing because if you look healthy in this dimension, they don't bother to do the test. So they just pinch your finger. If pink returns quickly, your blood oxygen is good. They don't measure your PF ratio. So what happens is we've got 60% very healthy patients sitting here, and the model is doing everything it can to come down, indicate they're low risk, and then go right back up. Indicate they're low risk and then go right back up because the patients on either side are low risk. Now, you might think we could fix this by editing the graph. We don't want to do that because if we edit the graph and we impute missing values for the mean this way, we'll be predicting that healthy patients are higher risk and that puts them at higher risk by possibly giving them treatment they don't need. We don't want to leave it the way it is, though, because there are really some patients who have PF ratio 323 and are unhealthy, and they're getting computed as having. And they're getting computed as having low risk. And that's not right either. So it turns out we have to change the way we handle this missing value. And I won't go into that now, but we've done an entire paper on this. It turns out that transparent modeling has just completely changed how we think about and handle missing values. Let me show you some stuff that's not medical data. We're trying to predict the housing price, and one of the features is the year in which the housing was built. This is a regression problem. So this is a Problem. So, this is dollars in the U.S. So, notice right at 1989, there's a spike. Suddenly, the model is thinking that all housing built in 89 is worth about $600,000 more than the housing built in 1988 or in 1990. That's obviously an anomaly. We look in the data set. We pull out all the things that were built in 89. We find there's this one Conda co-op timeshare where everything is listed as $8 million. There are different signs. And there are different size bedrooms and bathrooms, but they're not all worth $8 million. So we know there's some problem in the data. We dig deeper. It turns out all of these condos ended up getting the value of the entire building assigned to them as opposed to the value of the condo. So we have a number of records that are all 8 million. That explains why this spike occurs. We either need to edit this graph or we need to correct this data or remove this data. Let me show you some stuff from Wikipedia Malicious. Some stuff from Wikipedia malicious edits. So, Wikipedia gets almost 200,000 edits per day, about 10% of which are considered to be malicious. That is, people are either trying to spread false information or just trying to have fun by cursing in the document or breaking it. And what they do is they use machine learning tools to predict the edits that are most likely to be malicious because they don't have enough editors to look at everything and double-check it. This is a model we trained for them a couple years ago, about four years ago now. And this is time since. Now, and this is time since the editor who did the edit has registered to do editing at Wikipedia. And this is time in seconds, but on a log scale. Okay. And the interesting thing is, notice that every jump corresponds to a round number. This is nine months, one month, five days, 30 minutes, 60 seconds, five seconds. When every jump in a graph corresponds to some sort of meaningful round number, you know that that's a true causal model that it's learning. Let me zoom. It's learning. Let me zoom in on just one of these because it's really interesting. This is the biggest effect at one month. People who have been editing more than a month are very low risk, and people who have been editing less than a month are very high risk. And notice the difference here. This is plus two for the highest risk and minus a half. That's a difference of 2.5. That's more than a 10 to 1 odds ratio of doing something malicious. Okay, so this is really interesting. What happens is Wikipedia locks you out automatically after one month. After one month. And you have to remember your password and your ID, and you have to go to the effort to log back in. And we think many people who are trying to do malicious edits either don't bother to log back in or they create a new ID when they log back in if they want to continue doing things that are malicious. What's really fascinating is Wikipedia in the six months leading up to the last presidential election in the United States was worried about false news. And what they did was they decided for about 2,000 pages. Decided for about 2,000 pages that are election-related, they didn't want false news spreading fast and they didn't have enough editors to check these things every minute. So, what they did was they made a policy that the only people who could edit these 2,000 pages were editors who had been editing for more than 30 days. So, they basically took this graph to heart and they used it to make a decision leading up to the election of who could edit and who could not edit. Let me show you some severe maternal morbidity. This is where. Severe maternal morbidity. This is work with Ben Langrich. So, severe maternal morbidity means that the mother during delivery, something bad happens. You know, the mother needs a blood transfusion, has a hemorrhage, has an embolism, needs a hysterectomy, has blood pressure that goes crazy. And if you ask doctors, what are the risk factors for this? They'll describe features of the mother, like if the mother has existing hypertension or diabetes or obesity. When Ben and I trained a model. When Ben and I trained a model on this the first time, two features jumped out as being even more important than these features. These are correct features, they're very important. The features that jumped out was the size of the baby, the birth weight. So the larger the baby, the higher the risk to the mother. The other feature that jumped out was the height of the mother. The height of the mother is our best measure of the pelvic opening of the mother. So basically, women with taller frames have larger pelvic openings. Larger pelvic openings. And the taller the mother, the lower the risk. And what the model is telling us: it's the physics of birth that actually is responsible for a lot of the risk for mothers. Basically, a large baby coming through a small birth canal adds a lot to risk, but a small baby or a very large birth canal lowers the risk. This immediately led us to want to create a ratio of these things. So we created a BMI for pregnancy. So this is the baby's birth weight divided by the mother's height squared. And if we create this feature, Squared. And if we create this feature, it immediately becomes the single most important feature for predicting maternal risk during delivery. It becomes a very, very strong feature, stronger than all the other features by far. Let me talk about some cancer treatment, go through one other example, and then I'll wrap up. I'll be done in about six, seven minutes. This is risk as a function of age for a cancer treatment. And notice that risk goes up as you get older and then suddenly drops at 75. That didn't make any sense. We asked the doctor. Make any sense. We asked the doctor who collected this data, and the doctor said, Oh, that's because you weren't allowed to be in this trial unless you were under 75 years old. Or if you were over 75, you had to be extraordinarily healthy, and this had to be the only treatment appropriate for your cancer. So the doctor said, you know, the patients over 75 are actually lower risk, they're healthier than the patients who are under 75. So there's a selection bias here. And if we wanted to use this model more broadly, To use this model more broadly, we would either want to flatten this risk or cause the risk to go up monotonically. This is some more work with Ben. This is COVID-19 mortality. We've been working on that since the beginning of the pandemic. This is risk as a function of age, very similar to the pneumonia graph, another respiratory illness. Notice there's low risk flat in the 30s, 40s, 50s. There's step function rises in similar places as on the other graph, like a step function rise in 85. What's difference is we don't see a drop in risk at 100. See a drop in risk at 100. Instead, we see a huge increase in risk at around 95. And we see this for two reasons: one, we don't have details about patient age above 95 in some of our data, and that's because they're protecting privacy of these patients, and there are a few patients out there. The other reason is many patients over 95 do not intubate orders, and not being able to get intubation saves you from like the last chance of recovery. And they do that because intubation is an incredibly aggressive procedure. Intubation is an incredibly aggressive procedure, and very elderly patients sometimes succumb to intubation itself. But this graph is clearly saying that by withholding intubation, we are putting these patients at high risk. This is risk for men and women. This is well known. You saw this in the press. Men are significantly higher risk than women. And this is common for many conditions where the people who are dying are elderly. Older men are typically considered more frail than older women. So once you get to about 75. So once you get to about 75, 80, 85, men are typically considered to be more frail than women. And you'll find that by the time you get to 90 or 100, maybe 80, 90% of the people who are still alive are women as opposed to men. This was our first discovery. We had this the first night we trained a model. This is risk as a function of your lymphocyte count. As expected, risk goes up if your lymphocyte count is extraordinarily high. That means you have a really bad infection. This is the surprise that risk was very high if you have a very low lymphocyte. Is very high if you have a very low lymphocyte count, which some would interpret as being that you don't have an infection. It turns out the risk with COVID is high here because these are patients who are either immune compromised or their immune system is somehow just not recognizing that they need to launch an attack to COVID-19. This is really fascinating. This is meds that you've taken before you come to the hospital. And to the right is higher chance of dying from COVID-19. To the left is lower chance. We've turned the graph on the side. Left is lower chance. We've turned the graph on the side so that you can read these things. And I just want to show you at the top: this is peptic ulcer disease and platelet aggregation inhibitors. These are NSAIDs like aspirin and ibuprofen, and they seem to lower risk. Peptic ulcer disease, you would often be taking an acids that also contain aspirin. So we think both of these are aspirin effects. So it's interesting that aspirin taken before you came to the hospital seems to lower risk. These are meds that patients have taken in the hospital. Again, lower risk is on the top left. Is on the top left and higher risk at the bottom right. And notice that the first four things: XA inhibitors, anacids, catarloc, ibuprofen, again, these are NSATs. These are aspirin-like things, and they all seem to lower risk. What's interesting is we do have anticoagulants. This is things like heparin and warfarin, which are actually the methods of choice for anticoagulation in the hospital. And if anything, these do not seem to lower risk. So there's a chance these two medications. Medications, anti-inflammatory medications, have very different mechanisms. And there's a good chance that it's the clotting benefit of using aspirin is better than the clotting benefit of, say, warfarin or heparin. But this is something we need to study before we would suddenly change the way we treat patients. This is really fascinating. This is your neutrophil lymphocyte ratio. The pink is patients who did receive glucocorticoid steroids, and the blue are patients who did not. Notice that risk of. Who did not. Notice that risk goes down if you receive glucocorticoid steroids, but there's a very narrow range of effectiveness here for this medication. You have to have had a pretty high NLR for this to be effective. Most patients do not have this high NLR, and the medication is less effective for them, suggesting that they have a different mechanism and need different treatment in order to survive their COVID-19. Again, if you're interested in this, please ask Ben about it or get in touch with me and I'm happy to talk with you. Or get in touch with me, and I'm happy to talk with you. Okay, we've been doing some work in differential privacy with these EBMs. We have a paper on that. We have using these EBMs, what is now the world's most accurate differential privacy algorithm for tabular data. So we are the red lines. Notice that as we increase privacy by going from the nine-private setting to increasing this epsilon, which means we're increasing privacy as this epsilon gets smaller. Epsilon zero would be infinite privacy. zero would be infinite privacy. As we do that, our accuracy drops hardly at all, but the accuracy of the best competing methods drops a lot. And in fact, AUC, they drop below 0.5 sometimes. They're actually worse than random prediction. So that's amazing. But we have another capability, which is differential privacy works by injecting noise into learning to protect groups. And it ends up injecting more noise for groups that are smaller. So if you've got a race like Blacks or Hispanics who are Or Hispanics who are only 10% of your data, the model will have to inject more noise into their data in order to protect their privacy. And this can cause the model to learn very jumpy graphs. For example, your risk is low if you're 80, 79, but your risk is much higher if you're 78 or if you're 81. We don't want a model to predict that kind of risk. This is an editable model. We can do these edits without changing the differential privacy, which is amazing. Even more importantly, we can do Even more importantly, we can do it automatically. We can use things like the pool-adjacent violators algorithm to just say: we want risk as a function of age to be monotone, and it will find the optimal transformation of the graph from the original noisy graph to a smoother monotone graph and doing these edits, and it'll do it optimally. So it's really amazing that we can do that. And we've built now a tool, which is kind of like a Photoshop for these GAM models, which makes it easy for end users to do these. Easy for end users to do these edits on their models. And doctors really love this tool. And they've basically decided they're not going to use any other model that doesn't afford this ability to both understand the model and then to edit and correct the model when they have to. We've been very careful. We don't want doctors overruling the model too much. So we've made sure that there are safeguards built into this method. We've also, as I said before, we've figured out how to do the same kind of modeling, including editing of the model using Including editing of the model using neural nets. This is a collaboration with Jeff Hinton and some people at Microsoft Research and at Google. And we've also done some work now in differential privacy and causality, which I won't get to talk about. I just want to wrap up. Every data set has flaws. High accuracy is often telling you just that the model is exploiting those flaws, not that the model is doing the right thing. These glass box methods like EBMs and their neural net equivalent are really And their neural net equivalent are really tools that you need to understand the model, to vet the model, and correct the model before, say, using it clinically or in any critical domain. You know, finance, hiring, things like that, places where we're worried about bias. And one of the things we're able to do, by the way, is we're able to detect race bias using this kind of model or gender bias. And we're able to not only detect the bias, but again, we can edit and correct the bias. I didn't have time to talk about that. And we have an open source. That. And we have an open source package for this, which makes it as easy to train and edit these models. It's as easy to train these models as anything else, like random forests, boosted trees, or anything that you can do with scikit-learn. And if you're into neural nets, we have another tool for you to do that. Okay, so I just want to say thank you. Happy to take any questions. One of the things I haven't done is gone through the algorithm, but it would take a while to do that. If any of you are interested in how we train this model and then edit it, please get back in touch with. And edit it, please get back in touch with us. Talk to me, talk to Ben. We're happy to tell you the details of how we do this. But I thought it would be more fun at the beginning of this workshop to just see a bunch of examples, you know, kind of preaching to the choir as opposed to jumping into algorithmic details. Okay, hey, thank you very much. And what I'm going to do is stop sharing so that maybe we can see each other's faces. Hey, Ben, if you want to raise your hand just so everybody knows. You want to raise your hand just so everybody knows who you are. Sorry, sorry to embarrass you again. Hey, Ben is no longer a grad student. Ben has graduated from CMU. He's a postdoc now at MIT. Ben does a lot of work in machine learning for healthcare and a lot of work in interpretable machine learning. And he's got a really fun talk he's going to tell you about on Thursday. Thank you very much, Rich. We have already localized friends, so sorry to kind of go in COVID or anymore. So thanks for investigating all. So, thanks for a fascinating talk. So, do we have any questions? Either here in the audience or if you are online, you can also send a question by your chat. Yeah, so thanks, it was an amazing talk. Do me a favor, could you talk a little louder or go near a microphone? Can you hear me if I can? I can now, yes. Okay, sorry to make you scream. I apologize. But you were mentioning that sometimes you found something your mother had learned which. Something your mother had learned, which were not expected, right? And you had this test of ground numbers. So, my understanding was like, don't trust ground numbers and go ask the doctors and the physicians and really try to understand what's happening. But I was wondering if you think there might be a measure or another test that would tell you when you can actually trust what the model is learning as new knowledge. Like, how much can we trust what we are seeing and how much. That we are seeing, and how much do we know, like if it's wrong or not? This is a great question. By the way, Ben is an expert on these round number effects. So if you're specifically interested in round numbers, feel free to talk to him as well. It turns out the model has learned statistically correct things. So they're actually true, but they're true because of the treatment the patients are being given. Are being given. So, like when the model learned that property in 1989 was worth $600,000 more than property in 1988 or 1990, it turned out that was statistically true in the data. So the model had learned the right thing, even though we all know it can't be true, you know, causally. So the model had learned the right thing. And then what it did was it pointed us to a problem in the data that we then wanted to correct. It's the same thing with these round numbers. It's pointing us. Numbers, it's pointing us to the way patients are treated by humans, by doctors using these round numbers to make treatment decisions. And fortunately, the treatment decisions are effective, and that's why risk actually responds to these round numbers and goes down in some cases. So the models have learned things that are true. They're even causally true. Like it turns out asthma does causally reduce your risk, but not because it increases your health, just because asthma causes you. Health, just because asthma causes you to get better care. And that's a causally true statement. But if you were to remove patients' asthma, like if you could cure them of asthma, it would no longer be true that asthma was good for you. So it's kind of causally true, but it's kind of an inadequate explanation because we think the real causal diagram is asthma causes you to get to care faster, asthma causes you to get to higher quality care. And it's those things which make you lower risk, even though asthma all by itself would have made you higher risk. So what's interesting. So, what's interesting is that you really need causality. So, notice all those interpretations of the models were interpretations I was putting on the model done in concert with experts in the domain. So, you really have to get experts in the loop to figure these things out. And they're all like real statistics in the data. And the goal is to do the detective work to figure out why that statistic is in the data, why the model has learned these surprising things, and then to figure out which. Things. And then to figure out which of these should be: the data needs to be corrected if possible, or the model needs to be edited. Or in some cases, you just leave the model the way it is. And you just want to understand why is it doing that. For example, the blood urea nitrogen, we don't really want to edit that graph. We want to change healthcare. We want to lower dialysis treatment decisions to 85. So it's really a detective problem at that point, best done in combination by data scientists, statisticians, and Scientists, statisticians, and experts working together to understand the model and understand why it learned these things. Because the model doesn't know why it learned it, it just knows you gave it a data set and it's telling you what it can learn from the statistics. The beauty is that the model's really high fidelity to that data set. It really shows you fine detail, and then you can try to figure out what's going on. And we've learned to basically trust the model. We've got to be careful. If the error bars are large, don't trust. Error bars are large, don't trust the model. We have error bars on those graphs from a bagging procedure. So, if the error bars are large, don't bother understanding why there's a jump or a drop in the graph. But if the error bars are small, you should definitely trust the model and try to figure out what's going on and what you should do about it. And different things should be done about different, even in the same data set. In one case, we fix the data, another graph, we fix the model, and in another graph, we try to change healthcare. And in another graph, it's like. Change healthcare, and in another graph, it's like, oh no, that's a surprise. We'll just leave that there. Okay, thank you very much. Do we have a couple of minutes for another question? Yeah, I have a question. Yeah. Can you hear me, right? Yes, speak up a little bit more if you can. Okay. Oh, that's great. That's great. Okay, so once you let a physician edit the model, for instance. I edit the model, for instance. How can you be sure that it's not introducing another bias? So how can you be sure that maybe it's like, you know, creating a problem elsewhere and you are not aware? Do you like comparing before and after? This is a great, great question. And we worry a lot about this. When I give this kind of presentation to a computer science audience, a third of the audience wants to kill me because To kill me because you know, they're like, How dare you overrule the data? Like, how dare you allow a human to impose their bias on what the model has learned? And then I say, do you really want the model to predict that your 105-year-old grandma, who's had asthma for 30 years, who had a heart attack two years ago, and who now has advanced pneumonia and strider, is low risk. Like, is that what you want me to do? Is that what you want me to do? And they're like, no, no, I want you to get better data. You can't get better data. We are not allowed to withhold treatment from asthmatics or heart disease patients who have pneumonia to train a more accurate model. That's not ethical. We'll never have a data. Turns out every data set will have these problems in it. We will never, ever be allowed to collect the data that we would want. Just can't get it. So you have to learn to live with the data that you have access to. Have access to all of the data is going to have. We know every data set is biased in different ways. You can't just say, oh, go collect unbiased data, right? That's just not possible usually. So you got to live with the data you've got. And now you have no choice. And we do the kind of things you suggest. The tool is really designed to force the person who's doing the edits to consider the before model and the after model and the impact of these edits. But the truth is: here's the sad truth: if we really had enough evidence to know for sure. To know for sure that the edits were correct, the odds are we would have had enough evidence in data that the model could have learned the right thing in the first place. So this is one of those cases where we're putting the human expert back in the driver's seat and we're creating a sort of computer-human interaction here. And that's what intelligibility is all about, right? I mean, when you talk about an interpretable model, you know, it's always interpretability with respect to a human typically. With respect to a human, typically. So, what we're doing is putting the human back in some sort of control and we're allowing them to overrule the model, not because the model has made a mistake. The models are learning correct statistics here. Everything I showed you was statistically correct in the data. It's just that the data has all sorts of effects in it that you may not want to be represented in your model, and yet you may have no way of collecting data that doesn't have those. Way of collecting data that doesn't have those effects in it. So, what are you going to do? And here's the sad thing: this has been happening for decades. We've all, myself included, have been training black box models, neural nets, boosted trees, random forests. We have all, I have never even once seen a data set that did not have these problems in it. So, we have all been doing this all along and we just didn't know it. And now finally, we have a pair of glasses where we can understand a high accuracy model well enough. A high-accuracy model well enough to see the kind of things that they learn. These boost-a-tree random forest neural net models learning exactly the same sorts of things inside of them. It's just that you don't know what's happening. And even if you did know, you wouldn't really be able to correct it so easily. So that's the value of these high accuracy glass box methods. But you're right. We are absolutely concerned about people using editing power to put biases back in the model. Biases back in the model that we wouldn't want to be there, and that's why we would like to have at least several people editing the model in a collaborative way, as opposed to listen too much to any one expert. We need to do a lot of research in this area. Yeah. I wish I had a better answer for you. So just a quick question. Yeah, maybe a follow-up of the previous discussion. What would you say essentially or recognizable? Essentially, or uh, could you say that again and a little louder? Microsoft here is a way to come here and ask a question here because they are here. So, sorry. Yeah, so I just wanted to ask if, in fact, what you're doing is basically causal modeling, implicitly. Say it one more time, I'm sorry. It one more time. I'm sorry. Whether you are what you are doing is causal modeling implicitly. Is what modeling? Causal modeling. Causal. Oh, that's a really good question. We believe, because the model is restricted in its complexity to something that is interpretable, that it is more likely to find hints of real causal effects. Causal effects, but it is not a causal model. It is still standard supervised learning. So we often find when we look at like jumps in the model that we can then detect what causally creates those jumps in the data in the real world phenomena. But it is still just a standard supervised learning correlational model. It is not causal. Now, we have started doing some other work on trying to exploit the intelligibility for causal benefit. Causal benefit, and we're doing that. But sadly, you know, I wouldn't want to jump and believe.