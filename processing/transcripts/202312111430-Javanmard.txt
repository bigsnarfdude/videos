And today I'm going to talk about some of the works that I've been doing in the past couple of years on adversarial training and the trade-offs that exist in this area. And I want to also talk about how the low-dimensional latent structure of the data can help us to relax some of these trade-offs. And this is joint work with my former student, Mohammad Mehrabi, who is now posting. Mohammad Mehrabi, who is now a postdoc at Stanford University. Um, okay, so um maybe I should go to full screen. Okay, uh so let me start by giving a little bit background on adversarial training and then um the result that we have. So, we all know about the remarkable success of deep neural network in a myriad of applications domains, and these machine learning systems often have complex architecture. Have complex architecture consisting of multiple layers, and each layer has a large number of parameters that you need to train. And indeed, most of the state-of-the-art deep neural network operate in a so-called over-parameterized regime, which means that the number of parameters that you need to train is significantly larger than the size of the training data. And there has been quite significant amount of work and results on the virtues of overparamental. On the virtues of over-parametration for deep neural networks, both in terms of statistical point of view, like the double-distance phenomena, and also in terms of optimization and tractability, and how over-primization is going to make the learning process more tractable. So, these are about the good things about the system, but there are also some negative properties of machine learning systems in practice in particular deep neural network. Particular deep neural network. And that's about the robustness. So suppose that you have some classifier that you are using on image data. And the goal is that once you give the image to this classifier, it tells you what type of animal is in the image. And it's working with high accuracy. You give the images, classifies correctly. And then you probably have seen this example that you can take the image and add a very small noise to the image so that the result. So, that the resulting picture looks very similar to the original one. So, the noise that you add is completely imperceptible to the human eye. But if you fit this noisy image to your system, as of sudden, it's not going to work well. It's going to tell it's, let's say, given with high confidence. So, it completely degrades the performance of your classifier. And again, this is some. Fire. And again, this is some very, very small noise so that you cannot distinguish between the two images. And of course, this is very alarming. And there are many well-documented examples, not just in computer vision and image processing. In speech recognition, for instance, people can come up with commands that are incomprehensible to human or even inaudible to human, but then degrade the performance of voice system like Siri or Google now. System like Siri or Google. And this has motivated people to start working on this area called adversarial training and adversarial robustness. So if I want to give like what's happening pictorially, so let's say your image is a vector X in a high-dimensional space. There are different models for adversarial perturbation. In this talk, I'm going to focus on norm-bounded additive perturbation. So you take your image. Perturbation. So you take your image, you add the noise, let's call it delta, and the constraint that you put on delta is that the norm of delta is bounded. So I'm going to leave what type of norm you want to impose to be flexible. So that would be, for instance, if this is your image, you're going to move it to here. And the radius of this ball would be basically the power of the adversary that you have. That you have. And what happens in practice, as I was saying, you might have classifiers that have high standard accuracy, but then once you want to test them on perturbed input data, the robust accuracy would be very low. And I want to highlight that this model of perturbation is different from what we usually have in statistics. So, some of the important differences is that in classical robust statistics, Uh, robust statistics like contamination model, uh, you have a few few outliers. So, these are a few number of samples who have large gross error. And this outlier often happens in the response and also during the training. So, your training data contains some outlier. Now, what I'm explaining here is quite different because you could have many subtle errors. Have many subtle errors in the image, so you could add like all of these noises to all of these pixels, but you want it to be small, so you could have many subtle error. The perturbation is happening during the test time, so your training data is unperturbed or clean, and you train a model. But once you get to the test time, you're gonna test it on perturbed data. And the other one is that, at least for this model, I explained here, the perturbed. This model I explain here, the perturbations are happening in X, not Y. There are again models that consider perturbation to both X and Y, but let's say for now we are considering perturbation in the input feature X. So these are the main differences. And if I want to come up with like a formalized version of the problem statement, let's say you have data coming from a distribution D, and the goal is to learn the population. And the goal is to learn the population estimator. So that would be like the minimizer of some population loss that you have on this data. So I want to train data star. And the usual thing in supervised learning is that you, instead of working with the population expectation here, you work with the empirical risk minimization. So that would be the usual estimator that we have. Be the usual estimator that you have in supervised learning. Okay, now if you do this one, okay, I'm having some problem with the screen, but okay. If you do this one, you get an estimator that works well if the test data is also coming from the same distribution as the training data. But what we are saying is that this model is going to work poorly on adversarial exams. On adversarial exams. So, what would be the natural approach if you want to mitigate this behavior is think about what's important in terms of robustness of the system. I want to test the model on pretty data. So during the test time, what's really important for me is to minimize the worst case loss. So instead of just having the loss over x and y, I want to consider x plus delta. X plus delta, and I'm free and choosing delta as far as it's normally bounded by epsilon. And this is a strong notion of adversary because delta could depend on x, and the adversary can choose delta after observing the test data x and y. And then I want to have a model that works well with this with respect to this worst case loss function that I have here. Okay, now if this is the goal that you have, then The goal that you have, then it also makes sense to do the same trick. And instead of working with the population version, you go with the robust ER. So that would be like the robust version of empirical risk minimization. And this method has been proposed a few years ago, and it has been very well received and used a lot in different adversarial training systems. Systems. And what happens if you go with this model is that now the robust ERM is going to work better on adversarial examples. So as you expect, you get some robustness. But the interesting thing is that now, if I go with this model that is trained in this robust way, but tested on the usual test data, so there is no perturbation on the test input, it's not going to work as well as. It's not going to work as well as what you had on the usual ERM. Okay, so in other words, what's happening is that you want to hedge against the possible perturbation during the test time. So you go with this model theta at epsilon. But now if there's no adversary during the test time, you lose your performance compared to like the nominal, the classical one that you have. So when you see this observation, you might say, okay, maybe there is a trade-off between Between standard accuracy or what people call model generalization and robustness. And the question is whether these two measures are inherently at odds with each other, or is it because your algorithm was not sufficiently strong? So these are the key questions: whether there exists a fundamental trade-off, no matter how much data I give you, no matter how No matter how much data I give you, no matter how much computational power you have, do you still see a trade-off between these two measures of performance? Or is it something that relates to the algorithm that you are using? So the questions are basically on the fundamental trade-off and algorithmic trade-off that we have. In particular, is it possible to come up with an estimator that's optimal with respect to both the standard accuracy and the robust accuracy? So, a couple results that we got in previous work. We focused on linear regression and binary classification. For linear regression, when the features have Gaussian features, we show that there exists a fundamental trade-off. And fundamental means, again, no matter how much data or computational power that you have. And this trade-off is formalized in terms of paradox optimal front. So it So it tells you that there is a non-trivial boundary for this region of adversarial risk versus standard risk. And because of that, it's not possible to find estimators that are robust, sorry, that are optimal with respect to both. So there exists a fundamental trade-off. And we also characterize the algorithmic trade-off that you get for using robust ERM in different regimes, like when the sample size and the dimension growth. When the sample size and the dimension grow together, for instance. For Gaussian mixture model, again, it has been shown by other works that there exists a fundamental trade-off. And we characterize the algorithmic trade-off that you can achieve by using the robust ERM under general norm perturbation and general covariance that you have. So for these two contexts, we show that there exists a fundamental trade-off. Fundamental trade-off. But there are two natural questions that arise after this work. And the question is: can you break this fundamental trade-off by first considering more complex class of estimators? So instead of working with linear classifier or linear model, you go with like multi-layer neural network. And can you break this trade-off? And the second question is that how about data distribution? If it's not chaos. The distribution. If it's not Gaussian distribution, it's coming from some other distribution. Could it be possible that there's no trade-off, there's no tension between these two measures? Okay. So the focus of this talk is more on the second question. So we want to say that when there is this low-dimensional structure in the data, you can mitigate this trade-off between accuracy and robustness. Okay, so that's the high-level or takeaway message from the top. The talk, if you're bored listening to the rest of the talk, so that was a good summary. So, to dive into the results and give you more details, I'm going to introduce two data models. So, these are the data generated models. The first one is a Gaussian mixture model, but with a latent low-dimensional structure. So, how the data is generated in this model? Your labels are coming from Labels are coming from minus one and one with some probability pi. And then you have a multivariate vector z. This is a latent vector, so the learner doesn't observe z. And the mean of that is plus or minus mu, depending on which class you have. And you have some Gaussian like cloud of points around that center. So that would be like an example. So that would be like an example. You have two classes. Mu is one and zero, and you have like two Gaussian with these two central. So let's say Z is a latent structure of dimension D, of dimension K that's small. And then what happens is that this latent structure is lifted to a high-dimensional space that I would call it X. I would call it x. So x would be phi of wz. W is a tall matrix of dimension d times k, and d is much bigger than k. If I'm taking a low dimensional space of dimension k, lifting it to like a higher dimensional space d, and the mapping that I'm using is of this form. So it's a w times z, it's a linear one, and then you apply a possibly non-linear mapping phi coordinate wise on x. An x. So if, for instance, I take phi to be like tangent hyperbolic, you get this picture. And if I take it to be like sine of t minus half of t, you get another picture. And then the learner is given this x and y. So it never observes z, but the data is generated from this latent structure. Learner only see x and y. Okay, and learner only see X and Y and wants to fit some class. Okay, so before I tell you what's the result, I need to define some measures of performance. So the standard risk would be like, let's say your classifier is H, the probability that you make an error. This is like the usual standard risk. Adversarial risk is The worst case error that you could have. So you can change x, you can perturb it to x prime, as far as their distance in some p-norm is less than epsilon sub p. And if you make an error, that would be called adversarial risk. So adversarial risk is obviously bigger than standard risk. And then the difference between these two is something that it's called boundary risk. And you can characterize it. And you can characterize it by saying that boundary risk means I am classifying a data point correctly, but I can push it a little bit so it goes to the other part of the boundary and I misclassify it. So it's a probability that you have some correct classification, but then there is some perturbation of that. So the label is different. So the boundary risk somehow tells you how much volume. Tells you how much volume you have around the boundary of your classifier, and what's the probability that, of course, data is coming from that volume. Okay, so this is the boundary risk. If I tell you that the boundary risk is going to zero, then it means that there is no trade-off, right? So if I come up with some distribution, some classifier for which the boundary risk is zero, then there is no trade-off between these two. Okay, so the first DRM. So the first DRM informal statement is that the trade-off vanishes if the data has some low-dimensional structure. And if I want to give the formalized version is that under the Gaussian mixture model, if you let D go to infinity and D was the ambient dimension, under this condition, the boundary risk of the Bayes optimal classifier converges to zero. So the Bayes optimal classifier has the minimum The base optimal classifier has the minimum standard risk, but also the boundary risk for that is zero. So the adversarial risk is also optimal. Now, if you think about this condition, some comment on that. So obviously, when epsilon sub p grows, this condition becomes harder to satisfy because you have a stronger adversary. So this one makes sense. The other one is that sigma mina. The other one is that sigma mean of w somewhat measures the extent of low-dimensional structure that you have in data. In particular, if sigma mean of w is small, it means that there are some direction of the low-dimensional latent space along which the energy of the signal is not well transformed to the ambient dimension. So, what adversary can do is that she or he can choose those dimensions and Can choose those dimensions and add the perturbation along those directions. The signal is not going to be well transformed, but the adversary is adding the perturbation in the ambient space. So it can really sabotage the classifier that you have. Okay, so this one is also makes sense. Sigma mean w smaller, then this condition becomes hard. And some experiments. So let's say phi of t, I take it something simple, just the identity. Something simple, just the identity map, and W is a Gaussian IID. And I take the perturbation to be 2, so it's 1 over k so that the norm of each row of w is 1. And in this case, you can see the condition we have in this theorem would be simplified to this one. So as far as epsilon 2, because p was 2, so this is just the epsilon, the adversary's power. If it's a little over. If it's a little O of square root of D over K, then the boundary risk should go to zero. So the other way I can interpret that, if I take epsilon to be constant, but D over K goes to infinity, so the ratio of the ambient dimension over the latent dimension grows to infinity, then the theorem is saying that the trade-off should vanish. And this is what we get. So here we have the standard risk that we're. Here we have the standard risk, the adversarial risk, and the boundary risk is going to zero as the overkick grows. And then this is the behavior if I go with different choices of epsilon. Of course, epsilon larger, you would have larger boundary risk. But what you see for all of these curves is that as d over k is growing, the boundary risk is going to zero. Okay. Okay, so that was for Gaussian mixture model. The second data model that I want to present is through the GLM, the generalized linear model. But you have again some latent low-dimensional structure. So there is some Z that's going to impact both your high-dimensional feature vector X and also the labels that you have. So the way that we define it is Y is plus minus one with some function of Z transfer. With some function of Z transpose beta, but the learner never sees this quantity. And then you have some high-dimensional feature vector x that's generated from this low-dimensional. Again, you lift by a matrix W and then you apply a function phi component-wise. And the popular choices are like the logit model or probit model. So sorry, there's a typo here. So that would be another model. So, that would be another model for latent low-dimensional structure. And again, we get a similar theorem. And interestingly enough, the condition is exactly the same. So if this condition holds under this model, again, the boundary risk of the Bayes optimal classifier converges to zero. Okay. And here are some other experiments. It's repeating the same thing, but now we are working with a different. Are you working with a different data generative model? Okay. Now, the second thing that I want to talk about is: okay, we said that the boundary risk of the Bayes optimal classifier is going to zero, but of course, the Bayes optimal classifier depends on this lifting metrics W and also the mapping phi. So implicitly, I'm assuming that there is some classifier that knows W and phi. WN5 and it has like the best standard accuracy and the best robust accuracy. Now, I want to relax this assumption. So there are two ways. One is to try to learn W and Phi using the data itself. So this is an ongoing project. And the second one, the second approach is that how about models that are agnostic to this low-dimensional structure? So I know that this. Dimensional structure. So I know that the data is coming from some low-dimensional structure, but I want to use some model, some estimator that doesn't actively try to learn W and phi. Maybe there is some benefit just because data is low-dimensional structure. I still get the boundary risk going to zero, even if the estimator is not trying to learn these two, these low-dimensional representations of the data. Okay, so to test this idea, we consider linear classification. Uh, we consider linear classifiers that this is just sine of X transpose theta, and then this model theta is learned through some robust ERM. So that would be like a minimax optimization that we have here. And then the loss that I work with is the logistic loss. Okay, so again, the data that I have is coming from a low-dimensional structure, but the estimator doesn't really try to learn this representation. This representation. Okay, and what happens is quite interesting because now you see that even theta at epsilon that's completely oblivious to this low-dimensional structure, the boundary risk of this estimator is going to zero. So these simulations are for Gaussian mixture model. And different figures correspond to different choice of this function phi. So these are some random complete. Random complicated functions to test this speculation that we have. And you see that all of these boundary risks are going to zero as d over k is growing. And we have some rigorous theorem on this one for theta epsilon, but I'm not presenting in the interest of time. In the interest of time. So, the last part of the talk, I want to do some experiment on like image data set on MNIS. And we wanted to see if it really, if the theory that we have is working on like some real data, if having some low-dimensional structure is going to help you to come up with classifiers that are both accurate and also robust. So, what we do is that What we do is that we are using something called a mixture of factor analyzer, MFA. And this is a model that was proposed by Richardson and Wace a couple years ago. And what it does is basically like a mixture of Gaussian distribution that you have, but you have low-dimensional covariances here. So the number of components is capital M, but you also have low-dimensional. But you also have low-dimensional covariances AA transpose, where A is a tall matrix, and you could have some additive diagonal terms also here. So D here is diagonal to allow to have some heterogeneity in the variance of each entry of X. Okay. Now, why do we want to work with this model? Because it's a nice model. It gives you a low-dimensional structure, and you can play with this. Structure and you can play with this structure. You can choose the number of components that you have, you can choose the dimensions of A. So, this is something that you can try to play with and control the amounts or the extent of low-dimensional structure that you have. The second reason is that you can compute the likelihood of this model easily. So, you can try to fit this model to a real data set like MNIST, you get some estimation. Get some estimation for mu and a and d. And once you have them, you can try to actually generate images from this likelihood that you have. Okay. Now, these are some of the images that we get from this model. So we fit it to the MNIST, we estimate those parameters, and then we generate new images. So this is if you choose k equals 1, k equals 10, k equals 100. Now there is Now, the resolution of these images is not as good as if you do something like YAN. Of course, amnesty is something very, very simple, the images compared to like more complex images that you could have. So the model is not giving you high resolution like GAN or diffusion processes, but something that we can work with and control the amount of low-dimensional structure that we have. And also we can drive the Bayes optimal classifier. Can drive the Bayes optimal classifier based on this model. And once we have the Bayes optimal classifier, we can try to see what is the boundary risk of that and whether the boundary risk is going to zero as I make the data more structured in the low-dimensional space. Okay, so to test this one, we consider two adversarial attacks. One of them is based on projected gradient descent, and the other Gradient descent, and the other one is like the fast gradient method. So, these are two popular adversarial attacks exist on the literature. So, we use them to test the robustness of the Bayes optimal classifier. And we see that as D over K grows, again, the boundary risk is going to zero. And this is if you go with one component, so M is equal to one, or if you go with 10 components. So, for Components. So, for both of them, this is consistent with the theory that we have. Again, if you have low-dimensional structure, then you can come up with estimators that are optimal with respect to both the standard accuracy and also the robust accuracy. Okay, so with this one, I'm gonna finish.