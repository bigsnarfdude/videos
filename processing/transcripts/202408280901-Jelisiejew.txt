But the schedule has five talks today and June's within, I guess, three tomorrow. The people agreed to switch, thank you. And let's see. Oh, sure. Sorry, for the morning. Well, why don't I? Well, I think the chair. Yes, I'm direct and multiple. I'm going with the notes. Okay, and do I have to press on the situation? No, I think they're doing that for the ones. Recording is in progress, yeah. It's already been. Yeah, it's already been done. Okay, so good morning, everyone. And today, the first talk is by my wonderful author, Yochi Gelise, who will tell us about an analog of Rupert's schemes of points for Gorishine Market. Okay, yeah. Thanks a lot. Thanks for the invitation. I should say that the previous talks were really wonderful and they inspired me a lot. They also inspired me to actually make handwritten notes. Make handwritten notes, which may have been actually a terrible decision. So, if you have problems reading anything, feel free to interrupt. And in general, feel free to interrupt. I think that the best part is maybe some things that will be mentioned only in the background of this talk. So, you are free to ask questions about that. Great. So, this will be about an analog of Hilbert scheme of points for Gaurus and algebras, and there will be some small and There will be some small enumerative parts joined with Andrea Rikoffi and Rainir Schmirban, and maybe joined with Martin Kuhl as well. But most of it will be about some kind of construction of a space. So if this conference is about going beyond spaces, then I have to say that this talk will be mostly going beyond known spaces. Good enough. Okay, great. So let me recall first the Let me recall first the the setup. So the setup will be purely embedded and I will be interested in the Hilbert scheme of points, mostly because it's somehow the most down to earth part. If you care only about curves, but embedded curves, then you should be still good. Okay, so we know that there is the Hilbert scheme of points on a small projective variety X, or and we know that in general, there is hardly anything that can be said about its geometry, but Is geometry, but if we start thinking about this, then the geometry maybe is inherited from this stack of coherent shifts, and then the geometry is maybe not so bad because there is this hidden smoothness assumption. So, we have all the steps, and we can essentially build this more and more advanced machinery. And in the talk, I will try to say something about this bottom part so. This bottom part, so you should expect that there will be some analog of the Hilbert scheme that will be called IRNX, and there will be some analog of the stack of coherent shifts, which doesn't have a name yet, so you can see that it's clumsy. And I would be really happy to learn that there is some derived structure on it that makes all these things work. And in particular, I would be very happy to learn something more about the cotangent complex, but this is all. And complex, but this is all work in progress so far. And I have to say that I will be mostly considering x equal to the affine space because then there are some nice concrete connections to commuting matrices. This is definitely not a requirement. All this will work for any reasonable X for which the Hilbert scheme exists, actually. But we will stick to the affine space. So if you have some specific questions about Have some specific questions about other spaces, feel free to ask. And of course, partially the motivation is to just have this constructed, but also to have some enumeration on it and maybe if things go really well to understand some geometry for three folds, X. Okay, so I will try to now start with this upper line and then motivate why the bottom line. And motivate why the bottom line may make sense. Great, so we have the Hilbert scheme of points. As I said, this is just a set of closed sub-schemes in A to the D, which are zero-dimensional and which have degree n. So this only means that if we consider the structure shift, then it has length n. So it's an n-dimensional space over the field. And well, for d equal one, the Hilbert scheme of A1. The Hilbert scheme of A1 is just an. For D equal to it's for Garti's result that this is smooth. While for fourfolds and above, the Hilbert scheme of points has generically non-reduced components, and it seems hopeless to actually get something interesting, like some global result on it. It's connected, it's quasi-projective, but that's somehow the best thing you can write in a recommendation letter. In a recommendation letter for this mathematical object. And maybe if you do enumerative geometry, then you can enumerate on this using some virtual structure, but this is only available for d equal to 4, and this is already rather tricky. So if you are interested in this part, I think you should ask Martin. But the enumerative geometry part is somehow fueled by the observation that we have the full torus acting. We have the full torus acting on AD. So, also, we have the full torus acting on the Hilbert scheme. And fortunately, it acts with reduced finitely many fixed points. So, the fixed points are good. The space itself, well, not so great. If you would like to know something more about the singularities, then I'm very happy to talk about this because this is something I find fascinating, but I think Find fascinating, but I think I'm in a minority here. So, if you don't care about the singularities, don't worry, there will be no singularities during this talk. What I cannot help but to mention is that there is a small gap, namely there is d equal to one or two, and there is d at least four. And this is somehow significant. We don't really know what happens for d equal to three. We know that there is the superpot. That there is the superpotential description, so the singularities are somehow restricted, but we don't really know whether they are better than the superpotential or they are as bad as the superpotential allows. For example, we don't know any non-reduced structure, we don't know any components except for the usual one. Well, we don't know basically anything for the equal three, which makes d equal three very interesting. Go ahead. So for d equals four, it's already So for d equals four it's already a bucket logo kind of space or uh no this is only the well for d equals four we only know this essentially. So okay so yeah I should I should say that if we consider A16 then this is indeed Marfiz law. So every possible singularities non-reduced components non-liftable components Components, non-liftable components, all sorts of crappy things can happen. And then there is the expectation that 16 is not so magical number, 16 is just made so that some argument works reasonably cleanly. So I cannot say whether 16 should be reduced to 4, but for 4 it's becoming more tricky to just embed things into an A4. To just embed things into an A4 or into a smooth fourfold. So the only result I am aware about for is the generic non-reduced components. This is, by the way, for 21 points. So it's not that large number of points. So yeah, you could ask about embedded components for fourfolds. I would expect that, yeah. If if you really force me to guess, I would expect that d equal force has Morphe's law, but I definitely don't have any evidence for this. Okay, great. By the way, thanks for the question. You are more than welcome to ask tons of questions. Okay, so if we don't have any idea about the geometry, maybe we want to do viral. Maybe we want to do virtual stuff. And this is one of the situations where the virtual stuff is really, really explicit. So there is something called the global coronation chart, if you want to be fancy, which boils down to saying that up to some introducing auxiliary objects, everything can be made very explicit here. So the Hilbert scheme is here, and the stack of coherent shifts is here. So there is a forgetful map, but there are two more players. But there are two more players. Namely, if we want a coherent shift, then we can start with some vector space and some operators acting. Remember that we have coherent shifts over AD. So we basically have decommuting operators. So we can consider the space of commuting matrices here. And this has also a map to the coherent shift. And then we can consider the pullback when the pullback is not so great. The pullback is not so great, but not so bad either. It's the case of so-called stable tuples. So, let me go through this because this will be important in the following. So, for commuting matrices, we have D operators, and these D operators will yield the action of D variables on our fixed vector space. Here, the forgetful map in this direction essentially forgets about the basis of this vector. Forgets about the basis of this vector space, right? On this level, we have a fixed basis, we have k to the n, and we have operators acting on this, whereas here we just have some vector space and operators acting. So this is the forgetful map. And well, if we have such operators and we ask whether this resulting module comes from the Hilbert scheme, then this is the same as asking whether there exists such a subjection. So this boils down to saying that we want a vector. To saying that we want a vector v such that the vector generates the whole space under the action of the matrices. Okay, so here we have the space of stable tuples that will be named in the next slide. And it consists of matrices plus vector. There is the stability condition here and there is the commutativity condition here. Somehow it's important that the commutative condition Somehow it's important that the commutativity condition is purely closed, whereas the stability condition is somehow purely open. So you maybe you don't need to worry that much about the stability here, but the commutativity is something that really introduces singularities. Okay, so all these things have names, so these are commuting matrices, and now we And now you can see that my handwriting in real time is really bad. Sorry about that. Here is the Hilbert scheme, here is the coherent stack, and here are the stable tuples. One good thing about this is that every map in this diagram is smooth, so the singularity types everywhere are the same. But of course, neither this nor this map is surjective, right? There are definitely things in the coherent stack that do not confirm. Things in the coherent stack that do not come from the Hilbert scheme. Okay. And the Kuranishi chart is most conveniently built here, which is why I introduced the coherent, well, which I introduced the commuting matrices. And I should already say that there are two competing conventions for the indices, which I For the indices, which I just realized I also had problems here. So there will be d matrices n by n, and the idea is that the commutativity relation you can phrase in the most naive way possible by saying that you have a detuple of matrices, that over the matrices you have a trivial bundle, that you have a section that over the detuple. That over the d tuple just yields all the possible commutators that you have the zero section and that the commuting matrices are the intersection of these two sections. So this yields the virtual structure if you start thinking about this intersection as a derived scheme or if you start thinking about this intersection and get the fundamental. Intersection and get the fundamental class coming from intersecting these two classes in the vector bundle, then this is how the virtual structure arises. Sorry, can I ask a question about the previous slide? Oh, of course, yeah, sure. Let me go back. So the one here or the previous slide? Yeah, just that one. So this smoothness, so this on the bottom, it's saying something like: if I try to deform. Something like, if I try to deform a sort of a quotient as a quotient of the structure sheet versus just in general, that those are somehow compatible, what is the yeah, because somehow the difference between object here and object here is that you have a surjection. And the point is that if you ask whether this map is smooth, it's the same as asking whether the surjection deforms nicely, and it does. Nicely, and it does because here we are zero-dimensional, even so it's just saying that, okay, yeah, you know, I see. Thank you. So, indeed, the deformations are the same, but of course, it's important to remember that this is just some open subset. So, for example, for d equal to this is this is smooth, this thing definitely has obstructions to the formations. Okay, any other questions? Okay. Any other questions? Alright, so we have the virtual structure coming from this intersection. And well, maybe it's hard to grasp the details, but at least you can see here that this is indeed perfectly explicit. There is nothing, no abstract theorems. It's down to earth. It's down to Earth. And if you have this, then you can use the previous diagrams with smooth maps just to cook up a virtual structure on the Hilbert scheme. So we had one virtual structure on the cometic matrices, so we pull it back to stable tuples and then we push it forward to the Hilbert scheme, essentially. And the important observation is that if we do this, then the dimension we get for the Hilbert scheme will be. We get for the Hilbert scheme will be n. So let me try to go through this. So we had, we are now considering pairs, A2. So we had a pair of matrices, right? N by n matrices. We had this vector, but then we also had the requirement that our matrices commute. So this is just n square. And then to go to the Hilbert. And then to go to the Hilbert scheme, we divide it by GLN. Okay? So the virtual dimension obtained from this one is not what you would expect 2n. This thing is smooth of dimension 2n but the virtual But the virtual structure obtained on it is so dimension n. I have a question. In the global cases, for a threefold, you expect virtual dimension zero, but for a surface, you say you expect virtual dimension n. Well, for three faults, to get virtual dimension zero, you need to be a little bit more careful. So Bit more careful. So I think if you did threefolds in a very naive way, then you would just have three matrices, but the commoditivity conditions will also be three. So this count will be totally off. And then to get the dimension zero, you actually have to pack this three commutativity conditions into this superpotential. So you need to say that essentially they cannot yield three n-square conditions, but they yield a few. Square conditions, but they yield a few less because they are CDs. Okay, thanks. So, for threefolds, there is this trick, and also for fourfolds, I should say that to get something reasonable, you need to use this automas theory that also reduces the number of syges. Again, by just observing that the relations here are, by their very definition, dependent. Okay. So we have this virtual structure of dimension n, which is not the one coming from the smooth structure. So we ask why do we care? Well, we care because this is the one that we will obtain on this IR thing later, but also we care because this is the one coming from the cockstack. So if you consider the map from the Hilbert's. Map from the Hilbert scheme of A2 to the stack of coherent shifts, if you consider the cotangent complex there, truncated and pull it back, then it yields exactly this structure. So it's not about commuting matrices and about all these choices. It's something about the virtual structure coming from the stack of coherent shifts. All right. So this is all very well known. But then if we want to go beyond. But then, if we want to go beyond the Hilbert scheme, well, there are no obvious generalizations of the Hilbert scheme because this would be written in FGA or somewhere a number of years ago, but actually there are obvious generalizations of the commuting matrices that people do consider. And this is simply the idea to not consider all the matrices, but to consider some subsets of them. And the most natural, maybe, idea. Maybe the idea would be to take to observe that the matrices are just the Lie algebra of the GLN group and replace the Lie algebra of the GLN group by any other reductive Lie group. So this is done and it's known very classically by Richardson that you always for pairs of matrices you always get something irreducible. So you should think of this as an analog of saying that the Hilbert scheme of A2 and more generally the quality Of A2 and more generally, the quad scheme of A2 is irreducible. And somehow, perversely, we will not take Allie algebra today, but we will take the space of symmetric matrices. Which is not Allie algebra because the commutator of symmetric matrices is anti-symmetric, so it's almost Allie algebra, but not quite. Okay, so we will be interested in the space of commuting matrices coming from the symmetric matrices, and we will try to construct this IR as the We will try to construct the sky as the analogue of the Hilbert skin. So that's the goal. Any questions? Some of those groups, like SL, you could interpret geometrically in terms of looking at sub-schemes where the center of mass was zero or something, I think, is what. Yeah, that's a very good observation. Yeah. And you're just going to. No, as far as no, no, no. So you just observed that SL essentially yields nothing new. So if we start with SL, then we get just sub-schemes somehow with bary center at zero. And at least in characteristic zero, this is nothing new. But for these other guys, I think they are fundamentally different. Actually, I'm sure that they are. Which is quite interesting that the varieties of commenting matrices have been investigated, the corresponding spaces not, and maybe I will explain why not in the next slide. So, okay, so we would like to now mimic the construction for the Hilbert scheme on this level of symmetric matrices. So, we can consider detuples of symmetric commuting matrices. Matrices and well to get some kind of a stack we would like to divide it but divide it by the orthogonal group maybe this should be more capital O because we want to keep the symmetry so we cannot divide by gln that will kill the symmetry condition altogether so we want we just declare that we divide by the orthogonal group and then we ask what this thing and i just Ask what this thing, and I just define it to be coch or an ad. Okay, all these green things will be new objects showing up, so it's not something that exists, it's something new, at least for this talk. And to understand what's its structure, well, one other thing here which exists on this level is a quadratic form. Is a quadratic form. If we think about the matrix as being symmetric, it's the same as the matrix preserving the usual quadratic form, right? So, all these matrices here satisfy this condition. And if we divide by O, the quadratic form survives. So, on this level, what we get is a vector space with some operations, exactly like here. Operations exactly like here, but also we get a symmetric quadratic form, actually, which is of full rank. And we get this condition. And that's all. The claim is that the stack that we obtain this way is the stack of such vector spaces. Okay, and then we have some kind of forgetful map here and here. And here, but they are not really obvious in the sense that nothing here will be Cartesian because we both restricted our operators and we also restricted our isotropy groups. So none of the right-hand side is induced from the left-hand side. But it will be somehow crucial for us that this is the new player showing up. Okay, so this is just summarized here. So, this is just summarized here, and I should say that starting from now, I will assume that 2 is invertible. So, we get this, okay, sorry, this should be D. We get this tag and the objects are just modules over O of A D together with an isomorphism of these modules, which is self. Modules, which is self, well, which is symmetric, and which corresponds to the quadratic form. Okay, so this is it. Here we had only modules, here we have two objects, a module and an asymmetric isomorphism of the module with its dual. Why is it in the projective space? What why not? Oh, this is somehow because I didn't divide well. Because I didn't divide. Well, yeah, that's true. Okay, so I should have said that also here I divide by k star. That's right. That was sloppy. It doesn't really matter that much because it's an isomorphism, so it's non-zero. I want to keep it in projective space because we will try to form some kind of a proper space. Form some kind of a proper space, and this is to eliminate some obvious non-properness. Okay, so I should say that on this level we are more or less done now, and now we would like to go to the Hilbert scheme. So, to go to the Hilbert scheme, well, the usual Hilbert scheme was just a pullback. So, also here we can introduce a pullback and get Pullback and get something which is called the oriented Hilbert scheme of AD. And it's all right, it's a perfect space, but it's definitely not proper. It's non-proper because we started with AD, but at least, but it's even non-proper if we started with something projective, right? So the morphism from the Hilbert scheme to SIM N of AD is proper, so the Hilbert scheme. Is proper, so the Hilbert scheme is essentially as proper as this thing, but this map even is non-proper. I will explain this. It's maybe not so obvious, but this is somehow the main problem that this Hilbert scheme has too many holes. And if you think that enumeratively it's fine that it's non-proper, it could be fine, but it's not. And I will explain this as well. So we would like to have. So, we would like to have this IR thing that will be better than this because it will be more proper. Okay, so let me explain why is it non-proper. And for this, I just need to write down what are the points. So, recall that on the stacky level, we had on this co and or we had pairs, a module, and phi. So, here the module. So here the module just translated in the usual way to a finite subscheme, but then phi was just phi, so it was an isomorphism from the dual of the structure shift to the structure shift itself. So such a phi is actually called an orientation. And now you may see that the sa that the superscript or appearing is for oriented. Appearing is for oriented things. And such an orientation actually forces the sub-scheme to be Gorenstein. That's more or less the definition that the dualizing shift should be locally free. So we are in the zero-dimensional case. So it should be just free. So we get Gorenstein subscript. Okay. And maybe I should say that in terms of I should say that in terms of Mora's talk, this has something to do with Hermitian K-theory. So such a space exists. And the non-properness of the map from this space to the Hilbert scheme exactly boils down to this Gorenstein condition. So I should say something more about the Gorenstein condition. And the first thing I want And the first thing I want to say is that this condition also showed up on Monday, I think, in terms of this non-degenerate pairing. So the Gorenstein subscheme is the same as commutative Frobenius algebra. All right, that was a lot of new notions. So if you have questions, feel free to ask. If not, I will explain something more about the Korenstein thing. Sorry, when you write OZ2O. Sorry, when you write OZ dual, what's the dual into what? What's the dual? Well, that's yeah, that maybe you are happier with this notion. That's that the dual derived dual yeah, I mean this is zero dimensional, so this is the dual on in OZ modules. And in OZ modules, since this is zero-dimensional, the derived dual and all the other dual. And all the other duals agree. So, for example, we can just say that, well, OZ dual is the same as take the global sections, take the dual vector space, and Schiffify this. But I think the better way is to say that in the zero-dimensional world, all possible dual functors agree. So you can pick your favorite one and it will work. Isn't it concretely just like X chief X D? Chief X D of whatever, O Z O Right, but I mean, this, okay, but if you want really concretely, then maybe you can just assume that this is this one, this shift with some induced structure, the H0 and the dual vector space. Here we are assuming that everything takes place over a field. I will explain more about this in the next slide. So I will erase this here, but it will reappear. Okay, thanks. Any questions? Any more questions? Okay, if not, then let me go to the Gorenstein algebras. Okay, before going there, maybe I should mention some things about the expected dimension, but maybe I'll skip it. Expected dimension, but maybe I'll skip it. Okay, so there are zero-dimensional Gorenstein algebras, and I should say that if every zero-dimensional algebra was Gorenstein, I will definitely buy into such a setup. But unfortunately, they are not all Gorensteins. But the ones which are Gorensteins are usually much better behaved. So an algebra is Gorenstein if and only if the dualizing shift is cyclic. And yeah, I will just assume that everything is over a field. So this is just the dual. This is just the dual vector space of A. And if you think about how to make it into a shift, and there is only one possible way, if you want to act on this and get another functional, then this has to be done this way. Okay. So that that's the different that's how the A module structure is obtained. Module structure is obtained. And in most concrete formulation, an algebra is Gorenstein if and only if the localizations are. And if you want to ask whether a local algebra is Gorenstein, you compute the annihilator of its maximal ideal, and you should ask that it's one-dimensional. This is somehow a very small part of the self-duality is requiring this, but requiring. Requiring this, but requiring this is enough to get self-duality. So, if you get bored with the talk at some point, maybe you want to do this algebra exercise. But thanks to this lemma, we can see that, for example, this one is not Gorenstein because the soccer here in this setup, the soccer is just. Is just given by the maximal ideal. The square of the maximal ideal is zero, so the maximum ideal is all in the circle. Okay. Fine. So, okay, so why do we care? Essentially, we care because most of the natural constructions. Because most of the natural constructions yield Gorenstein algebras, while cohomology rings yield them by Poincaré duality. Complete intersections or even locally complete intersections are Gorensteins, and R-tine reduction of smooth varieties are also Gorensteins. But regretfully, most of the monomial ideas are not Gorensteins. This is another exercise that maybe you want to do if it rains too much today or during the week. Or during the week, that the only monomial ideas which are Gorenstein are just very naive complete intersections. So this makes enumeration on the Gorenstein locus very weak because we just don't have enough torus fixed points. And it's even worse, but maybe let me say first, why is it better? So if we consider the Gorenstein locus, it doesn't have many fixed points, but it Doesn't have many fixed points, but it's open by general nonsense. And crucially for deformation, if you're in low dimensions, it's smooth. So the Hilbert scheme of 8,3 is definitely not smooth, but the Gorenstein locus inside is smooth, which is something which gives some hope that maybe there is a better compactification of the Gorenstein locus, which will be smooth. That's very. That's very far off what we can construct, but it is quite intriguing. It's bigger than the smooth locus. Sorry? Is it smaller than the smooth locus? Yes. Actually, well, yeah. If you want to know the conjecture about the smooth locus, then I'll be happy to share some preprint that is being prepared to Friedwig, who is also online here. Line here. So that was a perfect question, but that was not arranged before. But it's strictly smaller than the smooth locals. Yeah. All right. So let me say a little bit more about this problem with the Taurus fixed points. So even if we forget about monomial ideas, and even if we just consider ideas and we even if we just consider the scalar multiplication on AD then we start having problems already for three points so for three points if we consider the Hilbert scheme of AD and the Taurus fixed points then these are ideals given by sorry these are sub schemes given by homogeneous ideals and if we kind of require them to be Korenstein and even fix this orientation then what we Then, what we get is only the things coming from the schemes inside A1s. So, if we just try to take the GM fixed points on this oriented locus, then we only get schemes which will show up already for A one. So nothing new. So we can do some enumeration, but it will not yield anything other than for hilt A one. For Hilt A1. All right, and that's somehow the main problem. I would like to construct, and I will construct later, some proper space which compactifies things like that. Okay, let me pause a little bit. But no questions, so let's go. And for compactification, we need to figure out something new. New and there are two observations here. First of all, the observation is that we had this full rank quadric showing up already in the stack. And being full rank quadric is definitely open, but it's definitely not proper. So we may start asking what kind of compactification for full rank quadrics we would like to have. I mean, we have this set, even if we don't have any operators acting, and we need to have. We need to have some compactification of it. And fortunately, there is a very classical compactification that is called complete or completed quadrics, and that goes all the way to the 19th century. So this was introduced by Chussels, and then a lot of people worked on it. But if you want some references, then Tyrrel, Laksov, and especially Kleiman, and then Kleiman. Especially Kleiman and then Kleiman Torup introduce it in full generality. And it's a perfectly nice space. It's very prominent in enumerative geometry, but it's non-existing in deformation theory or moduli spaces, at least not yet. And it's somehow, it's great, but it's involved. It's linear algebra, but it's an involved linear algebra. So I will only mention something about the construction. And the construction is that we can. Is that we can consider full-ranked quadrics, and then full-rank quadrics are a homogeneous space, and such a homogeneous space admits a wonderful compactification, and this is the space of completed quadrics. That's one take. The second take is that you can construct by hand a space of completed quadrics, which is smooth, and crucially, it's projective, and it's even explicit, so that you can really write down the local. So, that you can really write down the local coordinates. The local coordinates are not even given by rational patches, they are given by full affine spaces. It's as explicit as Grass-Mannion say. And then, if you ask about the construction, then I should say that there are standard constructions which are nice, but only nice if you start thinking about them for long enough. And I will just mention one of them: so you start with the space of. So you start with the space of all quadrics and then you consider the blow up of rank one quadrics. Then after that you consider the blow up of rank two of the strict transform of rank two and you go on and then you stop at n minus one. So that's the construction, it doesn't tell much about the geometry. So, the obvious question is, and I can almost answer it, is you don't want to just include full rank into PSIM to V, which would be projective, but that's exactly. Yeah, if I did, if I did that, I don't want to just take the obvious inclusion because if I did that, that would be essentially forgetting about the quadric altogether. Somehow, it will yield. Somehow it will yield a space of algebras with a quadric and maybe some compatibility condition, but no non-degeneracy condition. So I don't want this. I will just get something in the Hilber scheme. But indeed, I want to take more. So this space of completed quadrics has a projection map to this space, which exactly will yield the usual limit. Where do you stop the rank n minus one, n minus two? Where does it stop falling up? Well, it stops at it stops, I think, at n minus two, because rank n minus one is already a divisor. Okay, great. Okay, so I will not say much about the construction, but I will say something about the points in the But I will say something about the points in the next slide. So, if you care about the construction, you should ask now. If not, I go to the next slide. And the points are, well, somehow it's iterated blow-up, so also the points will be iterated. So, the points will be given by sequences of quadrics, varying length sequences, and flux. The flux will be induced by quadrics, so you can forget about flux if you prefer. So, first of all, we have the obvious. So, first of all, we have the obvious candidate. We just pick any quadric as the first one, but this quadric may have a kernel. And the observation is that if we want to keep track of this non-degeneracy, then we should also introduce another quadric on the kernel. This another quadric will have a smaller kernel, so we can introduce yet another quadric and go on. There is a very beautiful There is a very beautiful interpretation in terms of some kind of space of maps from P1 to some Lagrange and Grassemagnan. And then, in this space of maps, you can have just full rank quadrics. Maybe if this quadric is full rank, then this thing is zero. So all this gets discarded. But maybe the quadric is not a full run, but it's of some rank. And then there is this next quadric, and the next quadric, and you can. Quadric and the next quadric, and you can really interpret this as breaking p1s into well into chains of p1s. And that is very beautiful, and I'm very happy to talk about this. But I think after the talk, it's I think one can really give a really nice talk about this variety itself, but I would like to use it for something. So, can I just ask a quick question though? Sure. So, are you Are you really using the fact that it's a quadric here? Like, couldn't you do this with any space of linear maps from one vector space to another, or just something right? That's that's completely possible. Yes. So I'm thinking about a quadric as a symmetric linear map. So this is possible to do for any home from one space to another. And you get like a wonderful compactification type thing here? Definitely, if you start just from the endomorphism of a given. endomorphism of a given of a given space than you do. Yeah. In general, I think you well you would need to cook up it carefully. Yeah, maybe I should think whether you get a wonderful compactification because if you start from a home from V to W, then you should probably introduce the action of G V times Gw and then try to see whether the stabilizer is Whether the stabilizer is the same as fixed points of an involution. Okay, let me think about this, but I will be unable to answer on the spot. Sorry. Thanks. Yeah. Yeah, so I should say that I think this space is really, really cool. It hardly makes any appearance outside enumerative geometry, but I think it's a shame. So if you even if you don't care about this IR thing that I will introduce later, this is this is. That I will introduce later, this is a cool space. Okay, so there are two parts of it, which almost appeared previously. So, first of all, there is the part that does nothing. We just take full run quadrics, we don't have any other quadrics, and that's exactly the open part. So, open orbit, say. And then there is the part which is most degenerate. So, the most degenerate. Part which is most degenerate. So, the most degenerate part is that you have, you cannot take a quadric which is just zero because they were projectivized. So, you take a rank one quadric and you keep taking rank one quadrics. So, what you get in the kernels is a full flag, right? The kernel of the first quadric will be of corank one, this will be of coranc two, and so on. So, we'll get a full flag. And the important observation is that if you start with a such That if you start with such a full flag, then the quadrics are actually discarded. Because if you have this flag already, then the QI is a quadric on a one-dimensional space. And up to projectivization, this is just a point. So, in this way, we get a flag variety inside the Inside this space. So if you do this, the first quadric is just a union of two hyperplanes, and that continues. Is that the idea? Well, the first quadric should be a square, right? So it's just non-reduced hyperplane. If you take a union of two hyperplanes, then you get run two. Ah, right two. Okay, thanks. Yeah, which you can totally do. And You can totally do, and these two examples are just the extreme ones. Somehow, everything else will fall in between. In the second example, the flag is all the data. In the first example, the flag is non-existent. And what is somehow worth mentioning is that the flag has varying length. It's not that the flag is fixed, there will be as many elements of the flag as the quadrics. Okay, other questions? And I should probably speed up a little bit. Okay, now comes the new part. So if you really care, then this is new. And it's very naive. Namely, you can take a point of completed quadric and you may ask, okay, if the point was just the usual full-run quadric, we could say what is a symmetric matrix with respect to that. matrix with respect to that. We want to degenerate this condition. So we would like to say some, we would like to say what is a symmetric operator with respect to any point of completed quadrats. Okay, so there is we fix a point and now we ask about the compatible operators or you may think intuitively symmetric operators. And the definition is that such an operator should work with the flag. Should work with the flag and on subquotient, it should just be symmetric. Because on subquotient, recall that this thing is of full run on the sub-quotient. This is how the sub-quotients are constructed. So, this wonderful compactification makes a full run quadrics break into several pieces, but they are still a full rank. And if you do this compatibility, And if you do this compatibility and check what you obtain on two examples, then well, if we had a full run quadric, say just diagonal one, then we obtain the flag is non-existent, so we obtain a symmetric matrix. But more interestingly, if we just consider the usual standard flag, then the compatibility condition boils down to saying that the matrix is upper triangular in the basis. And the wonderful thing is that the dimension between here and here match. Okay? So if we start asking about this degeneration, then there is a hope that maybe indeed one can say that the space of upper triangular matrices in this setup is a degenerate version. Setup is a degenerate version of the space of symmetric matrices. And this hope is true because there is a sub-bundle on the whole space of completed quadrics such that the fiber of the sub-bundle consists of compatible endomorphisms. So there should be compatible endomorphism. So if we consider this varying So, if we consider this varying in completed quadrics, we also get a varying spaces of compatible endomorphisms. Okay, and this very poorly visible neon-green thing also recalls me that for further use, I would need a slight alteration of the compatible endomorphism. So let me go back just to the definition. Just to the definition, and say that I also want anti-compatible endomorphisms which will correspond to anti-symmetric matrices. And the theorem says that there is also anti-compatible bundle. We would need an anti-compatible bundle because the commutator of two compatible things will be anti-compatible. Okay, great. Then now we have all the ingredients necessary. We have the completed quadrics. We have compatible operators which will be the analog of symmetric matrices. And now we can just start constructing. So in the Hilbert scheme, we had commuting matrices. In this oriented version, we have Oriented version: we have symmetric commuting matrices, so the commutators were actually anti-symmetric. Sorry, and now the new version says that we consider things relatively, no longer over a point, but rather over completed quadrics, and we consider only compatible ones. So, the completed quadrics is completely new ingredient here. On the Hilbert scheme level, the completed quadratics were. Scheme level, the completed quadrics were just a point. So we can consider tuples and consider commuting tuples, and the commutativity condition is something on the anti-compatible bundle. That's like a lot of new notions, so maybe let me pause a bit in case you have questions. Question. Isn't there any condition on the Why isn't there any condition on the diagonal elements of the upper triangular matrices? Is there any condition on the diagonal elements? No. For compatibility, not really, because the diagonal elements will be the multiplications on this sub-quotient, but it will be scalar multiplications, so any such multiplication preserves quadrics. Any such multiplication preserves quadrics, okay? So, formally speaking, there are conditions because we need these to work with the quadric, but since they are just scalar multiplications, they will always work with the quadrics. So, that's a good catch, but there is no additional condition. No, I mean, the point is that the condition we get at the end of the day. We get at the end of the day is not so nice, but maybe I can just write it. It will be this. And then, well, and then since everything is one-dimensional, then this is just such a condition. Such a condition where alpha i is this diagonal element appearing somewhere in xi. And this condition is automatically okay just because the quadrics are bilinear. So just let's take the case two by two matrices. Symmetric matrix. And what's the degeneration? You take like the lower corner going to zero and you don't do anything. Border going to zero and you don't do anything to the upper one. Is that the idea? Yeah, I mean, if you, if, yeah, exactly. If I want to degenerate with respect to the most standard one, then I do exactly this. So flag tells you which corner to degenerate. I think, yeah, depending on depending on the generation, I think it will be. Oops, sorry. Oops, sorry. It will be something like that. So you start with a symmetric matrix and then you multiply by you multiply each entry with this. So by this, I mean you just, yeah, you multiply this one by T1, this one by T2, and this one by T1, T2. And that's the generation. Yeah. Okay, thanks. Any other questions? Okay. Yeah. So we get the bundles and then we get the we kind of form this new variety of over the completed quadrature. The completed quadrics, which consists of compatible endomorphisms which commute. Sorry, was there a question? No. Okay. And then finally, we can form IRNAD. So IRNAD is just formed by taking the usual Hilbert scheme of endpoints, considering the completed quadrics of the universal bundle. So the completed quadrics which I introduced. That quadrics, which I introduced, were for a vector space, but they work the same for a vector bundle. And inside the universal bundle, you can consider compatible quadrics. Okay, so here in the universal bundle, you have a complete quadric, and you also have the point of the Hilbert scheme. So you have some variables acting. And the closed subset that we are interested in. Subset that we are interested in are compatible quadrics. And you can say the same thing starting from commuting matrices, so starting from the variety of compatible quadrics, you consider this space of a vector and a bunch of matrices which are compatible and such that the whole thing is stable. The stability condition is the same that was before. And then you can force commutative. And then you can force commutativity. If you do all this, then you get the same result, modulo, GL and action. Okay, so that's our modular space. And one thing, there are two things to observe. First of all, Okay, that's a tricky one. I think it should be GLN because essentially the difference between here and the universal bundle is that we fix a basis and this basis has no idea whatsoever about the quadric. But I have to say that I would need to carefully check whether which one else is. Sorry. Yeah, sorry about that. Yeah, sorry about that. We will be interested in this part starting from now, so I may have made a mistake here. All right, so one important thing is that this map is projective. And it's projective somehow, it's projective precisely because we took a projective variety here. And the other remark is that. And the other remark is that if you ask what is IR, then IR is the name of Tony Aarubino, who in completely different language in the 70s investigated Taurus limits on this space. So there is a GM to the D acting on this space, and you can consider Taurus limits. So that's called the Yarabino scheme. Okay, so that Okay, so that's exactly what we wanted to construct. Make sure I understand something simple. The image of this projective guy is some lower-dimensional thing, right? You mean the image of the Ciarabino thing in the universe? Yeah. Actually, no. As constructed, it's projective. Because you Because you have this very degenerate point here. So, yeah, for every image is not some closed set of lower dimension? Somehow it's objective for a fairly stupid reason in that if you start with a finite scheme, you can fix some kind. Fix some kind some flag of sub-schemes where the difference is in any case one. And then you can cook up at this totally degenerate point here corresponding to such a fluck. But yeah, I mean, I don't claim that's a full explanation, but maybe it helps to see. So it's not, it's still the case that. It's still the case that this HILP R sits inside and is open. We just have a projectization. I'm a little confused about the terminology because you define compatible endomorphisms. Right. You're just saying compatible quadratics are just defined by this diagram. Oh, sorry. I mean, yeah, I mean, compat, well, yeah, compatible. Compatible, say okay, and the morph is compatible with the quadric, yeah. Yeah, yeah, and this this makes if we force this compatibility, then we can have indeed an interpretation of the points here as. Points here as some flux of sub-schemes with self-dual subquotients. So they are not necessarily self-dual sub-schemes anymore. Sorry, but they are sub-schemes with a flag, such that sub-quotients of the flag are self-dual. So the idea is that a sub-scheme which is self-dual, if you push it to the limit, it may stop being self-dual, but it will break as a flag of self-dual things. All right. So now I would like to, in the final part, which is I think like two minutes, if I understand correctly, I have something like that. I would like to briefly say something about the enumerative geometry. And the enumerative geometry of this is surprisingly nice. So this is work in progress with Rainier and Andrea. But one thing which is remarkable is that this HILPR. This Hilp R has some expected dimension, and if Hilp R is smooth, then it has this dimension. So it's natural to ask whether if we go from HilpR to this IRN, whether this thing has any virtual structure of this expected dimension, say for small d. So it is so, and it is quite similar to the human. Is quite similar to the Hilbert scheme. So, if you start with A1, or more generally, if you start with a curve, then the IR and A1 is just smooth. It's a smooth space of dimension 2n minus 1. And if you go to A2, then it's no longer smooth, definitely, but it has a perfect abstraction theory with virtual dimension 3n minus 1. And this perfect abstraction theory is exactly cook up from Hookup from this commuting matrices stuff. And we don't know what happens later on. So we don't know what happens for A3. That's a very interesting question. And what happens for A4, but there's some hope. So the good part about doing this Yarabino thing is that even though it's maybe exotic, the arguments don't The arguments don't go through as in the Hilbert scheme, but some versions of them do go through. And to show you what does go through, maybe I should spell out two things. So let me first spell out that if we consider the forgetful map in the case of curves, then this map is flat, it's not smooth, but nevertheless, the push forward of the structure. Nevertheless, the push forward of the structure shift of IR is the structure shift on HILP. So it's a funny map with singular fibers, but it's still possible to prove that it is somehow all connected. And in particular, the Euler characteristic counts on both sides agree. That's one remarkable thing. And the last thing that I can mention is that the Sierra Bino scheme will. The Sierrabino scheme will have much more natural line bundles than the Hilbert scheme because there are some nice bundles which come from completed quadrics. The completed quadrics has picar1 and minus one, and so we have some natural bundles for which we can compute the order characteristic. And conjecturally, if we take all this into account, not just the structure shift, then we get a pretty surprising formulas for which are definitely not tautological. Which are definitely not tautological. That if we consider the bundles, then what we get is a contribution from the structure shift, a contribution from the square of the determinant of the universal bundle on HILP. And all this is really mandled together for different k's and different n's. So the n is just the number of points, the k is the index of this bundle, but they get glued together somehow. They get glued together somehow. So, these bundles in total have some intertwining geometry. And currently, we are investigating this for A2 and the virtual structure as well. Okay, over time, so I should stop here, definitely. Thank you for the talk. Are there any questions? 