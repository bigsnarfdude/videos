Robust estimators for models with the censored covariate. So this talk, it's really, I'm just going to dive into this problem that I've been thinking about and exploring for the past few years. And it's really just showing you some of the neat things that have popped up. And at the end, I'll hint at some solutions, but really. At some solutions, but really it's focused on the problem. So, the motivation for this statistical problem comes from a neurodegenerative disease that I've been fortunate to look at for now a decade. It's known, it's called Huntington disease, and this is a disease of the brain that leads to three primary impairments. The first one that it's most known for is a motor impairment. For is a motor impairment, basically the loss of your motor ability. And this picture here is a picture of a person with chorea, which is basically uncontrollable motor movements. Another impairment is cognitive impairment or memory loss represented by these fading leaves. And a third impairment is behavioral psychiatric. Psychiatric, such as depression and anxiety. So, looking at this triad of symptoms, you might be thinking of more prevalent neurodegenerative diseases, such as Parkinson's disease, which is also very much a motor impairment, and also Alzheimer's, which is recognizable for its cognitive impairment. So, but what makes Huntington's disease really, really Huntington's disease really, really special in this family of neurodegenerative diseases is that it is 100% caused by a genetic mutation. So it's a mutation in this gene called the Huntington gene. So all of us have this gene, and you can know if you have this gene mutation with a simple blood test. So what happens is someone takes your blood. Happens is someone takes your blood and they count, they say you have a mutation in the Huntington gene if you have more than 36 CAG or trinucleotide repeats in this Huntington gene. So if you have more than 36, you are guaranteed to develop this disease during your lifetime. If you have less than 36, you're not, you won't ever develop this disease. So now I want to clarify this just a little bit: is that so you are born with this gene mutation, or you could be born with this gene mutation, but it doesn't mean that at birth you have the disease. What typically happens is that typically in your 30s, 40s, or 50s, the disease manifests or begins. You meet the criteria for a clinical diagnosis. For a clinical diagnosis. So in 1990, when this was discovered, this gene mutation was discovered in 1993, and the Los Angeles Times hailed it as one of the greatest discoveries in genetic science because it opened this possibility that, wow, we can study people who have this gene mutation before they develop any, oops, any of these. Any of these impairments or symptoms, we can recruit them from very early on and observe them and see what happens. When do these impairments begin? When could we start treating them? And that's exactly what has happened. The NIH and also other government agencies have funded these studies, one of them known as Predict HD, which was roughly a decade-long study. Roughly a decade-long study that recruited individuals with this gene mutation and observed them over time. So what was really neat about that data set and many others similar to it is that it opened this possibility that maybe we could search for an effective intervention. So I want to show you some of the data here. So what I'm plotting here. So, what I'm plotting here is the, it's kind of hard to see, but there's these gray lines, these spaghetti curves, where basically these are quantitative measures of a patient's symptom severity score. So basically, you can think of it as those a combination of the motor, cognitive, and behavioral psychiatric score combined together to Combine together to quantify your symptom severity. So the higher the score, the worse your symptoms are, the lower the score, it's not so bad. And on the x-axis, I'm plotting the data in terms of years in the study. So if I just plotted the data this way as I have, and so again, just everybody's longitudinal scores and then just connected with straight lines. Scores and then just connected with straight lines. And this solid curve here is just the lowest average. I call it a symptom trajectory. So this is from the Predict HD study, an observational study. There was no intervention given. So if I were to just look at this symptom trajectory, I could maybe infer that, okay, well, the symptoms appear to get worse. To get worse. And then, oh, wait, a couple years in, it starts to get better, like lower. And so you're like, huh, why is that happening? This is a neurodegenerative disease that other, many other people have said it worsens over time. And so maybe some of you are already guessing at what's going on, but the problem is that because of the way that I plotted the data at baseline, so when people entered the Baseline, so when people enter the study, some people enter the study at different, I would say, disease stages. Some are more advanced in the disease than others. So by plotting everybody's data, no matter how severe or how far in the disease you are, you're going to miss that. You're going to see this kind of not realistic curve. So, how can we adjust for the fact that people do enter the study? That people do enter the study at different phases or stages of the disease. So, one idea that's been proposed from the Huntington literature is, hey, why don't we plot the data in terms of years to clinical diagnosis? So the Huntington disease researchers have agreed to some agreed-upon definition of what criteria meets. Definition of what criteria meets a clinical diagnosis. Yes, there's controversy, but I'm going to put that to the side for now. And so, what I'm showing here is the same spaghetti curves, but now shifted in that this negative six here represents six years before a clinical diagnosis. So, zero is the when diagnosis occurs, and this is four years after a diagnosis. So, here after So, here, after moving the data that way, now if I look at the average symptom trajectory, this is much more realistic in terms of what people have seen in that the symptoms, it's not so bad, like this far out, and then it slowly starts to increase and accelerates after clinical diagnosis and then continues thereafter. So, this is a much more realistic. This is a much more realistic representation. So, I apologize, the gray curves are a bit too light, but if I go back, you might see, hey, there's a lot of gray spaghetti curves there. And here, a lot less. What happened? Did I just throw out a bunch of data? Yes, I did. Because the data here is these spaghetti curves are from people who were diagnosed. were diagnosed during the study period. Okay, so we're dropping a lot of data. There are 80% of people in PredictHD during this observational study did not meet the criteria for a clinical diagnosis. And the reason for that is because like Alzheimer's and like Parkinson's, Huntington's disease progresses very, very slowly. Very, very slowly. So, if we could run a study for 100 years, then we could probably capture everybody's curve, but we can't, right? Predict is only about 10 years long. So we have this problem. Okay, how can I, I'm going to, I want to model this Y, this symptom severity score as a function of, I'm going to call it X. Of, I'm going to call it x, time to clinical diagnosis. So the x here represents time to clinical diagnosis, but this is potentially right censored. Okay. And I want to clarify it is right censored because, as I said, these patients that were recruited, they have the gene mutation. They will develop the disease sometime in their lifetime. Now, I'm putting aside apologies for the graphics. Suicide, apologies for the graphics, like if they commit suicide or they're, you know, something else happens, if that doesn't happen, they will develop the disease. So, meaning that if they weren't, they did not meet the criteria for a diagnosis during the predict HD study or the observational study period, it just means that we know their diagnosis will occur after the study ended. We just don't know when. And that is the definition for a right-censored. For a right-censored data or covariant. Okay, so this is what's called the censored covariate problem. So I'm going to, so the data I showed is longitudinal, but I'm just going to very, very much simplify it so that we can get to like the crux, the core issues. So here, suppose, again, very much simplification. Again, very much simplification. Let's just consider our regression model with a right-censored covariate. So I'm going to have, oops, okay, an outcome y as a function of x here is going to represent my time to diagnosis, z are other patient covariates. And this m here, again, for simplification, it's going to be some. Simplification, it's going to be some function, whether linear or nonlinear, that I know up to a parameter theta. So, for example, it could be a logistic function or even just very simply just a linear function plus error and where errors are friendly, normal, zero sigma squared. But because of right sensoring, instead of observing x, we're going to observe w, which is the minimum. W, which is the minimum of your time to diagnosis or some random sensoring time. And delta here is the sensoring indicator. So do I observe my time to diagnosis or do I observe the sensoring, random sensoring time? And so the observed data is this Y, W, Z, and delta. And my goal is just to construct a consistent estimator for beta. A consistent estimator for beta, which are basically the parameters in my model. So, how can I do that? So it's just a friendly regression model that many, I think most of us have seen in our training. So, when I first saw this problem, I was thinking, you know, let's explore the tools that we already have in the literature. So, the first idea. So, the first idea that I and others considered is like, well, okay, X is right censored, but hey, that's kind of like missing data. Like, either observe it or I don't. Okay, so why not? We have missing data literature is huge. Let's explore that. But in the missing data literature, yeah, you only you observe X or you don't observe a value and you don't know what's the And you don't know what's the range of that missing value. With the right-sensored covariate, you have additional partial information, meaning that, yeah, I don't know if a person is right-sensored, meaning they did not meet the criteria for diagnosis during the observational period, but I do know that their X is larger than their sensoring time, C. So X is larger than C. So I need, I can include that information into. include that information into my analysis. And that adjusting for that partial information isn't, you know, if you were just to take a run of the mill missing data technique in an R package, say it wouldn't adjust for that right away. So that's important to adjust. Another view you could think of is just we saw Grace's talk. You could think, oh, well, maybe this is like a form of measurement error in a way. Of measurement error in a way, like X is slightly mismeasured or imprecisely measured. But again, when you have mismeasured data, the data that you observe, like the true values, you don't have any inclination, at least as far as my recollection, like whether it's above a value or below a value, which again, with the right-censored covariate, you do have, again, that additional partial information. So, that said, so I'm trying to say that it's not just pulling up one of these existing techniques, but that said, we can borrow those techniques and try to adjust it to this particular problem. And that's what's been done. So one idea borrowed from the missing data literature is: how about imputation to replace all of the censored X with the expected? X with the expected value of X given that we know it has to be greater than C and Z. And so for those of you probably familiar with residual life functions, this has that similar flavor. Another idea is, hey, why not maximum or pseudo-likelihood estimation that accounts for covariate censoring? So I'm going to have to ask you to trust me on the math. To have to ask you to trust me on the math, but if this is my model and I assume, just for simplicity, assume that X and Z are independent, or sorry, X and C, the sensoring time are independent, then it breaks down into a part that's uncensored data and a part that is censored data. Now, in both of these methods, what's really critical here is actually. Here is actually this density for x given z. So, what's my density for my time to diagnosis? It pops up here. It's important. And also, sorry, this S here represents the survival for time to diagnosis. So this is a function of this little F. So, what's really crucial here is getting those densities correct. And so, So you might, people, I know we've seen a lot of semi-parametric and non-parametric talks. You might be thinking, just let's use one of those. And definitely, I totally agree. And so we were exploring those different options. But before I get to that, I just want to explain, and maybe you don't need convincing, but what can go wrong if we get this density wrong? So kind of seeing. So kind of seeing where this is coming from or what's the relevance of this. So in, let me pause from that and step back to like some ongoing clinical trials for Huntington's disease. So one of the goals is, can we slow this disease or even stop it? But stopping it has not worked out so far. So can we slow this disease? What do I mean by that? Disease. What do I mean by that? So, for example, I see that symptoms, there's a quite a large slope here. If I would love to see, like, if I applied a treatment, my slope goes, like, I expect people who don't, who receive the placebo to have a symptom trajectory like this. I'd love to have a treatment that makes them go down, like the symptoms go down. Here, Here, so that's like seeing that change is quite important. So, if I were to recruit people, for example, in this timeframe where it's pretty flat, if my curve is just flat like that and I apply a treatment, there's really no chance that I'll see any effect, right? It's not going to change very much. So, one of the goals is when can I recruit patients so that I expect them to have a pretty Them to have a pretty high slope in the symptom trajectory so that I can very visibly see there's a change. So estimating the slope is important. So I'm going to further simplify this. I'm just going to have, let's consider this linear model where I just want to estimate the slope in front of this right sensor X. And again, I'm going to apply existing methods, whether imputation or these maximum or sort of likelihood. Mute maximum assertive likelihood, where what and I just want to see what goes wrong if I estimate this wrong. So we ran just a simulation study. And I'm just showing one of the results, but the results are similar across, where we generated data where there's heavy sensoring, and by that I mean 80% censoring. And what I'm showing here is the estimated slope, so the slope in front of the right-censored covariate. The right censored covariate. And if I were to somehow know the true density f given z as my sample size increases and I use one of the existing methods, things work out really nice. My, excuse me, this dashed solid line is the true slope. So I want to try to get as close to that as possible. And the red box plots are pretty close to that. So that's great. But if I use, if I But if I use, if I incorrectly estimate that density, even as my sample size gets quite large, I'm going to have quite a bit of bias. And these estimate, this incorrect density, it's even just a very slight difference. So we even tried, for example, like a Cox model where we didn't, we had, we missed an interaction effect, for example, in the hazard function or just one little X, one little hazard function thing. X one little hazard function thing was off. So, and so we explored: okay, well, yeah, that bias kind of sucks, but does it really matter? Like, maybe it's not a big deal, right? So, we ran a power analysis using techniques from the Huntington disease literature, what, like what their power analyses they were going to do. And so, what I'm showing here is if I used, if I had used the results from this blue analysis, so this incorrect. Blue analysis, so this incorrect one, and I use that slope to calculate: okay, how many people do I need to see a significant decrease in the slope? To get 80% power, again, just on the simulated data, I would need about 99 people. But that is the wrong slope. If I had used the correct slope, using those 99 people, I'd only have about 25% power. 85% power. So, this makes a difference. Like, we want to get this right. And so, we explored many different ways to estimate this FX given Z. So we started with parametric, which maybe all of you are saying like, no, you shouldn't, don't even go there. We did try the Cox, as I said, even just little changes, we still saw this bias. We still saw this bias. We explored even like some sieve estimators and even kernel smoothing estimators. And even if that was slightly wrong, it still led to bias. And I think part of the issue is because that density is right here. And when there's high sensoring, doing this integral, basically, you don't have a lot of information in the tail because of high sensoring. So it's really, really hard. High sensoring. So it's really, really hard to get that estimate correct. So we've been exploring ways like: okay, are there techniques that we can get this wrong, this density wrong, but still get these parameters estimated without bias? That would be awesome. So I'm just going to end with the big question: can we get robust estimators? And I'm just Get robust estimators. And I'm just going to show you. Yeah, I'm ending the talk with a bunch of equations, which, you know, that's maybe I've already lost you. But anyway, we were just exploring, you know, hey, let's start simple, like what estimators are out there that maybe, you know, can we evaluate do these dents, so these densities are in that likelihood function? And can, you know, how. And can you know how good are different estimators? Can we misspecify these distributions? So, we were very simply exploring other things, simple things. One is, for example, a complete case estimator, which again, just throwing out the data, just use the uncensored data. And yeah, it sucks because we're losing a lot of data and we lose efficiency, but at least it's robust to misspecification of. To misspecification of these entities because they're nowhere in there. Okay. We thought, okay, yay, that's great. We tried inverse probability weighting, which we see is actually a special case of complete case where you just inversely weight by these, the probability of censoring happening. And that it is a special case of complete case. And yay, it's also robust to misspecification, but there's, we have loss of efficiency. And so we thought of like, oh, the And so we thought of like, oh, the augmented IPW, that could work. And so I'm going to ask you to trust me that this is the correct form. And yes, it's, you know, it has some nice properties in that, yes, it's robust to that f of x given z that we wanted. But interestingly enough, like in most augmented IPW, it's usually a doubly robust estimator. For this problem, it's actually singly robust. Robust. It is not robust to misspecification of the sensoring distribution. And I'm just going to end with these, just a box plot. We ran these, just to test out these estimators. And I'm going to point your attention to this one. This is 80% censoring. These are the different estimators, the oracle where we don't have any censoring, the naive where, hey, let's forget, like pretend it isn't censored. Pretended isn't censored, and all these other ones which I listed. So, at least we see robustness to misspecification of FX given Z, but there's more work to do. And I'm out of time, but happy to talk later if anyone's interested. So, thank you. Thank you, Tony. Thank you, Tanya. Does anyone have questions for Tanya? Larry? So, this is really interesting. So, on the misspecification table, I'm just curious. So, if you misspecify the model for C given Z, the IPW estimator still is unbiased. But so, is the AIPW is the different form of double robustness where you have to correctly specify both X given Z, C given Z, or Z? Z C given Z or Z, is that what it is? Right. Yeah, good eye, Larry. Yeah. Yeah, so I'm happy to sit down and show you the math, but we can show that this is robust to misspecification of X, given Z, because actually you can, when you do, when you one key step is showing that the expectation of this term, this whole thing is zero. This whole thing is zero. And when you do that expectation, you can condition nicely, and this will actually not play a role. And you'll be able, because this has mean zero, you'll be able to condition out. The part that's the reason AIPW is only singly robust is because we can show that this term, both of these terms, excuse me, this and the augmented term, also with conditioning, you can get rid of parts that aren't affected by X. Get rid of parts that aren't affected by X, given Z, but this, this one minus delta over probability of C, that depends on the density of C given Z. And it's this term that really needs to be zero. Otherwise, it won't be like unbiased. So I can show you the math if you're, I know that's kind of speed math, but yeah. Thanks. Yeah, I just have a question. So, can I try to say it to you and you tell me if I understood it correctly? Okay, so you're saying that you're, well, actually not your outcome, but the clinical outcome is disease onset, kind of. I mean, not the, not the why, but just like for a person. Sure. Disease onset. Sure. And On set. And if you were to conduct a clinical trial looking at a treatment where the hope is to delay onset, that might be hard. So if you instead, you know, you convinced us that actually you could just look at symptoms, like consider the longitudinal trajectory, the symptom trajectory as a surrogate. As a surrogate for disease onset. But to do that, if you're using basically the slope of the symptom trajectory as like your outcome in this clinical trial to see if the treatment has an effect on this slope, you better make sure that you have the slope correct. That's right. Okay. That's it. All right. So for the sake of time, perhaps we'll go to the next speaker. So on the schedules, So on the schedules, Lengu was super