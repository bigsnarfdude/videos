A flexible way to model dependence in extremes. And this has recently been extended to a spatial setting by Jenny and John Tong. And yeah, this offers a flexible approach in terms of the types of tail dependence that you can capture when you think about X streams. So in their paper, what they did in terms of inference was to use a likelihood-based approach. Approach, in particular a composite likelihood approach, which was able to handle hundreds of locations in space. So what we're trying to do now is to increase the number of spatial locations that we're able to handle. And you've already seen the title, so spoiler, it's using Gauss Mac of random fields and INTLA. So just a bit more on the comment about On the comment about conditional extremes models being flexible in terms of tail dependence. We've also seen this extremal dependence measure chi in earlier talks this week. So the idea here is that we take our two random variables x and y and look at the probability that one is greater than some quantile at level u, given that the other one is, as this u tends towards one. So if Towards one. So if this chi is greater than zero, we're in the setting of asymptotic dependence, whereas if it's equal to zero, we have asymptotic independence. And so the conditional extremes framework is useful because it allows us to capture both of these or either of these settings, which is not necessarily the case for several other models for multivariate spatial extremes. Extremes. So, in environmental applications, we have kind of spatial structure, and we'd expect the value of chi to decrease as the spatial distance increases between locations. So, if this x and y correspond to values of a process at two different locations, we'd expect this chi to decay as we move further apart in space. So, we have already seen some of this, but I'll just give an overview of the conditional spatial extremes model so we're all on the same page. The idea is that we have a stationary and isotropic spatial process denoted by X, and this has to have margins with exponential type of tails, and that's due to the conditional extremes framework so that this conditional approach actually works. This conditional approach actually works. But in practice, that's not too big a restriction. As we saw in Simon's talk, if you're not already in that setting, you can transform so that you are and then back transform once you've done your inference to get back onto the original scale. So the idea of the conditional spatial extremes model is to condition on values of the process at one location, which we'll call the conditioning site S0, being above. Being above some high threshold, and then to build a model for a normalized version of the process at the other locations. So, this is what the assumption looks like. So, if we condition on X at S naught being above U, so this conditioner here, we can look at the joint behaviour of a normalized process normalized using some functions A and B. Some functions A and B, and also exceedances of the threshold U at the conditioning site. The assumption is that these tend, as U tends towards infinity, the threshold, these tend in distribution towards two things that are independent. So let's take the easiest thing first, which is the exceedances above the threshold at the conditioning site. These can follow or are going to follow some exponential. Sort of exponential distribution because we started with our margins having exponential tails, so we keep that. And then the normalized version of the spatial process is going to tend towards this process Z0, which is called the residual process. So, because in distribution, these two things become independent in the limit, we can actually just focus on the first thing that we're actually interested in. On the first thing that we're actually interested in. So the normalised process at the other locations, given that we've got exceedances of the threshold at the college onset. So this is all great in terms of the theory, but for modelling, we need to do a few things to make this more useful. And the first is that the assumption was for the threshold tending towards infinity. So in practice, we're going to have to take a finite threshold. A finite threshold, but finite but high threshold, and condition on the process being above that threshold. So instead of taking u to infinity, we take x at s naught equal to x, where x is above the chosen threshold u. And then we can undo the normalization that we had on the previous slide so that we are now modelling the process X conditioning on. Process X conditioning on these exceedances at the conditioning site. And these are going to be approximately equal to our first normalizing function A plus the second normalizing function B times our residual process. So if we want to use this for modelling, we need to make decisions about the three things on the right-hand side. So the A function, the B function, and the residual process. So there are So, there are a few constraints on these because of the conditioning construction. The first is that if we think about the value of the process at the conditioning site, we want to be able to recover the value that we were conditioning on. So, to do this, you need to condition, oh, sorry, you need the constraint, that A is equal to just X at distance to zero, and also that the residual process is equal to zero at the zero. Process is equal to zero at the conditioning site. The other thing is that if we have in environmental settings, as we said, the dependence is expected to decay with distance. And because this A function is going to capture a lot of that structure, we'd also expect that to be monotonically decreasing with distance as well. So the way that Jenny and John did this was to impose parametric rules. Impose parametric functions for A and B. And these were motivated by looking at theoretical results based on lots of known spatial models, spatial distributions. So these are motivated by theory. So I'll take each of the three things in turn, so the A function, the B function, and Z naught, and explain what they did and why. And y. So for the a function, what they did was to take it as equal to x times the function alpha, which depends on distance. And they allow this to be equal to one up to distances delta from the cognitioning site. And then it decays exponentially with these two parameters, lambda and kappa, being involved, beyond that distance. So what this allows is for atom to Allows is for asymptotic dependence up to distances delta from the conditioning site and then asymptotic independence beyond that. So, for what I'm going to talk about now, I'm going to assume that delta is equal to zero, so we're just in the asymptotic independent setting. And actually, that turned out to be the most useful in the application that we were looking at. So, in terms of the second normalizing function. The second normalizing function, there are kind of two options that Jenny and John proposed for when delta is equal to zero. The first is to take this function as x to the power beta. And if you remember what Simon had in terms of the bivariate Heffin and Tarn model, this is the same function that they had in that case. The other option is to take B as one plus your A function to the power beta. To the power beta. These two options just have different behaviour in terms of the features that they can capture. But it's just a modelling choice. Okay, and then in terms of the residual process, as I mentioned, we need to be able to recover zero at the conditioning site. So in the bivariate Heffalin and TON case, as Simon mentioned this morning, it's usual to take the It's usual to take the assumption of a normal distribution for the residual part. So, a natural extension of that is to take a Gaussian process for the residual part in the spatial setting. But we can't just take any Gaussian process because then we don't have the constraint that is zero at the conditioning site. So, instead, Jenny and John had two options that they looked at. The first was to just start with the Gaussian process Z and then subtract the value. And then subtract the value of that process at the conditioning site from everything. And the other would be to construct a conditional Gaussian process where you condition on the process B0 at the condition site. Okay, so as I said earlier, in Jenny and John's paper, they were looking at likelihood-based inference. So this is doable because the Is doable because the likelihood can be written down and maximized using standard numerical maximization routines in R. And the model itself is obviously constructed just based on a single cognitioning location, but you can actually construct a composite likelihood instead, where you take contributions from like multiple cognitioning sites and multiply their likelihoods together and maximize a composite likelihood instead. A composite likelihood instead. So that kind of combines information across different cognitioning sites. So this is great and works in dimensions up to hundreds, but it does have restrictions in terms of dimensionality, which come in because of having tangled large covariance matrices in the residual process. So, as I said in their paper, one of the things Her paper, Wadsworth and Torn, looked at hundreds of locations, and Jenny and I actually also extended this model to space and time, also looking at the Red Sea data. And we were able to do about 270 space-time locations in a reasonable amount of time on my laptop. So that was kind of the restrictions that we got up to. So now our aim is to kind of use the power of inward. Kind of use the power of INWER, which we've heard about this week already, and try to substantially increase the number of spatial locations that we're able to handle. Okay, so I've mentioned already that we'll be looking at the Red Sea data, so I'll just talk a little bit about why this is important or interesting to look at. So monitoring sea surface temperatures is useful for assessing the health of marine habitats. Health of marine habitats. But it does depend on the location. So, if you think about coral reefs, for example, they are able to withstand some changes in temperature, some extreme events, because they're kind of adapted to their specific location. But if the extreme sea surface temperature events are particularly big or persist for a long period of time, it can lead to issues like coral bleaching and potentially coral marsh. And potentially coral mortality. So it's interesting to look at the spatial extremes question in terms of how big an area might we expect to be affected by these extreme events. And then in the space-time paper, we were also able to look at how long these events are likely to persist as well. So this is the Red Sea, and the And when we were looking at the space-time case, one issue that we came across is that there's quite different behaviour in the north versus the south of the Red Sea. And the conditional exchange framework is assuming, in the version that I've shown you, assuming stationarity. So rather than trying to handle that spatial non-stationarity, we're just going to focus on this region in the south where we're able to assume approximately stationarity. Issue approximately stationarity. So, this region does have over 6,000 locations. So, if we're able to do inference in this region, that's already a massive increase on what we've been able to do in the past. So, the data we're looking at, we're going to transform marginally to Laplace scale so that we're satisfying this exponential tail criteria, and then we're just going to focus on data from. We're just going to focus on data from July to September because those are the hottest months in the Red Sea. Okay, so if we're going to try to handle these large dimension data sets, large in spatial extremes contexts, then we need a latent variable approach. So I'm just going to talk briefly about that in a general setting. General setting. So, what we have in the latent variable approach is our observed data at D spatial locations. I'm calling these V1 up to Vd. And we need to consider a later Gaussian process W of dimension M, which is less than D. And so, this is what is going to allow us to be able to do things in higher dimensions. And so, there's an assumption of conditional independence. Of conditional independence of the V's with respect to the W's. But you can make a link between the V and the W to recover the V's from the W's. And the way that this is done is using an observation matrix A. And you can also have additional parameters in the model denoted by theta. So, in terms of constructing the observation matrix, we've already seen lots of plots like this already, but this is Like this already, but this is a triangulation mesh that we computed using the R inlet package over our spatial domain. So we've constructed a mesh with 541 nodes. That would be our M. Our D is over 6,000. So the way that the observation matrix is constructed is that there's a row corresponding to each of the original locations and at most three And at most, three of the values in each row would be non-zero, with weights telling you how close you are to the nodes in the triangle that you like in the mesh. So, if we're going to use this framework, we'll be taking a Bayesian hierarchical modelling approach where we've got some hyperparameters theta and we've got our latent Gaussian process W conditioned. Gaussian process W conditional on theta. So this is a multivariate normal of dimension n with precision matrix Q. And then the overall likelihood of the V's can be calculated conditioning on W and theta as being independent. The other thing that we need if we're taking this latent variable approach is an additive noise term and that's necessary because we're in this latent. We're in this blatant variable framework. So this introduces this extra sigma squared, just like a nugget or extra nice term. Okay, so this slide is almost identical to the slide that Janine showed earlier in the week, showing how not to explain what INLO is doing, but hopefully I can get away with it here. So if we're in this latent Gaussian framework, Latent Gaussian framework, that's what we can use in LA. And the stochastic partial differential equation approach, which we've heard about this morning and earlier, is integrated in our INLA to allow for fast and efficient approximations if you've got a return covariance function. So rather than talk about the details of the SPDE. Of the SPD, which there are many people here who know more about than me. I'm just going to talk about how we link this framework to the conditional extremes framework. And in particular, remember that this is the model that we're trying to actually fit. So, conditioning on large values of the process at the conditioning site, we need to have an A function plus. An A function plus the B function times the residual process. So at the time of working on this, there were certain restrictions in terms of what INLA could actually do in terms of these functions. I think some of these may have changed with the introduction of Inlet group, but I will just talk about what we actually did. Okay, so again, I'll take each of these three things in turn and just Each of these three things in turn and just explain what we can do. So, the first thing that we did is to think about the A function, and it's actually really nice within INLA that you can quite easily use splines for these for the mean rather than imposing parametric forms. So, we could have taken a parametric approach and estimated those parameters through the hyperparameter vector, but I think the flexibility. But I think the flexibility that's introduced by using a spline is quite nice. So, what we do is to set the A function again as x times alpha, depending on distance. And we can use a spline with the constraint that it's equal to the one at the conditioning site. And then to increase the flexibility, we also looked at adding a second spine function, gamma, which is not going to be multiplied by x and is constrained to be. And it is constrained to be zero at distances of zero. So we motivated this by thinking about brown resnik type processes, but it also just adds a bit of extra flexibility into the model anyway. In terms of the V function, I think this is where things might have changed within Libre, but we were able to use the X to the beta option. The x to the beta option with beta estimated through the hyperparameter vector, but not able to do more complicated things like the 1 plus a to the beta option. One thing to note is that sometimes it might be sufficient to just have beta equal to zero, and we'll see in a moment that that can speed up the computation. So, that might be an assumption that you want to add in as well. And then finally, And then finally, for the residual process, obviously this is going to have a Gaussian structure, but we need to have this constraint that is zero at the conditioning site, and we can do that by manipulating the observation matrix A. Okay, so the last kind of tweak that we need to make to the conditional extremes model is to remember that we're using a latent Gaussian. Latent Gaussian process framework, which means that we have to have this additive noise term. So, this is what the overall model would look like with our two spine functions, the x to the beta for the b function, and then just some additive noise at the end. Have missed off some i subscripts, I don't gain that, sorry. So, in practice, you could fix the sigma squared to be small, but we actually just Small, but we actually just estimate that again through the high parameter vector. And in the application that we're looking at, it came out as estimated very small. Okay, in terms of then actually using the model, this is kind of the general form, but actually sometimes more simple versions of this might be useful. So for example, we could just set the alpha function to one. Set the alpha function to one or the gamma function to zero or beta to zero and come up with simpler models. So that's what we actually do. And then we compare these models based on some model fitting criteria. So the first model would be just to take X plus the residual process. So this will be completely asymptotically dependent, not very flexible. The second one. Flexible. The second would be to allow the alpha spline to be in there as well. I won't talk through all of these, maybe, but yeah, so there's just different versions of the model with different parts of the model form involved. Okay, so we fit each of these models to the Red C data. We picked a conditioning site near the centre of the domain and then compared all of these using the W. Compared all of these using the WAIC. So, another really nice thing about our Inlet is that there are various information criteria automatically available to you if you use it. One of those is the WAIC, which is what we can use now. So, once we've fitted all these models, we can then compare them. So, in this column, I've just subtracted the best WASC, which was for model four. Which was for model four, and I'm also showing the runtime, which is in minutes. So you can see that we're nowhere near as fast as Andrew's approach with the less than a second. Yeah. So when you compare the models, you can see that there's really not much in it between model three and model four in terms of the WAIC. But the runtime for model four is actually over three times, it's not over three times. Times it's like over three times slower than model three, so maybe we prefer model three and for its efficiency in terms of computation. One thing to note is that for model four, the estimated value of beta was around 0.29, and that's the 95% posterior critical interval. So it is away from zero, but it still looks like setting it equal to zero is doing something reasonable, anyway. Anyway, so then what we can do is to just have a look at what the output of the model is kind of actually showing us. So, in particular, we can have a look at the spline functions, so the alpha and the gamma. So, these are for model one, two, and three. Model one just had is in blue, that just had the alpha function, but no gamma. Model two is in orange, has the gamma function, but no alpha, and model three has both. Model 3 has both. So, one feature that we can pick out is that we've kind of got this decaying behaviour in the spine, which we'd expect because of the decaying extremal dependence. That's particularly clear when we just have one of these functions involved. So, if we just look at model one, it's clearly decaying in alpha. Sorry, alpha is clearly decaying in distance. If we just look at model two, Distance, if we just look at model two, gamma is clearly decaying. For model three, we have this kind of jump and then it goes back towards zero. But actually, if you look at alpha times large values of x that we might be seeing in the data plus the gamma, then it was overall kind of decaying as well. So, I think I'm running out of time, so I might skip a few slides. Yeah, I'll just make a few questions. Yeah, I'll just make a few comments about what we've done. So, yeah, so using INLA in this or integrating the conditional extremes framework and the INLA framework has allowed us to do higher dimensional inference for spatial extremes. It's also allowed us to introduce more flexibility into the A function by using spines rather than parametric versions. Parametric versions. It's really nice that Inlet already has lots of convenient things within it, like information criteria, the CPO, things like that. And it's also impossible to do this in a space-type setting. I think what we've talked about, the spatial one. So one kind of issue that we have compared to the composite likelihood approach is that, obviously, in that approach, the idea is that you combine it from. The idea is that you combine information across lots of different conditioning sites, whereas here we were only able to take one conditioning site and do the inference for that one location. But if you have a look at Basilius's more recent work, then they've done some work to allow for this, so adjusting the posterior so that you're able to use multiple different conditioning sites within this framework. And then the final. And then the final comment is just that as there are developments in INLE, some of what we've done could be improved even more, such as having more flexible versions of the B function. So I expect things to be able to get better in the future. So we'll just finish with some references.