Everyone, thanks for being here again. And it's a pleasure to welcome Nina Oto to talk about a topological perspective on weather regimes. Thank you very much, Nina. Thank you very much for the introduction and thank you for inviting me to this workshop and also for organizing this. So I'm going to talk about So I'm going to talk about some work in progress with climate physicists at the University of Oxford. This is part of a collaboration that started about three years ago while I was still a PhD student in Oxford. And then so we released the first preprint this year. But it is, let's say, ongoing work. And so basically our motivation was to try to. Our motivation was to try to study some of the very large weather data sets that they studied in their work using techniques from topological data analysis. And so while, let's say, yes, working on this, at some point we realized that we could actually try to use these techniques to come up with a rigorous definition of weather regimes. And so this is the overview of what I'm going to talk about. So first I'm going to Of what I'm going to talk about. So, first, I'm going to give some motivation for why one would even want to try to define weather regimes. And then I'm going to give an overview, very high-level overview of the topological techniques used. And then ultimately, I'm going to talk about some of our results and how this can actually be useful. So, what you see here is circulation. are circulation so these are these are changes in I'm sorry I need to minimize the heads so what you see here here is our daily measurements of pressure at the sea level so this is this is a height of let's say more or less five kilometers let's say on the on the sea level and on the North Atlantic in 2017 and And so, the types of data sets that let's say meteorologists try to study consist of such measurements that span over. The idea is that one would want to be able to discern some pattern to simplify this data set. Because, I mean, as I hope it's clear already looking at this video, it's quite difficult to try to understand what is. Difficult to try to understand what is happening. The hope would be to be able to identify some recurring patterns of circulation and then maybe to also be able to have some sort of market chain model with a very limited number of different states, regimes, and so that one would be able to easily, let's say, model this system, predict possibly also how it will evolve in the future. How will it evolve in the future, and those account for most of its variability. So, the issue is that this problem is actually very hard. And so, this is lots of different systems that actually climate scientists use that then lead to different definitions of regional. To different definitions of regimes or patterns. And so, one of the most useful one or known one is what is called the North Atlantic Oscillation Dipole. And so, this gives a very, so these are two different patterns. One is, let's say it's the positive phase and the other is a negative phase. And this corresponds to differences in anomalies of pressure. Of pressure measured on two different regions. And whether the circulation is in the positive phase or in the negative phase. So, what is depicted here is what is called the negative phase, gives a very good approximation for winter weather in Europe and to a lesser extent also in Northern America. And so, to give a very concrete example, this was so the So the 2020 winter in the UK was characterized by lots of flooding and lots of storms. And so what happened from the point of view of this North Atlantic oscillation pattern is that the circulation stayed for a long time in the positive phase. So what this means is that there were higher than That there were higher than average differences in pressure between these two regions. And a better way maybe to understand this is to look at, let's say, from the point of view of the atmosphere, looking at winds or pressure are two signs of the same coin. And so if you look here at what the winds were doing in this period, so between December and January of 2000, so December 2019 and January 2020. And January 2020. So we had that the jet stream was pointing. Sorry, I can't see how to use my pointer, but so the jet stream was pointing directly to the UK, which basically meant that whenever there was a storm building up in the Atlantic, this would be picked up from the jet screen and sped out onto the UK, which then resulted in very strong storms and a lot of... Yes, very strong storms and lots of flooding in the UK. And so there is no rigorous definition of what weather regimes are. And there are, let's say, lots of different ways that researchers use to try to compute such regimes. For example, here in the case of In the case of this data set, in this circulation on the North Atlantic, one method that researchers use is they try to cluster the data points into different numbers of clusters, and depending on the methods used, they come up from everything between two and six different clusters, so regimes. And so, for example, here is an example. And so, for example, here is an example from in which there are four different regimes. There is the positive phase, the negative phase, so the two that I discussed, and then two other phases. Or then there are some more, let's say, ad hoc methods. For example, in this one on the bottom, in which they studied preferred latitudes at which the jet stream would Jet stream would let's say would travel. And so, here they found these three modes for the circulation. And so, in this case, here one can talk about jet regimes. And one problem with this, of course, is, as you can imagine, is there are many, many choices in pre-processing steps. So, even just with k-means, we are actually already deciding how many clusters we want to see. Many clusters we want to see. And so this result in different lots of different regimes that might be related to each other, but we can't actually compare. The methods with which we arrive to the different regimes, they are also not equivalent. Another problem is that we also want to be able to detect regimes in data sets in which we actually there is, let's say, consensus. Um, there is, let's say, consensus that there are regimes, and so these are non-linear dynamical systems that have been proposed over the years. So, there is the butterfly, so there is the Lorentz 63 dynamical system, which is one of the first models that has also, in some sense, convinced researchers that this idea of having distinct Idea of having distinct regimes might have some theoretical soundness. So, because here one can distinctly see the two regimes as being the two wings of the butterfly. And later on, Charney and Devore suggested this system in the middle, which is much more complex. But so, here we also have two regimes. So, we have one which is the We have one which is the blob in the middle, which consists of, let's say, very high density points. And then we have all the outward loops that start and come in and come back to the center, which are low density points. So this would be the two regimes. And finally, there is another model that was suggested by Lawrence in 1996, which we say something more about this. We say something more about this in a bit. And the problem is models and this real-world data set, then many of the proposed definitions for regime would actually not be able to account for what we would think of being regimes, whether regimes in these four different examples. In these four different examples. And I think that maybe the data set that exemplifies this best is the Charne-Devore, because in the Charne-Devore data set, as I said, we have this, let's say, high-density blob in the middle and then this low-density loops in the outside. And so this would not, let's say, these definitions that, let's say, many of these definitions would actually not be able to capture this complexity of this data set. And so. And so, well, so one approach or answer could be: well, weather regimes are like pornography. Well, we have all sorts of different examples. We don't have a very gross definition, but we recognize them when we see them. And so you are probably familiar with this very famous court case in the United States in the 60s. States in the 60s, in which Supreme Court Justice Potter Stewart said that he couldn't or didn't want to give a rigorous definition of hardcore pornography, but that he would recognize it when he saw it. And in that case, what was discussed was not a case of it. And of course, we could do the same, but well, I hope, at least in the context of this workshop, you would agree that it's not very That it's not very satisfying. And then, and of course, I mean, that was also partly what was criticized about this statement made by the justice is that it's a very subjective also assessment. And so if you go back and look at these data sets and stare for long enough at them, and we think at them, let's say, a bit more topological, we put on our topological glasses. Or topological glasses, then we could see that what is actually what they all have in common is that there are some sort of combination of high density regions. So we could think of them as being connected labels and some loops. And so these actually, these features we can capture with topology, by computing, let's say, homology. And so here what And so here I wrote modulo density because, well, this is something important to keep in mind. So it's always important, and so this we will also see more in detail, to threshold actually, we need to choose some density threshold because, for example, the Lorentz 63 model, if you were to, let's say, to consider the points at all densities, so if you were to let So if we were to let this system run to infinity, then those two, we would no longer see the two wings of the butterfly, we would no longer see the two loops. And so we are not interested in actually studying all the points, let's say dual density levels, but we are interested in thresholding them so to actually see some interesting features in phase space. And so we'll now say more about this in detail, but so very high level experience. But so very high-level explanation is that there is a method in topological data analysis called persistent topology that actually allows us to algorithmically find these blobs and loops, let's say the ones that are longer lived. And the nice thing about this is it amounts to reducing some, well huge, but yes, reducing some matrices. So it's just linear algebra computations. Just linear algebra computations. And we get rid of many choices that one would otherwise need to perform at a pre-processing step. So for example, yes, we do not need to actually give us input how many clusters we want to see, but also we will see from the point of view of at what scale one is looking at the data set or density thresholds, we do not need to perform those choices. So this is very good also for. So, this is very good also from this point of view. And so, this is a summary. And so, yes, we'll also come back to this at the end of the talk. So, of kind of output that we get from our methodology. So, here we have, we see actually in color, we see representatives of the regimes, so representatives of loops and also of components. And another thing that is good about this is that, let's say, up to a specific point in the, so let's say once we have computed all the distances between the points, the methodologies that we use, let's say the computational complexity of those algorithms does not depend on the embedding dimension of the points. And this is very important because I This is very important because actually, many of the other methods that are used in climate science heavily depend on whether the data set is in six dimension or four dimensions and so on. So, this is a very important feature of these methods. So, next, I want to give a bit more insight into what these methods actually are. And so, we have here an example of We have here an example of, let's say, a point cloud, and so we would want to be able to capture in an automatic way how many clusters or loops holes it has. And of course, here we see it by eye, right? We would say we have, well, yes, maybe here also it wouldn't be so clear, but we could say we have two clusters, right? One with points on the left, one on the right, and then we have the two holes which come from this letter eight on the left. And so, what we do to actually study the number of holes and components with persistent tomology is that we so we first on the top we see a filtration of spaces and the way to think about this is that we are looking at this point cloud from above different let's say resolution scales. Resolution scales. So here at zero, we are just considering all the points. And then let's say at 0.5, we would be joining all the points that are a distance at most 0.5 by an edge. And then we would be filling in triangles of triples of points that are a distance at most 0.5. And similarly, then for higher thresholds for distance. And so you see, we get a nested sequence of. Begetonested sequence of spaces. And then with persistent homology, we can study the lifetime of components, so of clusters and of loops. So here we have in blue what is called the barcode for the clusters. So we have that each blue line corresponds to the lifetime of one cluster. Time of one cluster. And you see, when we start at zero, we have as many intervals as we have points. And then as we increase the threshold, we will have the clusters merge. And at the end, we will have just one cluster left. And similarly for the loops, so in purple, we can also see that we have actually two intervals that are much, that they significantly longer than the other intervals. And so here we can actually. intervals. And so here we can actually, so you see this dashed this dashed line that cuts through these two barcodes and the filtration. So we can see that when we are at distance, let's say distance threshold more or less 1.4, we see that this dashed line cuts through two blue intervals and two purple ones. And so from this we can read off that if you're looking at our point cloud at Looking at our point cloud at the scale of, so we are thickening all the points by, let's say, 1.4. So, two points at distance 1.4 are considered to be, let's say, equivalent or the same. Then we have two clusters and two loops in this data set. So, and one thing that is very important also for many applications of persistent tomology is that, well, we are not only like here interested in actually. Only like here, interested in actually knowing how many clusters and loops or how long they live, but we want to then be able to go back into our data set and be able to say, well, these specific points are actually give us a representative for a specific cluster or loop. And so here is a, let's say, toy example. We would have two, so this in blue and the dashed one in red. These are two representatives. Representatives of the same loop, which is this hole in the middle. And of course, you can imagine that, well, in this small data set, it might not make such a huge difference, but then in the actual data sets we work with, it does make a difference whether we have, let's say, a loop that has a minimal length or a very windy one, long one that we can't actually even interpret from a point of view of the data that we are working with. We are working with. And so it is very important to have optimal representatives for loops. For components, it's easier in that sense. And there is a lot of research that is being done in this direction. The problem is that it might not be too surprising, that it's actually NP-hard to compute, optimal representatives. And the actually outputs, the colored loops that you saw in the pictures I showed before, these we computed with this software purse loop. It with this software, Perse Loop software, and this is the accompanying paper. And there are problems with this software. It's actually quite, let's say, unstable for some of the data sets that we're studying. So we are planning in future work to find better algorithms and software. And but so, of course, in practice, the data sets we study are not as nice as the one I showed you. And so they look more like something like this, where we have like something like this where we have let's say several regions of points with that have different that live in regions with very very different densities and the problem here is that if we just were to apply on this method with a distance based filtration we might not actually be able to to infer as in the previous example that here we would want to see that we have one interval uh for the barcodes for the for the loops that is significantly longer For the loops, that is significantly longer. And so, the idea here is that now we build a density distance by filtration. And so, we have two parameters now. So we have, so let's say going upwards, we are increasing the distance. And so, going to the right, we are considering points with lower and lower density. Density. So at the beginning, we are including the points in the higher density regions, and then the further we go to the right, the more points we are including also from lower density regions. And so then now again, here we would then want to study the lifetime of cluster components and loops now in this two-parameter space. And the problem is that there is no generalization of these barcodes for. Of these parcodes for multi-filtrations of spaces. Nevertheless, there has been a lot of research done on studying invariants that one can use for applications. And usually one says that one would want such invariants to be, that one can actually compute them, that they are stable in an appropriate sense, and that capture what it means for topological features to be persistent. And there have been several. And there have been several different approaches taken in the literature. It's beyond the scope of this talk to actually give an overview. So here I've only listed some approaches that are directly relevant to what we are doing in our work. And so Lesnick and Wright, they developed a theoretical framework to study bifiltrations of spaces by studying barcodes along one-dimensional, let's say, slices through these two parameters. Slices through these two parameter spaces. And so we'll just say something more in the next slides about this. And then during my PhD, together with my supervisors and Henry Schengt, we proposed invariants that capture topological features that live along only certain subspaces of the parameter space. So, for example, here in the two-parameter space, it could be, let's say, features that live only along certain, let's say, lines or Certain, let's say, lines, or but you can imagine if you have higher parameter spaces, you could have, let's say, in three parameter space, you could have features that live along lines or along planes and so on. And then, yes, and then this year, together with some collaborators, we actually developed a general framework to study numerical invariants for multi-filtrations of spaces and their stability. And in particular, we also studied the stability of this. The stability of these invariants that capture topological features that live along certain subspaces. So let's say the most naive way to study to apply this method by Lesnick and Wright, so to study restrictions to one-dimensional slices of the two-parameter space would be by taking Taking by studying here barcodes along restrictions to vertical lines. So, what this would mean would be that we are thresholding first our points by considering only points with a certain density, and then we are computing this distance-based filtration as explained in the beginning. And so, for example, here we would obtain that actually, and so here I'm depicting on the bottom the barcodes for the holes, so for the loop. For the holes, so for the loops. So, what we would see here is: well, the two, let's say, two restrictions on the left, we have, well, the barcodes are actually empty, and then you see the barcodes for the two on the right. And so you see that, well, we do, right, we can, in some sense, capture that we have some sort of fall, especially the one, let's say that the third one from the left, because there is this interval that is slightly longer. But again, in some sense, you can think, right, the problem. Can think, right? The problem we are coming back again to the problem that we actually wanted to solve with persistent tomology in the first place. So, not having to try to infer the best value for a parameter, because in some sense, persistent tomology is allowing us exactly to do that, to not have to infer the best value for a parameter, but actually just consider all the values and then see, let's say, what happens. And the problem also with looking at these vertical slices is actually this is not this. slice this is actually this is not this does not this is not a stable procedure because you can think that well if we were to just continuously probe our two parameter space with these vertical lines we can have features that suddenly appear or disappear and so this is actually something that Lesting and Wright discuss very rigorously in their work and so and so here we have this is a screenshot from the software that they developed which is based on this work Uh, which is based on this work that I talked about, and this is to my knowledge, this is still so they started this project around I think 2015, and this is still the only software that we have to compute persistent tomology for multi-filtrations of spaces. And so what they do, so what one can do in this software is one can probe the parameter space. So this is again, you actually see the output for the same toy example that we have here on the bottom. So we can probe this with lamps. So we can probe this with lines and if we choose lines of positive slope then actually we do get barcodes that vary in a continuous way. And one can then let's say study and yes, study let's say for which choices of slope one gets more interesting barcodes or longer lived features. And so yeah, so this is for the for the clusters the screenshot and this is for Screenshot, and this is for the holes. And so, yes, just wanted to say the last word about these other invariants that capture, let's say, topological features that live along certain sub-spaces of a certain dimension. One can actually also read this off from this, let's say, this interactive user interface with this software by Lesnik and Wright. And so, for example, here. Nek and write. And so, for example, here, so this is the screenshot for the would be for the clusters. We would see that we have, let's say, one component that lives, let's say, in the full parameter space, which would be this gray shaded area. And then we have some other components that only live in certain, let's say, subspaces of the parameter space. And so, to come back to our weather regimes. regimes so this is the so the methodology that we set up is well we first well normalize our data then we choose some density estimations of points and we then do build this density distance by filtration and Unfortunately, we were not able to use this Rivet software because of computational limitations of the software. So this is also something that we plan to do in future work. So for the moment, what we did is we limited ourselves to studying parcodes along these vertical one-dimensional slices, which means basically that we chose. So once we Chose. So once we constructed this, we chose this density estimation of points. We then computed distance-based filtrations for points from between 10% of the datance points up to 100% of the densest points, so all the points. And then for each of these barcodes, we consider the five longest-lived components and loops. And then, yes, ultimately, as a significance testing, we, well, yes, we started with the principle that points sampled from a Gaussian distribution shouldn't have any weather regimes, any regimes or interesting topology. And so we would say that, let's say, a data set, a dynamical system would have regimes if they had If they were corresponding to topological features with life spans greater than the ones found in Gaussian noise. And well, there are many little details that I'm not going into here. They are all actually in the preprint that we have on the archive. But so maybe just to say something about some choices we made, of course, one could say, well, it's an arbitrary choice to choose the five longest-lived topological features. Topological features. Well, we did it on one hand to have, let's say, plots that are actually visually interpreted. Looking at here, this was not a limitation. And actually, depending on some of the parameters, we were observing less than fewer than five longest-lived than five topological features. And also, there are many of the parameters or choices we've made here that can actually be easily. Choices we've made here that can actually be easily adapted to study different types of data sets. So we see this as actually being a very flexible methodology. And so yeah, so the results for these different data sets, so first we have the Waldega. You see there is not, there is no, so here I'm depicting the first longest lived Longest-lived cluster in pink, the second one in blue, and the third one in green. And so you see there is nothing actually, well, yes, interesting happening from that point of view. And then we have the Lorenz 63 system. And so you see we have the right wing of the butterfly is the longest lived loop, and then we have. Loop, and then we have the blue one on the left is the second longest one. And similarly for the Lawrence 696, so Lawrence actually when he did when he suggested this model, he identified two regimes. And this here corresponds to, so one of the regimes he identified corresponds to this green loop, the outside loop, and the other regime. loop and other regime to the to the pink uh to the pink points uh which are the which is basically a low density region in the in the middle of this and so here we have the charny divorce system and so here actually especially in this uh we see there are also some limitations with this uh software set that we are using to plot the the loops um So, because this is actually, depending on some of the choices, it's highly, let's say, unstable, the type of representative that it gives. Part of the problem with this software is actually it was designed for 3D graphics, so actually for data sets that are intrinsically in three-dimensional space. And these are much higher-dimensional spaces. The data sets that we are studying in here to plot them, we are then projecting onto the three principal components. Components, but that's partly why we think that this actually is unstable and fails. And so, this is part of the future work that we're planning to pursue. And then finally, we have the main motivation actually for our work. So, this real-world data set of these measurements of pressure at the sea level on the North Atlantic. And so, here we see that we have this, if you consider only the Nettland. Have this. If you consider only the 10 density points, we see two distinct clusters, but then they disappear as soon as we include more density points. And so yeah, so I want to say something more about this is actually in some sense the fact that we only see these two regimes can be seen as a limitation of our Be seen as a limitation of our approach because there is, especially in the type of data sets that we studied for the plots I showed you here, one would expect three different regimes because of these three modalities that I also showed at the beginning for this jet latitude. But one way to understand this, and also we think that that could be This and also we think that that could be better explained is by looking at is that basically here we're just seeing one perspective on this data set. And so if you remember here we are looking at we're taking these vertical one-dimensional slices in our bifiltration. And we hope that if we are able to use to actually also compute barcodes along slopes with posit along slices with positive slope, that we would actually be able to capture To capture, let's say, different numbers of regimes. And what we're seeing here with this method can also explain some of the curious inconsistencies in the literature. Because, as I actually said, it's depending on what methods researchers use, they come up from anything between two and six regimes, sometimes even more. And for example, in And it's, for example, in weather prediction models, it's usually so the two because the two regimes that we're capturing here are actually the ones that are the best suited for the prediction, to make predictions for weather. And so, in some sense, what it seems that the ones we're capturing here might also be more robust in a certain sense for weather prediction. And so, we're confident that actually what could be seen here is some sort of maybe weakness, the fact that we are actually only. Of maybe weakness, the fact that we are actually only seeing these two regimes and then a blob could be more than a bug, actually, a feature of our methodology. And we just need to be able to further exploit the methodology. And so, yes, as a summary, we have these different regimes that the expected ones in these models and well, and these two regime system in this Atlantic jet. This Atlantic jet. And as I said, future work would be to develop more efficient software ways to study one-dimensional slices with positivity slope in this spy filtration to develop more stable algorithms to compute representatives for loops. And also, yes, something I actually didn't say is I actually didn't say is what types of density estimations we used. So, for all this, so we used two different types: one is a kernel density estimation with the Gaussian kernel, and the other is a more crude adoc system in which we were just binning our space and then counting, let's say, the points in each bin. And this binning method was very important for the Charney divorce system because there you Because they're using this kernel density estimation, was not able to capture, as you could also expect, this very low density loops going outwards from this high density center. And so there, what I'm showing you here, the output on the left is actually by using this binning method. Whereas with the kernel density estimation for the other data sets, we use For the other data sets, we used both and they were both giving similar output, but we were getting smoother results with the kernel density estimation. And so, an interesting question would be whether other density estimations would be maybe better, could be better results for all these data sets and also others. And so, this is something else that we want to explore. And yes, and so this is just references to some of the papers that I mentioned during the talk. So, thank you very much, Ina. That was a wonderful presentation. Really enjoyed all of these relationships. I have a couple of questions, but maybe I should let the people in the audience, if they want to ask something first, they can go first. Does anyone have any questions for Nina? No? Okay, maybe I'll go first then. So, I mean, could you comment a bit more on the relationship between the complexity of Between the complexity of the state-of-the-art TDA algorithms for persistent technology and like point cloud dimension embedding. I think you mentioned this at some point, that this was in relation maybe to other algorithms that people use in environmental sciences. So yes, when I said this course of dimensionality, right? So basically, once we compute, so yes, here, right, when I said, yes. So basically, the computation of the The um so yes, maybe let me go back to the pipeline for this pipeline. What we need to do is so we have our data points and then we compute the pairwise, let's say, distances between them to construct this distance-based filtration. And that is, I think, there the computational complexity of that is linear in the number of, I think it's linear in the dimension, in the unbidden, computing the pairwise distances. So that's why I said once you compute that, once we have, let's say, our distance. That once we have, let's say, our distance matrices, the complexity no longer for this specific filtration that we are using here, which is the Vietery Strips filtration, does not depend on the embedding dimension of the space. For other types of filtrations, yes, it does. There is actually a survey that I wrote during my PhD, a roadmap for the computation of persistent homology, which does give a summary of the different, let's say, complexity of Different, let's say, complexity of different types of algorithms and filtration. So, I would suggest as a first, I have read it, and I remember that it is. So, there are several tables in which there are actually summaries for Vitris RIPS, Czech, Alpha, Complex, and others. And so, I would suggest to go look there as a first of course, I mean, I'm biased, but I just don't remember it by heart. That's why I was probably asking about it. So, yeah, so here I forgot to say this. We are using the V3 strips complex. Using the V3 strips complex. Thank you. And maybe the other question, because this thing about using densities is something that we tried because we ended up trying to analyze a data set that was way too large for the persistent homology to be computed. But we didn't get it to work, unfortunately, with that one. But I guess it was also because it was too large. So, with the KDEs, with the Gaussians, just I guess as a first attempt, or maybe just as a first question. Attempt, or maybe just as a first question, rather. Is there yet a systematic empirical study of the computational speed up of using these density filtrations? Or is this something that's like work in progress? Because I know Frédéric Chasall and their team have also been working on a similar approach, but maybe with different directions. So I don't know, maybe if you could explain that. So you're saying the difference to measure filtration that's what you're talking about? That's what you're talking about, right? By Frederick Fred, I don't think, as far as I know, there is no systematic. I think it also really depends on the as far as I know, there is no systematic, let's say, write-up or project, yes, on that, in that sense, yes. And yes, as I said, actually we are also planning to try to study other different types of density estimation. Different types of density estimations, and I think it also depends a bit on the data set, probably, right? The insights you have already, the things you already know about your data sets, which yes, would lead you to choosing some over others. So, maybe out of curiosity, how many points or what were you trying to do? Oh, yeah, so basically, this was we started with matrices of correlations, and we started with a network. We started with the network and then correlations between nodes. And then the size of that, I think it was something like 170 by 170. But then on top of that, we created a three-dimensional complex that was represented by spheres. And then we wanted to calculate the three-dimensional persistent homology of that. And that was too much because it's very fine and kind of slightly fractal. And I don't know, maybe not, not really fractal, but just like really fine-grained. Fine-grained. But also, the problem is if you do up to dimension three, then you need to build your complex up to dimension four, which is really large. Yeah, that's why we couldn't actually get to it, but that's why I wanted to, we started trying doing some, but even that was too expensive competition. That's why I was. So actually, I mean, what we also, what I didn't, I mean, touch upon, I said there are many details actually in the paper. We also use some sparsification techniques that are actually. Techniques that are actually. So, what we used is the Goody library, Goody implementation of the VIITREST context. And that actually has lots of different sparsification algorithms also implemented. So, I didn't say anything about that, but we actually also use those. That actually also sparsifies your complex. So, I would suggest, if you're not already, I would suggest looking into using that because there are a lot of things that can help you. Of course, then you need to make sure that it doesn't actually affect, but usually, this, I mean, this. Effect, but usually, this I mean, this comes with some even theoretical or heuristic guarantees, these parsification techniques. So, I would suggest looking into that. Sounds great. That's very good tips. But yes, in my experience is when the computations don't work, you need to think of way of simplifying your problem, right? Or tackling it from building other types of filtrations that are much sparser. I guess that was also a problem that we kind of just started using all of the filtrations. Kind of just started using all of the filtrations and some of them just wouldn't work because they're too expensive. Um, maybe Renata has a question. Go ahead, Renata. Yes, hello, hi, Nina. Thanks for the nice talk. So a question that just hopped to my mind now because you said that you use these sparsification techniques for the Vitoria strips. So I was just wondering if you could maybe comment a little bit why would you choose for that approach instead of, for example, using the alpha simplicial complex, what would you use? Simplicial complex, what are you doing? Well, I mean, the problem with alpha, so the thing is, I actually didn't say anything here about the dimension, the embedding dimension of the point clouds. We considered because, I mean, these are things that I'd say, there are lots of pre-processing steps and ways to reduce dimension of these data sets, which are actually very high-dimensional. And I think ultimately, the data sets we considered were between three between three and six dimensional. The problem with alpha complex is it does actually sets in R3, R2, but then no longer so for higher dimensional data sets. So the alpha complex would not be a good choice here. So in that sense the the Viet ReSchrips complex is agnostic of of the embedding dimension of your of your points. Of your points. Okay, thanks. But then, if let's say that you're working in two or three dimensions, do you have any advice on then what would be the better approach? So, sparsifying Tori strips or? Well, if you have true three dimensions, then the alpha complexes, if you really have true three dimensions, because the thing is here, we don't know how many dimensions we have. For us, it was important to compute the, to actually for the same data set, to compute, to basically pass it through this. basically pass it through this this our pipeline for for different uh dimensions so we because we don't know how many dimensions we have and that's the important thing is we actually don't need to make that choice and we actually were seeing that that what we were getting was not depending on the dimension so that was very important but yes no otherwise i would suggest using the alpha complex is of course it's definitely it's it's extremely sparse and things that are impossible or can only dream of with restrips complex they work With Risrips complex, they work very nicely with the alpha complex. Okay, awesome, thanks. Well, if no one else has any other questions for Nina, Ms. Jeremy, and thanking her again, at least virtually. Thank you very much, Nina.