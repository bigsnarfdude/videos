Was supposed to be here, but I'm kind of okay. Let me see, I just need to do got it. Okay. Okay, so it's a joint work with Salim and Ivan Durshma, who is also here. So it's and my former students, Swanan Kade, who is currently in IBM. Okay, so again, the focus of this. Again, the focus of this work is on kind of trying to think about and reason about locality, but in rank and subspace metrics. So it's basically kind of taking a regular notion of locality and trying to construct rank and subspace codes that will have this locality property. Okay, so I think it's no need to introduce locality, I will just do. Use locality. I will just do it anyway because just notation is slightly different. But traditional notion of locality in locally recoverable codes is to have a set of messages, right? So in this case, it's 11 messages, and then have global parities. In this case, it's four global parities. And then in addition to this, we have local parities, and local parities as a linear combination. Linear combinations of messages and also global parities. So, and local, just the notion of locality, it's allow us to fix some of the erasures and maybe errors locally, because those local codes here, they will be, for example, we can do them local MDS codes, for example, on M1, M2, and this PL1. And so they're like small errors because those local codes will not be having like large minimum distance. Having like large minimum distance. And when we're really in trouble, so we have like a lot of failures, then we can use global parity. So it's basically kind of trying to do trade-off in engineering systems. So local, small errors we can fix locally by accessing just local nodes. And global parities, we just need like it cost us a lot to access all those global nodes, but we can deal with a larger number of errors. Okay. So again, like here, for example. So, again, like here, for example, by adding two local parities, we can ensure that each of those local codes will have a humming distance of three. And, you know, we can use MDS with Humming distance of three, so we can correct two erasures, or we can correct one error. But of course, like if we need more, we will use global parities. And the idea of those locally recoverable codes is basically to try to find this code that can be partitioned into smaller codes. Partitioned into smaller codes, and also, like, in globally, it's also have sufficient distance. Again, it's, I'm just sorry for repeating this, it's just kind of one of one of this locally repairable codes. And there's like a huge sequence of works, kind of first generated by private practical needs. And people talked about regenerating codes in the beginning. So, in the regenerating codes, we need to. Engineering codes, we need to repair some of the failed disks. So, in the code as will be basically recover some lost symbols of the code through minimum bandwidth, right? So, basically, in practical, we don't want to download too many bits to recover like just one symbol, right? Of course, we can download like k bits, k symbols, and recover everything. But since we only need to recover one symbol, the idea is to minimize total. Idea is to minimize total bandwidth. And then people started to see that in practice and kind of for practical constraints, we don't want to bother everybody, right? So if you have something small, we can do it locally. Okay. So again, like this is notion of locality in traditional classical codes in which code word is a vector, and the notion of locality is that we have certain relations between symbols of this code. Of this code. And in this work, what we're trying to move into rank matrix code and which code words are matrices and notion of a distance is rank distance between matrices. And also move into subspaces. And this case, it will be column space of this matrix. We can think about this as a subspace. And I will talk about this later, but we will, for practical reasons, only talk about subspace of fixed dimension. Of fixed dimension, and again, we can think about the notion of locality in such codes and random matrix codes. For example, if you're taking this submatrix that corresponds to only certain columns, we want this kind of some relations or this code that obtained by just puncturing everything else and just looking on this local set of columns. We want this to have another distance. This is to have another distance which is sufficiently large to fix some local issues in this code. And since it's a run distance code, we can do some interesting stuff. For example, we can deal with low run errors, right? We cannot deal with traditional codes. And we can deal with crisscross type erasures and so forth. And the same in subspace distance, if you look on subspace defined by columns of this metric, so if you take just a subset of This metric. So, if it takes just a subset of the columns, we can still have a code obtained just kind of by projecting into the subspace of columns that also have sufficient subspace distance that also allow us to repair those vectors if they get lost or something like this. Okay. So, again, this is just a motivation why we're looking in a rank and subspace. First of all, is the ability to deal with mixed failures. So, for example, Failures. So, for example, if it's like SSD array on your computer or something like this, and you have one disk failed, it's kind of amenable to fix with those run distance calls. And also, mixed type of failures, right? So you have one disk failed, and you have also one element like an SSD3 failed. Of course, we can correct it with traditional codes, but it's much kind of more efficient to think about run distance code. Run distance code. And also, like in data centers, we have those types of crisscross errors that you have failed, like one column and one row. And again, like practical motivation could be like in DRAMs, that's kind of part of your computer that data is stored as a metrics. And sometimes you have like just the reader for each column and each row. And like, suppose one column and one row failed. And again, it's much more efficient to fix it with run distance codes. Okay, so again, another advantage of those run. Then, another advantage of those rank distance code is to deal with rank errors. So, for example, this error has very, very small rank, right? So, you basically have a matrix, and then you have some patterns that, like, for some reason in certain region of your DRAM, you know, you add once. And for like, if you consider in traditional code, we have like many, many symbols being corrupted. But if you look at rung distance, actually, the rank of this error is very small. So we can be able to correct it with run. So, we can be able to correct it with run errors, run distance code. Sorry. Yeah. And why we're looking into local recovery, because again, the same motivation for locally recoverable codes, because in practical settings, all data is stored in multiple hierarchical units, like racks, for example, in computer center. And if you have some small problem in one rank rack, we don't want to kind of download data from another rack, right? Or if your data Right, or if your data is stored in multiple data centers, we just kind of want to first fix the problem locally if something fails locally. And if you have catastrophic failures, then we might reach out to some other centers. Okay, so this is basically motivation. So again, this is just another example in which we have, like, for example, in here, it's three racks or three data centers. And then, for example, here, if you have like a ratio of example here if you have like a ratio of one um row uh this ratio is uh small that we can deal with this locally right so we can for example if local codes right so codes which just kind of consist on those three columns it have sufficient minimum rask run distance so then we can handle each erasure of a column or row locally right if you have like more than that like for example Have like more than that, like for example, here we have. Oh, sorry, a ratio. Yeah, so it's yeah, so it will be corrected locally. But if you have more than that, like for example, three columns failed, then we need to do more work and kind of bring data from another centers or race. Okay, so again, just generalization to subspace because we can deal even with more interesting set of scenarios. More interesting set of scenarios or errors. And it's motivated by work that started by Kotter and Shishank and Shashank and Silva in network coding that you can use subspace codes. They have many, many advantages. And one of them is you don't need to send coding coefficients because you receive a subspace. And then, like, for example, if you do all these recovery through some noisy network, you still be able to do it, right? So this is like kind of provided this. Like, kind of provided this noisy network doesn't introduce too many errors in terms of kind of the subspace we receive is like close to one of the subspaces that we want to send. So, again, it's just another example that power of those subspace codes. Okay, so again, like what we will do in this talk is first talk about rank locality. So, basically, it's just a generalization of traditional locality. Of traditional locality in a code to the rant locality. Then, of course, like the first natural step is to think about singleton-like bound that we have for locally recoverable codes and just trying to see what happens in MRD. And then, of course, we want to kind of construct those class, class of codes that match the singleton bound, that optimal in this space. And then later on, we will talk about subspace. Then later on, we will talk about subspace locality and generalize everything into subspace. Okay, and this will be done by lifting rank metric code. So, this basically transition from rank metrics to subspace will be relatively easy exercise using the same tools that Kotter and Sari Shishank and Silva did. Okay, any questions at this point? Okay, so if you can. Okay, so if you consider again rank matrix code, right? So it's basically we operate on matrices over some base field Q. So each code word is M times N matrix. And we have a distance and kind of a round matrix code. So distance is defined very, very kind of natural, right? So if you have two code words, we'll just run distance between those two code words. run distance between those two code words. So each code word is a matrix. So we'll just subtract one code word from another and say like what is the run of this matrix and this will be runk kind of this is a distance of the scores and can be verified that this actually distance that satisfies all properties that distance should satisfy. Okay and again like this work basically started with Gabidulin and some others kind of worked on this later. This later. So they basically have a lot of analogy with traditional MDS codes, and there is a singleton bound for those codes which is written here, right? So and kind of definitely rank matrix code, also MDS codes. And kind of just on complication on the singleton because it's a matrix. So it's have like minimum n by m, but again, as was shown, this is a kind of singleton bound. This is a kind of singleton bound for those codes. Okay, and construction that we extend for localities will be Gabidoulean codes. And just to remind you about those Gabidoulean codes, so we have this encoding polynomial that looks like very similar to encoding polynomial by for traditional Ritzalomon code, but degrees are instead of x to the i, it will be x to the q to the i. So using kind of Kind of those degrees of x and evaluation points will be set of n elements and f q to the m that are linearly independent over fq. So it's basically additional requirement because in traditional MDS codes, we just want those evaluation points to be different. Here we're looking on those evaluation points as elements in F Q to elements in f q to the m but if you can write them as a vector in fq and those vector needs to be linear vector needs to be linearly independent over smaller field and again like evaluation code is just by substituting those evaluation points and then we go from like from k to n by just evaluating this polynomial in n points so this is basically just a reminder about gabidulen code and Gabidoulin code and so again, this locality, what we want to do, so we still kind of larger code will be actually shortened Gabidoulin code because we want to preserve some global distance, right? So we don't want to, of course, like we cannot achieve because locality, we need to pay price for having those local redundancy and inside of those or local recovery ability to recover locally. But still, we want some global properties as well and global distance. And if we, okay, so this kind of taking this code and so basically locality properties will require that for each column, for every column in this world, there is a set of gamma called gamma i of small size. So the size of this is r plus delta minus one, such that if we're looking Such that if we're looking on this code just defined by those columns, they have sufficient local distance. So the local distance is greater than parameter delta. So we have two parameters and this rank locality R and delta. So delta is basically kind of requirement that run distance of this local code should be greater than delta. And r plus delta minus one will be the size of repair group. Be the size of repair group, right? So R can be thought about like, yeah, if you can see this code, if you're just talking about erasures, it can tolerate delta minus one erasures by contacting R nodes. So we can recover every column on this node. But in addition, this local distance gives us the ability to tolerate some crisscross errors and some kind of low rank errors as well. Errors as well, okay. So, any questions here? So, this is basically what we want to do. Yeah, so again, uh, R and delta will be parameters of this code, right? And the code will be of dimension k and each code word will be m times n run matrix codes. So, for those, like if it satisfies those property, we will call it code that has this r delta locality, okay. Delta locality. Okay. So for example, here, just an example. So we have 4, 3 rank locality. So it's R is equal to 4 and Delta is equal to 3. So we see that in this group, like all those repair groups will be of size 6, right? And we can tolerate two erasures and run distance should be at least three. So the idea is basically how to construct it and also like generalize single. Also, like generalized singleton bound. So, the first thing is minimum run distance bound. So, this is the kind of generalization of distance bound for LRC codes. So for code with R delta rank locality, the minimum distance is basically less than this. And this is basically just proof as. This is basically just proof is very very simple by just taking advantage of thinking about MRD quotes you can definitely map it to consider we can just consider this rank distance code as a block code of length n over FQ to the M. And then basically just generalizing traditional singleton bound for Hamming metrics, we can extend it to run distance metrics. And this is because run distance metrics is actually more powerful. Distance metric is actually more powerful than traditional Humming metrics or whatever. It cannot be larger than as value specified here. Okay, so this is the first step. And again, like this step was relatively easy, just extending the singleton bound for the codes in rank metrics. Okay, the second step is to construct those locally recoverable codes. Again, this is intuition from Tamo Bark, and we definitely use We definitely use this construction, we just extend it to run distance. So we have this global polynomial, which is in blue, right? So we have this high-degree polynomial of degree k minus one, and we evaluate it in endpoints, and we will get this global code. However, we want some locality, so for those some of the kind of in-repair group, if we evaluate on those groups, we need to get like a low-degree polynomial that will allow us to. That will allow us to repair those local issues. So instead of traditional polynomials for unclaquality, we will use linearized polynomials that are used in Gabidoulin construction. Okay, so how to construct this, right? Again, it's like a little bit of a mass, but kind of a lot of like technical kind of notation here, but if you just miss it, we'll have like an example, really, really simple. Like an example, really, really simple examples that kind of just makes much more clear. But again, like we assume some divisibility properties, for example, that you know, R divides K and so forth. And again, for encoding polynomial, we use this kind of shortened Gabidoulean polynomial because we can see that some of the degrees here will be missing. And then the whole idea is basically kind of how to. Is basically kind of how to construct this polynomial and also like how to construct evaluating points, evaluation points that will form repair groups that in each of those sets we will get a Gabidoulean code. We can get a local Gabidoule code if we're evaluating. Okay, so the main idea is to first kind of divide the set which you call alphas. So it's an alphas, the set of from alpha one to alpha r. Of from alpha one to alpha r plus delta minus one, where it is the basis of q to r plus delta minus one as a vector space over q. So it's basically those will be independent vector in this smaller space. Then the second step is to look on field FQ to the n as a vector space over the same space that we used here. Okay, this is basically F to Q R plus delta minus one. R plus delta minus one will be kind of in the intermediate vector space. Sorry. Yeah, so this will be an intermediate vector space. And then once we constructed those two sets, then we set kind of constructing a set of evaluation points where each set will be taking all those alphas and multiplying by element. By elements in this set betas, right? So for example, if you start with beta one is equal to one, so the first P1 will be just set of alphas, the second will be set of alphas multiplied by beta two, then we'll have set of alpha multiplied by beta three and so forth. And then creating the code is again the same, taking this polynomial and evaluating those in those points. Okay, so let me just try to do it by example. It will be much simpler to show. Example: It will be much simpler to show. So, this is a special case when dimension k is equal to four, and we have it's a nine-four code, so uh, length of the code will be nine, r and delta both equal to two, and we'll just for simplicity will see like base field will be binary and all matrices will be square matrices, so m is equal to m. So, suppose delta will be primitive element of f to two to the nine. Of f to two to the nine. Okay, so then if you substitute all those parameters, this will be an encoding polynomial for this code. So we'll see that it's almost like Gabidoulean polynomial, but we're missing x to 2 to the 2, right? This is basically the price that we're paying because we kind of shortened this code a little bit, right? So then, okay, so set of alphas again, it will be here, it will be one omega to 73. will be one omega to 73 and omega to 146 and this is basically one of the basis of f over two to the three over um f2 right so just a binary field okay so those like if you're writing them as a vectors um in kind of of lens three right those three vectors will be independent and f8 uh when considered f2 over uh consider it over f2. Okay, then we'll take this big field like two to the nine and compute a basic of this two to the nine when we look on it over two to the three, right? And those will be kind of three vectors, right? So again, vectors of length three because we're writing it at over f8 and this will be basis of those. So now we can construct set of evaluations. We can construct a set of evaluation points. So, for example, P1 will be one times this. So, it will be exactly a set of alphas, right? This will be the first repair group that we find kind of have. This will be the second repair group, and this repair group is basically just multiplying a beta two by each of the elements here. So it will be the second one, and this will be the third one. So, then we get like those nine evaluation points, and this is basically construction. And this is a basically construction of the scout, right? So, again, it's just the idea is to find a suitable encoding polynomial and then a set of evaluation points. So, now what we need to do, we need to prove that this construction satisfies this singleton bound. And to do this, this is relatively natural. Natural to see because kind of to show this, we need to show that those all evaluation points, all nine evaluation points that we constructed here, will be linearly independent if you're looking on them as a vector over F2. And again, this proof is just by construction. So we can say, like, you know, all those points are linearly independent because by way of contradiction, we assume some. And we assume some linear combination of them will be dependent, it will violate independence of those and independence of those, right? So, this is just a sketch of the proof here. Okay, and then if you kind of prove that all of those points will be linearly independent on over F2, we basically got a Gabidoule encode, but just some degrees missing. So, it's basically will be shortened Gabidoulian code, and then by just properties of And then, by just properties of Gabidoulin code, we will see that it will have this distance. And again, we have this penalty due to R and delta. If you substitute R is equal to 1, then we all kind of reduce this penalty, right? So it's basically just how much we pay for adding locality. Okay, so this is basically a subcode of 9.5 Gabidoulen code, and we have a distance of this. Have a distance of this is equal to five. We can achieve this distance with larger k, but we have to pay this price because of locality. Any other questions here? Okay, so then we need to show that this code will have R delta run locality. So basically, each subcode of this code will have sufficient distance. And for this purpose, we can define this report. Purpose: We can define this repair polynomial again, it's like a little bit of calculations, but um, it's easy to see that kind of um, it's proven the paper, but this repair polynomial will coincide with the generator polynomial in those points and PG, right? So, for each of, so basically it's just a proof that, um, yeah, so those two polynomials, one of So, those two polynomials, one of high degree and one of low degree, they basically coincide on those points. Again, maybe like the better way to just show it by example. So, again, if we come back to the same example that we had before, right, this was our encoding polynomial of high degree. And then if you do repair polynomial to R1, so if you substitute omega. substitute omega 73 in there. So what we get, we get a repair polynomial R1. And this polynomial again will coincide with, this is low degree polynomial of only we have x and x square. So it's only degree 2, but it will coincide with this polynomial of degree x to the 16. Okay. And this is because just from a proper And this is because just from a property that because those are basis of two to the eight. So of course, if I'm raising them to degree two to the seven, it will be one. So it's basically x to the two to the three will be equal to x, right? And then we'll see that those are low degree polynomial over not the original messages, but some linear combination of the messages. So the same for the second repair group. So if you substitute. Second repair group. So, if you substitute this into a repair polynomial, again, we see that this, sorry, if we substitute this into generator polynomial, we get R2. Again, R2 is a low-degree polynomial, but on a linear combination of messages, and the same for R3. So then, since those, we can think about each of those local codes as a Gabidoulian code on a different set of messages. Set of messages, right? But it doesn't matter because we can always repair some of the columns in this matrix. So this is basically the intuition. Any questions here? Okay. Okay, so again, like what we said, the local code is basically just evaluating those linearized polynomial in those three points, and this will be linearly dependent. This will be linearly dependent over F2. So, and then, since each local code is also a Gabidoulean code of lens 3 and dimension 2, we also get local localities that will allow us to deal with local issues. Okay, so this is pretty much all on rank metrics code. So, if any questions here, please ask. I know this is quite a group. No, I'm standing between you and Lunch. No, I'm standing between you and lunch. So, okay. So, okay, so then just kind of last part of this talk is to try to extend it into subspace codes. Again, we can kind of say like, you know, consider like set of all subspaces of F QM and then consider Grant's Mannian just fixed dimension, right? So for example, N. Fixed dimension, right? So, for example, n-dimensional subspaces of FQ to the M and those subspace code each code word is basically will be a subspace. And the distance metric or subspace metric here is as written here. So, the dimension of some of those two subspaces, or union, or kind of if you take product, yeah, so it's basically. product yeah so it's basically kind of subspace spanned by all vectors that span those two subspaces and then minus intersection and and we have like if it's a code so of course we interested in minimum distance of this code and this will be just minimum distance of any two subspaces formed by this code okay so again generalizing it to subspace locality is Is very, very kind of similar to what we did in rank metrics. So, again, like each subspace code is formed by some vectors. So, we can have like for each vector, we can find some group of subset of vectors of size less than r plus delta minus one such that dimension is equal to this gamma, right? So, dimension of entire subspace. Now we dealing again with lower. Dealing again with lower, smaller codes. So it will be dimension will be limited to this gamma, which is less than r plus delta minus one. And we also want that a local code with sufficient minimum distance, or the minimum distance of this local code needs to be greater than delta. Okay, and yeah, so okay. So the construction is basically It's basically trying to get like rank matrix code that we constructed before and just lift it, right? So because we're taking like, for example, one code word in rank matrix code, and then we append it by identity matrix and then consider this subspace that corresponds to x, which will be lambda x, which is a column space of this code. And it's basically the same what's Basically the same as what's used by Silva Quaternion Shishang. And this way, from rank matrix code, we can immediately obtain subspace code. And the properties of the subspace code, that the distance of the subspace code using the metrics in subspaces that I defined before, it's equal to twice the minimum distance in the rank metrics. Okay, so now again. Okay, so now again, if we had so that our construction is taking a rank matrix codes with R delta locality and then design subspace code by just lifting this. Okay, so we will get twice distance because the distance is defined on subspaces and also locality we'll get of parameter R and two delta. So this is basically just the same lifting procedure. The same lifting procedure. Okay, so just the conclusion again, what we did in this work, we talked about run clocality. So again, idea was to construct local codes that have a good run distance. And we computed kind of singleton-like bound on those codes and also constructed optimal solution. And then we extended it to subspace locality. Locality. Okay, so I think there is a lot of future direction. For example, singleton-like bound for subspace locality. I'm not sure if it's known. So also construct rank distance code that every column as well as every row associated with local code and also like trying to see how to apply. Trying to see like how to apply those subspace locality over networks with errors. But there is like definitely more kind of direction that can be done from here. So it's pretty much all. So if you have any questions, let me know. Thanks, Selene. Thank you. Are there questions? So we hear you. So we hear you what, so if people in the room want to ask questions or Alex, can I ask you to go to the definition of locality for rank metric codes? Oh, yeah, sure. You want me to go to the slide? Okay, go ahead. Yes, just now, like I was just looking. Yes, here. Okay, so. Okay, so you're not only restricting the distance, but also the locality, the columns. Can you do also partial, like maybe creating, I don't know, boxes in the matrices that you can work on instead of taking the whole columns and the whole columns, but also selecting the rows, maybe. I see. So it's kind of a weird pattern, right? So because right now we just consider like you take this matrix, you just slice. You take this matrix, you just slice it and kind of set of columns, but you want some kind of a weird shape, right? This is like, yeah, we didn't think about this. It's kind of maybe Ivan Salim can, it's kind of maybe interesting, but yeah, because it's kind of a metric, so we looked on it from the perspective of kind of regularity instead of columns. But the traditional For the the traditional LRC code, like a latrix format, there's anything known there. I don't know traditional code is like more like a vector. So I don't know if people hear my question. Yeah, yeah. Actually, yeah, sure. I'm not sure if you stand clear to me. I will just try to. Yeah, sorry, I'm on. Yeah, go ahead. Oh, okay. Alex, let me repeat. Alex, let me repeat the question here from the room. If your question, Felicia, makes sense also for other LRC codes that use a matrix format. Yes. And is anything known in other cases of LRC codes? No, for me. No, no. So it looks like a big general question, but yeah. Yes, yes, indeed. Yeah. Yeah, maybe something to think about this. About this, yeah, uh, kind of following days, okay. Thank you, thank you, Felicia. I'll think about this, yeah. There is some other, yeah, we also here like we just divide them into disjoint sets. I'm not sure, like, if any advantage of it's probably kind of optimal, but you can also think about kind of have some kind of weird structure and also like non-linear, right? So, because here. Because here, yeah, I'm not sure what is it kind of. It's a lot of different types of things. It's very interesting. Thank you. Thank you. Any other questions? Audience? There's another question online by Chat Chem who raised this hand, I think. Good afternoon. Good afternoon. Hey, but you give a definition of You give a definition of local code in harmony, and you give a construction of local code in hard metrics, but you don't say anything about a decoding algorithm. How can you decode local code in a hard metric? So, I think, like, you know, since whatever we constructed is all Gabby Doulin, right? So, it's global code, it's shortened Gabby Doulin, and a lot. Shorten Gabidoulin and local code is also Gabidoulin, so you can just use whatever decoding you have for Gabidoulin codes. But yeah, so in this case, we just use whatever techniques are already available. But we didn't try to optimize like complexity of decoding. So it also might be some other, like do it in a more efficient way that, because Gabidoulin is might be not so super efficient. Does it answer your question? Does it answer your question? If you're going to decode local code in hand, you decode your algorithm or gambling code. Yeah, so yeah, so kind of as I said here, for example, here, right, you see last bullet. So it's basically local code. CI is a Gabbadoulian code of lens 3 and dimension 2. So it's smaller Gabbadoulian code. But But yeah, so you can just use a Gabitoulin construction. Okay. Okay, okay. Yeah, again, like this is just one proposed construction. So we don't claim it's optimal with respect to decoding. So it might be interesting to see if we can use some other codes as underlying building blocks. So, I have a question too. So, this may be a little far-fetched, but I was wondering, so if your construction is, let's say, based on the Tamil bar, but just replacing the usual polynomial with the corresponding linearized polynomial, and then picking as the evaluation point, the basis of the corresponding. Point the basis of the corresponding vector space of the various species of the linearized polynomial that you're using for the local codes. Would you be able in similarly, maybe one can hope similarly to... So my question, I guess, is, is this the case? And if so, do you think that maybe one could use some other contractions like those that Giacomo presented to construct more Um, construct more variety of codes with different parameters, yeah, yeah, it's actually what we discussed with Ivan here. Like, and yeah, because here, like, it's a lot of intuition from Tamil Bark, right? So it's basically like they have a cosets and we kind of doing very, very similarly. They just had like independent, yeah, we have over vectors, but yeah, it's possible to have like any other construction of LRCs to generalize and. LRCs to generalize them to rank metrics. Maybe Ivan, you want to comment? Okay. Okay. Yeah, it might be really nice idea and kind of pretty natural also. Yeah, yeah. I mean, and maybe there's things to prove, right? Because there's not really an equality between the Hague distance and the Hamming distance. So there's an inequality only. So you're not, it, I mean, I haven't thought about it one second just while you were speaking, but I would imagine that there's things that need to be. That there's things that need to be proved that is not just automatic, but that would be a family of codes maybe to try. Yeah, yeah, it's probably like leveraging those as a machinery and then figuring out like, you know, all those details. Yeah. Maybe students here can write thesis. So if anyone has So if anyone has any other questions, there's still a little time. So if not, we thank Alex again and also and also all the speakers of today.