I'm going to talk about exploring the real parameter space and highlighting some challenges that come up. So things that we've done throughout the years in various papers to start answering this question or this journey and then kind of highlight some challenges, which is maybe a weird direction, right? Highlight problems with the strategies that we've used. But hopefully that gains some insight. That gains some insight into how we would move forward with further answering the problem. So, first, I'll give an introduction about what a parametrized polynomial system is and some examples of where we see them and ones that I've used and dealt with in my research very often. And then the talk will take kind of two flavors. So, one is finding real solutions, which will come in three parts. And then the second part. And then the second part will be about analyzing the behavior of the relationship between these real solutions. And then I'll summarize and kind of highlight some questions that'll pop up throughout the talk as well. So first, as many of you know, right, and many problems in science and engineering, we can write down the structure of polynomial systems, right, that describe some behavior or something that we want to solve. So some people That we want to solve. So, some people describe such polynomial systems in their pitch talks last night, and then we've seen them throughout the week so far. But parametrized polynomial systems are a little bit different in that they're dependent on a set of, well, parameters, right? And so those parameter values change the number of solutions and then would also change potentially the number of real solutions. And so, some examples of these are. Goals of these are, well, the five-point problem in computer vision, which we saw in a talk yesterday, right? So we have our, oops, I need something. That's not what I want to do. So we have this polynomial system here, and we want to highlight that our Yi and our Xi are these parameters that depend on the two cameras, and then that will. And then that will either change, you know, the solution E, if we're talking about two different cameras, two different images, right? And will change the solution to this polynomial system. Okay, we also have the three RPR mechanism in robotics, where now here the parameters are leg lengths of the robot, and the leg lengths will grow and shrink, and so these L1, L2, and L3. So these L1, L2, and L3 will grow and shrink, and then that will cause a motion of this mechanism. And dependent on the leg lengths will depend how this is able to move and different structural components there, where these are various rotational joints that allow the platform to have a different orientation, where the platform is, that pink triangle is fixed. And this isn't so bad, right? We can fit it on one slide. We can fit it on one slide still. But polynomial systems don't always look that nice, right? So, and then the final example that I'll mention for now is this chemical reaction network. So, minimally bi-stable ERK sub-network. And so, we have this chemical reaction network where our parameters are these reaction rates between our various concentrations. Various concentrations of chemicals, and we can set up a polynomial system where we set our variables to be the various different chemical components that we want to look at. It's their concentration value is that variable value. And then this reaction network is dependent on a set of ODEs, but we're solving for the steady-state solutions. We set these ODEs equal to zero, which makes it a nice polynomial system, and these are pretty nice. And these are pretty nice equations, quadratic at most. And so analyzing this problem is a parametrized polynomial system as well. So they can come up in a lot of different ways, but analyzing how the parameters change the behavior of these systems and how many real solutions we have or whether or not we even have one real solution can really be a difficult question to answer. So, first, in terms of how I go about solving these problems, is using numerical algebraic geometry, specifically homotopy continuation methods. And so, this here is a graphic that describes a parameter homotopy. And so, a parameter homotopy will occur in two steps, where the first step is the expensive one that you kind of do offline, and this allows you. Line. And this allows you to get an initial starting set of solutions that is relevant to the system that you want to solve, right at some base point parameter. And then you can re-solve for different parameter values relatively quickly. I say relatively quickly because it depends on who you talk to. So if you talk to computer vision engineers, they would never use the word quick. Use the word quick. And then some other people, we don't necessarily care about the speed, right? But relatively quickly could be one of the computations would take a week and one would take only an hour. And that's relatively quick, right? And so the reason we do this structure is so that we can have at least some differential in speed, right? So you're only tracking the non-singular isolated solutions here rather than the Bayesian factor. Rather than the base we found on those chests, which is usually many more than is true for the system. So first, we're talking about this structure of finding our real solutions, right? And as we know, a lot of times that has to do with just solving the complex, for all the complex solutions and then filtering, right? So let's go back to our five-point problem. So this is a problem. So, this is a problem in computer vision, right? Where we talked about this parametrized polynomial system here that we can solve to find our solutions, which are these essential matrices that I think Sam talked about yesterday. And our parameters, we choose some values xi and yi. These can either be synthetic or real data from various data sets that people collect. And so, our goal is to solve for these entry. The entries of this E-essential matrix. And there are many techniques that can be used to solve these problems, right? There are Grogner-bases, resultant techniques. There are specialized softwares that computer vision engineers have made, especially for this five-point problem that run crazy fast, especially when you compare to some of the numerical approaches. Then there are also numerical homotopy continuation solvers that you can use. You can use Solvers that you can use. You can use kind of a black box one, or there's a paper that John and I did where we look at adaptive randomization techniques. So, because a lot of these problems are overdetermined, so how can you make them square so that they're numerically stable? There's a specialized software that John, I, and Anton, I think those are the only people in the audience today, that worked on a specialized solver called Minus. Specialized solver called minus, and we didn't create it for the five-point problem, but theoretically, you could put it in there, and maybe that would be faster than some of your black box solvers. But the common theme with all of these is we solve for all the complex solutions because that's nice. And then we have to filter out our real solutions. And now for the five-point problem, you're only solving for 10, and we have this result that there are four real solutions. There are four real solutions on average, right? But even so, you know, the difference isn't that crazy because it's just 10 total solutions. So the difference in tracking time is maybe not that cute. And we can do some more difficult problems with these types of softwares. So we have a project that looks at three cameras now and the view and what The view and what we're taking as our data is a little bit more complicated, and this might be relative to some curves rather than just points. So we have three points and two lines that connect these points. And these types of problem formulations help us deal with maybe blurry images or images where some of the items in the image are kind of close to each other, so they look very similar. So, how do you difference? They look very similar. So, how do you differentiate and map points to one another? We want to say that this point xi is the same reference as this yi in the two cameras, for example. So matching those points is more difficult. And so we have homotopy continuation techniques that still solve all the complex solutions, but are able to find solutions for these more complicated problems. So, for example, the Chicago problem. Chicago problem. So 312 is the area code for Chicago. So that's why there are 312 solutions to this trifocal camera problem. And so we named it the Chicago problem. And there's also the Cleveland problem, which if anyone here knows the area of Code of Cleveland, but it's 200 something or other. I don't remember. Don't remember. And so these are very large problems to want to solve and haven't quite been solved by current, you know, symbolic or other computer vision methods. And so, for example, we have this data set. So as I described, there are shadows and things look pretty similar. It's hard to maybe distinguish between various features. So it's a slightly more complicated set of graphics that we're working with here, right? That we're working with here, right? And then on top of that, we're looking at three cameras instead of two, right? So there are a couple things that have added to the difficulty of our problem. And there is one solver that has been able to look at problems like this. And so that is this CoalMap solver that comes from computer vision engineers. And what we can see in this first set of images where See in this first set of images where there's a little less shadow, and you know that can is in all three pictures. And both of our method with minus and this co map can solve for this. So the green is the ground truth, so that's the actual solution. And then R's are in red, and the coal map one is in blue. So we're both able to solve this first trio of images. And you know, it's negligible the difference between our It's negligible the difference between our solutions and also with respect to the ground truth, right? We have pretty similar results there visually, but then the numerics also say the same thing. But for these second set of images, we were able to find a solution that seems to behave pretty similarly in terms of error between the found solution and the ground truth, but Colmap failed for this. And so, And so, you know, numerical and homotopy continuation techniques can be really useful in being able to just solve some of these harder problems. But again, even for this problem, the difference between the total number of complex solutions and the total number of real solutions is very different, right? So there are 312 complex solutions, and they're only on the average of 40 real solutions. So imagine if you only had to track 40. So imagine if you only had to track 40 paths in a homotopy continuation algorithm versus 312. I would much rather track 40. But that's not trivial to only do that, right? And so our first kind of task in terms of being able to just focus on these real solution sets was a heuristic method where we were tracking over the complex numbers. Over the complex numbers, so we're tracking all solution paths. But now we truncate some of the solutions that we say don't have any chance of becoming real. And so this is heuristic, right? So what we did was we were able to plot the curve that's the imaginary component of these values along the path. And we took the tangent at that point and then the vector towards the zero. Vector towards the zero, the origin, and we said, okay, is that angle small such that there's a chance that that path will continue and the imaginary part will become zero, such that it is a real solution? Okay, cool, we'll keep tracking that one. Or is the angle big, where there's no chance that that imaginary component of the path will disappear such that at t equals zero, it will be a real? No chance, what, in the near future? No chance what? In the new future? Well, you're only going down to zero. Yeah, we're only going down to zero. So you only do this check when you're close to zero. Right, exactly. Yeah, yeah, yeah. So, so, but I'll get to that problem, right? Yeah, so, so, you know, we have this angle, we said it's too big, it will never become zero, right? And this actually, you know, showed some promise, right? It actually did, so in our It actually did, so in our adaptive randomization paper, these were different randomization techniques, which I very much have not given you any information about. But what it indicates here is that when we use the truncation method, we decreased our average number of steps even more. So we were able to improve the results, take less time to find our real solutions. Yes. And you still find all the solutions? In this case, yes. But I will. In this case, yes, but I will note that in just a moment. So did you make this angle check at a fixed T or did you check it along the way? So that's another challenge that I'll talk about. But we started at, I think, t equals 0.3. Did it once or did it all at once? Did it once. And decided at that t equals 0.3, are we throwing it away or are we keeping it? And if at that point we decide to keep it, And if at that point we decided to keep it, then we kept it. It's a heuristic. It's heuristic. I believe that at the time I did try different values. So the ideal point is to try and get past halfway, because in our parameter homotopy, you want more of the influence to be from the target parameter versus the starting parameter, but you don't want But you don't want to be so close to zero that there's no point in making the decision, anyways. So point three seemed to be a sweet spot without doing very much searching. Possibly. But then you have to decide what angle, right? Which is a different situation. So. So, challenges, all of these have already, Sam, I've been brought up, right? So, what angle do you use, right? So, we want to make sure, like Frank said, that we got all of the real solutions. So, which angle choice is good, right? And so, that was a heuristic, right? We knew for the five-point problem kind of what to expect and what behavior we are seeing with these casts, and so we could choose one. And so we could choose one, and we knew which solutions to expect, so we could check it, and it worked. But what if you're solving a problem that you don't have as much information about? Or there are so many more solutions, it's not necessarily simple to choose one that works for everything, right? So that's one challenge there. Paul brought up a different one, right? How do you know where to start this test? Do you do it multiple times? Do you do it just once? You know, you. Do you do it just once? You know, you want to do it after the halfway mark, but after that, you know, there's not really necessarily a best choice based on what we've currently done, right? You know, how do we not leave out any real solutions? How do we make sure that we get the one that is related to the ground truth, right? Because you're solving this problem, but you solve it for tons of different. But you solve it for tons of different two-camera instances where you're trying to map back in all of these cases to the same 3D image. So you're looking for not only the real solution that occurs in that one solve, but the real solution that's common among all thousands of the solves that you do. And so, well, what if in one of the solves you've dropped that one that matches all the other ones? And so, how do you know that? And so, how do you know that that doesn't happen? Right? So, okay, it's a heuristic. We know what heuristic means and what challenges that will bring. But, you know, it's maybe not a bad idea, but when you're trying to run for tons of different parameter values, how helpful is it really, especially when you don't know very much about the problem? Okay, well, so maybe we get rid of that heuristic idea and we go. Puristic idea, and we go instead to something called a real parameter homotopy, which is kind of what it sounds like. So instead of just tracking all complex solutions, can we be smarter about our parameter homotopy to only track the real solutions? So this is a picture of the parameter space for the Kuramoto model for n equals 3, which is a polynomial system that describes some oscillations. So the parameters are the Oscillation, so the parameters are the natural frequencies of this oscillatory problem. And so, what this picture is, is it shows decision or different regions of the complement of the discriminant locus, where it has the same number of real solutions in each of these components. So the red is zero real solutions, and then it goes up by two. So orange is two, yellow is four, and blue is six. And blue is six. And the total number, the highest number of possible solutions to this system is six. So we actually achieve the highest here in real solutions, which isn't always the case. For the n equals four Kuromoto model, you don't hit the total number of complex solutions equivalent number of real solutions. But what we proposed here, right, is if we have data points in this parameter space, we can say, okay, let's say the bottom of this black line. Say the bottom of this black line is the parameter point that we want to find the solutions for. Well, then we can do a nearest neighbor search for another parameter point where we have the solutions, such as the top of this black line. And in this connected component of the complement of the discriminant locus, we know that the real solutions stay constant. We don't hit anything nasty, right? Everything is okay. And so we can just track the real solutions. And so we can just track the real solutions to the real solutions, and we're smooth sailed. Okay, smooth sailing unless you cross a discriminate locus piece, right? So unless you're this purple line, right? And then you're crossing over the discriminate locus, so the real solutions are not necessarily staying the real solutions that you'd want. So that is considered a failure. Well, we didn't see much of that failure. So, in this Kuramoto n equals 3 case, we were able to go from on average 1.3 seconds tracking all complex solutions. So, it's still that parameter homotopy. So, we've done that like first phase of the two-part phase. But we decreased it to under a tenth of a second. So, that's pretty good, especially when you only have six real solutions, right? Only have six real solutions, right? And so you could imagine for Chicago if it's 312 to 40, that would be even better of an increase in speed, right? And for the four oscillator Kuramoto, so this is three-dimensional parameter space, we went from 3.4 to 0.15. So there's a pretty special. So you don't try to avoid the distinctive. Yes. So we've sampled our our period. So we've sampled our parameter space. So we have these points along our parameter space that we already have solved for our solution sets, and we have these kind of stored somewhere. But that brings up a good challenge to this, right? Well, can you store all of these things? Have you sampled your parameter space densely enough so that you know that you have a parameter point available in the nearest neighbor method? No, the thing is, you have like this set of points where you know the function, and then you start somewhere and go from the nearest point that you have in your storage and go the straight pipe, the one you want to find from this thread. Yeah, so we have, let's say we have this parameter point, but we don't know the solutions there, but we want to know the solutions. Then we say, we use the nearest neighbor algorithm to find. Neighbor algorithm to find the closest parameter point that we do know the solutions to. This is what you say you have in your storage. Yes, exactly. This is then in our storage, and we can use that as our start system where we have those solutions stored somewhere. And then we can do a homotopy from that start system to our target one and find those. So the whole speed of 100 both systems. Exactly. Yeah, so in this case, you have a two-dimensional parameter space. Carter's general tenant. Cardos general 10. Right. Right, exactly. So it's, it's, we have an understanding of kind of, you know, we can map the discriminant logus in many cases in a two-dimensional parameter space, not always. But we have an understanding then of maybe what densely sampled enough is in two-parameter space. It gets more difficult even if you add just a third. Difficult even if you add just a third parameter. So, you know, what does it mean to sample densely enough to know that you have a nearest neighbor point where you have these solutions stored such that you wouldn't cross the discriminant locus if you do this real parameter homotopy? So that's kind of a challenge there. How do you know that that's good enough? Sampling the discriminant locus and understanding where this curve is is non-trivial. This curve is non-trivial. For two parameters, it's usually okay. Not always, but usually okay, maybe you can get some idea depending on other things. And like Frank seemed to indicate, usually we have more parameters that we want to involve, right? So even for the chemical reaction network problem that. The chemical reaction network problem that I'll talk about in a second, right? It actually has 13 parameters where we fix a lot of the other ones to make it a more simple problem. But ideally, we'd be able to analyze for all 13 of the parameters, right? That would give you much more information. Yeah. So you're doing these neighborhoods like traditionally local, but perhaps like a different geometric shape would be more productive. Geometric shape would be more productive and going into higher conditions. Yeah. Yeah, I mean, I think that could be helpful. And I think likely the shape for this that you're referencing would depend on the structure of the discriminant locus and what the complement of that looks like. So what the connected components look like. So for the n equals 4 Kermodo model, it looks very similar, but it's kind of an egg. But it's kind of an egg with these types of shape chambers inside of the egg. And there's actually another component. Well, there's a couple more in there, but there's this one that kind of looks like this. So it has like a very skinny region in the middle. And it was hard for us to be able to get sample points that were in that middle portion, to be able to know that it wasn't, you know, kind of a pinch point with two regions. There was a There was a space there. But in a lot of cases, this then hits the sampling of the discriminant locus challenge, right? Where if we don't know all the components or what they look like or have even a way to represent what they might look like, then how do we choose the shape? But that could be an interesting thing to explore. Yeah, true. This one, do you, when you track it, do you even put a gamma trick on the wheel solutions? Trick on the real solutions. This example in this example, you do hit the discriminant, but you hit it twice. You kind of like come up, so you would have used a gamma trick on your real solutions. Like two of them, I don't know if you like dropped or whatever, but then you kind of would have come back. Yeah, but you don't know that the solutions you dropped are the ones that you that come back. It's possible that maybe it would have moved a little bit between the two. Yeah, so I think we did this with the black box burtini, so I think it automatically did the gambling. So, I think it automatically did the gamma trick. But I don't think we looked into it deeper to see if the ones that dropped are the ones that revive on the other side. And that's the case. I think if we did do the gamma trick and we were able to say the ones that dropped are the ones that came back, then you're right, that path would not be problematic, right? But we did not check that. But that could be a check to add as well. A check to add as well. I will note that in a different problem that I did work on, it was very rare to have the same ones pop up again that did drop. That was just one problem that I looked at though, so I don't know how rare or common it is in general, but it could be something to check out for sure. Okay, so you know, higher parameters, how do we keep moving? Higher parameters, how do we keep moving forward? There are challenges. Okay, and then for the chemical reaction network problem, we make it even more challenging by saying we not only care about the real solutions, but we care about the real and positive solutions. And so, oops, there we go with our backwards and forwards again. So, just as a reminder, this is our parameter, our This is our parametrized polynomial system that we're working with. We set these equal to zero. These are conservation equations that come about, right, because we're working with chemical species. This is also why it needs to be positive, right? We can't have negative of certain components. And so, okay, again, like I said, right, we kept two of our parameters free and we set just all the other ones equal to one. ones equal to one. And we analyze the parameter space. So I think this is the k on and this is k capped. And what we found was that this blue region, both of the blue regions, have three real solutions and then the yellow have five real solutions. But over the entire parameter space there is only ever one real positive steady state, one real positive state. Steady state. One real positive solution. So the minimum is one. The minimum is one. But it's interesting that there's a, you're looking at real solutions, positive solutions, yet there's a parity I see in the three numbers here. You expect the real pairs, the other pair of the real numbers would have to be positive. Yeah. Okay, so then we went even further and we said, so the conjecture related to this system was there's a maximum of five. Can we actually achieve five? And there was a conjecture that said, no, you can't achieve five, you can only achieve three. And so the goal was, can we find this region with three? And then, okay, is there any way to certify and say that that's the largest you can get? You can get. And so, very annoyingly, yesterday I recognized that the colors are flipped in these graphics, and so I apologize heavily for that, but there was no time to remake them. But what these two graphics are doing is it's showing not only the discriminant locus curve, but also what we were calling the positivity curve. So, we set up curves that were the inflection points for each of our variables of these. Of our variables of these XI so that we could determine which regions had this flip, such that we could analyze where we had real positive steady states. And this came about by looking and having a parametrization between our parameters. So this t variable here is we had some of our, so k on and k cat were still free. Kon and Kcat were still free, but some of our other parameter values now were dependent on this new parameter t, which we set to some value where some of them were equal to t and some of them were 1 over t. And this parametrization of the other parameters was found from some Newton polytope computation that Nita Obataki did. And this was the parametrization that we were able to find. That was, we were able to find these three regions, one, four, and seven, in both of these different values of t that allowed three positive real steady slopes. So we found the region. So, but we, there were a lot of questions, right? And these are still open questions that we have. And so one of these is: okay, we found this parametrization. Okay, we found this parametrization with this new polytope. Is that the best one? How do we know that that's the one that would achieve the largest number of positive steady states? Is there another parameterization that would give us the five? How do we find such another parameterization, right? But it seemed from these pictures, right, that That it was dependent on the structure of this parametrization rather than the parameter values themselves. And so how do we determine what this parameterization is? The other thing is, is in these pictures, right? These pictures, right, we've obviously truncated the values of these parameters that we're looking at. And what we're assuming is that as these curves move forward, we've captured the infinite behavior, right? We're assuming that these keep going, they don't create any new regions that would yield these higher levels of positive steady states. And maybe that's okay, because maybe for the, I vaguely remember talking. I vaguely remember talking to Nita about what were reasonable values for these parameters in the application. And so at times, that might be okay to assume that this is the behavior, right? Because maybe the parameter values that we would find are so unbelievable that, yes, it achieves mathematically five positive steady states, but that doesn't mean it's meaningful for the application, right? So again, that depends on who your audience is, right? Well, if I remember where you were talking about just kind of. Well, if I remember when you were talking about just chemical and not biochemical, these parameters can vary many, many ways. Really quite pretty. Yeah. And so that's the other problem. I think they can get pretty large and still be meaningful. So then this question comes back again. What do we do if there are more parameters? This is still only the two parameter case, and it's complicated enough. Complicated enough. And then the last piece, right? And this is a piece that Taylor loves: can we certify this, right? Is there any way to look at these curves and the sampling that we're able to do and say we know we've gotten everything even in just the portion that we're looking at? Let's assume that there's nothing else that happens or that we don't care about any of the other parameter values. Can we certify that this is really all that we would see in this region? Really, all that we would see in this region, right? Maybe there's like a little bubble right here that we weren't able to capture because of how we sampled our space. We weren't close enough in our sampling slices. So is there a way to certify that even just in this picture, we know we've gotten everything that we wanted to look for? Okay, and then in the last bit, we'll talk about this analyzing. We'll talk about this analyzing what's useful. That last thing is these, these depending upon how this formulation has, but these discriminants are often rational varieties because the prediction is some sort of a vector bundle. And so you're not going to find necessarily, you do not expect to find like isolated little circle in the middle. You can see that in pictures you have. Right. They look like rational curves. Yes, yes. They're not always, but oftentimes they're the They're not always, but oftentimes they're the projection of some type of blocks in the direction. Right. But in like the Kuramoto case, right, it's a closed curve, and then maybe that has one, right? So that might have to be filter how okay, so now we're going to kind of the part two flavor of the talk, which is, okay, maybe you've found these real solutions and you've met all the Solutions, and you've met all the challenges that I outlined. And now you want to look at how your real solutions interact with each other. And the premise of this comes up specifically, or we got introduced to this idea specifically for a problem in robotics, where can we take different leg lengths or a starting set of leg lengths and travel in some path and end up at the same leg lengths but have a different orientation? But have a different orientation of this pink triangle. So, do we have a monodromy action that happens? But we don't want to just go for the complex monodromy group, the classic monodromy group, right? And the reason for that is it actually very often is just the full symmetric group, which doesn't tell us maybe any underlying information about the real solution set, right? So, if over the complex Right, so if over the complex numbers, this is actually the full symmetric group. But there is underlying structure with the real solutions that comes about if we just look at the permutations of the real solutions. And so what we did is we came up with a definition called the real monodermy structure, real monodermy action, that just looks at the real monodromy group structure for the real solutions. And so, yeah. And so, there I go again, press the wrong button. So, as a reminder, right, we have this polynomial system, which isn't so bad, it fits on the slide. And what we did was we again just simplified to our two-parameter case, where we set one of the leg lengths, C is related to L by a square. So, C is L squared. And so, we chose one length to be fixed, and then we had the other two that were able to grow and shrink. That were able to grow and shrink based on how it moves along this path. And at this home position, which we chose very specifically from a paper by Manfred Houski, there are six non-singular real solutions, and six is the total number of complex solutions that are possible. So in this example, we're able to get all of them to be real for certain parameter values. And these are those six solutions, right? So all of these mechanisms have the same leg length. Have the same leg length parameters, but you can see that the orientation of this triangle is very different. And so our goal is to see: okay, can we start at x1, for example, and move along a path in our parameter space, any path in our parameter space, and get to the fifth solution, maybe? The answer is no, but let's look at how we did that. At how we did that, right? So this is the complement, or this is a mapping of the discriminant locus, right? Where we have zero, two, four, and six real solutions colored in increasing darkness of blue. And so what we did is we took random loops in our parameter space. And by random, I mean they were prescribed by me. Described by me sitting with a marker and determining which paths to take, right? And so, this is one of the questions related to this problem that we'll get into in a second. But what we did is we took various loops entering various components of the complement of our discriminant locus, and we were tracking these paths over the complex numbers, but we were keeping track of which one stayed real the entire time. So now So, not even like the question that Julia answered earlier, where they dropped and then came back and were the same solutions. These, when we crossed a discriminant locus, they stayed real. And so we wanted to say, okay, along these paths, which ones stayed real the whole time? And after that, which one stayed real the whole time? And were there any permutations to other orientations? Orientations. And so we found some, which is maybe not necessarily surprising because we had this other result from Manfred Hussey that we were trying to confirm. But if you remember, I said that the complex and classical monodermy group was the full symmetric group. So it didn't capture this behavior, right? That there was only this partial switching. But we found some between x4 and x5. And x5. And so this is an example of such a path. So we started in our region of six real solutions, and we traveled along this path where we kept track of not only of the order of the solutions to know which ones permuted with each other, but we kept track of which ones stayed real the entire time, even though we were tracking over the complex numbers. We want it real the whole time. Okay? And so we can see. And so we can see the legs grow and shrink as we travel along this loop in our parameter space, such that we end up at a different orientation of this platform, same parameter values. And what we actually found was that, yeah, I don't have that part, but what we actually found was that there were three solutions that could permute with each other, and three solutions that could permute with each other, but not cross-permute. So there were kind of these two. So there were kind of these two trios of sets of solutions and that structure is not part of the full symmetric group which was described by the classical monogram group. Which is the full symmetric group in this case? Is the full symmetric group? It is in this case, yes. Yeah, so it doesn't give you any kind of interesting information, specifically when we're doing a problem that cares about only the real solutions, right? And so this was able to capture that structural information. Able to capture that structural information related to those real solutions. Yes. I don't know. I don't know. But it's, yeah, I should look. But I don't know. Well, if it's, I mean, if you work, if you're in a really like a real space and you're moving, you're not going to flip over the Going to flip over the orientation thing, serve orientation. I would think so. Yeah, I would. This looks like the image of a disk that's folded over installed with cusps. Yeah, so we actually don't have the actual polynomial that describes this, I don't think. This, I don't think, if I'm remembering correctly. So, this is actually just a sampling of the discriminant language here. So, let me end with some challenges, and then I'll go into kind of future questions. So, again, this is two parameters. What do we do if there are more? And the big question, right, for what do we do with more is how do we know what loops to take? And how do we know we've How do we know we've gone all possible loops that could get us an action, right? A monodermy action? And so we can kind of see in this two-parameter picture, especially when we have this nice representation of the discriminant locus, we can see which paths we should take and then which ones are similar to those. And, you know, I mean, it took many sheets of paper and I looked very crazy if anyone would enter my apartment at that time. Would enter my apartment at that time. However, we were able to do it, but I don't even see a way to code that set of decisions into a computer for even the two-parameter case, let alone increasing the number of parameters. And then you also come into this question of: can we even sample the discriminant locus, know that we've achieved kind of every part of the curve that exists? Curve that exists. And then the other two we talked about. I mentioned. So, kind of the main goal and main theme of this talk was supposed to be, okay, we care about these real solutions. We care about how the parameters affect this number of real solutions, whether that's in terms of behavior with these real parameters or taking loops in this real parameter space. Loops in this real parameter space to understand the relationship between solutions, how the parameter values change, maybe a maximum number of real solutions, whether we know by analyzing the real parameter space whether there's always at least one real solution. And so these are the type of questions that I hope to continue to work towards answering that all involve this analysis of the real parameter space. The real parameter space and trying to take a lot of the theory and techniques that we have over the complex numbers, and can we make them just work over the real numbers with maybe some assumptions, right? Because, for example, in Sam's talk, she talked about what computer vision engineers care about, right? And so, for a lot of these problems, they care about speed, and we're not quite. And we're not quite there yet. Even our minus formulation was, they accepted our speed, but it was a semi-tough sell, right? And it was, a lot of that was due to the fact that it hasn't really been achieved in other methods yet. And so can we continue this work to get to that speed component, to understand more about max or min number of solutions? So this is all connected to various papers and feel free to ask me any questions and thanks for listening. Questions?