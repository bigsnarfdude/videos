And today I'm going to talk about our recent work in the network cultural regression for incomplete matrix-valued temperatures model. So this is a joint work with Fifi Wang and Zheng Li and Yin Yuan Ma. So in this work, we are interested in modeling the matrix-typed time series. Matrix typed time series data. So here are a few examples. The first example is from the brain image. So here we can take pictures from the human brain images and this yields the MRI data. So basically we can relate the specific active regions of the human brains to some type of, for example, the human brain. Type of, for example, the disease. And our second example is from the trading volumes among the countries evolving through the time. So basically, we can observe the pattern between different countries remain quite steady, but they are still slightly changing over this time. Our last example is user scoring system, which is also the recommendation. System, which is also the recommendation system. So, in this example, we have a lot of users and a lot of items. And if a user likes the rattan, he will give a high rating of this movie. Otherwise, the rating might be very low. However, in this example, we will find that there are missing entries of the matrix data. So, for a specific user, he may not watch all the movies. Watch all the movies. So we have some missing entries here. So basically, in this work, we are focused on modeling the matrix time series data with data missing issues. So let me briefly review the related literature in this regard. So to model the matrix type data, we have basically the row subjects and The row subjects and the column subjects and the corresponding matrix entry. And a very straightforward way to model the matrix-typped time series data is just to vectorize it in a long vector. And we know a bunch of tools for modeling the vector-type temperature data, for example, the vector models, the vector autoregression models. However, by doing so, we may break the We may break the intrinsic structure between the row and the column subjects. And in addition to that, for example, if we need to use a vector autoregression model, we may introduce a large number of unknown parameters for doing so. So this will result in an unreliable estimation result. So to model the dynamics of the matrix temp series data, Um, matrix temp series data, we need to develop specific tools for the matrix type data. So, basically, in this regard, there are two main approaches. The first one is a matrix factor model. So, basically, this approach models a high-dimensional matrix using a low-dimensional structure. So, here our factor is a low-dimensional factor matrix. And we will see that the loading matrices is in the left side and in the right side of the low dimensional factor matrix. And the second one is the matrix autoregression model. So this approach allows us to make prediction of the future observations. So basically, the matrix autoregression model takes this form. So we can all Takes this form. So we can observe that the autoregression matrices is multiplied in the left side and in the right side of the matrix in the previous 10 points. And we can also compare the number of unknown parameters for this matrix autoregression model with a vector-type autoregression model. And we will find that the total number of unknown parameters in this model will be large. Model will be largely reduced. And also, recall that we have the missing data issues, and this naturally connects to the matrix completion problem. And in this regard, the literature talks about multiple missing mechanisms. For example, whether the missing is related to the covariance, whether it is related to the response, or others. So, basically, we assume that the matrix entries YIJ is partially observed. is partially observed and it is generated by a population matrix A0 and plus noise matrix and the A0 matrix takes a low rank structure. Okay, so in summary, we would like to focus on the matrix autoregression model. However, in literature, we find that they cannot deal with a high-dimensional problem because they assume that the Because we assume that the dimension of the A matrix and B matrix are finite. So if the dimension is high, we will have a large number of parameters to estimate. However, on the other side, in our application, we still find that we can collect the network relationship for the row subjects and the column subjects. And we would like to incorporate such information in modeling. Such information in modeling the autoregression matrices. And on the other hand, for the data missing problem, for the matrix computation task, in literature, most works are focused on the static setting. So they ignore the potential dynamics among the matrix, among the metric time series data. So we would like to estimate the auto. Estimate the autoregression coefficient and the Low-Rank structure in the presence of the data missing. So, before I go into the details of our model, I would like to first introduce the application part. So, we use the YOUP data set, which is a public data set collected from the Yeop platform. And the Yeop is a little bit like the Dajong Dianping in mainland China. So, on this platform, China. So on this platform, we have a lot of users and a lot of restaurants. And if a user pays visit to a restaurant, if he likes the restaurant, he may give a high rating of this restaurant. And also, a user cannot pay visits to all the restaurants. So we still have the data missing issues here. And on the other side, for this application, we can also construct. We can also construct the network relationships for both the raw subjects and also the column subjects. For example, for the row subjects, on this platform, we can actually collect the social network relationship among the users. And for the regions, spatial regions, we can construct a spatial network, which is spatial adjacency network among the locations. Among the locations. And also, we conduct some preliminary descriptive analysis for this data. So, this shows the spatial network for the spatial network in Las Vegas. So, we segment the city into several regions and we calculate the average score for a specific region in one year. And we can observe that the region. That the regions who are neighboring with each other tend to have similar scores, which have similar colors. And also, we visualize the social network for the users in Toronto. And larger circles imply higher ratings. So, we also observe that users who are friends with each other tend to give similar score here. Similarly score here. And also, we calculate the average matrix here and we conduct SVAD decomposition for this static matrix, which is average yt over the whole time period. And we find the rank of this matrix is 12, which is also much lower than the original dimension of this matrix. Of this matrix. So, this suggests a possible low-rank structure of the static part for the time series matrix data. So, this motivates us to propose this model, which is, we call it matrix network auto regression model here. And our response is YT. And the YJT may be the average relating of the IC user to the JSON. Of the IC user to the JS visual region. And here we have the data missing issues. So we use a binary variable Rijt to denote whether the corresponding element is missing or not. And we use logistic regression to model the observational property. And here we use two adjacency matrices, A1 and A2, to record the network. And A2 to record the network relationship for the users and for the spatial regions. For example, if the user i is following the user J in the social network, then the A1ij will be equal to 1. Otherwise, it is equal to 0. So we further conduct the zonalization and the column normalization of these two adjacency matrices, and this yields two. Two weighting matrices, W1 and W2, and we model the dynamics of YT using this autoregression form. So the first two terms are related to the network effects from the social network effects and also the spatial network effects, respectively. So this part, W1 times YT minus one, is actually the average score. score. For example, the S row for this matrix is the average score of the S user, sorry, of the S users friends. So the Lambda matrix is a diagonal matrix. So the S element of the S diagonal element of this matrix is actually the social network. The social network effect of the S user. So, similarly, if we can record the spatial effects in the gamma matrix, which is also a diagonal matrix, which is so the JS element of this matrix records the spatial effect of the JS location. And also we have, we model the static part, which is also we can treat it as. Also, we can treat it as the intercept of the matrix autoregression problem in this form, which is a low-rank structure. So here we can collect some covariate information of the users. So the X beta, the X beta part, we can treat it as a known low rank structure matrix. And the B here is an unknown low rank structure. An unknown low-ramp struggle, low-ramp matrix. So, how do we model? How do we estimate this model? So, basically, if we observe all the full data, so we can use the least squared type objective function to estimate the unknown parameters. So this includes both the autoregression coefficients and the static and the unknown parameters in the static part. And we would like to first focus. Like to first focus on the autoregression coefficients. So we use a profile estimation to profile out the static part, which leaves only the autoregression coefficients here. And we can just minimize the profile objective function to obtain the autoregression coefficients. And also remember that we have the missing values, so we need Missing values. So we need to take care of the missing data here. So basically, we can use the propensity score and we can define a new variable which is Zijt using this formula. And one can, and we can verify that expectation of Zijt is just equal to the expectation of Yijt. And we can replace the Yijt with the Zijt. IJT with Z IJT, but we need to make further corrections to the replaced objective function because we need to make sure that the expectation of the objective function is equal to the expectation of the objective function with the full data. So these three terms are just used for the corrections. And we have the new objective function, which only includes Which only includes the autoregression coefficients. And then, after we obtain the estimate for the lambda tutor, for the lambda and the gamma, we can proceed to estimate the unknown parameters for the static part, which is beta and b. And in this regard, we need to impose further penalties to encourage the low-rank structure of the B matrix. So basically, we need to. So basically, we need to incompose a nuclear norm, a penalty function for the B matrix to obtain a low-rank structure of the B estimate. So our estimation can be divided into two steps. First step is that we need to estimate the lambda and gamma matrix. And here we use an iterative algorithm because we find that if we fix the gamma parameter, we can obtain. We can obtain an analytical form for the solution of lambda. And then we iterately estimate lambda and gamma until the convergence. And then after we obtain the estimate for lambda and gamma, then we can estimate beta and b. And in this regard, we can obtain an analytical form for both b hat and for sorry, for both beta hat and b hat. For the beta hat, it is just a For the beta hat, it is just a least square estimation. And for the B hat, it is actually a soft stress shoulding function imposed on the S V D decommission of this matrix, which is also easy to obtain. Okay, so we proceed to discuss the theoretical properties here. So, first we develop the symptotic property for the theta tutor, which is which which is which consists of the auto regression coefficients which are the lambda tilde and gamma tilde and we find that the convergence rate is related to mt and cp so m is n1 class n2 here we allow the n1 and n2 can diverge to infinity but in our theoretical framework we assume that they diverge in the same speed. Uh, in the same speed, uh, so we use M here. So, uh, and also it is related to T. So, if we have a longer length of time periods, we will have a faster convergence rate. And also, Cp here is actually the observational or observation property, sorry, probability. So, if we have Cp is equal to one, we will not have any missing data. So, the convergence rate. The convergence rate is only related to M and T here. And we will also have asymptotic bias in the here. And the order of the asymptotic bias is roughly the 1 over T. And the other term, for example, this log M term occurs due to that in our theoretical analysis, we need to analyze the network. Need to analyze the network dependence. So, this term is due to the network dependence here. And then we can proceed to discuss the property, estimation properties for the beta hat and bet. And they are also related, the error bounds of these two estimators are also related to the MCPNT and also the network dependence part. And also, we find that it is also related to the bio. Also, related to the bias terms. And I would like to make a note that the analyze for the B part, for the B-hat part is more difficult because recall that our B-hat matrix is a low-rank matrix. So to derive the error bound for this part, we actually need to first derive an operator error. Derives an operator error bound for the residual term in the first step. And in that analysis, we need to analyze the time-dependent structure for the matrix time series data. So in that regard, we need to use some tools of the random matrix theory. So we refer to upper supplementary materials for the details. Materials for the details. And also, we know that the property of the B-hat and also the beta-hat involves the bias part. So a natural thought is that we need to further conduct the bias reduction. So actually, our bias, so actually we can obtain the formula for our bias. So very natural way is just we separate. We substitute the estimators in the bias formula, and then we can obtain the estimation for the bias part. And then we subtract the estimation of the bias, and then we can obtain a debiased estimator. So we can also investigate the properties for this debiased estimator, and we find that it is. That it is the bias does not diminish, it just reduces to a smaller order from one over t to one over roughly t square. So if we conduct our round of de-biasing, we may have a very small order of the bias. So in our numerical experiments, we find that roughly two or three rounds of the bias in this. All three rounds of device in his procedures are enough. Okay, so also Van Referee suggests us to add more abundant covariate information. So remember in our first step, we only incorporate the covariate information for the users, but also we have more information such as the columns covariates and the interactive covariates. So if we So, if we add all the queries in our framework, we can obtain a very flexible modeling framework. And we also compare our modeling framework to some existing works here. However, by doing so, we need to deal with several challenges. The first one is we need to give the identification conditions because not only we need to We not only need to separate this low-rank matrix from the B matrix, but also we need to separate the unknown parameters, the different parts with each other. So this need to impose new identification conditions. And also, we need to discuss the estimation procedure and the estimation properties. But due to Properties. But due to the time limit, I will skip this part, but we include this part also in our supplementary materials. And let me, then lastly, I will introduce the real data analysis part. And here we focus on five cities in the EO platform and with most business shops. And here are our YIJT. And here our YIJT is defined as a error score that's user-eyed comments on the shops in the district J during the year T. And here Rij T is better user eye comment on shops in the district G. So we also include some covariance here for the users, for example, the duration, which is the number of years from the user's first registration on the EOP. On the earlier. So, for example, in Las Vegas, we find that users with lower duration tend to give higher scores in Las Vegas. However, this phenomenon is not very obvious in the Toronto. And also, we include other queries such as the VIP, so which is whether the user is VIP or not. And also, we include several queries like the accumulated number of users. The cumulated number of useful, cool, and funny comments given by the users. Okay, so we compare our approach in terms of prediction with some competing methods. So we consider the missing mechanisms in both the missing at random and also the uniform missing. And this is a prediction RMSE values. So we can. So, we can observe that this is the original approach of our model without any de-biasing procedures. We find that the prediction RSE is relatively lower. And after one or two rounds of biasing procedure, the prediction power can be further enhanced. So, the prediction RMSE is smaller than before. And also, if we spice And also, if we specify a missing at random, which is we use a logistic regression for modeling the observation probability, the prediction RMSE is lower than the uniform missing. And also we compare the prediction with several approaches, such as this approach, we ignore the autoregression part, and we only use the static part for making the prediction. For making the prediction, and we find that the prediction RMSE is larger than our approach. And this approach is suggested by also one of our referee, and he says that you can use SVT for imputing the data at each time point, and then you use your model to make prediction. We find that the prediction of this approach is also very low, but our Low, but our approach is slightly better than using this approach. Okay, lastly, this is our estimated result. This is the social network, estimated social network effects. And we find that basically the estimated lambda values is very small, it's around zero. But we find both positive and negative values for the estimated. Negative values for the estimated results. So it implies that you can be positively or negatively influenced by your following friends. And also for the spatial effects gamma had, we did not find negative values, we only find the positive values. So it implies that the region is positively related to their neighborhood. Positively related to their neighboring regions here. So that's enough of my talk. Thanks.