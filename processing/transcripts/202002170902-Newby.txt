Some of this, you know, might be this might be a problem that you're all extremely aware of and I might be overexplaining it, but just I hope you'll bear with me. So I'm studying gene expression system and I'm studying the rare events or the metastable dynamics therein. And I'll also hopefully point out how I came upon this problem and Came upon this problem and some of the extensions that we've been applying it to since this. So, this is the basic idea of a gene expression model, and there's no feedback regulation here. So, you know, we have a production and degradation of mRNA and protein. And so, we have these sort of three levels of the process. And depending on the on-off state of the gene, we have production of mRNA. And you can see fluctuations. RNA, and you can see fluctuations not only in the copy number of the protein, sort of intrinsic fluctuations there, but fluctuations on a larger scale that come from the gene. And this is mostly what I'm interested in, is focusing on the fluctuations that arise from the gene turning on and off. So now we add some sort of feedback regulation to the system, and we can To the system, and we can get nonlinear behavior out of the system, and that's what we're interested in: some sort of a dramatic shift in the expression of this protein. And these things can have functional implications, and people have been thinking about this for a while now. Randomly switching between different expression profiles can be important in tissue. Tissue development and differentiation. It can be important, well, it can be a mechanism for antibiotic resistance. And you can even get transient changes that later become more permanent. This is an example of an excitable system that might be metastable. And towards the end of the talk, I'll talk about a slightly different excitable system that we System that we've studied with a neuroscience application. So, what I want to emphasize, I guess, in this talk is what makes this different from a simpler chemical system, something that we model with a chemical masser equation, say, where you have intrinsic fluctuations that are of large n type, or something where there's a very traditional large volume scaling where you get a concentration. Large volume scaling where you get a concentration. So, in all of these metastable problems, we think of them as perturbations of some deterministic system. So, we have to think about what that deterministic system is first, and then to find some small parameters, some epsilon. And then, at least the way that I study these systems is with a perturbation theory and large deviation theory. That's not the only way. So, the reason why this system is different is because we have this fluctuation. system is different is because we have this fluctuating gene and that can't be treated as a concentration. So the way we reach a deterministic limit in this kind of system is where the gene switches on and off very fast. So we call that an adiabatic limit, right? So I'm looking at this dual limit where we can, the fluctuations in the sort of the intrinsic fluctuations In sort of the intrinsic fluctuations are very small, but also that the gene switches on and off very fast. And the combination of these two would lead to rare events. Can you explain why you don't have a mean field limit when the gene switches slowly? Alright, because those approximations basically work based on, or that limit basically works under the assumption that you have a large Assumption that you have a large number of copies of the gene or the mRNA or the protein. So that plus one, minus one, since you have to have an integer valued quantity, that that is a small perturbation. So that in the central limit theorem, first they're just continuous fluctuations, and then those slowly die out. So the fluctuations become less and less significant. So I'm I'm a bit confused because in a little bit of very very fast switching you just get basically promoted as a constant rate of transcription. An average rate, yeah, a constant rate, yes, that's correct. You still have mRNA fluctuations will be Poisson and the proton fluctuations will be negative binomial, right? You still have a fluctuating system, right? That's right. So there are two noise sources. There's the noise of the switching gene and then, well, I guess there's the cut. I always just ignore the mRNA. I always just ignore the mRNA, right? So that's just out. So there's the copy number of mRNA, right? And then there's also the protein. So I guess in this case, there are like really three sources of noise. And you can eliminate any one of them, right, by tuning the transition rates in the model, in the master equation, and focus on, say, if you wanted to, you could focus on the effects of just, you could isolate the different intrinsic noise sources that way. So, you could tune things so that you can have or eliminate any of these noise sources. So, yes, you would still have the fluctuations in mRNA if you didn't. A follow-up question is that. Isn't it enough that the mean number of proteins be large for you to use the switch? Yeah, that's a good way of thinking of it, too, right? Yeah. But also in the limit of slow switching, because I understand that the number can drop in that case, right? This is the problem, but in the This is the problem, but in the central limit it's sufficient to have a mean number which is high, right? Yeah, that's right. So, I mean, these little fluctuations that you have here, you can completely eliminate them. And then the whole model could be deterministic in between changes, the turning on and off of the gene. That's actually the limit that I first studied or I approached this problem with. Because I've studied similar, we call them P-C. Because I've studied similar, we call them piecewise deterministic Markov chains, and I've studied those models in other contexts. And I think that this is something that was studied in Kepler and Elson's early paper on this. All right. So the idea is that we have a stochastic process, and this is just a very simple example of bistable switching. A very simple example of bistable switching. So, we always have this, you know, the stationary or this probability distribution that's connected to the stochastic process, and we jump back and forth between study. I probably don't have to say this in this group, but in a math department, probabilists, they study this, and they don't so much study this, where I'm the opposite. This is usually governed by a Chamokomogorov equation, and that's a differential equation, and I use Equation and I use asymptotic methods to study that. So that's what I'm going to talk about today. So on a very, very fast time scale, we get this thing more or less converges, right? On a much longer time scale, we get switching back and forth between these two, and this distribution takes a little bit longer to go stationary. But eventually, these two peaks should be exactly the same size. This particular model is completely. This particular model is completely symmetric. So, oh man, I just skipped to the very end of the talk here. And now that's my email. Sorry about that. For some reason, this slide system has this sort of bar at the bottom that can like work you inexplicably. Don't press that button. Okay. All right, so we have some stationary distribution and we're thinking of bi-stable switching, so it's got two distinct peaks. I think that this is sort of the maybe a good stochastic definition of bistable switching, that the process spends a significant amount of time. A significant amount of time around two points that are very well separated. And then, if the distribution is very small in between, then in some sense it's rare to switch between these two. And the way we define a potential, because we like this sort of description, just take the negative log of that, and it has this sort of potential well-like shape. Push that button again. All right, a couple of quick definitions. These are two that I'm going to use throughout this whole talk. So, epsilon, I'm just going to think of as tuning the noise strength. So, in the bistable switching example above, it was just a Langevin equation. It's very explicit how epsilon appears. It's just the white noise term. But in master equations, we kind of have to tune the transition rates very carefully with epsilon. So, hopefully, I'll point out how that works as we go along. As we go along, and then I can define this quasi-potential again, just the negative log, but I multiply that by epsilon and take the limit as epsilon goes to zero. And this is an important quantity. Why? Because we're interested in the time scale of these metastable events. Do they occur on the time scale of the lifetime of a cell? Are they relevant, or is it like more the lifetime? Or is it like more the lifetime of the universe, right? So we can use this sort of Kramer's idea that if we know the difference in the height of this potential well, then we can get an estimate up to this pre-factor, of course, for the mean time for switching back and forth between. And this is sort of the goal. But there's a lot of other things that this large deviation theory framework can tell us about these problems. Framework can tell us about these processes in addition to this. And so, I want to talk about why the fluctuating gene makes this problem more difficult. It's because you have to have this different limit. You have this adiabatic limit where the gene sort of averages itself, and that complicates the analysis. So, these tools for large deviation theory have been applied. These tools for large deviation theory have been applied and been developed for like Fokker-Planck equations. So you could have a diffusion approximation, or for the chemical master equation when you have like a Kurtz type limit. But not so much for these piecewise deterministic Markov processes where you have some sort of a fast-slow separation in your system. So that's basically been my contribution to this problem is thinking about that problem. Thinking about that problem and combining it with other types of noise, like the standard birth-death process, where you have the system size limit. So, when you have both going on at the same time. Question. Yeah. Does this method work also when you're in the off state where the demographic noise is large? Yeah, yeah. I mean, that's so if the gene state is fixed, or if you took an adiabatic limit, then that's more of a traditional that's been. Traditional, that's been well studied. So that would be, you know, yeah, so I think Diekman and others, there's these path integral formulations for those. So this framework is well known for Fokker-Planck equations and for that type of chemical master equation, but not so much when there's a mixture of the two. And that's what they think about. So one of the approaches that people typically use is to derive a diffusion approximation. So you can, you don't have to fully take the limit, eliminating any of these noise sources. You can look at the central limit theorem. You can include a diffusion term in some sort of a Langevin equation that accounts for all of these noise sources and then think about this large deviation problem. But I mean, and it depends on the I mean, and it depends on the particular question you're asking, but there are some downsides to using this diffusion approximation. And I hope that one of them is that you get the wrong energy barrier height, which is a pretty important component because that gets amplified exponentially. And there's some other things too that I hope I can point out along the way, but really the framework and Framework, and there are Monte Carlo methods for sampling these, which I'm not an expert on at all. So I think we might hear a talk from Elizabeth on this. And you might wonder why even bother with the asymptotics if you can just plug this into some Monte Carlo method. But I think the two really complement each other well. So you can get more understanding by. You can get more understanding by using both at the same time. Anyway, I won't talk about the numerical methods, just the asymptotics. All right, so I hope that I can make the case that the framework that you would use, the large deviation framework that you would use to get the asymptotics for a diffusion approximation, they're essentially the same as what you could do if you just applied them to the full model. So there's really no benefit. There's really no benefit. There's really no simplification that you get by going to a diffusion approximation unless you can somehow get a fully analytical result, which is pretty rare, especially when you have multiple genes. So I'm going to think about this genetic toggle switch just because it has two genes. This is a workshop on networks. So you've got to have at least two genes for a network, right? Not just one. Not just one. So, this is a nice toy model for this, and lots of people have worked on it. And the idea is that you just have something that's very symmetric. And well, you've all seen this, I'm sure. So this is what the deterministic system might look like: where you have the quantity of the y-gene and the x-gene, and you have two, it's a bistable system, so you have two stable fixed points, and then there's several. And then they're separated by that separatrix in the middle there. Right, so if you start sort of on one side, then you know you're going to, I don't know what the dynamics are, I don't have a vector field or anything, but you're just going to get convergence to one or the other of these fixed points, and that's basically it. Oh, now I did it again. Alright, I promise not to do that again. Alright, so for the rare event analysis, what we're going to think about is we're going to start with transition paths, and they end with this quasi-potential that gives you the energy barrier. The energy barrier tells you the time scale of the transitions. So, how are these connected? So, what I mean by a path is something like, you know, you maybe start You maybe start somewhere and then you converge to one of the stable fixed points and you fluctuate around it for a long time. So it's metastable. In that example I showed earlier, you get this really fast convergence of that histogram around one of the two states. And then on a much longer time scale, you might have some fluctuation that takes you maybe something like through the saddle and then at The saddle, and then it goes over to the other fixed point, and you fluctuate there for a long time. So, this is the transition path that I'm talking about. So, given that a transition occurs, you can say maybe look at the process the instant it hits that saddle, and then look backward in time. How did that fluctuation occur? So, that's what we mean by a transition path. And it turns out that. And it turns out that these two things are really connected, and that might seem pretty mysterious, but it's true. So I call these most probable paths, paths of maximum likelihood, whatever you want to call them. The idea is that just take two points and maybe you condition on these paths starting here and ending here, and you just look at where they go and then draw a big sort of circle around them, and that's kind of their extent. Their extent. And we have this parameter, epsilon, right, that controls the amount of noise in the system. So as epsilon goes to zero, the noise disappears. And so what happens is that all of these trajectories tend to be confined, not always, but tend to be confined within this sort of two, right? And then in the limit, as epsilon goes to zero, then this is a smooth path. It's not a stochastic sample that's. Stochastics, a sample of the stochastic process. So, this is what we try to derive. And I haven't derived for you where these paths come from or what equation they satisfy, but I will. But this is a really nice numerical method for actually computing these. It doesn't rely on any kind of detailed balance like a lot of these methods do. And you can specify the beginning point and the end point. You don't have to use a shooting method. Point, you don't have to use a shooting method, and it's pretty scalable with dimension. All right, so this is what they sort of look like in the model. So what I did is I, on this half, I plotted what we would get from the diffusion approximation. The full diffusion approximation contains all the noise terms, and this is just the same thing but applied to the full model. So you can see these transition paths that go to the saddle, they're very important. Go to the saddle, they're very important. These are the ones that would determine the energy barrier height. Because along all of these paths, we also integrate the value of this quasi-potential. Jay, can you say a second? So what are the other curves other than the red-blue one that you've shown? They're paths starting from different points in your XY space? Or what are they? So you can see how I really should put some arrows. I really should put some arrows on this. The idea is they all start at the stable fixed point for the deterministic system and they travel outward. And they all have a different end point. So I just picked, I just discretized sort of points along the boundary here and then computed the path that would take you from this fixed point out to the point on the boundary. So the idea was just to kind of cover the space in these metastable transition paths. So these are. Stable transition paths. So these are all paths that this system would require noise to actually traverse, but these are arrive at a final point at different time. Yeah, just a quick idea. Just looking at this, it looks like the diffusion approximation works very well here, gives you very similar answers to the formal. Is that what I'm... Could you explain some of the differences? Because I see some differences near the origin, but up above in the top quarter look similar, right? Yeah, the formal is the. Yeah, is the message that they actually look similar and it's all in the time scale? Or if you just look at the trajectories because you don't have time information here, and that's what's missing? I don't. I haven't really looked at the actual time, just the trajectories. But it's interesting, I think, how, as you pointed out, in some regions they're really similar, and in others they're very different. But even probably the most important paths, the ones that connect to the saddle, they're Saddle, they have a slightly different shape. Does that matter? It might not matter in terms of the actual trajectory. But what does matter is now what I'm going to do is I'm going to kind of flip this whole thing and look at the quasi-potential. I hope that sort of visually makes sense. And so we get something that looks like a potential well, and these are the two paths that determine the height of that potential well, and this is something that. Well, and this is something that I have found: I don't have a theorem to this effect or anything, but in systems that have sort of a fast noise source, one of these adiabatic components to it, when you take the adiabatic limit, or even if you do a diffusion approximation where you keep some noise from that source, you over-constrain the system and you vastly overestimate the time for transitions. Because this fast subsystem is capable. This fast subsystem is capable of doing behaving far from its typical distribution, its quasi-steady-state distribution, and it probably does during one of these rare events. And that's one of the things that the diffusion approximation over constraints. Jane, I'm still a little lost. So the red and the blue are your quasi-potential? So this whole, this whole, so the quasi-potential is a function of x and y. A function of x and y. It's a scalar function that sort of covers the whole space. So, what are the other curves other than the red line? All of these curves have a value. So, it's like I'm using the method of characteristics to cover this sort of surface. So, I'm integrating these to try and get a picture of this full surface. The curves are a constant of what? They're not a constant. Those curves, what you had on the previous slide, and you're plotting the x minus y point along each curve. So each curve you had on that one is a curve that is here, and you're representing on that x-axis the x-coordinate minus the y-coordinate. And he's showing the quasi-potential that you get as you integrate the whole path integro along those curves. Yeah, so when I integrate these, there's a third component to this. I get the quasi-potential. To this, I get the quasipotential, the value of the quasi-potential along all of these curves, everything in here, right? Including these two, but these two are just really important ones. Now, what I'm doing is I'm sort of looking at it from this angle. But you kind of have to imagine that there's information coming out of the board here. That's this quasi-potential. So you can kind of see this well-like shape. These are the two stable fixed points of the deterministic system. Fixed points of the deterministic system. This is where the saddle is in the middle. And these are the two paths that connect through the saddle. Right? And so what you see, this height of the energy barrier, this appears in this Cramer's rate. So any error here gets amplified by this large parameter, becomes more and more extreme, and amplified because it's in an exponential function. So it gives you a very different, and everything in here is independent of epsilon. And everything in here is independent of epsilon, right? This is where epsilon appears in that approximate. K0 probably depends on epsilon too. So these effects get amplified as the noise strength goes to zero. And so, I mean, what I did to generate these, I applied the same exact machinery. I just had a different Hamiltonian function, which is sort of like the kernel that we start with to generate. Kernel that we start with to generate all of these approximations. And that's what I kind of want to derive for you next. How do we get that Hamiltonian? It's something that's very quadratic, like an Eikonal equation, when you apply it to the diffusion approximation. And it's a little bit more complicated for the full model, but still workable. Any questions? Exactly, you define the most probable part of the. Yeah, that's what I'm going to talk about next. Because that, I mean, in the full model it's a discrete state space, right? And here we see smooth lines. That's right, because in the limit, as epsilon goes to zero, we have a concentration limit for the amount of protein. That's what x and y signify. So we treat them as concentrations in that limit, right? Whereas the gene has two states, there's one copy of There's one copy of each gene, we can't treat those as continuous variables. Okay, I see. That's one of the issues involved, right? So, in the limit, right, there's no, we're not also plotting the gene state here, so it sort of averages itself. All right, so I'm going to study the Chaplin-Kolmogorov equation. So, this is the matrix that we would have for. This is the matrix that we would have for the gene state. It's just a 3 by 3 matrix. And then these are just jump operators for production and degradation of the protein X and the protein Y. So you can think of this as just one big matrix. Although, I guess it's an infinite matrix. It's a big master equation. And N is going to be the gene state, and X is the vector of X Y for the protein concentration. So, this is what we assume for the transitions on the gene state that one can be on, one off, or both on. Why not both off? I don't know. I guess this is simpler. And I think we, I just take alpha to be two. So, the idea is like a dimer of the gene, that the protein does the regulation. And then And then the birth and death for the protein for x and y just has very standard dynamics. And see, we have an epsilon in the degradation term, and that's why we get a large number of proteins in the limit. Epsilon goes to zero. So there's no mRNA in the model? No, no. For simplification, we. I have done some work on including the mRNA in other systems, but not this example. Don't you think that things like that are a big Think that things like that are a bigger effect than the difference in diffusion model? Absolutely. That's why we studied it. We're just taking this kind of one step at a time, including more and more stuff to try and develop this machinery. Actually, I think that the mRNA should be treated as an adiabatic quantity, too. I think it's probably pretty unrealistic to assume that you have a concentration. That you have a concentration, a large number of mRNA. But that's a different. Yeah, that's a different story, though. I'm not talking about that example today. Yeah, so that's where I tell you that Mary's outside. Azaro. Alright, so this is the sort of derivation of this from the point of view of large deviation theory, and this is the book I like for that. Like for that. Okay, so just a couple of quick definitions. Logarithmically asymptotic just means a really crappy approximation. So something that doesn't look great, right? You say put it on a log scale and there the approximation looks awesome. So I think that the core idea of large deviation theory, what gets applied over and over and over, is just Laplace's. And over and over is just Laplace's method. You have some integral, then exponential, and a large parameter, right? And the largest contribution to that integral is going to come at the maximum of this exponential, which is the minimum of whatever this function f of x is. And so you get a crappy approximation, a logarithmically asymptotic approximation, just by evaluating the integrand at that point. And of course, you can do better than that. But for many things, you can't. It's really hard to do more than that. You can't, it's really hard to do more than that, but you can get surprisingly far with just a logarithmically asymptotic approximation. So I'm talking about paths. I want to make the link between paths and this quasi-potential. So I need to start thinking about a path. So I'm just going to discretize some path. Five minutes? Okay, I got a cook. All right. So you can easily write down a chapter. You can easily write down a Chamokomogorov equation where you just integrate over all the different points in the path. And this object here is like a probability law. And for a Markov process, you can just take the product of the different transitions. But we want to average out the gene state, so then none of this is Markovian any longer. But that's okay. We can take some sort of a continuum limit and think of it as a path integral. And you can posit this Anzatz, like an exponential Anzatz for the law, and you get this action functional. And there's this Legendre transform. So all of this, of course, is drawing parallels to classical mechanics, but it's only mathematically similar. It's not like literal. But we just call them Lagrangians and Hamiltonians anyway. So, what does this give us? Well, first of all, you can get this sort of impractical limit that says if I have this function, this Hamiltonian function and its convex differentiable function of q, then we can say we have all these approximations and they're nice. But this isn't very useful for actually computing the Hamiltonian. But nevertheless, that's what we want to get at. And if we have it, then we can approximate the solution to our Champ-Gold-McGorov equation. Our Chen McGole McGorov equation, and we don't want to evaluate a functional integral, a path integral. So we're just going to use a Place's method. I find the minimum of this action functional. That's the path that minimizes it. And that's going to be the largest contribution. Notice the logarithmically asymptotic there. So I want to compute the path that minimizes that action function, and that's just going to be determined. I use the calculus of variations, just Hamilton's principle of something fancy. And get this system. Fancy, and get the system of ordinary differential equations. And along with that, there's a condition for the quasi-potential, except it requires the h equals zero paths. So the Hamiltonian is a constant of motion for this system, and if we set h equal to zero, then we get the special paths for which we can get the quasi-potential, and that's what I showed you earlier. So I guess my contribution was to. I guess my contribution was to derive this Hamiltonian using WKB from the full system. And you can just define it as an eigenvalue. These are just the matrices that appear in the Champico-McGrove equation. You can just write these down. And often you can actually solve for this eigenvalue. And if you can't, you have a couple of choices. You can use a numerical method to get this eigenvalue, which is actually not too difficult or computationally costly. Computationally costly. Or you can just take the determinant of this matrix here, ignoring this, and that actually works too. It's not convex, there's no reason it should work, but it does give you the same quasi-potential and it gives you the same trajectories. So as long as you're not working with a numerical method that requires the convexity of the Hamiltonian, you can just take the determinant of that matrix instead of taking its eigenvalue. All right, so just to quickly wrap up. So, just to quickly wrap up, we also apply this to a neuroscience model where we have fluctuating ion channels. There's two populations. The red are sodium channels, they're fast. The blue are potassium channels, they're slow. And there's this ODE that propagates depending on the state of the ion channels. So this is the fraction that are open. This is the orange curve here. This is the fraction of potassium channels that are open, the blue curve. And you get these sort of action potentials. Action potentials, and if the applied current is dropped low enough, then you just have a stable resting potential, and nothing particularly exciting happens, but what you can get are spontaneous action potentials. And maybe one will happen as I'm talking. There we go. So we can talk about the mean time for this type of metastable phenomenon, except that the deterministic Phenomenon, except that the deterministic system here only has one fixed point, so there's no bistable switching. So it makes some of the analysis more interesting. We also look at a problem, stochastic cavitation. This is inspired by a model of cell-cell contact. So suppose you have a T-cell receptor and it wants to bind to something on an anigen-presenting cell, but these transmembrane proteins need to get Transmembrane proteins need to get out of the way first. So, what's the first passage time for this region to be empty of random walkers? That's rare in the limit as the number of walkers gets large. So, we've also studied this problem. And yeah, that's it. You mentioned you took alpha equals to 2, meaning you have a nonlinear term here that makes it a demistic system bi-stable. And then you can introduce these quasi-potentials for the stochastic system, and you can compare the two. For something like alpha equals 1, you're not going to get biostability in the dynamistic limit, but as you transition and you have a stochastic system, you could still get something that spends a lot of time over here and a lot of time over there. So you get a quasi-potential that will look Quasi potential that looks different from the domestic domain. Have you played around with those types of systems and can you say something about those? Actually, yeah. So the Morris-LeCar model that I talked about towards the end is such a system. There's only one stable fixed point, but you can get these sort of transient, excitable events that sort of foray out here. So it spends some time out here. And the quasi-potential, it looks like a well. Quasi-potential, it looks like a well, and this sort of looks like a shelf, just sort of flat out here. So, yeah, I think that you can get these quasi-potential profiles, and they tell you a lot about the behavior of the system. So, I would expect to see maybe something similar. I don't know. I haven't looked at that. I was focused on the bi-stable switching. Let's take the speaker in.