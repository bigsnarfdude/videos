And thank you all for all the great discussions. So, what I'm going to talk about today is what is the crappiest experiment we can do and still get the right answer? And in order to do crappy experiments, we first have to learn how to do good experiments. And before we do good experiments, I also want to try to engage in good scientific communities and good science. And so, with that in mind, I'd like to start the talk with an acknowledgment of Henrietta Lacks. Acknowledgement of Henrietta Lex, whose cells we will be using throughout this presentation, and who we're taking from her without her knowledge, consent, or compensation. I want to express the gratitude and deep respect we have for her, her Spy and family members for their tremendous contributions to scientific progress and advances in human health. And at the same time, I'd like to mention a disclaimer. In this presentation, I'm going to be drawn on scientific concepts. On scientific contributions and artistic contributions from Ronald Fisher and Pablo Pagaso to white men who lived in substantial wealth and privilege and from whose work I have drawn substantial inspiration. However, in a spirit of respect for the sacrifices and contributions of Henrietta Latz and an interest in maintaining an inclusive scientific community, I believe it is first important to acknowledge and then condemn the extreme racism of Fisher and Massages. Racism at Fisher and the misogyny to Kadas. Okay, so with that, let's get into other acknowledgments. I'll make acknowledgments to my team as we get through the presentation. So I've been working with a lot of fantastic experimentalists who are great at collecting very good data and do very good experiments. There's some examples of some yeast cells provoking kinase signaling and then gene expression. And you can see there's a lot of heterogeneity in the GFP. In the GFP, despite massive, very, very clean homogeneity in the amounts of the kinase signal that's coming data. So that's looking at the signaling, the translation. One can be a single molecule fish, and we'll talk about that a lot more, where you can count the individual molecules of different RNAs, see where they are, how many they are, and do that for hundreds and thousands of cells, and getting good population distributions and see how those distributions change every time under different. Change over time under different experimental conditions. Here's work that I've been doing with my colleague Tim Stesovich at CSU. And in this video, we're looking at transcription. In that video, we're looking at translation. And here we're looking at transencryption in three different colors. The red color that's shown on the video is the messenger AB measured with MS2, the MCP approach. And the other two colors are different phosphoro forms of RNA. Phosphoro forms of the RNA polymerase showing as the fluctuations in time or the bursts of the transcription. And if you look really closely and do a little bit more analysis, you could get the autocorrelation and cross-correlations between these signals to see how quickly the polymerase gets phosphorylated and how much time it takes for transcription to occur. On the right side here, what we're looking at is individual molecules of RNA that are labeled in red. They're labeled in red, and you can see those diffusing in it's certainly not well-mixed environment. And this is a Gila cell, and we see different characteristics of these. Most of these spots are just red. That's indicating that they are not being translated. Whereas some spots have red and green collocalized with each other. That is a live translating messenger RNA, and the green is a protein that's being formed and then recognized by short animals. And then recognized by short antibodies fragments that are labeled in green. Here's one that's actually translating both in the canonical zero frame as well as in the minus one frame. So, very, very exciting experiments. All of these different studies collected a massive amount of data. And with these data, we're able to fit very precise computational models that are able to reproduce the dynamics and heterogeneity of these systems, both for different experimental conditions and different environments and different genetics, and as a function. Some different genetics and as a function of time. Whether we're looking at kinase transport, we're looking at the probability distributions, and I'll show some of these in greater detail a little. Or looking at, so this was back to the heli-cell transcription, the model and experimentally measured probability distributions for the amounts of the different phosphoro forms of the polymerase, as well as model and the data for the number of nascent RNA per protein. RNA per protein, as well as the experimental autocorrelations of all the three signals, which are the colors. And then the line is a simple five-parameter model fit to those to reproduce all the data and cross-correlations as well that establish the time scales of these processes. And then on the other side, doing just a simple TASEP, a totally asymmetric, simple exclusion process model, basically a traffic model or a move model. Traffic model or moving model to describe the transport of the ribosomes along the messenger RNA. With that, we can predict the number of basic proteins that are associated with each of those messenger RNA and their characteristic time scales and biocorrelations. Okay, all of these, this is a lot of fantastic work on the experimental side. They collected a massive amount of data, and when I wanted to start my own experimental lab, I knew that there was. Experimental lab, I knew that there's no chance I could ever compete and collect data of this sort. And so we decided to go for crappy experiments. And as I said, in order to, well, I guess I should show the master equation. I hope we've seen the master equation a couple times here. So this is back in 2006, Mustaf and I developed the finite state projection approach to approximate the master equation with known variables. Known error bonds. With this, we can also take the sensitivity. We've developed a Ford calculation for the sensitivity of the master equation solution desk, and we can use this to ask how likely is a set of experimental data. Now that we can solve the master equation for p up here, we can ask what is the likelihood of some data given a model that's parameterizable, parameterized theta. And for a single molecule fish experiment, at least, these. Fish experiment, at least, these are independent. Each cell is measured only once. And so the total log likelihood is going to be the sum over every experimental theta or the lack. And with that also, you can ask what is the, because having a sensitivity, Kelso asks what is the derivative of the log likelihood function with respect to the parameters or the sensitivity. So that comes in very, very, very simple form. That involves the sensitivity calculation from here. Sensitivity calculation from here and the probability distribution calculation from there. And so now we can ask what is the gradient of the log likelihood, and we could use that for parameter estimation to search for this problem. Okay, so now we've got a mathematical tools, and I took inspiration from my colleagues to do some experiments. So I wanted to do my first set of experiments in my lab. So I hired Linda from Tim's lab, and she helped build a single molecule fish in our lab. Fish in our lab. And here we're looking at the temporal response of the anti-inflammatory gene DUS1 when activated by a corticosteroid, DEXA menfosol. Basically, you give DEX, it caused the gluten corticolore receptor on what we've called GR from metal on to translocate into the nucleus. It's going to act with a responsive element. It's going to activate transcription of DOS1. And we can go in and measure and count spots that are a little easier to see here. To count them. Here, to count the number of messenger RNA. And we can do that for thousands of cells and many, many different replicas. It's a very reproducible experiment. Each time you give the DEX, you get a probability distribution. Well, prior to giving it, most of the cells have a very low expression of DUS1. After you give it, it turns on, and then, as you can see, it's slowly turned back off over time. Very reproducible experiments, the replicas were reproduced. Replicas reproduce the solution of the master equation very, very nicely. And the master equation fits that is shown in gold. The experimental data there are all shown in blue for one replica, but every other replica looks basically this one. Why is it important? So, Dust One is an anti-inflammatory response system, and so if you your cells don't have activated dust one, they're not going to respond. Activated dust spawn, they're not going to respond as much to inflammate. Oh, I'm sorry, they don't have dust spawn, they are going to respond to an inflammatory signal. And so, give it allergen, these guys are going to get itchy. Big picture. Okay, so from looking at experiments like these and the ones I showed with my collaborators, we found that transcription and translation occur in random bursts that can be measured at single-cell resolution, different bursting mechanisms. Mechanisms affect the measurable statistics in subtle ways, and that can be used to infer what those mechanisms are. And by testing different models with different mechanisms and different experimental conditions, different stress or drug response conditions at different points in time, et cetera, and so forth, the fluctuation finger widths, the shapes of those distributions, how those distributions change over time from real feeling sight into which bursting mechanisms are more likely and which bursting mechanisms are less so. Lesson. That's really cool, and I really value, I love doing that with my collaborators and all. But when I started doing my own experiments in my own lab, I realized all the details that I kind of was aware of, but not really. Single cells are very expensive. I knew they were noisy, that was the whole point. But we were thinking intrinsic noise, not the noise in what actually comes to us as a data point. And they're vast. Points. And there are vast numbers of different possible experimental designs that one could consider. Some of the experimental design considerations, this is just a tiny fraction of those, the number of cells that we are measuring, the sampling time periods, the sampling times periods at which we measure them, which fluorophores we use, where we place the probes, which genes RNA that we actually measure in the first place, what drug conditions we give them, and so on and so forth. And then also measurement error considerations that we had to take into And error considerations we had to take into place is we have many options in which microscopes we're going to build or buy, what resolutions we actually image under, how we do the image processing. I was surprised really how big an effect that actually has on these calculations. The photo bleaching that comes if you're doing lifestyle stuff, autofluorescence, it's disruptive in fluorescent signals that you could look at, the exposure time that used to look for your camera, and so on and so forth. All these different things are each one of these is going to affect what it is we think we're measuring. Is going to affect what it is we think we're measuring about the cells. In order to get reliable and reproducible insight from these kind of microscopy studies, we need to account for not just the cellular heterogeneities, the stochastic processes that we're interested in, but also all these experimental uncertainties that are leading to distortions not data. And at the same time, because you have so many choices to choose from, experiments are going to need to be designed to minimize the expected uncertainty. You need to minimize the expected uncertainty of the model after you do that and in the context of all those measurement errors that essentially are available. We're going to talk, in this study, we're going to talk a little bit about choosing the number of cells, the sampling times and periods, which fluorophores and artifluorescents, but I'd like to say that all of these items mathematically can be written in the context of the same formulism that we're going to use. Okay, so to give another kind of motivation. Okay, so to give another kind of motivation and show how important the choice of fluorophore can be, the first experiment that Linda did in my group was to simultaneously measure the same cells at the same time using two different fluorophores, so two different mechanisms for measurement. The first she's going to use the MCP GFP approach, where you've got these hairpins that are recognized, these are the MS2 hairpins, and they're recognized by the MCP. Recognized by the MCP fluorescently labeled molecule in order to get a signal. I'm sorry, that's the second one. Right now we're doing fish. We've seen a little fish already. So this, she's doing fish in purple. So then it just assigning these probes to bind to the different regions, overlapping with the answer. And with that, we can go in and count and we can see where the transcription sites are. We can see how many messages are RNA partners to those cells. We can quite count. Counters of the distribution. And at the same time, she also did the MCPGFP approach for the same cells. And this is the same individual site. See the transcription site gets both labels. But if you look closely, there's a lot of cells shown in spots shown in white where you're getting both signals. But there's also a lot of spots that are showing up just in the MS2 channel and spots that are showing up just in the fish channel. Anything we actually tried to adjust. We actually tried to adjust all the image processing so, in order to get these to be as close to each other as possible, so we're getting the same number of spots for each. And the distributions match okay to one another, although the fish, there's a lot more cells that have zero spots than fish. We could take that and we could plot the basically a correlation of the measurement we get with fish versus that we get with MCP, get a nice scatter plot. It's clear there's a strong correlation, but again, there's a bunch of cells. But again, there's a bunch of cells that only worked in one of the two assays. You can't really say which assay it worked for, but there was a zero measured for fish and, like I said, 100 molecules measured using MCP. And there's quite a lot of variability, even though it wouldn't be the same cells. We tuned that to be as close as possible. In order to account for this, we developed something that we're going to call a probabilistic distortion operator, PBO, which essentially is just a Markov kernel. It acts on the It acts on the true probability distribution, which we obviously don't know usually, but in this case, we're empirically assuming that it's the one measured fish. And this distortion operator acts to the kernel that gives us the observed distribution, the thing that we're actually sampling in the microscopy that we do. And we could make assumptions for different models of that distortion operator, and that's what's shown in the colors here. We parameterize an empirical distortion operator with. An empirical distortion operator, which we learn from these data, and we try different models, and then we use the VIC criteria to choose the distortion operator that is best supported with the given data. And we've confirmed that through cost-validation, it also does the best to predict y from x in my whole base. That distortion operator can be used, and it's still pretty good for other experimental conditions, as can be shown here. This is just Shown here. This is just a different point in time after a transcription suppression response. And that's just one distortion we could look at. We could also look at other much more severe distortions for various reasons. Here's a distortion where instead of counting the number of spots, we don't even do any spot detection. We just integrate the fluorescence intensity as if you were doing a flow psychometry experiment. And once again, you can see that there is clearly a correlation. You lose a lot more information through this distortion. A lot more information through this distortion, right? There's not as much correlation, but it's still consistent between the different experiments. And so, this is the distribution you can measure with fish, and this is the distribution that you're measuring with fluorescence. I mean, the units aren't even the same, but you can see that through the distortion operators, if you were to take these data, apply the distortion operator, you get this curve. And so it's a good prediction of what we would have got from the fluorescence. We would have bought from the fluorescence, and now that we have this calibration, we can just run a model for the fluorescence. We don't have to do spot detection anymore with some loss. Okay, so now how do we deal with what the loss is after applying this kind of distortion operator? And for that, we now need to modify the finite state projection. So this is a slide from before. And the only thing I'm going to do here is instead of P, I'm going to substitute C times P. So very, very simple mathematics, just a linear operation applied to C. Operation applied to C, I to P. Same thing for S. It's just a linear operation, but it's a little bit more complicated because you have to take the derivative with respect to theta. And if C depends upon theta, you get the sensor. Once you have that, you just plug the, oh sorry, I should do C times P there. And, oh no, it's P hat. P hat, yes. And then S hat there. Otherwise, everything else is. Otherwise, everything else is all the same. Now we can use, now we've got a sense of this is a really important construct. This is the building block of Fisher information. That's the structure. So Fischer information matrix is defined as the expected value of all data that could be generated from the system. Yeah, three to four minutes. Okay, of that derivative. We're essentially looking at the sensitivity type thing. Yeah, this sensitivity calculation. All right, it's easy to calculate using the things that we've already calculated on the previous slides. And this is why it's important, sufficient information, asymptotic multivariate estimate for the results of an unbiased estimate. If theta star is the true numbers that you're trying to estimate, theta hat is what you get from your maximum likely estimator, then the difference between those times the square root of the number of experiments you do is going to be normally distributed with zero if it's an unbalanced. With zero, if it's an unvalued estimator, and that covariance matrix equals inverse of the fisher information matrix. In other words, if this is the spread of how your parameters are going to identify what they are, the eigenvalues and eigenvectors of the fisher information tell you the directions of the most uncertainty you're likely to encounter, which corresponds to the smallest eigenvalues of fisher information, and the directions where you're going to get the most information, or the least uncertainty corresponding to the highest eigenvalues of the fisher information. The eigenvalues of the fission information power. Okay, so let's do experiment design. You choose the bigger fissure information, that leads to the smallest predicted ellipses. And just to verify that this is working, here is our original data set. This is a metropolis-hasting posterior quantification of the parameters given the data. And then we calculate the fish information and overlay it, and it does. It's not guaranteed to do this every time. Proof the statement I said is a little different than this, but it's still a nice validation that Fisher information is giving you the extent of the uncertainty that you're likely to see. And now we can go back to this experiment and say, well, what would happen if we did the crappier experiments instead? Like, for, well, we'd use visual information to say when should we have measured things? And we could find if we did the same number of cells using just those time points, we would get more information for an easier experiment. For an easier experiment. So the ellipses get a little bit smaller. If we were to do this distortion operator, we don't lose much information at all, so long as we account for that. It gets little, little worse, but not much worse. If we use this much more distorted distortion operator, we're going to get a lot less information and we're going to lose our parameters. But this is a heck of a lot easier experiment to do since we don't have to do any swap detection. It's easier to analyze it if you've done it lower resolution. Easier to analyze it if you've done it lower resolution. And then, if you just doubled the number of cells, you would go back to the original amounts of information. So, do the cheap experiment, but do more of it, you could get the same amount of information. Okay, so finishing up, measurements have measurement noise. You have to account for that. Theta there. And so, this were our lab first experiments. I like to think of that as the early Picasso, which is a case of a teenager. Basically, a teenager. And this is our current, our future experiments, which are going to be a lot messier. And I like to think of that as the later Pakasa. And for anybody in the audience, I suspect you could immediately recognize where this came from. Nobody would have known the shotguns. So doing crappy experiments on the right-hand side is how I hope to make that sound. Okay, all this, all software for this is available. All this software for this is available. It's freely online. It's MATLAB-based to do all these different things, to generate models, solve them, discrete stochastic models, do reductions based upon course meshes, time scale separation, places, assumptions, POD, hybrid. You think of it, you let me know. I could encode it back in model fitting, in data lessons, and so everything takes just a few lines to code to from starting with the model fitting data. And that's that's all. That's that's all. Yeah, sorry, I'm a little excited. Does this also work with deterministic models as well? Like, can you apply the same approach? So the official, you need to. Sorry, distortion matrix idea. Ah, the distortion matrix. It depends on the type of distortion in that case. So you need to know how the distortion or You need to know how the distortion, or parameterized form of how the distortion could affect the measurement of, say, your mean, the thing you're fitting the deterministic model to. Unfortunately, you can't really derive a deterministic model directly from the master equation. The probabilistic distortion is, it can allow for any type of distortion. White cells that just didn't work. How do you convert that to a change in the meaning? That would require. That would require solving the entire master equation and applying more complicated distortion and then reducing that down to the distortion equation. Gaussian. Gaussian distortion. Oh yeah, it's already done. That's the original formulation. That's the original formulation that used a very easy calculation. Okay. Just put the formula to the easy way. One last question if we can start switching computers. Yeah, so if the light we have Yeah, so if the likelihood function is it's a function of the distortion operator, do you think there's any goal like do you think you could like take the gradient of your likelihood with respect to the distortion operator and like directly optimize your distortion operator?