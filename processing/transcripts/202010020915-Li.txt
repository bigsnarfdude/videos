Thank you, Lily Lee, for the last talk of our workshop. Lily is a PhD student at the University of Toronto, and she's going to talk on the computational complexity of linear discrepancy. Thank you very much, Lily. Go ahead. Thanks for the introduction. So, okay, let me share my screen. So, hopefully, everyone can see my screen. Okay, great. Yes, we can. Excellent. All right. So, today I'll be talking about some joint work with Sasho, wherein we discuss results in linear discrepancy. The concept was first introduced by Spencer, Lovas, and Vestergambi in 1986 in their paper, The Discrepancy of Set Systems. Their paper, The Discrepancy of Set Systems and Matrices. So, before I go into talking about linear discrepancy in depth, I'm going to give a motivating problem. So, suppose we're given as input an array A of n real numbers. We're going to let S be the array of subset sums of the elements of A arranged in increasing order. Arranged in increasing order. So hopefully you can see my mouse. Here we have S on the number line and all the subset sums. Notice elements such as 7 here, which are the result of two different subset sums, are only represented once. So what we want to do is find the largest gap between two consecutive terms in S. So here the largest gap would be of length 2 and it's Be of length two, and it's achieved at two different intervals. Okay, so that's maybe a question you should keep in mind when you're listening to my presentation. Before we give the solution for this problem, we're gonna give you a few reasons of why you should care and some definitions. So, first a note on point distributions. So, this is going to be an application of linear discrepancy to geometric discrepancy. To geometric discrepancy. We are going to take as input all axis aligned boxes in the unit hypercube denoted R sub D and the leg measure lambda sub D on R to the D. Okay, we're going to output points at P in the unit hypercube, which minimizes this supromium here. Okay, and this can be modeled by the following rounding problem. So we are going to So we are going to discretize the unit hypercube to a finite set X of size n. And we're going to take an incidence matrix A. So in this incidence matrix, every rectangle R in R sub D is going to be represented by a row. So when we take W to be the all-once vector scaled by little n over big N, AW is a close approximation of N. close approximation of n lambda sub d of r. Okay, and this is basically the area of the rectangle r scaled by n. What we want to do is find an endpoint set p which minimizes the geometric discrepancy and this is equivalent to minimizing the difference between aw and ax under the infinity norm. So in particular we're going to take x to be the indicator vector of p. And when this happens the number of points And when this happens, the number of points in p differs from n by at most the infinity norm. And we get this by taking the unit hypercube to be our rectangle. And the maximum over all w of the minimum over all x is exactly the linear discrepancy. And it follows that the discrepancy of p is at most twice the infinity norm and hence at most twice the linear discrepancy. At most twice the linear discrepancy. So it's an upper bound for geometric discrepancy. Next, we're going to consider the integer linear programs, and these consist of three parts. So we have an objective function here, a set of m constraints here, and a set of integer-valued variables here. And these are going to take on values 0 and 1 for my purposes. So integer linear programs are So integer linear programs are typically hard to solve and we will only be doing so approximately using the standard paradigm. First, we're going to relax the integer linear program to its corresponding linear program with values taking over the reals. Next, we're going to solve the linear program and then we're going to round the linear program solutions to integer values. We're going to use linear discrepancy for this rounding step. So, this term here. Okay, can't draw a straight line. So, this term here represents the difference between the rounded integer solution and the linear program solution. Going to be On each of the constraints, and if we take the maximum over all possible real value vectors w, then we have the linear discrepancy. So, in particular, the linear discrepancy of a matrix M of dimension M by N is the largest gap between all realist solutions and their closest integer counterparts. This relaxation and rounding paradigm has proven quite useful for designing approximation algorithms for hard combinatorial rounding problems. So, the result of Roth Voss in 2013, as well as Hoberg and Rothvoss in 2017, use this technique to obtain an additive logarithmic error for the bin packing problem, matching certain combinatorial lower bounds. For this and other reasons, we want to better understand the computational complexity of linear discrepancy and Of linear discrepancy and design algorithms which either solve it exactly in some restricted cases or approximately. Okay, so let's think about the case where m is equal to 1. So in this case, all the matrix vector sums appear on the number line. And for any 0, 1 vectors x or x prime, the product is a subset. The product is a subset sum of m. So we would have, say, like mx here and mx comm here. So the real value vector w, which maximizes the largest gap, is achieved by taking mw to be the point in between two consecutive subset sums, which achieves the largest gap. So like if this is a largest gap, then we would have like m here. You're here. Thus, the linear discrepancy is half the length of the largest gap. This would be the geometric interpretation of linear discrepancy. And twice the linear discrepancy is the length of the largest gap between two consecutive sums of the elements of x. So this is twice. Right, so before I go into the proof of my original motivating question, a summary of my results, our results. So we solve the problems in a restrictive case, in particular when d is equal to 1, and we also find the hardness of linear discrepancy. Finally, we have an approximation result. In the approximation result. In the presentation here, I'm going to focus on the second item. So here's my claim. Given an array A of n positive real numbers sorted in decreasing order, we can compute the linear discrepancy of A in linear time. So we're going to recall the definition of linear discrepancy. And this result is actually kind of surprising. So if you recall the definition of linear discrepancy for one row, Row. For a fixed W, if we compute the linear discrepancy of M for this W, this is actually the same as the subset sum problem. And some of you might know that this is MP-hard, which means it's very unlikely to be solved efficiently. So you would expect that the one row case for linear discrepancy would have the similar type of complexity, but that turns out not to be the case. out not to be the case. So before we begin there's two things to note about the claim. First all elements of A are positive. However by modifying the proof that we will soon show this is possible to it's possible to remove this restriction. So you should see our paper for the full result or consider as an exercise. Next we're going to know that A is sorted in decreasing order. If the input array is not already sorted doing so will only take n log n time. Doing so will only take n log n time. And this time will dominate the running time of the solution. So, on to the proof. We will need the following lemma. So, let A be the array previously described. It contains positive entries and they're going to be sorted in decreasing order. Then we have the following relationship between the linear discrepancy of the prefixes of A. In particular, we consider the largest gap between two constituents. We consider the largest gap between two consecutive opposite sums of the entries in the prefix a sub k. This is the larger of the current entry, this guy, and the difference between the previous largest gap and the current element. That's this guy. So we're going to apply the lemma to our example from the beginning to just get a better feel for what's going on. Okay, so recall that this is our Recall that this is our example from the beginning. We're going to start with k equal to 0, and this is going to be the largest gap, and it's going to be of sine 0. When k is equal to 1, we have two possible subset sums. This is going to be 0 or 7, and thus the largest gap, and equivalently twice the linear discrepancy, is 7, as we can see here. When k is equal to 2, the largest gap should be the maximum of the current entry. Current entry, and that's five. Sorry, that's not right. This is the five that we're talking about. And seven, so that was the previous. And five is larger, so the largest gap in A2 should be of length five. And we see two such intervals of length five. When k is equal to three, the maximum of the current entry, two, and the previous largest entry, Largest entry five is three. So three is and sorry, is that a question or just noise? I guess it's just noise. Okay, fine. Anyway, moving on. Right, so the difference, the larger of the two is going to be three. And we see two gas to size three. And there's one more. And when k is equal to four, the largest gap would be the size of the current entry. That's two. And the gap. That's two, and the gap, uh, the difference between two and one, which is two. So, two is bigger, and so we see two gaps aside two, which gives us the right answer. So, that makes me feel good. Okay, moving on. Okay, good. So to prove the lemma, we're going to show that the inequality holds in both directions. In the first case, we're going to show that twice the linear discrepancy, or the size of the largest gap, is bounded above. The size of the largest gap is bounded above by this maximum. So we're going to consider two consecutive subset sums, Si and Si plus 1, in the matrix A sub k minus 1. Okay, and this is going to be at most twice the linear discrepancy of A of k minus one. And now we're going to suppose the interval is greater than a sub k. So this interval is greater than a sub k. Then after adding a sub k, the interval is divided into two. One of length a sub k, that's this guy, and one of length of length s of i plus 1 minus s of i minus a sub k. And this is at most twice the linear discrepancy of a sub k minus 1 and minus a sub k. Oops, a sub k there. Okay, that proves the second. Okay, and that proves the second, the first item. So, for the second inequality, we're going to consider two different cases. If AK, the current element, is the larger one, or if it's not the larger one. So the maximum is, or sorry, the difference is the larger one. So in the first case, if AK is the larger one, then we can find two consecutive subset sums as follows. So the elements are sorted in decreasing order. the elements are sorted in decreasing order, so a sub k minus 1 is greater than a sub k. That means 0 and a sub k are consecutive. So here's an interval of length a sub k. Now we're going to suppose that the difference is bigger. We're going to consider two consecutive subset sums of the elements of a sub k minus 1. So we're going to say si is a sub k minus 1 x one x and five plus one is a k minus one x prime. Since a k is strictly less than the difference, the element s sub i plus a sub k is a subset sum of the current over it. And suppose for a contradiction that s sub i plus a sub k, this guy, and s sub i plus one are not consecutive. sub i plus 1 are not consecutive. This would mean that there exists another subset sum of the array a sub k in between. Since these two elements, s sub i and s sub i plus 1 were originally consecutive in the previous matrix a sub k minus 1, that means this element must contain a sub k. Right, so Right. So that would be this. Now consider the sum S sub j plus A sub k without that term A sub k. This is going to be the element S sub J. But because this difference is less than A sub k, we're going to end up within this interval. And that's impossible because, again, S sub i and S of i plus 1 were supposed to be consistent. And S of i plus 1 were supposed to be consecutive. So these two elements can't exist, and so this interval is the length of two consecutive subset sums. We are going to apply the lemma iteratively, and this would prove our claim. Okay, so again, onto our previous results. Sorry, onto some other results. So first, the hardness result. So linear discrepancy is MP hard. So, linear discrepancy is MP-hard, and again, NP-hard means the problem is not efficiently computable. This is not too surprising because a closerly related problem, which is the covering radius problem on lattices, is pi too hard. We have also an exact result in the restricted case where D is a constant and the entries of M are bounded in magnitude. Note this is weaker than the extension of our one-dimensional result because that works for real matrices. That works for real matrices. And finally, we have an approximation result. So, finally, some directions for future work. We'd like to improve our hardness result for linear discrepancy. And in particular, we would like to know if it's MP-complete, that is the hardest instance of an NP class, or perhaps pi two-complete. Though again, the latter is more likely because of the similarity between linear discrepancy and the covering radius problem on lattices. Covering radius problem on lattices. We'd like to improve the approximation gap and we'd like to extend our exact algorithm for the one row case to the exact algorithm for a D row case. We tried doing a couple of weird things and they don't work. So new ideas would be much appreciated. And that's it. Great. Great, thank you very much, Lily, for your talk. So it's the last time that you have the opportunity to ask questions, and I see that Matthew de Coursi Arlt raised his hand. So Matthew, go ahead. Can you say more about the weird things, please, in the tessellation picture? Oh, sure. They look nice. That's mostly why I added them. But they're also linear discrepancies. There are also linear discrepancy where instead of doing L2 norm or L infinity norm, I do an L2 norm. And this is equivalent to sort of finding the Voronoi diagrams. And so I'm sort of like trying to tessellate the space with a bunch of different Voronoi diagrams and trying to figure out how this might change basically the size of the largest ball which fits inside. And it doesn't behave very nicely in like the one-dimensional case. So that's kind of why it has. Dimensional case. So that's kind of why it hasn't worked. Thanks. No problem. Are there more questions? I would again have one question which would really interest me. And it's, is there any uh maybe it's a dumb question, but um I wonder if there's any connection to Seremba's conjecture. Excuse me? Saremba's conjecture. Seremba's conjecture. So it's about partial quotients. Oh, I'm not very familiar with this. Yeah, that'd be interesting. We should talk more about that. Yeah, I have no idea if it's connected. I just wondered when I saw your problem. So wait, how do you spell this? C-A-R-E-M-B-A. I'll yeah, so there you Ah, seriously. Yeah, no, I'm not familiar with this. Yeah, no, I'm not familiar with this. Sorry, I've heard of it, but I don't remember the exact conjecture, and I also don't have an answer. Sorry, interesting. Something about boundary coefficients in continued fractions. Yeah, if you have a given alphabet, it asks can you get any denominator, continued fraction expansion? That's very roughly the question. Okay. Yeah, sorry. Okay, yeah, sorry, I'm not familiar. Yeah, it's still open and it's very famous, I think. Are there more questions? There you got the link to the famous paper, I suppose. Yeah, this is some partial progress from the late. Some partial progress from the late, great Jean Bourguin with Alex Katorowich, they proved parts of it. Are there more questions? If not, then let's thank Lily once again. Thank you very much. And I'll stop the recording. And I'll stop the recording. So