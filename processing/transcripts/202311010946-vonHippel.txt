The order of thoughts and David Van Danik will be talking about stratified learning, improved learning under covariate shift. Thank you very much. So I wanted to give a little bit of an advertisement of a recent work that I've done with a couple of people are here, Max and David Stenning, and also my co-author Roberto Trata. And the question is, can you learn about a population from a sample that only partially represents the population? Of course, that comes up all the time in astronomy. It's a general statistical method. I'll describe it a little bit. Statistical method: I'll describe it a little bit if I have time at the end to go through some of the examples. But we're really here to look for additional examples. So, if this rings the bell for you, please talk to one of us later in the meeting. So, what are we interested in? We're interested in a case where we want to do some sort of prediction or classification based on a training set, but that training set doesn't fully represent the full population where we want to do inference. So, here's a classic example: PhotoZ, and you can see this photometric data. And you can see there's photometric data and spectroscopic data, the spectroscopic data where we know the relationship between the photometric data and the redshift tends to be dimmer, I'm sorry, brighter sources than the photometric data where we want to use predictions. There's a difference that we need to account for in our statistical methods. So, a little bit of notation. We want to use a training set that has, you know, the predictor variables here, the photometric magnitude that we want to predict redshift. And we learned. And we learn the relationship of the training set. You want to apply it in the target set where we want to learn why the redshift from the photometric magnets. It's okay, classical supervised learning. Okay, so formally, we're in a situation where we believe the relationship between the variables is the same with the training of the test set. So, you know, the relationship between photometric magnitudes and the redshift is the same, but Is the same, but the distribution of the photometric magnitudes is different in the two sets. So that's what's known as covariate shift. Here's another example. We want to classify supernova. We want to identify type 1A. This, the gray points, this is a whole data set where we want to do a prediction where we have classification, again, based on spectrometric data. Spectroscopic data is different. It tends to be brighter and nearer sources. And again, we want to try to account for that difference. Want to try to account for that difference in our predictions. So, how we're going to approach it is we're going to take a page out of a different part of statistics that I think I've ever seen in astronomy before, and that is causal inference. So, you know, the classical statistical problem in medicine where you want to see if a new drug improves health outcomes. So, what do you do? You split your patients, your subjects, into two groups. There's a test set, I'm sorry, not a test set, the treatment group and the control group. The treatment group gets the new drug, the control group. Group gets the new drug, the control group gets a placebo. But what if those two groups are different, right? What if the treatment group happens to have older patients or more healthy patients? Because the physicians were worried that the treatment might be a little bit difficult for patients, so they tend to give it the treatment to the more healthy patients. Well, it's clearly going to bias your results. So, what do we do? The classic solution is to randomize the gold standard, flip a coin, and decide who gets the treatment. Trinket and the transbo. In astronomy, flip a coin to decide which sources you're going to follow up spectroscopically. Of course, that's possible, right? Same thing in biomedical. There's a lot of cases where you can't flip the coin. You're trying to figure out if smoking posits can't flip a coin. So a classic method to deal with this, Rosenbaum and Rubin propensity score, they find a propensity score is the probability that you're in the treatment group given the covariance. Make root given the covariates, given the data that you observe for every bike. And they showed that this thing is what's called a balancing score. That means if I condition on the propensity score, then the distribution, the conditional distribution in the control group and the treatment group of the covariates is the same. So that's the magic. And what's really nice about this, and that's what we want. We want things to be comparable, right? We want the distributions to be the same. The distributions to be the same, and what's really nice about this is that the x's are observed, so I can check this. So, you know, in practice, we have to estimate this thing, we have to identify the right set of covariates for this to be true. So, we, but we can see if we've done a good job up here by just looking at the distribution of the covariates in the two sets. Okay, so bringing it forward to our problem, Stratlern, what do we do? Well, just like I described, we estimate the propensity score. Now it's the probability. The propensity score now is the probability of being in the target set given the covariance instead of the probability of being in the treatment group. Again, we can check and make sure that things are balanced, that the photometric, the distribution of photometric magnitudes, for example, looks the same in the treatment group human test shed. So the way we do that is we stratify. So this is an example from the supernovas, which is stratify on the propensity score, high propensity score, low propensity score, middle propensity scores, and within each of those. And within each of those groups, we look at the distributions of the photometric magnitudes and make sure they're the same. This is an idealized toy example. We're only looking at p-variables. Actually, we look at a whole bunch of variables, so things are a bit more complicated. So again, we stratify and then we classify data separately. So within each strata, we do our analysis. So we do the same kind of photo Z analysis that we would have done, but we do it separately in each of the five strata. In each of the five strata, and we do our prediction that way. So that's the method. Quickly go through a couple of examples. How much time do I have? Oh, okay, still a few minutes. Let me just go through one example. Let me go through one example quickly. This is the example we're working on right at the moment. Weak gravitational lensing, you know, the story where we have a giant mass along the line of sight and it distorts the shapes of galaxies in the field of view. In the field of view. Shear tomography bins the galaxy on photoZ to map the 3D distribution of the mass. The resulting estimates of the cosmological parameters that you get from this method in the Lambda-CDM cosmology model are inconsistent with CMD, the cosmic microwave background, whatever that is, cosmic microwave background. A lot of certain bias is that this binning is not done well, and that's where we come in, right? Find an estimate. That's where we come in, right? Find to estimate the photo of Z and do the binning and learn about the population distribution of Z better within each of the bins. So we use Strat Learn to improve the tomographic binning of the galaxies on PhotoZ. So we get better estimates of PhotoZ. And then we estimate the Z distribution within the bins, adding a nice Gaussian hierarchical model. So let me just show you the results of that. This is the best classification. Classification: so again, tomographic bins on the redshift, these are the uh the actual uh redshift bins. There's five of them, these are the predicted ones. So, we'd like things to be along the diagonal. This is the best existing method before we came along. This is what we've done. It's a lot of numbers here, but if you just concentrate on the diagonal numbers here, they get significantly higher here. So, we're getting things in the right bins. And the other result is we want to know, we want to know about the distribution of the redshift. Distribution of the red ship within each of those bins. And we need to estimate that with really high precision. And we're able to remove about 40% of the bias from them. And then that's the end site. Well, thank you for your attention, Al. So all the other examples you can see, she downloaded the slides, but there's the references for the papers. Thank you very much. Are there questions in the audience? In the audience, um, loads of questions, um, but uh, so initially you would talk about populations, so you're able, I would ask two questions: one are you able to get at that population distribution, and the other one being, um, is there a, I'm going to say, is there a design of experiments thing here in the sense that, okay, to condition on that data, I would like to say what I'm going to do next. Is there a, yeah, no, yes, I mean, but I've never listened to what I say exactly that, but I mean, what I Say exactly that. Well, I mean, what I would say is: you look at a picture like, you know, any of these pictures, but this one's kind of showed the where's of this one. What you really want to say here is: can you just give me some spectroscopic data or down here? Right? I mean, I don't know. That's the answer always seems to be no. That you would get a lot of power in your predictions by getting a little more data, just a little bit more data down here. Data down here, you know, you should see you can't get spectroscopic data because it's expensive. Get spectroscopic data because it's expensive. The value of your spectroscopic data here is much higher than it is up here, right? So, it's clearly there is a design of experiment element. Whether or not we can kind of make it happen is another question. Your first question, population. So, I mean, mostly what we're interested in is learning the prediction for individual units. In this pro, I don't have the results here, unfortunately, to show you, but in this problem label. In this problem, they also do want to learn about that distribution, and then we so we do a weighting thing to learn about the distribution. It goes a little ways away from this methodology, but that questions is also on their mind, so we can do that as well. But then, you know, where we'll weights can be very variable because the denominator in these groups where you have low probabilities, and that's the thing in the denominator. So, our strength comes from getting away from those weights and using stratifying. And using, you know, you know, stratifying instead of actually looking at those, but so there's there's there's issues on how we have, but we do.