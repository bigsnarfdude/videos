Today, I will talk about compression of enumerations and gains. So, basically, that means that if you are given a recursive enumeration of a certain I set, and we would like to compress the information given in that enumeration in a more tight way. And we also care about the gain, which is the additional information introduced in this process. In this process, and everything here will be in terms of RK degree, which I shall define later. So, let's first of all look at some of the basic definitions. If we have a finite string sigma, then we will use C sigma or K sigma to denote its plane Komograph complexity. Basically, that means the shortest program that halts and output the sigma. And basically, C sigma is the length of the shortest program. The shortest program. And if you choose the universal program to be prefix-free, then you get the prefix-free version of Komograph complexity. And also, it makes sense to talk about relative Komogorov complexity, where you could put an oracle tau here, which is also a finite string. So Komogorov complexity of sigma given tau is basically the shortest program that could output sigma while reading tau. Output sigma while reading tau as an oracle. And in this talk, we will be particularly interested in the subsets of omega. Let A be a subset of omega. We will care about the initial segment complexity. So the initial segment of A up to n is basically the first n bits of A. You could regard this as a set which contains all numbers of A that is less than n. That is less than m. Or you could regard it as a binary string a0 a1 up to a n minus 1. So this a should really be the characteristic function of a. And this becomes a 0, 1 string. And as it is a string, we could consider its comogorof complexity. And basically, we are interested in this particular function that maps A to the complexity of the first n-base of A. First embeds of A. Roughly speaking, this describes how the complexity of A grows. So, with these notions, we are ready to define some central reducibilities that happens in the complexity theory. First of all, we have these upper two reducibilities, C reducibility and K reducibility. Basically, that means the initial segment of some set is dominated by the other one. Dominated by the other one. So, this, like, for example, for the C reducibility, this says that the first n-base of A is kind of simpler than the first n-base of B, but it does not tell you how you can compute the first n-base of A given B. So there is no operator here, and this is really a very weak reducibility. And now we have this RK reduction, which is which will play the Which is which will play the main role in today's talk? That means that the Kombogorov complexity of the first n bits of A relative to the first n bits of B is constant. So that means if you are given the first n bits of B with the help of some additional information, which of constant amount, you could recover the first n bits of A. And as here we care about how the initial segment. How initial segment complexity grows, we will be also interested in this two particular Turing reducibility, which with oracle use bounded by identity or identity plus a constant. So like for example for IBT Turing degree, given the first n bit of B, you could compute the first n bit of A. But what's the difference between RK reducibility and IBT? Reducibility and IBT. It is that with IBT, you could definitely output the correct answer of the first n-base of A. But with RK, you could only get constant number of guesses for the first n-base of A. And you need the help of some additional information to recover the truth. And this additional information could be non-uniform with respect to the index N. The index now, what we have here is some trivial implications for these five reduction symbols. Basically, someone is stronger than the other one. These could be easily verified from the definition. But what is interesting is this one, which requires a little bit of work. It says that RK reduction implies Turing reduction. If you look at the definition here, RK reduction is really. Here, RK reduction is really defined by common growth complexity. It does not say that there is an operator, but we really have one. So we have this equivalent characterization of RK reduction. It says that if you are given two subsets of omega, then A is RK reducible to B if and only if there is a partial recursive function, which takes two inputs. One of them is the first. Inputs. One of them is the first admins of B, and the other one is the additional constant amount of information. So there should be a uniform constant K that bound the choices you can make. And one of these K-many guesses, you only reply that one of them is correct. You don't know which, but so basically, one of them halts and outputs the first. Pulse and outputs the first n bit of A. You could also regard this archive functionals that takes the first n bits of B as input and output the set of all these values. This could be a uniformly recursive enumerable set that guesses the first in bits of A. Now let's come to our motivation for this work. Are the partial orders of z Are the partial orders of these degrees within the RE sets? Are they dense? So there are some previous results about this. It was shown that the IBT degrees of RE sets are not dense, and also the CL degrees of RE sets are not dense. So why they are not dense? Basically, that's because the reduction is too strong, and there is nowhere to code the information for stats. Information for sets. For example, for IBT and CL degrees, there is not even a complete set. So, what about the other three degrees, the RK, KNCL? The question remains open. And in this work, we almost solved it, but there is still some gap, which I shall explain later. Now, let's look at how most of the density results in RK degrees were proved. So, basically, the schema is like this. So basically the schema is like this. Here you can take regard this less than symbol of any degree, for example, Turing degree. So basically all the proofs work like Sachs density theorem. That's essentially the same. So if you are given I set A and B and with B is strictly less than A, you would like to produce something that sits strictly between them. So you would like to construct another I set D. To construct another IE set D, which contains only part of information of A, which means it has information of A, but not all. So strictly less than means B is reducible to A, but A is not reducible to B. And this reducibility, I haven't put any markers on it. You could say it is Turing reducibility. Okay, so we would hope that. We would hope that B join D is something sitting strictly between B and A. So you would have to guarantee these three conditions that D is reducible to A and these two reductions here are strict. So the first condition could be done using a permitting method and the other two conditions could also be done by considering the length of agreement and use a priority. Of agreement and use the priority agreement. That's very standard. But what happens in the CL or RK degree is that we don't know a joint operator. If you look at the joint operator for Turing degree, so basically this joint means that you code the information of B into the even number bits and you code the information of D into odd number bits. But if you do this for the five reductions, we have. For the five reductions we have considered before, then the first n bits of information will be stretched to the first two n bits of information. We do not want that to happen, so we must produce some alternative for this join operator, which basically is the least upper bound. So let's now look at the question: which kind of sets could be joined to a least upper bound? This gives our Bound. This gives our definition for compression. So if now we have an IE set A, we call another IE set D a compression of it if it contains the information of A. Basically, that says A is RK reducible to it. Now, if we take D to be an RK complete set, you will say that it is the compression of everything. We do not want that to happen because we do not want to introduce additional information. We are doing the compression. Doing the compression, otherwise, you could get an upper bound, but not the least upper bound. So, we require that in addition the compression is gainless. So, basically means you do not introduce additional information and the two sets are actually arcade equivalent. And another condition on this is that we need it's actually to compress things. So, we need this compression to be strong if the compression, the set. The compression: the set D is a subset of even numbers. So if you could do this, then it could be written as some A prime joint empty set. Now what we have here is that if you have two RE sets A and B, both of them has a strong gainless compression, which is written here, then you ignore the empty set part and you can conclude that A prime joint B prime is a least upper bound for. B prime is a least upper bound for A and B in the Rk degrees. And as an immediacy consequence, we have this one of our main theorem that says that if you have RE set A and B, B is strictly lower than A, and both of them has strong inless compression, then there will be a RE set that sits between them, strictly between them. And here, the reduction symbol could be any of Could be any of the ones that the question is open. So basically, that's because if you have the previous drawing operator for the RK degree, it will serve naturally as the drawing operator for K and C degrees. And the rest of the proof will go the same. So I will only introduce the proof of the RK case here. As I introduced in the previous slide, we would slide, we would like to enumerate this subset of A prime, which contain only parts of information of it. And we join it with B prime, so we hope the saying we get is sitting strictly between them. What we need to do is to make these two RK reductions strict. So we would consider the length of agreement for the other side of the reduction. For example, we do not want any RKA function. Any RK functional phi to reduce A to the set B prime joint D prime. So we will consider at any stage how long the length of agreement is. And we construct two lists of conditions. And so basically there are conditions PE and NE. And if the length of agreement PSE, which corresponds to the Which corresponds to the right part of the reduction. If this is large, then we will always want to enumerate. Sorry, let me first explain what happens here. What happens here is we want to construct a subset of A prime to be D prime. And whenever we found some enumeration occurs in A prime, we will immediately decide according to this condition whether or not it goes into D prime. So now if the uh so now if the length of agreement for the for the right part of this reduction is large then we'll also then we'll we will always want uh the number a to go into d prime and if the uh the other side of the uh length of agreement is large we would want to skip a and we uh we organize these uh two lists of conditions into a priority line uh so what we are doing here is whenever uh an element a uh an element a comes into a prime we will ask p0 and l0 do you want do you guys want to enumerate a into d prime or not if one of them has the answer then we do as i says but if both of them do not have an answer we continue to ask p1 and n1 so uh obviously we should uh put an upper bound on the question number of questions we ask we could use a itself it does not matter very much and now after we have And now, after we have constructed this set D prime, automatically D prime will be RK reducible to A prime. That's because whenever something goes into A prime, we are immediately deciding whether it goes into D prime or not. And also, by doing an induction on E, we could prove that the two reductions are strict. So, suppose otherwise there is a function of phi E. There is a function of phi that carries that witness the RK reduction, then the corresponding length of agreement will automatically go to infinity. While this thing goes into infinity, we claim that A prime is RK reducible to D prime. That is because if you want to decide something is in A prime or not, you wait for this length agreement to grow above your number. And it will be in A prime if and only if it is in D prime. And with this, And with this relation and some simple calculation, we will conclude that A is actually reducible to D to B, which contradicts the initial assumption. And similarly, if we had the other reduction is not strict, we will have the similar result. So basically, that completes the proof. What we have done up to now is that if you have two sets that have strong gainless compression, there will be something. There will be something sitting in the middle of them. So, the natural question to ask is: Does each IE set have a strong gainless compression? That will be the rest of the talk. We have not solved this, but we have some different approaches to approximate this result. But before introducing them, I should introduce this useful lemma when we are actually constructing RK reduction. Constructing RK reductions. So, RK reductions itself looks like Turing reduction in the general case. But in our case, we only care about RE set. So, it is really very simple. We have this equivalent characterization, which says that if you are given to RE set A and D, then there is a R K reduction if and only if there is a recursive enumeration of them and a constant K, such that within the fixed range, The fixed range zero up to n for each k element enumerated into a, we have at least one element enumerated into d. So why is this? Let's say that we have enumeration AS and DS that satisfy this condition and we want to show that A is archae reducible to D. What we do now is if you are given the first n bits of D, you just simultaneously do the enumeration for A and for A. Do the enumeration for A and for D. You wait for a stage where your set D matches the given information. And after that stage, the set A could only change no more than K many times. So you could output this K minus one guesses for the set A. So that's the reason we have this lemma. It is very useful, which is a very simple characterization for RK degree within the IE sets. Within the RE sets. With this lemma, we are able to prove that each RE set has a strong compression. Note that we did not require it to beginless here, so that means we are allowed to introduce additional information. The process is shown in this picture. So we divide the, we cut the set of natural numbers on every power of two. And so typical. So, typical interval will look like 2 to the power n up to 2 to the power n plus 1. And for the upper part of this graph, we only care about initial segments. So, if we have eight numbers enumerated into this red segment, we will, in our construction, enumerate one number below. And this number is chosen. And this number is chosen to be even, and that we have not enumerated it yet. And this thing happens simultaneously for all. So there is a similar thing happening in the blue part here. Whenever eight numbers is enumerated in A, up to this blue part, we will enumerate something here. This happens for all and so they enumerate things simultaneously. We do the We do the verification for this. The first thing to verify is that A is indeed archreducible to D. This is because if you consider any initial segment like here, any eight numbers enumerated in this initial segment would be eight numbers enumerated in this red segment. There could be more, but at least eight. And that will cause another element to be enumerated here, which is indeed. To be enumerated here, which is indeed in this specific range that we have fixed before. So it satisfies the condition of the previous lemma. So A is archie reducible to D. The other thing to note is that D does not run out of number. That means if you consider the length of this red interval here, it is 2 to the power n plus 2. And the length of interval here is 2 to the power n. And don't forget that you can only use even numbers. Can only use even numbers when you are enumerating D. So there are actually eight times, so the available number here is actually eight times the available number here, which is exactly the eight here. So we do not get wrong out of number. So there could be some impacts for this theorem. If we take the A, the set A in the previous theorem to be actually complete, then complete, then by the previous theorem, it should have a strong compression D. And now the strong compression is itself Re, and A is RK complete. So D is RK reducible to A by the definition of RK-complete set. So this compression is indeed gainless. So what we have now is that every RK-complete set will have a strong gainless compression. And this result was obtained much earlier, while back then there were no notion. While back then there were no notion of compression, but basically they had the same result. What we do now is to introduce another approach to approximate the conjecture that every I set has a strong G-less compression is where we lower the we soften the condition a little bit. We do not ask it to be strong, but instead we ask it to be weak where the number of elements in D Elements in D is roughly do not exceed the half of the number of elements in A, which automatically it will not exceed divided by two. So the existence of weak gainless compression does not itself contribute to the density theorem, but we actually found that the process or method used in finding a weak gainless compression will help. Will help us to find a strong compression. So let's introduce this result. Another of our main theorem says that every RE set have a weak Gainless compression. What we do here is we want to transform the original version of RK reduction characterization into a game form, which could be easier for us to consider the problem. For us to consider the problem. So here we have a game and we have two players A and D. Basically, that is the corresponding set A and D. And they play on countably many counters. These counters are initially set to zero. And in each round, or we could say in each stage, player A enumerates some natural number n. And for all indices bigger than it, we set negative counters to zero and add one to the positive counters. And add one to the positive counters. And after this process, PlayRD could enumerate some m. If he does, then for every bigger index, we said we do the contrary. We set the positive counters to zero and subtract one to the negative counters. So intuitively, this counter, so if the counter is positive, then it represents the number of enumerations that is less than n since the last d enumeration. So we have Enumeration. So we have all this if this counter becomes negative, then it is the other way around, which is the number of D enumerations since the last A enumeration. So we have this equivalent characterization of RK reduction. If this sequence AI has a uniform upper bound, then A is RK reducible to D and also the other way around. With this transformed form of RK reduction, we are able to prove. We are able to prove our theorem that every I set has a weak gainless compression. The construction is really simple. So you play the game and whenever you find that some counter is bigger than or equal to 8, you will find some medium counter such that all counters between m and n are bigger than equal to 4. And you enumerate this number m into d. But we need to verify. But we need to verify a bunch of things. So, first of all, each number is enumerated into D only once. And also, that we have a lower bound and upper bound for this sequence AI. And finally, the number of elements enumerated into D is not very much, is no more than half of the elements of A. I will skip the verification here because they are too complicated. Basically, you just Basically, you're just doing the counting. Now, these games help us to obtain a weak gainless compression. We will naturally ask if it could help us get a strong compression or not. So now I have changed the game a little bit, just here that we require player D to only enumerate even numbers. So I also added the winning condition for both players. Winning condition for both players. If the sequence AI obtained in the game, throughout the game, is uniformly bounded, then player D wins. Otherwise, player A wins. And this game is itself easier than the problem, but easily presented than the problem, but we find that it is very hard to solve it. So we would rather consider this alternative version of the previous game. Previous game. This game is presented very easy, such that every primary school student could understand this game. You consider the game GK parametrized with K, which is basically this uniform bound in the previous game, and you have two players A and D. In each round or in each stage, player A choose K numbers, L1 and up to NK, and player D must. n k and player d must choose an even number that's in the range player a gives which means that is it is not smaller than the smallest and not bigger than the biggest so if uh the player a and d are actually doing enumerations here so they could not choose the numbers that he has chosen individually you could choose the number that other player has chosen uh has chosen now if player d run out of number then player a wins now Run out of number, then player A wins, and otherwise player D wins. So we have this alternative version of the game. I have to admit that there is no concrete relation between this game or concrete implications for saying that if someone wins this, then he wins that. But we believe that these two games have the same spirit and every strategy here would naturally transform to a strategy in the previous game. In the previous game. So let's focus on this. This game itself is quite interesting. I have been working a lot on this, on this, who wins this game. Let's see what happens. If the uniform bound K is small, then player A wins. That's basically because if player A wants to force something to happen, he do not need to enumerate very much number to cause trouble for himself afterwards. And we would naturally ask if there is a case such that player. If there is a case such that player D wins DK, and we will be particularly interested in the winning strategy. How much time do I have? Okay, so to help you better understand this easy game, let's show the winning strategy for player A in G3. Rather than showing the concrete winning strategy, I would like to Strategy, I would like to rather show a typical sequence of game where player is using his winning strategy. So, what we have here is that in this strategy, player A will also enumerate even numbers, except the last stage where he is ready to claim his victory. So, before the last stage, Player is also enumerating even numbers. And what will happen here is. And what will happen here is a key observation that player D has to follow him in the sense that he could not enumerate any number that player A has not yet enumerated. So basically, if player A plays 100, 200, and 300, then player D must choose one of these three numbers to play. If he jumps elsewhere, then player A could immediately win. And another observation for this is in the final stage, if In the final stage, if there are two even numbers that are near each other and both of them are used by both players, then player A could win the next stage. So this sequence is shown here. I wouldn't read it. If you are interested in this, you could look at the sequence itself. So basically, what we have here is player A will enumerate numbers that are far from each other. The distance From each other, the distance is enough for him to win the later stages. And he forces player D to produce two even numbers that are close to each other. The strategy here is particularly interesting because we could translate this strategy into this again another alternative version of the game. In this game, it is even similar. In this game, it is even simpler, where player A and D no more play on the natural numbers, but they play on a single string S. This string S is initially empty, and in each round or in each stage, player A inserts k many O into the string. So, for example, if K equals to 3, player A plays 100, 200, and 300, and he inserts 3 O on the axis. O represents the numbers. O represents the numbers that player A has used, but player D didn't. And player D then change one of these O into a string X. And the string X will represent the numbers that both player A and player D have used. For example, the 200 here. So after the first stage in this sequence, a string O A string OXO is produced here. And finally, player A may cut S at both ends, which means that he could take a substring of S to put into the next stage, not the string itself. And the winning condition is that if K minus one successive X appears, which basically is this two successive numbers that both player A and D have used, if this appears, then player A wins, otherwise player D wins. Player A wins, otherwise, player D wins. So, what we have showed in the previous strategy is that player A wins G3 prime. So, basically, the strategy is that this is the empty string. And from the empty string, player A was able to force an X to appear. And if he has an X, then he could force either XX or XOX to appear. One of them, this choice is up to player D. If he forces XX to appear, then He forces XS to appear, then he wins by the winning condition. If it is the other case, then he could use another stage to force XS appear. So this graph shows that player A wins G3 prime. And we also have this implication. If player D wins GK prime, then he wins the original game GK. And then I have done a computer search for the game G4 prime, which also shows that player A will win. Player A will win. So here he starts from the empty string and go all the way around, and finally, he could force three successive acts to happen. Maybe you could not see clearly, but basically that's it. After this search, I kind of believe that player A will wins GK prime for any K. But of course, I do not have space and time to search the game G5, but The game G5, but yeah, anyway, so let's summarize today's talk. I have defined the RK, K, and C degrees and argued that if we could have a strong gainless compression of every RE set, then the degree structure among IE sets are dense. Now we would like that every IE set have a strong GLS compression. Have a strong gainless compression. So we use two ways to approach it. We prove that each IE set has this strong compression, which need not to be gainless, and also it has a gainless weak compression, which need not to be strong. And toward obtaining a strong gainless compression for every IE set, I have studied some simply presented games related to this topic. So that's all of my thoughts. Thank you. Thank you.