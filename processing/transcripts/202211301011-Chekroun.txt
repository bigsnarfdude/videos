Especially after this COVID period, it would have been so nice to be in person in such a great meeting. So, shall I start? Yeah, you are. Yeah, okay. All right, so I apologize to the audience because, you know, I have a Mediterranean blood and usually I move a lot for those who know me when I give a talk. And here I'm sitting in my chair. So I will try to do my best. So we'll talk about. Do my best. So, we'll talk about this concept of optimal parameterizing manifold and how it can be used to derive radius system for stochastic transition. This is a joint work with Hangul Yu and Jim Mike Williams. And okay, let's start. So, do you see the screen, right? Yes. Okay. So, we So we are concerned, you know, the broadband, the broad picture is like that. We are concerned with the transition of large-scale recurrent patterns in climate. The goal that we said to ourselves, and we are far to reaching this goal, it's to understand this transition and the role of small scales that plays the small scales in them and to derive the approach that we want to put. We want to pursue is that we would like to derive a dynamically rooted parameterization framework to dismantle the non-linear interaction between the large and the small scale. And we want to do that in presence of time-dependent forcing for obvious reasons and climate tipping points that are represented in the right picture here with the North Atlantic sea currents, the MOC. Securance, the MOC, and the ice sheets from Greenland and Antarctica that are known to experience tipping point. And we'd like to see if we are able to derive radius models that are meaningful for such problem. So obviously it's a very complicated problem, but this is the motivation level. And we have to go back to basics to try to understand how. Try to understand how to address this problem. So, first of all, I would like to give a little survey on the parameterization problem for PDEs and how we can address it. So, typically, you have like, roughly speaking, you have like a PDE that is written like that with a term that includes like non-linear adviction and plus boundary condition. And we separate. And we separate the solution U in terms of like core scale plus small scales, UC plus US. And we assume that it's of this format with linear terms, including dissipative, damping terms. And then we are looking for a closed equation that describes the motion of the large scales without having to resolve the small scales. The troublemaker term is obviously coming from the non-internal. Is obviously coming from the nonlinear term. And when we expand here, we expand the u with uc plus u s and we expand this nonlinear term, we find that the nonlinear term is the total micro term is given by this term to parametrize that involve the US terms in red that needs to be parametrized. So, here I give you like an example from a vorticity field, classical problem, and we see that. And we see that the small scales are represented to the extreme right, and the large scale field is this. So, we'd like to, for instance, resolve what is in the middle without having to resolve the small scales. So, there is like several approach to do that. One of the approach is to first address the averaging problem. The averaging problem is addressed here furthermore using this concept of optimal parameterizing manifold. So, let me just explain what it is. What is it? So, typically, we are looking for a near-slaving relationship of this type that so that US is a function, US being the small-scale variable, is a function of the large-scale variables, plus a residual that should be as small as possible. And in this paper, we develop like an approach that, first of all, we have to realize that the We have to realize that the conditional average of the small scales with respect would give us actually this OPN, this optimal parameterizing manifold, which is just the average of the small scales with respect to this probability measure condition on the large scales that I will give a picture in a moment, and that gives the best nonlinear parametrization that minimizes the error epsilon. That minimize the error epsilon. What we realize in this work is that for regimes with many unstable modes, this OPM can be approximated via data-informed homotopic deformation of the center-unstable manifolds. I will explain what does it mean, but essentially, we know that center-unstable manifolds are great to add the onset of instability. This is the bifurcation theory. Bifurcation theory. But when we move away from the first few bifurcations and many modes become unstable, then this reduction becomes useless, typically because of a small spectral gap that comes into the party. But we realized that we could actually still propose an omotopic deformation, and we'll see in this talk how we can do that, that is informed by the full model to. The full model to optimize this deformation. And when we do that, we have an approximation of the troublemaker term that I was showing in the previous slide using now the parametrization to replace the US term. So, what is this conditional expectation that is involved? That is involved with the conditional probability measure. This is well known in the business. Here I depicted just like V as a reduced state space. Here, trajectory XT in this reduced state space. And we plot on top of it this conditional probability measure that you see can change. And Adam was talking about like non-Gaussianity, it can be highly non-Gaussian, and to parametrize that with a stochastic process, for instance. Of a stochastic process, for instance, it's not easy. Now, we would like to see what is the effect in average of these small scales as distributed by this probability, this conditional probability measure. And here I depicted like a picture, for instance, here from like if x is the resolved variable to the right and the unresolved variable are put in the y-axis, you have like. Are you putting the y-axis? You have like a cluster of black points that spread around a manifold here. It's a basic quadratic manifold. And this yellow curve that you see here is what we are after. This is what would be here, the OPM, the optimal parametrizing manifold. Now, I want to extend this program to SPDEs, and for that, we need to invoke SPDE. To invoke special forces. So, first, let's say a few words about the framework. Consider like SPDEs that are given under this form. Nothing new here under the sun. L is a classical operator, linear operator with part of it, generated contraction semi-group. Part of it generates a contraction semi-group and B is a bounded perturbation to it. F of U is a non-linearity that allows for loss of regularity, like the U grad U that I was talking in the previous slides. And eta T, this is maybe the thing that requires a bit more of attention here, is composed of like terms that could be either Brownian motion or just Could be either Brownian motion or gem processes. And the force EJ, you see, there is a sum here, and the force, the modes, where each EJ corresponds to a scale. It's typically the eigen mode of the linear operator that can be obtained, for instance, like by linearization around the background state or under min state. State and so we force several scales, and we would like to obtain a reduced equation, also known also as a closure, in a reduced state space that is able to capture the dynamics. For that, we need to keep in mind that we are dealing with stochastic system. And in stochastic system, we have two approaches: we have like the Markov semi-group. Like the Markov semi-group approach, and we have the random dynamical system approach. So here I recall that you know the existence of a unique ergodic measure for the Markov semi-group associated to SD is driven by Bronian, by Definite Gaussian noise, has been proved by the great work of Errer and Martin Lee and also Schudzo for a broad class of SPDs, including like degenerate noise. Including like degenerate noise. But what is interesting is like once we have this ergodic measure, we can build using also construction that we can find in this paper of Flando Lee, Benjamin Guess and Schotzau. We can defend random statistical equilibria obtained according to this average where S is the stochastic flow genotype by the S P D E, and that satisfies that when you take the expectation of this probability random probability measure. Random probability measure, you get in average the invariant, the ergodic invariant measure of your Markov summit group. They enjoy furthermore the property of being like S invariant, meaning that if you start on this probability measure, it will propagate the flow. It stays invariant with the flow. So here I presented just a few snapshots from this paper. Like just a few snapshots from this paper that some of you probably know. This is stochastic Lorentz 63 model. And the main point is that rho omega, whereas like mu can be smooth with respect to the Lebesgue measure in finite dimension, rho omega can present like fractal structures and it can like also experience a lot of change with time. Why I'm talking about this, what I call random statistical. What I call random statistical equilibria. Because when we deal with stochastic PDEs and we want to address the parametrization problem, we will have to encounter at some point this parametrization defect problem. So this is very normal definition where I have a parametrization phi that depends on the noise path omega, and I apply it to my coarse scale variable. Coarse scale variable UC obtained from the solution of the full SPDE, and US is the small scale variable, and I measure the defect, and I take the average in time. But if we assume that rho omega, the statistical equilibrium, is mixing in a pullback sense, then we have that this parametrization defect is given through this integral with respect to this random statistical equilibria. Equilibria. And that's why this quantum statistical equilibria actually arises naturally when you want to assess the parametrization defect of a given parametrization that typically you will assess along a single noise path from your SPD. So, this formula one is used along one noise path, whereas the formula two allows us to provide the precise characterization. Provide a precise characterization of what a stochastic OPM is. And this stochastic OPM, as we will see, is non-Markovian. And here is the characterization. If we assume very basic assumption that the small-scale variable has a finite energy, which is encountered in all the physical problem we are after, then we can show, and it's not complicated, that the optimal stochastic parametration solving this. Stochastic parametration solving this measimization problem involving this random statistical equilibria exists and is unique and is given by this formula, which is nothing else than the stochastic extension of what I have mentioned at the beginning of my talk, meaning the average of the small-scale variable here represented by y against this disintegration of this random statistical equilibria. Random statistical equilibria with respect to the projection onto the reduced state space. And we have that phi star is the one that minimized the parametrization defect without any surprise. But we can say more. We can say that actually if we introduce the random conditional expectation associated with this projector onto the reduced state space HC. state space HC containing only the large scale variable. And if we assume that, for instance, the nonlinearity, I mean the right-hand side of my SPDE is of this form is a linear R operator plus a quadratic term. Then this random conditional expectation is this integral with respect to this like disintegrated merge. To this, like, disintegrated measure, rotated omega X, of this statistical equilibrium. And we can prove actually that if we have furthermore that actually this random conditional expectation can be expressed thanks to the non this OPM, this stochastic OPM, plus a residual term that includes the high-high interaction. Include the high-high interaction. And if we assume that this residual term that includes the high-high interaction is zero, which is like a classical assumption in averaging theory, then the non-Markovian OPM closure is given exactly by this random conditional expectation. And therefore, this finding this like optimal stochastic parametrization gives me the random conditional expectation. And therefore, it has a meaning. It has a meaning. Now, how to approximate this optimal, this stochastic optimal parameterization? So, for that, we need to dig a bit into the theory of stochastic invariant manifold. And there is this great work of Daprateau and Debush, published in 1996, where they consider approximation of stochastic invariant manifold by solving these backward-forward systems. Where actually you solve backward actually the first equation, the p equation, and then you solve the q equation, and you need to do that simultaneously because here it's fully coupled. And they show that under a spectral gap condition that is encountered like in many other approaches, that actually if you stretch the tau, the time scale tau, to infinity, then it gives you this limit exists in L2 omega. Exists in L2 omega and it gives you the stochastic invariant manifold. So, based on that, we can design backward-forward leading approximation schemes. And to the first, if we take this natural iteration scheme, it leads to the following stochastic invariant 24 approximation, which involved these two integrals, one integral that involved integrals, one integral that involves the noise path from the past, and one integral that involves actually nonlinear interaction propagated by the linear flow. So while we say that the existence of this parametrization, the stochastic invariant manifold, is conditioned on the spectral gap condition, the two integrals here that are in the box are Here, that are in the box are guaranteed to exist under much milder conditions. And actually, I give you here some conditions that are non-resonance condition that ensure me, for instance, that these integral, the first integral exists, where the lambda are the eigenvalue, lambda n is the eigenvalues of the neglected mode, and the lambda jp are the eigenvalues of the resolved mode. Of the resolved mode. And here it's written down for F that is K linear R. So, based on that, we can have the idea of propose a scale aware backward-forward systems, which will be doing the job of deformation that I was mentioning before. To provide, we're going to deform the stochastic invariant manifold approximation scale by scale. And how are we going to do? Scale. And how are we going to do that? Well, especially when some of the spectral gaps are small, then we know that the stochastic invariant manifold is to an unsuffectory parametric of the unresolved modes. So the idea is actually to break down the modes and you see in the PQ equation, I will retain only in the Q equation the Qn part. And I will neglect also, instead of having this fully coupled. So instead of having this fully coupled terms, I will only solve the p-equation backward with the linear terms that I will plug into the nonlinear terms of the mode that I want to parametrize so that I can solve this analytically. I solve backward the first equation and then I have the expression of p, I plug into the f and integrate forward and I have the power expression. I have to say that here to simple To say that here, to simplify the presentation, I force only the mode that I want to parametrize, force stochastically this mode. And you see, there is like now a free parameter, tau n, which is a time scale. And q n, the solution of this backward-forward system, is supposed to provide the parametrization phi n that is aimed to approximate this amplitude of the nth mode. And we have an analytic formula for that. The analytical formula, when we solve this backward-forward system, let's say for in the case of f that is including quadratic and cubic term, then it gives that phi n is given by this term where you see here a first term that is involving random coefficient and then terms that are quadratic and cubic in the variables that are the That are the large-scale variables. And the Z term here is given explicitly and it depends as an integral over the path, the past of the noise path, and tau n being the amount of history we want to pass in. And tau also, when it has a very natural interpretation, it allows when we look at the formulas of these terms in front of. Formulas of these terms in front of the quadratic terms and the cubic term, tau allows for balancing the small denominators that will arise due to the small gaps. We see here, for instance, in the dijn term, we see that the lambda i plus lambda j minus lambda n that will come from quadratic interaction, if this guy is small, then you can balance this term with the right tau using the numerator. So, the scalar web parameterization phi n are optimized, as I said now, on a single SPDE path. This is called the training path by minimizing this parameterization defect. And the idea, so this is what is in the panel B, for instance, of this figure. You will have then that this Q will depend on the tau, the time scale, the backward integration. The time scale, the backward integration time scale, and then you will see, for instance, here a clear minimum. You could see sometimes several local suboptimal minimum. But when you have optimized that, then you can pile up throughout the scales from Q plus 1 to N, if Q is the cutoff scale, and N is up to which scale you want to retain the parametrization. And so now you have a data information. And so now you have a data-informed parametrization, right, using the parametrization effect, but it's also a theory-guided OPM approximation. The theory part being that it's a deformation of your stochastic inverse manifold parameterization, which is scalewear. So, now are these attributes useful for predicting statistical dynamical behaviors from opium-reduced system when the noise path is valued? It so actually, the idea is to have this manifold that tracks the solution path, this stochastic OPM, and the path that these coefficients, as I said, they are path-dependent, depending on the past. And they are non-Markovian involving then exogenous memory terms coming from the past of the forcing. The interest of having this coefficient is that, for instance, this integral here, the integral term solve a random ODE of the form that is given. ODE of the form that is given right there. And so we don't have, we avoid quadrature to simulate this random coefficient. Here is like a stochastic Burger Steezer near instability onset. We see, for instance, I pick the model is written right there, where I forced the second mode that is unstable, that is stable. The first mode becoming unstable here because there is like this lambda U term that creates a pitchfork scenario. It's called Work scenario. It's called the Burger-Sevashzinski equation for those who want to know. And here I plotted in the x, y, y is the variable I want to parameterize, x is the first mode amplitude, the trajectory. And I plotted here, you see the manifold here, the stochastic manifold is quadratic. And the pink dot is the actual location of the trajectory at this particular time instant. And we can see that the And we can see that the manifold is here tracking the solution path, and that's why you get this nice closure just with one first mode amplitude. But here we are near the instability on set, and actually, I don't need to do any optimization, meaning that tau is can descend to infinity. Let's go to a much more interesting problem. Here is a stochastic Alan-Kahn problem away from the instability onset. So, we have like the Allen-Kahn problem classical here. The Allen can problem classical here that is given right there. And I forced from mode five to eight. Here I forced with Gaussian noise, random Bronian noise, white noise. And so we go, we have passed several bifurcation, and here I plotted the typical attractor, which is with many saddles and steady state, stable steady state. And we see that there is like, you see, plus, minus, and plus, or minus, plus, minus. And plus or minus. Plus minus means that it's a sign-changing steady state. And a constant sign means it's a constant sign steady state. So we see that actually here plotted to the right, we see that we have a time is in the abscess. We see that the solution is a transition. We start from zero and the highly probable state that we reach over my fixed interval time. Over my fixed interval time that I selected is this sign change guy, meaning that we spend more time in here than going in there. The transition to a race reverse state to the opposite as a constant sign guy. The parametrization, when I apply the theory, the parameter is given like that. And if I do the optimization on a highly probable transition path, I get this black curve, the blue curve. Curve and the blue curve being the what we would get if we would suppress the stochastic term. So now what these are the results. The results shows something quite interesting because we have the sign-changing steady state, metastable state, that are the most probable. And we train on the transition that was like highly probable from zero to. Highly probable from zero to such a state. We want to know if it's able, the radius system is able. And for the radius system, I have to say that I cut below the cutoff scale, the forcing scale, right below the forcing scale, meaning that retain here only four modes that are not forced stochastically. And whatever is forced stochastically is in my neglected scales. So here we see that a good metric to see what's going on here. See what's going on here is to look at the mean plus max metric, meaning you just pick up the mean and the max of the solution profile, you add them, and you do a property distribution of this mean plus marks. And here you see a huge peak near zero, meaning that it's a dominance of the sign-changing guy. And you see the rare event here, and we see that the radius system that is written down there is able to produce. Is able to predict some of these rare events, although it's missing some of them because we see that there is like the mass that is located there is not exactly the same. But it's quite amazing that it's able, just by training on a single path, to predict what's going on. Okay, so I'm gonna go quick to this example. This example, this is a jump-induced transition. It's my last example where I pick up like a model coming from Tokamak here. It's a Gardner Chofranov equilibrium, the Gardano equilibria, for those who know in plasma physics. And here you have like an SPD that is of this type, with epsilon being a parameter that controls the existence of S-shaped bifurcation or not, and eta here. Or not. And eta here is a forcing term that we force, like E3 and E5, E3 and E5 being only the modes that we force here, where E3 and E5 are obtained through linearization of this unstable steady state that is obtained through the bifurcation diagram. And the forcing is a random square signal whose activation is randomly. Whose activation is randomly distributed in the course of time. And we want to have a reduction only in this when the reduced state space is the first mode. So maybe I just say quickly that here is like the dynamics of this first mode amplitude that we see that in Dash, you see the location of the three steady state. The blue are the stable steady state, and the red are the is the And the red is the unstable guy, and we see that the dynamics me and there across these guys. And I don't have really much time to explain what this panel below, but essentially I can summarize that it tells me that if you look at these two purple vertical dashed line, you can have two first mode amplitudes that are very close while the response are the SPD level. At the SPD level is very different, and therefore, meaning that the parametrization problem is not trivial. Yet, we are applying this OPM approach to this problem, the gem-driven dynamics, and we are able to reproduce quite well. Again, we train on one single path and we test on many other noise paths. Other noise paths, a large ensemble of them, a million of them actually, and to see if we are able to reproduce the statistics. And we see that the reduced system with its random coefficient G N that are given that are solving here an auxiliary equation is written down there here, are able to track the dynamics quite well. Albeit, we have pass-wise we are doing a relatively good job, except that Relatively good job, except that we see that there is overshooting here, but it's probably coming from the approximation that we did already into the class of parametric we are working with. But thank you very much for your attention. And we see that with noise, you know, usually we like to think into this as a mathematician into this spherical cow universe, but with noise, we can maybe try to make the cow dissembling. Try to make the cow it's something to something a bit more realistic. Thank you for your attention. Thank you very much for the talk. Questions? Yes? Mikel, thank you very much for the talk. In developing your parameterization, when you were considering the parameterization defect, I've considered it a couple of times, but I think slide 17 was one example. You're really focused on making sure that you had. You're really focused on making sure that you have pathwise agreement in the mean square, right, between the true process and the parameterized process, which is quite a strong requirement. It would make sense like in a numerical weather prediction context, but maybe the climate context, you might get away with just a weaker distributional convergence. Is that something which is possible, this framework? Would it change things dramatically? Yeah, it's a very good remark, actually. So the This parametration defect here that we see right there is one mean actually to calibrate your parametrization. Here you see, like if we have like a noise realization, with solution path, we can do that. But as you said, we could like try to, for instance, like parametrize distribution, right? So, so, so, so. So to optimize in a in a in a in a to optimize the parametrization by matching some statistics that are distribution, that are, so then it will involve like probability metrics or like other correlation. And what is like very interesting is that under the assumption of ergodicity, we would be able like more or less to deploy the same theory. Same theory, and we will get other optimal backward-forward integration time. And that can lead to indeed to certain problems to better results. Questions? Everybody waiting for coffee? Are you? Yes. Okay, so then in view of the time, because now there is a new photo, we thank you all for. Photo. We thank you once again, Michael, for the talk. Thank you very much. Those of you present in virtual space would like to turn your camera so I'll take your group photo. No, we can ask. Yeah, we need to for the crowd. Yeah.