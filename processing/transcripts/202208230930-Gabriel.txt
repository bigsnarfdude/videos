Okay, so thank you very much for this invitation. And thank you all for coming to listen to this talk. So I'm going to talk today about flexible evaluation of trial level surrogacy in easy and adaptive platform studies. So this isn't about designing or running a platform study, but using data from a platform study to evaluate. A platform study to evaluate a trial-level surrogate. So, this is work that I have done jointly with Michael Sachs, who's actually the first author on this project, Alessio Kripa, who's one of the main statisticians on the motivating study here, and Mike Daniels, the chair of the Department of Statistics at the University of Florida. Okay, so my slides are not in that form, but in a different one. So, bear with me. Sorry. A different one, so bear with me. Sorry. Um, can you see the whole slide? Yes, yes, okay. So the trial design, and this is the motivating trial, which is called ProBio, is in men with metastatic castration-resistant prostate cancer. And it is an outcome-adaptive randomization, which is Bayesian. And its goal is to identify. Is to identify combinations of biomarker signatures and treatments that perform better than the standard of care in that same biomarker signature group. So all of the treatments are already approved for use in metastatic prostate cancer. It's simply trying to figure out which one works best in which biomarker signature. So the biomarker signatures are combinations of particular biomarkers that are Particular biomarkers that are based on these mutations. So there are 64 actual mutually exclusive biomarker treatment combinations. And in the actual trial, they're analyzed only at the level of four signatures, which are not mutually exclusive. A signature treatment combination will graduate from the adaptive part of the trial when it has a posterior probability of superiority or. Of or percentage of 85%. It will then graduate to a frequentist confirmatory trial. So the primary outcome is timed progression or death. And why we're looking at a motivating example is that I was a small part of the team that looked at the design and randomization of the study. And they said, you work in surrogacy, we have this potential surrogate endpoint that we're interested in, circulating to interested in circulating tumor DNA, and we want to validate it here, or we want to evaluate it here using our trial. So this is the schema for this adaptive randomization study. It has five potential treatments currently within each of the biomarker types or subgroups. Subjects are randomized one-to-one between the standard of care and then the adaptive randomization arm where the adaption Randomization arm where the adaption in the randomization is between the treatments alone, not between the standard of care and the treatments. They're followed up at various visits, but of course, death will be a continuous measure that will happen when death happens. And once something is either found to be inferior at a certain level or superior at a certain level, it will either be stopped and then the randomization. And then the randomization will adjust to eliminate it, or it will be graduated and it will go on to this confirmatory study. Okay, so that's the setup. And we want to use this data, this study itself with this platform design, and then in general, platforms moving forward to evaluate what we're going to call trial-level surrogates. So we're calling these trial-level surrogates because we want the treatment effect on the surrogate to be a good perfector of the treatment effect. Be a good perfector of the treatment effect and the clinical outcome at the trial level, in the sense that we want to be able to use this in trials moving forward that are not just platform studies. So that would be at the trial level, but really our evaluation will be at the group level in these groups defined by biomarkers and treatments. So luckily at the trial level or at the group level in the meta-analysis framework, we can estimate directly. We can estimate directly due to randomization the treatment effects on the surrogate and the treatment effects on the outcome. And so, unlike COPS or individual level surrogacy, we have less concerns about identifiability. So, a very, very light literature review. Sorry, that just very light literature review is that in 97, there was the Danielson Hughes paper that used a Bayesian approach. That used a Bayesian approach to meta-analysis for trial-level surrogacy. And then, please forgive me, Buzzkowski et al., with other authors in the audience, which also use this meta-analysis framework or a similar one to look at trial-level surrogates. And there were many papers before and between, I'm not saying there aren't. And then in 2012, there's the Dying Hughes that tries to unify this framework. Unify this framework, not making it simply based on what the type of outcome was. So, I just want to point this out to say that I in no way think I'm the only person that works in this and that it started long, long ago. And I, you know, I've only started working here. So, in 2016, a paper with my co-authors, Beth Hollerin and Mike Daniels, and then in 2019, we And then in 2019, we extended that method and included the author, Michael Sachs, who's now the first author on this paper, where we tried to use those same meta-analysis frameworks, ideas, concepts, but we wanted to look, allow for the most flexible association between the treatment effect on the surrogate and the treatment effect on the outcome. So we did this in a Bayesian framework and we can. Work. And we considered first a slime model in 2016 that allowed for individual level data from each of the trials. And then in 2019, we used a Dirocher process mixture, but only at the trial level. So in both of these, we suggested that you evaluate surrogacy using a leave-one-out cross-validation for the evaluation of the predictive value of the treatment effect on the surrogate or the treatment effect on the outcome. So this is going to be the So, this is going to be the same criteria that we use here. We're going to use the out-of-sample absolute error as our criteria or our measure of predictive accuracy. Obviously, you could use others or another or an additional criteria within the same framework just as a different measure. And we're in particular going to use the posterior probability that the absolute error is lower using the surrogate or a combination of surrogates than the. Or a combination of surrogates than the null model that does not use the surrogate. So we're going to use this meta-analysis type approach, but because it's a platform study, we can do all of this within, or we say we can do all of this in the same trial. So it's a single trial, and we treat each of these biomarker treatment groups as its own trial for the meta-analysis. So we want to extend our previous methods. Extend our previous methods using this rich platform data that lets us look over different treatments and over different biomarkers, maybe different characteristics of patients. And we want to not just allow for heterogeneity, which is of course going to be there with the different treatments and the biomarkers, but we want to be able to characterize the heterogeneity in the surrogate quality. So our method, our proposed method is a hierarchical Bayesian model. A hierarchical Bayesian model. So, the setup is that we have this individual level data from the platform study. We have SI, which is the surrogate marker, which is here the CT DNA, and YI, which is the time to death or progression. We also have the treatment groups, which are these dummy variables W, and the biomarker groups, which are these dummy variables B. We are also going to allow for some group level information, which might be Information, which might be how related the treatments are, if they all act on the same mechanism, through the same mechanism, or how similar the biomarker groups are. So this allows for some group level information that is not just the biomarker and the treatment. Okay, so our first stage model is a very standard, fully parametric Bayesian model that assumes normality. So this is just a model for the condition. This is just a model for the conditional mean of the surrogate marker and the conditional mean of the log of the outcome. These are fully saturated models where our interest lies in these coefficients of the biomarker interaction with the treatment. And although I keep going out of, sorry, full screen mode. Oh, that's better. Okay. So these are, so although. So, although we're sort of ignoring it here for a second or pushing it under the rug, we know that there is censoring here and we do deal with that. We are dealing with that in the paper, but we deal with it in a very parametric way, a very standard way where you need the censoring to be independent and to be standard right censoring. And I will talk about that at the end. That's an extension that we'd like to get working. That we'd like to get working before we actually analyze the probiodata. So, the parameters of interest from the individual level models, and what we're going to put this stage to the second-level model on, is this new and new vectors. And those are the treatment effects within the biomarker treatment subgroups. So, the remaining parameters, just the main effects of the biomarkers, and we treat those as nuisance parameters. The ones that we're really interested in the association of is this new and new. Of is this mu and mu. So, one thing that you could do, and the classic hierarchical model, would be to put a multivariate normal distribution on this, potentially conditional on those group level covariates as well. And then you could look at a linear relationship between the mu and mu vectors. So, that is the model that we compare to, or the method we compare to. Or the method we compare to in our simulations. So instead, to account for and characterize that heterogeneity and surrogacy that we believe will be there and that we want to look for, we suggest a non-parametric Dirac process mixture of normals on that second stage. So here we're going to use this multivariate normal mixture where we have the same nu and mu. ν and μ, but now we also have the covariates in that mixture. So, just as before, this is going to be a normal, multivariate normal distribution, but now we have a basis, which is that G naught, and alpha, which is a concentration parameter that controls the clustering. So each of these could technically be its own normal. There's no single parameter governing them, and it's going to give you a mixture of normals. Going to give you a mixture of normals, and alpha is going to control how many normals you have, like the clustering of those normals. So, then this is just the hyper second literal hyperparameters, some of which we set to make iteration easier, and some of which we set based on estimates from the data. So, why do we want to use this Diricher process mixture of normals? Well, technically, it should, in theory, be able to give us an infinite. Should in theory be able to give us an infinite mixture, but obviously it's going to be confined by the total number of subgroups that you give it originally. And the clusters are fully defined by the distinct omega at each iteration. So groups can move in and out of the clusters. And there's a way that we're going to talk about to decide what cluster they should be in at the end to label them. So it allows for. Label them. So it allows for data-driven flexibility in the modeling in the relationship between these two or two plus D, depending on the dimension of Z variables. So going back and recalling our goal is that we want to predict this mu in a new trial as best we can, given this new and potentially some group level covariates. So I'm not going to go through the whole estimation and resampling procedure. I just want to point out. Procedure, I just want to point out that because it's hierarchical and because we're doing this grouping, this clustering, it's a little more complicated than the standard iteration or updating procedure. We need to first update the individual level models to get the parameters from there to update the labeling for the clusters. Okay, so how do we assess? We've run this somewhat. We've run this somewhat complicated Bayesian hierarchical model, and now we have all of our group, our biomarker, treatment group subgroups in certain clusters. But how do we actually evaluate whether or not S is a good surrogate for Y? So we want to know if new, the treatment effect on S, is predictive of the treatment effect on mu. So what we're going to do is use a leave one out procedure where A leave one-out procedure where we take the J treatment or the J group subgroup based on biomarker and treatment, and we only include in the model the information on the surrogate, all other information on all other subgroups, and the covariate information for that group. And we iterate until we get a posterior prediction for mu out of the model. So, once we have that prediction, that is an out-of-sample. That prediction is an out-of-sample prediction for the trial, the treatment effect on the clinical outcome. And we're going to say that this D that's our estimate. So that's the absolute difference. If we knew the true mu here, we could just directly estimate this with the leave one outs. But since we don't know the true mu, we just have an estimate of the mus in other settings or estimates. Or estimates that we get in a frequentist manner or from our full model, we're going to use our full model, fit on all of the data, to put in a estimate for that mu k plus one. So we're going to compare the full model view k plus one using all outcome information from all of the groups and compare that to our leave one out prediction. So that is d hat as we're calling it here. And then that could. And then that could be large or small, depending on the setting and depending on the model. And so we need something to compare that to to look at surrogacy. So our proposal for comparison is this D naught, which is the null model. So it's the same procedure of leaving one out, but without any surrogate information in any of the models. So then we calculate the posterior probability that this D hat using the surrogate is less than the D null not using. On the DNAL, not using the surrogate information. So now let's say that we have a high posterior probability. We find that it is a good, by our standard, is a good surrogate. Then how are we going to use it in practice? So the leave one outs give, like the iteration of the leave one out for getting at D hat sort of prescribes how to do this. So we're just going to do the same thing that we did previously to estimate that d hat. We're just going to rerun the model. Dehat, we're just going to rerun the model, locking it at the point where we have all of the previous trials information in there. We're going to add in the new trial with just its surrogate information and its covariate or group level covariate information. And we're going to iterate that model until convergence. It's going to give us a group label. As you can see here, the open bubble or the open dot here is the unobserved where we only have surrogate information. That's going to give us a group level or a cluster level for the level or a cluster level for that trial or group. And then based on the cluster that it's in, we're also going to use the leave one outs that we did previously to get an error, a prediction interval based on that error within the cluster. So we did run several simulation studies and these were based on the power calculations from the ProBio study itself. We looked at a variety of settings. We looked at a variety of settings for different associations between and mu, and we always compared to the simple hierarchical model. So, as you might suspect, this is pretty computationally intensive. And one replicate using that has 64 of these biomarkers/treatment groups took about 30 minutes on a 64-core cluster. We don't think this is unfeasible if you're only doing this for one. Feasible if you're only doing this for one study or only for a few potential surrogates, but it was pretty, it was pretty intense for simulations. So, some very small number of results here. We look at the posterior probability, the estimated one versus the true one of the surrogate giving us lower absolute error than the null model. And we can see in the completely null case where there is no surrogate quality, we slightly overestimate the probability. We slightly overestimate the probability of superiority, but it's well below our suggested level, which is 0.5. So in all of the other cases, the interaction, for example, we have differing surrogate quality over different groupings, some based on the treatments, some based on the biomarkers, and some based on those covariates, those group level covariates. And so although these are all pretty close to the point. So these are all pretty close to the 0.5 level. That's because this is the overall surrogate quality. So you would further want to look at within the clusters that were defined this posterior probability versus the null. So what do I mean by those clusters? So this is one of the examples of the interaction setting. So you can see here there's a set of groups, a biomarker. Groups, biomarker treatment groups for which there is like little to no surrogacy. There's also very little treatment effect. It's just a flat line, and there's one that has very strong positive and linear surrogacy. And you can see that the clusters, the color of the clusters, and these contour plots are based on the full model. So in the full model is correctly identifying what cluster each of the What cluster each of these groups should be in. And then when the colored dots are the leave one-out predictions. So you can see that based on for this single iteration in the simulations, you can see that the extremes are being pulled down to the other cluster. They probably were misclassified, but that in general, it's clear that there are two distinct clusters, even when we're predicting out of sample. So this is in contrast to the simple. So, this is in contrast to the simple hierarchical model that fits a single line that's between the two lines and would suggest that there is surrogacy overall, low level, but that there is surrogacy overall. So, overall treatments, all biomarkers. Okay, so in conclusion, we think that the novelty here and the benefits are the hierarchical Bayesian model that allows for you to. Bayesian model that allows for you to incorporate the individual level data as well as get at the final goal of the association between those treatment effects at the group level. It's a very flexible model. So I didn't show you pictures of this, but we look at very non-linear settings, settings with step functions, and we're able to, by clustering, pick those different pieces out. We're allowing for the We're allowing for the incorporation of group-level covariance, which might need to better predictions overall, both in the null model and the model that includes the surrogate, which would in turn suggest that surrogates aren't as useful. And we think that that's appropriate. If you have group level covariates that are measured at baseline and they take away some of the surrogate value, that's something you should be aware of. So the clustering here gives us a way. Clustering here gives us a way to characterize and look at the heterogeneity over the different treatments, over the different biomarkers, and potentially over latent clusters that we were not considering. And we can look at the surrogacy within those once we run the model. So some future directions. We definitely want to apply this in the actual probio study, but we're not going to have the data released to us for between two and three years, depending on current enrollment. Depending on current enrollment, we want a better and different way of dealing with censoring, particularly because we know that at least partially this is interval censored data, and we are not currently dealing with that very well. We want a way of including categorical group level covariates. Currently, we're just including it in that mixture on multivariate normals. And so, we want a way, maybe just conditioning in that to allow for categorical group of equivariates. And then, finally, we want to. Covariates. And then finally, we want a different and better way of dealing with the bias and the effect estimation due to early stopping. We feel that we are dealing with it on some level with the clustering. The clustering does pull all of the effects towards the grand mean within a given cluster. And so we may be accounting for some of the bias at the top and the bottom end groups that were stopped early due to failure and groups that were stopped early due to just, you know. Early due to just abnormally large effects, but we want a more direct and better way of dealing with it. Okay, so I'm happy to take any questions. Thanks, Erin. So, any questions from the audience? Yeah, thanks, Erin. That was a very good question. Yeah, thanks, Erin. That was a very complete development. I just had a comment, I guess, which was that this is all for use of surrogate to sort of select promising treatment or treatment biomarker combinations for graduation into a definitive clinical trial. And if that's the case, then sort of as long as your method gets sort of the ranking or the order of the projected treatment effects correct, you're still probably going to do well. And so, you know, the. To do well, and so you know, the your method, I think, for this situation has a kind of built-in robustness, given that you don't perhaps absolutely care if the point estimate is within epsilon of truth. You just care, can I pick winners and losers using like this appropriate treatment effect metric? So, that was one comment. Okay, so in response to that comment, that wasn't a question. So, our vision is that these would be evaluated. Our vision is that these would be evaluated in this platform and yes, used in the platform later when they move to non-metastatic, not castrate, non-castration resistant prostate cancers. So the clinical endpoint would be much further away. So we do want to use the ones evaluated here, there. Obviously, we need to also evaluate them partially in that setting, in that context, to be sure. So it's true that comment for the use of the surrogates in the same Of the surrogates in the same setting in which we are evaluating them. But we're hoping that these surrogates evaluated here are also useful in frequentis trials, are useful in other, you know, in other Bayesian trial settings that are not just these platform studies as well. Thanks. So, hi, this is Faye. So, I have a question about the simulation studies you have. Simulation studies, you have. So basically, I think you have like two clusters identified from the simulation studies. I wonder if like there's a true number of clusters there or so, like how to interpret that. Right. So there are truly two clusters there, we know by simulation. So we clustered them based on the association between new and mu. So, in the cases where we have multiple clusters, distinct clusters, we always know what the true clustering should look like or what we think it should look like. We get more clusters than that, like in a non-linear model, it will give you three clusters for the different pieces of the non-linear, like if it has. So, in that case, it's giving you clusters, but it's not necessarily suggesting that those. Necessarily suggesting that those should only be evaluated in those clusters. Those aren't just some clusters that we like that we have given that we know the truth for. But like in your algorithm, you don't need to specify like how many clusters there should be. It should be like identified by you. No, it is data driven. So it identifies the clusters. Great, great. Thanks. You're welcome. Hi, this is Boris here. Thank you very much for a very interesting talk. I just had maybe a technical question about the Bayesian priors. Have you had trouble with setting the prior on the variance of the clusters? Yes. And how do you deal with that? Okay, so I think a proper answer to that is too long for right now. Answer to that is too long for right now, given that. But yeah, we had a bit of trouble. There was a bit of trial and error in determining how to deal with the variance there. Something that was dispersed enough, like large enough that it did allow for multiple clusters, but not so large that every point ended up in its own cluster, if that's what you're getting at. Yes, I had a similar issue, and so I'm interested in knowing how you. interested in in knowing how you deal with it yeah so if you throw your uh oh no you're not on the uh you're not on the email but uh if you i can email you and and we can i can i can send you our solution to that that was um kind of low tech but seems to work very well and is a scalable to other settings i think great thank you very much you're welcome so is there any other questions So, is there any other questions from the audience? All right, so we're a little bit over time. So, now let's welcome our next speaker, Boris. So, you will be sharing from here, right?