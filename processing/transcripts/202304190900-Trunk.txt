Good morning, everybody. Welcome to the third conference day. This is actually just a half day today, I learned. So we will have two plenary talks in the morning, including discussion sessions, and afternoon is for playing with no. Okay, first speaker is Carson Truck, and Carson will speak on spectral theory for matrix pencils. Yes, thank you, Duce. Yeah, thank you, Yusuf. Also, I thank a lot for the opportunity to be here. It's this very nice environment. Yeah, I gave the talk the title spectral CUV for matrix pensets. Of course, I'm aware that there exists spectral CUV for matrix pensets. So just the idea was here more to The idea was here more to indicate that, and I wanted to make it short, the aim of the talk is to give some, let's say, to find some object which allows spectrum theory for matrix tensors as we know spectral theory for linear operators. Of course, it cannot be exactly the same, but with the same techniques, let's say, with the same view. Okay, let's see what I have in mind. Okay, we can see what I have in mind. I posted here. What I have in mind. So, and well, the idea is to use linear relations. Yeah, let us see. Because linear relations have a spectrum theorem and the main topic of the talk is maybe to relate linear relations to matrix. Linear relations to matrix penses and to show how you can do it. So at first the definition of linear relation is very simple, it's just a subspace and the only restriction is it is a subspace of the fatisive product of some underlying space. H could be infinite dimensional, not in this term, but many things, not all, can also be in infinite dimensions and also if Be in infinite dimensions and also if I'm stressing in infinite dimensions. These could relate to finite dimensions. And what is the connection to matrix mencils or how can I relate them to matrix mencils? Well, the idea is not very difficult. It is this relation. Okay? First of all, let me recall or let me define what is an inverse of a linear relation due to this. Due to this subspace property in the Cartesian product of the same underlying space, I just can flip the elements. And this is my inverse. If you think of let me stay there. This is the inverse. If you think of matrices, you see that this is it is indeed a generalization of the inverse of a matrix, but this we will discuss later. We will discuss later. So, this is the inverse, and now the idea is the following. I look at the matrix pencil, and if E is invertible, which is not in general, I just invert E. And then I can come back from matrix pencils to matrix theory. If E is not invertible, I want to try this notion and study this object. This object. So, this is again a matrix, but this is a linear relation, and there's a product in between. So, we have to give sense to all these objects. Okay. This is a little bit the outline of the talk. I have to have a little bit more. I just know everybody here. Oh, no, this was one too much. I know everybody knows this. I just. I know everybody knows this. I just want to put the things I want to do into the big picture. That's why I'm recalling all this very old stuff. So one place where the pencils pop up is differential algebraic equations, like this, is let's say some initial value, which is not of today. And then, as in the classical Cauchy problem, it is natural. Cauchy problem, it is natural to study this matrix pencil. And now I make some comparison a little bit, a very simple one. Underlying equation, corresponding eye value problem. This I would like later to rewrite, as I indicated already, with the help of linear relations. The relations and then in classical theory one has here a Jordan form, and this is unclear so far. What is the corresponding object here? And what I can continue with solution theory, which is not the topic of my talk. So yeah, so this is what I wanted to say. This will be my main concern for today. Why the unit by the star score of the connected canonical floor is not enough for the Jordan 4, the largest dashboard of the connected canonical form of a LEC is not enough? This is not what I'm saying. You put that question on Mars? No, no, no. No, I yeah, I it's it's totally right. This is somehow connected to to this to this to this expression. So this expression is a lin linear relation. Well, signature type I will tell you about. And it shows how one can derive the drawdown form for linear relations and how this is connected with funicer. I know that funny exists. Of course, I have to relate it to funicer. So Yeah, quick. I try to make it as quick as possible. Let us start with the linear relation. And the standard example I mentioned already is, of course, a matrix with some finite dimensional space. And then you identify, you look, you need a subspace, and the obvious one is the graph, which is this one, and this is the linear subspace in the Cartesian product, obviously. And this is. Obviously, and this is, of course, it's a graph of the matrix, but it's also a linear relation. And this is a standard example. And I want to use this occasion I want to use to mention that it is very popular to identify these two notions. We identify, in some sense, a matrix with the linear relation coming from the With the linear relation coming from the matrix, or matrix with a graph. As often in mathematics, one needs to make notation simpler. You don't want to write always graph and multiply the graph of a matrix when you just multiply two matrices. So you cannot do this. So let's say from the context, one usually knows how to read it, but it makes sense to identify these two modes. It makes sense to identify these two notions. You will see it later, also. And now, this was example by linear relation. This is eigenvalue equation. And so if you look at the eigenvalue equation for a matrix, we can also write it in. You can also look what does it mean for the elements in the graph. mean for the elements in the graph and it means that this element is element of the graph of our matrix. So lambda and x are eigenvector eigenvalue if this element belongs to the graph. And now you see how we are speaking just about tuples, just about pairs. So this can be used as a definition for general linear relations. So if this element So if this element is in our if this element is in our subspace, then we say x is eigenvector of lambda is eigenvector. See, this is the way how you can translate many notions with which we know from matrix worlds into linear relation. I don't want to bother you with all the details because I think it should be clear. One can do this. Clear, one can do the same with kernel, notion, domain, range. I just put it on this slide. All first elements of all pairs are our domain. Kernel is all the guys which are mapped to zero, let's say. Range are all the second entries. Eigenvalue Eigenvalues are those lambda such as the kernel of A minus lambda is not zero. There's a special thing, of course, or not of course, we add here infinity as an eigenvalue if there exists such a zero x element. Because if you think about it, it is precisely the case when a zero is in the kernel of the inverse. The inverse. Okay, inverse I already mentioned all the guys with zero or zero y, all the y's with zero y of the relation are multivariate part. Okay, no surprise. This is a little bit more tedious. I mentioned it in the beginning. We also need the product of two linear relations. And while you do it as often like with operators. So you find the z. So you find you find the z in between such that you start with x and then you will find the z such that xz is in b and z y is an a. I was thinking why not to make an to make an exercise it it's not so important for what is going to follow, but I For what is going to follow, but I like this exercise. And of course, this is the object. If you remember, this is the object I want to compare with my pencil. So why not to train? Okay. This is the calculation. I make it slow. This is the product of two linear relations. And we just here adjust x z of the first one a. X, Z of the first one, A, Z, Y of the second. Maybe I go back just to the definition. Yeah. B is the first, L is the second, so XZ in the first one and Zy in the second. Xz in the first, Zy in the second. Just definition. And now, well, now we use the definition of the Well, now we use the definition of the inverse. These two entries change. Okay? And now, so now we have more, we have a little bit more E and A are the entrances. So this is, as I said, our physics identifications. What does it mean? That is not somehow. We precisely know what Z is. That is just AX. X, the image of X under A. So that is AX and E is and Z is Ey. So I wrote it here super long. So now I substitute Z by Ey, Z by AX, but actually this information is not really helpful. What is helpful is this one: AX equals Ey. Z equals EY, Z equals AX. So AX equals EY. And these two are always clear. There's no restriction anymore. So we end up with this. And this, if you like, we can rewrite in this way. And this is not A minus E, but it is first matrix A and then matrix minus E. And you see already this later, not in this talk, will give rise to the notion of kernel representation. It's auto-range representation. If you might apply from the other side. But this is also I don't want to speak about today. Okay. Now, as I said, I want to develop. As I said, I want to develop some Jordan-type representation for linear relations, and it's good to look at the Jordan canonical form for matrices as you all know. This is just a decision. What I want to do, as we did before, it makes sense to rewrite such things in a matrix, linear relation standard, let's say, to generalize. So we start with the matrix, and well, now comes Jordan chains. You all know it looks like this. This is a Jordan chain for the you look only at one eigenvalue and eigenvalue lambda. Now I want to rewrite it and I do it in this way that I similar with the eigenvector from the beginning, with the eigenvalue and eigenvector. Now the situation is a little bit more complicated. I rewrite it in this way. I rewrite it in this way. This is totally equivalent. It's just adding something to the other side. And now, when we think of the identification, I mean, I just write it. I mean, I write it like this is an E. Actually, this is a graph. This is an A. Graph of A, what I identified. And we see the Jordan chain, when we look at a graph, it appears, looks like this. Yes, looks like this. Okay, and it's called the Jordan shape. Okay, not a surprise. I don't do it because I don't need it, but now this I could generalize to linear relations. So this would be a linear donor shape for linear relations at lumbar. What I'm going to use instead is a little bit more, next step. Step. Now, what I did here, this is in fact the Jordan structure for one eigenvalue. You collect all chains. You collect all chains in this notion, written as these tuples in the graph. And here what we have, these are all, so you see i runs here between two numbers. Here between two numbers, and you see this are all chains have lengths one, next row, length two, and so on, and so forth and so forth. So if you look how many x1s I have here, they go from one to w1. So w1 counts the number of chains of length one or longer. Or you could say actually w1 is the dimension of the curve. Of the club. And the next is W2, W3, yeah. This is the amount of chains for banks S, your longest chains connected with number. And now, so I want to, so these numbers I want to use and they have a name, they are called wire numbers, and well, yeah, they depend on the eigenvalue, of course, and Of course, and they get the name here W lambda. Okay, and what is Jordan canonical form? Same canonical form says this is all what can happen. This is the graph consists only of chains of this form or I don't have to state it very precisely here. Two matrices are similar if the wire numbers form size and and also I can. Okay. And how to calculate wire mappers? What? Different ways. I like this one using the extortion space. Not necessarily. We can do... There are many ways to calculate them. And because I need them at one point, Siegel numbers. Siegel is the code. Seeker is the so-called conjugate of wire numbers. Seeker is this one count, the size of the drawer logs. Wire counts, let's say, amount of chains of a specific length, data counts, not counts are the other sizes of the jolly boards. Okay. And maybe I don't know. Maybe I will. I don't know. Maybe we could make an example, but maybe it's I don't know how well you how much you are familiar with these versions. It's maybe in the details not so important. So there's one way to go from wire to single, and so on. Okay, that was somehow five minutes on John Canonical Flow. And obviously Obviously, it's clear, we already saw examples. A subspace, linear subspace, linear relation is a much more general notion than matrix. So there are chains which are not of the form I presented you here, which are not of the form of Jordan chains of matrices. So let us get a feeling, let us discuss a little bit what can happen. And so first of all, the problem. I have here three examples of that one. And the first one we saw already, this was this multivariate part. There is a guard which belongs to the eigenvalue infinity, 0x. And this cannot be, I don't, I don't go back, but this cannot be represented, clearly not represented with border chains of matrices. So I add here one more example: there's a chain that becomes longer. To indicate not only zero x, such change will also be happening, cannot be expressed as the order chains of vertices. But there are more, they are more. This guy starts at zero at exit zero. Looks looks not dangerous, but it is really yeah. Yeah, serious sentence here. I will show it to you why it is a little bit yeah when it comes to spectral theory. Is this the guy you don't like, let's say? So what happens? Let's say our linear relation looks like this. It's a subspace. So we can take elements and add them, multiply them, and add so on. We take this element, multiply this element by Multiply this element by lambda and add these two. Then we get this one. But this one, remember, is an eigenvector for the eigenvalue lambda. So, but so this means I wrote it like this. So, but but lambda, this lambda you could take any lambda, right? So all for all lambda. For all lambda, x0 is an A3 minus lambda. It's a kernel element, or this one is an eigenvector eigenvalue. So we see that the spectrum is a convex plane, and as we have 0x already in our ligamulation, also infinity is our spectrum. So it is all of the plane. And you see a little bit this effect is somehow also familiar from basing. Familiar from what you for the Lafayenbe. And of course, I can extend the chain like this. Okay, just to let you know that not the only possibility. Last example, we call it multi-shift. Let's say our linear relation looks like this, and these two guys are linear independent. guys are linear independent, then clearly we cannot produce an element of the form x number x because first and second entry are linear independent and here first and second entry are dependent. So you don't have any eigenvalue and again this chain could be also longer. Okay and somehow now we come to the main result. Somehow, but that's why I put here these three three cases. That's why I put here these three cases. And sloppy speaking, these journal chains of matrices, this, this, and this is all what can happen in a subscript, which is maybe not so clear. Okay, let me present the result. It is somehow joint decomposition for substrate. Jordan decomposition for subspaces. Of course, into other subspaces with specific properties. Otherwise, this would be, yeah. So we have the right way to look. I will explain it. So we have such a decomposition. Actually, it consists out of four parts. If you look at, because I will say maybe I'll say it. Yeah the the this j lambda something, these are this is shorter than canonical formula. This is exactly what we had before. This is you see a linear relation is a generalization of a matrix. So there should be somehow Jordan canonical form inside. So if I drop A M. So if I drop AM, what I call it AM, J infinity in this guy, I just have a matrix in Golan canonical form. This average I showed in a second, this J infinity corresponds to infinity and is connected with the eigenvalue infinity. Or with the multivalue part. And then we have two guys, singular part, multi-shift, which don't. Shift, which corresponds to the corresponding things I showed you before, to the singular gates, to the multi-shift. So, let us start suddenly. So, first of all, this is a reduced sum decomposition. What is reduced some decomposition decomposition? I don't want to say. It is connected. It took us really a while to notice this. When I'm telling you, Jordan canonical has something more which we somehow don't notice because we always have these Jordan blocks in our maybe you notice I haven't noticed which also means that this gives you invariant subspaces the Jordan canonic form also has some invariant subspaces into the various spectral subspaces here but this this you need to reproduce here all To reproduce here, also, otherwise you get something different. It is very dangerous to decompose the subspace into four because you have two, you know, even difficult to compose to decompose the subspace into two. But into four, you have too many possibilities how to decompose. And this makes things very, very difficult. Okay, however. So there is hidden some because what this talk is not This talk is not, it is for the theory, but it's super important for the talk now. There are some invariant subspace properties. So, slowly speaking, these guys, this is direct sum, these guys, this is not only a direct sum decomposition of the linear relation, but also of the space. And each of the components within this, yeah, so Yeah. So, it is also good question. I have not. I just said I have a what we have here is a finite dimensional vector space. That's it. 2n dimensional. No. For simplicity, I wrote here C M. Sorry. I wrote here C M, but you see, I never make use of C M. This could be any final. What I want to say is, I have. Maybe finally what I want to say is, I have no inner product. Of course, CM has inner product, but I assume just let H be a finite dimensional space. And in this sense, the only object I have is direct target. Okay. Yeah, so now I want to discuss a little bit. I did a bit these components. First one, Jordan blocks for matrices, all these guys here. But I go quickly back, but I mean, the reason why I wrote it down like this. This one, yeah? The J's looks like this. As everybody knows, from Jordan form, and okay, now comes J infinity, which is. Now comes day infinity, which is essentially also simple because what you can do, this corresponds to the eigenvalue infinity, to the multi-value parts. If you invert this linear relation, you get an operator with eigenvalues only at zero. But why not? How does it look like? Well, no surprise. Multivariate parts, they Multi-value parts, chains of links, two chains of links, and so on. These guys, you can slowly imagine what I was what I have in mind. I want also to count the chain pads. I do it in this way. And I call it A. And now the next slides are of similar stuff. So we go. So we go now to a thing and it looks like, as in my example, of course, a little bit more gent. This is the shortest one. Second, you arrange them like this, and you get some numbers. And these are, I'll call it just wire numbers at D or at singular chains. And the last one. One multi-shift somehow looks like this, like hot numbers. And you see, I find it very nice, four types of chains. Jordan chain, singular, well Jordan chain at infinity, and multi-shift, of course, for different tracks, for different types of chains. It is clear where we are. It is clear where we are heading to, that's it. Yeah, this is copy and paste. So now I collect all the numbers because I will use them later. Okay. And I call them wire numbers. Why not? Because we are counting the We're counting the chain, we're keeping track of the chain links, that's it. And now, first important test is this one. If two linear relations are, I say what strictly equivalent. Two linear relations are strictly equivalent if and only if the wider numbers coincide. Now this is somehow that's why indicates that maybe this is the right thing to do. The right numbers. The right numbers. What is strictly equivalent? Yeah, it is this. You see, it is a stone from the matrix world. You multiply from left and right with some other invertible. But this is important with some invertible matrix. So T is a matrix. Again, we are sloppy identifying matrix with its graph. T is a matrix, but this then is a But this then is a it's also the multiplication is in the sense of linear relations Okay, so this is just the why does quantite meeting also with the I values first multiples Oh no I want to avoid this. They exist if you have multi well If you have multiple, well, the problem is the what is what we want to consider as an eigenvariant, and the problem is the following. I go back to the I go back to the problem is this what if in your linear relationship Is this one? If a new linear relation pops up this guy, the singular chain, then the standard notion for eigenvalues gives you that the spectrum is a complex pair. So actually what is happening here is I try to avoid this to present. That's why I just want the exist eigenvalues. But these are specific special ones. And what you do, we can do it more precise later. So there is, this is also part of the theorem. There is finitely many eigenvalues which are especially good eigenvalues. Which is actually, if you think of chronicle form, other eigenvalues in the job are also chronicle form. And here it's the eigenvalues that may be a bit sloppy, which are not coming from this process here. So you look at all chains, of the algebraic subspace, and somehow you not subtract, but you remove all these bad at the singular chains. But if there's still something left, then this is a proper, we call it proper. then this is a proper call a proper idea so and these guys are uh but then you then you can show exist or definitely maybe and this are here i i i shorted it for the presentation okay going back strict equivalent we discussed it and this is the subspace uh it would have better answered no Now comes the subspace we have to subtract. This is a singular subspace, and there you collect all, it looks a little bit ugly. There you collect all bad elements from the singular chain. Singular chain. Okay. Singular change was something like 0x xy make it shorter. Y 0. So you see, this is, this guy is in the query of A, right? And this guy is at the tanks. This better here. This guy is in the parallel of A squared. This guy is in the X is in the multivalued part of A. And this guy is in the multivalued part of A squared. As you see now, for example, you can check what is it Y is in the kernel of A and Y is in the multi- A and Y is in the multivariate part of A squared. And this is precisely what you are doing here. These are all chains starting at zero, go up. Very long. These are... Sorry, this was wrong. These chains are starting at zero, go up. And this comes from somewhere and goes down to zero. And this means the guy in here has one chain that goes to zero and another. As one chain is go to zero and another and another one before, which also goes to zero. Looks, I mean, looks like this. You have to say all that. Every element from here looks like this. Maybe you have to add your first longer chain. And these are precisely the one I don't like. No, I don't like. And the next question is: if you Nobody asked me, but the natural question would be: How can you calculate these numbers? Without any help, this is impossible, right? Of course, you have to make the right decomposition. However, there is a better way to calculate the operates. You know, also why numbers for linear relations are not really easy to calculate. Somebody asks you. Oh, somebody asks you to use here three by three matrix, if you three by three or four by four, to use all chains, tell me what chains we really have. It's not so easy, I would say. And this is the formula for it has some beauty actually, I would say. So here's the typo, J. What are you doing? You do. Doing you do, yeah, you do here's j minus one, you do these portions, you do these via numbers for matrices. But we already discussed a little bit, it will be spoiled by the singular chains. And this you take out, we add above, here below, and then you take via quotient space, you take them out. You have to You have to add them. No details. A little bit artificially, you put all singular chains in here, but the important thing, you portion it out by this one. This is some space here, and this is the space you make the portions, but that contains R C. That means here, each element in the above. That in the path, which is only in R C goes to zero because it is in the lower part of the quotient. You have to read it like this. This gives you the wire numbers related to Jordan chains. Okay. And and um and the other three guys also have formulas. Also, have formulas. This is of the same style, right? This is connected with algebraic subspace as infinity, and you kick out the singular chains. And this one is the opposite one. You concentrate on singular chains because in this portion you want to measure singular chains. And the last one is you see already what's. You see already what they said in range and so on. But okay, now we finish with this. Comes back to more known, well-known things. Chronicle colonic form. I don't think I do it as quick as possible. I write it in this way. Standard indices. So yeah, there is. There is for here alpha beta gamma for the three different parts with the conflict makes you know this. One thing is really important to mention, and this is what is usually done in Kronica, we do block sizes. So block sizes is secret, not wired. So the Konica in this comes as Secret numbers. Becomes a safer number. And this is one thing I want to mention. And the second one is better than me, there are different ways to count. What is K gamma and so on? And we like to count starting with one. And many others start to count with a zero. What? I'm also a member of the one logging because it fits better to what I'm presenting now, the operations of the process. But of course, it is just adding or substrating the one. Okay, and this is just a repetition of the four indices where we see for. The four indices we received from Conica, just to recall the names. And now I want, it's clear what's clear what is coming now, right? I want to compare them with my numbers, with the navigation numbers. And of course you cannot compare immediately because these are written down in Sega and I'm in the wire world. So we have to make, as a first step, we have to make. We have to make sure we just make a small exercise. So let's say single. Simple looks like, let's say seven, five, one, which means I have one block So, yeah, I have seven. Seven. One, two, three, four, five, one. So, one chain is seven, one chain is five, one chain is one. Or one block. Block seven, block five, block one. And now I make wire and then you write there: seven, five, one, which you have to make, keep in mind that this is zero, of course, is. Course is well, how many chains of length one I have? Three length two, two lines right I feel so maybe I need some cover and then one one, I hope this is one. This would be one so I have to answer. So it's right. Yeah, looks good. Okay, and now comes the wire. I mean, yeah, I mean, it's it. No, it's it. So we start with a matrix pencil, arbitrary matrix pencil, corresponding Konega indices as above. And then we take what I have. What I have let's say in some sense the corresponding formulation and it has I hear the j's are maybe not so good with the corresponding wire numbers I just introduced and then we get this formulas. So what is it? They coincide. They coincide. The first but only the first three. But only the first three and then there's a shift. Okay, so if the third, if the third one, this guy, looks like this, I lose in my emulation the first two. So C and C is of course I didn't go to. C is of course additional voltage there. C is these are the CJs. So C is I mean it's clear, yeah, but nevertheless is CJs if they are not indexed. Sorry. The deep sea bit C so again if the if the gamma is of the if the third fourth minimum indices is gamma actually gamma conjugate if I have this entries in my gamma conjugate then the big C of my linear relation looks like this which means I lose the first two The first two entries, going from the conical EDCs to the wire numbers of my linear delivery. And I will not put it here. In some sense, you start in this theorem, you start with the pencil and you calculate the numbers for the linear relation. And there's also one, you start with the linear relation. You start with a linear relation and calculate the number of the penses. But if you start with a linear relation, you don't have the matrices in the beginning. You have to make some extra care with the matrices. But it is all under control. So this theorem and the one I presented allows you to jump from one world to the other. To that. Let me thank you. And as a last remark, this allows for spectral theory of linear relations in the scale of relativity spaces.