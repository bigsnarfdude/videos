And uh to to kill the suspense uh so zero, one and so there's going to be a threshold uh depending on sigma one, sigma two uh so before this threshold, so minus one as well, before this threshold you have an ash equilibrium in uh in the pure strategies and right after the threshold you need to you need weak strategies, you need to hide what you're doing. So, intuitively You need to hide what you're doing. So, intuitively, the player behind the red player needs to hide what he's doing. And the usual way to hide what you're doing is choosing public emails. Starting with the same initial volume. What do you do? What do you do? No, it's not a draw. Yeah, I will answer that. I will answer that in the answer will appear in despite me. So another game that you all have played and that I uh still like to play uh with my son is rock, paper, scissors. And this is an elementary example where you have no Nash equilibrium and in this uh three uh three element set Uh three element set. If you allow mixed strategies, which are just probability measures on this three element set, then you uh the couple of strategies given by uh the uniform measures on rock, paper, scissors, they form an actual. So the question is how do you do you define a nice notion of mixed strategy now if you have continuous time and if you have ordinary motion driving the processes. So there is not so much media. There is not so much literature on that problem and you have early papers on Eliot, Marcus Calton and people around him that were doing mixed strategies for deterministic differential gains. You have all the papers looking at mixed strategies where only the twift is controlled, and this is a big difference. I will try to explain why. Explain why. And maybe the paper was a framework that is the closest to what I'm going to show is a paper by Big Dan Lee and Pencropois. So what they do there is they discretize, sorry, the time interval. And along this discretization, at each point in time, when you play a probability measure, it's easier to define. And then they look at the limit when the size of the discretization turns to zero. Let me also mention Let me also mention a paper by Thila, Nisa, and Jan Feng, who are all here, where they study gains in weak formulation. And at some point in the paper, they take a D2 via relaxed formulation to prove well-posedness of some signal border. And let me also mention a recent paper of Stefan Julian, a student in Vienna, where they do some similar games. They do some similar game in a discrete time, discrete space setting, so it's a random world game, but where what you allow as the step sizes is very small because the combinatorial complexity explodes very quickly on that type of bit. But they also have correlation in it. Okay, so let me take a few minutes, two, three minutes to or take a detour of the uh deterministic control problems to try to motivate the introduction of uh Motivate the introduction of relaxed controls. So, and by looking at the standard and elementary phenomena, chattering phenomena. So, in this control problem, so you have a control process x u, which is just the integral between 0 and 1 of ut dt. And you want to minimize the integral of the square of xq dt. And the controls u they take values in minus 1, 1. Used in minus one one, only minus one one, nothing infinite. And in that simple framework, the infimum is equal to zero. And to see that it is equal to zero, you can look at this particular sequence of controls, which are functions that alternate between one and minus one along the possibility of zero, one. So for these functions, the integral of u is going to be a triangular function, and if Triangular function. And if you take the integra of this triangular function squared as a function of n, you get something like one over n squared. And so the as a limit when n goes to infinity, of course, zero and the inference is zero. But there is no, this inferenum is not attained. And because if it was attained, then uh it would imply that u uh have to be equal to zero and this is not uh this is not allowed. So this is very very standard. And uh the idea is to embed the Is to embed the class of controls into a larger set with more convexity and more compactness properties. And this is choosing probability measures, or intuitively at each point in time, to choose the probability measures on P. Okay, now if you want to do that, so it's really an embedding, in the sense that strong controls, if you want a strong control, then you just have to choose a Dirac mass. Then you just have to choose a Dirac mass at time t. You choose a Dirac mass at point U of T. And this sequence Un that did not converge before now in the space of probability measures converges weakly to the uniform measure on 1 minus 1 times dt. And if you plug that into the controlled, relaxed controlled process, well you get that x Well, you get uh you get that x itself is equal to zero, because you integrate the mean of that particular measure, so zero and uh the integral of x squared is also zero. But the message is that the infinite is now attained. Um okay, so end of the detour now exactly the same framework but in a stochastic control setting. Now you integrate U, but with respect to a Boolean motion system. And so the temptation here would be to do. And so the temptation here would be to do exactly the same, to take the relaxed control process and to plug in there the mean of the measure paying the whole relaxation. Well, if you tell them some reasons for which you should not do that, so I will give you one or two reasons. So one is that, yeah, not do that. And one reason, so if you go back to this example of functions going back and forth between group one and minus one, so As well. So integral of Q n with respect to Brownian motion now. So this process here, it's a Brownian motion itself. Okay, so it's bracket is T, it's a Brownian motion itself, but in the limit, it's going to be zero. So this is saying that the relaxed control process is not continuous in the controls. And intuitively, what's happening, so why did it work before and not with one-hand motion? Is because these functions going back and forth between one and two. Functions going back and forth between one and minus one, they are very noisy when n is big. But this noise in front of the Boolean motion, Borneo motion says I don't care. In distribution, it stays exactly the same. So it's not doing anything to the Boeing motion, to the law of the Booyant motion. So if you integrate with respect to such an irregular process, you have to do something different. And one idea: so we saw that relaxation when you have DT is maybe. When you have DT, it's maybe easier. And for the game that I want to look at, I know exactly what the bracket of the control process should be. So I can do the relaxation for the brackets and then look for a probability measure on the canonical space such that the canonical process has this given bracket that I need. This is almost exactly the definition of a martingate problem. And so the Martingate problem formulation here is a Problem formulation here is a natural way to state the problem. And some references describing all that is: so you have this, I was going to say well-known, I don't know if it's so well-known, paper, so it's compactification method in the control of the general diffusions by FRW from Brown and Bien. Also, have a paper of Osman around the same time with similar results. And I would advise a nice lecture notes of Dan Leker about Of Ban Maker about relaxations and applications in mean field games. And I think Udo Ric mentioned that yesterday. So now I'm going to introduce really the formally the framework that I'm interested in. So you have a finite time horizon, your A is now sigma one, sigma two. So the strong controls, they are functions from time interval. From time interval times R2 in A. And yeah, I look at the canonical formulations. We have a canonical process with X interpreted as the state of layer 1 and Y as the state of player 2. And the generator of this system of processes is going to be the following. So you can write it in a compact form like this. It in a compact form like this: this matrix times the matrix of second derivatives of F, which is just saying that you want a coefficient diffusion equal to A for the first process, B for the second process, and you have a cost derivative appearing because you have correlation. Now, because I want to do relaxation, I need to put probability measures in there. probability measures in in there, in L. So we'll keep calling it L, but it's a L depends on mu1 and mu2 and uh and what you do is just integrate with respect to the product measure mu1 times mu2 to define this generator depending now on probability measures and the set of relaxed proposals. So what I'm going to show next, I don't use compactness arguments to show Compactness arguments to show existence of a Nash equilibria nowhere, I just use a set of relaxed controls to exhibit a particular explicit solution. And because of that, I can simplify a little bit the definition of relaxed controls in this context and define it as maybe what you would expect more intuitively, which is that at each time you choose a probability measure on A. So this would be the set of relaxed controls. Port and so this is a martingale problem. So if you give me Q1 and Q2, relax controls as defined in the previous slide, I want to define a probability measure P depending on Q1 and Q2. And I will say that this probability measure is a feasible distribution if so the first condition is just saying that at initial time I wanted to start at x and y, and the second X and Y. And the second property says that I mean describes the law of the canonical process under this measure, because it's saying that for a class of regular functions, I want this quantity here to be a martingale. And this quantity here is just the beginning, if you want, of the Ito formula applied to f of x s and y s. So it's really telling you what the law of x the couple f x y should be. And okay. Yes, so maybe let me say right away that it's not clear at all that there is existence for such a martingale problem. It's not clear also that there's uniqueness. We give some sufficient conditions in the paper. So for example, if now one is strictly positive, as in the talk, and rho is strictly less than one, then you have existing. Less than one, then you have existence. Otherwise, you can still define the game, even if you don't know about existence and uniqueness of the Martingale problem, by using a trick that we took from the paper I was mentioning of Dylan Desar and Jumpfront. But in the talk, I mean I can show you the trick if you if you want, but in the talk I will just to simplify a little bit, do as if I had a solution that was unique, not to define the game. It was unique to define the gain. But I could define the gain anyway. So the criteria is now: so I have this P, Q1, Q2, and I take the expectation under this measure of a function, G, only depending on the difference between X and Y at time capital T. So this explains the term ranking games in the title. So it's ranking because it only depends on the difference between X and Y, and in the first And in the first uh example transit, what I showed you in the the first example was function g of uh z was just indicator of r plus of z. You want to end up in f. But it could be so here we allow for it as a general class of functions, and I will show you some examples in the end, some other examples, this one. So we can define the upper value function and the lower value function of the game. So ν is just the class of relaxed controls that I defined above, assuming that I have existence and uniqueness to the Martin Gale problem. And now I would say that the couple of strategies, Q one star, Q two star is a saddle point of the game or an equilibrium of the game if these couple of strategies are optimal responses to one another. Responses to one another. So, for example, the maximizing player, if you know that player two is playing Q2 star, then his optimal response is also to choose Q1 star. And similarly for player two. And now I can show you an explicit solution in this uh this framework. So to show you the explicit solution, I need this function w of t and z. So this function is, I mean the formula looks complicated, it's pretty simple. I mean the formula looks complicated, it's pretty simple, it's just an integral of the terminal condition, terminal the criterion G with respect to a Gaussian kernel. And in this Gaussian kernel you have a variance C of rho squared. And C of rho is this value depending on sigma 1, sigma 2 and rho. And you see that the value changes uh depending if rho is uh uh less or bigger than some some threshold depending on similarity motu. Threshold depending on sigma 1 and sigma 2. And it's not continuous, there is a jump here. Yeah, so clearly also you can see W as a solution to the heat equation with constant diffusion coefficient C of O and with terminal condition given by G. So heat equation with uh terminal condition. And if you reverse time you can see G as the initial source of heat and then uh heat temperature propagates. temperature properties. So it's just a solution to and in particular because of that W is very regular. And because it's regular, it's in particular C2 is the space variable. Because it's C2 I can look at where it's convex and where it's concave. So these are the convexity regions of the this function w and the and the concavity regions. So now that I have d plus and d minus, I can look at what a solution, a Nash equilibrium, looks like. So I'm showing in this first slide a Nash equilibrium if rho is small enough, if rho is smaller than this particular threshold, then this couple Q one star, Q two star, constitutes a Nash equilibrium of the game. So what is this? Okay, so Uh okay, so let's say rho is very small, so that's the maximum between uh um yeah, let's imagine that the this is equal to sigma one. So you are in D1, so you are in the concave region, which means you want to play safe and you are going to choose a sigma one. So here you see that the measures are only Dirac masses. So actually here So, actually, here what you find is strong controls. So, this is saying that there is a Nash equilibrium in strong controls if the correlation is smaller or equal than this constant threshold. Now, if it's larger than this threshold, so this is a Nash equilibrium, and now you see that one of the players really randomizes really randomizes because now you have Randomize it because now you have a proper probability measure here, and it is a convex combination of only two Dirac masses at sigma 1 and sigma 2 with some explicit weight. And here, so yeah, so the player, so here is in D plus, so he is in the convex region. So this the maximizing player here is behind, and if you are behind, you want to hide what you are doing, and you are going. To hide what you're doing, and you are going to hide what you're doing by playing this particular probability measure, this particular relaxed performance. Okay. Do you have any questions so far? It seems that you have no problem to have x equals. Yeah, so now let's come back to that. So if x is equal to y, you just have to know if t is equal to plus. If T is rho is in B plus or exactly, exactly. But intuitively, what is yeah, so I was trying to find an ex intuitive answer to your question. What what do you do? But we can I'm going to show animations in the end and we can try to see the point zero where it is okay so one remark. So and by the way to prove so in the paper we for example for this yeah for this couple particular Yeah, for this couple, particular explicit couple q1 star, q2 star, it's not clear at all that for this one you have an existence of the associated martingale problem. So to prove that, what we do is we write a stochastic differential equation representation of the martingale problem, and then we show that this particular SDE has a solution using either existing results or in the case rho equals one, for example, you have to explicitly construct it, but it's You have to explicitly construct it, but it's not particularly hard. And here is the SDE representation of the martingate problem that I showed before. And so you take PQ1Q2 solution to the martingate problem of the previous slide. And yeah, modulo, an extension of the probability space, you can represent the canonical process. Uh the canonical process XY as solution of a system of SDEs, where now you have additional terms. I remember if these red terms were not here, this is exactly the temptation that I told you we should not write. And in fact, if you want to hide what you're doing, you need an extra term depending on an independent variant motion. And this simple context, the integral in this extra term is just the standard deviation of the measures that you are using. So if you choose strong controls, standard deviation. Choose strong controls, standard deviation is of course equal to zero, and you just end up with the usual STE formulation. But now, if you relax, then you have this addition of terms in the stochastic differential equation. So this being said, I still have a little bit of time. I can show you where these measures come from and how do you end up with such measures. So this metric. Such measures. So, this matrix C, which was useful to define the generator, you okay, and also make it depend on measures by just integrating all its values with respect to the product measure mu1, mu2. So, this is just a simple thing depending on the moments, the first two moments of mu1 and mu2. And the proof is based on just HGB equations for the value function. Equations for the value function. So we write the upper HDP and the lower HDP equations associated to the gain. And you have a non-linearity H plus and H minus. So H plus is going to be the tube in the H minus of this compact representation of the generators that have been before. And yes, so yeah, so H plus depends on a matrix because it's you give him Because it's you give him the matrix of second derivatives of. Okay, so there are three maybe big simplifications that happened that allow you to solve the game explicitly. The first one is to realize that given the formulation of the game, so let's imagine that x is not equal to y, as I put here, if you start very high or very low, but with the same. Very low, but with the same difference, it should not affect the value function, because it only depends on the difference between only ranking. So let's first assume that the value function is like this, so it's a function only of the difference, and later in the end we verify that it is indeed the case. So we assume that, and if you assume that, then this matrix of second derivatives simplifies a lot because of this form, and And sorry, can be a function V add of T minus C is the because I don't see symmetric I don't play the same if I am a 100 volt U that if you have a 10 100 volt B? Yes, because in one case you have a volume. Yes, because in one case you are behind, in the other case you are. Ah, okay, yes. X is your own position, let's say, and Y is the position of the other player. And V is the value, it is minus the value of the game. And V is the value of the game. And V is the value of the game. Patterns are symmetric, right? If you look at what you showed. Yeah, if you s switch. Yes, the strategies are indeed symmetric, yes. Sorry, what? Yeah, okay, so strategies are symmetric and the value is independent of uh so you will see in that it's not going to be a symmetric function of this this variable here, let's call it uh z now. Z and uh okay, so now if you write the exact condition that uh H plus you look at what it looks like the exact condition H plus equal to H minus on this particular subset of matrices, then it simplifies a lot and H plus is going to be equal to H minus if and only if this happens so you have some kind of associated static gain for Game formulated with probability measures, and the criteria of the game is this very simple criteria. So, this now comes the second simplification. So, this has always has an equilibrium family. This static game always has a Nash equilibrium. And it turns out that you can compute it explicitly. And one way to do it is to use all the results of Winkler, because you Winkler, because you, for example, if you want to compute inf, you can completely calculate it completely explicitly. So you first compute the supremum of this here, but this is the supremum of a linear function in mu1. So it's going to be attained at some particular Dirac mass, but it there is a constraint in mu2. So if you write the problem, it's going to give you a moment. So depending on the first moment of mu2, you are First moment of mu2, you are going to choose a Dirac mass either at sigma 1 or at sigma 2, or even at the middle point. So when you solve the infimum in μ2, the infimum in μ2, you are going to have this. So you check, you are going to have the three conditions, you do the three optimizations, and you take the one that is minimal. But these problems consist in minimizing a linear function of 2 with a moment constraint. With a moment constraint on mu two. And you know that uh the minimizers are going to lie in the set of extremal points of probability measures on A with a with one moment constraint. And the extremal points, so if you look at the results of Winkler, it tells you that there are Dirac masses at here two points at most, because it's the number of constraints plus one. So this and these two points, when you can These two points, and you can calculate them explicitly by solving this particular static. And the measure that you get here, so it's not completely elementary, but it is rather long to do that completely. But then you get this explicit measure that I showed before. And the last simplification, so this was the second one, and the last simplification is now you plug this into you look at the You look at the HGB equation, so for example the HGB equation for the upper value function, but remember now Z represents x minus y. Now we assumed that it was only a function of the difference. So now it's one dimensional here. So it simplifies to this. And so it only depends on the sign of the second derivative of this function in Z. So in general, this should be non-linear. So in general this should be non-linear, this should be non-linear as well, non-linear PB. But if this guy here is positive, you can take it out of the inf. And if it's negative, you also can take it out, but this inf sup is going to become, yeah, this is the minus sign is going to become sup inf. And we know that because of what we calculation that we did in the slide before, that the sup inf is equal to the inf. Simple in is equal to the in SU. So, in any case, you pull it outside and it's multiplied by the same value, which is just this the value of the static gain that we were looking at right before. And now, what is this? Well, it's just a heat equation with a particular constant diffusion coefficient with G so this is the C of rho that I was uh showing before. The value C of rho comes from the value of this uh static gain. This static game. And okay, and the last step, so this was the third simplification. It was that the PDE becomes linear somehow. And the last step is to perform, now we have a candidate and we verify that it is indeed uh it gives the Nash equilibrium of the game. So the last step is to perform with a verification and this is pretty pretty standard. And this is pretty standard. So, this is how you get the heat equation. And for that, I can show you five minutes. I am not in the I'm not in the right spot. Okay, I have to turn around. So, what is this? Yeah, so I just took a particular terminal condition. So, okay, by the way, you can take many functions and explicitly calculate the regions where you should play one measure or the other. And for all the examples that I'm going to show here, you can, in fact, you don't have to show an animation, you can compute them. And for so here, what did I take? I take took an indicator of I think zero, one. Of I think 0, 1 plus twice an indicator of being larger than 1. So this is how I compensate you for being above the other. And you see that the solution only depends on the concavity and convexity regions of that surface. And actually, so yeah, so you have time 0 and 1, you have this difference x minus y. And so you need, so for a fixed time, to know what you play, you look at, so here's, it's Look at so here the it's first convey and then it becomes convex. And this uh deflection point, it's not going to be constant in time actually in this example because if you zoom in, it's not very visible, if you zoom in very close to the maturity, it's going to be concave, then convex, and concave, and then convex again. And these regions you can you can write them down quickly. And another example For example, it works trying to execute this it's hard to do this. Why he's not executing my thing. I was like, oh yeah, okay. Yeah, so now I took another terminal condition, which is just the indicator. So g of z is the indicator of 0, 1, which means one of the player wants to end up very close to the other one, and the other one wants to escape. And so this is like. To escape. And so this is like a super simplified version of prey and predator game. And now you see that this region, again in this example, we can write them down completely, but it's not that you just need to know where this surface is convex and concave and again around the maturity it becomes a it becomes weird and you have you have new regions popping up from from nowhere, aren't you? From now I launch. At this time, maybe I will stop here. Thank you for your attention. Here we have some time for questions. Yeah, I found the result of the integral change, depending on the level of the whole collision, very super intricate. But I'm I'm not sure how to interpret it because in applications Because in applications, the correlation is actually controlled somehow. So I'm wondering if you can include that. So the reason I'm asking is because I relate what you're doing to a very simple application, which is if you want to buy a house in silicone valley, the best thing that you can do is to invest in tech stocks. Because if the tech stocks go up, then everyone is going to become wealthier. So in order to come. worked here so you in order to compete for that house you need you need to have a correlation so i'm i'm wondering if the correlation can be controlled in your framework in a model with two bronions and then you you modulate the exposure to each pronion this way so i'm wondering if this is a simple extension of your so first of all I I don't know if you could So first of all, I I don't know if you could uh extend it like this or not, but you could imagine that you have uh so in this particular case two players and yeah, so it's not going to answer your particular example of buying a house, but you choosing the level of correlation coefficient row could be a way for some principle to give incentives to the two players to behave in a way or the other by choosing the correlation. Other by choosing the correlation coefficient. And because here the model is simple enough that the results are explicit, then the probability that the problem of the principle, if you choose row, is solvable. But I don't have any application in mind where you can control the row. And also, it's pretty. I was also thinking at some point of what does it mean? Thinking at some point of what does it mean? It's to integrate with respect to the product measure mu1 and mu2. There you also have some margin. You could imagine that they have to hide themselves with probability measures that are not independent. And maybe there are but this is mainly theoretical. Or questions? For the technical question of the martingale problem, you said that if sigma 1 is positive and rho is away from 1, then you have you should have some solutions directly, right? So can I just pass two limits to get the limiting cases? Martin, you're probably generally Generally, because of the beast of hardness of the measures, such as the limit could be easy, but there are other variables in the base of eight, so it doesn't I don't know because in the the way we did it is that in the case for example where sigma one is strictly positive and rho strictly less than one we used existing results and I don't know if this for example And I don't know if this is, for example, existing results by Philoff, and I don't know if these existing results are constructive or not. And when rho is equal to one, then we explicitly constructed by hand the solution. And I don't know if taking the limit in this constructive procedure gives you the explicit construction that we did or not. But to answer your question, I should have a look at the existing proofs. Of this particular LTEs, these constructive proofs are not, and then can we take the limits there when sigma 1 goes to 0 or when rho goes to 1. And I don't think the proofs are constructive. All right, if there are more questions, let us thank Naim again. 