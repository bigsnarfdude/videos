So, the next speaker is Michael Wiener from Earthly Lab to the US. So, he will be talking about some examples of machine learning and statistics in some real cases. Yeah, I apologize that I didn't have an abstract. And the reason for that is because I didn't have any idea what I was going to say a week ago. And this talk is a lot like the pictures I've taken in the last week in Spain. It's sort of a slide. Spain. It's sort of a slideshow. They're not particularly artsy, and they're really only interesting if you haven't seen them before. So, hopefully, that'll be the case. Standard disclaimer, I'm only speaking for myself. The first use case I want to show you, maybe something you have heard of, because it kind of made the news. I call it the Facebook app. And can we use pattern images? Pattern recognition, image recognition, machine learning to find storms. Now, you know, in the real world, storms are relatively easy to identify because they happen in real time. But in simulated worlds, they happen very fast and you can make lots and lots of years. And I tried counting them by watching movies. That's not really feasible. It's a quick route to insanity, I'm afraid. And so I use a machine instead. And so use a machine instead. Some kinds of storms, hurricanes, and tropical cyclones were the things that were most interesting to us at the beginning. There are algorithms, heuristic algorithms, attract these. And my colleague Prabhat said, well, can we train on those data sets? And some other kinds of storms are not so easily tracked by machine algorithms, heuristic algorithms like frontal systems. I'll show an example on that. And so, you know, can we use these? And so, you know, can we use these image recognitions that are on three channels, send them to science by adding more channels? And the answer is yes. And this fellow here is Mama. He was the leader. And they ended up taking this on my model data and winning the Gordon Bell Prize for high-performance computing. They were the first scientific application to reach an exa-hop on the bridge machine, the frontier. Machine frontier. So that was kind of a big deal. Let's see what this is his slide from Prabhat. You know, the Facebook app is up there. You know, we have a kitten, see a bunch of kittens and dogs. And there's a hurricane in there somewhere that it can find and local on this. And that was pretty cool. I told them, yeah, that's cool, but I already have good algorithms for this. Can you tell me something I don't? Can you tell me something I don't know? And this is where the frontal systems came in because these kinds of storms that move through continental interiors are not well tracked by heuristic algorithms. There are some out there, but they don't work very well. And so, what the United States National Weather Service does is they still have people doing this. And the way it works is twice a day, three meteorologists draw the fields. Draw the fields and their boss picks the best one. That's the official frontal system. And so this data set goes back to 1950 over the continent of the United States. And so Jim Baird and Ken Kunkel at NOAA basically repeated the same thing with the TensorFlow code that Prabhat did. And we published it in a journal that you can get an advertisement for later. And this is still being used at NCAR by Katie. NCAR by Katie and others. And it's kind of interesting. This is an example of what one of the training set days looks like. And this is what the machine made. He was trained on half the data, then looked at the other half. And actually, this is a better picture than the humans. And when we first saw this, my first reaction was, how could the machine be better? How could the machine be better than the human? And Prabhupada said to me, Well, actually, in image recognition, machines have been better since about 2011 than humans. Some standard data set you may know about that looks at numbers. And the reason for this is that from 1950 to about present, when this data set was made, there were a lot of different meteorologists. Meteorologists, and it was always three different anyway. And so they make different errors. And so the errors that the humans made were random. And machine learning algorithm actually, when it's giving that kind of, for the hurricanes, we use one heuristic tracker to train. It replicates that pretty much accurately, the same way. And so the errors that the heuristic tracker made, machine learning algorithm made. But here, But here we get more power actually, so this is actually better. Of course, now I can this could be applied to ascetia to climate. But you know, tell me some more. And so what Prabhat and some others did is they said we don't really have a good labeled data set for storms. For storms. And so they ran a campaign where they sat people down in a hackathon and they made a thousand labeled images. And you can see here, you know, they took different kinds of storms. Here they've labeled some atmospheric rivers. They did tropical cyclones and extra-tropical cyclones. And when they apply that into this algorithm, into this pattern recognition algorithm. Pattern recognition algorithm. Now we can get contours of the storm. And that's kind of interesting. We get a fingerprint, a footprint rather, of say a tropical cyclone, which is better than what we had done before, which was just draw a circle around the center. And here's one where an atmospheric river is contoured as is up here, cycles. So that's something we just didn't have before. So, was this project successful? And honestly, not really, because all the machine learning experts were attracted to industry. You're laughing. And you know why? Because we can't afford them. And so we never got to what we really were looking for was to do some unsupervised learning and find things that we don't know in these large data sets. Data sets about four petabytes actually. About four petabytes actually. And I never really got to exploit those masks that I just showed you. And this is, I thought about this a lot. This is really my fault. And my fault was that I let them give me these pictures without learning how to make them myself. And so there's a lesson here is that we really have, as we do these kinds of projects between status. Projects between statisticians and machine learning experts and climate science, we have to have the technology transfer happening at the same time because unexpected things happen, like people get better jobs. But at least one part of this project remained from the label data set. Travis O'Brien, who also left our lab, but he went to academia. He's at Indiana University. He developed something for He developed something for atmospheric rivers based on that labeled data set to track atmospheric rivers. He called it T. Gavard. And here's some of the output from that. This is an interesting, we have this project that I dreamed of called ARCMIP, which is the Atmospheric River Tracking Method Intercomparison Project. There are a lot of different definitions of what an atmospheric river is. We wanted to try to compare to those. We wanted to try to compare to those. Tigabar turns out to be a more conservative algorithm than some of the ones that people dreamed of heuristically. And one of my young colleagues, Yang Zhao, has now used that output. She analyzed the entire ERA5 reanalysis. And this year has been a big year for rain in California. And it's a weird year because it was a decline. A weird year because it was a declining La Niña, not an El Niño. People would have said ahead of time that it would have been yet another drought year. We've had a whole bunch of drought years in a row because it was a very long La Niña. And in January, we had eight storms in three weeks. So this is one of those temporally compound events. And so, you know, we really had a lot of interest in that. And so she went to her database of Protika Bard of these atmospheric rivers. Bard of these atmospheric rivers, and we tried to cluster them and say, Well, how many were in two days, how many were three days, and it didn't work very well. And her husband is a machine learning expert, and he said, Well, why don't you just do a machine learning thing? And she called it an unsupervised machine learning non-parametric centroid-based clustering algorithm, which is a lot of words, some of which I understand alone, but not together. But she gave me a picture of a particular year, 2020, for the For the Abazarine River season on the west coast of North America. And there are various clusters, and some of them are dense, these where they're really close together, and some of them are sparse. There's a category scale for atmospheric rivers. And so there's a couple of things that she's learned: that the strong atmospheric rivers, which are the most damaging ones, tend to come in these dense clusters. Clusters. So, you know, if you have a few in a brief period of time, it's more likely that a big one's going to be in that. Whereas if they're spread out in time, then they tend to be weak. And if they're really alone, we kind of call it like these events lone wolves, they tend to be weak. She also found, and this is really important, that there's actually a weak connection between these clusters, the dense clusters, and El Nino. And El Nino, and a much stronger connection to the Madden-Julian oscillation, the MJO, which is a much higher frequency mode of natural variability. That, I think, goes against the conventional wisdom. So this is something we learned. And this is an example of machine learning on top of machine learning. So we're pretty excited about this. She emailed me again today saying, can we talk? Because she's got new results. That will be even more exciting. So I'm really hot on this one. A lot of you know. A lot of you know about ForecastNet, and this is an interesting effort. Now, Karthik was one of these guys that worked for us who got lured away to NVIDIA, but he's still working on climate. There's a large group there at NVIDIA. And the forecast net is a machine learning training algorithm where you train on these sets of variables that basically winds at various vertical levels and geopotential heights. At geopotential heights, not temperature, and not humidity, and not precipitation, which is interesting. So it's a limited set of the prognostic variables from the ERA5 analysis. This is a slide from NVIDIA's presentation, and this is a citation. It turns out that this thing is a fantastic weather forecasting model. It has, this is a skill. This is a skill plot that's just typically used by forecasters to judge how well they do over lots and lots of forecasts. And it's competitive on short time scales, and it's actually superior at seven to ten day scale than the actual European Center forecasts or the National Weather Service. So that's really kind of interesting that you could do that. They originally had a code that was crashing after 14 days, which we thought was interesting because that's sort of the limits of weather predictability. But they since tweaked it somehow that I don't understand and run it quite a bit longer. And I know they made it run up 240 days. It could have run a little longer. They just stopped it. It's very fast. Five orders of magnitude. It's a much smaller team, in fact, than a Smaller team, in fact, than a forecast model development team would be. And you can see truth versus the actual forecast there for a figure A. And my colleague Bill Collins has an idea that he calls HENS. The LENS stands is something we use to describe the large ensemble. HENS is humongous ensemble. So the idea is to, his idea, and it is his idea, is to idea is to uh is to make um uh tens of thousands of of ensembles of ensemble realizations for an ensemble and look at um what he calls low likelihood high impact events so like big storms or big heat waves or things and and there are many challenges here some of which are computational but some are philosophical and and one of them is this on the the spread of ensemble uncertainty Ensemble uncertainty realistic, or are all of them going to be the same? It would appear from some of the things we've gotten from NVIDIA that the ensemble spread is realistic. I read some more since I made this slide that gives me more confidence in that. But as they say, the proof of the pudding is in the eating, and I really need to see this myself before I'm convinced. But it does appear very. But it does appear very promising. We're not ready to do these experiments yet. We have a big machine at Berkeley Lab that is composed of thousands of NVIDIA chips. So it definitely is tractable to do this. Some things will be interesting, like these event identification. So one of the things we'll want to do is look at hurricane statistics. We won't be able to store 10,000 simulations worth of three hours. Simulations worth of three-hourly data for seven fields on any disk system we have. So we'll have to do that on the fly. That's a huge data reduction algorithm, so that's good. We could end up with gigabytes or terabytes of track data. We could do it on the fly, but we haven't developed that software yet. More challenging, I think, is to do extreme value analysis. I think that's an interesting thing to do here, but we're going to have to store the block maximum. Block maximum, so doing the bird beat, that's going to be a fair amount of storage. But I think it's tractable. Um, if we do annual block maximum, seasonal block maximum, I think, I think we'll so that's that's something in the works. I think it could be very interesting. It could be a game changer. I'd like to pivot away from, I'm not a machine learner, but maybe that should be obvious. I'm not even a statistician, I started a statistics journal problem. Started the statistics journal. But I really like statistics, and I really am a firm believer that we need to have much more of this in the climate sciences. Anything we can do to make that happen is important. Mark Grisser and I have been trying to get the event attribution community more aware of Granger causal inference. Typically, those studies are pearl causal inference statements where you Causal insurance statements where you run a climate model with and without climate change, just like you would give a group of patients the medicine or a placebo. And one of the questions, so this is a data-driven approach rather than a climate model-driven approach. And one of the questions I asked them is, how many covariates for temporal non-stationarity can we get away with? And it's a surprisingly large number. In this first study, I'm going to show you. In this first study, I'm going to show you. We kept the shape and the scale parameter in our GV constant and station. And we made the location dependent on the log of carbon dioxide, because that's the radiated forcing term from humans. ELI is an ENSO index, the Arctic oscillation, the Nina oscillation, Pacific Northwest anomaly, the Anomaly, the Atlantic Meteorontal Oscillation, and volcanic aerosols. So, all these, but carbon dioxide are natural forcing factors, and all of the last one are natural modes of variability that we thought were important for precipitation over the United States and relatively dependent. And so, here's the result from the extremes, and we find that we can detect. And we find that we can detect an influence of ENSO, especially in the winter and the fall of the United States, a positive where this is the difference between the high value and the low value of the covariate, everything else being kept the same in the 10-year return plan. And so you can see that El Nino makes a big difference, and the Pacific Northwest was making things moisture in an El Nino phase, and the Pacific North. And the Pacific Northwest anomaly makes things drier when it's in a high phase. But this is, I think, contrary to what many climate scientists believe, that all these natural modes of variability, not including carbon dioxide, all the natural ones, including the volcanic aerosols, only explain 3 to 5% of the total variance. So it's really not important, even though we can detect the effects. The effects. For the mean precipitation, we get a similar result. The same things are kind of important, but it still only explains about 10% at the most of the total variance. So that this article has been published. I think it's kind of upsetting some folks who believe that El Dino causes everything, it just doesn't. Which it doesn't. So, now getting into the real meat of this is: can we use these Grainger causal inference kind of techniques to detect and attribute human influences? So, Mark cast this into a similar language as Francis talked about on the first day, where we divided up the total Total probability into what he calls force, which is the union part, and then the other low frequency stuff. So, all that stuff I just showed you is in this low frequency driver. That's the internal variability. And then the human part are, we considered greenhouse gases, so log and carbon dioxide is a proxy, and sulfate aerosols. And this works because. And this works because the green wellness greenhouse gases here on the left are always increasing from humans. But over the United States, aerosols decreased after 1980 when the Clean Air Act was passed. And so these two fields are uncorrelated. And in fact, when we looked at that study with all the natural modes of variability, we deliberately chose things that had very low correlations or otherwise things that are kind of. Or otherwise, things are kind of redundant and fall apart. And so, again, this time we can actually detect and attribute changes to both greenhouse gases and salt-based aerosol pollution and precipitation, both in the mean, which is the top panel, and in the 20 return values, the bottom for seasons. The bottom for seasons. And the interesting thing is that the effect of aerosols is different in the winter than in the summer. In the winter, the aerosols suppressed precipitation, mean and extreme precipitation. The reason for that, we think, is that it's cooler. And so this Clausius Clapeyron constraint on available humidity causes things to be less humid. But in the summer, it's the opposite. But in the summer, it's opposite. So it's still, it's still the aerosols would still make things cooler, but aerosols also affect cumulus convection by having different cloud condensation nuclei. And so in the summer, you have large convective storms, especially during stream precipitation. In the winter, you have these large-scale storms that don't have a lot of convection. So the effects are different. And that actually really comes to play in the time to detection. Time to detection, the aerosols delay to time to detection. In fact, we don't detect a human influence in mean precipitation in the winter. It's delayed from, without aerosols, it would have been around 1960-something. It's delayed well into the 1990s in the winter. But in the summer, it's accelerated. And so we can detect before 1960s. And finally, I want to talk about impossible. And finally, I want to talk about impossible events. This is something that Lee Kung is leading. We looked at the Pacific Northwest Heat Wave. This is the data set we use over the U.S. Only branches and like that. And what Likun did is he calculated the upper bound of the upper bound using a Bayesian technique. And you can see that the 2021 record is outside the upper bound of the upper bound. The upper bound of the upper mountain. So you would conclude that this event was impossible without climate change, but it's impossible with climate change. And so, obviously, something is wrong. And so we've done a bunch of things that will be on the next slide, but I wanted to show that I think this speaker would be interested that we could have developed a tail-dependence model that shows that things are very tail-dependent. So when that is built into the So, when that is built into a spatial model, it helps. And so, all the crosses here are events where the station data for each station is outside the upper bound of the upper bound. You can see that if we just do a point-wise analysis, just looking at greenhouse gases, that's essentially what Gehr Jan, a lake, great friend of mine, Georgian von Oldenborg did. Of mine, Gerd Jan von Oldenborg, did an initial rapid attribution study, and you can see that 30% of the stations are impossible. If we put all these covariates in, in this case, we use greenhouse gases, we use a drought, that's PDSI, there's always drought in Washington State and Oregon. We use ENSO, and then there's an urban thing that I'll talk about in a minute. If we do all those, Um, if we do all those, it doesn't help much. Still, a quarter of them are in 2021 are this one are still impossible. Mark had a spatial model that we used initially, and that brought it down. That helped. I mean, that brought it down to only 9% were possible. But with Likun's tail-dependence, Technique, only 1% of them are impossible. Now, before we put the urban one in, Portland was still more or less impossible. And we put urban in it, it helped a little bit, but if you ever watch Portlandia, you know it's a straight. Portlandia, you know, it's a strange place, and Portland stays weird. Yeah. They are buying bagpipes on fire unicycle. That's how weird it is. So just to summarize all these different, my slideshow here, we think pattern recognition works pretty well. There's some things to be learned. You know, it is a lot faster, if any of that, than the heuristic algorithms. This forecast net has a lot of potential to be a game changer. I mean, we really can do 10,000 simulations that actually, you know, are replicating what we can do with the climate model. That really changes things. There are a lot of questions that I've learned about this week that concern me. You know, can, if we're really interested in these low-likelihood, high-impact events, will we? Impact events, will we get any in this 10,000-member ensemble that aren't already in the training data set? And because the variables that are trained are not the ones that we're looking at, they're trained on these prognostic variables, wind speeds, and geopotential height. When we look at temperature and precipitation, does that mean that it's going to be the same extreme events, just more of them in the big ensemble or not? In the big ensemble or not? I don't think we know the answer to that. I don't know the answer to that. I think there's a lot of nonlinear questions here. I don't understand. Granger causal inference and these lots of covariates, I think is really an interesting thing to do for event attribution. I'm not saying that we should do this instead of the traditional pearl causal inference studies we do, but rather in addition. And when we get In addition. And when we get the same answers both ways, our confidence is increased. And this is the kind of activity that the public is very interested in. If we are confident, then we can convey that confidence when the news people show up, which they all do. And finally, the upper bound of the upper bound is really interesting, and hopefully we'll have some papers on that soon. Oh, and the ad. A few years ago, I started this journal, Advances in Statistical Climatology, Meteorology, and Oceanography called ASQL. And I was able to convince Francis to be an executive editor with myself. And this audience are my people here. I really want you to seriously consider submitting articles to this journal. We do about 25 articles a year, but we're interested. But we're interested in methods, and so results are interesting to us. But if it's only results, we reject it out of scope. What I want are our methods. And this is supposed to be a bridge journal between the climate science community and statisticians. So please consider that. And thank you. 