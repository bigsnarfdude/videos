Actually, before starting, I would like to come back to the question that was asked during lunch time regarding the evolution strategies and the fact that nobody wants to work in it. Actually, I was one of those that were forced to work with evolution strategies because at that time Luis and Serge, Luis Vicente and Serge Graton, they got a grant from Total Energy. A grant from Total Energy and Total Energy, they were using actually a highly parallel evolution strategy to help solving actually a seismic imaging problem to find a starting point. And they really believe that the algorithm is working well and they wanted to see if there may be some improvement on this algorithm. They don't want to use it. So by a discussion with the So by a discussion with Luis, Luis was not in favor of working with the evolution strategy, but we convinced Serge and the other guy from Total convinced him and we come up actually with a modified evolution strategy where we include some sufficient decrease condition to make the method actually enjoy similar properties as directional direct search methods. Research methods. The method, well, was, I mean, for evolution, for people that work in one evolution study, for them, this is not an evolution strategy at all, because they believe that the strategy should only be rank-based. And the sufficient decrease condition is not, it actually changed the spirit of the method. And also for the food, people. Defoo, people probably think that it's not a good DEFU strategy, so it's in between. I have no answer. Okay, so coming back to the subject today. So here it's another topic. So I'm jumping from evolution strategy to Bayesian optimization. I don't know what will be the next, but this is Bayesian optimization. It's one of the subjects that start more recently to receive a really great. More recently, to receive really great attention from practitioners, especially related to machine learning. It seems that it handles noise, noisy black box optimization quite well, in particular related to hyperparameter tuning problem. So, and for that reason, by discussing with Victor Pichne, Rodolphe Lorich, we said, okay, we can do something because they had some issues. Had some issues, and from my dayful background, I had some suggestions, and we should test them, and it works. So it is working. Okay, so the context is like everyone. So black box, 3D, multiple inputs. So the black box structure is unknown. I think everybody here is familiar with. Possibly subject to a set of Possibly subject to a set of constraints. Here in this talk, mainly I will focus on the bound constraints, because including general constraints in the context of AIDS and optimization is horrible. So I'm explaining things in the simplest way. And the goal is we would like to minimize the function, hopefully find the global minimum, if we can, while actually minimizing the number of function events. Minimizing the number of function evaluations. So, we would like to reach the solution the fastest possible. So, to solve this black box optimization problem, so I'm following the suggestion of Charles here. We use derivative-free optimization methods, and those methods can be classified actually to deterministic ones. Actually, to deterministic ones, like model-based methods, the trust region methods, direct search, and others. Another class, the second class is more like stochastic, heuristic-based, I would say, DFO methods. And there you will find really a bunch of nature-inspired algorithms. So, like particle swarm, simulated and even evolution strategies. And there we can. Strategies and there we can actually put Bayesian optimization since true it's a model-based but the models are stochastic so it goes in this class. So from now the rest of my talk I will try to concentrate more on Bayesian optimization and I'm assuming that as audience you are not very familiar, not all of you very familiar with Bayesian optimization. So I will really start from the basics. Start from the basics, probably also. And then I will go to test some, actually, to do some sensitivity analysis with respect to some parameters. And there probably it will be related to your question about how to initialize. So, one of my slides is actually trying to answer to give an answer. So, we'll see that. And once we all agree on. Since we all agree on the drawbacks, the advantages of Bayesian optimization, I will try to introduce you simple, I would say, improvement to the Bayesian framework using actually, for instance, like there is related to the previous talk, using the pool search mechanism. Okay, so by easy and optimization, it seems now, as I said, as a very powerful strategy to solve. Very powerful strategy to solve actually black box optimization problems. Most of the time, it leads to finding the global minimum for the black box. And what is really interesting, it leads to a solution at really a reasonable cost if your cost is estimated using the number of function evaluations. Okay. So this method is, it can use really very It can use really very few function evaluations, but if you see the superior time of your, I mean, the running time of your algorithm, it may take hours. Okay, so a disclaimer, if your object, if your objective, if your black box optimization problem is not expensive, look for another algorithm. Okay, so here is a sketch of a much more general framework, a simulation-based. Framework, a simulation-based actually optimization solver. So the idea is that given an initial set of data that we call the design of experiments, DOE, we try to build a model. Here it's a probabilistic model for the objective function. And then based on this model, we construct what we call an acquisition function. And this acquisition function And this acquisition function will help us actually to suggest new points that will enrich actually the model, our knowledge on the model, but also improve actually our knowledge on the minimum of the function. So if we want to see what matters in the algorithm, there are two ingredients. First one is how to better First one is how to build this probabilistic model. And second one is how to choose efficiently the acquisition tunction. And the word Bayesian optimization is coming actually from the way you construct your probabilistic model. Okay? When you use actually a model that is based on the Bayes rule, the model, the optimization method actually. actually, it's from the fact that the model is built using the Bayes rule that we call the optimization method, a Bayesian optimization method. Okay? And the idea there is like we try to express actually the posterior distribution using the Bayesian rule. It means that our knowledge on the function given actually the design of experiments can be actually expressed as Can be actually expressed as if you use the Bayesian rule as the likelihood times your a priori on the function. Okay, and then so to define to choose a model, we will try to work on having this to happen most probably. So we will try to maximize this product of probability. And there we need some assumptions. Need some assumptions. If you look for a general function, I'm not sure if it's easy. And one assumption that is very frequently used is to assume that the objective function itself is a Gaussian process. If you assume that the function is a Gaussian process, it means that it can be written as a Gaussian process of mean, I would say, mu f and standard deviation sigma f. Then, based on this rule, we can build explicitly a model, a probabilistic model that is also a Gaussian of mean mu and variance sigma. Okay, the choice of mu here is free. In most applications, they choose a constant. This is what they call the trend. So, the trend is most So, the trend is most in most applications is constant. You can take a polynomial, LBF, whatever you want. So, as far as it is linear, it depends the composition in a base, it goes in. And once you get this, so this is your a priori on F, you can build the mean and the standard deviation. mean and the standard deviation using actually by solving by computing an estimator that we call the best linear unbased estimator the blue and this is actually we we are able to build it because we assume more or less that we depends linearly on the observations as well as on our a priori As well as on our a priori form on the section. So you have a linear dependence in the input, in the data, as well as in the a priori of the function. Okay, so here is an example of a Gaussian process. So we try to give in a set of observations. So you will get a mean and So you will get a mean, and with this mean, you have the standard deviation. And if you so it is actually a random function, so we don't have this, but you have each time you evaluate it, actually you get one realization of this. But what it is sure is that your realization, they are interpolating on the points that you have. Okay, so this is the model that typically we use. So now we know how to construct. We use. So now we know how to construct a model in the Bayesian context. And this is what makes the method actually called the Bayesian optimization method. Second step now, how to take benefit from this model by actually constructing an acquisition function. So for the acquisition function, I listed here a few, but there are lots. And if you include constraint, there are many. Are many. So the list is very, very big. But one acquisition function that is very commonly used is the expected improvement. Okay. And this expected improvement, what is nice is that somehow actually it codes the tried to predict the trade-off between actually minimizing the function and exploring new areas by reducing the variance, actually. Okay, so the Okay, so the expression of the expected improvement criteria acquisition function is actually given by two terms. So we have this term that so it's this is the best point in my design of experiments. This is the opposite of my estimated mean on the function. So if I'm willing to minimize function, I need to maximize this quantity. Maximize this quantity, and on the same time, if I want to explore more, I need to actually maximize find the point that maximizes the variance. And so the coefficients are given here. The trade-off is given actually by the CDF and PDF functions. Okay, so if I want to plot this acquisition function, the expected improvement function. Expected improvement function. Assuming here we start actually, we know here three points. We have a model. If I build the acquisition function of this associated to this model, using actually the mean point and standard deviation, I'll get this red function. Okay. And what we do, we try to maximize to find the new point that to enrich our. So here, for instance, So, here, for instance, the maximum function is here. So, the algorithm we suggest to add a point here to improve our knowledge on the function. One thing that you will notice actually is to maximize this acquisition function, it's a really hard problem. Why? Because there is a lot of plateau, and so if you are here, you need really Really, you need a multi-starting strategy at least to be sure that you are able to explore space. If you just use a trivial local solver, you may not get the maximum. So, probably if you are lucky, you can get a point here or here, but you have no guarantee to get the maximum point, which is the most interesting one because it gives actually the best compromise. Okay. Another point is like once this acquisition function, once it is given here, it is explicit, it's given explicitly, you have access to the gradient, to the Hessian, so you have access to all the information. So you can use any solver that you want. It is something that given analytic. Yes. Normally, it is if you do Normally, it is if you do, I think it is because you get the derivatives, probably the true definition of UE, you have actually zero if, yeah, it's a max, but probably, but I'm speaking on this part, it's differentiable. You can this you derive it. That's what I'm trying to think, but it yeah, good question. But from what I see, this expression, so when the function is equal to zero, because I think the point where the acquisition function is not derivable are already not interested. Already not interesting for the optimization. They are the minimum. So if you, yeah, you need another algorithm that adapts, but it seems that for a turbulent for a Gaussian concept, sigma going to zero as turbulent. I think yeah. But the derivative of a Gaussian concept is a Gaussian concept. Yeah. And so that might be. It's the fact that sigma goes to zero that creates this issue. So if you do the optimization naturally, you will actually get your algorithm will not give value I think. But from what I see in the optimization community, you have the gradients. I didn't check the details, but I didn't check in details, but I think the explanation that Stefan gave is a right one. But good question. Okay, so Bayesian optimization method, when it uses actually this acquisition function, the expected improvement, it's more commonly called actually as EGO. So it's efficient global optimization. So EGO, sometimes people like confuse EGO and BEO. So I would say BEO, it's much more general and EGO it's BO, it's much more general, and EGO, it's when you use an expected improvement. So, if I run the algorithm, actually, here it's the first iteration. So, you have this model, and you try to maximize. So, it suggests to add a point here. Once you add the point here, so the whole, the model is global, so the whole model change. You update, you construct a new acquisition function, and again, it suggests to add a point here, and step by step, you You converge actually by you minimize actually the variance, and here you see the more you converge, the more plateaus you get. And so, to get this point, the optimization of the N-field criteria is getting very, very hard. Okay, so the question now is like, is this type of algorithm mature enough for the optimization community? Well, through that there is a lot of Through that, there is lots of open source implementations that code Bayesian optimization solvers, but I think still many open questions there. In particular, actually, there are lots of parameters to choose, to tune, or to understand. Okay, and recently in this article, Rodolf and Victor, they tried to study actually the Victor, they try to study actually that it's an extensive sensitivity analysis with respect to the initial DO size, the design of experiments, the trend, the mean function. I was saying that in general, people use a constant, but what happens if you use a linear or quadratic model, RBF? There is also to define the standard deviation, it depends on a kernel function choice, which also. Which also, in which there are some hyperparameters to define. So, how to choose this kernel, how to optimize these hyperparameters, this. The acquisition function optimization is also an important thing. So, as I said, the acquisition function presents lots of plateau. So, you need a good strategy on optimization. Most people, when they start doing a Bayesian optimization, Start doing Bayesian optimization to use local servers without multi-starting, you will see that it's really giving a bad result. So, if you want to use Bayesian optimization, you need to follow the good practices, otherwise you may not get optimal performance. Okay, so and another thing, so I will try to introduce here Another way to do tests. So, more of the presentation, more lots of presentation where using actually data performance profile to present the result. Here, we choose to actually use the COCU benchmark. At the beginning, I was not really motivated to use it, but you will see why, because the figures are really very ugly, so you cannot, it's not easy to read. But after discussion, Leave, but after discussion with Rodolphe Victor, they are convinced that it presents actually a potential for benchmarking. So here is the main idea. Actually, the benchmarks contains end constraint, bound constraint, and now constrained optimization problem. They have optimization problem that are, you can change, you can vary the dimension. you can vary the dimension the optimization the optimal position of the the position of the optimization solution you can do many things so it can you can create a lot of instant instances it contains mainly 24 for the unconstrained path 24 functions you can divide it in five groups but if you created all the instance if you created all the instances you can go to 360 problem instance i would say okay so with functions are of this structure and in all my tests i will try to since the solvers are so the bayesian optimization are stochastic so we will use 15 runs with this maximum function evaluation so 30 times function evaluation. So 30 times the dimension. And here the dimension can be two, three, five or ten. I will explain why I'm stopping in 10 later. And for the experiment, the default, when I will talk about ego, the default version, it will be a Bayesian optimization where the DO size is of actually this size, 7.5 times the dimension, and where Times the dimension, and where for the optimization of the acquisition function, we are using a multi-start BFGS. Okay. So here, actually, first thing to do is how to choose the initial DOS size. Okay, so what we did here is, okay, let's try three different DOE. Okay, let's try three different tried three different DOE so the second one the first one is of size two times d second one is larger and then the the third one is of 20 times the dimension and what we notice here is like working with actually a small medium a small medium or large DOE for small dimensions For small dimensions, it doesn't make change. But when you increase actually the dimension of the problem, it's better to work with a small initial design of experiments. When I said small, that doesn't mean to have a design of experiments equal to one. You need a minimum of information. And generally, from experience, you need at least like d plus one point. 2d it's Point. 2D, it's considered as a small. And here you can see if, okay. In most recommendations, actually in Bayou, when they give the budget, they divide it by three, they use in general 30% just to define the initial experience. And here we in the COCO actually test phase, we find out that it's possible to reduce this budget and you can get better results when the dimension increased. When the dimension increased, the same for the maximization of the axis detection. Here we used actually a different strategy. We said, okay, the M is for multi-start BFGS. Here, EI, it's for the expected improvement using a local solver, so single BFGS, and plus the random search. So there is no optimization. And what you can see actually, single BFJS is really behavior. Single DFJS is really behaving poorly, especially if you increase the dimension. So you do better to use actually a multi-starting strategy to maximize your acquisition effect. Okay, so here we go for the trust region methods. So for trust region methods, actually, it starts to be very used in the Bayesian optimization for different reasons. And one of these reasons is actually. Different reason, and one of these reasons is actually they would like to scale Bayesian optimization to make it work in a higher dimensional problem and for a higher dimensional problem. And for that, by using a trust region, by restricting the search space to a zone, and so probably eliminating the points that are outside of the trust region and doing local Bayesian optimization inside the trust region. Optimization inside the trash region. In all the numerics, it shows that improves the results of the Bayesian optimization solve. Okay. And here in this work, we following actually the using the UFO techniques where, for instance, the pull search mechanism. So John was a co-author of this nice paper, paper that suggests the pull and search step. We said, okay, let's Step, we said, Okay, let's try a Bayesian optimization where this time we use a Gaussian Bayesian optimization solver in the search step, but also in the pull step. So you have one global and another local. And to couple the two, we use actually a sufficient decrease condition and we have control on step size, which will also be the the trust region radius. The trust region radius, and to get convergence, we assume at that limit a point we have actually the density of the point. So, this is the framework of Trigo. So, what we do, so starting from a point, you do a global phase over all the omega, so you maximize the acquisition function. If it satisfies the sufficient decrease. The sufficient decrease, it's okay. So you accepted it, you and you go again to the global phase. If not, you go to the local phase and there you maximize probably you can use the same acquisition function, but you can use another one, but this time restricted to this thrust region. Okay. So which is there is two tolerances, epsilon mean, which is a small tolerance, epsilon max, which is a huge tolerance. Which is a huge tolerance to define the trust region. And based on this success condition, a sufficient decrease condition on the objective function, you declare success or fail. So you see that the scheme in general, it follows the pull search step mechanism. So the convergence can be done by inspecting the Clark directional derivative at the limit point. It converges by considering that F is bound. Considering that F is bound below Lipschitz, locally Lipschitz continuous, and by considering that the sequence of local iterates is dense around the limit point. There is a way to generalize Trigoo. I didn't mention this, but you can consider actually many variants. For instance, you can go to local phase and you do lots of local steps. You do the same for the global. The same for the global, and actually, in practice, it seems that when you go to the local, you do better to stay a bit. Empirically, we find that, for instance, at least if you go to four, that's really good. So, that means if you construct a local Gaussian process, you need to really take benefits from it. Okay, so here are the results. So, we have a We have selected some solvers here. There is a lot we couldn't put them because otherwise it will not be readable. So, as I said, here, what we have is like, we can interpret it as a data profile, like a fraction of instance solved. Okay, so the bet, the higher is the better. And what you can see here is three go. So, this is the ranking at Uh, ranking at the end of the budget. So the ranking, the best zero nine is an envelope of the best algorithm. So it's like a your topic algorithm that doesn't exist, but till 2009, it was actually constructed using the best algorithm. So we are doing well as far as we get close. So in three-dimensional here, you cannot distinguish, but seems that three go is well ranked. That trigo is well ranked. As far as you get increased the dimension, MCS, I think multi-level coordinate search starts to behave in better. EGO, what's interesting here is like EGO and Trigo. EGO is the basic Bayesian optimization solver, and Trigo is the trust region version of EGO. So improvement to have ensured all time over the Bayesian optimization solver. Easy and optimization solver, and we are very competitive. Here in the experience, we include NOMAD since it uses also this mechanism of pull cells. It performs NOMAD perform well, so with a slight advantage in between for a trigo. While you increase actually the dimension, at the end, the NOMAD is performing well, but if you stop till here, I think the Bayesian optimization has some. By using optimization, it has some advantage. The same problem if I actually look by class of functions. So we can have access to the multi-model, multi-model problems with weak structure, uni-model. And there you can see that basically for multimodal problems, 3GO, which is this trust region version, is very effective. The other solvers they The uh yeah, for multi-model, Nomad and Trigo have a really uh similar performance with a slight advantage for small budget. One thing that you can notice here is like the starting point, it's like you have two class of methods. Those are for the methods that they need one starting point, and the other actually for the methods that they need a design of experiments. So, that's basically it's So that's basically it's you need to wait till here to get the design of experiments and the optimization is starting for me. So it's really sometimes it's if you have one point and you have a strategy to use it, it can be efficient, especially at the early ages, at the early stages. Okay, I think.