Yes, you can see Avita's title, so go for it, Avita. Thank you so much for the invitation. It's a pleasure to talk here, and hopefully, we will get to meet in person soon at Banff. So I will get to talk about the interchange process since Ron skipped it. And so the first half of this talk will be about card shuffling. So the interchange process As I said, it's a card shuffle. Okay, making sure you can see what I'm writing. So we start with a finite connected graph which has n nodes. And we're going to assign distinct cards at the nodes of G. And of course, there are many ways to do this, and in fact, the configuration space is the symmetric group on n elements. Okay, so I'm going to define a Markov chain, and first I'll be doing And first, I'll be doing it on discrete time. Later on, I'm going to switch things to continuous time. So, we will at time t we will pick an edge uniformly at random and say we're going to flip a fur coin so we We're going to flip for a coin, so with probability one-half, we are going to flip the cards at the ends of this edge. And with probability one-half, we will do nothing. Okay. And the general question. And the general question that we ask in card shuffling is: how long do we need to keep on doing this process until the deck is shuffled well? Of course, I'm going to make everything rigorous in a moment or so. Before making things rigorous, I want to Making things rigorous, I want to say that lately we care about sharp answers in general. And here, even providing an answer in such a big generality is difficult. So we do look into specific examples. So we look into specific graphs. And so, for example, when we look into the complete graph, say this was the breakthrough result. This was the breakthrough result of Diacones and Sotohani in the 80s, where they used representation theory in order to answer this type of question, representation theory of the symmetric group. They had a different laziness, so the answer that they got will be a little bit different than the answer here. The answer here, a constant, a difference in the constant. But the techniques would be the same. And then, for example, if we want to discuss the star graph, again, we can get sharp answers. And this is again due to spectral techniques since Flato, Utlisko, and Wales diagonalize this chain again in the 80s. And soon, very soon, I'll go into the path. I'm going to write this into the next slide. But I just want to say that we don't know sharp answers for many more other graphs. We can produce lower bounds. So in particular, if you look at the georegular tree, the finite georegular tree, by studying, by diagonalizing By diagonalizing the simple random walk on the tree in joint work with Juan Guyan, we can produce a sharp lower bound, just a lower bound. And which in discrete time is this one, and we just put on the archive earlier this year. Earlier this year. So yes? Was there a question? Oh, maybe not. So there are lower bounds for other types of graphs, for example, a lattice or so. But really, so here, the reason why I mentioned this is because this is going to be my first open question for today. Going to be my first open question for today. I would be very interested to see an upper bound here for how long we need to repeat this process. And we conjecture that the order should be the same. Now, I want to go to the most recent breakthrough, however, in the interchange process, which was a result by Hubert Lacron. And that was the path in 2016. And La Poin proved that it would take 1 over 2 pi squared n cube log n steps. He provided the upper bound for the path. Well, the lower bound was due to Wilson, which was proven earlier in 2005, I think. Yeah. So I'm about to give you the rigorous setup, but before doing that, I want to say that the interchange process is not just about studying permutations and understanding them. Understanding them. It turned out that there is a big connection between the interchange process and the studying genes in biology. That is work you do direct. And therefore, getting sharp answers, even for the complete graph in particular, was very important into studying genes. So now I want to make everything rigorous. And I'm going to be talking a little bit about La Coin's main idea. Bit about Lacroix's main ideas. So the mathematical setup. So again, we said that the configuration space is the symmetric group. So if we take two permutations, we can build up a matrix PXY, which is the probability of moving from X to Y. From X to Y after one step of the shuffle. Okay, and we see that this is a matrix because as we change X, this gives rows, and as we change, as Y varies, we get columns, and so this is going to be called the transition matrix. Now, if we take powers of the transition matrix, the xy entry will give again the program. Y entry will give again the probability of moving from x to y, but after t steps this time. And just a notation, I will put the x down here. Now, classic theory says that if we let go to infinity, this will converge to the uniform measure. Okay, for every x and every y. And we want to study this convergence with respect to a low variation distance. That's the usual norm that we consider. So if u of y is 1 over n factorial, that's the uniform measure. Then we're interested in the total variation. The total variation distance, which is just one half of the L1 nu. Okay, and in fact, we will take worst case mixing. So we are, whenever I say total variation distance, really I mean that we maximize this quantity over all starting points. And so the question that we posed earlier, which was how long until we are very well shuffled, turns into a mixing time question, which says, if you give me an epsilon, I will tell you how small t needs to be so that dt, the total variation distance, is less than epsilon. And when I said that we are interested in sharp answers, and in particular for the path, I'm going to draw a picture. There is this phase transition that occurs sometimes, and we are interested in learning more about it. So, here we were going to write down the time, and here the total variation distance. And let's say that this is the graph. And let's say that this is the graph that I'm going to draw for interchange on the path. So the distance starts being very big, it's equal to one. And it stays almost equal to one, but then very suddenly there is a drop. And this drop, according to LaCoan, occurred at Nilsen, occurred at n cube log n, with the constant being. constant being one over two pi squared. Okay and we notice that so this is called the cat off phenomenon and it describes a phase transition and we notice that if we go a little bit behind 1 over 2 pi squared and q log n, we're almost equal to 1. And if we go a little bit further, then we're almost zero. Okay so that's what I meant by sharp answer. That's what I meant by sharp answers. So, in the case of the complete graph, star graph, and path, we have a full answer. As I said, in the case of the complete graph and star graph, and all lower bounds that we know for other graphs are due to spectral techniques, but Lacroix's technique was a purely probabilistic one. And I want to discuss a few main keys because Because they're going to be important later on, too. So, the property that Lacroix used is something that Wilson actually noticed. So, there is a nice partial order on a sand that behaves nicely in terms of this interchange process, in terms of this card shuffle. So, if you take a permutation and And you define the following statistic. So if you go, let's we'll be looking at the following statistics. So this is the path. If we look all the way up to position X, we keep track of how many cards up to position X are less than Y. Than y. This is the statistic that we want to care about, and we're going to name this x as sigma xy. So the partial order is that sigma is bigger than pi, another permutation on Sn if and only if sigma x y, let's put an equal here too, is bigger. Is bigger or equal than pi x, y for all x and y. And we can see that this is a partial order. Not all permutations are comparable according to this. But one thing that we notice is that the identity is the maximal element, if I defined, if I didn't mess up the inequalities, the order of the inequalities, and the reversal is the minimal element. Reversal is the minimal parent. And then reversal is the minimal element. Okay, so this is the first thing that is of importance. The second thing is that we have a nice coupling here, which is called a monotron coupling. So, if we have two copies of the mark of chain, copies of the interchange process, then we can couple them the following way. So, we will be picking the same edge at every time. Edge at every time, so we pick the same edge, and then we flip a fur coin. If heads, then we will be doing the maximizing move. I'll explain in a moment. If tails, we will be doing the minimizing move. Doing the minimizing move. So, indeed, this is a partial order, but two adjacent permutations on the Markov chain graph will be comparable. And so there are two things that you can do once you pick an edge. Either you stay fixed or you flip the cards at the end of the edge. One of them will be giving the biggest outcome and the other one will be giving the smallest. Outcome and the other one will be giving the smallest outcome. That's what I mean, maximum. It could be that staying fixed is the maximizing move for XT, while flipping the cards is the maximizing move for YT. So it doesn't mean that we actually do the same move. It's just doing max for both of them or min for both of them. And the nice property here with this coupling is that if Xt is less than X, Is less than yt according to this partial order, then after one step of the process and doing things in the coupled way, the order is preserved. So this was really important because then the third thing that, which is the main thing actually that Lacuan used, is that now he could use Confused is that now he could use FKG inequalities and sensoring techniques to actually get sharp answers. Of course, this doesn't say much. He, in fact, used a lot of information from the heat equation and a few spectral things to get sharp answers. But really, I just wanted to emphasize that quantitative was. That monotonicity was the key property here. And just to say this as a general fact, when we coupled the two chains, if we define the stopping time to be the first time that X is equal to Yt, then this is a standard tool in my chain. Tool in market chain mixing, the total variation distance is bounded above by the tails of this time. And this is called a couple time. By the way, this capital T. Okay, so Laquan had to consider different ideas. So it wasn't just doing monotone coupling, he was doing more. But one thing that we can notice. We can notice now is that assume that xt started at the identity, which is the maximum element, and yt started at the reversal. So I'm drawing a picture here. So if x0 is the identity and y0 is the reversal, then if at some point these two match, then any two other Markov chains starting Other Markov chains starting at something in between will be sandwiched and they will also match. So, somehow studying the chains starting at the maximal element and the smallest element could give a lot of information. So, not only this is these ideas were all mainly probabilistic, but they could be used in a more general setting, in a more specialized setting, I should say. I think I should say. So, this is a first theorem due to that I proven with Danny Naum here at Princeton that says that what if we looked at the interchange process and the path, but we turned everything into a time inhomogeneous setting. So, at time t, we pick the edge t. The edge t mod n minus one. In particular, you know, at time one, we are going to be look picking edge one, time two, we're going to be looking at edge two, edge three, edge four, and so on. And then we start again cyclically. Then again, with probability one half, we do nothing when looking at this specific edge. Looking at this specific edge. And with probability one-half, we flip the cards at the ends of the edge. So, this is a time homogeneous card shuffle, which we call cyclic adjacent transpositions. Positions and we proof cutoff for it. So the mixing time, well, the cutoff occurred at 1 over 2 pi squared and cube log n. And the main ideas that I discussed with you still work, but now we don't have a heat equation that we can use. So we had to use a second coupling and we had to use ideas. We had to use ideas from reflected Brownian motion. Also, for the lower bound, we couldn't produce necessarily a strict eigenvector and eigenvalue and use Wilson's technique for the people who know what I'm talking about. So we had to modify things here, but at least monotonicity went through. And this wasn't the case when, so time in homogeneous card shuffles are a thing. So there So there was work by Maurice, Mosul, Perez, and Sinclair, first of all, when studying the semi-random transpositions. And there, since the spectral techniques cannot really pass into the time in homogeneous setting, we don't have cutoff proven yet. And we also have work by Perez, Maurice Perez, and Ning for A ning for semi-random card cyclic to random. And again, there is no cutoff there because random to random also used spectral techniques in order to proven. So somehow spectral techniques do not pass into time in homogeneous settings while monotonicity passes. One thing, however, that this technique cannot do, and this is my next open question, is that we cannot We cannot, we don't know cutoff on the cycle. So, what about cutoff for the interchange process on the cycle? That we don't know. If the proof had been spectral, if we had been able to diagonalize this chain, which is an extremely difficult task, then we would get it for free for the cycle. For free for the cycle, but monotonicity breaks on the cycle, so we don't have it. So, I want to move on and I want to go into exclusion process a little bit, which is a projection of the interchange process. So, as I said, we would like to have general results for the interchange process. Results for the interchange process. And other than the proof of ALDIS conjecture, I'm not sure I know many big general results for the interchange process. But for the exclusion process, we have some more general results. So again, the setting is that we have a finite and connected graph. Again, we have n nodes and um And we have k particles in total living on the nodes of the graph, and each node has at most one particle. Okay, now the dynamics are the same. So we pick an edge. We take an edge and with probability at one half, uniformly at random. And with probability one half, we do nothing. With probability one half, we flip whatever we see. So in particular, if both ends of the edge are occupied, then nothing happens. If both ends are empty, nothing happens, but otherwise a particle moves. But otherwise, a particle moves from one position to the other. And maybe I prefer to discuss things on continuous time. Oh, and before I do, I want you to notice that basically we identified K cards. So it's like taking the interchange process and saying, okay, K of the cards now are red and the rest are white. And that's how you get the exclusion process. If we, so many ideas from So, many ideas from the interchange process pass along to the exclusion process, but somehow we lose some of the information. It's a smaller space, we don't have SN anymore. So sometimes it's easier to study the exclusion process. So on continuous time, I want to say that we can think as if each edge has a Poisson one clock. And when the clock rings, we do what I discussed that, you know, with probability one-half. Well, we don't have to do this necessarily. We flip. We flip the edges. Okay, of course, we can make this a poor son one half cock, but it isn't important right now. Isn't important right now. And here there are more results of general nature, for example, due to Oliviera or Hermon Pimar and so on. So that's good. I want, however, to move along to the exclusion process with open boundaries. So this is a very traditional model that we discussed now coming from physics. We're going to move to another very traditional model, again, coming from physics. Model again coming from physics when studying behavior of particle movement, excusion process with open boundaries. And this says that now we have the segment, we have the path again, and we don't need to necessarily move to the right and left probability one half anymore. Let's say we move probability p and minus. removed probability p and minus 1 minus p. So actually let me put a different color here. But what is different is that particles can jump in the path and jump out of the path at the ends of the path. Okay, so this is the picture here and there are many popular questions. There are many popular questions concerning this model, one of them being to understand the stationary measure. So, there is a lot of work by Cortiel and Williams and the rook trying to give a characterization for the stationary measure, and they use a lot of young tableau structures in order to describe the stationary measures. So, one thing that, so yeah, I should say here that everything that I'll discuss. I should say here that everything that I'll discuss from now on is joint work with Ganter and Tom Schmidt. Let me write this correctly. So we thought that a practical way to think of the stationary measure is instead to take this Markov chain, run it long enough, and then this. This you then say, okay, if you don't know what the stationary measure is, then you can use this Markov chain to sample from the stationary, meaning that you run it many, many times, and then the outcome that you get is something that is close to the stationary measure. And this is a mixing time type question. How long do you need to run this Markov chain until you get something that is close enough to the stationary? And so, our first theorem, which is Which is not necessarily the most interesting one is for the case where inside the segment we move symmetrically, so one half to the right, one half to the left, then the mixing time is of order, now it is continuous time, n square log n. And the constancy depends on the parameters of the boundary. Of the boundary. And the second part, which is very relevant to LaCoin's proof, actually, is that if, again, inside the segment we move symmetrically, but nothing enters or exits from the left, so we only allow jumping in and out from the right, then we actually get cut off. Okay, so we'll see in a second that the proof here was very similar to Lacroix's ideas, and that's why we don't consider it our main contribution. But it's always nice to get sharp answers. So let's take a very quick look to how these two proofs go. Go. So if we just have one half and we can jump in from I jump in and out from either the left or the right, then what we do is again a coupling. So we have two processes, two different processes. And so it could be that this one is this one while the other one is this one. While the other one is this one, and we define the difference process. This will be relevant later to more interesting results, but it's easier to define it now. So the difference process DT will be again on the segment. And whenever we differ somewhere, so here for example, we have different occurrences, we're going to Occurrences, we're going to place a red particle. Whenever we are both empty, then the difference is also empty. And whenever we are occupied by, but whenever both xt and yt are occupied by a particle, we are just occupied by a particle. In particular, let's add one over here. Okay, so this is the difference process of Xt and Yt. And YT. And we notice that when we have no red particles in the difference process, this means that X D and Y D have coupled. Okay, so we call the red particles first of all second class particles and the coupling time. And the coupling time is the first time that there are no second class particles on DT. Now, just because we are on the symmetric regime here, here by Here, by the way, we do the canonical coupling, meaning that we try to do identical moves for Xt and Yt, and we see the effect on DT. So now when we do this and we try to study the behavior of second class particles, that's very easy because really they behave as if they were particles. So really they move symmetrically inside the segment and we just wait for them to exit. We just wait for them to exit at the ends by either adding a new particle when the red particle is at the end or by making the particle exit. So it's very easy. It's just standard results. But later, this will be important when we have the asymmetric behavior inside the interval. For the second one, where we actually have cutoff, the main idea, and I won't say much more, is to mirror image our process. So if this is XT, then we are going to mirror image it as follows. So I put five positions here. So if here I have an empty position, then An empty position, then the mirror image will be occupied again. Empty, empty, so we get two more particles here. And then these are both occupied, which means that in the mirror image, we get nothing. And notice that, oh, by the way, in this case, nothing enters or exits from the left, so that's why we don't care about what happens on the left. This was the case where we can. We can only exit this way. And so notice that if something enters, then this becomes occupied and this becomes empty. So somehow there is an interaction between the two mirror images as we run the dynamics. And this idea made it clear that this is very similar to study. Very similar to studying the exclusion process on the path that has length 2n with 2n nodes. And the ideas from Lacroix techniques from the interchange and exclusion process on the path went along here very nicely. The only thing Along here, very nicely. The only thing that we really had to do is adjust the heat equation so that it takes beta and delta into account. Okay, so as I said, this isn't our main contribution, but it was nice to see this technique work again. Let's go into a theorem that is more significant. And as I said, everything here is joint work with Nina Ganter. Here is joint work with Nina Gankler and Dominic Twidd. So, the second theorem is in the three-sided case. So, if we have that we move isometrically inside, and by the way, I will assume that P is bigger than one half, but let's say that these are the only parameters that are allowed to be positive. Allowed to be positive, so alpha is zero, or so this is a three-sided case, or um or a beta isura. Okay, um, then uh, we proved that the mixing time is linear. That the mix in time is linear. And okay, in the upper bound, the constant depends on the parameters. And so, just to say a few words about how this works, well, let's look at the first. Well, let's look at the first case, and I think this should be intuitive enough. So, in the case where alpha is zero, well, we notice that in terms of the stationary measure, most particles will be at the end of the interval. So, you know, just producing a lower bound is easy because we will just look at the set where we have that. We have that there is an x in the first n minus square root of n positions such that let's say x, oh, x is not a good letter, let's say, if this is y, y at position x is occupied, then in terms of the stationary measure, at least in terms of taking a limit in terms of n. This will have zero mass, and this is something that you know we can get from the works of Cortill and Williams in particular. And then, you know, if we just choose a bad starting configuration that has, say, a one at position root n, you just have to wait for a linear time. Just have to wait for a linear time for this particle to exit. That's just how the lower bound works. It's not so difficult, but this intuition already says a little bit that we expect to see things towards the right of the interval in this case. For the lower bound, I want to say that it's much more interesting. For the upper bound, I'm sorry, it's much more interesting because we noticed the Because we notice the occurrence of shocks. And I'll say a few things here. I just want to say that the phenomena that I'll describe now, like shocks or occurrence, these are things that have been noticed by Ferrari and his collaborators. And then I'm going to write that down soon. But we really did use some ideas and facts that were proven in a work of Benjamini Berger. Work of Benjamini, Berger, Hoffman, and Mosul. So the shocks, well, let's get things first. So we will be doing the canonical coupling, canonical, canonical coupling, yes, for two copies of the chain, meaning that we will be trying to do identical moves. And again, the And again, the first time that we match will bound the mixing time. Okay, and so what we notice is that, okay, here are the shocks. And this is very related, as I said, to the work of Benjamin Berger. Hoffman and Mosul We notice that when a lot of particles have exited the interval in this three-sided case then Then the left part of the interval has to be empty with high probability. And that's why I actually showed you the lower bound argument, even though it is easy, because this pretty much says that. Because this pretty much says that we are very close to stationarity. Like, this is the behavior that we expected to see under the stationary measure: that you know, we won't have a lot of particles at the first part of the interval. And then for the second part, we proved that once we are in such a state Such a state means satisfying this property. Then we will hit the all-zero state very soon. Then we will heat the all-empty space, the all-empty state in linear steps. And this was really And this was really the main idea here. Somehow, hitting the all-zero state is enough for us to argue that we have coupled them. Okay, but this really works because of the occurrence of shocks. And okay, so now for the last part of the talk, I actually prepared already a few things because it is a little bit more because Little bit more because of a few things to introduce. So the last thing that we discussed is in terms of the high and low density regimes. So again, we have the same parameters, P1 minus P and alpha beta, gamma delta, but there are these new parameters A and B that play a very big role. Now their definition isn't something that I'm going to discuss necessarily. That I'm going to discuss necessarily, and it's something that physicists have really come up with and noticed. The important thing in terms of these parameters is that there is some understanding in terms of the invariant distribution. The invariant distribution is an interpolation between Bernoulli products on C with densities 1 over 1 plus A and 1 over 1 plus B, respectively. That is, you know, as we make N and N bigger. As we make NNN bigger. Now, the two regimes that we could study are the high density and the low density regimes. So that's the case where A is bigger than B and 1, and the case that B is bigger than A and 1, respectively. And then again, we proved that the mixing time is linear. And then, okay, we had also a side result that indicates that both A and B are equal to one. A and b are equal to one, we get the polynomial bound. That is less interesting. The way that we addressed this regime is again, we looked at the canonical coupling and we studied the difference process of XT and YT. The difference process was introduced earlier in the symmetric case. And we discussed back then that the first time that all second class Then, that the first time that all second-class particles have exited is a coupling time. And to study this capital T, what we did, I mean, this is, you know, not really showing you everything, of course, but just an idea. We considered two new copies of the Markov chain. The first one starts from stationarity. The second one has a different beta parameter. Beta parameter. It's a beta parameter that is bigger than the current beta, but still makes us stay in the high density regime, because okay, I wrote the proof only for the high density case. And we consider their difference process. Now, studying their difference process, this D tilde, we could actually get results for D. And the main And the main thing that we used was the existence of a current for this d tilde. The d tilde has this property that because we are trying to do same moves on x tilde and y tilde, we notice that if we have a particle here that tries to exit, it's more, it exits more often under the second process. Okay, so this. Okay, so this means that there could be second-class particles entering from the right in D tilde. Okay, so that's a different thing that happens here. You won't see this in D, you'll see it in D tilde. And so whenever we try to keep track of the number of second class particles that have entered minus the second class particles that have exited, there is a current. Exited, there is a current, meaning that we expect that on linear time many second-class particles will have exited. And this is actually really helpful for the actual process, because we managed to prove, and this is the most difficult dilemma in our paper, that knowing that many second-class particles have exited in D tilde, we can actually prove. We can actually prove that with high probability the d difference has no second-class particles. So, this is really the most difficult argument in our paper. And I will finish with an important open question here. It's probably the most interesting case, which is the Max current case. Which is the max current case, which is in these obscure parameters that were defined earlier, A and B, if both of them are less than one, we have no mixing results. And however, we do have a conjecture that the mixing time will be n to the three halves and there will be no cutoff. And this conjecture is based on results of. Conjecture is based on results of Balasepoline, and they gave you the realization that Corwin and Dimitrov have studied. So I am going to stop here saying that, oh, I do want to say actually that this last, the difficult lemma that I mentioned here did remind me a little bit of card shuffling in the proof because somehow In the proof, because somehow we don't only study second-class particles, but in fact, we define type 3, type 4, and type 5 particles. So, suddenly we have something between exclusion and interchange, where we have multiple types of particles, and each one has a different behavior. And as I said, it's too complicated to present in this talk. But I think I will. I think I will stop here and thank you for your attention. Okay, thank you.