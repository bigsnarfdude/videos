And one will be in Vide on the unit boss. And this is joint work with Joe Nick, Amit, and Yinpeng, who all should be here. The last guy will be here, I promise. Okay, so we're going to start with 2D. So what we're going to think about here are images, like this one. And what we're going to do is we're going to come up with the fast method of expanding this image into the image into the harmonics on the unities. So they are the eigenfunctions of the Dirgeley La Passion on the unities and they have the following expression here. So in polar coordinates you have Bessel functions in the radial direction and Fourier modes in the angular one. So I will also refer to this as Fourier Bessel functions or the Fourier Bessel basis. And some of them are illustrated here in case you want to see them. So like I mentioned the goal is to take an image like this expand it into is to take an image like this, expand it into this basis, and I've tried to illustrate that here. So here on the left, I'm using a very small number of basis functions and I'm adding more and more and more. And if we keep going, we're going to end up back at the front. So why are we trying to do this? Well, it's because we want to do computations with these kinds of images, and this basis has a number of nice properties. First of all, it's an orthonormal set of basis functions, so conditioning is good. Second of all, we want to Second of all, we want to sometimes be able to do low-pass filtering. And if I go back, that's exactly what I've done here. So here I'm retaining only the low-frequency components, so the small lambdas, and then I'm adding more and more high-frequency components. And there are some more properties that are nice. So one is that it is sterile, meaning that it's easy to rotate images in the spaces once you've expanded it. And I've tried to illustrate that here. So this is a picture of Fourier. A picture of Fourier. And if you really, really want to rotate Fourier, what you can do is you can take this image, expand it in the spaces, and then apply a closed form operation and then re-expand it back on the Cartesian. So that's what I've done with the techniques of this talk. I didn't just cheat and flip it. I did this whole. I did. I did. I'll show you the code after if you want. And then there's one more property, which is that this. This phase is actually nice if you want to do something maybe a little more specialized, namely take this image and convolve it with a radial puncture. It turns out that this actually becomes a diagonal operator and it will be nice in an application that's going to be in the next slide. Okay, so I compared this set of basis functions with a few other choices. It's probably too small for you to see, but the Fourier Bessel basis here is the last row. And the columns are whether or not they're orthonormal, sterable, and so on. Starable and so on. And then this last column is going to be what we'll talk about. So that is whether or not there exists a fast method to expand into and out of this. For the next 10, 15 minutes, we're going to talk about this green check mark. Okay, so this is the application that uses this force property. So these phase functions, they're a natural choice whenever you have a data set that has some kind of rotation acting on it. It could be in 2D or in 3D. Or in 3D, and this is a data set from CryEM, meaning that this is a biological molecule or a projection image of it. And these other images, they are projection images of the same molecule, but it's been randomly rotated. So that's what the data set looks like. And then it's probably natural to use a set of basis functions that respects this rotation. So we have two applications here. One is a rotation invariant covariance estimation. So I'm showing on the top row. Showing on the top row PCA using a clean ground truth data set. In the middle, I'm showing what we can achieve with PCA on noisy images using our techniques. And on the bottom, there's a comparison with an older technique that doesn't do it. And then the second application that I just want to mention is that we can do image noising. So these images are experimental, or sorry, they're projection images of the same molecule, but less cartoonish. But these are not the ones that. Tunish. But these are not the ones that we actually get in experiments. We get something that looks more like this. And it's so noisy that you can barely see that there's a signal there at all. But still, using the techniques of this talk, we can end up with denoised images that look like this. So they're not perfect, but if you come close and look at it, you'll see that there are some artifacts, but they are pretty. Okay, so hopefully now we're all on board with what I'm trying to do and why we're doing. What I'm trying to do and why we're doing it. But I first want to have this one slide talk about why it's not super easy. So I'm not claiming that it's hard, but it's a little harder than it might seem at first. The first thing you notice when you look at these spaces functions is that they're separable in polar coordinates. So if we're given samples of this image on a polar grid, we could just do a product erger and we'll get something that's fast. We can do that, of course, because mostly in applications you get images on a partition. Applications, you get images on a Cartesian grid like this. So you can't use this a priori. But then the second thought that you have is probably: well, why can't we just try to interpolate it from the Cartesian grid to this polar grid? And that might work in some settings, but not in the ones that we care about, because the accuracy of this method, if we do interpolation, would hinge on the accuracy of the interpolation. So on the smoothness properties of the underlying function and the pixel distance and so on. But we want to, again, be able to do it for data that looks like this. That looks like this. So it's very noisy, not continuous, not smooth. And even if this was not the case, there would be cusps and other things we don't really. So we are not going to do this. And instead, we're going to do something a little bit more involved. And that's the content of this theorem that I want to just go through. So we're going to let xj be an enumeration of the pixels in an image of size L by L, and xj are the values. Are the values. And the operators that we care about when we expand into and out of this basis are B and B star. And what are those? Well, B takes a set of basis coefficients, alpha, and just re-expands these coefficients onto the Cartesian grid by just taking the linear combination where we have xj being all the pixel bytes. And the adjoint then is opposite. So it takes the inner product of the image with all the basis. So if we So, if we want to do this in a brute force way, what we do is we would form this matrix B. So, we take all the basis functions and evaluate them at all the pixel values, and then we would just apply it. But this would be a large matrix. So, the number of rows would be the number of pixels, so that's roughly L squared. The number of columns would be the number of basis functions, also the order of L squared. So, applying this to the vector would be order L to the fourth, which is two slow curves. The fourth, which is too slow for us. So instead, what we're doing is we're coming up with a clever way where we can get away with doing L squared log L operations and we get corresponding accuracy guarantees. That could be why you have different norms, left and right. I was just going to say that, yes. So it is natural because we're going to use some non-uniform fast-forward transforms, and they are typically traced in these norms. But I will show numerical results that use stronger L2 to L2 paths. Stronger L2 to L2 paths. Okay, so for the rest of the 2D part of the talk, I want to go over how we apply these operators and also why it's fast and why it's accurate. And it actually boils down to one fairly simple idea, which is a continuous analytical identity, which is in this demo. So if we have a function f that is a linear combination of a finite number of basic functions. Of a finite number of basis functions, then there's actually a closed-form expression for obtaining the basis coefficients alpha. So, what we do is we define an intermediate quantity, I'm going to call beta, and it's defined for radius rho. And how do we define it? Well, we take the Fourier transform of f, evaluate at radius rho and angle pi, and then we integrate out the nth angle components. That gives beta n at radius rho, and then the And then the trick is that if you evaluate this beta at a very specific radius, lambda nk, and multiply by a constant, you get the basis coefficients. So here, lambda nk, that's the kth root of the nth vessel function again. So they're known and tabulated, and we can evaluate. Yeah, so just the four transform, but I'm writing it in in polar coordinates. Yes. It turns out it will not. So we will end up doing some interpolation later on in the smooth quantity. But for as phrased, yes, sorry, I agree. But we're going to replace this with some discrete operators in a second. And there will be no. Thank you. Yeah, exactly. So that's the idea. We are going to take the So that's the idea. We are going to take these continuous identities and replace them by the discrete ones. So we're literally just going to replace this continuous transform, forward transform here, by an UFFT, non-uniform fast forward transform. And then this integral is also going to be an FFT. But it already leads to two challenges. So one is it's not really clear what happens when we replace these continuous things by discrete things. We would want this to somehow give this operator B star that I talked about on the previous. operator bstar that I talked about on the previous slide and it turns out that it will but it's not really clear as of now and the second problem is that this is not fast enough either so as riften it turns out we need to do at least order L f of T's for each value of lambda and there are many of them that are order L squared so this is going to give at least L cubed which is just so for the second problem there is an idea that we can use to get away with this and This and that is to do interpolation, but not on the images, but on the smooth helper cognitive data. So we're going to evaluate not at all row equals lambda and k, instead just rho at a smaller set of interpolation nodes, and then we'll interpolate it up to the radii that we care about. So, this is the main idea, and with that, we can actually write down the algorithm, and just to be really Algorithm. And just to be really thorough, I wrote it down twice. So one is with words here, and one is with images. And let's maybe look at the illustration instead. So the whole framework is that we take an image in real space, then we do an NUFFT onto a polar grid in Pourier space. So in the angular direction, we have uniform nodes. In the radial ones, we have Chebyshev nodes for interpolation later on. And then once we have this, we look at each radial shell and we do an FFT in angular direction. And that gives us access to quantum. Okay, and that gives us access to quantity which is a proxy to this beta function I had on previous slide, but at the wrong nodes. So at the Chebyshev nodes, so then we do an interpolation step and we get it at the nodes that we care about. Okay. So I want to take maybe more slides, I think, if I understand correctly, and just go over why this gives the right accuracy and the right complexity. So if you look at So, if you look at all the steps I wrote down, you can write down the total complexity. It has this expression where s is the number of uniform angular nodes and q the number of radial chebysher nodes. And if we start this long enough, we'll see that if we can improve that choosing s and q to be of the order l, then that is going to give the complexity that I promised. That's what we're going to try to do. And to do that, I have to write down the total output of this algorithm. The total output of this algorithm. So, this is the composition of the three steps. So, I just took this and inserted it into this and then into this, and we get this quantity here with one caveat, and that is that we're doing these non-uniform FFTs and fast inflation that have some kind of accuracy to them. In this expression, I'm just ignoring that and pretending that they're completely accurate, and then we'll return to that issue. And then we'll return to that issue a little bit later. Okay, so the first question is: does this algorithm actually approximate what we want? So does it approximate the dense operator, which is this B star F here? And this lemma, I'm sorry, it's a block of text, but it's saying yes, it does. If S and Q are of the order of L, then the output of the algorithm is close to the dense operator, and the difference is bounded by an explicit constant C, gamma. A constant C, gamma is a user-defined position that you can think of as being small, and then there's a log term, and then there is this norm L1 norm of F that was in the main theorem. So we're going to quickly go over a proof sketch of this, just to give a flavor of the analysis. And I've tried to illustrate that here with a schematic. So there are three curves here: green, red, and blue. Green is the output of the algorithm, and I'm not. Is the output of the algorithm, and I'm not tall enough to reach the top node, so I'm going to do it here. So, these three middle nodes, that's the output of the second step of the algorithm over here. So, that gives us access to this beta quantity at Chebyshev nodes that I'm going to call t1, t2, and t3. And then this last step, what it's doing is it's interpolating this quantity from t1, t2, t3 to the lambdas, so to lambda one and lambda. That. So that's the output, but the quantity that we want to compute is the blue one. That's the action of this dense operator evaluated at these lambda nodes. But to prove that green is close to blue, it's helpful to use this helper conditor, the red one, which is the action of the dense operator, but at the wrong notes. So the only difference here, unless I made a typo, should be that this one has lambda and that has Jebushin. This one has lambda, and that has Jebusion. So we're going to show that green is close to this red, and this red is close to that blue. And then we'll use that together to show that this green is close to that blue, which is the output of the whole algorithm and what we actually. And we'll start with green to red. So what we have here is the output of step two. The alphabet of step two. And I'm going to take this inner term here and give it a name. I'm going to call it g and j. And then we're bounding this sum and this term here with the Bessel function. So if we look at that, the first thing here is just a Riemann sum of the G and J's. This second term, turns out, is exactly the analytical integral of G and J. That follows from the G and J. That follows from the Jacobi-Anger identity. So, if we want to bound the difference, what we're doing here is we're doing quadrature, and it's a periodic function, and we're doing the trapezoidal rule, so we get some very nice error term. So, all we need to show is that this quantity is small enough, and that can be done. And in fact, it's decreasing exponentially in S. So, S is number of angular nodes. So, if we look a little bit at the If we look a little bit at the constants, it shows that if we choose s to be just out of the order of l, then this term is going to be small enough. So the green is going to be close to the red one. And then for the next one, what are we doing here? Well, we're taking a function defined here, the Jebushi node, and just interpolating it to the other nodes. So to control this error, we basically just have to interpolate a Bessel function. A Bessel function from the temperature nodes to the lambda nodes. Okay, so the Bessel function is smooth and nice, so we can get some very standard interpolation estimates here and show that this error also decreases exponentially. So again, if we choose this Q, this number of radial nodes to be of the order L, then this is going to be small. So everything is small, meaning we can put this together and show that the quantity that we care about. That we care about. So this green minus that blue is going to be small. And there's just one more step. So we have to do the triangle in polynomial, and we get two terms. This is what we just found on the last slide, less than gamma. This whole thing is not what we had two slides ago, but what's inside the parenthesis is what we had. We showed that that was less than gamma. So we get less than gamma plus gamma times this sum of the Lagrange polynomials, but that's something that you can bound as well. That you can bond as well. So we end up with this log term that was in the lemma statements, and that's that. So it was very sketchy, but that's the main idea. And then I just want to return to this issue about doing things exactly versus having some precision. So this NEFFT step and this fast interpolation will have some error. And if you take that into account, that will give us two additional error terms. They're a little bit messier to work. They're a little bit messier to work out, so I'm omitting the details. But we end up with quantities like this. So we get the precision of the NFFT times the log term plus Q times the precision of the interpolation times the L1 norm. But it's all fine. We can absorb that into our constants and set the precision to take care of the log and of the interpolation to take care of the Q. And that is still going to work with the right complexity. Still going to work with the right complexity. So, this is the formula I had from before. And if you now put in all the expressions I talked about, we end up with this complexity that I promised. Okay, so it is actually fast. And just to show that, I have some computational results here. And I'm showing two things. One is the accuracy, and one is the timing. And here, I'm showing not the L1. here I'm showing not the L1 to L infinity bounds of the theorem but L2 to L2 which is stronger and one is for applying B and one is for applying the adjoint so this would be one of them and this is one this is the other this is one this is the other for a few different values of the image size L and accuracy epsilon and this these precisions they they should be less than these errors should be less than the precision epsilon and they These errors should be less than the position epsilon, and they are all the way up to when we're close to machine position, where we get some round-off errors. It's accurate, and it's also fast. So here I'm comparing it to the dense brute force way of doing it, which is explicitly forming this huge L squared by L squared basis matrix and applying it. And there are two figures: one is for the pre-computation, and one is for actually applying the operators. And for the advanced one, we could actually only do it up to roughly L equals 160, so that's why the top part is interpolated. But ours works well, and we get up to, I think it's six orders of magnitude improvement. And we ran this on a good computer, but a modest computer. So it's actually, it started out with 32 gigs of RAM, but then we unfortunately. Of RAM, but then we unfortunately spilled coffee on one of the memory sticks, so we had to run everything with 24 gigs of memory, and it still works. Okay, so that's that for the 2D. And then in the minutes I have left, I just want to say a few words about the corresponding algorithm for 3D. So now we're not looking at images anymore, we're looking at 3D arrays, so structures like this. But we also want to expand into some bases that's good for rotations and low-pass filtering and And low pass deltring n. So the eigenfunctions of the Dirt Dela fashion in P are these functions here. We again have a Bessel function, but now it's a spherical Bessel function in the radar direction, and then we have spherical harmonics. So we can actually do something very similar. So I've just written down the algorithm here, which has very similar steps. So we start with a volume in real space, then we do an IUFFT now onto a spherical grid. Now, onto a spherical grid, and for each spherical shell, what we do is we do no longer an FFT to go down here, but we do a fast spherical harmonics transform. And we get access to this beta quantity at Chebyshev radial nodes. And then we do interpolation to the nodes that we care about, and we use a similar continuous analytical identity. Okay, so we can write down a precise statement, and it's We can write down a precise statement, and it's very similar to the 2D statement. But now there are voxels instead of pixels. So xj is an enumeration of the voxels. And I change the L to N, so we have an N by N by N volume. But we again care about the operator that expands onto the grid and the adjoint that takes the inner product. And now, if we were to do this in a dense spirit force way, this would mean forming B, which is an N cubed by N cubed matrix. So applying it is N. matrix, so applying it is n to the sixth, but the past algorithm that I just talked about is n cubed log n squared, but again with the same accuracy grants. Yeah, so that this one here, that comes from the complexity we're using for the fast work of Monica SARS. Yes. But it also has a nice consequence because since we already have an Because since we already have an extra log term here, we can actually, even in this statement, replace L2 by L2. But just it's going to be a constant factor increase of the complexity. That actually why on the next slide I'm also going to show L2 to L2 and L1 to L infinity errors here. So let's see. This quantity and that quantity, they're L1 to L infinity. So these are the columns. And then these quantities are L2 to L2. These conjugate are L2 to L2. So these two parts. And it is accurate. But here, like I said, if we paid just a small constant factor extra complexity, these L to L to bonds would also be less than precision. I just didn't do it for this table, but it can be done. And for the timings, we actually have two variants that differ in the specific Spassberg harmonics transform we're using. And I think. And I think this gives 9 to 10 orders of magnitude increase. And I should say, even for these figures, we do have a GPU implementation that gives another factor 10 increase, but I didn't include it in the OK. I think I'll stop there. So a summary, we looked at two fast things. We want to expand images and volumes into the spaces that is very neat when you want to do rotations. Rotations, and we proved accuracy guarantees with L square log L and cubed log. Thank you. Hi, Oscar. I'm Dustin. Hi, Dustin. I have a couple questions. First, you walked us through the sketchbook group of Narabound. Do you have a sense of whether that bound is close to As close to empirical errors, you observe in practice? They are slightly pessimistic. So we do have some looser choices of these S and Q, the number, radial, and angle notes that do work well in PRP. My second question for the fast spherical harmonic transform that you used, you didn't make That you used, yes. You didn't make that. Do you think log squared's possible we could do something better there? Yeah. Hey, I'm Rob. So your numerical results were pretty impressive. It looked like you were processing 512 by 512 by 512 images. 512 by 512 initiatives in about an hour. Is that the dream? Did you already achieve an algorithm good enough for all scientific applications? Or is there something that you'd still like to do that would require further technology? That's a good question. I think this is at the limit of what this technique can do at least. Is this already sufficient for any cryo-elementization? Faster is always better, but I think this is hopefully. Is always better, but I think this is hopefully good enough. And this is also, in some applications, you don't need to use all the basis functions that we're doing here. So I'm basically using nq basis functions, but you might want to truncate that in some applications, and then it's going to be faster. You already killed it. I mean, that'll be large models, but it will be a little bit more. It looks like you're nearly there. Yeah. Hi. So, in terms of practical random time, so there were like previous approaches that exploited like the angular direction using FFTs, but not the radius. So, like from your experience, how much From your experience, how much faster is this new approach to? Yeah, so if you're thinking about the one doing it in poor space altogether, so those ones, depending on the machines that we tested on, were roughly three times faster, but we're also provably accurate, which those ones are not to the same degree. Hi, Joy. So from the point of view of someone who thinks that Borlates for real functions are the solution to everything in life, I think you made a very good argument for my question though is about your assumptions on the sort of function of spaces that this applies to. So if you have a lot of I think so if you have a lot of if you have an image or something like this with a lot of noise in this, it's difficult to say that it's uh that it's uh kind of uh in any sense kind of band limited. Uh what you would want maybe is to project the control space, but if you're using some kind of arditor it would usually provide that you're assuming that the that there is some kind limit. Otherwise it would be somehow Otherwise there would be some alias. Do you know or do you have any estimate of like how the noise that's outside of your basis of functions, how that would behave when you're like projected and then destructive? That's a good question. I am not entirely sure. So, what we're doing is we're setting a band limit for the number of basis functions to use, basically using an FFT heuristic to not have too many. And if you do keep increasing that, then this change of basis matrix that we're looking at is becoming poorly conditioned. So, we haven't explored that too much. Rob's question, which is that you want to go eight times larger, have you thought about exploring the sparsity of images? We are not. That could be interesting. For sure. 