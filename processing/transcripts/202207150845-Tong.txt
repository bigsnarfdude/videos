Thanks, Ladislav. So today I'm going to be talking about something I'm calling multi-scale earth-merface distances. And so we'll see some applications of this and maybe why it's useful. And then also relate it to graph neural networks and how Earth prover's distances on graph is sort of like scattering and sort of like these other things that through diffusion. Cool. So very, right, we have large dimensional data, but Data, but often we think about them as distributions of points. But what if we have distributions of distributions? So, like many different samples that are distributions themselves in some underlying space. So, this happens in single-cell data and other data types. But we need some sort of way of reasoning about these point clouds that live in some underlying space, and we want to understand how clouds are different to each other or distributions are different or similar. Are different or similar, and so one of the best ways to do that is through visualization. And so, we sort of know how to do this when we have a collection of points, right? So, there's many, many different methods of visualizing a number of points. So, you know, fate, PCA, maps, so many of these. And one key thing about these is that you need fast nearest neighbors. So, if you can compute your nearest neighbors very quickly, then you can do all these embeddings based on graphs. So, if you want to construct a graph between Construct a graph between distributions instead of points, you would need fast nearest neighbors in sort of a distribution distance. So that's what I'm very curious about, how you can get a fast distance between distributions. And we'll talk about Earth-based distance. So one of the nice things about Earth-based distance is it's sort of the most natural way of lifting distances between points to distances between distributions. To distances between distributions, because essentially what you're saying is: well, take a distribution between points and then take the minimum total cost of moving things. So given two distributions over some discrete data, the easiest way to think about it is sort of a matrix form. So you have the cost matrix and you have the joint distribution pi. And so you're looking for the pi that minimizes the total cost multiplied by the amount of mass you're moving. Amount of mass you're moving. And so this is sort of the most intuitive way of thinking about: well, okay, I have distance between points, I have good distances for that, but now I want to think about distances between the distributions. Cool. And so we're interested in nearest neighbors in sort of an EMD space. So for M distributions, each of some size N, we want to find the K nearest neighbors. So there's sort of some existing ways. So, there's sort of some existing ways to do this. So, most alpha transport, I would say, is done via the second method, the entropic organization. So, this is fairly fast in that it's just n squared cost. So, you have a cost matrix of size squared, and you're multiplying it back and forth to get the correct marginals. But it still has this cost of n squared. So, if you have a lot of distributions, then it turns out that you need to scan through each one to find my car near neighbors. So, just for a single K-nearest neighbors. So, just for a single point, right, I need to scan through M distributions and compute the output transport to all of them to find the minimum. And this is because the EMT doesn't have this nice geometric space. So a Euclidean space, you can cut things in half and say, build a tree on the space and find your nearest neighbors by eliminating half the points and then eliminating half the points again. But when you have the EMB distance, it doesn't have this geometry. And so we're going to talk about. And so we're going to talk about some multiscale distances that have this geometry. So some other ways of computing the MD is if your underlying space between points is on a tree instead of in RD, then actually you can compute this much faster, which was shown by some earlier work. And we also are going to show how you can do it on a manifold. So if your points lie on some manifold represented by a graph, then you can also do this quickly. By a graph, then you can also do this quickly, very approximately, with this method we're calling diffusion EMD. And this last method was VMD, so that's an earlier work. And you could also consider just discretizing your space into a small number of clusters and then doing EMD there. So those are sort of the different ways you could think about how you want to do EMD to your nearest neighbors in a quick way. So I'm just going to talk about optimal transport on trees to get an information of how this works. So two distributions. Of how this works. So, two distributions over the leaves, red and blue. And we want to say, well, how far away is red from blue? And so, you know, optimal transport says, well, I have to move the red points to the blue points or the blue points to the red points. It's currently very equivalent here, right, to try to move in one direction or the other. And then one nice thing to do is, well, okay, how many red points are below this node and how many blue points are below this node? So, you know, starting over here at this. So, you know, starting over here, right, there are two points, two blue points here and one red point. And, you know, in this node, same thing, this one, two, and three. And so it's actually the difference between these two numbers that is very interesting, right? So looking at how much mass goes on this edge, you can look at the difference between these two numbers. So the total opinion transport cost, right, is the amount of mass you moved times the distance. So to look at the optimal transport distance, So to look at the opposite transport distance, we would take these numbers and then multiply it by the cost. Yeah, so formally, right, so if we defined an edge weight on each of these edges and then looked at the trees that said how much mass is below each node, so take this and so gamma E of here would be two. And then you could find E of D on this tree domain by just subtracting, I guess, yeah, subtracting and taking the absolute. Yes, yes, subtracting and taking the absolute value and then weighing by edges. Cool. So another way to think about this is defining this function f that takes a distribution on the tree and maps it to some vector. So this can be a vector of the size of the number of nodes in the tree. And what it does is takes the edge weight times the amount of stuff for that distribution below it and makes a number. And so this is a way of getting this is this is a way of getting um so now you can write rewrite the cmd between the two distributions as a l1 difference between two vectors so this is sort of the main trick here in multi-scale earth murface distances if you can embed your earth motor distance into l1 then you sort of have this nice space right so now we can do tricks like slicing the space in half and collecting for nearest neighbors in only half of the distributions so So, the key here, right, is I take a distribution in some nice space and then I can embed it into a vector where the distance between the vectors in L1 is equivalent to the Earth curvature distance in some way. Cool. And so that was sort of a tree in one dimension, but you can consider sort of a tree in any dimensions, right? So a nice way to build trees, which was used before, is to take a random point location and then slice the space in. Point location, and then slice the space in half in every dimension at once. And so then you've got sort of two to the dimension number of nodes per branch. And so it turns out that this is a sort of a good approximation of the Euclidean distance. So if you build a lot of trees like this, then you can sow your sort of bounded relative resistance in using the Euclidean metric. So yeah, that's one nice tree construction, but there's many others. But there are plenty others. So another way of thinking about this is actually in terms of a wavelet basis. So this tree is really, you could look at it as like this node is representing one, a giant bin all the way across. And then the next one is sort of split into two. So each bin has half of the space. And so you can consider sort of the, if this was like on a real line, right, you're sort of dividing your space in half and half. So you could look at Space in half and in half. So you could look at these as sort of as either bins, or right, these are bins of different sizes, or as another person has shown with graphene, then you can use it as a hard basis. So this would be bins that you don't normalize, so they aren't wavelets. But if you wanted to normalize that they sum to one, then you can make a basis that would also be the same. That would also do the same embedding. So, the trick here, right, is that instead of thinking about it as a tree always, so you can also think about it as a weightlet basis. And so, once you can think about that, we can do many other things with weightlets besides just the hard basis. So that's sort of what we thought about in the next project. And so, wavelets in general, you can think about it doing EMD in a dual form. I'm not going to go into that too much, but the Too much, but the idea is that if you're sort of looking for this witness function in Earth memory's distance, and it turns out that this optimal witness function is easy to compute in the wavelet domain. So if you transfer everything to the wavelet domain, then the optimal witness function is easy to compute. And so you can look at it as just the differences of histograms in the wavelet basis weighted. And so yes, so this avoids sort of this annoying problem where you would have to compute. This annoying problem where you would have to compute pairwise distance matrices between all your points, you can instead compute a wavelet transform and then do the L1 distance in that domain. So, you know, we've talked about sort of Rd so far, but also you can do this on graphs. So one of the things that we were looking at is diffusion on graphs, right? And so this is just right centered at a point. Centered at a point, you can diffuse longer and longer. But then, right, so I think Frederick probably talked about this, but right scattering, you can take the wavelet bases and use that for a scattering transform, or we're going to use it slightly differently. So the diffusion EMD is the way that we're going to talk about this. So this is a way of doing EMD on graphs instead of on real spaces or on trees. On real spaces or on trees. And so the interesting thing here is that you are still using like multi-scale density estimates, right? Still multi-scale wavelets, but they're wavelets on graphs instead of on the equinox space. So this is just really, it's the same thing, but it's a different embedding function, right? So our embedding function would be this sort of t alpha k. And so it's just a weighted It's just a weighted combination of these wavelets. And the larger your wavelet is, the larger the weight should be. And so, using diffusion wavelets, if you weight it with, if they're doubling in size, then your weight should double also at every time, at every scale. Cool. So yeah, so we showed that this was equivalent to an Earth move resistance with a geodesic ground distance. With a geodesic ground distance. And so this is sort of the equivalent thing that we were doing in Euclidean distance, right? So people have done wavelets are equivalent to Earth Mervis distance for the Euclidean ground distance, but you can also show that this is true on a graph. Cool. Yeah, so some interesting data that I've been working on with our collaborators in London. The data is a bunch of The data is a bunch of cells treated with different drugs, and you have different antibodies attached. So you get a readout that gives you different antibodies per, sorry, this is from keynote, so it gives terrible transitions. Let's see. So the data cycle looks like this. We have 10 patients in three different sort of drug conditions. Sort of drug conditions, and then a bunch of different drugs applied to it with three replicates. So, this is a lot of different samples. And each one is a single-cell measurement. So for a drug, a patient, and a condition, you get a single cell measurement over these features here. And so, this is basically 3,000 different distributions of cells. And we would like to tell, well, which ones are doing, which drugs are doing similar things and which ones are doing different and, you know, how. And which ones are doing different, and you know, how are the patients different, and how is this other thing? Whether you co-culture with another type of cell, what effect does that have? And so what this looks like, right, in actual, if you take a look at the PDOs, so the patient-therried organoids are the thing we're interested in, and they're these clumpy groups of cells that we're looking at. And then the fibroblasts are this sort of. Blasts are this sort of spidery web. So, most people have looked at PDOs separately, and so that has some effect. But co-culturing them with these fiberglass has a different effect. And so, yeah, the end goal, I'm not going to show exactly what this is showing, but is to embed each sample to understand how the data, right, how the treatments affect thing, how the microenvironment that affects things, and how the patients are similar. The patients are similar or different. And so we're going to do this using four steps. So either we're going to build a sort of a graph or a tree, so in this case, a tree, but you could also consider a graph over the data. And then you could look at sort of the weighted histograms for each sample over the nodes. So like it's either the nodes of the tree or the nodes of the graph, and you're looking for the weighted histogram differences over these. Differences over these and then I'm going to talk about this step three. So, what's interesting is you can control for some sort of biases, and then you can find a neighbors in L1 between the histograms. So what's interesting about this data set is it's so big because you're putting and you're putting things on small plates. So you have these plates of 96 wells. Wells. And so when you put it through the machine, right, this 96 is sort of well controlled within itself, but between plates, you might have some technical effects. And so this ends up that you have controls on each plate, but you don't necessarily, right, the controls, even if they're for the same condition, for the same patient, the same biologically, right, they might appear different because they're on a different well and there are technical effects. And also the data is collected over a long period of time. Over a long period of time, so there may be effects, right? Different sort of people loading it or different machines. So, this happens here, but it could also happen more broadly. And so what we want to do is write control for the fact. And because we know that we have controls for each drug based on the same well, we want to say, well, what are the differences to the control, sort of the most close control that we can get? And so that should be, that would be the best way to sort of figure out exactly what the drug is. You sort of figure out exactly what the drug is doing rather than incorporating technical effects or other sort of biological variation. Yeah, and so when we looked at this data at the sort of the there's many different patients, right, and the density. So this is one marker. And we say that, you know, sort of the patient has a different amount of this marker, right, depending on which patient it is. And we're not really sure, right? There's no, yeah, there's no. Yeah, there's no clear reason why, like, we're not interested in why patient at one has, you know, a lot more of these cells versus a lot fewer, but maybe the signal that we're looking for has much smaller differences in this mention. So, you know, you could be hidden under sort of this technical variation between patients, the single you're looking for. And so what we looked at. And so what we looked at here is some interesting ideas so that, well, I want to say like what is the difference between these subtractive distributions basically. So if I have a control distribution for every for every drug, then I want to say what is the effect of that, right? So what is the difference between that control distribution and the treatment distribution? And then I want to take the distance between these. And so this doesn't fit in the frame. This doesn't fit in the framework of Earth resistance very well, right? Because Earth resistance, you have to have these sort of equal masses, right? But this would be zero mass because you're subtracting something with, you know, with unit mass and something else with unit mass. So it's zero mass. So if you tried to plug these into the standard primal solvers, you would not be able to handle it. And they can't even, yeah, you don't get a defined solution because it would basically say transport negative things. Negative things. And so, so, but what's interesting here is that there's still a valid distance between these distributions, even though they're not, you know, now they have negative values in them because they set to zero. And so we can actually just subtract the vectors. So we embedded each distribution with a vector, and now we can subtract the control vector from each one and take the L1 distance. And so it turns out that this is, and it has an interpretation as the Kentucky-Rubenstein norm of the Einstein norm of the sort of embedded vectors. And so, yeah, that's sort of the connection with the Earth resistance still, but it's slightly expanded in that you don't need the masses to sum to the same thing. They can be anything really. Yeah, so this is the sort of trick we're using to normalize out the distributions. And so now you can handle like the distances between distributions. Distances between distributions relative to some control. So now you can control out for some variable. Yeah, so finally, right, all you need to do is just sort of find the nearest neighbors between the histograms. So you can do this very quickly using any sort of nearest neighbors algorithm that can take in different types of distances between vectors. And so now we've Vectors. And so now we've simplified the problem back down to well, distances between points in high-dimensional space. So, yeah, this is an interesting framework for computing distances between distributions by reducing them to be distributions, differences between points instead. Yeah, and so we use this. This is the same data. We use this so that, right, if we have a control, one thing that happened was that the control sort of appeared. That happened was that the control sort of appears everywhere just because of technical variation. But when you do this sort of controlling and you subtract the control distribution from every treatment, now you're sort of embedding based on the effect of the treatment rather than the differences between distributions. And so yeah, that's what we're able to do here. And this turns out to be way more informative than Maria, who's doing the Who's doing the biological work here can sort of draw, figure out what exactly is happening. Cool. Yeah, so I presented some multi-scale earthwave resistances. So either you can do it on a tree or on a graph. And the way that you do this is embed it into L1. And then so when you're doing it on a graph, it's actually very similar to graph neural networks where you are diffusing signals. Networks where you are diffusing signals. And so, if you, for instance, did your scattering transform for one layer, so you just your wavelength transform, and then did the weighted L1 distances, then you would actually be doing EMD between your signals. It's kind of interesting. And then we showed that you can use EMD to control for these sort of covariates by just subtracting the distribution, even if you don't know anything about what's happening. You can just subtract if you have any match. Uh, you can just subtract if you have any matched control. Uh, so yeah, this is a way of uh expanding the interference resistance for the fact that you have multiple controls in some cases. Uh, cool. Yep, so that's what I have. Thanks.