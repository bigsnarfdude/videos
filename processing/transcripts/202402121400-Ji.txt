It's a pleasure to be here. 30 minutes. 30 minutes. Okay. So I'm going to talk about something a little different. Technically, I'm not in machine learning, although machine learning nowadays seems to just encompass everything, right? So if I work on productivity models, I also want to machine learning. So I'm a Bayesian by training, not by training, but brand watching by professionals. The first two fundamental theoretical classes I took were all taught by Bayesian. So I never had a chance to be exposed to frequencies. So I apologize for that. And so the topic here is called Platt Atoms models. If you have a Tesla, you know what Platt means. I actually don't know what PAP means, but look it up. Plaid basically means just looking for someone. It's a cross-pattern in terms of the shirt the Lingbo is wearing. So I explained why you can use this name. And if you read the abstract, it's actually a slightly different title, but it's basically the same. Oh, sorry. This is supposed to be for brand folks, not for Australia. So you didn't read that. So, what I want to do is to introduce. Want to do is to introduce something called Plaid Atoms model. If you don't know what that means, it's basically a class of Bayesian number magic models. So how many of you know about Bayesian number metric models? So I have to be careful of what I say. But not everyone does. So perhaps it's a good way to learn. And the research problem I'm interested in is basically. interested in is basically basically about a clustering. We're all no clustering, which one k-means, near sneezers. And then the type of clustering I'm interested in is something called dependent clusters. So for example, I work on clinical trials in drug development. It doesn't have to be. You can just treat these as three different data sets. And within each data set, there are observations, right? So here there are three trials and four subjects for each trial. For each trial. So the subject 1, 2, 3, 4 for trial 1 are different from subject 1234 for trial 2. But just call them subjects. And what I want to do is to cluster them and use colors to represent these clusters. And what I want to achieve is to define a power reading model that can generate these dependent clusters. For example, I have a gray cluster, a purple, a yellow. Purple, yellow, red, and blue. You can see that the gray cluster is shared across all three data sets. So there's a common cluster that is shared. And then the red cluster is shared between two of them, but not the first one. And the purple is shared between one and two, but not the third. And then there are some unique ones like the yellow and the blue. So I want to generate a pattern like this. I want to generate a pattern like this. And as you can probably now relate to why the model is called PLAD. Plaid means cross-patterns. So you can see this is the type of pattern I want to generate. We want to use probability models for that. I don't know how, well, I kind of can learn and apply an algorithm or optimization model to generate a pattern like this. But I want to use probability models to generate that. And the applications I'm interested in. And the applications I'm interested in, they're many. Anytime you want to perform a cluster analysis on multiple data sets and you want to generate clusters that are kind of dependent, like shared or unique, then you can apply this technology or this method. The type of application I'm interested in is in clinical trials where I want to find out patient subpopulations, these are the clusters across the trial. Across the trial, so that I can predict that their response could be similar in the same treatment. So, the BMP, the Bayesian non-parametric models. So, the foundation or the very first non-parametric Bayesian models is called the Deutsche process. The paper was actually published in 1973. I was reading this in the last year of my graduate school. In the last year of my graduate school, I was kind of bored and trying to find something to do. And Ferguson today is still alive. It's quite amazing. He's, I think, over 90, I think to celebrate his 90th birthday a few years ago at UCRA. He wrote a seminal paper about defining a distribution for distributions. So just let me explain that if you can read this. But if you write down a distribution, like a normal distribution or gamma, the moment you write it down, The moment you write it down, you already assume the distribution is fixed. It has a normal distribution, has a density function, two prime and a small values. What Bayesian is interested in sometimes like for any uncertain quantities, we want to assume they are random, right? And so if we don't want to fix the distribution, what should we do? Then we want to assume the distribution is random. Then we should want, we want to define a probability measure for the random distribution. And this is one of the first. And this is one of the first tools that was developed. It's called the division process, and the way to define a random distribution is very simple. If you still remember a little bit, measure probability theory, assume we have some random variable as a sample space of that random variable, and then you assume there's a partition that divides the sample space into, let's say, k different sets, A1, 2, B, K. For a fixed distribution. For a fixed distribution of G, let's say normal, when you apply that distribution to any set A, you get a fixed value. So, for example, a normal 0, 1 on 0 to infinity, that probability is 1 half, right? That's a fixed value. If you want to define the random distribution at least on the usual process, then you assume that the vector of these g's, so you apply g to the k sets, then Sets, then if you assume it follows a Dirushi distribution, then you are saying that these G, but these K probability values are random. It follows a very simple adulushi distribution. That's your assumption, then we're essentially defining a random distribution. That random particular random distribution process. Why is it processed? Because we're defining a random distribution for distributions, and therefore it's a stochastic process. Therefore, it's a stochastic process. So that's the underlying definition of the ushi process. And there are two parameters in the DUC process. The first one is the so-called concentration parameter that you get from defining that DUSU distribution. And the second one is actually a fixed distribution called G0. So let me go into a little bit. So when I say, okay, these k values are probabilities. K values or probabilities from the random distribution follow distribution. There are two parameters: one is alpha, and the other one is a set of k values. They are the probabilities of these k sets based on g0. And g0 is a fixed distribution. So the moment you specify alpha and g0, then you can specify the true process. It only depends on these two parameters. Depends on these two parameters. So G0 could be anything like a normal distribution or gamma, anything like that. And alpha vice will be positive. So the distribution process depends on these two parameters. And then it has some nice properties. For example, the mean of the distribution, of the random distribution G, is actually G0. And so when you define the usual process, when you write down G0, you kind of know, okay, this is what I'm assuming the mean distribution is for my random user. Is for my random distribution. And the variance is actually a very nice form, kind of like a beta variance or a D process, a D distribution process, right? G0 times Y minus G. So it's like a mean times Y minus V divided by the alpha possible one. That's in the beta distribution family. Okay, so that was 1973, right? So about 21 years later, there was a very important paper to show that the Dirichlet process can be. Process can be rewritten as something that can be manipulated easily using algebra. So you can write down a Duchy process in terms of an infinite mix of point masses. So if you look at the plot on the right here, this paper shows that the dilution process can be thought of as a random probability measure or random distribution in which there are two components. One is these dots. These dots, and they are called thetas. These are called atoms. So the thetas are randomly generated from a distribution, and that distribution is G0. So imagine you sample these thetas from G0, and these are the atoms. For each atom, you want to sample something called the weight, or the probability that the random distribution takes the value at that location. At that location. So the length, so you can view this as a stick. The length of the stick is the probability of that atom. And there are infinite number of atoms, and so therefore these weights, they are called W's, have to add up to one. And then later, so this paper shows that the division process can be rewritten as equivalent to this infinite mixture of the weighted point masses at these data items. Data items. So that was the proof. And how the one specifically for the usual process, the weight has to follow this generative process where it's got GM, GM distribution, where the weights themselves can be rewritten as for the H weight. It equals V of H times 1 minus. H times 1 minus the previous V's, and each V is IID from beta distribution with parameters 1 and alpha. Remember, alpha is the concentration parameter in the digital process. So if you generate these weights as defined in this process, the first that they add up to one, you can prove it. And second, this is equivalent to that. Okay, and proof. And prove that they're the same process. So fast forward. So why did I talk about this? So basically, if you can imagine, if I apply G to a data set, it will naturally cluster the observations because it's going to assume the observations are coming from these atoms with some weight, right? So as you can imagine, some observations will be coming. Some observations will become with probability coming from the same atom. When they are assumed to come from the same atom, they are believed to come from the same clusters. That's the idea, right? And so this distribution can be applied to a single data set with multiple observations, and you can naturally generate clusters based on that. Now, what if I have multiple data sets? So, obviously, this problem, now there's any new problem you can think of, there must be a solution out there. If not, then just ask. Solution out there. If not, then just ask ChatGPT, we'll provide a solution for you. But for this problem of generating dependent random measures, so dependent clusters across multiple data sets has already been done. So a famous paper was the hierarchical digital process, which was from the Michael Jordans. We wrote all the big names, machine learning, and AIs. So hierarchical digital process, you can ignore the math. You can ignore the maths. It will take at least half a semester to go through them, but I'll tell you from the pictures what do they do. So now we're talking about not just the 1G, let's say we have three Gs, we have three data sets, and we want to generate clusters across these three data sets. So what we want to do is, so for the HDP, assumes that there is a population level random distribution, G0. This is the different G0 from the fixed distribution. G naught from the fixed distribution. A G naught is like a population level, random distribution. It has these atoms and the sticks. And what this G1 is trying to do is to generate the atoms from the atoms in G0, but they generate different weights. So in other words, G1, G2, and G3 have the same atoms, but different weights. And that can be done using this hierarchical form of derivation. Hierarchical form of the issue process for the issue process. Simple English. Nasty derivation process is different. This is from David Dunson's group a few years later after HDP. They basically assume there is already a dependent random distributions at the population level, and these random distributions share no common atoms. So their locations are So their locations are exclusive to each other. So are the weights. It doesn't matter because if the atoms are different, the rates don't really matter. But then once they assume at the population level there are these exclusive random distributions, then at the observational level, these G1, G2, G3 are basically samples from one of these populations. So for example, G1 is identical to G2. identical to G2, something there. And G2 is identical to G3 stars, right? And G3 is identical to G3 stars. So in other words, in this case, G2 and G3 are identical. They not only have the same atoms, but also same weights. But G1 and G2, G3, G1 shares nothing common with G2 and G3. Not the same atoms nor the weights. So that's another way of constructing dependencies. The way of constructing dependent random distributions. And then this was a couple years ago, Jessica paper called Common Atoms Models. It basically says, okay, I'm going to assume there's also these random distributions at the population level, and actually the atoms are the same, but the weights are different. And then at the observation level, you just take one of these samples from the population. From the population, and then that's your random distribution at the observational level. Why do they do that? Because they want something like, okay, I want at the observational level, my random distributions are able to share the same atoms. Because the middle construction has two options for the observational level distributions. One is this. Level distributions. One is there's nothing common, or everything's the same, the weights and the atoms. The third, this paper wants to say, I want to have common atoms, but I want to have different weights. So that's what they're trying to do. So yeah, I'll skip because, you know, I find I already spent half of my time. So I want to generate, we want to do something a little different. We want to say, okay, I have. Okay, I have, for example, in these three trials, I have two colors. These are two same clusters or atoms. I have the gray color, the gray atom, and then the purple atom. For trial one and two, I have exactly the same atoms and also their weights. I have two gray and two purple. For trial three, I also have the same atoms, like gray and purple, but with different weights. This is the common atom. This is the common atoms model. I want a trial 4 where I can have a red color, which is unique to trial 4. I want to do that in a probabilistic fashion, and that's what we're trying to do. So more literature, but I really shouldn't talk about what other people have done at this point. But there are many different constructions over the past two decades. And to summarize, basically. And to summarize, basically it boils down to two spectrum with two extremes. One is assume the atoms are shared across the random distributions, or they are not. For example, the nest distribution process, where there's nothing that are common between two random distributions. And there's something in between, for example, between two random distributions, some items are shared, some are de and there's construction for that, and where we belong. Product, and we belong to the middle. In 2004, there's a construction that mixes common atoms and a unique atom random distributions together to generate these random distributions. This approach assumes that for all the GJs, when they share common atoms, those common atoms are from the same random distribution. Our idea is where the title movement is. Our idea is where the title and the abstract shows. Our idea is from atom skipping. The actual mass is extremely simple. There's some theoretical properties, but if you see what we're trying to do, you'll hopefully recognize it immediately. So, there's a paper on residence turning on the review. You can read about this picture we drew about the black ones being the previous constructions. The previous constructions, then the red ones are what we're trying to do. So, what we're trying to do is very simple. We just want to stochastically skip the item. So, if you still remember this term where in the beginning I showed the random distribution g can be written as an infinite mixture of point masses, right? So, these, the phi k's, sorry, I changed notation. It used to be theta k, now it's called phi k, and pi k used to be w, and these are the weights. And these are the weights and atoms. So come back to this picture here. These are the atoms, these are the sticks, and then the length of the stick is the probability, right? And what I want to do is very simple. So you can derive the weights marginally following, for example, hierarchical process. It turns out it follows nicely a data distribution. I'm just going to add one more thing. I'm going to add a zero to the domain of the type of the packet prime. type of the pi k prime of the weight. I'm going to assume there's a positive probability that atom k would have a weight zero. So in other words, I'm going to nuke out this state and make it zero. And then the random distribution will skip this atom and go to the next one. That's the idea. It's very simple. For one distribution, for Distribution for multiple ones, let's say they're G1 and G2, then I'm going to assume there's a different probability that for each one of these G's, it will skip an atom. So for example, in G1, maybe this atom, phik, is skipped, and for G2, maybe a different atom is skipped. When I'm allowing these atoms to be skipped, we can imagine it's like the colors are being skipped from Colors are being skipped from the picture. So let me go back to that picture. How do I generate red? If I assume up front there is a red cluster in all three four trials, and then the red is skipped in both in trials one, two, and three, but not in four, then I can have a unique red color to trial four. So that could happen. I mean, it depends on for the Could happen. I mean, it depends on what the data is telling me. But I'm defining a probability model to allow that. And by these adding skippings across multiple random distributions, I can generate this type of flat pattern. Currently, this method is fully probabilistic-based. It uses MCMC, so it's not ready to handle big data sets. I'm very welcome to collaboration. I'm very welcome to collaborations if you have clever ideas on how to generate the computing to feed data. It would be fantastic. Some interesting properties comparing these different models. So we did some interesting simulations. We simulated 500 different random distributions in one data set. For each random distribution, we generate 1,000 observations. Observations. Each one of these gray lines is a realization of a group. So there are 500 groups. And these are based on either one of these previously mentioned models like CAM and HTTP, but also our own platform atoms models. And then the blue line is the aggregated cluster. Cluster across all 500 groups. So again, the gray line, each line is a group. The blue line is the aggregated over 500 groups. So what you'll see is very interesting. So for example, let's go here where this model, or one of our versions of our models, for each group, the horizontal axis here is the number of clusters. It doesn't generate a lot of It doesn't generate a lot of clusters for each group, but when you aggregate all these 500 lines, that's the blue line. It's showing that it generates a lot more clusters than each group does. What does that mean? It means it's generating more clusters that are not shared across the groups. Then, when you aggregate the groups, you realize there are many more new clusters because the groups don't share a lot of clusters. Clusters. But when you look at, for example, HTTP or CAM, then you'll see the gray lines are longer, meaning it's generating more clusters. And then when you aggregate them, the blue lines doesn't really extend much further beyond the gray lines, which means there are a lot of common clusters that are shared across the group. Just to show the properties or behaviors of these different models. Not saying which one is right or wrong, they just behave differently. Is right or wrong, they just behave differently depending on the applications. So, in the next five minutes, I show some numerical results. We did some very simple simulations again to examine the properties of these models, different models. There are different scenarios. In the first one, we assume there are two groups, and there are eight clusters. These are the cluster means, and then for group one, there's zero weights on the first two, four clusters, and then equal weights. Four clusters and then equal weights on the last four. And for group two is the complete opposite. It's a complement. So these four clusters are of 25% weights, and then these four have zero weights in group two. So there's no overlapping clusters or items across these two groups. For the second case, we assume there are three groups, and there's a single common cluster across all three groups. I think it's this one here, plus the three, which I Here. Cluster 3, which has a mean 0, but has non-zero weights across all the three groups. For example, in cluster 1, it's unique to group 2 because in group 1 and 3, the weights are 0. And these are just the truths that we're assuming. For case 3, it's a more complex structure where it's kind of like a nested pattern of a cluster share. And the results are kind. And the results are kind of messy all over the places. So, again, we're comparing PAM, HDP, and our own model, PAM. And this is for case one. Case one, remember, is the case where there are eight clusters and four clusters in each one of these two groups, and there's no overlap. And then the bolded funds are the winning methods for that particular scenario, compared to the choose. Compare it to the shoes and power it reader. If you want to have a quick summary of the clustering performance, you can look at the adjusted random index or the orbits distance, which are underlying each one of these cases. The take-home summary is the proposed model is not too shabby. It likes to identify potentially non-overlapping clusters as that probably. Has that properties more than the other two methods by construction? So, why is this related to the theme of the day, interpretability? Because to me, I think we can, because our construction, we can assume zero probability for the weight of an atom. In the posterior inference, we can actually produce interpretable. Interpretable results where we can say, okay, in group one, for example, what is the probability that the second, the case atom, right, what are these? Oh, yes. So what is the probability that in the first group, in the first data set, the case atom, the case cluster is present, meaning the weights are greater than zero, but in the second group, Second group, the case atom has zero weight, meaning there's no cluster in the second data set. We can produce these results, posterior probabilities, and this is a unique feature because the other previous methods, they can't really produce that because they didn't define the zero as a possible sampling value in the sampling space. Simple, but I think. Simple, but I think it's kind of useful, especially when you actually care if something is present or not, rather than something has a very small weight. Kind of like qualitative versus quantitative differences. Some more simulation results to examine the different properties of these methodologies. These red lines are the truth. So you can assume each peak is a cluster, and then they're multi-peak. And then there are multiple peaks in a data set, meaning there are multiple clusters. And then the gray ones are the fitting. And then these three columns are different methods. This is the PEM about two years ago that was the paper, one paper published. This was the famous HDP, and this is ours. You can see that sometimes HDP has a hard time finding these fine-tuned peaks when it was applied to When it was applied to these random simulated data sets. So we apply this to, I mean, as I mentioned, this can be applied to many different cases as long as you want to generate clusters across data sets. So first one was taken from the PAM paper. The example was trying to cluster the microbiome expression. We're not trying to cluster microbiomes because they are the same microbiomes across the data set. Of the data set. Some expressed high or low, or some it did not express. So I just show the results. There are four subjects where their microbiome OTUs, the counts, are measured. And then the data found. So we clustered, we applied the PEM model on the four subjects. And then it turns out the model found eight clusters. The red ones are the clusters. The red ones are the common ones that are shared across all the four individuals. As you can see, the weights are non-zero. And there are two different types of populations, like subjects from rural Africa and then also subjects from the US. And then you can see the common ones, they are shared, meaning they're shared in a subset, not all of them. Shared in a subset, not all of them are shared. And then the purple ones is the unique ones, almost done. And the second application was on WARTS data sets where patients with WARTS are treated with two different therapies, immunotherapy and creother therapies. And we want to cluster, I think, I forgot how many subjects there are probably. There are probably about 30-ish subjects and grouped into two different, in terms of their therapies they received. And then we cluster based on their covariance, the patient's covariance. And again, this is the highlight of what we found through the modeling where again, this interpretable results is based on these weights being positive, meaning they are common between the two therapies, or one is zero. Or one is zero means it's unique to one of the two therapies. And there's some interesting results behind these unique ones where certain patients have certain characteristics in responding to one of the therapies. So if you want to insert a causal relationship, you can use these covariates as a prediction for future patients receiving a specific therapy. So I think that's my summary. So I think that's my some references. Thank you very much. One more quick one or two quick questions. How to generalize the high-dimensional it's a clustering problem, right? It's a clustering problem, right? And so you have cluster features or cluster covariates. In this example, there are four of them. If you have, I don't know, how many do you want? 1,000, 10, 1 million, right? It's going to be a dimensionality problem, right? It's a fight between the coverage size and the sample size. And one obvious, easy answer is to do dimension reduction. Reduction, right? First, do dimension reduction, and then you can use whatever TesNE or TCAs as your covariates to cluster. The type of applications where I didn't have a example, there's a different part. We apply to clinical trials, we actually use propensity scores. So that's another dimension reduction. We basically use propensity score as a probability of assigning a patient to one of the different trials, and that can be used as. And that can be used as a covariance to generate these. So the method itself doesn't really care the dimensionality of the covariance, but in practice, obviously, we do, right? And then currently the computing is based on MCMC, unfortunately, and therefore we need to be able to automate that. Yeah, I I it's possible. Uh but then it won't it wo it won't be this fully probability based uh model, right? Based modeling, right? Because right now it's just classical statistical models where you have data sets of these covariates, you can do dimension reduction however you want. After that, you feed them into the probability model. I think what you're referring to is kind of iterative process, where you want to optimize clusters, but at the same time, reducing the number of covariates. I think we should thank our speaker again. I think we should thank our speaker again.