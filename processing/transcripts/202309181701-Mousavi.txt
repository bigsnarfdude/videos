And yeah, this is a joint work with my ex-supervisor Zach. Okay, so I'm assuming that most of you guys know direct science your problem. You are given a directed graph. Squares are terminal nodes, non-terminal nodes. I showed them with like circles. We call them also signer nodes. And then H cost and a set of get. Cost and a set of terminals. So we want to find the minimum cost branching, like the BU said, such that every terminal is reachable from the throughout the talk, n will be the number of vertices and k is the number of terminals. And planar DSC is just the input graph is planar. So the DSC is a generalization of other combinatorial problems like on Great Design 3, group assignment 3, set cover. Set cover. So, this generalization comes at the cost of being very hard called. So, for undirected version, you have a good picture, but for directed version, there is an exponential gap between the lower bound and upper bound. And some tight result in the quasi-polynomial time regime, and a tight result for the quasi-pipal ch. Which means there is no edge between any two aspects. Means there is no edge between any two aside. Okay, so natural question is that in what family of graphs we can do better compared to the general graphs. So we partially answered this question. So there is a an off log K approximation for planar DSD. It's combinatorial, it's easy, and if I do a good job then I can convince you of that. I can convince you of that. And then we show that if you further restrict the plane RTSC to constant hypertension instances, we get constant hyper approximation. And this result is based on LP, like private-model algorithm, which bans the interactive gap of the card-based LP, and also extendable to like minor free graphs, for example, and more general framework graphs. Okay, so I'm Okay, so I'm gonna start with the first result. So it's based on Michel Thorpe separator, shortened spass separator, from more than two decades ago. So it's as follows. So we are given a planar graph on directed and root null. And it says that we can find three, up to three, the shortest path, rooted at R, such that if you remove them, each connected component has at most. Each connected component has at once and half minus distances. So the picture looks like that, but I draw it like in a directed setting. And why is that? It's because if you look at the statement of Ethereum carefully, it says that for any Spanish tree, you can find three nodes on the Spanish tree such that if we remove the path from the root to those three nodes, Then we get if we remove those paths, we get the desired security. So we can imagine that a spinal tree is like a shortest path, a spinal tree in a jerk tree. Okay. Trivially, so the cost of that separator is at most three times the furthest distance from good node. It works in the weighted setting, imparting. Weighted setting. In particular, I can find these three shortest die paths such that the weakly connected component, after removing these paths, each of them has at most k half monitor. Okay? And so similar separator was used by Vincent for unrated KMSD and isometry in like lanard graphs and minor three. Like lanar graphs and minor field graphs, and our result is inspired by that. Okay, so I guess now you guys know that where I'm going with this. So yeah, here's the algorithm, the slow version. We get optimal value. We can assume that by scaling up is at most n cube or something. And then we remove all the signal nodes that are further than opt away from R because they don't appear in optimal solution. Didn't appear in optimal solution. We compute the balance shortest path separator. So the picture looks like this. And then, so the green are the shortest path separator. Based on that, we compute the smaller subdistances. So we contract the shortest path to one single node R one and with one connected uh which we connected component forms a uh one substance. The one substance and with another connected component forms another substance and so on. Okay? And as I said, the cost of this separator is at most three times off because of the pre-processing step. Okay, then we solve each subinstance recursively. So this is the solutions for each of these subinstances, and then we merge them. And then we merge them together using the separator. Okay? As easy as that. And then one thing is that these sub-instances are HD structure. So the optimal value of this, the sum of the optimal values of these sub-instances is still at once ought. So I paid, the first time I applied the separator, I paid 30. And then for each of these subinstances, again, For each of these circumstances, again I am applying the separator. So, because of this inequality, for the next level of the equation, I was still paying 3R and so on. And each time I'm reducing the number of terminals by the constant factor. So, the depth of this equation is logarithmic. So, this is a off-log kipper situation. But it's not polynomial because the guessing part is it takes polynomial time. Takes polynomial time and the depth is not open. So it's part of what. And we can make it to run log n to log k by just guessing of approximately, but it's still not polynomial. Okay, so how do we get the polynomial to our algorithm? Well, we have to get rid of the guessing. So we let the algorithm approximately find the optimal value for us. So what do I mean by that? So let's say optila is an optimal value. So let's say optilda is an upper band on optimal value. Then this is the algorithm. So f is the algorithm, takes i as an input and this upper bound of tilde. So we do the pre-processing step, removing standard nodes that are up tilde away from the root loop, and then apply the separator result. But so what we pay here is three times optilda. But what if optilda is huge compared to odd? Is huge compared to odd, so we do another recursive graph. But this time at the same input, but we divide our upper bound by half. So if this is not a good progress, then at least that potential is a good progress. We are getting closer to the optimal value. And then for these sub instances, I'm going to start with the upper bound of the parent mode. I'm not going to really start with optical boundaries. That's crucial for the parent. That's crucial for the polynomial functory time. So the depth is again logarithmic based on k and n. And so the branching factor of this recursion tree, I say that is constant because, well, in this part, so we have like either we have opt build up or we get a bunch of subinstances. But if so we get opt to k mini subinstances, and then but if the subinstances are too small, But if the substance are too small, we can't merge them together to become big until we have like three substance denses. So the outbranching factor here is constant, depth is logarithmic, so polynomially many recursive calls. Okay, that's the analysis. Yeah, so if there is no question, I'll move on to the constant actual approximation. Oh, okay. So, before that, the extension. It trivally works for the node-based case, just running the shortest fastest separator based on the node weights. And also works for multi-rooted instances. I just should mention that in general graphs, we can reduce these problems to just the DSTV1 root. And edge rates, but it doesn't preserve the inability. So for the multi-routed case, we just extend the flow of separator to work in multi-routed case. But I'm not going to talk about that. Alright, so classify part of it. Plain RDs. So you get a constant factor approximation, and it's based on a modified primal dual analytics. So Pre-mode dual algorithm. So this is a code based LP. So for every stop versus that separates the terminal from the root node, there must be an agent trend. All right. So we know that we can run this solution of this LP for undritted graphs by using a two-factor. There are some lower bonds, like a kind. Lower bonds, like a kind of polynomial lower bond, in general graphs, and a tight result for classic bipartite graphs. Okay, so yeah, so it's based on pre-modual algorithm and some basics. So, pre-modual algorithm is usually the keep partial solution. So, I show it by Solution, so I show it by green edges. So these are partial solutions, it's not feasible. And each time we purchase new edges until we find the feasible solution. And then, so a violated set based on the green edges is a set that must have an edge entering it, but there is no green edge entering it. Okay? So, for example, this one is not a violent set, and then active sets are just. And then active sets are just the minimal violated sets. And then, so for this one on the right side, the green edge has cost 2, and then there are two dual variables or sets that contains the head, and their sum is equal to 2, then we say that that edge is tight. Okay, so then the pre-mondo algorithm, as Ishan explained, is so this is a general framework that we increase the active modes. Active modes until an edge goes tight, we purchase the edge, and then we do a post-processing of reverse delete of unnecessary edges. And then the idea is that if the total draw that we increase is comparable to the cost of the edges that we bought, if this comparison is constant, then we have a constant factor approximation. Okay. Is there a standard primary module or a The standard primordial dual algorithm. So it works for our SNS, a special case of DSD, where we don't have any STNI node. It solves the problem. It works in undirected network design problem. But for plain alcohol bipartite DST, there is a bad example. So let's go over this example. Example. At the beginning of the pre-module, every terminal, singleton terminal, is an active one. But these edges have cost zero, so they immediately buy these edges. So we have this picture. And then this dual increases its value from zero to one. And then all these green edges, downward edges, have cost one. So they become tight and equations. Okay? So you can see that now I read. So you can see that now I raised my duod by one dollar. I purchased four edges worth of four dollars. And these four can be extended, so I can extend this graph to the left and replace it by arbitrary number. So you can see that this is not comparable. The dual that we increased and the edges that we bought. Okay, so how do we fix it? How do we fix it? By defining different roles for edges. So these edges, like a standard mode to a terminal, we call it like antenna edges. And then the rest of the edges, we characterize them like this. So killer edge is an edge that if I purchase it, then it's gonna kill a mode. Like the mode becomes a map. And hence the name. And then the expansion edge is a And then the expansion edge is an edge that if I purchase it, then I have a new astronomy connector component, so the mode doesn't die, it just becomes big. So for example, this green edge, like the far left, is an killer edge for the bottom mode, but is an expansion edge for the blue mode. And then And then each active mode pays towards the corresponding bucket of an edge. So the blue mode pays towards the expansion bucket of that green edge and the bottom mode pays towards the expansion bucket. That's right. The other way around. And then the one of the buckets of that edge becomes full, then it purchases that. And if a bunch of them become tied at the same time, just break the ties arbitrarily by just Break the twice arbitrary by just being purchased in one of them. And then we have the standard delete reverse delete at the end. That's the algorithm. We might violate the dual constraint by a factor of two because this guy paid $1 to this green edge, but then this guy is going to come and again pay, but for the expansion, but For the expansion budget of this one. So, yeah, because for each H we have two buckets, we violate the dual constraint by the constant factor too. Okay, so is the algorithm kind of TV? So the analysis: first, because of the class of apartheidness, the structure of the actinos are kind of simple. So they consist of strongly connected. Consist of a strongly connected component part and then a bunch of antenna edges just interact. And then the active modes interact only on the assigned airports. They might like overlap on the sign modes. Okay, so again, as Ishan mentioned, so if we show that the average degree of each active modes Degree of each active mode at a fixed iteration is constant, then you are done. You have a constant factor approximation. And by average degree, I mean with respect to the edges that we purchase, I don't know, after the reverse degree. Okay, so we handle these cases separately. For antenna edges, so these guys, it's very easy. The degree of an active mode with respect to antenna edges is at most one. Is at most one. Because if I have, like here, for example, three antenna edges entering it, in the reverse delete, I'm gonna remove at least one. Because one is enough to reach all the terminals in this astronomy connector component. Because I'm buying these edges after I bought these edges. So in the reverse delete, I'm gonna see these edges first and then. Okay. Okay, so that's easy. So, but then what about killer edges and expansion edges? The claim is that if we bound the number of killer edges and expansion edges with respect to the number of active modes in a fixed situation, then we're done. It's not clear why, because for example, H over there can paste towards the degree of multiple active modes. So the number is. So the number is not enough, but then we apply the fact that planar graphs have bounded the constant average degree and every minor of the planar graphs also has a constant average degree. And then it finishes the dynamics. Okay? So, what about the number of killer ages? It's easy again to show that the total number of killer ages is at most of the total. The total number of killer edges is at most the total number of active modes because a killer edge kills an active mode, so you can charge it to that. So that's easy. The annoying part is the expansion edges. So and here's the very high-level idea of how do we take care of them. So we have like a tree structure, tree-like structure of active and inactive modes. Active and inactive modes and expansion edges. So in this trial structure, every leaf node is an active mode. And then let's look at, for example, U and it has like these are expansion edges, let's say. Then we show that either there is an active mode as an ancestor of U such that there is no expansion edge on this path. On this path. Or we show that there is an active mode as a descendant of U such that there is at most one expansion edge between U and that activity mode. So for example, here there is one expansion edge, here there is no expansion edge between you and that eigen. If we have that, paired with the fact that every leaf node is active more, then we're done. We can, by talking, We can, by token argument, we can show that the number of expansion agents is at most twice the number of active modes. Okay, so as far as I know, a p-task is open even for this problem. And even a constant factor for planar DSD, no classified partite method, is very tricky. is very interesting at least to make. And the integrality gap also for Planck graphs could be one. Thank you. That's my thought. I was just trying to track where you were using planarity. Is it just because it has average term degree? Exactly. And so any graph that has a constant average degree. A constant average degree, and every minor of that also has the same property, then it's enough. Other questions?