You get by Jensen's inequality that the average total entropy production is positive. On the ensemble level, such a relation has been known for a long time. Among others, I think Schnachenberg was the first to point out that the instantaneous rate of entropy reduction in such a system is always positive. That's easy to prove from this relation. That's easy to prove from these relations. Okay, so now we have identified the key thermodynamic concepts for such systems along individual trajectories. We need a second ingredient, and this is the second class of dynamics which we typically use in these systems. This is Langemin dynamics. So now we have a continuous degree of freedom, like a position. Like a position, for instance. So if you take a colloidal particle and you drive it with some non-conservative force across the periodic potential, you would describe the motion of such a particle by an overturned Langevin equation, where the velocity is given by the mobility μ times the force. Force can arise as gradient of the potential or as a non-conservative force, and then there is some noise. And then there is some noise. And the noise is white with the strength which is given by the Einstein relation. So the strength of the noise here is related to the mobility and the temperature. And again, as for this discrete case, you can ask, that's what Sekimoto originally did. What about the first law along such a trajectory? And it's easy to identify the respective terms. Applied work is forced time. Work is force times distance, total energy is position of the particle within the potential, and then you have the first law, the blue box, if you identify the heat as a difference, which you can then again identify with a stochastic increment of the entropy of the bath. Now, total entropy can also be quantified as a measure of Modified as a measure of the broken time reversal symmetry of non-equilibrium systems. So when you look at the time reverse trajectory, just at the movie being played backwards, it's an equivalent definition of total entropy production along a trajectory to say that it's given by the log ratio of the probability P of observing the original trajectory divided by the probability of observing the fictitious. Of observing the fictitious time reverse trajectory. And this is precisely the same definition as before. And as before, you have this integral relation for total entropy production. Okay, now I will mostly talk about non-equilibrium steady states. These are systems where the external driving is constant. And then there is the famous, in such systems, there is the famous fluctuation theorem, which is one of the Theorem, which is one of the original pillars of the field. So, this is a statement, a symmetry about the probability distribution of observing a certain amount of entropy production. It obeys this kind of boxed equation here. And here are distributions for this system, which we looked at some 15 years ago in Stuttgart with my colleague Clemens Beckinger. So, for instance, if you take the red history. if you take the red the red histogram here this is just the total entropy production sorry the total entropy production after two seconds this black distribution is the one after 20 seconds these are non-universal distributions but if you look at the ratio between P of minus divided by P of plus and you plot it plot it logarithmically you find in You find indeed this kind of linear relationship. And if you look at the slope as a function of time, you get this more or less constant behavior from the experimental data around one. Okay, so it's a nice relation, this boxed equation on the top here. But you could ask yourself, what have you learned when you have measured or verified or tested as it's sometimes called such? As it's sometimes called such a relation in your system. And a skeptical person would say you haven't learned much more than the fact that your system is well described by a Langeman equation or by a master equation. So in fact, it turns out it's more interesting to see that the symmetry is not obeyed in a system. And that brings me to my first biological example, because then you can learn something. So this is work which So, this is work which was done in Japan about 10 years ago. So, what they did is they took this F1 ATPA, this small molecular rotor, they fed it well with ATP so it should turn around, and they looked at the most simple model which could describe such a motion, which is a Langevin equation for rotary motion. So, left-hand side, gamma theta dot is the friction term, n is the Dot is the friction term, n is the torque, zeta is the noise, and then it's a triviality to find that the distribution for the angular distance delta theta obeys this kind of linear law if you plot it logarithmically. And here are the distributions, experimental distributions, and if they look at the logarithmic representation, they find a nice straight line. The problem was Straight line. The problem was that if they look at the slope, which should actually yield the torque n, that the slope was time-dependent, i.e., dependent on the time over which the data were allocated. So there was a problem which showed that this simple model is not quite right. So we tried to come up with a better model. And such a model, a better model needs at least two. Needs at least two ingredients. There is the small motor down here, which you can't see. I mean, this is a nanometer-sized object, you can't see that turnaround. That's why people attach this big probe particle to which they can apply a torque. So, if I map this rotary motion to a linear model, it looks like this. On the right-hand side, you see this blue dot. That's the molecular motor, which would jump 100. Motor which would jump 120 degrees with a concomitant hydrolysis of an ATP molecule. And then there is some link between the Bro particle, the big red sphere, and this motor. What you see is not the motor jumping, but what you see is the slower dynamics of this proparticle. So we need two equations. We need one for the proparticle, which is just the Langevin. For the probe particle, which is just the Langevin equation which we had before. And we describe the motor as a discrete system where we jump with W plus in the forward direction driven by ATP hydrolysis. And of course, there is a probability for having a jump against the hydrolysis direction with the torque. It's a general principle that whenever you allow in a system a forward jump force thermodynamic A forward jump for thermodynamic consistency, you have to allow for the backward jump, and the ratio, as I explained on the first transparency, the ratio is given by the free energy difference of these two states. Okay, so you take this model, you put it on the computer, you compare the probability distributions after a certain time with the experimental one, it fits well. You look at this pseudo-fluctuation theorem. Pseudo-fluctuation theorem type representation, and you find, like in the experiment, you find that the slope is time-dependent. What's the reason for the original failure? Well, the reason is that the system has two degrees of freedom, two relevant ones, the big sphere and the motor. And what you see in the experiment is only the motor, the sphere, sorry. So there is a hidden degree of freedom, which is the motor, and whenever you have hidden degree, And whenever you have hidden degrees of freedom, which operate on the same time scale as the observed one, and the observed one is in non-equilibrium, then the fluctuation theorem, if you just base entropy production on what you see, will not be obeyed. So, the interest, I mean, the point is that whenever you see the fluctuation theorem apparently violated, you learn that you see not all. That you see not all relevant slow degrees of freedom. I want to share with you a second experiment which was done with this molecule, and that's by Toyabe and co-workers at about the same time, also in Japan. And they wanted to infer the heat which is associated with that motion. And there's a general theoretical relation. Theoretical relation called the Harada-Sassa relation, which quantifies that heat, Q-dot, I mean it's the heat rate. It quantifies that heat through what you could call the violation of the fluctuation dissipation theorem. So you know in equilibrium the response function R and the correlation function C are related or they are identical if you do it right. If you do it right, so the integral would vanish. Harada and Cessa derive that the integral overall frequencies together with the mean velocity squared gets you the dissipated heat. And these are the experimental data here. The gray shaded area is the integrand of this fluctuation dissipation violation as a function of frequency. Okay, and with this data, Okay, and with this data, you can finally compare the heat and the work with the free energy difference of the delta mu of the ATP hydrolysis. And in experiment, this was done for various concentrations. And the basic finding was that this is a remarkably efficient motor. When we try to reproduce these. When we try to reproduce this data using our simple model, we found within the error bars quite good agreement. Okay, so what is the summary of this? The summary is that for this kind of systems, you can typically come up with simple models which reproduce experimental data well. But it's a model. I mean, our model has one or two parameters, in a sense, fit parameters. Parameters, in a sense, fit parameters like the molecular distances to the barrier and the like. So it would be nicer if we had tools which would allow model-free inference. And that will bring me to my next topic. But I want to motivate the basic question slightly differently. And that was actually my original motivation when we started this about six years ago. Six years ago. So, the question I wanted to understand, or the problem I wanted to understand, is quite simple. Suppose you have a watch or a clock. Now, this clock has to operate at finite temperature, 300K typically. We know that at 300k there are fluctuations. So, no clock will be infinitely precise. So, the question was, does a more precise clock A more precise clock needs more energy? Is there a relation between the position of such a clock, temporal position, and the energy consumption? And I'll give you here the result, which you don't have to understand if you see it for the first time. But it turns out that at 300 Kelvin, if you want a watch with the precision of one second per day, this watch will need at least six times 10 to the moment. 6 times 10 to the minus 11 should per day. And I guess there's no way how you could come up with such an estimate by yourself without doing a calculation. But that was the original question. So let me show how you get to this kind of statement and what you can do with the tools which have been developed for understanding these questions. So here is the first trivial calculation. This is work started with Andrew. Started with Andrei Barato, was posted with me then. So let's build a simple clock. A simple clock has a second hand, and whenever the clock makes a, let's say, step to the right, it advances along the face of the clock. And of course, in order to have an advancement to the right or clockwise and not counterclockwise. Clockwise and not counterclockwise, you need some energy, a battery, or in a chemical context, ATP hydrolysis. Okay, so how do we quantify that? After time t, this clock has made little n steps, and that's given by the difference of the rates times the time. As I said, at any finite temperature, there will be a variance of this output, which is given by the diffusion coefficient of the plot. By the diffusion coefficient of the process, which is k plus plus k minus. Now we can identify or define an uncertainty of that process, which is the variance divided by the output squared. The second player is the thermodynamic cost of the process, and that's given by the entropy production, right? Because the entropy production sigma is what you lose. And if you use And if you use the relations I gave you on one of the first slides, you'll find very easily that for such a process the thermodynamic cost is linear in time and it involves the two rates. And as I said, the ratio of the rates is not arbitrary, but it's related to the free energy difference that drives the process. So in this case, the free energy, the chemical Energy, the chemical potential difference for the ATP hydrolysis. Now everything is on a table. You just plug these relations together and you find that the cost times the uncertainty squared is given by a function, the blue function of the affinity. The affinity is the delta mu, and that function is bounded by 2 kBT. By 2 kBT. And that holds independent of the runtime. Okay, so this is the result of this very simple model. Now, the significance of this result is if you build a better watch or a better clock with some, let's say, feedback cycles or correction cycles, some internal error correction, any number of internal states. Of internal states, this red relation is still true. So we suspected this, we did a lot of numerics, Gingovich and Norowitz and Jeremy England proved that the cost of a process is at least as big as one over epsilon squared, which means that if you have a process which is one percent precise. Which is 1% precise, you have to invest at least 20,000 kBT. You can't beat that. It's the inevitable universal cost of temporal position within this model or within this big class, I shouldn't say model, within this big class of stationary Markov processes. And in fact, there is a slightly more general result which we will need. You can prove that for any current, and current is just any time asymmetric. Any time asymmetric, time extensive and asymmetric variable, i.e., any observable which is defined on the links ij and dij is anti-symmetric, nij is the number of steps. You can prove that entropy production is bounded from below by the average current squared divided by the variance of that current or the dispersion or the diffusion coefficient. Dispersion or the diffusion coefficient. This is all equivalent. Okay, so this is what is nowadays called the thermodynamic uncertainty relation. And my next slide, I want to show you what I think is perhaps most spectacular application of this relation, again using real data, because as you'll see it yields an estimate on the thermodynamic efficiency of such molecular motors. Of such molecular motors. So, again, I'm referring to experimental data. This is kinesine. Kinesine molecule here is walking along a microtubal. And in this experiment, a constant force was applied through this kind of laser feedback mechanism. So the kinesin runs against the constant force, but it's provided with ATP. And experimentalists measured the velocity. Measured the velocity, the fusion coeffant, and the ratio between these quantities, which is called a randomness parameter, L is the 8 nanometer staplen. And then they got data like this. So this is this randomness as a function of ATP concentration for different applied forces or at constant ATP concentration as a function of the applied load. So, what can we do with this data? We can now apply. We can now apply the thermodynamic uncertainty relation because it's a system in steady state. The model is a fixed concentration of ATP, a fixed force. So let's look at the efficiency of this process. What is efficiency? Efficiency is the output divided by the input. The output is force times velocity. Force times velocity, the input we don't know because we don't see how many ATP molecules per second the motor needs. But what we do know is that the entropy production is the difference between output and input. So we can replace the unknown input by the output plus the entropy production. And for the entropy production, we have the thermodynamic uncertainty relation, which Which, when we plug it in, means that eta is less than this right-hand side. And the key point is that the right-hand side now involves only experimentally measured quantities, like the velocity, like the diffusion coefficient. So here I'm now showing you the data I've just shown you, but I'm overlaying this function here. Function here. So this is the randomness, i.e., this diffusion coefficient divided by the velocity as a function of the applied load. The black dots are the experimental data. The colored lines are the right-hand side here. So let's take this data point here in the gray triangle, rectangle. So this was a data point taken at 2 piconewtons load. At 2 piconewtons load and with a certain randomness of 0.4. Our orange curve, perhaps you can't read it, but it says here 45%. So the orange curve means the upper bound here is 45%. So the statement we can make is that under these conditions, this particular motor was at most 45% efficient in transforming chemical. Efficient in transforming chemical energy into mechanical work against this applied force. And I think it's a remarkable statement because it's a statement which does not involve any assumptions about how that motor runs. You don't need to know how many ATP molecules it needs per step. You do not even need to know the chemical potential difference for the ATP hydrolysis under these conditions. Under these conditions. And that's, by the way, something which is often not easy to measure. So it's a completely model-free statement, and it's an upper bound. I mean, it doesn't mean that the motor is 45% efficient. It just means it can't be more than 45%. It could perhaps be 20%, or it could just be 5%, but it's certainly not more than 45%. Okay. Okay, this is one application of this thermodynamic uncertainty relation, which I kind of like most. So Udo. Yes, please. Yeah, Tom, I'll do a chair. Oh, Tom, yes. Yeah, I've seen this graph before, and I was obviously a thought just occurred to me, which is, obviously, you know, it's a bound, and it could. You know, it's a bound, and it could be less efficient than the bound. Do you think the shape is reflective of the actual shape of the efficiency or not? What is reflective of the actual shape? The shape of the curve. So the curve is saying, no, not necessarily. No, not necessarily. I mean, you know, typically. I mean, you know, typically, as you know, for a very small load, you could be close to quasi-static conditions, and then you would, for instance, have a high efficiency. No, it's just, I mean, the shape of this curve is just, is just a bound. I'm not claiming or saying that it's related to the actual efficiency. I mean, for our simple model, of course, we have looked at both. We have looked at the actual efficiency and we have looked at And we have looked at the shape of this curve. And there we saw that we had good agreement both in the quasi-static conditions, in the linear response regime, and under stall conditions. And in between, there were about 20% deviations from the efficiency. But of course, ours was a very simple model. If you had a very complex system, it could very well be that the bound is bad. And I mean, this is something we'll look at. And I mean, this is something we're looking at, and people are trying to see how tight this uncertainty relation is. I guess my prior would be something like: if you've got a tightly coupled cycle where you can't have futile cycles that burn ATP, then this curve is going to look very like the true curve. But if you have futile cycles, then it won't. Cycles, then it won't. Yeah, yeah, that's what I just said. I mean, for our simple model, which I had in the previous slides, this hybrid model with the this is this again is strongly coupled. And as I said, there we have at most in the intermediate force range, we have at most 20% deviations. Yeah, that makes sense. Okay, yeah, thanks. Thank you. Well, thanks so much, Tom, for interrupting. Excuse me, can I? Excuse me, can I maybe also throw in one question? Sorry for interrupting, but it is this figure. Any idea for a lower bound? You mean an even better bound? No, a lower bound instead of an upper bound. I mean, you know, the lower bound certainly is zero. The question is, is there? The question is: Is there a universal non-trivial lower bound? Yes. I'm not aware of it. But I would not exclude it either. Okay, thanks. Okay, good. Okay, further questions at this point? So I think there's a question in the chat from Daniel Polani who asked, why should there be anything above zero? Who asks, why should there be anything above zero? Above zero. Make the lower bound above zero. I mean, you can be as inefficient as you, you can make it as inefficient as you want, but don't you? That's what I'm saying, yes. So I mean, you see, it could be a bound if you have additional information. Of course, the universal upper bound is one, but you see, we have now a bound which is below one, an upper bound which is below one, but we have put in more information. In more information. The information is we have measured the velocity and the dispersion. So there is the possibility that you have a lower bound where you also enter such additional information. I mean, if you have no further information, of course, the lowest bound is zero. There will be no better one. So just maybe to explain my question, there is a strategy in computation and neuroscience. You look at the information. You look at the information transfer of neurons, and you can formulate an upper bound, and you can formulate a lower bound, and then you know in which range the true information rate will be. That is very hard to determine, you know, in terms of where my question came from. No, no, I agree. I think it's a sensible question, but one has to be aware that any such bound different from zero or one needs a different. Different from zero or one needs additional input from an experiment. Okay, so let me now, yeah, what I said is true for steady states. And then, of course, you could have the question, are there such relations beyond non-equilibrium steady states? And one big class is periodic driving with some period. Period, we knew from the very beginning that this tour fails in general. So if you have a periodic driving, you can have a higher precision at a lower cost. This is what we learned from this case study. But of course, it's a little bit unfair because creating a periodic driving, of course, has its own cost. And if you look at the super system where you include Super system where you include that, we are, of course, back at the ordinary S2. And then there's a couple of, I don't want to go through that in detail, there are a couple of further bounds, some of them are technical, some are operationally accessible. And you could even ask, let's give up this idea of Langevin equations or Markov on discrete state space. So you could look, for instance, at underdimensional. Could look, for instance, at underdent Langevin dynamics, then we also know that the tour fails for finite time. We believe that for infinity time limit, it should hold, but that's not proven yet. People have seen that if you have a magnetic field, it can be broken. You can ask, that's a little bit along the discussion we just had: what is the optimal observable? Which current would yield the best bound? Just to mention it, I mean, people are looking now at similar questions in open quantum systems. We have found recently, and I want to add this with two more slides here, we have found recently a relation which I think is now, in a sense, truly general because it holds for arbitrary time-dependent driving. So here the idea is that we have these states and we have rates, but we now allow the rates But we now allow the rates to depend on a driving protocol. And we assume that this driving protocol, this is something the experimentalist has in his or her hands, that it depends on a speed parameter, red V. So it means that if the experimentalist turns the knobs faster, V increases. And then he or she is supposed to observe the system for a t total time calligraphic T. Calligraphic T and then, for instance, observe a kind of current, mean current, which will depend on the time and which will depend on the speed. What we found recently is this kind of generalization of the tours. So on the left-hand side, you have the mean current J, the dispersion delta J, and DJ, and this new term delta J. And delta is an operator which is. Is an operator which is the derivative with respect to the speed parameter and to the observation time. And then it turns out that for any such process, any observation time, any protocol, this generalized tour will hold. Let me just give you one example where we see that you don't even have to look at currents. You can look at an observable. Let's say you ask. Let's say you ask in which state is the system at the final time, or more precisely, is it in state two? Let's do this for a two-state system. So here we have a two-state system. We raise one energy level as a function of time, we lower the other one, and we just ask, look at the observable little A, which is the indicator function of being in state two at the final time. Or we could look at the integrated time span. That's the integrated time spent in state two. So there's a question from the chat. Yeah. Wouldn't the total time t be coupled to the speed parameter v? Say it again, please. Wouldn't the total time t be coupled to the speed parameter v? Ah, if you reach the final value of the parameter, yes. Yes, but you don't have to do that. Okay. Right. I mean, yes, then they are coupled. Then they are coupled, but you don't, it's more general. You don't have to do that. You could increase the speed parameter, but you could look for an even longer time or for a shorter time. Is that clear? Yeah, makes sense. Okay. So these are observables. I should stress that. I should stress that this is what is called state observables. They are even under time reversal. So these are not currents. And for those, we also have this kind of relation, which does not have a counterpart in the non-equilibrium steady state situation. Okay, and here, for instance, you see now the quality of the bound in percent as a function of the observation time for the simple system. And if you look at And if you look at the blue curve, which is just the observable state 2 at the final time, yes or no, you can get 85% of the total entropy production of such a process, which I again find quite remarkable. Just by looking at such an time-even observable, you get information about the entropy production in a time-dependently driven system, and of course, entropy production is an odd quantity. Is an odd quantity. Okay, so this is, from my perspective, the kind of latest result concerning these thermodynamic uncertainty relations and their generalization to time-dependent quantities. Okay, Haman, doing with time? I think we still have a few minutes. I mean, we're right about time, but we have a fairly long discussion section, so we can always... Okay, okay, so let me give it, it won't take more than five minutes. I won't take more than five minutes for the rest. Okay, so I want to make a few remarks because I saw it's relevant for the further program as well about this issue of information and entropy production in cellular sensing. And we got into this basically about eight years ago, again, kind of asking a simple question. So here you see the interior of a bacterium, you see the outer Of a bacterium, you see the outside aqueous solution, and the bacterium wants to go where the food is. So, suppose it's not a spatial situation, but just a temporal one, so the food will, the concentration will change in time. So, from a conceptual point of view, there is an external process, that's the concentration of a nutrient, and the bacterium wants to take a record of that and store it in a kind of internal variable Y. A kind of internal variable y, which in reality then is the concentration of some phospholated proton protein. And the basic question you can ask is, is the rate of information which the E. coli acquires about this time-dependent external process X related in any way to the cost of maintaining the sensory system. I mean, it's a little bit like the question we had for the clock. Clock. And about this time, seven, eight years ago, this started a whole activity. I mean, not our question, but these key papers here from Lahn and Meta and Schwab. So we focused on this question I just formulated, and it turns out that there is a kind of general framework to address this, which are bipartite systems. So this is the stochastic thermodynamics of bipartite systems. What is bipartite in this context? What is bipartite in this context? We have an external world. This is the blue world. That's the water where the concentration of a nutrient changes. And that changes, let's say, stochastically with a certain rate and independently of the internal state of the E. coli, which is the Y variable. And what the Y variable tries to do is it tries to keep track with the different states of the external world. So in the simplest case, So, in the simplest case, we assume an overall non-equilibrium steady state. So, the total entropy reduction is the sum of these two processes, and the key quantity is the conditional Shannon entropy, which is the Shannon entropy of X given Y. So given Y means the Equoli knows Y and it wants to learn something about X. And we identified what we call the learning rate, which is the change in the Which is the change in the Shannon entropy of X due to the Y jumps. And it's not hard to prove that this learning rate is less than the entropy production associated with these Y jumps. And of course, this entropy production is less than the overall one. But due to this left inequality, you can meaningfully define the efficiency of learning or of sensing because that ratio is now bounded by one from. Is now bounded by one from above, so it qualifies as a decent efficiency. And as a side remark, I want to stress that the rate of mutual information is not bounded by the entropy production. Okay, now if you apply that to this sensing problem here, there are of course a number of players. There is the external concentration, there are then the ligands getting bound at a receptor, a receptor can be activated or not. Can be activated or not. There can be methylation, which will allow for adaptation. And then there is this internal variable, which is the key. I have a question. How come the learning rate and the mutual information seem to look at different things? Or what happens there? Yeah, they look at different things. I mean, my original guess was that the mutual information would be bounded, but it just turns out not to be. Bounded, but it just turns out not to be true. So, what's the learning rate instead? I mean, the mutual information is in the end what the organism is interested in, and that must be acquired by observing the environment. So, I mean, this end, but the entropy is simply your uncertainty about the state of the world. Yes, but you see, the state of Yes, but you see, the state of the world changes here, right? If the x changes and you want to keep track of the x changes with your y variable. And I mean, in a sense, it's a definition. It's the decrease of your uncertainty about the external world due to what you do with your inner drums. Okay. So perhaps it becomes more clear when I show you the first example. Okay. Yeah, let's go to that. Okay, let me jump over this. So here is the example. Look at this. Here is the external world jumps with the rate gamma between high and low concentration. And I'm just using one internal variable, which is the state of this Y protein could be phosphorylated or not. Are phosphorylated or not. And what you want is that if the external state is, external world is in this state of high concentration, you want the Y not to be phosphorylated. And when the external world jumps too low, you want the Y to jump to Y star. So basically, in such a four-state diagram, you want high probability here and high probability here, and essentially nothing on the diagonals. Diagonals. Because then by knowing y, you know x. This is the model. Now you make this model thermodynamic consistent, which means you have to introduce the delta mu of the phosphorylation. And then, for instance, if you look at the learning rate, it's just given in the simple case by this expression, trace the current along this cycle. You can calculate this learning rate as a function. This learning rate as a function of the delta mu. So it gets better the more you are in a non-equilibrium. And here this is a function of the switching rate between high and low. So let's look at the efficiency. So if the switching is low of the external world, then the internal variable has enough time to follow, in a sense, so the efficiency is actually. In a sense, so the efficiency is actually high. So you can't read it, but here is 95%. And if the external switching is too fast, the y variable cannot keep track, and then the efficiency of the learning is low. So like in heat engines or in the molecular motor case, you have an adiabatic case where the efficiency goes to one and fast external changes go to zero. Is it now more clear? Yes. Yes, yes, yes, I think I get that now. Yeah, okay, so yeah, just don't have to go. Yeah, sorry. The question, but I think that's probably something for discussion is actually about the mutual information. So learning rate, well, yeah, I think I get that one, but I feel that there's something I still don't understand. But we can discuss later. Okay, okay, okay. Okay, okay, okay. Yeah, this is my last technical slide. So, just basically advertising that paper where we did that. So, here we looked at a slightly more complicated model where we have the receptor being in two states, either active or inactive. We have methylation, which allows for adaptation. We have the external concentration changing. This is a 20-state mark. A 20-state model when we assume that binding is fast, and it's a 40-state model if we keep the binding variable as well. And then you can look again at these learning rates, at various types of entropy production. We don't have to go through this, at the role of matilation and how that allows for a consistent learning rate over a large range of concentrations, so here is five or six orders of magnitude. Five or six orders of magnitude. So, I guess we will hear more detailed talk about such models from Tom and others during the workshop. My point I just wanted to make is that with these bipartite systems and these, in my sense, clean definitions of quantities, we have a conceptual framework with which we can study these kinds of processes. Okay, so let me very briefly summarize. I hope I've convinced you that. Summarize. I hope I've convinced you that stochastic thermodynamics is a really nice tool for these kinds of systems. I've spent most of the time talking about such universal inequalities for thermodynamic inference, especially the Tour gives constraints on the cost of any process for a given position. I've shown you how to get bounds on the efficiency of molecular motors. We were able to generalize. We were able to generalize it quite recently to time-dependent driving. We talked a little bit about its bipartite approach to information processing. Let me mention and acknowledge the four co-workers I had in this field. Andrei is now faculty in Houston, Patrick Pitsonka is postdoc with my case in Cambridge. David Tartik is now at the MPI in Göttingen, and Timo Koyok is my current student working on the thermodynamic and CNC. Working on the thermodynamic uncertainty relation. So, thanks so much for listening so patiently to this presentation. I'm looking forward to the discussion. All right. Thank you very much, Udo, for that really interesting talk. So, I think for the open discussion, we've already kind of had a mixture of questions in chat and questions just directly from people unmuting and asking. I think we can more or less. I think we can more or less try to go along the lines of people can unmute themselves and ask. Unless we have everybody piling on at once, which case maybe we can kind of have people go back to chat. But yeah, so I'd like to kind of open the floor to discussion. Hi, this is Peter Thomas again. I've got a couple of questions about those last examples. Is the reason that the concentration is changing in time of the concentration of food because In time of the concentration of food, because the cell is navigating through a spatial environment where there's a gradient of food, for example, or does it not matter? That's the first of three questions. I don't know. Maybe I should. That's a very good question. The short answer is on this level yet, it doesn't matter. There are no spatial degrees of freedom here yet. Of course, one should now come up with a more detailed model where one has a whole concentration field. Has a whole concentration field, a spatial and perhaps time-dependent concentration field. But this is something we haven't done yet. So it's a kind of very coarse-grained, very coarse-grained picture of the effective external world. Okay, so for my second, my follow-up question, let's suppose that we did have a setup where there was a spatially distributed f food source. Source. Would you expect to be able to prove that a cell with a higher learning rate would necessarily be more successful at obtaining more food? That is to say, would optimization in terms of information processing offer any sort of absolute guarantee on optimization in terms of utility. Okay, I would be very. Okay, I would be very careful with absolute guarantee. No, I wouldn't want to speculate yet. I mean, this is very basic, very deep down. And as soon as you allow for spatial degrees of freedom, I mean, you have to generalize the stochastic thermodynamics to spatial degrees of freedom. This is what presumably Massimiliano will talk about tomorrow. So, you know, there's all. You know, there's there's a lot to do and I would I would be at present I would be a little bit careful with making such a such a prognosis. Okay, well I have a second follow-up and then I'll let other people ask their questions. Suppose that instead of just X, I had X1 and X2, that is to say two different types of food that I was interested in. How difficult would it be to generalize the framework to a vector of inputs? Of course, in the spatially distributed Inputs. Of course, in the spatially distributed case, we might make a decision between moving right and moving left. No, I don't think it's difficult that, in a sense, we did that because we generalized it at least for several internal variables. Of course, if you could have tripartite, right? Not bipartite and tripartite. So this actually This actually David Walbert did something on multipartite systems very recently. So from a conceptual level, this is not too difficult to do. Okay, thanks. Okay. So I think there's a follow-up question from Daniel. Daniel Palami the mic presumably was looking I was looking through one of the papers suggested and a lot lost last I wanted just to comment that there are certain limitations you can formulate for limited information intake so basically basically you you can sometimes say You can sometimes say how much value you can get for a given amount of information. Of course, the value can be arbitrarily high depending on the structure of your space, but there are certain limitations you can make. If you make bets, for example, then you can give an upper limit for the quality of your bets with respect to the environment that you live in, with respect to other players in that environment. And that is information theoretically limited. That is information theoretically limited. Okay. So there's a question from TM Haydauri. Would there be a balance between the gain and the cost? Balance between the gain and the cost? Well, I mean, coming back to the tour, the thermodynamic uncertainty relation, I mean, the gain is the precision, right? It depends on what you define. It depends on what you define as gain. So if the gain is the precision, yes, there was a bound on the product between cost and efficiency. So I do not precisely know what you mean by gain. Thank you. Basically, by gain, I mean anything that basically boosts the survival rate of. Basically, boost the survival rate of our organism. Okay, well, if you talk about survival rate, you see, you're on a bigger scale, you're on the level of the organism, you do presumably some evolutionary model. I'm very deep down on the kind of molecular level where all these quantities are very well defined. So, you know, we would have to integrate up to this survival. We would need a model. Survival, we would need a model for survival, right? Yes. Excellent. There's a question from Furshottam Dixed. So how difficult is this to generalize to non-stationary X of T? For example, the bacterium is swimming towards more food. Presumably, X of T will not be in a non-equilibrium stationary state, but will tend to increase. Yeah. Increase. Yeah, yeah. I mean, on this level, you could, for instance, assume that the concentration changes in a periodic fashion. That would be the kind of next step, right? So this is something you could do using this with these tools. Has David Walpot just tried to generalize in exactly that sort of direction in his later? Sort of direction in his latest paper on the archive. I think it literally has a sentence that's something along the lines of in our paper we don't assume a steady state input signal, something like that. It's less simple. The things you get out at the end are harder to interpret. Not that the and even the learning rate is not 100% unambiguous. The learning rate is not 100% unambiguous, right? So, um, but yeah, who is this guy? Sorry, I didn't catch the name. Uh, David Walpot, W-L-L-P-E-R-P. Okay, thank you. Yeah. I mean, you see, for the thermodynamic uncertainty relation, we now know how to deal with time with time dependence. May I ask a question here? question here um that that fits to this problem isn't it that you is the nature of the bipartite system not that that uh the nature or the environment does its thing independent of what the cell does is that correct no that's yes it's correct that we have made this assumption but bipartite just means that at any given time you either have a transition you either have a transition in Y or in X. And it's an additional issue to ask whether the X transitions are affected by the Y or not. So what I was introducing here was a particular case of bipartite systems where one part is not affected by the other. Yes. Yes. And if the organism or if If the organism or if the cell uses the information adhering towards the food, towards more food, then we would have a feedback, not even time-dependent signal, input signal. Yeah, signal that is affected by the perception of the cells. Yeah, that's correct. That's correct. That's correct. That's why it's getting more interesting or more complicated if you try to include that. Udu, I had a question, put it on the chat, but but and the question was whether there's more intuition about why the learning rate, as you define it, thermodynamic significance, not the mutual information, but there's also things like rate of transferentropy, directed information, that there's a lot of quantities that people have tried to define. Yeah, but you see, the rate of mutual information is a more delicate question. It's a more delicate quantity. It's not so easy to calculate. Yeah, that I understand. And also it has... Actually, as you might know, this is what we started with in our first paper. And then we realized that the rate of mutual information is also not bounded by the thermodynamics. Bounded by the thermodynamic entropy production. So that's more refined than the directed information or transfer, which is sort of a part of it that might be. So the rate of mutual information would, for example, decompose into the sum of two directed information rates from one to the other and back again. The other and back again. And so, yes, yes, this is what you could do. Yes. But we didn't, I mean, you will not be able to prove a bound between entropy, I mean, thermodynamic entropy production and weight of mutual information. This is what we found. I mean, this was, and from that perspective, from a thermodynamic point of view, it shows you that this weight of mutual information. You that this rate of mutual information doesn't fit well into such a scheme. I mean, it may have other advantages, but I definitely wanted a quantity where the ratio is bounded by one. Otherwise, it's not a decent efficiency from my perspective. As if fundamentally, you get information between two degrees of freedom in an equilibrium system, where there's no entropy generated. Entropy generated. And that's what Udo's saying, there's no lower bound because you can just do it with zero. What the learning rate, I feel, is kind of telling you is how much you're having to update your information over time because you're lagging behind a signal that you're trying to. And that's an interesting quantity, but it's certainly not the same thing. But it's certainly not the same thing as how much you know about the signal instantaneously. And you can see that because lagging behind a signal that's driving you, you can see why that is something that's going to cause entropy generation. So the more you're lagging behind a signal that's generating you, the higher you're driving you, the higher your learning rate, and also the higher your entropy generation. entropy generation in a very hand-wavy level you mean to keep up to keep to keep up with the changing signal yeah yeah yeah yeah no i agree tom certainly yeah so um we're kind of coming close to the end of the discussion section so maybe a couple more questions and then they'll be we'll take a short break and um at uh in about 10 minutes or so at 1125 eastern time uh so Eastern time. So 9:25 Mountain Time, we'll have the kind of group photo through Zoom. So maybe there was some people had hands up. Ilka Birchovs. Thank you for having the opportunity to ask a question. So I have a biologic question, Uru. It was unclear to me in your model about the learning about the environment, which models kind of a two-component system where you have signal transduction using also. Where you have signal transduction using also phosphorylation, of whether that phosphorylation cycle was crucial to your results. And related to this, in these bacterial systems, you have a lot of information processing simply done by receptors, which just kind of bind signals and then change their state, right? And so I would ask what would change, if any, whether there are any changes to your framework if you consider those simpler systems which don't have. Those simpler systems which don't have any ATP hydrolysis. I mean, nice to hear you after all these years again. It's yes, you can, of course, as Tom just said, I mean, you can have correlations in equilibrium, right? So you can have that, okay, sorry for saying it again, you can have correlation. Okay, sorry for saying it again. You can have correlations in equilibrium. Who is that? Sorry, I had a strange noise here. You can have correlations in equilibrium and in our more detailed model. Let's see. Do I still share the screen? Yes, I guess so, right? Ilka, do you still see that screen? Yes, I do. Okay, good. So look, here we have a model. Let's see, where is the strange thing? Here we have a model where we have the receptor in equilibrium. So that does not involve the phosphorylation. So you can, in the paper, you'll find this kind of model without. You'll find this kind of model without first correlation, and you also get the information. Just due to, in a sense, due to equilibrium correlations. And so what is then kind of, do you see then kind of the advantage of having the two component system, whether it's the one component system? And is that related to energy consumption? Yes, let's see. That's a good point. Um yeah, I have to think about it again. This was quite some time. I see your point, Elka. It's a perfectly valid point. I guess for some of the things you really need the you really need the free energy input. The free energy input. Now, in this sensing problem, you could get information without the input. Yes, that's true. Without the free energy input, that's true. I guess the question would be of whether the energy input is then providing any benefit, right? Which I guess is the point of view of bacterial signal transfer. Right, right. Yeah. So this question has been like Peter Reintern Walder has written loads of papers on it and some other people. It definitely is going to come up in my talk, but it definitely allows you to do things. It allows you to do things like break time reversal symmetry, which allows you to time integrate, for example, which the picture, this flagella motor circuit. Motor circuit is doing. It's doing some kind of time integration. And it allows you to do things like signal splitting, signal amplification that is much harder to do in an equilibrium binding and binding system. So it definitely allows you to do things. But yeah. Yeah, I mean, you certainly need it for any kind of directed motion. Of directed motion. That's clear. Tom, are there kind of general statements in this general bounds relating the non-equilibrium aspect to the equilibrium aspect? So, yeah, well, so this is exactly Peter Ryan wrote a couple of papers, I guess, 2014-2015. I could share them in the chat. 15, or I could share them in the chat, which were exactly getting into this question of what's the difference between equilibrium and non-equilibrium sensing, and why would you do non-equilibrium sensing? And indeed, basically, his argument was in his particular context, it made sense to do non-equilibrium sensing when it was useful to time integrate a signal in order to average and reduce noise. But there's also other reasons. So, Pancage. Reasons. So, Pankaj Mehta and David Schwab wrote a paper, a review called something like Landauer in the Age of Synthetic Biology. But part of what they do in there is they basically say, when is it worth having signaling systems that are pre-energy consuming, like a phosphorylation cycle, rather than just equilibrium? I bind to my downstream partner and I stay bound. And there, and a couple of things that you And there, and a couple of things they say is that it's useful if you want to amplify a signal because you can turn on, I don't know, a hundred different downstream proteins with one upstream protein, which you can't do if you're just reversibly binding. Because as soon as you come off, then the thing turned off again. I can, I mean, yeah, I'll share all these papers in the chat, but definitely it's it's there's you open a whole host of functionalities that you don't have otherwise. That you don't have otherwise. Everyone's able to see the chat, right? I'm not just sharing these papers to no one. Yeah, so I think at this point, maybe we can take a few minutes. So maybe like, you know, four minutes just for people to have a break before the next talk. So we'll reconvene in four minutes. In four minutes, and have the group photo, and then we'll move on to Sarah's talk. 