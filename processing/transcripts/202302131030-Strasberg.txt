We have three or four one-hour talks and the rest of half-hour talks, so we decided to do a few one-hour talks that include a survey of the field. And these are given usually in the mornings, as you see in the program. The first one is by Philippe, and I see with a lot of collaborators. So that's the nature of survey, I guess. So you know, you will explain to us a little bit about the field. Okay, Finik, please go ahead. Thank you. Go ahead, thank you. Yeah, thank you. Welcome, everyone. Thanks to the organizers for putting all the work done to get these things organized. Very nice. And also, I feel very honored to give this overview longer talk in the morning. I hope I will do well. So, what I want to address is kind of the holy trinity of three properties: classicality, my Classicality, Macobianity, and local detail balance. If you don't know what local detailed balance means, for the now, just think about consistent thermodynamics. And I want to present today a very different perspective on the matter, coming from, say, many-body isolated quantum systems prepared in a pure state and evolving unitarily. And in that perspective, I think it's really. That perspective, I think it's really different. We will use completely different tools than what people in open corner system theory are used to. But I will also show briefly how you can connect things to open corner system theory. And I hope you enjoy this kind of different perspective on the matter. And yeah, there are a lot of collaborators because I've been thinking about these topics more or less broadly for three or four years. The most important work, which I've done today, was done. Important work which I was done today was done together with the gang here in the first line in Italic. But I thought it's not a bad idea to rather thank a few more people than thank less people. So all those people who are influencing at least the work. Let's get started by telling you the outline. I have two parts. Part one is basically some background information. Where I present what van Kampen already knew. So I think you all have heard probably of van Kampen. He died 10 years ago. And he has written, in my view, a very nice paper, 1954, which I think deserves a bit more attention. And I will basically review the two main results or insights he presents in this paper. But you will also see that. But you will also see there are some open questions, and basically in part two, it's a bit more some recent work, mostly about these two papers, what we basically know now, and what we have made some progress, I would say. For those of us who don't know who Ben Kapman was, who was he? Well, he was a human. So he has this very famous book on stochastic causes. Famous book on stochastic processes. This is probably his most known thing, but he has worked a lot on stochastic processes, quantum statistical mechanics, all these kind of cumulant expansion things, which you might have heard in terms of master equations and all that. It's partially due to him. He also had a very clear view on what quantum mechanics is supposed to tell you and other things. Things. And he was an uncle of a guy, I can never pronounce Gerard, but the Nobel Prize for high energy physics. I don't know more about him. I never met him. Anybody met Van Kampen? Alright. But let's see. He's a local hero basically of this talk. Let's see what what you already knew. Talk, let's see what you already knew. But before I tell you that, just the overall setting for this talk is the following. So we consider an isolated corner system, right? There's no bars, no environment. But of course, you can think about the system and the bars as an isolated corner system. Everything is prepared in a pure initial state, and I restrict things to some initial energy shell around some energy E, because energy is strictly conserved. There's no driving, so we can. Driving, so we can do that. And then we're only interested in asking questions with respect to what I call a coarse graining. And if you knew statistical mechanics, that should be pretty familiar to you. But I just want to briefly repeat what do I mean by a coarse graining? Well, it's a set of projectors, and this set is complete, and the projectors are orthogonal, so they span the entire space, and that's the orthogonality condition. So basically, that divides a Hilbert space into That divides a Hilbert space into additively into subspaces, right? And you can associate basically to each subspace, like the subspace dimension, or I call it some Boltzmann volume term. So it's the size of the subspace. It's at least one, but typically in statistical mechanics, it's much, much, much, much larger than one. So your subspaces are huge because you look at the system from a really coarse perspective. Another way to think about it is in terms of an observer. Another way to think about it is in terms of an observable. So, any emission operator defines basically a cost graining by just looking at its eigen projections. But in principle, you know, eigenvalues don't really matter to define this cost graining, so they have in principle many observables with respect, you know, you can associate to one cost graining. Is the set actually final? Or do you have do you choose allow internet sets for cost gradients as well? Just curious the setup for. Just currently set up all of the. No, that's that even that even makes an agreement for now that your Hilbert space dimension is finite. Might be as huge as you want, but everything is finite. And yeah, so I will basically switch between these perspectives, talking about a cost grading one observable. I hope that's that's okay. Okay, now I will come to von Kamen's first insight or result. And for that, actually, I have to break it. And for that actually I have to break a little bit with the idea of having any fewer states because I will present to you a mixed state on the next slide. But don't worry, I will make up for that and explain how this can arrive at the end. So for now just look at the following thing. Namely van Gampen used the condition that he calls a repeated randomness assumption, and which says the following. If the state of your isolated system If the state of your isolated system is approximately for most times of this form, so what is this form? Well, you have some probabilities to be in one of these coarse-grained Merkel states X. And then you have a convex combination of these states conditioned on being in X. And what is this state? Well, this is a projector which just projects from the subspace and you normalize with its volume. This is basically given some non-equilibrium constraint in terms of these probabilities. Constraint in terms of these probabilities you apply, then the equal a priori probability postulate to all the remaining degrees of freedom, right? That's like a microcanonical state, you could say, generalized one. And he said, well, if this is more or less approximately true for all times, then the dynamics which comes out is classical Macrobial based local balance, and I will define what I mean with these terms on on the next slides. Terms on the next slides. I just wanted to know a bit who of you has heard about repeated renderness assumption, this specific terminology before? Please raise your hand. I've heard a paper of yours. Okay, that doesn't count. Before twenty twenty two. Okay, now so yeah. Some people used it, I would say, mostly in the StartNIC community, but it didn't become um Didn't become a common terminology, but we will connect it to other things that you know for sure in a few slides. So, what do I mean by classicality? So, that will be an essential and important definition here. And maybe before I talk about it, I should say there are many ways to distinguish a quantum in the classical world, and there's no one-size-fits-all definition. And I'm mostly interested in And I'm mostly interested in asking a question: okay, how can you explain the emergence of classical dynamics from underlying quantum description? And I think for that, this is a good definition. So how can you think about it? Well, imagine your favorite experiment you've ever read about. And then I'm sure this experiment involves the gathering of a lot of data and then, I don't know, repeating the experiment many times and forming histograms and probabilities and all of that. Probabilities and all of that. So, suppose you have done that, and then you ask the question: okay, can I explain the data at least in principle with some classical theory? Okay, that's the question you ask. And the answer to that question is yes, if the measurement statistics you got can be reproduced at least in principle by some classical stochastic process, okay? Stochastic process. And that process might be fictitious, that might not be the real thing going on, but if you have like a black box mentality where you say, I don't know anything else about my setup, I just got the measurement data. Can I explain them classically? Then, well, then it's possible if you can do this with a classical stochastic process, which I will define more thoroughly on the next slide. Yeah, I just want to mention that I think in recent years this idea got a bit more Got a bit more attention. For instance, I think Andrea was involved in two works, and he will probably very precisely talk about this this afternoon. I did a bit on that. And there's also a lot on, maybe I've heard about consistent or decoying histories, which has a kind of similar flavor, where things like that are, you know, are. Okay, what's the classical storastic process? Probably you know that, but just to repeat. You know that, but just to repeat. So, you have some system, you observe it at multiple times, you know, you collect all the data, you do histograms, and you get a joint probability for this multi-time distribution of measuring x1 at time t1, x2 at time t2, x3 at time t3, up to xn at time tn. And then we all know every probability has to be normalized and non-negative. Okay, that's clear. But then you use one more LED property or you use the normal normal. Basically, the property, or you impose one more property, which really defines a clinical stochastic process. And that property is known as the Komagorov consistency condition, or more plainly speaking, the probability sum rule. And that's important to understand, so let's go through it in detail. So here, this is a probability to measure x1 at time t1, and so on and so forth, up to xn at time tn. So you're doing total n measurement, that's what this. So you do a total n measurement, that's a subscript n. And then you take this object and you marginalize the average over xl. And then you ask whether this is identical to the object on the right-hand side. And that object is constructed by measuring x1 at time t1 and basically the same as over here, just with one difference. And I try to indicate this with this kind of slash-out notation, namely that you don't. Namely, that you don't measure at time TL. So you ask the question whether not measuring is identical to measuring and averaging. And we all know quantum mechanically, this doesn't have to be the same. Quantum mechanically, we add up amplitudes and then compute a final probability and not adding up probabilities separately. So this condition can clearly be broken. This condition can clearly be broken, and here is a famous double-stit experiment which clearly shows it how to break this condition, which you all know very well. So say starting here on site B, you have a coherent source of particles. It passes the slit, but you're not really measuring where it passes through. You just measure the final position. You get this nice interference pattern if you do things right. And then there are all these endless debates with, you know, between ISIS. Endless debates with you know between Einstein, Bohr, others. And people ask, okay, but suppose I could kind of find out where the particle goes through. So I do a measurement at a previous time on this earlier position x1 of the particle. And then whatever you do, once you do this measurement, even if you average out x1, at the end you get a very different distribution for x2 on your screen. So these two probabilities are clearly completely different. Completely different. We have a clear violation. And whenever I talk about classicality now in my talk, I mean this kind of notion. Maybe it's good to briefly ask whether you understood it. And as I said, there are different ways to define classicality. I'm not saying that this is the only way, just the one I use for this talk. This one query. Um obviously classical systems can still replicate that sort of behaviour, provided they have some sort of learning. Provided they have some sort of penny. So you can always build classical finite state machines that violate the colour consistency conditions by assuming that the actions affect the state in some way. So all statistics can still be replicated by a classical system. So could I clarify that what was meant before was that they can't be replicated by assuming that the classical system does not have to be a system. Assuming that the classical system doesn't, it can only be replicated if the classical system changes in some way when you make measurements. Yeah. So I mean memory is not the issue here. This is completely non-Macovian, if you like. The point is, exactly, also classically, you cannot force change your process. And the simplest example is if you do feedback control. So you use a certain measurement to actually then condition your future feedback. That's what happens about memory. Now, what I want to say, so what I have here in mind is a completely isolated system, there is no external Maxwell daemon. And then, if you would look at this classically, where you replace unitary evolution by a Hamiltonian evolution, and projectors are like characteristic functions on phase space, then you would always find this okay. I thought what Mila was saying is that you could classically have a system where you're measured using phase commodity systems. Exactly. What I'm just saying is classically, there could be invasive things. But here, I'm like in this idle. I mean, you can go back basically and replace all these things with classical things, idle classical system point and phase space, characteristics function and phase space, and then it will always be obeyed. Right? Of course, if you try start messing around But so um but like i if you're saying that saying it's classical, if you can come up with a classical model to describe it. With a classical model to describe it. Does that count? Like if you have invasive measurements or something? No, for me, it doesn't count because I want to answer the question, I can come up with a microscopic fundamental classical model which obeys that. If I can add just because if you love for invasiveness at the classical level, then what does classical mean? Right. So well if you love for any kind of invasiveness, personal, local one, then you can reproduce everything auto-automatically. So there will be border between classical and non-classicality. Well, that's easy because we already agreed on that things are classical, so I can just use a standard definition of Markovianity for these conditional probabilities. Uh who has seen that definition? Uh who has seen that definition? Please raise your arm. All right, okay. One person is writing there. So I won't comment on that. I want to briefly mention the condition of local heated balance. I will not talk so much about thermodynamics in this talk, but I think it's nice, it nicely completes the picture, and I think there are some misconceptions about in the literature. So it's good to review it what it really. Review it what it really means. So now you consider this conditional transition probability. You go from a state, from a cost-brained state x prime to x, okay? And you divide by the inverse conditional probability from x to x prime, and I have a tr in front of that. Anybody can guess what tr stands for? I'm reversal. Right, great. And then magically, you know. Magically, you know, this probability itself might depend on all kinds of microscopic details, might be super complicated, but the ratio of them, at least local data balance tells you, is very simple and only depends on these Boltzmann volume terms. And if you remember, Boltzmann's definition of entropy is basically the exponential of change in entropy. And what does it tell you? Well, it basically tells you that this classical Markovian process evolves in a way that it tries to maximize. Way that it tries to maximize entropy along its dynamics. So, on average, it always moves in the entropy along the entropy gradient. And you can make this basically rigorous by showing that this basically implies that you have a non-negative second law at each time step. So the time rigorous of thermal entropy is always non-negative. Okay? That's the meaning of local balance. Local heated balance. And you might not have seen that in this general form. So I want to make on the next slide the connection to open quantum system theory. And I want to do this within the typical paradigm where we assume to have a very small system but a very large bars, which is basically not changing from a macroscopic point of view. If you're interested in how to spell describe finite bars or more general explanation, I'll show you that. Explanation: What I show you. There are two papers. There's one tutorial about the second law and how to connect Boltzmann entropy to quantum thermodynamics and all that. There's one paper focused on a master equation description for finite bars where temperatures change in time. And if you have a lot of time, maybe after your retirement or so, you can also look at the book I've published where everything is a bit more detailed. So, the first thing is very easy. It's an isolated system with a homotopy. Thing is very easy. It's an isolated system with a Hamiltonian H. In system bus here, we have HS plus HB plus an indirection V. That's not the volume, that's an action-direction Hamiltonian. And then as a cost training, what I suggest is very meaningful, was to say, okay, first of all, you choose some cost grain for your system, and because it's very small, you assume it to have enough perfect control. You can basically choose any kind of project on an arbitrary system, basically. So measure your spin and sign. system basically. So measure your spin in sigma x or sigma y, whatever you want. And now people often say, well, and we don't assume anything about the bars in open quantum system theory, but that's not true. At least the temperature should be known of the bars. So the projector to that is to say, okay, you know, I measure in a causeway with some measurement uncertainty the energy of the bars. So I project on some microcarried energy shell. And we all know, of course, that energy E is basically All know, of course, that energy E is basically equivalent to some inverse temperature beta of the bars, according to that relation. So that's a cost-winning, you know, the perspective you have an open system theory. Now, what is a repeated randomness assumption? Well, it's exactly the Born approximation. It's the idea to say for all times t, it should be more or less okay to say that you have some system state at time t, and this is decorrelated from a bar. Decorrelated from a fast in thermal equilibrium. So that means at the initial stage you also start disentangled. Yes. Or maybe you're not disentangled, but at least you can describe whatever you're interested in from that perspective. And then what you get out is a classical Markov process and located the balance reduces to the fact that the transition probability. That the transition probability for some, you know, for your system being in S prime and its energy eigenstate with ES prime and its energy eigenvalue to ES divided by the reverse one is given by this kind of Boltzmann factor. But that one really follows from the general definition I put on the slide before, basically just by Taylor expansion. So that's Let's just be. Okay, so I hope this kind of connects what I said so far about isolated system to the open system paradigm. Now, you know, as I said, we want to do, you know, unitary dynamics of an isolated system. And some of you might have already noticed that this repeated randomness assumption or Born approximation is a bit fishy, not really good. Sorry, can I ask again, w what is assumed about the initial condition of this volume dynamical browser? Is that contained in there somehow? Well, yeah, I mean, you know, you can choose initially, you know, you look at your system from some basis. Could be the eigenbase of your density matrix if you want. So you can choose any system state, if you like. And then what you say, okay, about the bars, what I know is only roughly its energy E. Energy E or equivalently its temperature beta. So if you only know this, what should you assume? This state. I'm not saying that the true microscopic state must be identical to that. But this is from your cost perspective, that's what you associate to your information that you have. Knowledge of the temperature of the rubber beauty identified, like HP. Well as I said, so you measure energy but you know up to some cost graining. So you're not I don't assume that you measure a microscopic energy instead of Hp. There can be like an uncertainty which is you know for your because any measurement instrument has some kind of error and a large bar says like an exponentially dense spectrum and you don't have to distinguish between energy eigenstates, you know. I guess this is my question is you're actually here measuring energy of Since you are actually here measuring, partially measuring the direct through the yeah, it's fine, but I mean, as I said, it's fine, you just have a semometer there somewhere in your lab. Okay, you know that the environment in your lab has a certain temperature and that's it. That's equivalent to basically having knowledge about some energy for large normal bars because coolness of ensembles and all that. Yeah? Okay, cool. Um so now, okay, what's the big problem with this kind of reasoning? Well, it breaks macroscopic reversibility or unitarity, right? So for instance, the fundamental entropy of this state evaluated at different times is in general different, so there cannot be any unitary mapping between them. And the same for this repeated randomness assumption. So it is really a blatant contradiction to Blatten contradiction to the underlying quantum dynamics. And now comes the second insight of Van Kampen. He said the following. Well, this repeated randomness assumption should be effectively justified if three conditions are met. And what does effectively justify mean? Well, it means that it's justified as long as you only ask questions about the cost grinning you have specified. About the core screening you have specified. If you ask questions beyond, like you switch the observable, you suddenly want to know the microscopic state of this isolated system, then that might not be the right answer. But as long as you only focus on your core screening, then this repeated randomness assumption should be a good assumption for three conditions. First, the isolated system should be non integrable. I don't want to define what this means, just think about it's hard to solve. Just think about it, it's hard to solve. Your variable x should be caused. What does it mean? Well, x, say, as capital M different eigenvalues or measurement outcomes, okay? And you demand that m is much, much, much smaller than the dimension of the Hilbert space. And that's certainly very well satisfied for any realistic many-body system, any realistic measurement. And the first assumption is also very well. Assumption is also very well justified for everything that we have in nature. But then comes the third assumption, and that's really crucial and very subtle. And he says, well, X has to be slow. What does it mean? As I said, it's very subtle and hard to get these things rigorous. But I have two different points of view here. The first thing is the following. So tau x is a characteristic evolution time scale of your observable. What is that? Suppose that you plot your observable as a function of t. Plot your observable as a function of t, you know, then it changes, you know, your plot, you know, when it starts to change, you look at the time interval. What's the time interval? Okay, that's the characteristic evolution time scale, roughly speaking. And that should be much larger than h bar or delta E. What is delta E? Delta E is basically the spread and energy or variance of your wave function or the widths of the microcanonical energy shell. Then h bar over delta E might ring a bell because that Because that's really a microscopic evolution time scale of an isolated quantum system. That's the time that is needed to map one state in Hilbert space to some obsorbal state. That's basically the quantum speed limit, if you want. And you demand that your observable, your cost observables change on a much slower time scale than microscopic processes going on in your eye status. If you don't like this characterization, let's look at that. Let's look at that. Assume you write down x as a matrix and you do this in an ordered energy eigen basis, so you have ordered energies increasingly, then a slow observable is a very narrowly banded observable. So all off-diagonal elements quickly fall off to basically zero. That's what a slow observable means. How how should I think of this in the user? In the previous slide, you have the system. When I say here I want to take my constraint to the surround of the system. How should I think of this in terms of question? So let's assume we are recoupling and you look at the local system energy. Then that's a slow observable. Why? Because it commutes with its own local Hamiltonian and changes in it are only induced by this weak coupling to the bars. By this weak coupling to the bars. But for instance, not as slow observables, if you look at some coherent bases, where you have coherences already evolving very quickly due to their own Hamiltonian HS. So you'll need some decoupling between the system and the environment to be fairly well. If you want to apply it to open system theory, you typically need this. But the point is, there are other examples where I mean there's not even any system bars. There's not even any system bars division. So, typically, you would, for instance, say collective observables, which are like sums of local observables, like the total magnetization of some spin chain, is typically also slow because it takes a long time to change the global magnetization of some chunk of matter. Right, so basically, stuff that's pretty close to our Hamiltonian. Because if you have the proper Hamiltonian, it doesn't evolve in time. So, you want It doesn't evolve in time. So you want stuff that looks kind of close. You want observables that's this diagonal. That's a good idea. Yeah, is that because our diagonal terms are very small, your Homatonians can play. So you measured phases have very low coherence with respect to the energy basis. Is that a way of understanding that? No, that is not true. So it does not mean that you have low coherence. And actually, you can have maximum coherence, and you will need a nice little. And you will need an isolated system to actually change from one space to another. But you want, say, if you define closeness in terms of this picture, yes. You want closeness in the sense that you have a narrow debanded matrix. But the point is that there are still observables, so it's not always possible to identify a simple perturbative parameter in your home. So these two notions are not the same. Those two? Yeah. They are. I'll take that. They are. Yeah. Yeah. So if you demand that for all possible initial states things should change on a time scale much larger than that, this basically translates to demanding that all the off-diagonal elements are very small. Now van Kanten said that this should be the case and I think he had the right picture but he was not really able to give any kind of mathematical arguments and for sure not any kind of numerical arguments Any kind of empirical evidence for that. So you seem to be a bit frustrated by that, which you can see from that quote if you want to read that. So that's maybe almost, I think, the last sentence in his famous book. And the statement he means is, of course, is kind of repealed when it's assumption. He seemed to be kind of frustrated that he couldn't really. Of frustrated that he couldn't really prove it or identically derive it in a better way. Although he was completely convinced that this is the truth. I just had a I could sort of like more or less like a raw roster and Recoupling you cannot hold where you can touch you mean because of the cur recurrences? Well, I mean I can imagine that I'm recoupling a solution, right, that I can which means like eventually it it will just crash now. No longer predict. No, I mean okay, the seven sub so you know, when when you ask these questions about these constant thermals, you must ask basically questions about how things evolve on the constant time scale, okay. Was on the cost time scale. And what this is saying is that at least for almost all times, like putting away Ponka reoccurrences, maybe where you just suddenly recohere to your initial state after an astronomically long time. Then this says this should be a good assumption for the entire time script to describe everything. For all the time, say you are reasonably interested in a human, that's very lengthy, say. Human, that's better enough. Okay. Okay, I was just wondering, so this when you say that it's effectively justified, when you're looking at objects like local observables or correlation functions in time, right? But this type of structure, for instance, if you look at higher-order correlation functions, would not give you the correct dynamics. The correct dynamics, right? Because there are certain other assumptions that you have to make about those matrix elements. No, the important point is that even if you get super high-order correlation functions, it should be justified as long as you only construct them from the core screening I have put there. If you consider another observable, suddenly this might not be justified. But you know, the point is, though, yeah. So you're talking about hybrid recognition functions in space. Functions in space. In time. Okay, this is a very important thing. Maybe I should. The point is that you assume this not only to be justified at a single time, you know. That's why there's a repeated, you know, it's not there's an initial randomness assumption. The von Kampen says the repeated randomness assumption means that, you know, of course not maybe for every continuously chosen time, but on a cost time screen you're interested in, this should hold repeatedly. This should hold repeatedly. And actually, if you assume that, maybe I should have said this, you know, you can straightforwardly derive this condition. I'm not doing this in the talk, but then correlation functions between three colors. No, they don't become trivial. Well, no, not true. But calculated from the just this. Well, the pointers, as I said, for instance, these can still be very complicated objects depending on. Complicated object, you know, depending on all the microscopic details, but for instance, you find this kind of condition and all that. I guess you could rephrase my question in those terms. So if you look at higher order correlation functions, they depend on the lower order correlation functions? You can't necessarily get them from lower order correlation functions, but you get them from this table. You get them from just the density retrieves or something. So I mean you've proved Markovianity with that. And this assumption, of course, is the thing we want to look at. Look at. But in principle, it's a statement about multiple times in arbitrary I-order objects with respect to the cost framing. Okay, so this brings me to part two now. So the central question is, when can we microscopically justify exactly this kind of repeated randomness assumption? And he actually arrived repeated. And he actually arrived repeated maximum entropy principle because you might have noticed that this repeated randomness assumption or the Born approximation is just the maximum entropy principle applied repeatedly. So when you say repeated use of the maximum entropy principle, do you mean equivalently the repeated randomness assumption? Sorry? When you say repeated use of the maximum entropy principle, do you mean this is equivalent to the repeated This is equivalent to the repeated randomness assumption? Exactly. So the state I wrote down as a repeated randomness assumption you will find is the maximum entropy state given the information about the cost of X degrees of freedom. The same Bohr approximation is given the assumption of the system state and the temperature of the bath of the lexicon methods. And that's a very crucial question, and I think, for instance, James even never addressed that. I think actually James. I think actually James Broden's paper also three years after Finkel. And, you know, even if you only look at equilibrium transformation, equilibrium seminars, that's already a crucial question, because maybe you're justified to apply it at one time, this maximum entropy reasoning, but then you still have an isolated system evolving to another equilibrium state, and unitarity tells you that phenomena entropy needs to be preserved. Phenomenal entropy needs to be preserved. So, what justifies to apply maximum entropy reasoning a second time, or even a third time, or fourth time? That's the crucial question behind that, to understand this. So, sorry, you mean it was macroscopically justify the repeated use of the maximum entropy? Exactly. Macroscopically. Okay. Did you say microscopically? Macroscopically or microscopically. So, I want to show. So I want to justify this repeated use of Maximentary principle from microscopic quantum physics. Yes, sir, but okay, yes. So that principle applies to macroscopic things. But not macroscopic in the usual way. So as I said, an open quantum system quantified as macroscopic in a sense that there are still many unobserved degrees of freedom in the bars. So let's say it's a cost principle, not a macroscopic one, if you want. And you want to derive it from microscopic. And you want to derive it from microscopy. Okay. So that's the goal. But before I briefly review what we achieved on that, I actually want to give a warning here to that audience, also in quantum system theory, because I think often you address questions with a model which I think is not fully adequate to do that. To do that, at least for fundamental reasons. And I try to explain that. So, what's the conventional approach? Okay, this is standard. Then often you use like an integrable bars, you know, harmonic oscillators or free fermions. That might be also still fine. But then at least initially, what people, the majority of papers at least assume, is a Gibbs state for the path. And that, I think, is completely fine. I think it's completely fine and justified if you want to compute some relaxation rate or for many practical questions. But I don't think it's justified to address the fundamental question of where does classicality or Macombianity come from? Why not? Classicality? Well, this state is highly mixed. What is a mixed state? A mixed state is basically, you know, you introduce a lot of external classical noise by assuming an ensemble. Ensemble. That's kind of a catch-22. So if you at the end say, oh, look, things are classical, I understood the emergence of a classical world. Well, actually, only what you have done is put in a lot of classicality from the start, because you put in a lot of external noise in form of an ensemble. And then obviously you get classicality out. If you want to fundamentally understand where classical dynamics comes from, you shouldn't start from an ensemble description. Exceptions to this topic. What means exceptions? A lot of cases where you can start from something like this, but get something very outcomes. Oh, that is still fine. This does not contradict what I say. I'm just saying, if you want to fundamentally understand why our world looks classical, and you start from an ensemble where you basically assume external classical probabilities from the outside, then you should not wonder if you can derive classical things. This does not contradict that. This does not contradict that you might also get quantum effects with that description. This is not what I said. No, but it's a star description. I understand what that means. But isn't what you're doing with the coarse graining something rather similar, as opposed to considering a canonical state, you're considering a microcanonical state, which is also at the top? Well, the point is the coarse grain you also do here, right? Because it tries out the bars. Right? Because you trace out the bars. So both use core screening, and I think we have to agree that, you know, if you want to have any kind of irreversibility, and I think the emergence of a classical world is linked to irreversibility, you need some form of coarse granny. The question is whether you combine coarse graining plus external classical noise in the form of an ensemble, or you use coarse graining and pure state dynamics. That's the difference. Now, what about Markovian? Now, what about Markovianity also here? I think, at least in my view, there are some misconceptions, but I'm happy to discuss this. So, what's the common understanding of Markovianity? What people say, ah, this means there's no memory, memorylessness. But the problem is any unitary evolving system, there can never be any forgetting of any state. It's information preserving. The state of the bars has stored whatever initial state of the system. Whatever initial state the system had. So, how does Markovianity actually physically arise? Well, the concept that I like to use to think about it is called microstate independence. What is microstate independence? Well, assume you have a dynamics. Let's look at open corner systems. You have some system dynamics, and that dynamics depends only on some cost properties of the bars and not on the bar. Properties of the bars and not on microscopic properties of the bars. If that is the case, you get Markovian dynamics out, because all what the system actually sees from the bars is some temperature, some overall coupling strength, some coarse parameters, which you are aware of in an experiment. Conversely, if you have an open quantum system where different, say, microscopic states of the bars and some energy shells in the bars give rise to very different dynamics. Rise to very different dynamics than you have non-Macogen dynamics. So I guess if I think about it from this agent perspective, the agent has a limited perception of the environment, a partial observation of the environment. You can not covet any future observations of the agent doesn't depend on stuff he can't observe. So kind of think of it that way, because I think that would be a more generalized statement of the same thing. We can only access the microscope at the degree of freedom, and if we can only do that, And if we can only do that, as long as the Microsoft don't affect that, then things to us don't function like Markovian, even though they may not be underneath it. Yes. I mean, I think that exactly what this says. You know, your agent would be the open quantum system. It kind of senses what the bus is doing. But, you know, if it's not sensitive to different microstates of the bus, microbiology you know. And you can use this to really prove microbiology rigorously quite doing that. So even in Supers model we can uh take a phase space, we divide it into cells, yes, and then we have a like we go to tensor because we have a finite space of let's say index of the cell, an internal space, yes, and this internal definitions of freedom are in fact we obtain environment. And here mucuscle definedness means that no information is stored in the environment at some point. It is stored, but it doesn't affect the dynamics. Okay. Exactly. So, from the point of view of dynamics, we can forget it. So, there is no backlow of information from these internal degrees of freedom, which are hiding by a close grain. Yeah, that's the way you. Now, once you appreciate, say, this concept of microservices independence, it again becomes clear that you shouldn't ensemble average, at least if you're interested in fundamentally understanding macro-munity. Fundamentally, understanding Markovity. Why? Well, because ensemble average, you know, you heavily wash out the influence of different microstates. You know, you average over a giant amount of states, with very tiny probabilities. But if your system is very sensitive to different microstates, then maybe you should not do that. Again, at least to get a fully complete fundamental analysis of macroecology. I'm not claiming that for many practical calculations, I don't know, whatever system you'd like to look at, you know, doing ensemble. You know, doing on some level, it should not be completely fine. I mean, it saves your life because it's much easier. I just want to point out that, no, on a fundamental level, I think starting from pure states is definitely more satisfactory. Okay, that's all, I suppose. Okay, now why should all this kind of theory actually work? Why should you get out from a unitary evolving piece? Out from a unitary evolving pure state classical dynamics. You know, decoherence cannot happen on a global level if you have a pure state. And if you make any kind of transition from one state A to another state B, this can only be due to coherence. You will always remain diagonal, you can never actually hop between two states. How can it be that the coherence doesn't matter? Well, I have this very crude picture the way I like to think about it. It might be absolutely wrong, but well, it seems. But well, let's see. I mean, you will tell me. So, here's a density matrix, and you know, with respect to your cost grain, you can divide in blocks of populations and blocks of coherence. And here, this for me is a two-state system. Okay, so this is just a block with one element here, maybe spin up, spin down. And the way I picture coherences is like you throw a stone into a pond and then you see the waves going out of that. And these waves, And these waves then they influence the populations, and population acts on the coherences, and so on and so forth. So, if you have only one single wave going out, because it's just a one-by-one block, or you only throw one stone into the pond, you know, then you have very nice clear waves and strong impact of coherence. If you throw like two stones in the pond, then still also look kind of pretty coherent, you get nice interherence but then all that. You get nice interference and all that. But now you get coarse observables of realistic many-body systems. So these blocks are like giant, okay? So then they more or less resemble like you have heavy rain falling on a pond. So you have like each spot where a raindrop hits is like one coherence which sends out some wave. Now, remember that we have a non-atticlobal system. That means You have a non-indeglobal system. That means the time dependence of all these waves and the phase relation is essentially random because there is no regularity in the spectrum. So, what happens if you try to, you know, get an interference pattern of zillions of little drops all at different frequencies and phases. Or they will basically all cancel out. And that's it. So, for these macroscopic systems, of course, observable is a non-integral. Systems of course of those non-integral and all that. It's just very unlikely that all these coherences add up in phase and give a huge contribution to your dynamics. And that's why you can effectively neglect them. That's at least the intuition that I have. Might be wrong. So there's no formal arguments here. There's no mathematical proof? That's not a mathematical proof. But it sounds like it's something that could be formalized. Formalized. Okay, let's see then. So I have almost nice to show you the results, and then that's it. Maybe that's okay. There were many questions. I don't know. I try to be quick. So what can we make formal? Well, it's not too much, but I think it's definitely more than comp. So first of all, this first paper. What can we prove about classicality? Well, what we show is that the eigenstate summarization. That the eigenstate summarization hypothesis, I'm not defining it here, but I assume everybody has heard about it, suggests that these quantum effects are exponentially suppressed. So basically the difference between these two sides of this Komogorov consistency condition. This case is one over the dimension of the Hilbert space with some exponent alpha, which is not universal. So it depends on many properties. But typically, let's say you can think about it around one-half. So it goes like It's around one half. So it goes like with one over the square root of the dimension of the Rubber space. Now we have to assume some additional things on top of slowness and coarseness. So first of all, we can only look at three-time correlation functions. It's just horrible to go to higher orders. I mean, the expressions become longer and longer, and I don't know how to get them under control. You have to assume kind of typical non-equilibrium addition states. So, what does it mean? So, first of all, So, what does it mean? So, first of all, we can talk about non-equilibrium things. That's not an equilibrium statement. What does typical mean? Well, we all know that we can violate the second law. In a way, for instance, there could be a microscopic state where all the air molecules in this room point towards me with their velocity. You will now all die. And I don't. And that would violate the second law, right? We all know that you can find states. Let's do that. States that do that. And in the same way, there will always be states or some exceptional times where coherence suddenly really matters and you have a complete quantum effect in your dynamics. The point is that these things are not generic or not typical. So this has to be excluded a little bit. And the third thing, which is a little bit related to that, is you have to assume that there's no precisely tuned fine structure on the observable X. So all this observable X. So, all this observable X, if you look at its matrix elements, there could be precise correlations which actually filter out only like, you know, some dynamics in your Hilbert space and don't really mix well. And that's something one has to basically exclude. And I can tell you what this means in terms of ETH if you want afterwards. But if you use this, then I think very nicely from pure state dynamics in the many-body system, you define the S-may. And you find that estimate. Now I have written down here suggests and not implies, because there are mathematical physicists around, like Marco, and he would probably kill me if he sees my proof. And it's just some, you know, you write up terms, you estimate things, and you try to get a coherent picture out of it. I like it, but it really does not qualify as a rigorous theorem. I mean, it's way too hard, and that's you know, there's no systematic theory to do anything there. Theory to do anything there. But I think it's definitely analytical evidence. What about Markovianity? Well, here we use something which is called Lee's lemma. It's just a concentration inequality, result of typicality, if you like, which also suggests that this kind of microstate dependence is exponentially suppressed. So the conditional probability to go from y to x does not depend very sensitively on what kind of microstate you have, psi of psi prime, compatible with your conditions. Compatible with your conditions. Okay, but you expand it exponentially with respect to voltage? You have an E, and then there's an E to the power of minus, and then there's this volume term, the Boltzmann volume term. And this will typically be very large. Yeah. And again, it's kind of no fine structure assumption. And again, it's not a, you know, I think it's a good argument, but making a real solid proof of it. Making a real solid proof of it is that one. So for the macro reality, what uh uh what's the precise definition of the microstates? Is it just with respect to the cross-grading of the energy measurement? Well, the point of this result is that it doesn't matter what the microstate is. Right. Okay, right, right. But does it matter what the macro states are? Like what assumptions there for the macro scale cops are ultimately. No, that's the point you can apply this to non-equilibrium dynamics. That looks a little better. Actually, you can look up in the paper, follows from what I said before. And I just want to mention that we numerically verify these things for spin chains, up to 28 spins. I think it's very nice to really see these things emerging out of unitary evolving quantum systems. And of course, I mean it's not only nice to confirm it, but in principle also you can now really start to study numerically counter examples and understand better. Counter examples and understand better when things are broken, and we use it a little bit, but not too much. Last slide. This is a second paper I've published just a month ago. This is split into two parts. In the one part, I try to actually connect many notions of classicality which are around in the literature. And I don't want to explain the entire diagram here in detail. Now actually these arrows mean strict implications in a mathematical sense. Implications in a mathematical sense. And for sure, most of these errors are not due to me. But maybe some are. A nice thing to see, for instance, is that if you use the standard decoherence concept in open corner systems and use a proper definition, and if you assume Markovianity in a strict multi-time sense, you get all this common growth consistency, for instance. So if you're interested in these general notions and how this connects to other formulations, for instance, in this history formulation of quantum mechanics. History's formalism of quantum mechanics. I'm happy to talk more about that. Then I just want to say in this paper, I then used random matrix theory, which is closely related to the ETH, but it's basically a subcase you could say. And again, this suggests some kind of exponential subrepression of quantum effects. And it kind of assumes more because you start from endermatrix theory from the start, but then you have less. Start, but then you have less assumptions to employ. So, for instance, you can get rid of this kind of, oh, there's no fine structure of x because it's, you know, random matrix theory automatically takes care of that. And you can verify things with much smaller space dimensions. I mean, you have an up to 18,000 and you really see that these quantum effects are suppressed in a unitary evolving system. Okay. Here, conclusions. I won't go through them, you can read yourself maybe because it's talk is basically over. I think one thing I really want to emphasize once more, because at least from referee reports, I had the sensation that there's some misalignment. So here I was all the time talking about doing things repeatedly or looking at a multi-time perspective. That's way more general than looking at the evolution of a state of an open quantum system. Of a state of an open quantum system. The state does not allow you to infer anything about multi-time correlations unless you assume Markovianity as a quantum regression system. And I think to really understand these kind of properties fundamentally, sooner or later you should really think about what can be said about these things from a multi-time perspective. Because sending it only once for one special initial time, maybe starting from a Gibbs state, is very good. Gibbs state is very good, not too bad, but you know, I think it's far away from providing a complete picture, even if you're only interested in equilibrium transformation, as I said. That's the important thing. So, this is really not only about the density matrix. And yeah, so to conclude, what I hope I could kind of provide you with a different perspective, but which is actually fully nice and is what you know, I believe. For me, this was very easy. For me, this was very helpful. And what I want to emphasize is that I used really completely different tools. So, in all these things, I never used any system-based standard product decomposition. I don't use decoherence. I don't use Mecca G-Matz fancy. I don't use perturbation theory. I don't use the common Markov or Secular approximation. I use completely different tools. Of course, a Markov approximation is clearly hidden by assuming these time-scale separations. But it's formulated in a different way. But it's formulated in a different way. And at least for me yeah, this provided a helpful perspective, so I hope for you this was also helpful. And I'm looking forward to your feedback. Thank you very much, Philip. Anyone questions? We can maybe have a few more. Or if anybody in the audience remotely wants to ask a question. Audience remote camps to ask questions, they can answer that. I think there are chats there, but this is supposed to be taking more chats. The meeting is recorded. What does this thing say? There is chat, Zoom, and there is maybe a question. I'm not able to control it. Whoever had the question in the chat, could you unmute yourself and say? In the chat, could you unmute yourself and say the question out loud? Every one of us has to log in always to follow the channel. Next time. There's no question in the chat, just so you know, there was one comment earlier on, but it was about the image not being really clear. Can I ask a question now? I'm sorry. This is Lorenzo. Please go ahead. Thank you. In regard to message 3, I really understand completely the sense in which it fits here in this picture. However, from a mentality perspective, it is also known that EPH has a lot of violations and in particular random And in particular, random matrix theory is too unstructured to describe manually systems because it doesn't account for a locality of interactions that are present in the majority of realistic many body systems. Can you comment on these two aspects? At least ETH and RMT kind of things remind refine in your Uh refine in your verifications, uh perhaps? Yeah. Very good comments. Uh I myself, you know my background is not in that field. I use this as tools, I find them very interesting. So my perspective on that is ETH, yes, people have studied all kinds of exceptions, but whether these exceptions are generically true for realistic systems in nature. True for realistic systems in nature. I think many people would say it's not the case. But obviously, I think it's interesting to look at exceptions because maybe this gives rise to non-classical dynamics, for instance. And also the ETH, you know, I use ETH plus slowness and coarseness. So I'm not saying that whenever you have ETH, everything is classical. Please look at these two other assumptions here. With random matrix theory. With random matrix theory, uh i it's the same. I think you're right, one has to be very careful with applying it, but I think sometimes one is very well justified. For instance, in the paper I look at basically you have two bars and you know these bars are like some quantum systems, whatever they are, and they are just in contact. So like, you know, you just put them in contact in the contact you model with some banded random matrix. And I think, you know, for sure, this is not a completely realistic picture, but it's, you know, it gives you at least some. That it's, you know, it gives you at least some insights and it might suggest virtually extend things, which would be very nice. Thank you. Thank you. Slightly running out of time, but we can have one more question maybe from the online group if there is anything. Yeah, actually, I had one question. In the paper that you, like the first result you mentioned, He mentioned, gave this nice intuitive picture of this sphere, and the stage is doing something kind of like a random walk on the sphere. I wasn't sure how the slowness of the observable comes up in that picture, like how the slowness maybe is related to the rate of diffusion or why it's necessary in that picture. And also, you have this comment about certain symmetries that are being conserved and the symmetries confining the walk, whether it's randomly. The walk, whether this random refuge into some sub-sphere within that. I think the relevance of that restriction was also not super clear to me. So I was wondering if you could comment on those. Cheyenne, who are you? Would you like to maybe introduce yourself so we know who's asking the questions since we can't see the Zoom? Oh, sorry, I'm a PhD student co-supervised by Nicole and Malafan at the Perimeter Institute. Yeah, this is a good question. Um yeah, these are good questions. I mean maybe it's a bit hard because I didn't show exactly the picture you're talking about with this random walk on the sphere. Um okay well maybe to put the audience in context, in this one paper, you know, I imagine in this picture all you should think about things is like you know all your pure states in the Hilbert space are basically isomorphic to just looking at the surface of a huge sphere, you got a normalized state, and then basically And then basically performs, at least from a cost perspective, an effective diffusion random wall on this kind of sphere. And that's the way we motivate things. The question you ask are really hard and not so easy to see. I mean, it's just an intuitive picture which has its limits. I think slowness, you know, yeah, it's hard to see slowness in this picture, but slowness is important because Because, as I said, you can always have, say, atypical states which show quantum effects or non-Markovian behavior. But if you have a slow observable, then basically what happens on the time scale as observable observed is that there's some kind of self-averaging in the microscopic degrees of freedom. So it's really microscopic degrees of freedom evolve a lot, whereas the slope observable barely moves or changes. And so you sample a large fraction of this microspaces, subspaces, and due to that, basically things start to become to look typical. And you can, well, let's put it like this. You're allowed to apply typicality arguments even out of equilibrium when you're slow. But if you're fast, then applying typicality is a very bad idea in general. But that's hard to see from these pictures. But that's hard to see from these pictures, also from my raining raindrop picture. I agree. And the second one I think takes us even further off, so maybe just shoot me an email and we can discuss this or discuss them too. Surely I'll do that email or the second one. Thank you. Cool. Thank you. First question Anna, you had a question? Yeah, I had a question or a comment. I I'm not very I'm not sure if I understand correctly the argument of Messi 2. Why don't you want to use some models? Because, well, I mean, at least mathematically, if you have a density matrix in a parameter, which is not quite, I mean I mean, I mix a state. You can even think that the environment is something larger, such as that is in a pure state, right? So I'm not sure if I see... I mean you can modify any testing matrix. I am not sure if I get the point. Where is the purification very artificial in the sense that the purification space does not participate in the dynamics at all, right? So this is like you can also forget it. So, this is like you cannot forget it. And then, so the question is: you know, what does an ensemble describe physically? Well, at least, you know, there are many ways to look at it from a maximum entropy principle, since James was also saying that he doesn't like ensembles at all, and he was very happy that he could find a justification without using ensembles. But in principle, ensemble means that you consider many identically prepared systems, but all in kind of different states, then you all sample them at the end the average. Sample than at the end the average. But what you do is you introduce classical probabilities, noise, from the outside. It does not arise within the dynamics themselves. So if you start with classical noise from the outside and you want to fundamentally understand why some things look classical, well, you have to be very careful that you don't already put in your answer from the start. That's what I'm saying. I think it's is perhaps what you mean to say is that you have something similar to what happens with, for example, when you are analyzing this measurement problem, but you want to describe the measurement on the whole universe or something like that, doesn't take care. So I'm kind of averaging something like that. But a good idea. To to avoid to abuse uh Due to abuse because you are assuming that these algae varieties are due to the lack of ignorance or something like that. Well, I'm just saying, if you want to understand where classicality comes from, you shouldn't start from a classical description. Very plainly speaking. And now, of course, as was pointed out, also with ensembles, you can observe quantum effects and strong quantum effects. Quantum effects and strong quantum effects. I'm just saying, okay, but also if you observe maybe classical effects, it's maybe not too surprising if you have averaged over a very mixed state, that you see something mixed when you average over mixed states. I'm not sure if I have a fully idea, but I'm very interested actually non-classicality but you would like that to be robust. But you would like that to be robust to examine. Classicality, I guess you want the harder classicality to emerge without an ensemble. Once you've shown that it emerges even like that, then certainly most ensemble. That's a stronger definition. Sort of like it, but I also have a No, I would fully agree to what you said right now. All right.