I'm not talking about global, but I want to talk about the regional, especially for Canada. This figure shows our land covers in the country from the current regions in Clearly and southern Ontario, and we have also second largest people in the first, including matched and omega. Including mesh, then major forest, and we have some profit regions in the Arctic which are vulnerable to the change in climate, as we mentioned in the presentations. So, there are a lot of uncertainties, especially the sinks of GHG emissions in this region. So, we are required to get the flux estimate by using our component and the bordering system and at the most measurement of concentration that is concentrated. Concentration, that is concentration for sure. But the thing is that we are working on the ROS estimation using the folder model learn. But in case of using the online model, it is very heavy in terms of the computational cost. It is not doable running it for a short period of time over a decade of simulation time, for example. It's not doable yet. And also, the methodology is not mature yet. Address, so we came up with a different idea to make some alternative regional flux mission system based on lagrange modeling approaches. So we basically renamed it NCIS, that stands for the intricate simulation cover flux information systems. So that has some objectives. One is to obtain quantitative information of stretch fluxes, in particular we are the focus on the switch concentration of Canada from the Of Canada from the different scales, from national to provide scale, using this a lot of atmospheric measurements we have right now. And this other second objective is address the common sector from the Canadian perspective by utilizing some Canadian conference. We have in-house utilities and the modeling system and observation set. So, and another objective is making it routinely behind real time if possible. Okay, so now I reviewed how the inversion framework is working. So basically, we want to get some kind of loss estimates through the in-person algorithm, but that requires a lot of input data. For example, observation, one of the important parts. So we need some pre-processing to use them to get flow estimate and also provide information to the transfer model which location we are using to simulate. Simulate. And also, the transporter, especially radar model, is driven by some meteorologists so that requires some conversion process technically to make this input data working with transverse model. And also then finally to produce the footprint that is working as observation operator in the image system. And also these footprints are converging with some sort of prior fluxes to calculate prior concentration. Then finally by using the observations and prior concentration, Observations and pressure concentration and some parametric configurations, we are able to get some estimate about the fluxes and other trace guesses. Okay, so I'm not going over the get study from the cost function. Instead, I'm just focused on the analysis equation that I'm using for this system. So basically, we are estimating fluxes by applying the scaling factors that is. Applying the scaling factors that is defined on the land division algorithm in the inversion domain. So that algorithm is basically adapted from the US Carbon Technology system. So the reason for using the state-effective approach to reduce the number of the unknowns to be estimated to make some to get some fast computational speed as well as to reduce the unknown compared to the smaller number of observations we have right now. We have right now. And also, this is the prior scale factor, which is set to one, which means that just using the prior flux at each at the beginning of the simulations. And then we have the prior error covariance that is defined by patient right-hand side. So we can control the uncertainties and each case that is corresponding to the diagonal element of the prior covariance matrix. And also, we consider the temporary co-real. We consider the temporary correlations and spatial correlations like this to construct such a huge matrix. Because this analysis equation is applied just once for a year, simulations, so it has a lot of size, for example, that is number of grid and by the number of time steps, for example. And also the next component is R is a model data mismatch, which also controls the meaning of the observations to be used in the analysis. And also we have observations. And also, we have observations here. We end up using the actual torrential concentration, but instead, we want to see some concentration changed by the biogenic fluxes. We need our inverse model domain. To get this information, we are subtracting the background concentration that has the information contributed from the outside of the inversion domain, as well as some contribution happening within the inversion domain by other companies. Inversion domain by other components. I mean the relative biostate fluxes such as fossil fuel emission, biomass burning, and portion fluxes, although it is very small. And also in the K matrix, it's because the scalar factor is analysis, we need a kind of calculation by applying the observational operator with the prior flux in prior to calculating these analysis equations. So this H is corresponding to put flux. Is corresponding to put friends produced by the rugged models. And also, I checked the difference in number of weekly strength factors between one per week, which is doable in my setting, or which one was better than all similar. And also, this is huge matrix, so that it calculates computation by using the methods proposed by Yadaf and each. So, speaking of observations, I'm using the observations of Ben Log in the data set. There is OSPEC, the version Global View Plus version 6.1. It has a bunch of observations, especially over North America, but I selected some surface site as well as accurate profile data. So, this figure shows the first inversion domain I have. domain I have is uh by the solid line. Although it is defined as the longitude and the latitude, only the random regions are used for the inversion because I just try to estimate by the fluxes. And also the red dot here is indicating the surface measurement locations and the blue start indicating the aircraft profile available by weekly or monthly. And speaking of the quality control, I'm using the OBS flag. The obvious flag variables to filter out some data. If that flag indicates which one is influenced by local emissions, I excluded them to not to contaminate emissions in such flux estimates. And also about the averaging, for the continuous data, I applied the afternoon time average between 12 to 60 local time based on the longitude because we know the inversion modelers usually tip. the inversion modelers using typically using after and time data because the like time simulation is still not good enough to be used. And also for about the flask data and every profile, I don't apply the everything just using it as usual but most of the data is available. Okay so now speaking about the transport model so we are using the two IPDMs. One is flex part the other one is tilt. So which So, each model is driven by the different meteorology. So, speaking of how it works, I grabbed some figures that take how the model works. Basically, the particles at the measurement locations given the information of latitude and magnitude, height, and time. The number of particles are deployed, for example, to use 500 particles, and for users 5,000 particles, and these particles are. Particles and these particles travel along with the vintage using the vintage speed up to the 10-day back-end time for both models. And if they are resulted within the specified criteria, performability is for state or specified specific numbers anyway. So if the wall particles close the surface, which means it integrates as the strong footprint over those regions as well. Footprint over those regions as red colours. On the other hand, if the panic would just reside upper troposphere, then the footprint would be much weaker or nothing happening. So then you can see how sensitivity is at up in the regions with respect to the measurement point. And also, speaking of meteorology, the flexibility is typically driven by the HMWF VLS data set. So I'm using the ERA5 and ERA internet, testing both. As well as we are trying to add one more forecast component because we have our own transfer model which is online compared to operation weather forecast and the trajectory simulation. So that being said, it has own weather forecast. So it is due to using our weather forecast for driving a flex part model. So it has, I learned it on 45 kilometers. I learned it on 45 parameters globally, and all the data I provide to the flex part model. And for the state, this is the work simple there. To be honest, I didn't run this model on my end, but instead I downloaded the frame from CaboDoa Common Technology Fund library, so we didn't have any flexibility to control the frame here. Okay, so because the coupling our model reflex part is nothing new, I need to verify the calculating footprint. The calculating footprint is comparable with other footprint components within the FlexPile framework. So I show some examples of footprint at two sites, one at the Lara Bish, which is the closest side from this workshop venue in Agatha, and the other one is Igobit, just from north from Toronto. So that is basically what we mean for over January 2014. And the basic This uh basically uh the special distribution of footprints are very comparable at large scales. At some but you can tell the small details differences happening across the different metrology. This is what I expected. This is the same in other place as well. So they have they are well spread over the entire China region, so hoping that gives enough information about the uh fluxes that measure the ignorant. Measured equipment. So, this is kind of a qualitative comparison. So, I did some sort of quantitative comparison by looking at the calculated shield concentration by using Doji footprint. And I calculated pressure concentration in addition with the background shield concentration that will be explained later. So, basically, the black dot represents the observations and three lines from the using the Reliance from the using the articulated model such concentration by electrical difference, and even the prior such concentration is to capture the availability of the observed measurement. Also, this is only for the afternoon time data, because we are using just after time data, so night-time data is excluded, and this is out of interest at the moment. And also, looking at the ignorant, this is the same, but there are more signals. We can see that the difference is to be in use. To be used in for the flux estimates later on. And also, prior fluxes, we do have three prior fluxes, two from the carbon tracker system. They have the two prior fluxes. One is Class AG 4.1, the other one is Classic CMS from the CT29 TB version. And one more component is the clash model data. As we previously see, it provides the GPP and respiration. The GPP and respirator over the globe, especially 0.22 degrees over Canada and 1 degree by 1 degree over the globe. So the data is overriding the merges on the 1 degree by 1 degree spacing to match with the same input format with the common typical press to make each step. And speaking about the special distribution, this is an example for April 2018. So as you can expect, So as you can expect, it has the similar spatial pattern, but our model has somewhat different special pattern, as well as the magnitude looking at the muscle fluxes. 14, a system cycle is similar, but the magnitude is different, which ends up with a different annual budget in the cluster of data. So it's too somewhat larger for me, so I'm investigating about these fluxes anyway, but data is used as priors, hoping that it's corrected by the observation. Collective observation. And also looking at the other components such as fast fuel and fire emotion, as you can detect the phosphorus fuel emission is significant, so a lot of emission happening along the overall urban regions. But on the other hand, the fire emissions, we have some just like a point source search with lower internet as well as the ocean edge. Because all panels have the same range, it is that shown is deletable in just patching for something microbial. Something microbolt per meter per square meter per second is so but anyway all of components are included anyway to consider the contribution within the version domain okay so I I started with the OG to figure out the development system which can read the synthesis to introduce and with the known unknowns. So basically now as I explained there are 30 prior fluxes and you have four Predator fluctuates, then you have four fields. So there are trend experiment possible to carry out. And also, experiment period is 2014. And truth flux is again from the common tracker, but it is time this is optimized by spectrum fluxes and E. So the synthetic observations are generated by using the each footprint, each footprint and the truth fluxes, and also by adding some random noises with the german and some sort of With the geometric and some sort of the standard deviations. When I try to do this kind of sensitive test with the different numbers, the posterior just changes more or less, but not too sensitive with this number, but the number is much more sensitive to the mean biases, which means that the background concentration is very important in the real data emotions. And also, these observation locations and time is the same as the real observation. The same as the real observations to make the observation network similar to the real cases. And also, I did a bunch of sensitive tests, but here the main parameters here is the, as shown here, just using the constant value for the 100% of the diagonal element of the Q matrix. And the model data mix matches say VPN for surface and PPM for aircraft. And the special query length scale is 1000 kilometers and temporal query scale just one MIC. And also I'm teaching already applying the other. And also, I'm teaching already applying other methods to optimize such parameters, but I'm still a Q ⁇ A. Okay, so this is the result of fluxes. So I'm just focused on Canada, although the emergent domain is covered by the US and part of the US as well, but just focus on Canada to make the three simple. And so this shows the three different experiments as an example with the different correlation scales from seven. Correlation like scale from 750 kilometers to 7000 kilometers to 1500 kilometers. As you can see, this is the truth fluxes in black switch line, and that's right indicating three different priors. Also, this is a little bit different from the fluxes. And solely the lines, the 12 lines indicating the posteriors. Some lines will overwrite the two fluxes, which means that, okay, this is perfect. But some lines are not, which means that there are some differences. That there are some differences, discrepancies between the truth and the posteriors. So, this bottom figure shows the annual budget of each experiment. So, one thing we can notice that there are some sensitivities of the choice of priors. As you can see, this is 4. Because that is split down by 4 by 4x4 using different priors from two cover checkers and plastic models. And also, we can see some difference between the different transports. The retribution. The different transports. The reason for using, yeah, this is one good thing for using the different kinds of constant number experiment by considering different priors and transport because you can get some kind of uncertainty by using heterogeneous data as well. And also in terms of the impact of correlation X case, there are some little differences, but not too much. Uh the big change is happening uh between the using the one scaling factor per week or eight challenger per week, but there is not shown here. But there is not shown here. And also, looking at the model disease concentration, I just grabbed one of the experiments from the tribe experiment, indicating how it works well. So prior CO2 is in the red and posterior is blue, and x-axis for modern CO2 concentration, and the y-axis truth. But anyway, the inversion without any standard gives the almost perfect correlations and the slope as well. And the slope as well, which is good. And this is similar to other experiments as well, even with some random noises. And also looking at the post-trace concentration at specific sites, again, La Rabishe in La Rota and Igorota in Ontario. Yes, it is well capturing the choice fluxes. This is only showing the minus monitors ranging from minus 20 to plus 20. So it's working well, except for some cases where somehow it's too high or too low for the features. Okay, so now I'm moving on to the experiment with the real data. So now again, so we have the three priors and for transport, when in this case, we need to consider the contribution from outside. So now I'm using one global component, which is covert 2019b for CSU2 reflections. But I think that is not enough. So I'm working on to use another product, one candidate in our global model. Our global model. So it is a relative easy to use. So they just need to convert the data to just little modifications. And also the other candidate is using the HMW camps renowned data for the background resource concentrations. And also the expanded period for now is between 2010 to 2016. But as I mentioned, we don't have any flexibility of the learning of the state, which is now available afterwards 2018 and 2020. The 20s agent and 2020, so we are thinking of using the models for example high split to make some more flexibility to running the model on our end. As well as the ERA intern is terminated at some point in 2017, so that is some motivation to introduce other transport to consider the difference transport in the experiment with your data. And also, computation I did now and trying to do something is testify to test test, but here's the result. But here's the result shown here is similar to the OG configuration shown before. Okay, so this is the flux estimate for Canada. So I want to emphasize that this is very preliminary. So this is the final result yet. But looking at the comparing with the city data shown in black, the ACC results with a red line with some shading, which indicates the two standard deviation of the tribe members, shows some. Triple members show some kind of the range, put ranges as well. So, hoping that there are some showing some trendy variability, especially between the 2012, there are some advances happening. So, it would be interesting to analyze what happened. And also, it was inspired by the rules of yesterday's presentation. There are some another inversion product was shown to meet data, so I picked up the K-media results, but using the in-seat data only to make the comparison more. Only to make the comparison more like the apple-to-apple comparison. So there are two years over here. So quality, lovely. But I think personally, it is very black, hoping that provides results in our inversion systems. Okay, so there are some border research concentrations for the aircraft profiles. So I selected two sites, one in the west, in East Translate in Saskatchewan, one in the LF in Park Post in East County. So as you can easily expect that in You can easily expect that in the winter time, we don't see any such big contributions from the surface due to the body formation is very weak. On the other hand, we can see some improvement, the enhancement of biosurface fluxes in summertime. So as a result, there are some corrections happening in the winter time, even at the high altitude over four and five kilometers. But in the on the other hand, summer time, on the other hand, in the winter time, there is no events happening this year. There is no events what's happening this year, so which means that it is better to not use those data in the inversion, but using this data to correct unbiases and potential biases in the grammar cells concentrations. That is my plan, in my mind. And also looking at the eastern side, this is the same. You can see some corrections happening in the posterior cell concentration by looking at the perturbations. But this again, this is the enhancement by subtracting background contribution. But here again, there is 10 years ago, even both in prior and posterior. So maybe these data can be used to correct biases in the government system. Okay, so the final figure I'd like to show is that using the proper configuration is very important, especially because we are just interested in looking at the annual budget or the monthly fluxes, but we are zooming in the But we are zooming in the daily or more higher resolution, something weird is happening if the configuration is not correct. For example, this is one specific example showing the daily fluxes over November 2014. It looks the blue is posterior. It's comparable with the carbon check. It looks reasonable, but interestingly, looking at the three-hour sometimes happening, the bodies, this is the dinocycle flip. The dynasty dynasty flips over, which means that the things happening in night time are also happening in the daytime on average in the version domain. It's it yeah, it's insane. It's totally not physically making sense. So which I found that I will recognize that this is sometimes often happen with just one scale effect approach per week. So that is justify why we need to use the aid scale factor to us, just dynamic edge rate. Okay, this is summary. So here I'm presenting the current progress of our vision-scale inversion model system to obtain the flux estimates for Poisson Canada. But in the framework OT, it works. In the framework OG, it's working well. It well captures the truth, known trust, known truth fluxes. But there are some issues remaining in the using real data inversion, so we are working on it. And also, we are trying to utilize more number of trackports and geographic and et cetera to make the system working better and to better capture very quickly some of the uncertainties in the post-chain fluxes in the future. In the future. Okay, so I'm happy to take any questions and comments. Any other questions for Ching Wu? So did you say you had one degree footprints that you're using? And if so, why would you not use higher spatial resolution? Because I thought the whole, one of the rationales for using these regional models. For using these regional models, was that you could actually push it to higher spatial resolution? Yes, that's a good question. Because you know, this is adapting the CTL. First, which has one degree, so technically it's double going further down, for example, the half degree or a quarter degree, but we need to also consider the computational cost because you know this is just one inversion for a year. So, making the, for example, half that making the number of land degrees is four times, you know, that for now. For now I have the 3,000 over 200, so which ends with the 12,000. So I don't know. Maybe I can check with once the system is stabilized, then we can moving forward to get the system more higher resolution. But for now, I'm working on the stabilizing stability first. I'm just wondering, like, how many. I'm just wondering like uh how many uh spirals uh did you average over a lot of spirals and uh all the observations uh observed during afternoon or morning? Okay, so I haven't looking looked at the time in the afternoon or morning, but just using them as is. But okay, that's a location I will check if there is a morning. Check if there is some data. Questions? Yeah, I want to make some announcements about moving on. 