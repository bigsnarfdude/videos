Room, so hi Christian. Um, so let me start by giving a quick overview. I'm going to motivate optimal stopping problems and then how signatures come into play here. And then I will introduce the signature stopping rules. So the main objects of this work and also the first main result here. And then I will talk a little bit about linearization. So we use this. Linearization. So, we use this group structure of signatures to linearize in some sense the optimal stopping problem. And finally, I'm going to talk about a combination of signatures and deep neural networks and present a numerical result. So, optimal stopping problems, we start with some filtered probability space and a d-dimensional process, which we can, for example, think of an asset price process. Think of an asset price process. So there's a little bit of financial motivation here. So these are the different assets and they generate this filtration. And then we have a payoff process, which is one-dimensional and is adapted to this filtration. And we can think of this as the payoff of an American option again in financial terms. And then calculating the price of an American option means calculating the supremum. Calculating is the premium. So, overall stopping times with respect to this filtration, we're trying to maximize the expected payoff at that stopping time. So, this is a very classical problem. And in case, so in case X is a Markov process and Y is some function of that Markov process, possibly also of time, then there are many methods for computing this. Methods for computing this or approximating this optimal stopping value. There are regression methods, relations to PDEs, also dual methods for calculating upper bounds, and then these methods can be used to calculate also lower bounds. So anyway, so in the Markovian setting, this is well understood. And we will not assume that X is a Markov process, but we will assume that it has a lift to a geometric rough path. Lift to a geometric rough path. So you saw rough paths in different talks earlier. So essentially, this is not an assumption on the model, but rather on the regularity of these paths. And therefore, we may say that it's in fact model independent. So, how do signatures come in? I will use a little bit the fact that other people also introduced signatures in talks before. In talks before, so I'm not going to really define the signature here explicitly. But you can think of a signature as a representation of a path by some element in the tensor algebra, so a sequence of numbers which are given by the iterated integrals. And in certain situations, this representation is efficient. So there is not really, I think, by now a good By now, a good characterization of this efficiency in the literature yet. But if you, for example, look at approximating the solution to an SDE that is driven by this path X, then one can see that the terms of the signature can be well used for approximating the solution. So, this is somehow at the heart of the idea of rough paths. Now, we are not the first ones, or we were not. Now, we are not the first ones, or we were not the first ones to look at control problems associated to signature methods. So, there was this paper by Carl C. Lines and Perez Aribas, which essentially was a motivation for our work. And they studied an optimal execution problem. And so, you are given some initial amount of stocks, and you have to liquidate these stocks. So, sell these stocks by certain So, sell these stocks by a certain given time horizon. And they used a parameterization of the trading rate by linear functions of the signature. And they could show that with only this class of trading strategies, they can approximate the optimal solution to this execution problem. And we were, as I said, motivated by this work and we were trying to do. We were trying to do something similar for optimal stopping problems, so for pricing American options. But there's a significant difference between a problem of this type, the optimal execution problem, and an optimal stopping problem, which is that here you have your control variable is the trading rate, which influences your signal in an infinitesimes way. Way. So you're giving a trading speed, so to say, at each given time, but changing this speed a little bit will only change your solution a little bit. And this is not true for stopping time. So in some way, and I will show you explicitly how we do it, we parametrize stopping times by a given stopping rule. So this will be the step to introduce signatures. So we will introduce stopping rules depending. We will introduce stopping rules depending on signature. But then, still, changes in the stopping rule can lead to discontinuous changes in the evaluated payoff at that stopping time. And this is intuitively speaking what is different about the two problems here. And the way we solve it is by introducing randomization. Randomization. So, this will all be on this slide. So, the first main result, the signature stopping rules, and the randomizations are all contained. So, let me explain what this means. So, we have some geometric rough paths. So, this is our initial input signal, the underlying path, so to say. So, for example, the D-dimensional acid process. And we assume that it has a lift or And we assume that it has a lift, or it's given already as a geometric rough path that generates the filtration, and then we can augment it with time. So, one word about rough path generating the filtration. There was already a talk yesterday by Bruno Dupier who mentioned how this works. So, you look at your space of stopped rough paths. So, a given path segment. So, given path segments, and in this way, you can introduce a concept of filtration to your rough path and a concept of stopping a rough path. So this was introduced by Bruno yesterday, which we used also in this work. So, and also, this is also something Bruno did yesterday. He looked at the time-augmented path, which is essentially, so we extend our leader-mentioned process. Extend our D-dimensional process to a D plus one-dimensional process by putting time into the first component. And the use of that is essentially that in this case, the signature will uniquely characterize the path. So this is our input. Then we have an additional component here that introduces the randomization. So some random variable set, which is non-negative and has a continuous density. And it has a continuous density, and as that is independent of everything else. And now we parametrize a certain class of stopping times in this way. So we take a linear combination of words. So in other words, a linear combination of multi-indices. So this is where I'm not going to be too specific about because there were some talks about this yet. In any case, this represents a linear. Case, this represents a linear functional that I can apply to the signature. So this is the signature of the underlying path. And applying this linear function means I'm taking a linear combination of components of the signatures, which means this expression here is a linear combination of iterated integrals of the path. So this is a one-dimensional. So what I'm marking here is a one-dimensional process. Dimensional process over time, where I decide which signals this is by specifying this linear functional. And then we square this up to have a positive signal, and then we integrate it over time, which makes this some increasing signal. So this process or this signal here increases over time, and I will stop as soon as this reaches above this random independent threshold set. Independent threshold set. So, this is the way I define my stopping time. And I have the possibility in changing this stopping time just by changing this linear functional, the linear combination of iterated integrals. So, and then the statement of this proposition is that given a continuous payoff process, which is adapted and satisfies this boundedness condition, we can approximate. Boundedness condition: We can approximate the solution to the optimum stopping problem by only looking at stopping times of this type. So I think if one is not familiar with signatures, then this might seem at first quite surprising because it looks like a very specific class of stopping times. But then, if you know a little bit more, then you know that the signature of the time-augmented path uniquely characterizes the path. Uniquely characterizes the path. And you might also know that any continuous function of the path can be approximated by linear function of the paths. And in this way, you can see that we can really approximate or we can exhaust in some sense the space of all stopping times by looking only at these stopping times. But there's this additional step. So I just said we can approximate linear function. Sorry. Continuous functionals of the path by linear functionals of the signature. But there is this problem with continuity that I mentioned earlier. The fact that this payoff does not has to depend continuously on the rule, the stopping rule that I choose here. And the way, as I mentioned earlier, As I mentioned earlier, that this gets regularized is by introducing this randomization. So, how this exactly regularizes the problem is now here on the next slide. So, this was helpful in the proof. Is there a question or? Yeah, I had a question on the previous slide, or maybe this slide too. What is the relation between the why and the other objects? And the other part ingredients or yeah, so Y has to be adapted to the filtration generated by X. But other than that, so far is no connection. Okay, but that's important. Yeah, that's important. Yeah, that's important. So this is, for example, some payoff of some asset price. Of some asset price. But it needs to be adapted to the filtration coming from it. So in this way, the regularization by randomization comes in. So we can write the expected payoff at this given signature stopping time in this form. So as an expectation of an integral with respect to this payoff process times times or the integrated we are integrating the density of this random variable so i didn't mention this so for the sake of concreteness we choose here an exponentially distributed random variable and we integrate the density applied to this signal so the stopping rule signal integrated by one so as i mentioned this is helpful in the proof but it's also Proof, but it's also helpful in two other aspects. So from this point on, we can think about the group structure properties of the signature or the shuffle identity of the signature, and we can try to linearize this term here. So, this I will show on the next slide. But also, numerically speaking, this gives an idea how to approach. To approach a numerical optimization of this optimal stopping problem, since our parameters to optimize this expected payoff are essentially the coefficients in this linear combination. And we can easily see that it's a smooth function of these coefficients. So one easily may think of applying a stochastic gradient descent method here in order to optimize this expectation. In order to optimize this expectation. So, this is something I will also show on later slides. So, about the linearization. So, again, here's the expression. And this is something that was also done in this paper. Okay. I'm sorry. I didn't want to switch here. No, too far. So, one uses now the fact that we are looking at the linear function of the signature. Looking at the linear functional of the signatures and the signatures in the free Lie group, and therefore, the linear functionals on the signatures form an algebra. And the corresponding way to form a polynomial into a linear function of the signature is by using this shuffle product. So we can write the square of this linear combination of signature components as a linear combination of higher order signature components. Components. And then we can further linearize this integral by recalling that time is just a component of the path. So integrating with respect to time is just integrating with another component of the path. So this will again give a linear combination of iterated integrals, which on the left side of this bracket means just I have to concatenate these words by one, meaning I integrate one more time. Meaning, I integrate one more time by time. So we have linearized this term. And okay, I didn't mention this, of course, eventually the idea is if I can linearize this whole term and write it as something of this form, some linear combination of words applied to the signature, then I can take out this linear combination by out of the expectation due to the linearity. Expectation due to the linearity of the expectation. So, this is sort of the idea where we want to get to. But now we have this exponential here, which is not a polynomial. So, we cannot just easily do a trick like this to transform it into a linear combination. But of course, exponential is approximated by polynomials, and each term in the power series expansion of the polynomial can be linearized in this way. Polynomial can be linearized in this way. So, this would be the corresponding linearization in some sense of the exponential, taking these shuffle powers of these words L, meaning I take L times the shuffle product of L with itself, there, K times the shuffle product. But then I obtain here an infinite sequence of multi-indices, if you want. You want applied to an infinite sequence of numbers, so tensor series given by the signature. So then it's not clear that this converges. But we have to truncate our signature at some point anyway. So we can apply this sequence of words here to the truncated signature of level n. So only iterated integrals up to level n. And then it holds true. And then it holds true that taking then the limit for the truncation to infinity, we really obtain that this linearized expression converges to this exponential. So this can be used now here to linearize this at least approximately. So then we have also linearized this exponential here. And then finally, we want to linearize also the integral with respect to dy, and this is where. And this is where possibly the question from before comes in. So, if we want to linearize also this integral, we have to put some further assumptions on y. So, we will also assume that y is also can be expressed as a linear combination of signature components. So, this is this result here. So, as is just said, we assume here that y is given as a linear combination of signature components. Combination of signature components satisfies the same boundedness condition, and then we can approximate the solution to the optimal stopping problem by essentially optimizing this linear expression. So I did what I said on the previous slide. So after linearizing this term, we can take out this linear combination of the expectation, and we only have Expectation, and we only have to apply something to the expected signature. So, this, I will talk about these limits here in a second, but just conceptually, this is very nice because it means we have to calculate this expectation once. And then the optimization problem is completely independent of the probability. You have to calculate the expectation once, and the rest here, this is a Here, this expression here, if I evaluate it, given I have this expected signature, will be just a polynomial in the coefficients of this words L. So it's just a standard polynomial optimization, deterministic polynomial optimization problem. But then, of course, this is given that we did the truncation and we also truncated over this set of Over this set of words here. And then we had to also do a technical step by assuming that our p-variation, a norm of the underlying signal is bounded, and then also letting this bound to infinity. Then we can indeed approximate. So, this is nice theoretically. Unfortunately, numerically, this didn't work so well. So So the problem is essentially that you want to choose n very large and k rather small in order to decrease this error that comes from linearizing the exponential here. But then increasing n to large, you will very quickly come to the maximum capacities of your computer. So we didn't manage to really obtain good numerical results from this expression. Results from this expression. So since we were not satisfied with this numerics, we went back again to this expression. So this is what I mentioned earlier and wanted to do optimization directly from this expression. So again, we truncate the signature and we also discretize these integrals. So we look at a discrete time optimal stopping problem. This will, in the end, This will, in the end, all lead to the fact that we, if I disruncate and I discretize, leads to the fact that I will calculate lower bounds. So it's clear that I'm restricting my stopping times further, so I will at least get a lower bound to the optimal stopping problem. And then one can directly from this expression start to do the start doing numeric. So we can simulate a signature, the truncated signature, calculate an Monte Carlo. A Monte Carlo approximation of this expectation and use a stochastic gradient descent method to maximize this expression. Now, if you do it from this end, there is no need to restrict to linear functionals of the signature here. Since we are not using the shuffle identity of the signature, there is no need to really restrict to linear functions of the signature. The reason why we use the signature is because we think it's. Signature is because we think it's a good representation of the path. But then we can also use non-linear functionals here of the signature. And the most popular choice, I think, is using deep neural networks. So we propose here this deep signature stopping rules. So instead of using a linear functional of the signature, we use a deep neural network applied to the signature and parameterize the stopping time in the same way. In the same way, and then numerically speaking, if you apply a non-linear functional to the signature and you're only interested in some sense in the information contained in the signature, it also doesn't make sense to look at the signature itself, but to look at the log signature, because it's a much more efficient representation of the signature in a linear space. So the log signature is in the Is in the Lie algebra or in the truncated free Lie algebra and takes much less space on a computer if you save the log signature. Paul, just let me know. You have one to two minutes left. Okay, yeah. Okay, thank you. So essentially, we have the same approximation result for this type of stopping rules, which essentially follows from the previous result and the universal approximation theorem for deep neural networks. For deep neural networks. So let me just give this numerical example. We looked at stopping of a fractional Brownian motion. So fractional Brownian motion was also introduced in other talks. We have one parameter which gives the regularity of this fractional Brownian motion for h equal to one-half. We have a standard Brownian motion, smaller values of h, more irregular paths, and larger value of h, more regular paths. More regular paths. So we really try to find a stopping time which maximizes the value of the fractional Brownian motion at that stopping time. So it's in some sense a toy problem, but it's interesting because fractional Brownian motion is neither Markov for H not equal to one half and neither Simon Martingale. So these are the results that we obtain here. We use different Here, we use different time discretization, starting from 100 time discretization points to 10,000 discretization points. And we have one benchmark value from the literature, which is a paper by Becca Caridito and Jensen. They, just to quickly mention how they calculated these stopping values, so they also introduced a method that is based on deep neural networks. Method that is based on deep neural networks, but for Markovian situations. So, in order to represent your fractional Brownian motion as a Markovian path, you augment the time by the process by its entire past. So they looked here at a 100-dimensional process. So, the Brownian motion, the fraction of Brownian motion plus its whole past, which is then a Markov process, which then allowed them to use their result. Use their result, their methodology. So we see for the same time discretization grid, we approximate these results, but we can also use it for finite time discretization grids. And this is just the last point that I'm trying to make. So we fixed here one truncation level of the signature. And that means that increasing the number of points in your time grid, the number of the input layer of the deep neural network. The input layer of the deep neural network will always be the same because the truncation level of the signature is fixed. So the log signature here for truncation level three has exactly five components. So we feed a five-dimensional vector into the neural network. And that is independently of how fine I choose my time grid because that just defines how fine I calculate these integrals in the signature. But the mouse. In the signature, but the amount of integrals will always be the same, and therefore, also the amount of parameters in the deep neural network will be the same. So, this is to say that it scales very nicely for defining the time grid. Okay, so I end my talk here and I thank everybody for the intention. Thank you very much. Are there any quick questions? Right. Um, not let's uh thank Paul again.