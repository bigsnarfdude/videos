He's the person on the right there. That's his natural element right there, competitive programming. He's laser focused on solving a problem there. So I did this with him last summer while he was holding an NCERC USRA. But he's now at Waterloo as a master's degree. Thankfully, we've all been primed with the concept of graph coloring. So this is a very normal. So this slide doesn't go to waste. Here's my pictures of graph coloring. We've got a planar graph that's four colored here, and there's a clear. Planar graph that's four-colored here, and there's a clique of CS4. Bipartite graph, complete graphs, and if you paid attention to the title of the talk, this is a choral graph, and I'll define that soon. But graph coloring, the primary quantity of interest here is the minimum number of colors required to color the graph, which is the chromatic value. Now, part of my talk is about, look, my talk is focused on algorithms, so let me talk about some of the On algorithms. So, let me talk about some of the algorithmic successes and challenges with graph coloring before we segue into what I call minimum sum colorings. So, for two coloring, we can recognize in two colorable graphs with a simple linear time algorithm. We often teach it in our second year algorithms course. And there's other graph classes, so if the maximum degree is three, we can compute the chromatic number with, again, a somewhat simple linear timing algorithm. And there's other structured graph classes. I won't go through. Structured graph classes. I won't go through all these definitions here, but perfect graphs and many subcategories of it, graphs of bounded tree width, and so on. But there's algorithmic challenges. So unfortunately for a general graph, we don't have any efficient. So polynomial time algorithm to compute the chromatic number unless P equals NP. Even for other structured graph classes, we can't do that. Well, we all know that planar graphs are four-color of all the. Know that planar graphs are four-colorable, it's actually hard to determine if three colors suffice or if you're required to use one. Four regular graphs are hard, line graphs of non-bipartite graphs are hard. And I'm actually interested in estimating these parameters. Turns out that it's even hard to estimate the chromatic number. So just think about this. A very simple, a trivial estimation of the chromatic number is n. And throughout n will be the number of notes. I asked, if I. Of nodes. I asked, if I give you the value n, you know it estimates the chromatic number of the factor of n. But unless, think of this as p equals n p. So unless p equals n p is some cousin of it, then you can't estimate it within an n to the 0.999 quantity. No efficient algorithm is guaranteed to do that. So there's practically nothing we can do from the algorithmic stance for a general graph in polynomial. Now, my focus in this talk is minimum sum colorings. So, these are still graph colorings, but now we'll think of the colorings as integers, positive integers, and instead of minimizing the number of colors, let's minimize the sum of the colors. I like this example here. It's a bipartite graph. This coloring here, which uses more than two colors, has a sum of colors being 11, but if you restrict yourself to just the two colors, because it's a two-color row graph, you can Because it's a two-color ograph, you can't do as good as the. So sometimes to minimize the sum of the colorings, we need to use more colors than the chromatic number. And there's a nice little view of this. Maybe the nodes of your graph correspond to some jobs that you've got to complete, and edges indicate conflicts. I can't work on these two jobs at the same time because they conflict over resources. And so we're asking you to minimize the average or total completion time. The average or total completion time of all jobs with a minimum sum coloring. So that's minimum sum coloring, and that's what this talk is about today. So what's known, of course, just like with the chromatic number, we can't estimate the minimum sum coloring in an arbitrary graph within any sensible ratio. If you could, then practically speaking, P equals NP. So of course we want to study structured graph classes. Graph classes. And we started with trees, one of the simplest graph classes. At least you can do this in polynomial time. In fact, there's a nice little observation that was made that you never, even though you might need to use more than two colors to optimally mint some color a tree, you'll never use more than logarithmically mint colors. And there are examples that force you to use logarithmically minted colors, like a recursive construction of them like this. But if you know this, then you could give this as an annoying homework. As an annoying homework exercise in algorithms course, given this fact that you can compute it with dynamic programming in close to linear tree, so at least we can optimally mint some colored trees. And the idea would extend to graphs of bounded tree as well. But those really constitute the only graphs that we know how to do this efficiently. It's a surprisingly hard graph. So even in bipartite graphs, so two graphs. So even in bipartite graphs, so two colorable graphs, it's hard to compute an optimum min some coloring. I'm interested in approximations, and so I'm going to constantly refer to things like some constant approximation or some value approximation. So in bipartite graphs, there is an approximation with guarantee pretty close to one, but not quite one. And when I say something like this, I mean it comes up with a sum coloring solution for the graph. And the total sum of all the colors is at more. The sum of all the colors is at most this constant times the optimum min sum colour. Whenever I say sum approximation, I mean you're this close to the optimum min sum colour. And as a warm-up to the ideas of approximation, even though the natural two coloring doesn't solve it exactly, it can be used to give some bounded approximation. If you just consider the natural two coloring, the bigger side gets one, the other side gets two. Well, then the total number, the sum of all the colors is at most one point. The sum of all the colors is at most 1.5 times n because at most half the nodes were filled with 2. And of course, in any solution, in particular the optimum, it's at least n. And in fact, it can be as bad as 1.5 times n as well. So this algorithm, you can't analyze it any better than this. You have to be more clever if you want to drag it down. Another nice approach that was taken before we get into our new contributions was: suppose you're working in a group. Suppose you're working in a graph class where you can at least compute maximum size independent sets in polynomial time. Perfect graphs, qualify graphs, a number of graphs. Well, if you just take this simple approach of peeling out maximum independent sets and coloring those independent sets iteratively, so maybe we take these five nodes here and color them one, then these four green nodes and color them two, and then the remaining node gets color three. You can prove that this approximates the minimum sum color. That this approximates the minimum sum coloring of the graph within a factor 4, and that it's also tight in some examples. It's as bad as 4 in some examples. So it's something at least. What's interesting about this is if you try to apply this idea to the chromatic number itself, you can be off by a logarithmic factor, but never worse. So somehow this is performing better for the min sum objective than just minimize the number of colours. And here's a quick summary. Don't expect everyone. And here's a quick summary. Don't expect everyone to read it. I'm really just trying to impart the idea that people study special graph classes from the min some coloring perspective. And notice that for most of these, not all, but for most of them, these, we can compute the chromatic number in polynomial time. But we just need to have approximation, approximation, approximation, except for this one class I discussed at the start. And it's hard to compute a minimum sum coloring in all of these classes except exact. It's provably NPR. Our addition to this table here, so my work with Ian, is a 1.796 approximation for chordal graphs. Now, chordal graphs, first off, they fall under the class of graphs that you can compute max-independent sets. So we just had a four-approximation, trivially that way. But they also fall under the class of perfect graphs, which I won't define for this talk because I don't need to. So they already had a slightly better approximation, but we drive that down to the ratio that's known for integral morphs. Introduce. So let's get started here. What is a chordal graph? Simply, there's no induced cycles of length more than four, at least four. Sometimes people call them triangulated graphs, because in any cycle of length four or more, you'll see a chord. This is a chordal graph. It's a little hard to tell. You have to think about it, but there's easy ways to spot chordal graphs. There's efficient algorithms to determine if a graph is chordal or not. In fact, they run into Determine if a graph is portal or not. In fact, they run in linear time. And just as some motivation, sometimes my conferences like to see things like this. 95% of interference graphs that are encountered in the compilation process of the Java 1.5 library turn out to be chordal graphs. And that's not just some spurious example. Graph coloring is used to optimize, in compiler optimization, quite directly. So this is. So this is to those who like compiler optimization. Portal graphs. All right, so let's start digging into this now. I do want to discuss previous work on one particular algorithm before we segue to our work. This is by Helderson et al. So this is the approximation for interval graphs. Let me just go back a bit there. Notice their ratio matches the. Their ratio matches the, well, our ratio matches the interval graphs ratio. Let me talk about how they got their approximation for interval plus. So, interval graphs are an even more structured graph class in that not only can you compute maximum independent sets in time, but you can find a maximum k-colorable subgraph, maximum being the number of nodes. So, if this, an interval graph is, of course, vertices are represented by intervals, and two vertices are considered adjacent if their intervals share. Adjacent if their intervals share a common point. So this is a collection of intervals. I've grayed some out saying I'm not going to select them, and the remaining ones are three-colored in this case. And there's a simple greedy algorithm that will find a maximum k-colorable subgraph of an integral graph. So what they show is that in any graph class where you can compute this efficiently, you can get a 1.796 approximation for the min sum color problem. Approximation for the min sum color would probably be much better than the for approximation if all you know is computing maximum dependent sense. And the algorithm is pretty simple. So maybe just think of it this way. For now, what if we just considered powers of 2? It's something more general, but just think powers of 2 for k equals 1, 2, 4, 8, 16. Find a maximum k-colorable subgraph. Find a maximum k-colorable subgraph, color it with the next available integers, and pull them out. So it's kind of like the greedy algorithm, but you're coloring geometrically increasing k-colorable subgraphs. It's basically the generalization of greedy. If you stuck with powers of two, you would get a four approximation, yet again, not much of an improvement. But they also played with the base of the geometric scaling. They showed that if you pick the offset there a little bit differently, then you can actually get the guarantee down to this. Then you can actually get the guarantee down to this quantity, which we chose this base here to minimize this. So, this is how they get this approximation algorithm. And the intuition as to why this works, think about the powers of 2 again, is by time, let's say, 2 times 2 to the j, you're guaranteed to have colored at least as many nodes as the optimum has by time 2 to the j. Because that last step where you did 2 to the 2, Because that last step where you did 2 to the j, you'll get at least as many as what are left uncolored from the optimum solution by that time. So you lose a factor of 2 that way, but there's also this aspect ratio problem in that you lose another factor of 2. Anyway, so we don't need to know the analysis of this one, but this is just what they do. So when we looked at this problem from the perspective of chordal graphs, we of course considered this framework, but unfortunately the maximum But unfortunately, the maximum k colorable subgraph problem in chordal graphs is hard as well. So we can compute maximum independent sets, but if we want maximum k colorable subgraphs, it's a hard problem. So we can't just use it directly. Now the maximum k colorable subgraph problem before we got to this did have some approximation. If you just pick a maximum independent set and pull it out, you repeat up to k times, you get some constant approximation for the max k-colorable subgraph of. But it's not good enough. So first off, this. But it's not good enough. So, first off, this analysis, the algorithm and its analysis here completely fails if you have approximations. Even if we had a much better approximation, we couldn't use it in their framework. It just wouldn't give us what we want. And second, well, if you think about what, if you just use this approximation anyway, there's different analysis that shows it's a four-approximation, just the original bootyhead told us. We're just stacking these geometric groups together, we're really just running. So, this doesn't work. So, our first main contribution is coming up with a framework that's robust, one where you can incorporate approximations for k-colorable subgraphs to get approximations from inside colors. And kind of an informal version of our statement is that if you can get a really close to one approximation for the maximum k-colorable subgraph, so you can get 99.999% of the optimum. 99.999% of the optimum, then you can use that in our framework to recover the same approximation as there was already for each mobile point. So you can get down to that. But of course, that's a funny statement. This is the real stalact. If we have some row approximation, that's even allowed to cheat. It's allowed to use a little bit more than k colors, off by some gamma, but we're still measuring the number of things you've colored compared to the optimal k colorable subgraph. So if you're allowed to cheat a little bit, and you just have an approximation. Cheat a little bit, and you just have an approximation for that. Our framework gives, well, this tells us how to optimally select the base for the geometric scaling, because we have a geometric scaling approach as well. And this would be the guarantee. In practice, the way to use it is to work as hard as you can on the max k colorable subgraph problem and then see what the outcome choices simply. Alright, so we come up with this robust framework. This is my only slide on it, and this is the slide where I lie the most. This is the slide where I lie the most. So, the way we approach this is using linear programming and duality in conjunction with our approximation. Really, what we compute, the deliverable for what we compute, is a distribution over k colorable subgraphs for every k. Now, this distribution is in quotes because the total probability of all the items is slightly more than one. And the sum of expected still in quotes here, so there's a lot that it's not hard. There's a lot that it's not hard, it's just I just didn't want to go into it because this is because the other part is the part I want to go into. But the sum of the expected times of each vertex using our distributions is at most the optimum. So if we run the same algorithm as before, but whenever we need a k colorable subgraph for some geometric value k, we just sample from this distribution with the probability scale, so the sum to one. And if you do that, then we recover the same constant. The main Constant, the main fundamental difference between their approach and ours is they just kind of show on average vertices are still waiting like some bounded number of times that they would be in the optimum. But we show on a point-by-point basis, from a vertex-by-vertex basis, that the expected color receives is at most this constant times its expected waiting time in these distributions. So this is just linear programming. The variables of the linear program describe the probability distribution. Program to describe the probability distributions, and while we have many, many different k-colorable subgraphs, we can do this in polynomial time. If you really want the details, I'm happy to talk about it later. But that's not the part I want to focus on. I want to focus on how we actually address the max k-colorable subgraph problem in chordal graphs. It's more fun to talk about it. What we show is this. For any constant epsilon, you can get a 1 minus epsilon approximation for this problem. The running time degrades as epsilon. The running time degrades as epsilon gets close to zero, but if we regard it as being bounded by a constant, it is polynomial time. And we can even improve the dependence up there. I just don't have it here. So while we can't solve it exactly, we can get really, really close to the optimum in polynomial time, and we get to decide the time an approximation for alpha. We build on previous work for this, though. So while I said it's a hard problem, you can't. So while I said it's a hard problem, you can't find a maximum k colorable subgraph in general. Turns out you could if you can regard k as a constant. So I can find the maximum two colorable subgraph or maximum three colorable subgraph in polynomial time. So Gannicakis and Gavril leverage this one property about chordal graphs is that you can recognize, I think I realize a chordal graph as the intersection graph of sub-trees of a tree. So here, these aren't colors in the coloring, that's more just to help. Colors in the coloring, that's more just to help point you to different subtrees here. So, like for this light blue one, the light blue subtree actually ends up touching all other subtrees, and it's adjacent to all of them. Whereas for this blue one here, it's only adjacent to green, light blue, and yellow. Well, this blue is only adjacent to green, light blue, and yellow. It doesn't touch the other two. So, a graph is chorded if and only if it can be realized this way. And what uh And what they exploit is this tree-like property in a dynamic programming algorithm. And really, just if you imagine it as a rooted tree, pick a node V, guess in some sense the trees, the at most K trees that would share that node, and then break that down with dynamic programming. So this was no more. This is really our contribution. We show that as K increases, it becomes easier to approximate. And it's true. Approximately. And it's truly polynomial time. It doesn't depend on k in the exponent. It's just n to some constant time, no matter how big k is, even if it's like square root n. So you can get really close to the optimum if we're asking for more and more, if we're allowed to use more and more colors. Now, given what we do, we can recover our 1 minus epsilon just by seeing, well, is the k we're asking about at most some function of epsilon? If so, it's bounded by a constant, so just from It's bounded by a constant, so just run their algorithm. Otherwise, use our new approximation, which will be at least a 1 minus epsilon approximation. And let me finish off by describing what we do with some detail. We leverage a different property of chordal graphs, that they have perfect elimination orders. A graph is perfect if and only if you can order the nodes so that the neighborhood to the right is a clique for any node. So, again, look at this. So again, look at this blue node here. Its entire neighborhood is not a clique, so like this blue and dark blue and this purple are not adjacent. But if you order them this way, all of its neighbors to the right form a clique. So that's a perfect elimination order. And we're going to exploit this in our algorithm. The ordering can be computed in linear time. If you're just given the graph description, you can order it with a modification of a standard graph search. And just one little property, this is what we focus on, is that a subset of nodes will induce a k-colorable subgraph if and only if for each node you picked, its neighbors to, you picked at most k minus one neighbors to the right of it. So we're going to chase this in our final L. So we do use linear programming here, but it's in a relatively simple way. For every vertex, For every vertex, let vi in the ordering, so think i as its position in the ordering. We'll let xi be a variable that indicates whether or not we'll keep it. So we want to maximize the number of nodes we keep, but we're going to use the perfect elimination order to assert some constraints. So what this says is for every node, between that node and its neighbors, you should have picked at most k. But instead of asserting things your integer, because that's just a restatement of a hard problem. Because that's just a restatement of a hard problem. I'll allow us to use any fractional value between 0, any real value between 0 and 1. And now we have an optimization problem that we can solve in polynomial time. It's just not perfectly modeling the original problem, because some of the x's will be strictly between 0 and 1. But it's a linear programming relaxation, and the 0, 1 solution corresponding to the optimum solution is feasible, so our optimal solution to this relaxation will be at least as good. Will be at least as good in terms of its objective function value. What we do is we round this randomly. So for every node in order of the partial elimination order, so like from right to left, we're going to flip a coin. Its bias is going to be just a little bit less than xi towards heads. So if this part was gone, if we flipped a coin with bias xi towards heads, coin with bias xi towards hedge without this small little scaling here. You can imagine in this cons for any i here, the notes participating to the constraint, the expected number of heads you flip is k. So in expectation, you're getting what we need. We're going to scale that back just a little bit so we can use a good concentration on the event of having at most k. All right, so we're going to flip a coin with that bias towards heads. If we can add it to what we're building so far, What we're building so far and remain feasible, or so remain paid colorable, we'll do it. It's as simple as that. Okay, so let's just analyze this random process put beer. For every vertex, the expected number of heads you flip from its right neighborhood, excluding the vertex, well, just from the right neighborhood, is at most k minus this, the k to the one-third. So at most, sorry, that should say two-thirds there. Sorry, that should say two-thirds there. It should be a two-yielded by this. So at most k minus k to the two-thirds. And so then this should also say k to the two-thirds. And that pulls you back far enough from k that you can use any standard analysis technique for these independent coin flips to say that you're probably not going to exceed k. The probability that you sample at least k from the right neighborhood is going to be bounded by something that vanishes. Is going to be bounded by something that vanishes as k grows. In this case, the second moment method suffices. You can just use Chevy Chev's inequality and a simple observation that the variance of the number of heads you flip from the right neighborhood is going to be bounded by k, the variance of that rule. So that's standard concentration valves. And so finally, we have what's the probability that we actually keep vi? Well, if it's the number of values, If the number of heads from its right neighborhood is smaller than k and we flip heads, then we get it. That's an event that upper bounds the probability of, sorry, that lower bounds the probability of us succeeding, but that itself, but they're independent events, so this is the probability we just analyzed to bound the number of heads from there, and this is exactly the probability we sampled. Excellent. So we get every variable, a vertex. Every variable vertex appearing in the set with probability almost equal to its x value, so the expected size of our k colorable subgraph is nearly this. We can derandomize this, but I will skip that. By derandomize, I mean there's an efficient deterministic algorithm that we'll find with the same size, but it's using standard derandomization techniques. So that's it. That's how we get our algorithm. So the next thing that's at least kind of in my sights is: can we do better in perfect graphs? My sites is: can we do better and perfect graphs more generally? And we can use our framework, but we need a slightly better approximation than what's known for the maximum K-color of the subgraph. So, 0.704 would suffice. We have a 0.632. And more generally, this latency constant is a bit tesky. It's been showing up in quite a few problems. I'm wondering if it's fundamental, meaning it's hard to do better than that for some reason, or do we just need to be more clever with a different approach? Thank you. Any questions? Zach, did you do any experiments to show how often relaxation you actually have at all? Okay, I haven't done a lot of experiments, but I do know lots of the reasons why we get integer solutions in linear programs. But I can't say in a practical sense why we sometimes In a practical sense, why we sometimes fluke out when internal contributions? Probably a stupid question, but it's better than 0.704, isn't it? So we're talking about maximization here. So we're getting 0.632%, like 63% of the outcome, whereas if you nice to get a little bit more than 70%, it's not. Alright, well, let's thank the speaker again. Thanks. 