A great workshop. I want to maybe focus on the symmetry and graphs part of the workshop today, speaking about some very highly symmetric combinatorial objects. And I should say that this is joint work with Luke Postel from Waterloo and Tom Kelly from Georgia Tech. So why don't we go ahead and get started? So to start off with, what is a combinatorial design? And these have origins. And these have origins back to the 1800s in geometry, applications and coding theory. They're really fundamental objects. In case you haven't seen them before, let's just go through and rigorously define them. So if we have integers n, q, and r, we're going to say that a Steiner system is a collection of Q element subsets of some ground set of n vertices. Of n vertices with an additional property, namely that if you look at every collection of our elements, they are living in one and exactly one of your collection of q element sets. So even for q equal to 3 and r equal to 2, there's already a lot of interesting questions you can ask. These things are really heavily studied. They're sort of referred to the literature. They're sort of referred to in the literature as Steiner triple systems. So, if I say triple system, that's really what I mean. And if you haven't seen these before, you might well be asking, well, what do these things look like? Do these things even exist? And these are perfectly valid questions. So, concretely, for n is equal to 7, let's look at the Fano plane. So, here we have a collection of lines and points. And points in the plane. And here we're taking as our collection some series of lines. So these lines contain three points and have the additional really nice property that every pair of vertices lives in at least one and at most one of these lines. So exactly one of these three element sets. And in general, And in general, even in 1847, we had a characterization for when these things existed. Kirkman showed that you have triple systems if and only if n is congruent to one or three mod six. So what about more generally? And I mentioned designs in the title. Maybe let me say a few words about designs. So if we So, if we could generalize this notion even further, let's think about n qr lambda being integers. And here we're taking q element sets from a ground set of size n with the same property, except we're modifying it slightly. These are element subsets instead of belonging to exactly one element. Let's say they belong to exactly lambda of these elements. Now, I'm a graph theorist. You can secretly be thinking. You can secretly be thinking of these things as appropriate hypergraphs and thinking about simple hypergraphs. Lambda equals one is kind of a natural thing to look at, but you could generalize this and ask this for lambda. For what NQR lambda can you find in NQR lambda to find? And sort of you'd need certain obvious divisibility conditions to even be a candidate for having an. Even be a candidate for having an NQR Lambda design. And you say your N is admissible if you have these divisibility conditions met. So the big conjecture in the area is that for sufficiently large n, you can actually find an NQR lambda design provided n is admissible, provided you have some number of divisibility. You have some number of divisibility conditions. And there's been a lot of work in this direction. So, over 100 years later, so the triple systems, again, you were thinking of Q is 3 and R is 2. Hanani thought about 4, 2, 4, 3, 5, 2. Wilson was able to show it for R is equal to 2 in general with a really lovely algebraic proof. And interesting. And interesting in its own right is the technique that Rodel used to show instead of actually finding NQR Lambda designs, finding really, really large partial ones. But the semi-random process, the robot rotal nibble, is really an important technique in graph theory today. But till this decade, until 10 years ago, years ago for the large values of r we didn't really know much and then in 2014 this really lovely manuscript appeared on the archive wherein kibosh asserts that the existence conjecture is true and i want to say a little bit um his method is also algebraic in nature he uses randomized algebraic constructions Algebraic constructions and he provides a one-step approach, which is really nice. But subsequently, a purely combinatorial proof was by Block, Kuhn, Lowe, and Austis. They utilized a technique called Chalchan. Called Chilchan and how weatherproof it's do a good job. I don't know my internet says it's unstable, so hopefully you can still hear me. Okay, so on leap day 2024, Luke Postel and I put a manuscript on archive with a new proof of the existence. With a new proof of the existence conjecture using a technique that we call refined absorption, with the benefit that it's both combinatorial and a one-step approach. And I'll maybe say a few words. So on the same day, we released some of the results, us, some with Tom Kelly, a series of four papers using refined absorption and art. And our goal, kind of setting out here, was not to provide a new proof of the existence conjecture. We really were looking at a more general thing, the high gerf existence conjecture, which I'll get to in a second. But we did think it was worth writing this separately and sort of outlining a roadmap because, as you'll see, at least the flavor of in this talk, all of our subsequent applications follow the same roadmap. Applications follow the same roadmap, and some of the steps require extra machinery on top of it. But we wanted an eye for people who wanted to do things of a similar nature to be able to follow a simpler roadmap than what's outlined in the other paper. So the main goal was the high girth, but along the way, we developed this new proof of the existence conjecture. So I'm So I'm going to say a few words about, and some of you are experts in absorption, and some of you may not have seen it. So this may go a little bit quickly if you've not seen absorption at all, but bear with me. So maybe let's switch gears. Let's talk about uniform hypergraphs. So meaning each edge has exactly our vertices, and we're going to denote. Vertices and we're going to denote cliques here, KQR. That's a complete uniform hypergraph. And let's throw up a definition. You have a KQR decomposition if you can find a partition of the edge set into copies of cliques. And why am I bringing this up? This seems a bit different. Well, if you've seen designs before, Designs before you've probably seen well, these things are intrinsically related. There's if you can find a KQR decomposition of a complete host hypergraph, then you can find a Steiner system and vice versa. And just I wanted to also mention, even if you're not dealing with a complete host graph, there's still a natural notion of divisibility. Divisibility of having certain necessary conditions. So, in general, absorption is a very powerful technique. And you often are doing some problem in hypergraphs, either finding a large spanning structure or finding some sort of edge decomposition, maybe with certain restrictions. And perhaps it's ambitious. And perhaps it's ambitious to find a perfect solution, a complete decomposition all in one go, but you could find some sort of approximate solution and then hope to be able to fix it, to make it a perfect solution. And one strategy for this is you could set aside some portion of your hypergraph that will help you later. Will help you later. So, you set aside some portion, you find a partial solution, and then you use the absorber to fix whatever things, whatever stragglers were left over and actually have a complete decomposition. So, formally, let's say that you have some leftover, but potentially decomposable hypergraph. It's KQ. Decomposable hypergraph, it's KQR divisible, that you want to fix, that you want to come up with a complete decomposition for. Then a hypergraph A is said to be an absorber if, well, your hypergraph lives on a subset of the vertex set of this A, but you don't want to span any edges of A. So you live sort of. So you live sort of in the same area, but L itself isn't going to span any edges of A. It's independent within A. And if you have the property that A has a clique decomposition and L union A has a clique decomposition, then the idea is this is wonderful. You can fix this leftover by sort of doing some sort of switching and getting a complete solution. A complete solution using your absorber. And you might ask, well, do these things exist for our problem in particular? How does this work? Well, Glocklo and Austa, Glock, Kunlo, and Austis were able to prove in their original proof of the existence conjecture that if your leftovers are divisible, then you can find an absorber, say AO. Absorber, say AL, for A. So you should be able to do this. Now let's even ask for something stronger. So their proof is combinatorial, but they're having to do some sort of iterated process. They're trying to really control where the leftovers end up and have a good handle of where the bad things in the hypergraph are with this iterated absorption process. Let's ask. Um, let's ask for something even stronger. Not just for one fixed leftover to have an absorber, but say you have some target X that you're gonna push your leftovers into, and say you have an omni-absorber for X. If within X, every single possible candidate for leftovers, every possible KQR divisible subgraph has a KQR decomposition. A KQR decomposition of L-Union A. This would be wonderful. You normally don't know what exactly your leftovers are going to be, but you know they live in a certain area. And depending, so you have leftovers like L1, L2, L3, etc. Maybe you want to then be able to accommodate any of those. Well, this is not surprising that they exist. Namely, you go in and look at L1 and you Look at L1, and you have some absorber AL1. Then you look at L2 and then you have some absorber AL2, and so on. You can take a series of vertex disjoint private absorbers and then take that as your omni absorber. And the idea here is if your problematic target is small enough, this is wonderful. If you know that the leftovers are going to end up If you know that the leftovers are going to end up in a very small region, then you can accommodate any possible situation that'll arise by making really an absorber for each of these things individually. And this thing, though, is exponential. You know, you're going to have two to the size of x to the r or something thing. So it's maybe not as efficient. One of the drawbacks here is by doing this, By doing this, yes, you can account for any leftover, but you really need x to be small. If you're constant, wonderful. If the size of x is constant, wonderful. If it's even polylog in the number of vertices, you can get away with it. But like, can we hope for more? Could you be a bit more efficient? So, construct something polynomial in the size of x or even in the max. Or even in the max degree of x, something of that nature. And more than polynomial, could you even hope for linearly efficient? So, this is the big innovation in our paper. This is really the core ingredient that constructing omni-absorbers that were efficient enough allows us to do all the applications that we're going to want. So, towards this end, let me just say here when I'm talking about degrees, I'm going to Talking about degrees, I'm going to really be meaning r minus one degrees. So the idea is: so, this is a bit technical, but if you have your x, if the r minus one degree is bounded by n over some constant, then you can find an omni-absorber for x that also has bounded max degree and bounded by some constant times the degree. So, another way. So, in other words, you can find something that's linearly efficient in terms of even the max degree, but the condition on X is not necessarily that it's small, just that it's dense enough, that you can push all of your potential leftovers into a target that's dense enough is the key idea. And then you're good. So, there's a four-step. So, there's a four-step approach to our proof, and maybe let me say a few words. So, the first thing you do is you pick your target. So, you just pick this uniformly at random. You use the omni-absorber theorem saying if you could push the leftover into this target, you can deal with it. Then you have to be a little bit careful. And we, the other proof, the Glockun. Proof, the Glaucu and Loostis also have to do something similar in theirs, and we actually use their result, their lemma in our thing. But we want to be applying a one-step semi-random process. And to that end, we need our hypergraph that we're working with to be not necessarily regular, but we want to be very careful. But we want to be very careful about the degrees. We don't want it to be too irregular. So we've picked our target, we've picked our omni-absorber, and then we look at the rest of the graph, and that's G. And G itself has to be amenable to doing some sort of semi-random process. So, here you could do this a few different ways. You could do what we call nibble with reserves. You could also just run. You could also just run more irregular nibble and do some sort of cover down. But you do something here and find a partial decomposition with the key property that within G itself, we're good, we're happy, we've covered all of the edges, but there could be some problematic things within X. So we worry about what's happening. So we worry about what's happening in X, this target. I mean, if you luck out and just do the whole thing, great, but you are probably going to have some weird leftover happening in X. But now I claim we're actually done. So why are we done? Well, I claim that the leftover is going to be divisible. And then because we had an omni-absorber, then we're going to be able to find be able to. Find be able to fix it with this omni-absorber. So we take our original partial decomposition and throw in all these extra things that are coming from this decomposition of A union L. And that gives us our complete decomposition that we wanted. And I will say, in practice, this four-step approach is the one that we use in all of our That we use in all of our applications. And really, steps one and three are kind of standard and don't really change much. But really, the work goes into possibly two and four if you need something a little bit more clever. And in particular, with two, here we're dealing with KQR decompositions. But what if you wanted a situation where instead of KQR decompositions, you want KQR decompositions. You want KQR decompositions with additional restrictions. Then, now, all of a sudden, we have to be a little bit more clever and a little bit more careful with how we go about doing this process in particular. So, I said that this is called refined absorption, and this notion of an omni-absorber is maybe a nice way to talk about this concept, but maybe let me just try to say a bit about refining. Try to say a bit about refined absorption. So, for an integer C, we say that an omniabsorber is furthermore C refined if you can cook up a family of KQR decompositions of H and L, where you look over all possible KQR divisible L's. And the key thing here is that within this collection, you want Collection. You want the property that every single edge, E, is going to be contained in at most C different cliques amongst all the different decompositions of this family. Each edge knows it's in constantly many of these possible ways, which is really going to help us a lot in our application. So, strictly speaking, it's maybe not even necessary for the existence. Been necessary for the existence. But putting this extra restriction and doing things in a refined way will actually be necessary for, say, the high Gerf thing that I'll touch on later. And we'd like to find ones that are efficient, even linearly efficient, but also refined. And we're able to do this. And this is a lot of work, but essentially, we're doing some sort of induction on. We're doing some sort of induction on uniformity at some point towards the end, and this sort of flattening process we need this refine this to make it work. And this can be used in many other settings. And once you've proved it once, you can use it as a black box. So I don't have terribly much time left, but I wanted Time left, but I wanted to say at a high level a little bit about two of the applications, two of the other applications in the four-paper series. So there's this notion of girth of a Steiner system. It really is the smallest integer where you contain a set of k edges spanning at most some number of vertices. For Steiner triple systems, this is k plus 2 vertices. This is k plus two vertices. And Erdős was concerned about this problem. He conjectured that not only should you have Steiner systems, which we know from Kirkman, but furthermore, you can find ones of arbitrarily high girth. And there was some partial progress on this, sort of in the vein of Rodel's results on the existence conjecture. But really, there's this. But really, there's this lovely proof by Kwan Saw, Sonia, and Simpkin that was recently accepted to the annals using iterative absorption and saying that Erdős's conjecture is true. And this is the paper that we saw in archive that sort of prompted this whole exploration because we asked, well, okay, great for triple systems, this is really lovely, but what about NQR Steiner systems in general? And it turns out other people. General. And it turns out other people had also asked this very natural question. And in 2022, Luke and I came up with this matchings with forbidden submatchings technique, which has subsequently gained traction and been used for a lot of applications. Independently, another group came up with what they call conflict-free matchings, and we both solved this. Solved this approximately. But really excitingly, in February on leap day of this year, we announced that the high girth version is true. And this took substantially more work than I think we bargained for. Namely, we have to follow this proof of the existence conjecture, but for steps two and four, we have to do more. Have to do more. So, for the Omni Absorber Theorem, we actually need to also ensure we're doing things in a high girth way, which is highly non-technical. Sorry, highly technical. And we need something even stronger than what we originally put in our matchings with forbidden sub-matchings paper for the nibble process at the end. So, there's sort of five extra pages of computation that are needed to do something a little bit stronger at the end. Bit stronger at the end, but the overall proof outline at a high level is very similar. So, I also wanted to mention, so Yoji touched on spread a little bit on Monday. I don't want to say too terribly much. The papers, again, are on archive. But I did want to mention this really lovely conjecture concerning binomial Q-uniform random hypergraphs. So, Kang Kelly, Kun Methuku, and also. Kelly, Kuhn, Methuku, and Alstis conjecture that provided P is at least log n over n to the Q minus R, then asymptotically, almost surely, the binomial Q uniform random power graph should contain an NQR Steiner system, provided your divisibility conditions are met. And I should mention that for r is equal to one, this was the celebrated problem, and in particular for Steiner triple systems, there's been a lot of work on this conjecture and. Conjecture and their original work ran through fractional concoli and spread distributions. And they were able to get log squared over n for Steiner triple systems. And the additional log n factor was shaved off by Kivash and independently Jane and Pham subsequently. So for triple systems, a lot. So for triple systems, a lot is known. And using refined absorption, we were able to say something for when r is equal to 2. So not quite the full conjecture, but we're able to say something when r is equal to 2. And it's possible if you understood these things deeply, maybe you could improve. Deeply, maybe you could improve this as well. And the overall proof strategy is very similar, except now we have to be a bit careful and do things in a spread way, whatever that means. So I just wanted to mention as a final wrap-up, our existence conjecture proof uses refined absorption. Uses refined absorption. And really, really, our goal was to solve the high girth existence conjecture, but along the way, developed a roadmap that I think could be useful on a variety of problems to come. So thank you for your attention, and thank you for inviting me to speak. Any questions for Michelle? Maybe quick question? How do you, thank you for your talk. I think you can see us. I get so confused. Yes, you're very small, but I can see you.