So, today I want to talk about the network-guided covariance selection and downstream applications. So, I start with the high-dimensional data, and I think so everyone here knows some things about the high-dimensional data. So, just a quick introduction of the notations. So, for the high-dimensional data, we have the data matrix, which is an N by P matrix. Which is an n by p matrix. So here I use n to denote like n samples, and p is the number of variables, or here I write it as covariates. So for the high-dimensional data, the problem is that this p is quite large and it may introduce noise. So there is a curse of dimension, right? So the problems of interest for the high-dimensional data is that, so at first, so because for the high dim, when p turns large. When P turns large, the classical methods do not work. So, at first, the thing is like: so, how to modify the classical works methods so that it can also work for the high-dimensional case. And then later, so it's like, so the assumption that so actually among all the variables, only part, only a few of them are significant. them are significant, are meaningful to the result. So the variable selection and so do the variable selection first and then apply some methods on it. So this kind of this has become a quite popular method. But now I think there are also like problems that there are weak signals, but more so but not that sparse variables. So there are also kind of this kind of research like recently that what Like recently, but for today, I just talk about this thing. So, there are tons of literature in the past decades on this field. So, for today, so I begin with an example. So, here for this example, we have observed, so in this data set, this is a data set from an online musical form. An online musical form, a platform. So the users there can make friends with each other. And also for each user, there is a list of their liked artists. So therefore, if we check like this list of their liked artists, then it's like an NYP matrix, which is high-dimensional, right? But besides that, we also have this new data source. Have this new data source, which is a mutual friendship between users. So, other than the high-dimensional data, we also have this kind of network. So, this kind of data format, I think so. This morning, it is also mentioned by Prof Jin's talk, so where he considered the citation network, and also together with the collars like the journals. Covariates like the journals, the abstracts, the years, this kind of things. So we have this kind of complex data. So here, so we are interested in this kind of covariance with network. For the covariance, if we consider the covariance data, this X data, it is high dimensional data, which again suffers a curse of dimensionality, right? And if we consider this network data, Consider this network data. Then, for the network data, so because it is a relationship with the other users, therefore, for the new data, for the new users, we have to wait a while and QA developers, the relationship, the connections with other users. So, it's very difficult to observe the network's connections with other users for the new users. Okay, and also, so. Okay, and also, so there is the degree of heterogeneity, and which means that for some users, there are very few connections, and we can hardly observe the information from them. So, if we solely focus on X or on A, on the covariance or on the network data, then they have some drawbacks and it's hard to gather statistical inference. But this kind of data is common. Is common. So, like in the psychological study, so they have like multiple data resources, but focusing on the same set of participants. And then in that case, we may treat one data resource, one data source as the network data and the others as the high-dimensional data. So, in the social platform, which I just presented, brain image data, etc. etc. So, okay. So now we said that so X and A, they have the individual problems, but because they are focusing on the same set of samples, the same samples, so they share the same latent information, the same latent information. Therefore, we hope that if we combine X and A, then we can get better result. Get better result. So, this is not a new idea. So, how to combine the network and the covariates, there are many literature. So, use the covariance to help on the community detection in the networks. There are already many literatures. And now, we consider the other direction where we want to use the Direction where we want to use the network to help on the covariance. And especially because we consider the high-dimensional covariance. So we think about how to use the network information to help on the variable selection or the covariate selection. So this is not a new idea. And it was first introduced in the CS area. So they have some literature about that. About that, and then so there are also some statistical papers on that. So, I think this one is the one from this one is on the biometric, which is like the dimension reduction for the network and covariance, such kind of things. So, however, for these literatures, so for this one, it comes. So, for this one, it concerns on the when we see that it's high dimensional. I think in this paper, the dimension is not that high, it's just like reduced from maybe 10 to 5, something like that. And other than that, other than the variable selection, we are also interested in the next step. After we do the variable selection, so how to do the next step, the regression and classroom use the variable, the selected variables. The selected variables to enhance the regression and clustering results. There, we mean there, we have the data that we observed the network A, the covariate X, and for the new data, we only have the covariates, and then we use the new covariance to do the regression, to make predictions and do clustering. So, that's the goal. Okay, yeah, so for this talk, we have we propose a network-guided covariance selection algorithm for the variable selection, where we use the spectral information of A to construct the test statistic. And based on the result, we have designed the algorithms for clustering. algorithms for clustering and regression and the results. Okay, so the algorithm. Okay, so back to the high dimensional covariance selection or the variable selection. So we have the data. This is an n by p matrix and each so x each column x j is an n by one n by one vector which one vector which gives the results which gives the observations on this single covariance we want to decide whether this covariance should be selected or not then for the covariate covariate selection so i know there are many methods but so basically our idea is that first we propose a statistic which is built on this x Which is built on this xj. And then for this statistic, we find out the p-value. Then we use a p-value to select the covariance very standard way. So, okay. So now, so to do it, we first have to give like, so to introduce like the general idea that what COVID. That's what covariates should be selected, right? Then that is related to the latent structure in the covariance. So the data matrix is X, and we let Y to be the latent information. So this Y is an N by K matrix. Each row of Y is the latent information for the I sample. Okay, very simple. Sample. Okay, very simple way. And then for the jth covariate, so if it is, so we assume that the expectation of x j, it is the latent inform, the latent information times the mean. So, okay, this M xi, this mji can be viewed. Can be viewed as the mean vector for the latent information. Okay, so okay, so if we think this y as a class labels, then this mj gives a mean vector for each class. So this mj is a k by one vector because so so then so then when they see that whether the covariate should be we say that whether the covariator should be selected or not we mean that whether this mji equals zero or not if mj equals zero it means that no matter what the latent information is so x j does not depend on y the the mean of x j does not depend on the latent information then we do not want to select this x j and if m j is non-zero then we want to select that covariate Covariant. So, our goal is to identify the set that mj is non-zero. So, now we consider like the normal distribution. If only x is available, only x is available. That's the original case where we have the high-dimensional data only. In that case, the optimal statistic would be the The optimal statistic would be the cost square statistic formed by that column. And you can see that here, the degree of freedom for this cost square statistic is in here. And this one, we can view it as some kind of unsupervised learning because we can only compare this column with the standard normal distribution. And if the latent information y is known, in that case, we can apply, for example, the marginal regression of y on x, j, and then get the result, which is equivalent to this one. So we take the left singular vectors of y, use that matrix to form a linear combination of x, y, and use it. x j and use it the new the this linear combination to construct the chi square statistic. In this way, it's kind of a supervised learning because we already know the latent information. Then in this case, you can see that the chi square, the degree of freedom for the chi squared statistic is k, which is the dimension for the latent information. Usually we have the low dimension. The low-dimension assumption, which means that this k is a constant. Therefore, the requirement, so if you check like the non-centrality, non-centrality, non-centrality parameter, it's at the similar scale. But if we check the degree of freedom, which gives which matches the noise level, then the constant is much less than n, right? So, therefore, that's why the So, therefore, that's why the supervised learning is always much better than the unsupervised learning. So, but now we know X. We do not know Y, but we know A. So, direct idea, we use A to guess, to have some information about this casi. We do not need the exact information of why because here you can see that in this. Here, you can see that in this statistic, we only need the left singular vectors of y. We do not need the exact y here. So, we want to use a to guess. So, here is to guess casi. Then use a to guess casi and where this casai is related to the latent information. So, we have to check the how A depends on. A depends on the latent information Y here. So for the network, there are multiple models, I think. So starting with, so for in the community detection literature, I think the most commonly used is a stochastic block model and the group. block model and degree corrected stochastic block model and in this case the yi is the uh y i gives the label okay yeah so why i give the lab the community labels and the expectation of the adjacency matrix is y times the b matrix times y principles matrix times y transpose and this b is a community by community connection parameters something like that okay and for the degree corrective stochastic block model there is a degree heterogeneity parameter theta here okay so that's how that y performs in the stochastic block model in the random dots product graph so uh so So this yi is the latent position of each node, and the connection probability is given by the inner product of the two latent positions times a density parameter rho n. Okay, so and so therefore, so Okay, so therefore, so in network, the effects of y depends on the model. But we can see that no matter for which model, the spectral methods always work. So in the community detection literature, where we consider the stochastic block model and degree corrective stochastic block model, we can use the top eigenvectors of the adjacency nature. vectors of the adjacency matrix A or the top angle vectors of the normalized Laplacian matrix, then we can normalize the angular vectors, apply the key means to find out the communities. For the latent position embedding, we can again use the top eigenvectors of A or the Laplacian matrix. And for the latent positions, we use the eigenvectors times square root of the eigenvalue, which gives the latent position. Which gives the latent precisions. So, and also, it is also connected to the modularity approach. So, overall, we can see that spectral methods are useful to extract the information about y. It works for various models, no matter how y works. So, we do not need the exact y again. We only need the information about y and spectral matters, it do provide. Matters, it does provide information no matter what model we consider. If we want to really get an accurate estimation of y, this backform method may suffer like the column y's constant factor, which is like the, so for example, in the latent position model, we have to times the square root of eigenvalue for each column to get an accurate estimation. In the community, in the stock. In the stochastic block model, the degree corrective stochastic block model, we have to turn the rule wise constant vector to adjust for the degree heterogeneity. Also, we suffer from the choice of the number of eigenvectors. So how to decide is k? But no, again, we only need this cosine. We do not need the exact y. So, and our goal, so, so then, given this, we recharge. this we retrack these problems so if we do not if we only need this side the left single vector matrix do we care about the column wise constant factor we do not care about that left single vector the column factor always give so the the the norm is all the the column norm it should always be one the row wise constant factor it will change the result but it can be Change the result, but it can be controlled if we have some conditions on the degree heterogeneity. And for the number of angular vectors, the choice of K. So our goal, so recall that here. So how this latent information helps us? It reduces the degree of freedom from any. The degree of freedom from n to k to a constant factor. So, as long as we can reduce this n to such a k, so such a constant order, we do not really care about a constant inflation, right? So, we do not need exact k, but of course, we hope it's a inflation because we need to include all the signals, okay? So, therefore, we do not care about this. So therefore, we do not care about this kind of sufferings. So we propose our test statistic. We have that A and X, and we have the tuning parameter k hat, but it's just a constant inflation of K. So it's easy to decide. We find, then we calculate the hop eigenvectors of A. Eigen vectors of A, use them to construct the spectral matrix, and then return this matrix to X, construct the chi-square statistic, and find the p-value. So here, although I write like cassette hat here, but it's not an estimate of the cassette matrix. It's not an estimate of the It's not an estimate of the cassette matrix. You can see that because, so here we so, so for example, we do not care about the rule-wise constant factor, and we do not care about this k, the choice of key here. So definitely it cannot be a good estimation of casai. But what we care about is that it can construct a good test statistic, which have similar power. Have similar power of with Kazai. Okay, so now that we have finished the step one, we have a network-guided test statistic. Then with that, so we have the, we use this test statistic to category the p-values. So then with the p-values, we can select the so we. select the so we only need a cutoff to decide like uh which we should take for the uh so like sorry to decide how many covariates we should select so here we use a hierarchism thresholding because it can provide a data-driven threshold okay so uh in the in the in yesterday's talk so uh So, Prof Lin and also Prof Q, they both mentioned this hierarchism thresholding. So, I guess I do not need to introduce it again. Basically, it's just like provide one way to decide the cutoff for the threshold. It's not initiated by us, so we just use it in our algorithm. Therefore, we have proposed our We have proposed our algorithm. So we have the step one is to calculate the test statistic and p-value. Step two, apply the hierarchy thresholding to find out this S-hat. Here, one notice that, so in the spectral methods, we can say that, so for all the models, we can use the adjacency matrix A directly, all the normalized Laplace. All the normalized Laplacian matrix L. And in our algorithm, so the same. So, similarly, we can calculate the top key hat eigenvectors of A to construct the test statistic. And we can also use the Laplacian matrix to calculate the eigenvectors of the Laplacian matrix to construct the statistic. Both of them work and have the similar power. Have the similar power. So, here is a quick simulation example. So, in this example, we checked that what if we the four methods that the green line is that we use x only. The green line is the x only. Okay, so the x-axis is the number of positives. is the number of positives this and the y-axis is the number of true positives. So we use different thresholds and to test to check the power of the test statistics to check like how many true positives we have selected. The green line is a case that we use X only. So the unsupervised case, you can see that the power is quite low. And the Low and the blue line is the supervised case where we already know why we already know the latent information, so the supervised case. The black and red line, both of them are the new approach. The black line is related to the where we use the adjacency matrix to construct the test statistic. The red line is the new approach. Is the new approach where we use a left-latch matrix to construct the test statistic. So you can see that the blue line is slightly better than the new approach because it's an optimal case, right? But very similar. We zoom in that region and we check it. So you can see that they are very similar. And the blue line and the blue line and And the sorry, the blue line and the red line here are the hierarchy thresholding given by the so that's the cutoff for the number for the coercion. We will select these queries. And we can see that the hierarchism thresholding gives the nearly optimal result. Optimal results selection. So complements? Okay. So yeah, so then we have present like some results that it improves the test power, sorry, about the consistency. So I think that the result is quite natural. Is quite natural. The thing is, the result is that if we take that mj y transpose cosine k hat, if we take that one as a signal, then when the signal is larger than the log p, then we can almost exactly recover the covariance. And that means, so the signal is larger than that, means that for the under the For the under the random duct product graph model, it means the mean vector, sorry, sorry, the mean vector, it should be at the order of square root of log p over n. So you know that for the unsupervised case, usually the signal strength, the signal strength should be at the order of n to the power. Should be at the order of n to the power of negative one-fourth. But here we have the rate same as the supervised course, supervised keys. For the degree corrected, stochastic block model, the same. And then we have the cluster, so the clustering and regression results here. So it's okay. I just skip them and also we have. Skip them and also we have some covert selection results which shows the good for simulation, you know, that is always good. Okay, finally, I want to show this one. Okay, so for the data. For the data, we have applied our new approach, our new approach with the Laplace. We also think about like some other approach. Think about like some other approach, which is kind of so for this unsupervised approach. And the last one is that we try to estimate the community first. So we give the exact, we try to estimate the exact y and then use that y to construct the test statistic. And you can see, and here the y-axis is a clustering error. So lower is. Flastering error, so lower is better. And the x-axis is a number of covariates selected, the number of selected covariates. And you can see that for our approach, there is a sweet spot where the clustering average is quite small. But if we try to estimate the community label and use that to construct the test statistics, we do not have this sweet spot. Okay, so that's so yeah, so finally, yeah, so um so today we present one way to combine the high-dimensional coercion, high-dimensional data as a network. I think this kind of research is quite interesting. So because it's um because it's like solely on the networks, the problems are quite limited, but Quite limited, but there are so many problems about the high-dimensional data. So, how to combine these two things is very interesting. And so, what we and so there are many others, so future works that can be explored, like for the network A. So, what if we consider the hypergraph, dynamic network, such kind of things? And for the X, what if we have the nonlinear relationship. We have the non-linear relationship between the X and the latent information Y, and also the partial common latent structure dependence of this kind of problems. Thank you.