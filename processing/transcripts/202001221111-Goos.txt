I'm at a proof complexity workshop, and thanks for inviting me. And in fact, I haven't done proof complexity until only recently. And let me just kind of go through the story of how I got interested in proof complexity. I was a student in Toronto under Tony Pitacian. Of course, Tony loves proof complexity, and she was suggesting all sorts of problems every now and again. But back then, I was really busy with I was really busy with wanting to study communication complexity and this lifting stuff. So at that time, I was busy enough not to dip in. But it's funny that after I graduate, then I got sucked in. So what happened a couple of years ago, I was a postdoc. So Dimitri asked me this question. Can you do lifting for DAG-like models, like things like monotone circuits? And I thought that's an excellent question. Excellent question. And we started working on this. And one of the really simple observations in this area, monotone circuits, is that monotone circuits can simulate resolution. So it's very plain when you study functions like XORSAT, so there's a monotone version of XORSAT, and you're given a linear system of equations, mod 2, and ask, is it satisfiable? If you try to compute it with a monotone set, You try to compute it with a monotone circuit. Well, it turns out the monotone circuit can simulate bounded with resolution. So I care about proving monotone circuit lower bounds, and it seems like I need to understand resolution. I absolutely need to prove lower bounds for resolution in order to do what I want. So it seems like proof complexity is more fundamental than the thing I wanted to do. And so, you know, now I'm kind of forced to understand it. And so this project resulted in this lifting theorem that essentially says. Lifting theorem that essentially says that the converse is also true for structured enough monotone functions that monotone surface and resolution are equally powerful. So yeah, this convinced me that 3-complex is cool. I kind of dipped into it and now I'm interested in it for its own sake. And so that's what I'm here today to talk about. It's the results you can derive out of this Austerius-Müller breakthrough that we just have. Mueller breakthrough that we just heard. It's like the fallout of their new proof. So there's very little lifting. I'm really, really proud of this, that I'm contributing in this manner. The content is I'm going to first give a slightly simpler proof of what we just saw, which is fine in itself, but what's more is it's simple enough that it generalizes a lot better. They generalize this a lot better, that you get almost as immediate consequences the NP harness for automating a bunch of other food systems. So, for example, the NP harness for automating mule standards up some polynomial calculus. I should say there were previous results in this regard. Galesi and Loria had shown non-automatability under a fixed parameter tractability assumption. Essentially, the Retractability assumption, essentially the one inherited from this Aleknovich-Rasparov paper. So, that paper, which was the state of the alpha resolution before the Alsirius Müller paper. So, now in this work, we can upgrade the assumptions to an optimal assumption of NP hardness. Well, we can also prove that Shirali Adams is uh non-automatable under um on this uh N P equals P. There we didn't have any previous uh results. Have any previous results. And similarly, we get the first known automatability results for cutting planes. I hesitate a little bit to include it in this list. It's not like this simple proof immediately gives this for cutting planes. It requires a lot more heavy lifting. But in any case, these are the four proof systems. We now have this optimal non-automatability result. And there's an obvious omission which would be. And there's an obvious omission which has been mentioned a couple of times already in this workshop. So sum of squares, we can't tangle. And you'll see very clearly why at least our this modified, simplified proof failed, sum of squares. Okay, so I really do claim that this, our version of the proof is simpler, so simple that I do want to try to explain it to everybody. So I want to spend time, go slowly, and Spend time, go slowly, and really try to get basically the whole crux of the proof. I'm going to repeat probably many of the things that Albert already told you, that this is their theorem. I'm not ashamed of repeating these things, and it's kind of necessary. Also, I think there's a Spinnish saying that says that repetition is the mother of all learning. I don't know if there's an English equivalent, but yeah, it's good. But yeah, it's good to see things multiple times. So, okay, yes, we've slightly simplified their proof of this result. Now, if you recall from the previous talk, they devised polynomial time reduction that produces large gaps in resolution proof length. And the way you constructed the reduction was really in two steps. So the first step is you're given this formula, it's the satisfyable part. Formula, it's hyper-satisfiable, parsatisfiable. And you first construct this ref formula and prove a width lower bound for it. It's this variety of width they call it index width. We prefer to call it block width, maybe we think it's more descriptive. But we would consider this width lower bound to be maybe the crux of the proof. At least it's technically the most involved one. Involved one. And once you have a width lower bound or formula, well, you want to translate it to a size lower bound. Now, Albert mentioned this relativization technique. I would call it lifting. They're very similar processes. And this I would consider relatively simple, at least conceptually. We've done many of these kinds of lifting tricks, translating one type of complexity to another. Translating one type of complexity to another. I guess the one thing to be said about it, I won't spend too much time on it, but it's that in normal lifting, like Mark talked about, you would lift individual variables. Here we'd have to do whole blocks of variables. It's not really an issue. So, really, the crux of the argument is the first step, and that's the one we simplify. So, I want to spend So I want to spend time on the first step and really see how the crux is proved. Crux really improving the lower bound on which in case the input formula was unsatisfiable. And here's the one line punchline of uh how our uh sim uh simplified proof is obtained. We just prove it by a reduction from the pigeonhole principle. Prove it by a reduction from the pigeonhole principle. The pigeonhole principle was already mentioned by Albert, and we just make it formal. But actually, you can prove this with lower bounds by a reduction. One reduction that kind of preserves width. So we know many results, the same pigeonhole principle is hard for width, you get large width. And that gets translated into this block with lower bound. And the reduction is really as kind of simple as possible. uh as kind of simple as possible. So it's performed by just three light resolution. And I suppose the one caveat or catch in this, which makes it a little surprising, is that it's not that efficient. So normally when you reduce formulas to one another you would consider a reduction efficient if it involved resolution proofs of depth poly log n. But here we're actually going to be spending a small polynomial Spending a small polynomial depth. Okay, so it's not going to affect the parameters too much. I mean, we can start with lower bound for Pigeonhole, which is a large polynomial. And in the reduction, we lose a small polynomial, but it still leaves some polynomial lower bound for the red hand formula. Okay, so that's really the high level. So we're always in the case. So, we're always in the case when f is unsatisfiable and we want this to be the lower bound. So, I'll spend some time maybe revisiting the definition of the reff, highlighting the properties that are important, and then I'll tell you the variety of pigeonhole principles we use. Okay? So, here's how I want to picture this record formula. So, again, it's this encoding of the statement that this formula Encoding of the statement that this formula F admits a short resolution reputation. So short, maybe I choose the parameters again that it's n cubed. And so the variables of the RefF formula, they come partitioned into blocks. So each yellow blob is one block. I arrange the blocks in n layers. And by the way, n is always the number of variables in F. Number of variables in F, like in the previous talk. And so each layer then contains n squared blocks. And so, yeah, so each of these blocks is supposed to represent a clause in this hypothetical short refutation of the formula. So inside each block you would have, firstly, variables that indicate which literals are part of the clause. So maybe two n indicator variables. Indicator variables. Also, a block would contain pointers to two premises. If this block, this clause was derived by a resolution rule for some other two blocks, you would name those clauses. Or maybe the block is a leaf, in which case it should be implied by some axiom of the original formula. So in that case, the block would contain variables that name which axiom of f um we're talking about. We're talking about. And, okay, so these are kind of the variables, and then you just impose enough constraints on these variables to check that, in fact, what we're describing here is a valid resolution refutation. The one important thing, really, is the resolution proofs are DAGs, and the way we're going to enforce it in this encoding is to say for each block, its children must be picked from. Block its children must be picked from some lower level. So the layers are ordered, drop-down. And that's a way of ensuring that whatever topology this object comes out has, it always will be a tag. There are no loops. And I think this is the most complicated slide in my talk. So if you're comfortable with working with this formula, then we are fine. So somebody ask me one clarifying question just to make me feel good. Me feel good. What's the n layers? So the length is the number of blocks. Each block is a clause. I guess when I split it into layers, maybe I'm imposing some topological restrictions. Topology of your circuit is fixed. No. Of your home frame, sir. No, so the blocks they can name their children. Name their children. We just the only thing we require is you name a block that is below you. So this picture is just an illustration, but we need to use the formula. Yeah, so what you see here is a resolution reputation resulting from one particular truth assignment. So if I vary that, I could vary the literal sets, the topology. I sorry, I didn't get it. So the the height of this uh proof is n. The same number of as you say that n is the number of variables. Yeah, I've just randomly chosen these parameters to match. It's an arbitrary choice. Pointers are also part of your report and they are not fixed. Yes. So or those IH are part of the report. So, okay, so this is this is a CNF encoding that the variable valid resolution reputation of F, which we fixed beforehand. Yes. Why, okay. Why can't we, this is a very naive question, why can't we just use Cook Levin to translate any checker for a Cook Reckhaus system? Checker for a coke-reckhaus system into a SAT formula? I guess we could. I mean, we have to, at the end of the day, work with a concrete encoding, so that's why I have to give some details. And like in some cases, this formula should be easy to refute, and it actually matters how some of these things are encoded. And so it's sometimes very sensitive to that. Okay, so basically the reason we can't do that instead is Reason we can't do that instead is because if you run it through Cook Levin to get this thing, instead, that trashes facts about this representation that you will need. So I think the guiding principle is that if you would get formulas like that, then okay, they're probably hard in one case of the reduction, but you couldn't prove the upper bounds because the encoding wasn't clean enough to work with. Okay. Okay, so this is all old, although maybe the only new thing here is I've chosen this the layering for future purposes. And the reduction was from this pigeonhole principle. We use something called weak bit encoded invertible function. So it's weak firstly, there are twice as many pigeons as there are holes. And well, usually pigeonholes are unary encoded. And I believe there's a Boolean matrix. The variables come in a Boolean matrix. The variables come in a Boolean matrix of size 2m times m, indicating which pigeon maps to where, but we use a binary encoder. So we think of each pigeon being associated with some log n variables that name a hole where that pigeon is supposed to map to. So each of these guys is really this clump of log n variables, but also vice versa. So each hole has a bunch of variables that are supposed to name Variables that are supposed to name the unique pigeon that's housed in that hole. So you have these pigeons naming holes, holes naming pigeons, and then I say that a pigeon I is mapped to hole J if the two objects name each other, they point to one another. So this is already enforcing that whatever mapping this is describing, it's going to be an injection. You can't map two bits. You can't map two pigeons into the same hole because the hole needs to decide which of the pigeon it's going to receive. And it's also efficiently invertible. So if you're a limited model of computation, like a decision tree, well it's very easy to decide which region is going to be mapped to a whole. You can go both ways in the mapping. So of course we insist in our formula that this is a function. In our formula, that this is a function, that every pigeon is mapped somewhere. And this is what is going to make a contradiction. So, this is, how does this differ from the ordinary, I mean is this just weak bit, like BPHB is, or invertible also. Oh, it's invertible also. So, okay, so invertible Earth PHP. Alto Earth PHP. No, this is not onto. I was going to say, we don't insist on two less. This is a subtle point. Whether you include onto axioms or not isn't going to affect the proof of resolution, but it is going to affect it when you want to generalize this proof to other proof systems. Point is, this is our source of hardness. If you insist on onto that, then the Pigenoma principle is actually easy for North Stella and so that's why we leave them out. So that's why we leave them out. Yeah, it's functional. It's functional. Fox pig. Functional bit pigment, apparently. So the bit string corresponding to some holes, there will be some canonical bit string that means unassigned or something. You could just put in garbage. You have two mappings, one from the pigeons and one from the holes. Yeah, so there's like the. The pigeons want to go, the holes want to go somewhere, and when I draw pictures and define like a proper Define like a proper coupling when the both of them agree. So the hole can name something, but it's not. Yeah, then I just understand that as, okay, nothing's get mapped to it. And again, it's not a constraint violation to have a hole that's empty. It's not okay. Okay, so there are existing lawyer bounds for the pigeonhole principle. Principle. A couple of papers proving them for polynomial cocoluterize atoms. I don't know if this is the best reference, but at least do it. They use a different thing for encoding. But it's fine, because these R encoding and the usual unitary encodings, they are interreducible with low degree proofs. So we can use these lower bounds that say, well, you need degree linear in number of holes or Linear in number of holes or pigeons for the two most powerful proof systems that our result holds for. And this is also the point where our proof is going to fail for some squares, because pigeon hole is easy no matter whether the on-two axioms are there or not. Okay, so now we're ready. Is this clear? So I'm kind of ready to give this reduction from our pigeonhole to red. From our pigeonhole to refer. But before I describe it, the intuition for the reduction is the same intuition as in the ad hoc-with lower bound from this serious Muller paper, which is that if f is unsatisfiable, we know there exists an exponential-sized tree-like refutation of f. And, okay, maybe there's no poly-sized refutation, but maybe. But maybe I can fool a proof system into thinking this is a valid refutation by just making it locally seem as if it is this larger object. But yeah, it's almost as if these are the pigeons, these are the holes, and I'm trying to fit an exponential size object here. What makes it slightly complicated is that it's not an like an arbitrary mapping. We're going to respect the levels, for example. The levels, for example, the level i is going to be embedded in level i here. Both are height n. So that's just this intuition that we're going to follow. Okay, so now I'm ready to describe the reduction in more formally. So whenever you give a reduction, it's going to be the rule. The rules of the reduction game, if you want, and what does it mean to give a reduction? Well, so it means that we're given a pigeonhole instance, like the variables of a pigeonhole, and from those I want to determine the variables underlying this reff formula. So each variable here, describing this refutation, is a simple function of the pigeonhole variables. And simple function for us is a decision tree, a shallow one. Tree, a shallow one. So this instance depends very simply on this instance. And moreover, the reduction is correct if I can verify each of the axioms of the ref formula, assuming the pigeonhole axioms. Okay, so I'll just concentrate for a start on how do I construct this reff instance? How are Reff instance, how are they going to depend on the pigeon mapping? And well, I'll start with a part of the reff formula that just doesn't depend on the pigeons at all. So I'm going to look at roughly the log n topmost levels. I'm going to insist that's just a full binary tree, which is exactly matches the well, like the log n first levels of the First levels of the brute force tree-like refutation. I'm just going to simply copy these values in there. So they're unchanged. I guess in the tree-like refutation, I've got a contradiction at top. You resolve the first variable, then the second variable in all possible ways. Okay. So, I mean, clearly, this is looks like a valid refutation so far. How do we fill in the rest? Well, Well, so here's when I look into the pigeonhole instance. I'm going to define the layers, the connections between these two levels as just exactly this mapping. Each block has two children. They're going to map to these blocks. So these are the holes, and there are twice as many pigeons. This block has left and right children. So I just copy the variables. It's not even a decision page, just the copying of pointers. And I'm going to do that actually for all of the levels, not just the first. The same pigeonhole instance is going to be copied here and here. And so this already determines the topology of the proof. Of the proof. It's actually a tree. Yes, the pigeonhole is an injection. And again, the intuition for my reduction is I want this instance to look locally as if it came from this exponential size proof force tree-like refutation. And I already fixed the topology. All I need to do is now fill in the blocks with the right literal sets. So how do I do that? How do I do that? Okay, let's look at this block. So I don't know yet which literals I should put in here. I wanted to just match this brute force tree-like refutation. Any guesses? Like what would you put in there in order for this locally to resemble the tree-like refutation? Yeah, so it's the left child of this guy. So you would just, maybe you might convention when you go left. Maybe my convention when people left, you include the next variable positively. And okay, how can I compute this set? I mean, this was the intended value, but to compute it, my decision tree would follow this arrow in the reverse direction, because the function was invertible. So that was easy. This is now locally looking like it's actually a valid proof. How about a block all the way? How about a block all the way down here? This is like the starting to be the crux of the reduction, because so far what I've said is very simple. You've done maybe one query to the pigeonhole instance. But this is where it gets more trickier. Have a block very low down. How do I fill in the literal set here? Yeah. You've seen this talk before though? You've seen this talk before though? Yes. Yes. So it's pretty costly to compute the literal set here. I just look at the each block as a unique parent. So I can follow the pointers in reverse for many steps, maybe n steps in the worst case. So that gives me some path. And then I just check, okay, the path was left, right, right. And that gives me, no, the Me, you know, I pick the variables from this large tree. And so, this is what makes the reduction rather inefficient, because to evaluate some of the sets, I need n queries to the pigeonhole principle. Not polylog, but n. And that, well, that allows me to fill in all the blocks where this process works, this process of following unique parents. Parents. But it could be the case that actually there's no parent. I mean, what if the bidden hole mapping would be one where I have an empty hole? So I just deleted this mapping, this one pair. So I could have blocks without any parents. And this process would never reach the hard-coded region. Well, in that case, kind of cop-out. Kind of cop out. We just say, okay, I couldn't get to the hardcore region. I say this block is disabled. So, like in this relativized argument from Alfred is, you maybe just include in this ref formula the option to completely disable that. So they're not part of the proof. So maybe a subtle point, but this basically completes the description of how you can Description of how you can um set these variables given any assignment to the pigeons. So this step. Okay. Okay, so you use the same mapping, LS mapping between two N square and N square at each level of this. Yeah. Sounds a bit wasteful, but you know, we can afford it. We can afford it. Question? Why is it twice the pigeon? It's because each block has two children. So two children gives me this factor two. So I mean, you don't have enough leaves for this amendment, because, yeah. Okay, so it's quite true that this proof is quite true. This proof is going to have flaws. I mean, I have clauses here. No children. So mistake in the proof. But that's part of the second step. Now I'm going to argue why the axioms of the pigeonhole principle imply the axioms here. Or, as I like to think of it, contrapositively, is every violation of an axiom here implies a violation of a pigeonhole axiom. And it should be pretty. And it it should be pretty obvious because, I mean, by construction, I claim the only types of violations in this instance are those where one of the children is missing. So, I mean, here's a clause, it's missing a child, but that just means that the corresponding pigeon wasn't mapped anywhere. So, violation immediately implied a violation for the pigeon. Cool. Sorry, I still haven't got the computer variables, so let let's see the So let let's say the the first one, well uh let's say uh x1, x2 bar, x3. So x1, x2 bar comes from uh from a tree image and why do you have x3 there? Okay, so I think maybe I didn't say it explicitly enough that in this proof it force tree-like refutation, maybe I have the convention that whenever you go Maybe I have the convention that whenever you go left, it means you include the next variable. It's a level, layer 3, so the third variable positively. If I had followed this one, I would put in negation of x3. Because that's what the large tree dictates. Don't you have a a violation of the fact that lower levels should include x of x. I mean, closes of f? Yeah, so these are I guess, yeah, so these should be axioms. So these blocks would also name an axiom that, I guess, implies this clause. And that should be possible, that is possible because the formula was unsatisfiable. So I could actually weaken all these leaves into axioms. And that's what the exponential size proof does. That at the leaves, you have the principle that is signed. Yeah, alright. So yeah, this is how you. Yeah, alright. So yeah, this assigns all. Okay, so I claim the reduction is complete. And it cost me to evaluate the whole contents of a block. It cost me roughly n queries of the Pigeon Hall principle. So we'll be studying the block width of this Ref Hat formula. So that means we're given a clause in the variables of this formula, the block width, again, it measures how many different blocks do I touch. Different blocks do I touch? So if I touch k blocks and I want to know what are the contents of those k blocks, but it takes k times n queries to the pigeonhole principle. So that then would correspond to a k times n width clause for the pigeonhole principle. But now cleverly the parameters work out. I chose the layers to be so fat that the pigeonhole principle gives me an n-squared lower bound, and I only look at the lower boundary. n squared lower bound, and I only lose this factor n in the reduction. Okay. So any questions about the reduction? If people understood this, I'm so happy. So this is supposed to be like an easy principled way of viewing the With Lauren, but they proved in a more ad hoc fashion. Okay, so just to conclude, like the half of the theorem is we have in The theorem is: we have in the untapped case the block with lower bound, and then, okay, this was the step where you apply lifting. Okay, you kind of lift whole blocks, not individual variables, but it's not hard at all. And so, at least now you kind of saw the proof the simplified proof of this unsat case. The SAT case, I guess we don't have really any simplifications, it's simple enough. But It's simple enough. But what we've just now done is I've given you a proof that generalizes. So we had for the pigeonhole principle lower bounds also for polynomial calculus and Siralio atoms. So you also get not block width but block degree lower bounds through the same reduction and then you apply block lifting, which is easy for these algebraic systems. So whenever we Um so whenever we have a lower bound, no, it's like this is these are the systems we care about for our theorem. Whenever we have a lower bound, it's for the most powerful systems we consider. Whenever we have an upper bound, we also would like the upper bound to hold in the weakest systems we consider. So really to complete the proof of this, uh our extension theorem, which by the way implies that all of these uh proof systems are non-automate. Fruit systems are non-automatable. The only thing that remains to be done is for me to tweak the upper bound and handle Nustellenzatz. Okay, so let me do that. So I'll start by kind of recalling the upper bound for resolution from my own perspective, the way I think about it. So we're now in the case where this F is satisfied. F is satisfied, there's some satisfying assignment x that makes the formula true. So clearly, if it's a satisfiable formula, there's no refutation of it. So it should be easy to refute the formula that claims there's a refutation. And my one-line explanation for why it's easy for resolution is because, in this case, the red formula reduces to these pebbling formulas, which are famously very easy for resolution. And just to recall. To just to recall the pebbling principle, it's blade on a dag, there's a pebble at the root, and then the axioms say: if a node is pebbled, at least one of the children must be pebbled. So that is a contradictory system. I mean, if you reach a pebble, well, I should, I mean, you know why it's contradictory. So I claim I claim a little informally that there is a reduction, at least at the high level. And that's because you can think of the reference formula, the blocks being pebbled or not, according to this rule. You say a block is pebbled, it's falsified by this fixed truth assignment. So I claim that under this interpretation, it really is a betting formula. The root is The root is pebbled, because it's contradictory, it falsifies all assignments, so that's pebbled. And then the proof is using sound rules. So if X falsifies a clause well and it's derived from two premises, at least one of the premises must also be falsified by X. That's the soundness. So at this very high level, this is kind of the reason why it's easy for resolution, because resolution can do pebbling. Because the resolution can do pebbling. It's true, there are some local refutations you have to do to pass from one block to another, but that's a detail. And I guess, you know, my belief is that this actually you can do it in regular resolution like we were discussing, but maybe I don't want to commit to it. But definitely you can do it in reg regular resolution. Sorry? Yeah, it's certainly not tree-like. I mean, you can't expect tree-like upper bounds. I think we discussed this. Otherwise, it would prove tree-like resolution is non-automatizable unless B equals MV. Okay, so that's my clean snow version of the resolution uppercut. Doesn't really help for North Stellensats though, because pebbling is hard for North Stellensats. Actually, we believe that this whole formula is just not doable efficiently in Nolstellen's dots. It's Bebbling-like enough. Maybe you can even do a reduction the other way. Anyway, we don't know how to do a Null-Telenza upper bound for the red formula as it is. So that's why what we do is we tweak it a bit and we look at a tree-like version of it. So we call it tree-ref. So it's just the ref formula. So it's just the red formula, but now I insist that the topology of the underlying proof is a tree. So each block can have at most one parent. And so it's asking, is there a small tree-like reputation of that? And well, this little tweak does the trick. I claim this version is easy for Nostellenzatz. And that's because, well, we can. And that's because, well, we can reduce it not to peddling, but something called, I like to call it end of the line. This language from TFMP, Complexity Dottle MP Search Problems, the complete problem for PPAD. It's equivalent to on-to-pigeon hole. We discussed on-to-pigeon hole a few slides ago. We said on-to-pigeon holes is easy but no stalensas, and that's what I'm claiming you get out of this. So, but I like to use this end-of-the-line language. But I like to use this end of the line language for this picture. So that's this principle where you have a graph. It doesn't need to be a DAG. This happens to be a DAG, where some special node is pebbled, the root in this case. And then for any node that's pebbled, there's a unique parent that's pebbled and a unique child that is pebbled. So the pebbles are claimed to form just a path through the graph. Start somewhere and then it just goes on forever. Well, it can't go on forever. On forever. Well, it can't go on forever, and that's why it's a contradictory principle. And I claim that in this tree rep formula, if you use the same rule for thinking whether a block is pebbled or not, you now get exactly an end of line instance. The root is pebbled as before, and because it's a tree-like refutation, I should, for technical correctness, I should also mention we insist that tree-like resolution. Mentioned we insist that tree-like resolution proof doesn't use the weakening rule, but that's a detail. So it does then happen that each pebble block has a unique child that is pebble. Okay, so that's a sketch of the upper bound and why it works. But we just modified the formula. So what about the lower bound? It still works because if you think back to the reduction I gave you, Because if you think back to the reduction I gave you, the reduction actually produced trees. So the same argument still works for this modified tree-like formula. Okay, so this should complete the proof for showing the non-automatability of the three systems, four systems. So the one I mentioned, which I haven't touched on yet, is this cutting planes. Yet, this is cutting planes. And this really odd one out. So, the difficulty is not in proving with law arounds. For this result, we can use what was already proved by Artserius and Müller. It's that the lifting step is problematic. So, the result is exactly what you would expect, but the proof, the burden of the proof is now The burden of the proof is now in the second step. How do I go from these width bounds to a size bound? And there is this earlier work, it's been mentioned a few times, that does say that there is a lifting theorem for resolution width, to quote-bench length. That's what it says. But it lifts width and not block width. So it's a lifting theorem that assigns a gadget to each variable. We really need to assign one gadget per block of variables. Scattered per block of variables. And this is absolutely no issue in all the other previous proof systems I mentioned, but for cutting planes, that becomes the issue. So we actually do prove the block-wise lifting theorem. And I guess one advertisement I should say about it is that the way you prove these lifting theorems is again, if you remember Mark's talk, you introduced communication models. Communication models. Two-party communication models are sometimes equivalent to monotone circuits. And all cutting-place lower bounds in existence have been proved via communication and monotone circuits. But this is maybe the, I claim, maybe the first cutting-place lower bound that's proved not through monotone circuits. So we don't use lifting to a two-party communication model. Yeah, I think it might be the first. Yeah, I think it might be the first. It sounds grandiose, but actually, the only thing we do is we weaken this intermediate model a little bit. Instead of two players, we consider many players. That's the technical twist in allowing you to do block editing. Okay, so I don't really want to say more about this. Just to sum up, the page The papers, where can you find these results? Well, there's a very nice logical progression starting with this breakthrough. The lower bound for cutting planes is in submission. Maybe it's online soon enough. By the way, I'm really proud of the honor creativity in naming the papers. And I mostly talk today about these algebraic systems. The proofs are simple, but But the speed with which we are writing this up, like project is going to be an inside joke. And obviously, this is missing. So, you know, you should prove it. Okay, thank you. So let me have a meal. So let me uh let uh let me ask you a question. Uh in the cutting plans uh um we have one with uh weak visional principles. So good, good, you can actually use it. Yeah, so the problem with that is cutting blades is not closed under these reductions. Yeah, I mean imagine a notion of width, yeah. So that's that's one It's a little surprising that the lifting preserves the upper bound. Not surprising. Let me tell you why. I mean, if you do an exodification of the usual one, then it breaks the upper bound. Right, because you're... Just to finish. So I believe, and you correct me otherwise, I believe the reason it works is because the block width is constant. Is it right? Right, so it's like I didn't go into this, but in block lifting you associate one gadget per each block. So when you want to kind of compute the output of the gadget, you just need to remember a little bit per block. And if you have a proof that only consults the constant number of blocks, you only have to remember this little bit for each of the blocks. Just roughly. More questions? And the function principle for a for resolution are are uh easy in REST too. So did um did uh your first reduction uh imply anything about this version of the Vision of Principle, the complexity of this Visional principle that you are using in REST II? I have no idea. I have never thought about REST. I'm a novice in this, by the way, so okay. Let me end with an advertisement. I'll be an assistant professor at ETSL starting in September. So I'm looking for postdocs, people who want to work in proof complexity. So if you have strong students who are graduating, want to do this, and don't mind a Swiss salary, then contact me. So it needs to say about the height. Yes, forget it.