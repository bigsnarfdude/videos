So today I'm told you talk about the uh threshold regression to uh accomplish a query that is subject to sense rate. And so I'll give some background and motivation for this problem and then I'll describe our proposed threshold regression methods. But then I'll show you the simulation study and an application to Bozeman's disease. Finally, a short summary. A short summary. So we know that if we have a sensor response and we want to regress this common problem, there are many deviable methods from a while ago, such as the Cox model and the accelerated video time model. But if we have a sensor covariate or predictor, it is a less common problem, but I will still happen in practice. I'll give you some examples in a moment. Example in a moment. So, methods for sensory gravity are still under development. And so, here are a few examples. The first one is a family history study in Alzheimer's disease where the investigators are interested to see the association between the terminal age of onset of dementia and LZMR aminoid imaging outcome. Imaging an outcome. And I'll describe the first example in details in a moment. It turns out that this exposure of interest is subjective by sensory. The second example is study for cardiovascular disease. Well, the biomarkers are subject to the limit of detection. When it's lower than a certain threshold, then it will. Threshold than what we recorded here on. And the third example is also in Algemba's disease study, we have this progressive biomarkers, such as the tau imaging to measure the tangles in the brain. And the baseline value of this biomarker is of interest, but for some participants, the measure happened post-baseline. And because this tall imaging Because as tau imaging biomarker changed monotonically over time, we can view this as the sensor query. Well, the baseline value is subject to that sensory. And now I'm going to talk about the first example in details. So we know that age is a major risk factor for Alzheimer's disease. Besides that, the family history is another. History is another major risk. So the beta aminoid deposition is a well-known Bowser disease imaging marker. So there's this pilot study conducted at Seventh General Regional Venus Hospital with about 150 participants. So the investigators trying to look at the relationship between the maternal age of onset of dementia and the spin-a-vinal identification in the participants. Deposition in the participants, which are offspring. And there are a few additional queries to adjust for, like the age, education, and gender. The challenge here is this, the queries of interest, maternal age of onset, is right-censored. So, here I will give you a conceptual diagram to illustrate this problem. So, we have the time by kind of time. By kind of time. So here are four subjects. We probably can report their maternal age about set. And in reality, this quantity was ascertained in a clinical visit, like the doctor has questioned. And so, by the time of the interview of the offspring, if the mother haven't developed the dementia yet, so this variable is such. Dimension yet. So, this variable is subject to right sensoring. And so we use X to represent this two electron set, and C is the sensoring time. So, for example, here, the first two subject has a sensor on set. And so, there are a few methods available of that. Yeah. So whenever you interview the offspring, other has to be alive? Not necessarily. Not necessarily. And the diagnosis of dementia had to be born before or not? No, if they haven't developed dementia, that's okay. So you know, you're trying to understand that the third and the fourth persons are persons that have. Yeah. Are persons that have developed dementia developed dementia? Yeah, yes. Before the time of the dementia. So we know the exact age. There's no major difference in the fact that they just have a second one has a like maybe a larger age. Yeah. Identify the offspring of. Identify the offspring and mothers as in they've already developed some symptoms but not. Are you defining onset as diagnosis of Alzheimer's detention or onset of symptoms? Because how else would you know that? I think it's like onset of dimension. They look into the middle graph to determine that. I will continue. So the earlier literature focused on the sensor query focused on the names of the detection problem, where everybody has the same type of sensoring or type one sensoring. There are parametric and non-parametric methods developed. And for random sensoring, the method was developed more recently. The methods were developed more recently, like the imputation, model imputation approach and inverse problem weighting methods that Hargo had talked about. And for a comprehensive review, there's a nice paper by Sarah and Tanya that is publishing. And people know that if we use the sensory coordinates as it is, we're going to see the bias estimation and inflate is type one arrow. And there is a simple approach for complete case analysis where we just exclude the subjects that have the sensory query and do the complete case analysis. It is valid in some cases, such as the sensory is independent of the outcome conditioning of the coordinates. But even if it's valid, it's maybe inefficient. Maybe inefficient, especially when we have a moderate or heavy sensory. So now I'm going to describe this. Sorry, can you go back to the picture side? So what's it called if someone just says, like, I'm going to make a baseline covariate that's called mom had dementia and it's like zero zero one one right for you. Right? 0011. What's that called? That's not complete. It's not complete case. It's called, you kind of dichotomize those into two groups. But it doesn't have like a name or anything. I think there's, I I saw the method matters in the early literature. Like um I but it's probably not yeah. Yeah. Okay. Okay. Yeah. Okay. And so now I'm going to describe the proposed threshold regression. So here we're trying to do two things. So we select a threshold, little t star, and we construct a binary, a derived binary coordinate to see whether x less than t or greater than t. And then we are trying to make inference. Then we are trying to make inference on the gradual coefficient of this x, of leaf of the original x. And this including the hypothesis testing and the estimation. And so here is a multiple neural remote model. Y is the outcome and x is the coordinate of interest, which is subject to right censoring by C and Z are additional coordinates that are completely observed. That are completely observed. So, in terms of the data that we observed, in reality, we observed the minimum of atom and c and the sensoring indicator equal to 1 or 0. And the outcome 1 additional course is z. So here we assume the random error term in this model 1 that is independent of x, z, and z, and the error term has a mean of zero and finite variance. And finite valence. Our primary interest is alpha one, which is the coefficient that characterizes the association between x and y. And specifically, we want to test whether alpha y equals zero. And if not, then we want to obtain a consistent estimate of alpha one. So I'm going to talk about the three special regression approach. Threshold regression approach. I'm going to focus on the first one, the so called division threshold, and briefly discuss the extension of this method, the so-called complete threshold and residual threshold. So first, the deletion threshold regression. And we choose the threshold little t star, and then we define the indicator variable x star in this way. And so x star equals 1, if So x star equals 1 if both x and c is greater than t star, and is 0 if x is less than t star and x less than c. And for a certain subject which falls into this category, that c is less than t star, but it is sensor, x is sensor, we are uncertain about the order between x and t star, so we exclude those groups of subjects. And to understand this in an alternative way, you can think about you have time and you choose a little t star. And our information is we know the minimum of x and c and delta. And if delta equal to 1, then it's easy. We know where x is, left or the right to the t star. But when delta equal to 0, we know that x. We know that x is greater than c, so when c is greater than t star, well, it implies that x is greater than t star as well. But when c is less than t star, we're not sure whether x is less than or greater than t star. So that's the idea here. So with this, the indicator variable x star, the original multiple linear regression model implies the volume two, like the equivalent. Like equility. And then we can, based on this, formulate another linear regression model where the conditional expectation of y given the x star and z equal to this linear combination. So x star becomes the broader this equation too. Yeah, that's equation too. So we can fit this model easily. And also we show that there's a relationship between the coefficient of t star, sorry, the coefficient of x star in this model two and the coefficient of x in the original model one. So the difference is a multiplicative factor, which is the difference between the conditional expectation of x. The conditional expectation of x, even at star equal to 1 versus at star equal to 0. And when x and z are independent, even by star, we will simplify this a little bit. And I call this the difference between two conditional expectations as the bioscratching term. And because it is always positive, we can show that the well, if we want to test whether our Well, if we want to test whether alpha 1 equals 0, we can simply do it based on beta 1 over here. And the test is valid even if we have a dependence between x and c. And of course, you know, that the beta 1 can be used as a function of a t star. So the power of the hast might be impacted by the choice of a d star. I'm going to discuss that later on. That later on. And so now for the consistent estimation of alpha 1 when x r and z are independent, our strategy is we first obtain this beta 1 hat for biofeeding model 2, which is easy to do, just a regular multiple linear regression model. Then we estimate the balance correction term. And this conditional expectation is easy, it's just an empirical estimate because it's only important. Empirical estimates because it only involves uncensored observations. The second piece is a little bit more complicated. And as Sarah discussed yesterday, we can see the similar results. It can be expressed as this one. And the first term is basically the expected residual time. And then we can estimate, so this expression. So, this expression involves the survival function of x. We can estimate that by Kauvin Mirror, for example, when X and C are independent. And then approximate this integration by triploid rule. And we can write this out, write this approximation out, which is expression, a complicated expression. And with mu hat, we can, we can do the bios crashing. Within the due to balance crashing and get the original estimate of alpha 1 pack. And there are some technical details in estimating the tail part of S of X. I will skip this due to the time. And we started the asymptotic property of alpha 1 hat using the empirical processes theory and basically showing that the mapping from the data and the Our data at the alpha one is a compact differentiable, and then alpha one hat is a bi-lean estimate in a mapping, so we can establish the consistency and asymptotic normality of alpha one hat. And now I'm going to discuss the selection of this threshold to try to maximize our power and improve the estimation efficiency. Efficiency. And so the idea is that we found that the hypothesis test based on beta 1 equal to 0 is actually a two-step task by comparing the mean of two normal distributions with equal variance. So we can write out the power function and notice that the power function involves alpha 1 and the balance fraction is the mu, mu t star. Star. And so, if we try to maximize power, it's equivalent to maximize this I call an objective function, which is the absolute value of the mu divided by the term related to how many observations below force to a category that x star equals one versus x star zero. And he's And this quantity does not involve the association between x and y, so we don't need to do a correction for the maximum selection. And so that's the deletion threshold. And so now I'm going to briefly discuss the extension. One is an alternative strategy called completeness threshold. It's a similar idea, but It's a similar idea, but we retain all the observations and we do the special based on Q. And it turns out that we're going to gain some efficiency, especially for the estimation of the coefficient of Z. But we sacrificed some efficiency in estimating the coefficient of X due to the potential means construction. Due to the potential misclassification of the scheme-determined observations. So we do a threshold based on Q, which is minimum of X and C, and we use a similar strategy, introduce this binary indicator, and do the bioscrashing to get it back to the original regret coefficient alpha 1. And we can also do the test show that the test based on this. The test based on this new binary variable is valid for the original test. And then so far we assume that x and z are independent, but even x star. So we can actually test that independence within the category of z, like within the subset of x star in 0 and 1. 0 and 1. And so the natural question is: what if this independence assumptions violated? So we propose a simple strategy to accommodate that. So it is called residual threshold, which is a two-step procedure. We first obtained the alpha 2 estimate using the compute case analysis. And then we calculated this residual y minus alpha 2 hat type C and use it as the new. And use it as the new response variable in a regression model and use x as a query. So x is still subject to right sensory. Now we are fitting just a simple linear regression model between this procedure and x. And we can do the estimation using our proposed division threshold or complete threshold method to get the alpha one hat. And the optimum threshold selection. The optimum threshold selection procedure that we proposed is still applicable. And one thing I want to point out is: we here, because this new response involves the estimation of alpha 2 hat, so we need to use a bootstrap procedure to estimate the values of a beta 1 hat as well as alpha 1 hat. And the isotopic properties of beta 1. Starting properties based on this procedure threshold can be established similarly. Here is a side point where we have a category Z. So I'll skip that. Now I'm going to show some simulation results. So we set up a multiple linear regression model and generate the Z and X and and we consider a light, moderate and heavy sensoring. Light moderate and heavy sensoring 20, 40 and 60 percent, and two thermal sizes 200, 500. We do 1,000 replications. And for type 1 arrow, we did more. And so this part shows the validity of our proposed threshold, optimal threshold selection procedure. So there are a lot. So there are light models, you have the heavy sensoring scenarios, and each of them, the green curve, are the integral power of the test under division threshold regression. And the blue curve is the value of our proposed objective function to select Docker threshold. Those two curves the train matched pretty well with each other, which shows the validity of our proposed Of our proposed spectral selection procedure. Dotted lines are the power of the from the complete case analysis. We can see that on the heavy sensoring, the power of a complete analysis is pretty low. While the proposed method has a bigger advantage, and this table summarizes the simulation results when x and z are independent and sums 200. Independent and sometimes 200. We can see there are complete case deletion threshold and completion threshold. If we look at the bias of our one hat, we can see all of them are virtually biased. And the variance of the standard deviation of the division special estimator is much smaller than the convened case when we have. Smaller than the complete case when we have a heavy sensory. And similarly for the standard vision of alpha 2, and for the complete threshold method, we can see that the major advantage is estimation of the efficiency, the estimation of alpha 2. And this is similar results when we increase the sample size to 500. So the integral bias is Bias is even closer to zero. And we also run a simulation when the x and the z are definite. So here the rho represent the stress of the coordination between x and the z. And what we can see is this roads that highlight yellow color, the deletion threshold is going to give us. Is going to give us some bias when there's a strong coordination between X and Z. But the residual, this two-step residual threshold procedure give us like virtual bias estimate. So it's when I equal to 500, the bios is even closer to zero. And in terms of the power, you can see that residual threshold has a big advantage over communication. Has a big advantage over complete-case analysis. Although both of them are biased, but the residual threshold is much more powerful. And then here's the application to this motivation example that I discussed earlier, the Alzheimer's disease example. So our autopa Outhook of interest is beta ammo deposition. It ranges between some value from less than one to slightly greater than two. And a quarter of interest is the fraternal age of onset of dimension. And we control for some additional varieties that are completely observed, including the age of this participant's pitupation and the gender. Had patient and gender. First, we use the proposed threshold selection procedure to get the optimal threshold for the division and the complete threshold approach. So both of them are around 83. And then while we do the analysis, so first I want to point out that the helper one hat is tiny here because the scale of the outcome and the Outcome and the coordinate. So the outcome, as I mentioned, is like around 1. And between 1 and 2, and the coordinate here, our H are binder. And so what we can see from this analysis is there's an association if the larger If the mother developed the dementia earlier, there's an increased beta amyloid in the offspring. And also the results from the divisions from all these methods are comparable. The p-value are slightly greater than 0.05, except the complete threshold, which has a larger p-value. And that might due to the uncertainty in the sensor requirement. And so we also found that there's a slightly difference between the deletion threshold and the precision threshold. Also, we tested and found that there might be some correlation between the X and gender. X and gender and age of the participants, but that coordination does not have a major impact due to the association between X and Y. And finally, a short summary for what I discussed. So we proposed a special regression method here, which has the property of trans property of simplicity and the avoid complicated model. And we can And we can estimate the regression coefficient and also do efficient hypothesis testing for the sensitive query. And we develop a threshold selection, optimal threshold selection procedure that can be used to get the most efficient hypothesis test. And in terms of the comparison between deletions threshold, the completion threshold. Deletion threshold than the complete threshold. They leave the comparable Type 1 narrow, but the deletion threshold has a higher power, especially in the heavily sensory scenario. And between the deletion and the complete case analysis, here the deletion threshold again has high power modeling and heavy sensoring. And we found that the complete spread of regression approach may lead to a more efficient estimate of the coefficient for z compared to other methods. So if our interest is, for example, adjusting for the sensitive query as a confounder, and we are interested in the coefficient of other co-rates, and the residual threshold method is a preferred. Acid is preferred if we have correlation between sensor coercion and additional complexity observed coavics. And the last two points are what we did recently. Due to the time, I didn't show you the detailed results. So we noticed that this threshold, each measure of deletion, threshold regression, they still delete the They still delete this non-informed subjects, where we can't determine the order between X and the T star. And that may incur some loss of efficiency. Also, we propose this alternative strategy, complete threshold regression. The main advantage it shows is on the estimation of the coefficient of z, but not on the x. So we So we recently used a pseudo-optimation approach to improve this division threshold of the regression and we get a higher efficiency in the estimation. And so that's all. So here are some references. Also, I want to thank my collaborators, including Dr. Rebecca Batensky from NYU and From NYU and Stephen Cho from Southern Moscow Institute University and Holfac University of Texas. And Keith and Jacqueline are our collaborators for this bioside disease study as general. We also thank the funding support from NIH. Thank you. There's plenty of time for questions. There's plenty of time for questions. Yeah, go ahead. I might have missed it. So I know that you said that the standard errors, well, I think you said.