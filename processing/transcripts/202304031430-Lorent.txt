And as is my style, I'm going to present an elementary talk, which is not designed for specialists. There's some absolute experts on this right here. This is not for you. This is for people who have perhaps never even heard of the Duda's Giga. I hope to give you some key points, key methods of the subject that you might take away and find useful. Towards the end, I'll talk about my. So towards the end, I'll talk about my own research with these excellent guys, Sedley Lami, who's in Tales, and Guy Inpeng, who is in Western Polytechnic Institute. Okay, I'm really serious about this being understandable for an extra, so it would help me if you ask me questions throughout and interrupt. This is. This is really not meant to be people behind. It's just meant to be understandable. Alright, so we start with an elementary question. So this is our question. Suppose we have a function in some locks, let's say function w, belonging to, say, w1p, over there. We're going to do everything in a Omega, we're going to do everything in R2. Some of this doesn't have to be in R2, but we're going to do an R2 just simplicity. So let's go with function from W from R2 to R2, and we have that there exists a set K inside a space of 2 by 2 matrices. Then if we have a differential inclusion gradient w belongs to K, Radiant W belongs to K, it will just imply that gradient of W is worse. That's our starting point. And why would we ask this? Well, there's a classic example. Let's take K to be the same. Let's take K to be the conformal matrices, by which I mean the matrices will form like this, A minus B, B, A, or any A B two numbers. Then, if we solve this, let's write the coordinates of u as of w as u v. Then solving this, what would it be? Then solving this, what would it give us? It would give us that du by dx is equal to dv by dy by dy expressing by d by dx, right? We recognize contradiction equations, you know? And you might say, okay, this is not your hypothesis because we took a solvoid function and we know that there is smoothness in this situation because of complex analysis. So having that the point-wise gradient belongs to this set is completely equivalent to the Cauchy-Limit equations. Equivalent to the Cauchy limit equations. But here we're asking something less. We have a sum of a function with great existing signal set, but it still works out by some little bit of linear algebra. So if we have A which is inside the conformal matrices, and we take the cofactor matrix, it leaves it invariants in the same thing. And we know that the divergence of the cofactor of a gradient is always equal to zero. Of gradient is always equal to zero. Distributionally, just if there's the curl condition for each coordinate u1, u2, then just having divergence of the first row would be that you can take the partials in every order, in either order. So this thing is equal to zero, the distribution constraints in omega, where I mean divergence of the first row and divergence of the second row. That's a standard fact, any single gradient, right? And since if we have different And since if we have differential inclusion into the formal matrices, then this thing would be the same as the divergence of the actual gradient. You have divergence of the actual gradient, then you have divergence of the first row, divergence of the second row. So each coordinate function u1, u2 will be weakly harmonic, right? So this implies that uk is weakly harmonic and therefore. And therefore by lowest number is worse. So it's certainly true for this one particular set, right? Let's go in the opposite direction. Let's try and think about what it's false. So suppose we have k 2 by 2 and there exists a and b belong to k where the rank K, where the rank of the difference is equal to 1. We'll call it a rank one connection. Okay, so it's minimum 2 by 2. This is equivalent to there exists a unit vector v such that a of v is v of v. And we can create a rectangle square where one side is parallel to v, the other side is parallel to v curve. And then let's cut up into layers. Cut up into layers. And let's put A here and then B here and A here. So as I say this, I'm aware this is a basic example, some of you know incredibly well. In case you haven't seen this, you can build a Lipschitz mapping with gradients A, B, A, B like this because of the rank one connection. Because this thing right here means that you can integrate along this interface and both modes into the function continue to agree. So you build a nice literacy. So, you can build a nice literature function with grid A, grid B, grid A, thanks to this condition. And such a mapping would solve the differential inclusion into K, but of course has no regularity, because the grid is going to jump. It's going to jump from A to B to A to B to A to B. There's no possible regularity if you have a regular connection. So we have examples where it's true, we have example where it's false, let's talk about the most general theorem. And this is down to straight. And you sit down to start in 93. So given k in 2 by 2, smooth submarine fold and more record connections, of course. So from now on, record connections are going to be R1 connections. Connections are going to be R1 connections and elliptic, in a sense I'll describe in a second. Then if you take a Lipschitz function such that the gradient belongs to k almost everywhere, then the gradient is as smooth as k is. Is as smooth as K is. It's not exactly the correct statement, but it's good enough for our premises. It's as smooth. I should have said that it's connected. As K. So we have a fairly general result. If you have some manifold which has no random connections, this condition of ellipticity This condition of ellipticity is needed, as we'll find out later. What it says is that if we take any point on our sub-manifold and we take the tangent space, then the tangent space has no random connection. So we have to if it's not equipped, we just do that. T of A is the intangible space of K as A. So I know some of that and T V A has the R. Turns out that's equivalent to insisting that we control the difference between Control the difference between A and P with the determinant. So I'm going to write this thing for that as a constant. This. This is equivalent to this condition for any A and B inside or set K. And it's a kind of quantitative version of having no rank one connection. 2 by 2, if the determinant of the difference between the two is equal to 0, then it would be a rank 1 connection. Connection. So this is saying something stronger: that not only is different matrices not having the difference between them having things like zero, but they actually bounded below by some formatic thing. And it's just a Taylor expansion from this. So please do this. All right, cool. So that's what elliptic means. And that is Strat's. You know, I'm a big fan of Strat's papers. I've made a big part of my career by reading them carefully. I recommend them strongly. It's a very simple idea. It's a very simple idea. It's so tempting to give you the proof of this, but I won't. But if anybody wants to see it, it's very, very natural. It's so tempting, I'm probably going to do it now. Alright, so if we define this, dh x d u of x plus h minus u over x. Then what we know, we know we got the integral of the determinant of this or the derivative. Therefore, the derivative of this thing, dh u times the test function integrated is equal to zero because the summary written as a divergence, right? And then you can just break it up and control the higher order gradients by the lower order gradients and use this formula. And that's it. That's the approach. Alright, so that's only so I said I wasn't going to talk to experts, that is just for experts. Alright, holding the rest of that stuff back. Alright, so that is the positive result, and that's the negative result. And now we're going to discontinuously jump into something which looks completely different. I'm going to talk about Dubai's Giga, and then in about 15 minutes, we'll connect the two. So don't forget any of that stuff, because now we'll jump to DBD's Giga. Been studying for a long time. So, classical formulation, let's write it like this: data value function, and we're going to have this energy function. Okay, like this, and after we put some additional condition, not just to ratio A, but also the reading. Ratio A, but also that the gradient u is equal to the inward pointing unit normal around the boundary. And this thing is a model for the magnetized cloud. It's a model for blistering in the divergence 3 version of this. Since when 2x2 curl free is the same as divergence 3 after rotation, the divergence 3 version is more anything like nice films. This thing has attracted a lot of attention, particularly around 2000 when I was. Particularly around 2000 when I was a postdoc in PISA and I started to get interested in it. And what is, well, there's many of you think this is interesting, but from a mathematics perspective, this is the natural hierarchization of modern Mortola. So modical Mortola is this. So again, theta value functions like this, and we're going to have. Like this, and we're going to have that integral of w zero. So this part forces w to take values like plus or minus one. This is constraining the oscillation, right? And this guy was one of the first, if not the first, example of chemical intelligence proving full strength. All right, all right. It's on video. I'm gonna just use these abbreviations. I'll show you all the right away. So let's look at this guy. So standard trick, we can do this. Like this, and this thing is And this thing is bounded above by this energy, terminal constant, like this. And if you have a sequence over two sequence, all right. So if we have a sequence. Oh no no no, okay, but you need to have a square. One minus mod is three squared. Yeah, speak up with the typos because I put them on purpose to see if you're paying attention. That's what's up. Alright, so look at this. If we have a sequence of bounded multiple mortala, then we have this expression here bounded, right? This expression here bounded, right? And get rid of the epsilons. So if such that m epsilon k w k are less than infinity uniformly through all k, then this is uniformly bounded, right? And the part of the function forcing it to take values plus or minus one means that that it's mostly going to take plus or minus one. And when it jumps from plus one to minus one, then this thing will be of order one, and this thing is controlling the gradient. And this thing is controlling the brain just in the L1 sense, right? So it's not a surprise that you can prove that the limits of this thing will converge strongly because this is almost as good as an L1 norm on a radium. And it's also not a surprise that the limits of this sequence of functions will be Bv. In fact, WK will converge strongly to W, which is Bv, taking values just in plus 1, minus 1. And this limit. And this limiting energy will just be one-dimensional measure of the jump part of W, okay? And the sense of convergence of gamma convergence. So all u m epsilon of t2 with this integral condition will then converge to, let's just call it m, and the space d ph omega round is in plus 1, minus 1, where Where m of w is just the H1 measure of the jump intersect on the ground. And deboconvergence is a lengthy definition, and this is not something I can go into in this talk, but just think of the most emphatic way you can turn a minimization problem into another one. That's what gamma convergence is. It it it is the most complete everything you could possibly ask when you translate one minimization problem into another one. Fall into another one, it comes from a vergis. It totally does all the things you possibly could want. It's a very strong definition. So, this is nice because it revealed the actual properties of this thing. What this thing is actually doing is a minimal surface minimization problem at a time, which maybe doesn't entirely look like it is, but after this trick, it's also not entirely surprising. What makes this guy super interesting, and why is it so open? Is it so open? Is that it works in a completely different way? You don't control the jumps on the gradient with the same control. We control the jumps on the gradient with the power 3 as opposed to controlling it to the power 1 as we can here. And it's rather easy to show that, but it would take about 10 minutes, or I can't show it to you now. But if you're interested, I can talk about this forever. I love this topic. I will show you this. It's a nice little calculation. It's a nice little calculation, and it's really revealing as to why this is so hard. So, if a sequence of bounded energy here, the sequence will not convert it to something which is in DV. It might do if you're incredibly lucky, but there's no reason a sequence would have to be such the limits in BV. So you can't apply this function theory to analyze the limiting functions and to produce a limiting energy. You're out of tools. You're completely out of tools. So these. So the people back in the day, these names have erased and I'm running out of like. Where is that? Under the early heroes of this subject, I have a question. So you're does it boil down to the fact that if if if the u is vectorial, m epsilon doesn't work? M epsilon doesn't work the right way. Yeah, right? I mean, it in some sense boils down to the fact there's more freedom for this to move around the circle. Yeah, there's also some geometry of the circle that circles curves. So the curvature of a circle means that you can move around a bit in the circle and that won't be counted as badly as when you count jump from minus one to plus five to each. Oh, and that's again another completely different technical. Being another completely different technique of this webcam. I think you asked this question for security factorial. How do we do that? Because it's a combination constraint. Otherwise, it's not dimensional than it is. It's a huge. I'll show you the stuff dimension after this. Alright, cool. So, to get this working. To get this working, the early heroes of this subject, the Simoni, Covent Model, Otter, Delez Matagatza, Praise Delez Matigatza, particularly the first group, they use some theory from scalar conservation laws. So, scalar conservation laws, what do we do? We have some u, and we say u up to this equals zero, right? There are many, many solutions of this, infinitely many solutions of this, so there needs to be a weak solution. So, they need to be a weak solution that more effectively captures this. And what they did was they came up with this notion of entropy-entropy flux pair. So, they have this like this, phi pi equals right. And then if you consider the divergence of phi of u, theta of this, of this, you do this calculation, divergence, and you take the partial inspector t of the first coordinate inspector. T with the first coordinate, the plus expected x with the second coordinate. You're going to have this, and then you're going to have this. And then you're going to put this guy inside this guy, and then what you'll find is that any smooth solution by the chain rule will be such that it will have to equal zero. See what I mean? So we can just factor out the triangle u, where u of t plus the triangle of u u of x is zero, one of this condition. Zero this condition, and therefore you have a smooth solution to cancel. It's an additional equation, you have a smooth solution. And that's the general idea behind English. You get this, you take something which would be zero anyway, and then you say, okay, anyway, you get a little smooth. And you say, what happens if we're a bit less than smooth? Do we get something? And in the case of scalar conservation, of the alpha is yes. What you do is you dring rise like this, because of the algebra of the situation, if you do it like this, and you have a sequence that converges. It's like this, and you have a sequence that converges at infinity to a limit, that limiting function would be such that whenever you take an entropy-entropy flux pair and you have that f is convex, then this thing would actually be a measure. So u epsilon would tend to infinity, this is nice to explain everything's weak to work. 10 to infinity is a mu, the convergence of field u is a measure. It's actually a positive measure. Yes. So they said, all right, let's study the space of functions which weakly satisfy our original equation, which then is this. And then let's add the additional condition that for every entry empty flux pair we want it to be a measure. There are such things. They come from limits after regularizing. Let's consider that as our notion of weak solution. And that's a spectacular successful thing. And that's a spectacularly successful theory. It's one of the most successful theories in all the VEs because you have uniqueness, you have regularity, you have all the good stuff you want. So, this is machinery they want you to copy because the limiting functions from the will be things that satisfy the Eigenhold equation. The gradient of U will be equal to one almost everywhere if you can show that gradients converge strongly. But there are many such solutions and we want to find the one that's relevant as a limit of this thing. So, what I do. Well, this is a bit of modern retelling of what I do. So we say that five That phi from S1 into R2 is an entropy. If and only if when we do this, okay, so this is something I'm doing this stuff right now. When we write e to the i t, we don't mean the complex function, we dantify complex numbers with points in the plane, okay? So my case would mean cosine of t. Cosine of t sine of t0. It is an extremely convenient notation, so we're going to use that all the time. So, and if we do this, we want this thing to be zero. Let's take a function that satisfies this. That's our definition of entropy. We can extend it in a trivial way. Like this, where phi is, where eta is just some colour function which is smooth and equal to 1 around y itself. Eta, here we are at 1. Okay, so we can trigger the extended. And then with these guys, we can firstly show compactness. We can show compactness in the sense that if we have that UK. That UK are such that they're probably bounded, then the gradients after passed into a subsequence are not going to be enabled, will converge to some empty function 0. And we can do this in short using the diff curl lemma because of this. Because of this class of functions is very rich. In fact, the model proof only uses two of these guys, the Jimmy Cohn entropies that come all the way back from his school up in the early 90s with Jimmy Cohn. I can't tell you too much details about this, but again, I will if you want to know. This thing converges strongly towards the gradient, which is highly non-trivial because the function itself is not constraining the second gradient in any natural way. In the second gradient in any natural way. If we do the same trick, then we have this, like this. And as the gradient gets closer and closer to S1, this constraint is weaker and weaker. So there's no uniform constraint on the second gradient. There's no reason aparae that the gradients should converge, but they do. And it's usually in the toolbox to compensate the compactness. And actually, nowadays, you can show from scratch compactness theorem that any of us. Stracks and Pactness theorem that any of us set in 2x2 without micro connections and not measures are true. So, this is one of the ideas by these guys, the Simoni called Mola Otto. They used whole family mentories to do this. And the original guys that Brody did as Matthew Gatson just used two entropies, the so-called gin-current entropies, which are sometimes rather nice elementary entropies to do this. Piece to do this. That's the first thing. So you have a limit which is a really good satisfying regular quote. But you'd also like to know what regularity it has. If it can be anything, then you can have a problem with the kind of convergence conjecture. What you'd like is that this thing is E V, but it's not E V. So what you'd like is D V like properties of this radio. Properties of this radium. And that is stuff that you can get then with the concept of entropies. So here is a key result. If you take the divergence of this extended entropy on one of these guys, then it is bounded by some constant depending on the entropy times this expression, like this. In other words, In other words, it's bounded by the energy. So these guys will be a bounded sequence. This will converge strongly and easily enough, so it's not hard to show that weakly this will converge to the entropy applied to the natural function mu. And in particular, if you have that the energy goes now to zero, then the divergence of all these entropies will be zero. If the limit as The limit as k tends to infinity of this thing, then this implies that you have a divergence of every single entropy applied to your limiting function, actually applying graded like this, the function is equal to zero. And this was used by the group Fatan, Jan. group R Fatam, Japan or Javin Fatam and Ottom to show that the gradient therefore is rigid in a particular sense. It's rigid in the sense that it has only a discrete number of singularities, a locally finite number of singularities. Around each singularity, it forms something like this. Just this function. It's going to be x goes to x minus x0, or hold on x minus 0. x0, x minus x0. That will be the gradient around the singularity. And there's only going to be one singularity per convex subset of your domain. So zero. Jump in auto if divergence of phi, and I realize that this definition is not for the gradient, it's for the divergence free. So let me formulate this. And if this thing is equal to zero for all entropies and And m is just a function from omega to S1, which is divergence free. Then for any convex U containing an omega, if U intersect the VAT set of singularities just consists of a single point, then M then m dot z is z minus z0 over z minus z0 orthogonal. So it's just a classical vortex, as you would know from Ginsburg land or yes. So if your domain is convex, there is only one singularity. If your domain is the entire plane, then there isn't, there is just one singularity at the most. Alright, so that is So that is part of the part of the background of this subject. And the P2 is the strength that having something like this gives you. That if you have a wide class of functions where the divergence acting on this guy is equal to zero, then you can. Is equal to zero, then you can get additional stuff out from it exactly the way that people do in scalar conservation laws. They really taunted a lot of these methods of cross, and now there's a kind of cross-fertilization. Okay, so this is background. Let's talk about more recent stuff. So, back in the day, about six years ago, I had the great privilege of having Buckingham be visiting assistant professor in Cincinnati with me, which isn't a good job, but it's a very hard-working job. He's a very hard-working young man, so we decided to work on this kind of stuff. And what I wanted to do was to say, okay, can we reduce these hypotheses instead of having all the entropies vanish, what if we just have these two classical entropies of the gene colour? And the reason to do that is that the gamma convergence conjecture actually needs to be formulated just in terms of these two. I can't go into details about why. So let's try and weaken this, not have an infinite family, but just have two. We just have two. So, what if divergence of gamma k of m zero in a distribution of sense? Same hypothesis that m is going to s1, it's divergence-free. Can we prove the same rigidity? And the way we thought about this was like, okay, if this is true, then, so i is going to also denote the rotation, then that's the same as saying that the divergence or Saying that the divergence or the curl of i times the wind of this thing is what? These are new tasks. Yeah, the basic is one of the wish which you didn't give us. No, I didn't give this to you. I will give this to you. You anyway. I will give this to you in a little bit. We'll see if you are nice guys. So, yeah. These guys are divergence-free, then we're going to have something which is curl-free rotating around, right? So, for k equals 1 and 2. And therefore, if omega, our domain, is simply connected, this is a film number 2. There exists some w from omega into r2. 2 such that rate of w belongs to this set K, which I'm about to define, given my P of e to the IT, where P is this mapping given by the two code like this. This or the din con rotation. So this thing is a perfectly nice mapping from S1 into the space of two right to matrices. And being curl-free means that we must be able to find such a grain with W such that this is true of this and I think these things. So now we're the territory of different conclusions again, as promised. Promised. But if this set K had random connections, then there's no way to get any additional regularity by the counter example talked about initially, right? If K was elliptic, then that would imply that ring of W is smooth, smooth as this is, this is as smooth as can be, it's analytic, which would imply going backwards that M itself is smooth, and that's also not true because we have a singularity and that's a possible solution, right? So it turns out. So it turns out that K has no rank one connections, but it's not elliptic. In fact, it fails to be elliptic at every single point. So every single tangent line to this one-dimensional set, space of 2x2 matrices, is itself a rank one line, which I possibly are right for those reasons, has to be the case. So, Warren and I thought, okay, that's interesting. Let's look at Spherox methods. It's not hard to mess around with Spherox methods, and so you can get something out of it. And what people get is that you get a bit. And what people get is that you get a bit of additional variables, which maybe w belong to, because Besoff space, a third for infinity lock. So you get something like a third of a gradient. If you've never seen Besov spaces, if you don't want to see them right now, I'll tell you a little about them, or at least a little about this subclass. But this is somewhat exciting, at least it was for us at the time, because it's a beautiful result by. Result by Camille de Lelez and Randy Lugnak, that if you actually fractional solvol space out of a flow of three with these conditions that you are such a nickel equation, m is divergence free, normal m equals 1, then you are rigid in exactly this sense. And that worked by getting enough of the chaining to work to show that all the entropies vanish for such an m. So if we had a bit stronger notion of a third of a gradient, we managed to be done. But we didn't. This is a weaker. But we didn't. This is weaker than the fraction of some of our space. But nevertheless, there was enough here to keep pushing and pushing until we eventually proved the result. So the conclusion is true that this, in the end, will apply that in the sense of Platam, Javi-Notto theorem, that there are just discrete number of singularities in our new singularity form of vortexes, defined by singularities in other words. Cool. So we were kind of happy with that, and then we got really. And then we got really. Yeah. What did you add? And it's rigid. Yeah. What do you mean by rigid? Rigid means this: that for any convex subdomain, if you are intersection with the bad points B, you can define B, so B is singularities of M. Any intersection is just one point, then around that one point. Around that one point, it has to be a vortex. It's entirely defined by its singularity. There's other questions because we got extra lucky a bit later than that because Dengue Nami visited Cincinnati. In Cincinnati, and we three started to discuss the really bigger. And one of the great things about this is that after many years of stagnation, the subjects come back to life thanks to the ideas of some young mathematicians, such as Abbey Lavi and Ramar Conney. And what we decided to, or one of the things we decided to work on, was to ask this elementary question: what if we just had a differential collision into this set by itself with no other? Into this set by itself with no other conditions. So we got a different set from starting with a divergence free M and built in a different inclusion and then went back and used properties of M to actually prove it's rigid. What if you just ask this expression by itself? So you have this set K. If gradient U belongs to K most everywhere, does this imply that the gradient of W is rigid in the sense that there are just discretely many singularities and around the singularity? Singularities and around the singularities for example, like a vortex. And this is a perfectly elementary question. And now let me tell you what k is because k is actually really pretty. So if you work in two by two matrices, the universe of math wants you to decompose two by two matrices into conformal and anti-conformal. So linear algebra says that any matrix A can be decomposed into a conformal. Can be decomposed into a conformal part plus an anti-conformal part like this. So we have this A minus B B A plus C minus C B D for numbers A, B, C, D. This is unique decomposition, it's orthogonal decomposition. For whatever reason, if you work in dubytic matrices, that's how the world of math wants you to think about it. Wants you to think about it. So I really advise you to give this a go if you've got any problem into writing matrices. You can pose it like this. Let's see what the calculations tell you. If you want to write out k in this form, then you can write it in the following way. So it is going to be like this. It's going to be e to the i s. So we can identify every vector with a point of conformal plane. Identify AB with this guy. So we're going to write this guy confirm. This guy conformal like this, and then one over three e to the i 3s, and now we turn this into an anti-conformal. So in the obvious way, we turn this vector into conformal, so we identify AB with this guy, and then we identify CB with this guy, and that's what I mean by this notation here. Is that clear? So then this step for s belongs to 0 to pi is Here's our set. Here's our set. It rotates once in the conformal as it rotates once in the orthogonal direction of anti-conformal, which we're hitting three times around. And it's not hard to see that this is not elliptic. You take the derivative of this, you take the terminal, the terminal is zero, go ahead, by Ephis. It's a metric or it's a vector. It's a vector. So Ephvis. It's a vector. So ETBIS, which is always going to be this thing. I'd like to try the matrix with the datasets. Alright, so I promised to be having an entry, and it seems I'm not. You said it very fast. I said it very fast. I said it very fast. So, yeah. Cool. So, yeah, so then this is a vector which we identify with the matrix, and that's how we might create this. And that's how we can create this one-dimensional sub-manifold in space of two by two matrices. All right, and this also now answers your question as to what are the gene colours. So the gene colours are the first row of this, and the second one is the second one of this. Cool. All right, so that's our question. Can you get some additional regularity just on this? That'll be an improved regularity result without ellipticity. And the answer is yes. The brain of Delta is rigid. In the sense that so the ring w at every x, by definition of being inside this set of d to c of m of x, where p was this mapping I have over there, right? Was some m of x inside the rib. N of X inside the room, the S1. And just rigid in the sense of Japan Partin Portel. So the same thing. This guy is entirely defined by singularities. If you go in a convex domain, there's only one point where it's singular, and around that singular point, it's just rotating. Cool. So we were happy with this. Well, I am particularly happy with this because I have this sort of fetish for really. Sort of fetish for really simple statements and really simple questions. And this is exactly that kind of thing. And that kind of got me excited to try and see if, or got us excited, to try and see if we can develop a theory of regularity of differential inclusions into one-dimensional sub-manifolds that don't have right-wing connections but are not elliptic. And that's ongoing work, and there might be some general. And there might be some general theory there or general theorem there where this is a special case. Sorry, I'm probably missing again something. So you consider that specific K. How did you come up with that? That K is exactly the same K as this K. So what's the clarification of that? What one's the first thing? I'm not sure what you guys have to face around. No, because maybe this question comes later. Maybe his question comes later because I don't understand what you wrote here. So, K is what is E, T I S is the rotation of angle S. No, no, no, E T Is just this vector. It's just this vector, cosine of S, sine of S. That's the composition of the square. So I was trying to write down avoid writing stuff down plus another term. Sorry, it's annotation. No, it's a rotation. It's the rotation of angle x. No, no, no, this is not a rotation. This rotation has a scalar multiplexer. No, that one. E to the i x. E to the i x. No. Minus sun. Hold on. When you turn it into a conformal matrix, it is rotation of angle x. And the other angle 3s, 3s is the angle. That's the anti-formal. That's rotation times reflection of angle 3s divided by 1 over 3s. Divided by one over three. All right, all right, thank you for this questions. I mean, you're saying these gammas that you wrote there is the same case. We haven't seen the gamma. Yeah, but now I showed you the gamma. This is the gamma. So gamma one was the first row of this, gamma two is the second row of this. That's not gamma. Of course, yeah, just definitely state. Yeah, yeah, exactly. Perfect. All right, cool. Cool. All right, cool. Away with me, everybody. Well, not everybody. Everybody is still awake. Anyway, any other questions from those that are still awake? Yeah, we have very few minutes to finish. Alright, so this is the direction we're going in. going in. And I want to propagate some of the interesting questions around the Ubidis Giga because as I said, there are now new ideas, new blood in the field, and questions that are accessible, perhaps, that are really nice. And here's one of the questions that I like a lot. So, one of the consequences of the Biggest Giga is that you expect that That you expect that functions that are so in the eigen equation to which all the entropies are measures are just going to be measures supported in a rectifiable set. Just like the case if it was BV. And so therefore, if the entropies are absolutely continuous, they should vanish. So here is one of the consequences we figure. If you have that divergence of gamma k of m belong to L P, Belongs to LP, then this should imply that the thing is just vanishing and therefore rigid. And what Wayne and myself could show is that this being true, it's equivalent to m belonging to fractional solo space 1 over 3p infinity over a certain range of p between 1 and 4 over 3. 4m and this divergence. And what we did was just the doubting methods of Garadi and Lami to do this. And what Giradi and Lami showed was that if you have divergence of every single entropy like this, or actually, actually those are actually all of them. HD goes out actually all of them. For all entropies, this is equivalent. And what they showed was that if all entropies are measures, this is equivalent to this thing, a slightly weaker space. Okay? So what that means is that if the obedience begins true, then the threshold regularity for the Eigen equation, the mass of scale. For the eigenvalue equation on the Mesov scale is exactly b1 over 3 through infinity. If you're b1 over 3 through infinity, you can have line singularities from, you can construct examples where the singularities are entire lines rather easily. And if you just increase the integrity slightly, then having these guys in that space is equivalent to being LP integral, which, if the ability theory is true, would mean they will value. If the ability is true, we mean they will vanish, which means they're all rigid, so there aren't any singularities except point singularities. So, this is another elementary question that you could answer. So, this is one of the most simple non-linear PDs you could possibly ask for, right? And we would understand completely when we have regularity of this as part of the really conjecture. Conjecture. And you don't even need to solve much of the VDC conjecture, just to show that if the NPs are LP, then they have to vanish. So if you're interested in that kind of problem, don't let me design this right now. That's important. Sure. Any more questions? Yes, so so to ask the equivalence here. The two that's the equivalence here. So they are supposed to pull for any entropies or just for the two G equivalents. So as an extra results, this is for all entropies. There is weak results for just two entropies, but it's for all entropies. So if this is true for all entropies, it implies this. But it's wrong if you only assume the LP integral T associated to the two GL. I don't think it's wrong. I think that's just not proven. That's also part of the ability to go. Not proven. That's also part of the Vidis-Biger's conjecture. And the same for the other equivalent, the linear? This is only known for all of them. And again, if it's unknown, it's just two. Very good for just two. So your matrix with the gamma i's, right? Um they always run too, right? Uh yeah, they're always running. They never come too close, right? They never come too close, right? The two vectors. In the sense that these two guys never come too close. Yeah. Okay, so there's this part of the community that we like to think about these things that when the two vectors don't come close or close together and you have divergence-free constraint, then you so for free you get the extra rate value. So maybe kind of the better point, but I mean there's more more tweet, there's also Andre of course. There's also Andre has worked on that. So that could be one of the reasons why, even though the K is not elliptic, you still get to inside. Okay. Okay. So if so, then that would be a different approach to proving these kind of regularity results. I mean, there's this paper by De Levis, I don't remember. It's like sort of easy proof of some coercion formula where he also plays with columns that don't come to close. It's also related to Giovanni's. I know I have better. I know it's not Berbist theorem. It's in the notes of the journal theory. So that's interesting because maybe there's many other proof case that will be the same as long as the vector song controls, but it's quite interesting. So being like one is a public power not to come too close. I mean I need to see yeah okay let's talk more about that that's interesting um I didn't quite catch this thing yeah so so I feel a bit bad because I promised an elementary talk and not just didn't work out our way or maybe the first ten minutes yeah so so um part of the conjecture is to replace the to replace the the uh the black of B V with studying these guys of the verging zone and the one that working zone and the two well so this thing is affected by deep measure right and the proposed limiting energy of the if it's figure conjecture is the absolute uh is the total It's the total variation of this measure, right? So let's call this measure i. And that is our limiting energy in theory. That should be the energy we have as the limiting energy. And it's conjectured that this thing will be a rectifiable measure, which means it's supported in a rectifiable set. And a lot of it is known about this, that there exists a rectifiable set on which, not this guy, there is a grand measure where you incorporate all of the possible elements in this measure. The possibilities in this measure. That rand measure, there is a one-dimensional rectifiable set on which m has got traces on either side. This is Otto 2003. And but the actual conjecture you need to formulate this, the brand measure is not the right angle for the move to the next first thanks, dance, and not a Nice, dense, and not at all an elementary talk. So, then we can continue discussing. Thank you, Mr. Speaker.