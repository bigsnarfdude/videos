Okay, so we're going to pick up with the second part of this talk, which is by Milland. Take it away. Okay, yeah, so as Jacob already said, I'll be talking more about some of the basic ideas of the proof. So again, just to restate the theorem, we have this sub-polynomial error in the estimate. In the estimate for the probability of L, where epsilon is the probability of the same event for Brownian motion. So I won't be going into the main details. It's too complicated for a short talk. But I'll be talking about some of the framework in which the proof operates, which was actually introduced in an earlier paper of Alan. And we build on that to get the result of this paper. So many of the ideas are already present in the earlier. So many of the ideas are already present in the earlier paper, but this is what I can focus on. So the basic step is to find a process whose nature is somehow in between that of L and that of Brownian motion. So Brownian motion you understand really well, and L is very hard to understand. So you want something which you can understand, but still is variable to L. And we prove the estimate for J using the factor Gibbs property. Using the factor-Gibbs property. And in fact, J is constructed kind of using the Banner-Gibbs property. Once you've got the estimate for J, you can then transfer it back to L using some of these earlier results of Alan that I just mentioned. So this is the broad idea. And now I'll be talking about the Boundary Gibbs property to remind you and therefore describe what J approximately is. So as Jacob said, we embed L in a system of infinite, an infinite system of System of infinite, an infinite system of non-intersecting curves, which has this Browning Gibbs problem. And what that means is, so here I've got on the right-hand side, I've got these figures. So you see, suppose you kind of erase the gray part of the first, in the first panel between the dotted lines, and you ask for its distribution given everything else in black. And the Brownian-Gibbs property says that the distribution of the gray part is exactly a Brownian bridge between. Is exactly a Brownian bridge between the endpoints conditionally intersect the lower curve. So that's what the Bankish property is, but I'm going to kind of give you a slightly a reformulation or a different way of thinking about it, which will help us get to what J is going to be. So you can think of this unconditioned Brownian bridge as a candidate process. And it has to pass, for it to become what we want to become, namely L, it has to pass. L, it has to pass a test. And the test is just what we said before: it's non-intersection with the lower curve. And if it passes this test, it gets the right distribution. And if it doesn't pass, we just try to find a different candidate, which will pass. So we kind of we keep resampling these boundary bridges and waiting for it to pass the test. And once it has passed, we have the right distribution. So obviously, the effectiveness of the candidate is going to depend upon how well it passes the test and by how Passes the test. And by how well, I mean, what is the probability of it passing the test? So, if you were in Ivan's course, he calls this, I think, the accept-reject probability. So, it's kind of a similar idea of passing the test. Unfortunately, the Brownian bridge candidate, the unconditioned Brownian bridge candidate, is not very good. It's very difficult to get it to pass the test. And there are basically two issues. You have no control over this lower curve. Control over this lower curve, so it could do anything. And you'd be able to pass the test kind of well, no matter what it does. So there are two problems. One is if the lower curve is really close to the upper curve. And in that case, you have a hard time passing the test because you want to touch the lower curve very soon as you start because it's so close. And the second problem is if the lower curve has this kind of internal peak in the middle of the interval, because then the Brownian bridge has to do this kind of big And then the Brownian bridge has to do this kind of big jump to get over the peak in order to pass the test. And so you'll see that the candidate J, which we're going to describe, is going to kind of have features which allow it to overcome both these difficulties with a higher probability. And the way we handle these difficulties with the candidate is that we change the candidate, right? That's what J is going to be. But also, more generally, depending on the problem you want to, depending on the problem. You want to, depending on the problem or the event you want to estimate, you can come up with different our main theorem J is a useful candidate, but it could not be, it might not be the one for all situations. So the more, the broader philosophy is that depending on the problem, you can decide what candidate to take to be best suited for that problem. And the way to get these different candidates is to choose what data you condition on. So if you think about it, the brown. So, if you think about it, the Brownian bridge, the candidate had no conditioning. It just had the right endpoints, and then you hoped that it jumped over everything you need to jump over. So, there was no conditioning at all in that candidate. So, you can provide data to the candidate to make it better the final goal of jumping over everything below. And so, you can decide how much data to give to the candidate for it to be able to pass the test well. So, there are two things you can do. You can give it more data, you can give it less. Things you can do. You can give it more data, you can give it less data. The more data you give, the less brown it becomes in a kind of a vague sense, and that makes it hard to analyze. But the less data you give, the harder it is for it to pass the test. I think we've kind of achieved this balance between giving it a certain amount of data so it can both pass the test and at the same time be analyzable without too much difficulty. And that's what I meant earlier when I said J is kind of a midway processor, or it's mid, it's somehow in between L and Browning motion. So now let's look at some of the things we do in order to make J pass these problems. So, like I said, it has to pass non-section near the boundary when it's very close, and it has to avoid internal peaks. So, here I've drawn you a sample second curve, which has both these followers at the same time. And to handle the first problem, we use the fact that before we did anything, The fact that before we did anything, the top curve of the L ensemble was already avoiding the bottom curve. It already had, it is already doing the right thing to avoid the bottom curve. So if we can make use of that kind of knowledge it already has, that will help us pass the test later. So what we do is we take the form of that path and save it for later. So you see, I've just taken the thing at the side and I kind of finally shifted it to get the thing on the top over here. thing on the top over here. And that's how we're going to use the form a little bit later. And the second thing we do is we want to have some idea of where this peak is and how high it is so we can then try to jump over it more easily before we have to pass the final test. And to do that, we coarse grain the lower curve and then just save its height of one particular point as a kind of a pole, like a physical pole over which you have to jump. And that's this red line I've drawn over here. Over here. And with these two things, we can construct our jump-on-solved candidate. So let me tell you what that is now. So first, we take a Brownian bridge and just make it go from between the two X's I've drawn and condition to jump over the red pole. So notice that it can intersect the lower curve as long as it jumps over the red pole. That's all it has to do. And you can see over here that in fact it's not passing a non-intersection test. Not passing a non-intersection test because it's intersecting the lower curve on the left side. And this is where we use the forms of the side paths which we saved earlier. And we're going to put these over here and erase this part of the blue curve. So to see that, we're going to, so you see that we've taken the same thing and put it over here. It has the same form as above, but it's been kind of shifted compared to what it was in the earlier part. And you see. Earlier part. And you see that having done this, the side bridge now avoids the lower curve. And now this whole thing, this yellow, blue, yellow put together, is our jump ensemble. What do you save it? You save part of a motion, like part from the area sheet before you do the resampling or from like a previous sampling? What are you saving it from? From the so we try to learn something with the beginning with, right? And then we condition on certain things. And then we condition on certain things. In that conditioning, I have saved this side path, but without the endpoint value. So the point is that I have forgotten what this thing is, what this point that I'm kind of circling over here is, and I've saved only the form of the path, if that's clear. So you mean that's just a path condition to go from one point in time to the other without touching the lower curve? So that path is sort of fully. Lower curve. So that path is sort of fully conditioned on the lower curve. So you see that we want to, it depends on what you're conditioning on. So you've got this Linus level to begin with, and this thing, because it's the top curve, already does non-intersection, right? And we want to resample it in a slightly more sophisticated manner than the earlier things. I think maybe the confusion is that earlier I was sampling only in between these dotted lines. And now I've kind of stepped back a bit and I'm sampling. Step back a bit, and I'm sampling kind of throughout this whole interval. But I'm saving party information on the side parts and nothing in between. So it's kind of a slightly more sophisticated way of conditioning than the earlier Brownian Gibbs, the pure panella Brownian gibs that I talked about to begin with. And so this is a path taken from the original Line ensemble before doing any resampling. Before doing any resampling, so there's no conditioning involved because it already does the non-intersection by definition. Okay, so you're changing the path a bit outside the window as well. Yes, yeah, in a sense. Okay, so yeah, so using this path and putting it back in in the way I've indicated, you get this overall Canvas path. And this is what is basically, so I've skipped a few technical. Is basically so. I've skipped a few technical things about how to pick where this pole is and so on. Um, but this is at the core of it, this is the junk ensemble. Um, yeah. And I just want to come back to the earlier thing I said is that this is one candidate which worked for us for our main theorem, but for other situations, you maybe want some other candidate which is maybe easier to analyze or maybe it's closer to L's nature or whatever it may be. And so, by thinking about what kind of data you want to save, which will help you. save which will help you uh in proof you need to prove uh and and doing and and considering that uh candidate um yeah so thank you okay thank you so um we certainly have time for questions so actually maybe i'll come back to a question somebody asked uh jacob about the the types of events which are maybe extreme um for for the theorem so For the theorem. So, the basic point is to find an extreme thing for the theorem, you'd like to like an event which is big for L but small for B. That would kind of give you something, that would make this tighter. So, because L has this lower curve, which is kind of pushing it up, you would want to look for events which are somehow make the, are the curve being higher. Because Brownian motion has nothing pushing it up, but Browning motion has nothing pushing it up, but L does have this thing pushing it up. So, but it's unclear whether what the right extreme event is and whether you get a different exponent of five over six. In the proof, there are many places where that five over six kind of magically is the right thing. So it might be either. So it's kind of hard to definitely the proof, I think, cannot.