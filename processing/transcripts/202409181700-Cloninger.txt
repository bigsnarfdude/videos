Okay. Our next speaker is Alex Blinger from UCSD, who will be telling us about learning on and near low-dimensional subsets of the Wasserstein nanom. Awesome. All right. Thank you. Thank you so much for the invitation. Love it here. Great place. Great conference. Okay. So this is collaboration with Carolyn Busmuller, who's a former postdoc in my group. Muller was a former postdoc in my group, she's now a professor at UNC. Really a big driving force behind a lot of this work. Varun Quirano is one of my former students now at Brown, Keaton Hamm, who many of you probably know. Scott Mahan, former student now at PNNL, Pacific Northwest National Lab. So I am interested morally in the same type of question that Dustin, but it's in a very, very, very different setting, okay? I want to think about problems in which my data points are point clouds. Are point clouds or measures. So, what do I mean by that? What I mean is that my point, which in this case is a point cloud representing a mesh of a 3D object on the surface of a 3D object, that entire cloud gets one label, right? Or I want to talk about distances between those clouds, okay? And so what I want to do is basically set up basic machine learning. Basically, set up basic machine learning problems on this setting. How do you do basic machine learning on the space of probability measures? Right? So, I want to be able to construct classifiers. I want to do clustering. I want to do dimension reduction. And this problem shows up a lot. So, I will later talk about flow cytometry, which is a medical test. You draw people's blood. Each person is like 50,000 point cells in nine dimensions, and you want to say if they're healthy or sick. Generative models, we're obviously running statistical distances at the end of the pipeline. Is at the end of the pipeline. You can actually treat images as densities and think about this as being some sort of density on R2 or the unit square. And then also, I'll talk a little bit about shape statistics. So the basic questions are, all right, and this is exactly what I was going to go on a whole diatribe about it, but Dustin already beat me to it. All machine learning is just set up in Hilbert Spaces. So I want to be there, right? I want to be able to do simple things like build linear. Simple things like build linear classifiers and take L T and distances that are really easy to compute. So, my question is: can I find embeddings of my distributions that are going to mimic a statistical distance that I actually care about? Okay, and similarly, can I do it in a way where I'm embedding each distribution on its own on a single pass? So I don't need to, you know, actually be loading multiple distributions at the same time to say how far apart they are. Because that obviously. Apart theoretically because that obviously is going to be highly computationally inefficient. And similarly, once I'm there, can I then build classifiers? So, once I'm in a nice space, can I build simple classifiers? And also the obvious question, can we control the embedding, you know, embedding error, classification error, if we only have finite samples or if we have perturbations of the thing that we actually care about? And additional goals that I won't go into as much now, but that make a lot of sense. This injectivity, kind of this one will come for free. I don't want to map two things. Come for free. I don't want to map two things that look very different into the same point. And I won't talk about it here, but parallelizable as well as like federated. So I can actually have different representations on different computers and never have to pass the point clouds themselves, just some representation. Okay, so what is the distance I want to care about? So the distance I want to think about is optimal transport. So we've had talks about optimal transport. I won't go into it too much, but it's a very natural geometry on probability measures. And the reason I'm going to be able to do that. Probability measures. And the reason I was really interested in this one was because it satisfied, it doesn't have like what I'm going to call a distance saturation. In other words, as you move your distributions further apart, you can identify that fact, right? You know, and similarly, it follows a lot of nice properties to simple group actions, which we'll talk about in a minute. So, again, quick thing on OT. I think this has probably been shown in previous talks. This is mostly what we're going to use. COPS. This is mostly what we're going to use: what's called VOS scheme 2. So it's basically the you want to find a transport map that minimizes the work of moving from mu to nu. Okay, and that is work in the like physics sense if my cost is squared. Not wrong. Okay. So I'm going to refer to that a lot, that optimal transport map a lot, denote it this way, t from mu to nu. Quick theorem that if anyone knows OT, this is like the first thing you learn. Bernier's theorem says that for W2, as long as That for w2, as long as Î¼ is absolutely continuous, then any transport map is actually the gradient of a convex function. So that's your potential. The gradient of it is this cancel. It's the one that takes you through. There is this idea of if you're doing this finite problem, what's called cantivoric relaxation, allows mass splitting. So like you're going to take a mass and send it to multiple locations. There's one point at which that will come up and I will immediately get rid of it. So just know it's a fact and we'll talk about how to get rid of it in a minute. And we'll talk about how to get rid of it in a minute. Okay, so one other aside, space probability measures is quite large. So if I'm just going to work there, I'm done. Like, what am I going to be able to do? This is an infinite dimensional space. It's very hard. So the assumption I'm going to make here, and throughout most of this thought, is that my data is actually not filling the space of all possible probability. And I think this is a pretty natural assumption, right? Natural assumption, right? If you think about, say, a point cloud of someone's blood that's trying to say if they're healthy or sick, there's probably a prototypical healthy point cloud, the Platonic ideal of healthy point cloud, right? The Platonic ideal poor terminology, but of a zip point cloud, right? And what you're getting as data points are push forwards of that or perturbations of that in some way, right? So maybe the point cloud got shifted by a little bit. Maybe it got. Got shifted by a little bit. Maybe it got skewed. Maybe it got rescaled. Maybe there were some perturbations that were occurring in some way. Right? These are the types of data we're going to be thinking about. And when you make this assumption that all of our data is coming from pushforwards of one or a handful of base measures, what that means is you're actually arcing out some sort of nice sub-manifold. And I'm using the word manifold very loosely here, but it is in a lot of circumstances. But you're arcing out some nice sub-space or Nice subspace or subset, let's say, of the Vostosphine manifold. That's really what I care about. Okay, so I'm not going to try to embed the entire Vostosphine manifold into a Hilbert space. I'm out, right? But on the problems that I care about, I want to embed those nicely and be able to build nice classifiers. Okay, so that's kind of this. Does this make sense? Any questions? Yeah. Okay. My data. Yes, yes. No, no, no. Yeah, this was, yeah, there are several. And in fact, in a moment, I'll have multiple base measures and talk about how to classify them. Yeah, so it is multiple. Yeah, that's the or several. Just dot mathematically written. Okay, so what do I want to do? So first, I'm just going to tell you how we're going to do it, and then we'll talk about what it can do. So this is what's called linearized optimal transport. Not my idea. This is a pop. My idea. This is a popular, quasi-popular idea in the optimal transport literature. I think more studied recently. One of the first people I've really found to work on this was actually an engineer at Virginia named Gustavo Rode. He was an undergrad with Akram Al Drubi, which I found out later. Now they work together on this stuff too. So that's kind of fun. So, anyways, he kind of came up with this idea, but didn't put a lot of theory behind it. And then this is kind of what I want to do: talk about the, what's the idea? What's the idea? The idea is: I have a button, I have n distributions that I want to work with. I'm going to fix one distribution, I'm going to call my reference. And the spoiler to this, which you'll see in a minute, is it really doesn't matter what my reference is. It does computationally, but in practice, that reference can be really pretty much anything that's absolutely continuous. Okay, so for real purposes, you can think of this sigma as being a Gaussian. Okay, so now what I'm going to do is for each distribution. Now, what I'm going to do is for each distribution, each data point, I'm going to solve the OT problem that takes me from my Gaussian to my point cloud, to my person's blood sample, right? But I don't care about the distance. The distance is large. The distance makes no sense. Where I put the Gaussian is going to be affecting that distance. That's not the right thing. What I'm going to pay attention to is the map. So I want to pay attention to how I transform a Gaussian into this person or that person or this shape or that shape. Person or this shape or that shape. Okay. That function is the thing I care about. And that function is actually this that embeds distributions into that L2. Okay. And now I'm going to work there. All right. So that's the idea. And that's the functional analysis perspective. I want to give sort of a more classical OT perspective, which is if sigma is my Gaussian here, I take a sample from sigma, it's the star. I take a sample from sigma, it's the star. For mu1, it goes here, for mu2, it goes here, for mu3, it goes here. What I have now done is defined a registration between mu1 and mu2, which is namely the star goes to the star. Okay, and the diamond goes, or the triangle goes to the triangle, the heart goes to the heart. That is my registration. It is not an optimal registration, but in a lot of circumstances, it's actually a close to optimal registration, which is going to be good. Which is going to be good. So that's the idea. And then the question becomes: how do I work with these things? So, first off, I can define what I call W2LOT. This is now just the lengths of those registrations integrated over all my samples of sigma. Okay? So this is a simple thing to calculate. I can compute T sigma to mu independent of T sigma to nu. Now these are just, I mean, they're functions, but in practice, they'll be samples of functions. Samples of functions, and then I just take you know, LT distance is between them, and that is my LT distance, my LOT. And similarly, learning on that, again, I could build some function on that to embed that into a classifier. Or similarly, if I take a finite number of samples, you can think of this, you know, finite number of samples pushed forward by T sub mu is basically going to be a matrix. Now I build a function that takes in matrices and spits out what last. Okay? And so there's questions. And so there's questions, right? What are the natural transformations that will allow this to be classifiers to be efficiently learnable? How close is W2LOT to W2? How stable is this with respect to sampling? There are a lot of questions here. And unfortunately, what I'm going to have to do is kind of go quickly through some of the answers. Okay, so the issue is that in describing this, this is not going to be one paper. I think this is kind of a summary of like four or five different papers, mostly by. Different papers, mostly by people that I listed there, one by my student. It was actually done with Keaton independent of me, but it's still relevant to this question of just kind of summarizing all of the properties you can get here, and then we'll show how this is done. Okay, so first off, what are the types of transformations that are going to be really, really, really nice? So I'm going to call these compatible transformations. These are the things I can do to a base measure or several base measures that arc out my subspace that is going to be really easy to work with. Okay. So the easiest ones, mathematically, like So, the easiest ones mathematically, like truly, the thing we're looking for are actually going to be pushed forwards s such that, well, first off, whoops, sorry. If you, if you have a sigma and you push T sigma to mu, and then you compose it with s, that gets you to s push forward. That's a pretty simple statement. That's always going to be. The interesting question is, when is that actually the optimal map? In other words, when is it that going from sigma to mu and mu to s? That going from sigma to mu and mu to s didn't create some crazy amount of triangle issue, right? And so these are going to be sets, like anything that does that perfectly is going to be what I'll call a compatible transfer. Okay, and we'll talk about a lot of them are in a minute. I want to give one other perspective of this, a differential geometry perspective. You have a Vostin manifold. This is a really bad example of a Vosty manifold because it's a sphere, but in reality, the Vostokian manifold is in like the The Moscow manifold is in like the positive quadrant of infinite dimensional space. So it kind of doesn't bend back on itself, but it's fine. My point is that what we're doing here is we're picking a sigma, which is somewhere on our Vosterstein manifold. That's our reference. And then we're actually just arcing out the tangent space centered at sigma. And the questions of distance are going to be: all right, I have my t sigma to nu, t sigma to mu. There's a distance between them. How close is that? How close is that to the geodesic distance between mu and Q? Okay, that's kind of the question I was asking before about how close is LOT to W2 distance. And this is how, this is one way to think about this. All right, so what satisfies this compatible transformation? The shift scalings, Atlantine transformations, these are very easy. You shift and measure, that's what I'm going to call one of these compatible transformations. So we'll be able to work with these very easily. Global rescalings. Re global rescalings. This will be able to work with easily some combination. Yeah. The types of transformations we can do to our base measure that are going to be very, very easy to work with and where we'll be able to easily apply our. So you have a healthy person, you have a shift of the healthy person, you have a rescaling of the healthy person. And in a moment, what I'm also going to say is a perturb. And in a moment, what I'm also going to say is a pertigrant. No, arbitrarily, arbitrarily large. Yeah. And arbitrarily large rescalings up to the fact that I don't actually want to go all the way down to shrinking it to it. And so these are the things that are what I would call compatible transformations, which have nothing to do with my assumptions on sigma and mu. And then there's a lot more. So if I make assumptions on sigma and mu, if I take like a barycenter between sigma and mu, any point along that. And mu, any point along that line is also going to be what I'll call compatible transformation. Certain shearings of my mu will be compatible transformations. And then I'm also going to find this notion of almost compatibility, which is basically perturbations. So this is, you know, transformations that were close to a compatible transformation. So something like I shift and then sort of jitter it a little, right? This is a perturbation. This is some sort of perturbation. And then there's also things. And then there's also things about temporal transitions along absolutely continuous curves on the Bossy manifold. I'm interested in these, not going to talk about much in this. So, what is the point of all of this? The point of all of this is that there's a lot of transformations one can do to my base, to the base measure to get the relevant theta points, right? Shifts, scalings, shearings, perturbations, the sort of natural things, you know, deformations, the sort of natural things we. You know, deformations, the sort of natural things we would expect to have happen when you're, you know, sampling a bunch of people's blood, let's say. So, with all of those, what can you say? Right? So, first off, on full distributions, there's some very simple theory. This is not hard to prove. This is sort of borrowing from previous results and making slight changes. But basically, if we make some assumptions that sigma and mu are absolutely continuous, then we can actually say that this LOT distance minus the Vosselstein distance. Minus the Vosselstein distance is bounded between zero and something that is basically on the order of the amount I'm perturbing to the one-half power. This is pretty good. Namely, if I'm only focusing on compatible transformations, I don't take deformations into account. What I just said was that all of those transformations of mu I can do, I'm creating an isomer, an isometric independent. If I have perturbations, I have a near isometer. Okay? Okay, so that's nice, but also what this means, for example, in terms of parallelization, is that, you know, previously, if I wanted to find all pairwise Vossoscene distances between my data, I would need to do n choose two optimal transport problems. Now, I can do n optimal transport problems and n choose two cheap L2 distances and get the same distance maintenance or almost the same distance. Almost okay. If you remove some assumptions on mu, the coefficients get worse. This actually can be better. Yeah, 215ths is an insane number. There are ways to actually push this down to like one sixth. So my point is that these things are stable with respect to perturbations. They're not the smoothest. It's not the smoothest embedding in the world, but it works. And still hold a code. Okay, classifiers. So, question that Dustin asked. Let's say I have a mu and I have a bunch. Let's say I have a mu and I have a bunch of transformations of it. Okay, so think a healthy person and a bunch of transformations. And I have a sick person, new, and a bunch of transformations of that, right? Perturbations, all of this. So I'm going to assume, and this is not a bad assumption to have, which is basically the set of all of these transformations that I can do is some convex set. So that'd be like convex specific shifts, galliums, things like that. Or, you know, or have a convex fall. Or have a have a context all even. It doesn't have to actually be all of my shifts form of context, like, you know, or an I do all those. So these are, this is my, basically my generator of all of my data, right? So these are all my examples of healthy people. These are all my examples of sick. Okay. As long as there was no way to simply transform mu and a new. In other words, as long as like healthy people and sick people's point. Healthy people and sick people's point clouds are not just like a simple shift at each other, and it's something more complex. And there's a little bit, whoop, and there's a little bit of minimal separation. Then it turns out when I go into LOT space, when I embed, now I have, you know, points of healthy people, points of sick people, these are going to be linearly separable sets. And it's actually quite easy to prove. It's basically Hanban. And we can also be extremely explicit about. Also, be extremely explicit about these constants if we want. I'm not going to go into the details of it, but one can be very explicit about the constants under various assumptions on mu and sigma transformation. And also, another thing that I'll say is that this was when I did this with one distribution or one reference set, right? How does the Gaussian transform into each one of my data points? You could also have multiple reference sets. You could also have multiple references, right? So start making some assumptions. You know, your reference sets are like a few samples you know, and you do how you transform this sample into all of the data, and this sample into all of the data, and this sample and all of the data, and concatenate. And basically, all it does is strengthen all of the theorems that I was just saying before, treat it, treat it, the set of all the transformations that I could do to my base measure mute. That I could do to my base measure mu. So this would be like a convex set of shifts, scalings, shearings, stuff like that. Again, in reality, I'm going to have some finite draw of these. Basically, what I'm assuming is that there's nothing in the convex hole of the things I'm doing to mu that can turn me into mu. That's basically works. Okay. Yeah. All right. So that's the idea. All right, so that's the idea. And I just want to emphasize that this can also be done on finite sampling because I was just talking about it for infinite, right, for the continuous version, right? But in reality, what are we doing? We're sampling, right? We get some samples from some UK data point. So at some point cloud, we want to classify. And also, we're going to have to draw some finite number of samples, Zi, from our base measure, our reference measure, sigma, right? And push those. Sigma, right? Push those. Okay. And there's many ways to solve this discrete optimal transport problem. One can use linear programming. One can use Syncorn. One can use input convex neural networks, which we talked about on Monday. All of these are ways to solve some version of an OT problem. And what we want to do is say, okay, well, this LOT on the discrete is not far from this LOT on the, but there's a comp. But there's a problem, and this is where I said the counter board shows up: mass splitting. If you have a finite sample and a finite sample, and you find the transport between them, you get a couple, which means that one point splits and goes to a handful of points. This is not the case for the input and convex neural networks, but for the other two, it is. So, for linear programming for Synchorn, you get a coupling. How do you fix this? Because I really do want to map it. Okay. The way to fix it is this thing called the barycentric projection. The barycentric projection. Very simpler. It looks weird there, but it's a simple idea. If this point maps to a distribution over here, I'm going to define a map where this point goes to the mean of that distribution. Okay? So think about it as you have a plan. One point goes to a little family of points over here. Take the average. Weighted out. Weighted by that. And now I'm going to say this point goes to that one. This point goes to that point. Okay. So there's one weird thing about this, which is that now that transport map, it is a map, but it does not take you to the finite sample anymore. But that's fine because I'm interested in going to large scale limits. So it turns out that statistically, as the number of points gets larger, this is actually very, very good estimate of the continuing unit. Okay. Okay. Energy. What do you mean by energy? Oh, well, honestly, it's that if I didn't do that, I wouldn't really have a way to define this linearized optimal transform. Right? It has to be, it has to be as a map. I'm taking a point to a point. And so this works. Similarly, for input-convex neural networks, you could say for each person you loan an input-convex neural network, like a generative model from one Gaussian. Like a generative model from one Gaussian to this person, then to this person, then to this person. And then, if you want to define the cell at t-distance, what you do is you take a point from the Gaussian, you map it to each person, you define distances. There might be, I don't know how, I've been trying to think about this for a while. The question is whether there's a way to compare the plans directly. I'd actually be very interested in that. I agree. But it turns out that this works out pretty well. And it also makes it so that, again, I'm just going to, I'm pushing things to the Going to and pushing things to the space of matrices. And so, this is like a meta theorem because there's actually like 12 theorems hidden in here based off of the smoothness assumptions on the density, the support of the density, whether it's finite or infinite, the type of LOT approximation, linear program, syncorn, input convex neural networks. They all basically satisfy the same, which is that the LOT distance minus the Vasselstein distance for these types of For these types of families of distributions, I'm referring to, satisfy something that relates to the compatibility of the transformations, something about the number of points to the minus one over dimension where the distributions are concentrated, and a number of samples I'm drawing from my Gaussian that you can use basically like Radahawk. And all of the, you know, this is multiple different papers. This dimensionality can actually be fixed with stronger assumptions on. Fixed with stronger assumptions on the density and then doing smoothing on the transport map that you learn. Using smoother input convex neural networks will also ameliorate this dimensionality issue a little bit. And one last thing I want to mention, which is that I've been talking about like supervised learning and stuff like this. You could also do PCA. So what if you add n by n matrix of vostoscheme distances and you want to do an MBS embedding? Distances, and you want to do an MDS embedding of that to find a low-dimensional set of points. Well, you could build that matrix and then take the first couple of eigenvectors to get the MDS embedding. Or you could just concatenate all these transport maps, mean center them, take the left singular vectors, and you can prove that basically whatever your PCA error is bounded by basically the error that you had in approximating the possible. So now we've said that we can do something with supervised learning. We can do something with unsupervised learning, dimensionality development. And so I want to, I know I just have like a minute, so I'm just going to go really, really quick through some examples. In fact, I'm actually going to just skip the synthetic ones because I want to go to the real ones. This is a fun data set. There's a professor at Brown named Lorraine Crawford that told me about this about, oh no, eight, nine months ago. It's really fascinating. Know eight, nine months ago. It's really fascinating. Uh, but I've heard about it before because Ingrid worked on it, other people worked on it. These scans of lemur teeth. So that's a point cloud, right? Two-dimensional point cloud in 3D. And there's these really simple questions that people want to ask about it. There are four different types of teeth. Can you classify them, right? There are four different types of teeth. Can you find some clustering of them or a low-dimensional representation? There's a very small number of samples of these teeth. Can you generate a new example of a tooth that That looks right, but is something that hasn't been scanned. And my argument for all of this is that once you do this LOT as a pre-processing, basically what it's doing is assigning sort of a pre-registration between all of the teeth. And then off the shelf. These are now, it's all, you can almost think about it as though I'm ordering all of the points from each tooth in a particular order. Order, right? It's a smart ordering ball. And then once I do that, I can deploy off-the-shelf ML methods. Deploy an off-the-shelf linear SVM, 100% classification test. Dimensionality reduction. This is a, I may be small. I'm sorry about that. This is a LDA embedding. Each color represents a different cluster. You've got your red, you've got your green, you've got your blue, you've got your orange. And that's just in the three-dimensional space. Data generation. So we can do very So, we can do bary centers now, right? You know, bary centers in the Voss scheme problem is really hard, right? You have two, you have to solve a minimization scheme to find the thing that's in the middle. It turns out that after you've done LOT, the barycenter of two maps is the average of those maps. It works really, really well, right? Yeah, right. No, so that's the one thing. There is a free rotation that they have done. Yes, that's a very good. Yes, that's a very good point. If they hadn't, there are ways we're actually thinking about how to work on that, but I don't have those. Right, so you can just average two LOT maps, you get a new LOT map, you look at what it is, and it winds up actually looking very, very similar to the true bary set. In fact, what we can wind up saying, what we can wind up doing, I'm not going to go into the details of, we do a slightly smarter way to construct the reference set, but basically you can get something where your LOT bary center is about 1% relative. Bary center is about 1% relative error from the phostostine barycenter. And that relative is with respect to the size of the cluster. And the last one I just wanted to mention is this flow cytometry thing, which again, you draw off blood. It's a way to test for AML and leukemia and stuff. And basically, this is three of nine dimensions of a healthy person. This is three of nine dimensions of a sick person. These are very complicated, weird points. And there's very strange artifacts that you're trying to find here to say whether or not they're healthy or sick. Well, it turns out that. Well, it turns out that with very small amounts of data, we're talking, I keep about 20 people, 26 people in the training set, and then the test on the other 23 very quickly with an LOT ensemble, which this one actually did through a deep learning method that I mentioned, the ICNN. You can get great accuracy and pretty good precision recall, and it's much better than deep sets, which is the sort of state-of-the-art ML approach to dealing with point clouds. Better by an order of magnitude. Not an order of magnitude, really, but like it's better by order. Really, but like it's better by all. And I should note that this actually is a deep set algorithm. I didn't mention that before. It actually, you can frame it as a deep set neural network. It's just, it's a very geometric one, right? And so it works quite well. So this is another direction we're interested in. So yeah, that's kind of what I wanted to highlight, which is that this LOT method, I think, is a very nice way to think about how to deal with point clouds that basically can work as a free process, right? And once you're there, Right. And once you're there, there's math to back up a lot of the statements. There's really strong performance. What I talked about here was a series of papers. And then from that point, there's a bunch of extensions that we're looking at that we've done, including modeling flows of particle systems, extensions of this to graph settings. Varoon worked on this where instead of taking samples from density, you take like Voronoi cells and have a deterministic sample. Have a deterministic sampling of the density, you can get similar results. Caroline has gone in some interesting directions about more general, like manifold learning on sub-manifolds and velocity and manifold. And then this LOT is a pre-processing step is a paper that we're about to put out. It just is kind of like a case study to show that this really does work. So that's about it. I'm just going to make a quick shameless plug. I'm sorry, I ran a couple minutes over, which is basically we have a bunch of different opportunities to hire postdocs. In particular, there's an NSF. In particular, there's an NSF grant that's funded between myself and Gaul, as well as some colleagues at USC. Ironically, it's not on either the work I talk about or the work that Gaul is going to talk about, but it's on some really cool geometry and deep learning. So we're looking for a postdoc there. We have visiting assistant professorships, a bunch now. So please, if you have interested students, please, please, please send them that way. Okay? All right. Thanks. Okay, we're over time. Okay, since we're over time, who want you set up and then we can take Westminster two? Yeah.