So what I'm going to talk about today is also very much a work in progress. And it's the two kind of separate projects, one with precipitation and the other with flooding. So there is a large group of people involved because we have some statisticians and then we have hydrologists working with us on the flooding side and meteorologists working with us on the precipitation side. And just see how I can. There we go. So the sort of the reason for this work is a very concrete applied question. And that is when we're designing or constructing an infrastructure. There's a lot of regulations that have to be followed. Some of these have to do with how the infrastructure is designed with respect to precipitation. Respect to precipitation and flooding. Specifically, they have to be designed so they withstand certain rare events where we need estimates of these rare events for a given location where the infrastructure should be built, a given duration or accumulation period that depends on the type of infrastructure, and the frequency or the return period that we're focusing on. That we're focusing on. Now, each one of these comes with sort of an interesting set of questions or problems to work on. Of course, we also need estimates at locations where there is little or no data available. And there has been a lot of work on this in recent years. There's a number of people here in the audience who've worked a lot on this. And then, even if we might only Even if we might only need estimates for a specific duration for each type of infrastructure, it is of interest that their estimates are consistent for different durations. And this is simply because if we are, say, estimating that we're going to see more precipitation in a three-minute period than a five-minute period, we know for certain that something is wrong with our estimation. So we need to do a little better than that. On that. And then related to the return period, of course, we have the obvious question: what does it mean to talk about a 100-year event in a non-stationary change of climate? And then what I'm going to focus on in this talk is the second point about the consistency between the different durations. And interestingly, sort of most of the work on this question has been done either in the meter. Either in the meteorological or in the hydrological literature. So there's not too many sort of statistical works related to this. So how, just a little bit about where our data come from. So this is, we're going to start with the example on flooding. So what we have to start with is a time series of observed stream flow at a reference duration, which is our Duration, which is our finest resolution that we have available, and then we can create sets of annual maximum for any other duration that is coarser than this by taking sort of running averages. And here we see an example where we have the mean stream flow over one hour, five hours, and 10 hours. And then if we measure this in cubic meters per second. This is in cubic meters per second, we see that we have the highest observation for the shortest duration. And then what we then have is we had so-called flood duration frequency or QDF models that try to build a parametric relationship between the quantiles of the distribution for the different durations. So, what we see here in the figure are Here in the figure, are essentially three separate data sets indicated with three different colors, three different shades of blue, where we have fitted a distribution to each of the data sets that gives us three curves, which are then the quantile functions, where we have the return period on the x-axis and the magnitude of the return level on the y-axis. And so the Kiki. And so the KGF model then is a model over all of the three curves simultaneously, where we have a parametric relationship that associates one curve with the next curve. And then what would sort of be from a statistical point of view, sort of a natural next thing is to think about how the data is built, that we start with the sort of finest resolution that we have. Resolution that we have, and we build coarser and coarser resolutions and try to use some sort of stochastic process theory potentially to build what this relationship should look like. But then the issue is then that if we and then we would model this using say the G V distribution and what the hydrologists do is they use a transformed version of the G V distribution where we have a little more explaining. we have a little more explainability in terms of what the parameters actually mean. So we have the shape parameter and then instead of location parameter we transform that to the median of the distribution and we also transform the scale parameter into this parameter beta. And then when we have done this parameter transformation then the quantile function is given here in the bottom equation on the slide. Bottom equation on the slide, where we see that we have the median, and then we multiply the median with a factor for each probability level, where this factor depends on the two remaining parameters. And then if we then look at our data, and so now we want to try to build a relationship between these distributions for the different durations. Distribution for the different durations. And then here is an example of what our data can look like. So here we have on the bottom panel, we have these mean stream flow observations for one to nine days of average. And then the dots that you can see are the annual maximum for this year at this location. At this location. And what we can see is for the three shortest durations, so one, three, six days, we can see that the annual maxima are coming from one event that happened on July 21st, whereas the remaining three annual maxima are coming from a different event that happened earlier in July. And then what we can see on the top panel is On the top panel is what Manuela also talked about earlier. Just she was talking about a lack of bees, and now we're talking about excess bees. So the flooding can sort of be caused by two different processes. We can have snowmelt or we can have rainfall. And the darker blue curve is the amount of rainfall that is happening in the period. And the lighter blue. And the lighter blue curve is the snowmelt. So, what we can see here is that the flooding event that happened earlier in the month is mainly caused by snowmelt, whereas the flooding event that happened towards the end of the month is caused by rainfall. And so what we see here is that there isn't a direct necessarily a direct relationship between the annual maximum. mean the annual maximum of the different durations because they might come from different processes. So it's not sort of, we're not trying to model a single stochastic process, but a number of different things. So thus, our approach here is basically to think the parametric relationship between the different GEV distributions in a way that fits our data. And if we look at the literature, then Look at the literature. So, we have this model here, which was proposed by Javel in 2002, where he proposes to use to push the duration dependency into the median parameter of the model. So, we have now that the median depends on the duration d and has this additional parameter delta, which is positive. And then what we can see. And then what we can see what happens in the quantile function is that we're basically multiplying all the quantiles with the constant that depends on the duration. And we also looked into an extension of this where we now have two different sort of places that we put duration dependency into the model. So we put it into the medium as before, but we also put it into the scale parameter. It into the scale parameter. So then the effect of this, if we look at the quantile function again, is that now we are sort of pushing the duration dependency into both components of the quantile function. And then if we look at what this looks like, so here is an example where on the left we see the original model with a single duration dependency parameter. Where all the quantile functions have the same slope, but they sort of are transformed up and down. Whereas on the right-hand side, if we have these two delta parameters, we can change both the sort of the placement of the quantile function as well as the shape in the tail. And then we fit this to Then we fit this to a dataset consisting of several time series of stream flow from different types of catchments in Norway. And here are some examples that you can see. So here we're looking at fairly short durations from one hour to 72 hours. And we can see sort of the different dots that are the observation. Different dots that are the observations and the lines that are the fitted marks. And we've done the fitting by using Bayesian inference. So we can also get these bounds on the estimates based on the posterior distributions. What we see here is that we can stare at these as much as we like, and it's kind of hard to see a big difference between the two models if we're just staring at these. So, what we did is that then What we did is that then we assessed how well these models do in predicting out-of-sample durations. And this is for stream flow data especially important in that especially older measurements were not sampled at a very high frequency. So it is often of interest to extrapolate to higher frequencies than are available in the data set. And so here we looked at So here we looked at estimating or predicting the distributions we get for either one hour duration or 12 hour durations when these are left out of the data. And what we see, and this is sort of a general conclusion, is that if we are interested in estimating very short durations, it seems we need the two parameters, the two delta parameters, on a more flexible model. Flexible model. Whereas, if we're interested in sort of longer durations, we have a smoother process. And there, the simpler model actually fits better. And then we look at a little bit and move over to subtition. And meteorologists look at similar things and they plot them in completely different ways. So, and for rainfall. So, and for rainfall, these are called IDF curves, where we have now estimated rainfall intensity for a given duration and return period or frequency. And here it is common to plot the estimates where we have the duration on the x-axis and the intensity on the y-axis, and then we have different curves for the different return periods. Return periods. So, what this means is that the different dots that you can see sort of in a vertical line for given duration, those dots are estimated together based on that data. And then we move over to the next duration and we get our estimated dots for that duration. And then, once these estimates have been obtained independently, Have been obtained independently, then we connect the dots. And how this is done is that we fit G V distributions to annual maximum series. Now, of course, as we heard in the first talk yesterday, G V distributions might not always fit these block maxima, particularly when we sort of move over to looking at very long durations. Durations, but they are currently sort of the standard thing that is used for this type of presuming. And then we fit these independent GV distributions to each topic, combine them afterwards, and the issue that can occur is that we can have these crossing curves. So in this case, this basically means that we're saying if there's going to be less rain in a five-minute period. Rain in a five-minute period than in a three-minute period for a certain quantile. And then the question is, now, does this really happen in practice? What does this actually look like? So we looked at some data. We looked at data from 83 different observation stations in Norway where we have one-minute data. So we can start at one minute and move up to 24 hours. And we fit. And we fit 16 different durations in this time period and we found that the results were inconsistent for 25% of the stations. So this was really a substantial issue that we looked at. And our idea to solve this was to take a very pragmatic approach and to basically take the independently obtained estimates and see if we could do some sort of a post-processing of these. Do some sort of a post-processing of these estimates so that in the end we get consistent curves. And the motivation for doing it this way is that all this estimation here is done using Bayesian inference. And if we look at the Bayesian inference, we can see an example here on the very left-hand side where we can see the solid curves, which are the posterior median. Which are the posterior medians from the posterior distributions of the inference. And then the dashed curves are the 98% probability found in that estimate. So we can see that there is huge uncertainty in the estimation. Of course, a question here is really, can we use these estimates for anything when they're so uncertain? They're so uncertain. And there is a little bit the point that Anthony made yesterday is that people actually need a number so they can design the infrastructure according to the law. And then we have to try to give them the best number, but we might not need to pick medium from the distribution if that gives the consistent estimates. So what we propose What we propose here is to pick a best estimate that is as close to the posterior median as possible under the constraint that things are consistent across different durations. And so then for a given duration, we might pick posterior quantiles for something close to median, but not quite the median. Meeting. And then we have some sort of an iterative algorithm that searches through to find the best solution for the estimate. And then if we applied this to our Norwegian data set where we had 25% inconsistencies, we find that we really don't have to go very far from the median in order to obtain these consistent estimates. And then we perform And then we performed a little simulation study to check so that we had a truth to compare against, where we found that the album returned something that was not only consistent, but actually gave us better estimates than we had before from the unadjusted estimates. Meaning that we were actually implicitly borrowing strength between the different durations. Between the different durations. And so then the work on the latter work of the idea of curves has been published in a paper last year. And we have on our package on GitHub where this fix or this post-processing is performed. And this could take in any data that is obtained using a Bayesian estimation. Obtained used in a Bayesian estimation approach. What is very nice about this method is that it is flexible, it is very quick, there is hardly any work to be done. On the downside, it gives us curves that are not quite as smooth as polyparametric models. And it is not suitable if we need to extrapolate to out-of-sample durations. And so, for that, we have the parametric models. Parametric models that are less flexible, but give us smoother results, more appropriate for extrapolation, but on the other hand, can be challenging to fit. And that was it. Thank you very much.