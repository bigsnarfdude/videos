Sorry, a short introduction. So it's wonderful to be here, as always, being in BAF, and also very nice to be in this meeting. And hello to everyone who is not actually present in the room. So I'm going to talk about a rather classical problem in combatorial optimization, but I'm very soon going to steer that into a nice familiar look. A nice kind of familiar-looking extreme comatorial problem. And well, I won't actually be involving much in the way of geometry, but actually there is a sort of topological basis to what I'm going to tell you. Unfortunately, I probably won't have really much time to go into the details of the topological aspects of it, but but it is there. But it is there. I will try to say a little bit about what the topological approach adds to this topic. Okay, so let me also see. Okay, so what is the Santa Claus problem? Its official name Santa Claus problem. Its official name is the restricted maximum allocation problem, but it's more fondly known as the Santa Claus problem. And an instance of this problem consists of the following things. There's a set P of players, and you can think about the players as being some children. There, secondly, a set R of resources, and Resources, and I think of the resources as being toys or presents that the children would like to have. So here they are packed in colorful boxes. And the third component, oh, and I should mention this is an important addition on this problem that every resource has an intrinsic value, some positive amount you can look up in a catalog to see what value each toy. What value each toy has. Those are the values. And then, thirdly, for every player, there is a specified subset of the resources. I call this the light set, LP, of player P. And that is just a subset of the resources. And you can think of those things as the toys that that child would like to get as a present. So this child is not interested in this toy here, but would like to have. interested in this toy here, but would like to have any one of those, well, any one of those toys. So that's kind of a wish list for the player. So this is an instance of the problem. And what is a solution? A solution is an allocation of a subset of each like set to each player. So VP is the kind of bundle of toys or presents that player P is going to get. These resources Get. These resources are indivisible resources, so the bundles must be disjoint. So that is a kind of distribution of the toys or the presents to the children. And how we measure the success of a particular allocation is by this parameter, the min value. So the min value of a solution is the minimum total value that any child gets in his bundle. Child gets in his bundle, his opera bundle. So it's the minimum sum of all the values of the resources assigned to child P. And the aim of the problem is to maximize the min value. So you consider it a successful solution if the maximum of this This min value is as large as possible, so every child gets a total of a large quantity of total value in the toys. So Santa wants to distribute the toys so that the minimum received by every child is as large as possible. So this is a kind of standard allocation problem. Problem. And here we're really allocating things that are nice. So these are things that people want to have, and so we therefore would like to maximize the min value. There's another also very well-known complementary problem in which you minimize the maximum value, and that's more of distributing things that will maybe like chores or jobs and things that people don't want to have. So you don't want anyone to suffer significantly more than anyone else. Than anyone else. Maximin allocation problem. Okay, so that's the problem. And this has been very well studied by many, many people. I'll be unfair to all of these people by not telling you all the important things that they did. But I will just pick out in particular something which looks quite unexpected, which looks quite expected, that it is NP hard to solve this problem. Is anything hard to solve this problem, or even to approximate it to within a multiplicative factor better than two? That was known already in 2005. And so for this reason, the main interests in studying this problem have been in finding good efficient algorithms that will either estimate the optimal value or even find good allocations for the problem. Okay, so Okay, so this is the problem. And of course, I would like to formulate this in a more commentarial way, at least a way that looks more familiar to me. So we can formulate the problem in terms of hypergraph matching. So if we have an instance i of a problem, we can fix a target value. Let me call it little m for now. For now, we're kind of hoping that we could get an allocation with min value m, at least m. So to find out whether we could do that, we can do the following. We can construct a bipartite hypergraph. Bipartite, I just mean the vertex set is partitioned into two parts. P, these are the players, and R, these are resources. And every hyper-edge is going to consist of one player. Of one player and some subset of resources. And so, how do we define this hypergraph depending on I and M? For every P, I look at the like set of P. These are the resources that the player P would like to have. And every time I find a subset of these whose values add up to at least the target value M, I make this a hyper edge together with P. Okay, so this plays. Okay, so this player, these two add up to at least m in value, so that forms a hyper edge. I might as well take only minimal such subsets, so let me say that those are going to be the hyper edges associated with P. This one happens to be at least worth at least m all by itself, and those three together are worth at least m. Okay, so I can formulate a Okay, so I can formulate, I can form this hypergraph, HIM for every instance i and value m. And then what would I be looking for in this hypergraph? A solution to i with min value at least m is going to correspond to what I call a complete matching in this hypergraph. I have a matching, so a set of completely disjoint edges that saturate every piece of the. Every P here in this picture is getting a subset of his or her light set whose total value is at least m. That's corresponding exactly to a solution to this instance of the problem with min value of east m okay, so that's how we can reformulate the problem. We want to find the largest m so that our hypergraph h i m has HIM has ephemeric matching. Okay, well, so right, so that's the problem, and these hypergraph managing are also hard to solve. So a standard way of approaching problems such as this, especially when looking for approximation algorithms, is to consider fractional relaxations of the problem and see. And see whether, so this is something that is normally easy to solve. One can have an efficient algorithm to solve it. And then we would like to compare fractional solutions and the integral solutions and see if we can obtain any information by understanding how these relate. So, if we have a fractional solution, so what does that mean? There are various things it can mean. It can mean. You can suggest various different models of a fractional solution. But normally it means that you think of some way of imagining that you can, after all, assign fractions of resources or toys to players. And we have this notion of the integrality gap for a particular choice of fractional solution, type of fractional solution. Solution is just the ratio of an optimal fractional solution to an instance i to the optimal integer solution. So the one we really want to get. This f for a particular instance. And if we could obtain a guarantee, a guaranteed upper bound on this integrality gap for some particular choice of fractional solutions. Fractional solutions, then this F will also give us a will tell us that we have an efficient algorithm for estimating the optimal integer solution, everything we want to know, to within this factor f for every L. So there is an interest in obtaining fractional solutions to this problem and knowing that Knowing that there is a bounded integrality gap, and then that will correspond to an algorithm for estimating the optimal solution. Okay, so this is something we would like to do, and so we want to think about what kind of fractional solutions might give us a good bound on the integrality gap for this problem. So, here's really the most So here's really the most natural, probably fractional relaxation of the problem. So we think about indeed just instead of assigning a particular toy to one child, we imagine we can assign some proportions of this toy to various children. So that's going to correspond to things like this. So we'll have non-negative vectors like here, B, P, R. So P are layers. R are P are layers, R are resources, and BPR is just imagine assigning this BPR proportion of resource R to players. So what conditions should we put on these vectors? Obviously, we're not going to assign any proportion of, any non-zero proportion of resource R to P if P doesn't like R. So if R. So if r is not in the right set of p, these things, this quantity will be zero. And the obvious natural condition with respect to resources is that I should not assign more than 100% of a resource to any of the players, to the total of the players. So the sum overall of P's of this portion you're assigning R to the P's, that should be at most one for every resource. One for every resource. Okay, so that's definitely a natural fractional relaxation of this. And then what do we want to maximize? Here, we'll just take the minimum of the sum over all the resources of these quantities times the value, and we would like to maximize that. And it's also clear from this that if I happen to have a value, That if I happen to have a solution where all of these numbers are either 0 or 1, then I really will have a genuine integer solution problem. Okay, so this is a natural choice of fractional relaxation, and this is easy to solve, very easy to solve, so that's the good news. But but the bad news is that that the uh this does not in general give us any kind of bound at all on the integrality gap. At all on the integrality gap. So here's a particularly illustrative small example. Just imagine that the set of resources is actually only one resource, some large, expensive objects, let's say a house. Everybody wants the house. But as long as there's more than one player, well, the minimum value is easy to calculate. I would just assign to every player the one of the To every player, the one over the size of p proportion of the house to every player. Great, that's the min value, but the optimal value of integer value is obviously zero. You can only give the house to one person. If there's any more than one person there. So this does not, unfortunately, give us any kind of bound at all on the integrals we have. Okay, so that's not a good choice then of fractional relaxation for this problem. So in So, to look for better fractional models, this so-called configuration LP was introduced. This is by Monsel and Surodenko. Very nice and important development in the study of this problem. So, here's how the configuration LP goes. We start out the same way as I We start out the same way as I said before, so I'm going to choose some target value. Now I'm going to call it faculty, but I'm going to consider exactly the same hypergraph as we did before. So this is a target value, and you're wanting to know, is there a, in this case, fractional solution, I'll describe what it means in a minute, with min value at least. So we define exactly the same hypergraph. Hyper edges are some edges are subsets of the resources liked by P, whose values add up to at least this target, T. And these are called the configurations for P. And again, I'll take only minimal ones, might as well. So these are configurations for P, I'll say configurations owned by P. And what I call the set of all the configurations owned by P for the particular choice of T. The particular choice of T, this simple T. Exactly the same hypergraph as before. But now we're going to have a notion of a fractional solution to this configuration LP. So what is that? So a fractional solution to the CLP, once you've chosen the target value t, is the following thing. This time it's a vector, and now I'm going to have an entry in this vector for every player, P, and also for every configuration. And also for every configuration on the pipe. This is a potentially very, very long vector. It's a clear exponential length. What are the conditions? So the XPC, I think of it as player P is kind of putting a claim of some value between 0 and 1 on the configuration C. So we do that so that, first of all, So we do that so that first of all every player puts a total claim of at least one on all his or her configurations. And for every resource, we have basically the same condition as before. If I look at a resource, I look at all the configurations that contain this resource, and the total claims on all of those should be at most one. So again, you're not going to use up more than one. Use up more than 100% of this resource. That's what this condition says. Okay, so these are the constraints for this CLP. And we're actually, any feasible solution at all to this will turn out to be a good solution for what we're looking for. Again, we can observe that this, if we happen to be able to find this vector. Find this vector where every coordinate really is 0 or 1, then again, every player will be claiming exactly 1 of his or her configurations. And so he or she will get a bundle satisfactory to that player. And we won't over assign any resource to one player. Okay, so this is our definition of solution to Solution to the fractional solution to the configuration LP. And so we can think about whether this will give us a better integrality gap bound. So in order to check that, we should first of all check that this one-house problem, the instance of the previous one, is not going to be a problem here. And indeed, it isn't, because, remember, Because, imagine going back to the one-house situation: if there's only one house, everyone wants it, then there's actually only one configuration for every player. It's just that house, singleton. So, if everyone is going to put a claim of at least one on their configurations, then this is already not going to be possible if there's more than one thing. So, these constraints will prevent there being a non-zero solution to Solution to the COP for this case. So this will be zero, and the optimal solution will, integer solution, will also be zero, and so this does not ruin our chances of getting a bounded integral to that. Okay, so it turns out that indeed this choice of definition of fractional solution does give a good integral bound. Bound. Now, however, if you think about the form of even this problem, the CLP, well, these vectors could have exponential length, so can you even solve it? That would be a potential problem, but it turns out that you can. I'm not really going to explain that, but it's a striking fact, at least me when I see it, but this is in fact a kind of standard thing. Of standard thing in optimization that one is able to find the polynomial time algorithm for this problem to determine whether, given a particular T, whether the CLP for this T has a fractional solution. So this implies that just by guessing appropriate T's, say by doing binary search, for example, one can find a T for which this For which this CLP has a fractional solution and it's as close as you want to optimal. So you can approximate it to within any desired accuracy. So we are going to think of this CLP finding an appropriate T here, which is as close as you want to best, is going to be a problem which one can solve efficiently. So our job is to invite the customer from optimization who comes with an instance of the Santa Claus problem and with a T for which the CLP has a fractional solution. And in order to say something about the integrality gap, we would like to demonstrate that our hypergraph HIM hypergraph HIM has a complete matching for M as large a proportion of T as we can do. So we would like to find a large alpha, as large as we can, so that HIM, our hypergraph that we defined before, has a complete matching where m is alpha t. And then 1 over alpha will be an upper bound on the integra together. Okay, so I have a small example here which demonstrates that we could not do better than one half for alpha. We already expect we can't do better than one half because of the NP hardness approximation result, but there's actually a very small concrete example on which you can see that. I will maybe just say what this I will maybe just say what this picture means, but not go through the details because I don't have very long. But what this picture means is that these four black dots are players. So I just have four players. I have a simple hypergraph here which describes their light sets. So for example, B likes this red toy and this blue toy, this green toy. The toys have values either one or one half. The configurations The configurations then for the choice of target t equals 1 are: there's one configuration which is the red toy by itself, because it's worth 1, and one configuration of a pair of green, adding up to 1, because they're each worth 1, huh? So the configurations for every player here are one singleton and one pair. And from this picture, it's easy to figure out, but I won't go through the details. Easy to figure out, but I won't go through the details, that there is a solution, a fractional solution with target t equals one, but it's not possible to assign everybody a bundle that's worth at least one. So the best you can do is somebody is only going to get one of those blue or green things, and they will only get one half, and therefore the minimum, the min value is almost one half. And it is one half in this case. Okay. Okay, so that's what we would like to do. Now, let me just tell you the history briefly. When Manson and Swedenko introduced this, they were able to show a very slow-growing, but nevertheless growing, function of P for the integrality gap. But I think this was already an exponential improvement on what was known before. Later, a large unspecified constant was. Unspecified constant was achieved by Wagen, and that was made more algorithmic by these other authors later. Then it was shown quite an important development that the integrality gap for the CLP is almost four by Asapur Veigeri. And since then, this four has been gradually chipped down by using more and more elaborate and intricate And intricate adjustments to the general argument of this first one until the current 0.808 something for the integrality. Okay. So I will just say very briefly how this problem was approached and even more briefly how we altered the approach. But But it was based on, well, I said at the core, there was this hypergraph matching problem, and one really has to solve or approximate something via this hypergraph matching setting. So I think due to time, I won't describe all of these details, but I will just go to this hypergraph matching theorem, which was. Which was used. So, this is now quite an old theorem from 1995. And it tells you that if you have a bipartite hypergraph that does not have a complete matching, then in the setting that we are in, where we have P and R, it turns out that there will be a subset of the players, a subset U of P, and think of it as a bad subset, and a small set of high. Small set of hyper edges incident to the vertices of U, which has some properties that aren't so important here, but this is the most important one, that every hyper edge incident to the set U is also intersecting one of these hyper edges in these special hyper edges, of which there are at most to be minus two. 2 minus 2. Oh, picture. If there's no complete matching in a hypergraph, then there's a subset, a bad subset U of the players here and a small set of hyper edges, we happen to be in very new group here, that form a cover of all the hyperedges incident to you. So every hyper edge incident to you also has to intersect one of these. So if there's no If there's no complete matching, then there's a small cover-up. Now it turns out that if we look at the consequences for our setting, and I will not go through details here, just say the same thing, if there is a subset U and a small cover, then it turns out that we can construct a feasible solution to the dual of the CLP. The CLP. And here I've written down the dual, and I will not go through the details again because of the time. But you can find a solution to the dual, and then LP duality will take that solution. It looks like this. We'll take that solution and give you, in the end, an inequality that gives you a lower bound on the total value of the cover. Total value of the cover, but then we will also have an upper bound on the total value of the cover, which coming from the Heinkel graph matching theorem. And those two bounds will allow you to estimate this alpha, and you will find that the alpha is bigger than one quarter. So this is a way of formulating the proof of Assaf, Prof, Elga, and Savoni that the integrality gap is the most important. The integral to get is the most important. Now, we have two minutes left, so I will just tell you what we do differently. And I should have mentioned, it was written on the slide, and this is joint work with DePor Sabo. What we do is replace that theorem A, which is a combinatorial condition for a complete matching in a bipartite hypergraph, with a different criterion for the existence of hypergraph matching. And this is based on a topological Matching, and this is based on a topological notion on topological connectedness for the independence complex of the line graph of the algorithm. It's quite long and I don't have time to explain, but I will just tell you that it has a topological basis. And this criterion is really strictly stronger than theorem A, in the sense that it implies theorem A, and there are applications of theorem A that we know that as Sorry, the applications of this theorem that theorem A is not strong enough for. So I will just tell you what difference it makes, so what we get in terms of results, first of all. Here I've written down again that progression of work on the integrality gap for the CLP. And using this new approach, we can really bring this down substantially. The bring this down substantially further to a little bit more than three and a half, from 3.8 to 3.8, which was the best earlier. That's in general. And also, there's a particular special case which is also well studied, a special case of a problem in which the values of the objects, the values of the resources, are actually come in only two types. They're the expensive ones, which are worth one, and the cheap. Expensive ones which are worth one and the cheap ones which are worth etc. And this particular further restriction, the integrality gap, is known to be at most three, but with our methods we can really get significantly more detail on that. Well, it's still at most three, and it's really less than three unless this basically the ratio between this epsilon and the epsilon and the optimal CLP target T star is a very specific number, either one-sixth or one-third. And if you're away from those, then the integral of the gap is significantly better, 2.75, and as x goes to 0, it's below 2.5. And we know the bound of 2 from below, it still applies even in this case. So it's quite significantly better. better. And then just the very last thing I will tell you is in 30 seconds, what difference does this new approach make? Because we're using this different criterion. It's very hard to apply in general, so it's really quite technical in how one applies it, but in this case it is possible. And the difference really is. And the difference really is that in theorem A, which I will show you again here, in theorem A, it tells you that if there is no complete matching, then there's a small cover. But you really have very little choice. You essentially have no choice as to what the small cover is. In our approach, we are able to generate many, many small covers, which via basically the same approach, via the dual, will give us. The dual will give us bounds on the integrality gap. But the fact that we can construct many different covers means we can optimize over these covers and really make a difference in the integrality gap. We will find that one of these covers, these small covers, is really quite small and therefore improve the results. Okay, but my time is up, so I will stop and thank you for your attention. Maybe one quick