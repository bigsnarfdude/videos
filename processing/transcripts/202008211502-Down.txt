Okay, so everybody can see my screen okay and can hear me? Yeah. Yeah, okay, good. We can hear you. Great, thank you. So my name is Doug Down. I'm Edmaster and you're gonna have to put up with me for the next few minutes. This, my student, Parsha, was originally going to present this, but Was originally going to present this, but was unable to. So I'm sure she'd do a far better job than I will. But so the idea behind this is I want to give you a little bit of food for thought here. So there's not going to be a lot of definitive answers, but I think there's some interesting questions here. So I'm going to give you some motivation about what we're doing here. So, you know, the talks that we've heard today, most of the body of literature of queuing theory focuses on the situation where we assume that we Situation where we assume that we know underlying distributions. But in this day and age, I'm not going to say that machine learning is the be-all and the end-all, but certainly there's been an explosion of work on enabling the prediction of quantities using machine learning. Of course, there's other statistical predictors that one can build and things like that. But I do work in some areas of computer performance evaluation. Computer performance evaluation. There are machine learning models all over the place for predicting execution time of computer jobs. There are, I've seen some literature in the healthcare area in terms of, you know, I think there's probably predictors for length of stay. I invite you to listen to Nali tomorrow. She's going to talk about where there's a predictor for demand in an inventory model. And so if you believe me, So, if you believe me that we now have increased ability to predict quantities, I'm going to give you a scenario here. So, an initial scenario is: suppose that we have an MM1Q, everybody's familiar with that, but now there's an associated predictor for the service time. So, every time a job arrives to the system or a customer arrives to the system, it comes equipped with an estimate for its time. So, the question is, how do we leverage those predictions? How do we leverage those predictions? So, this is an optimistic view. So, I'm going to do an optimistic view and a pessimistic view. So, I have all this distributional information, but now I have this extra information. So, the pessimistic view, which is closer to my viewpoint on things, but the pessimistic view is kind of the other way around. And this kind of comes more from the computer science side. So, suppose that we know the service times of income and customer. Know the service times of incoming customers, but with an error. So you can think that this is exactly the same as what I gave before, but this is more the scenario, you know, in the computer science. Suppose I'm downloading a file. So we know that the downloading of a file should be proportional to the size of the file. And if I had a certain amount of bandwidth, I could get you exactly the time to download a file. But there's going to be some variability introduced. So I might have an estimate of that time. Estimate of that time, but here I'm thinking about an error. So the pessimistic view of the same problem is how best to compensate for these errors. Okay, so to do this, I'm just going to mention a little bit about size-based scheduling. I'm not sure that people are, everybody's familiar with these kinds of things, but in size-based scheduling, so now I know something about the size of a customer when it arrives. Customer when it arrives. So there's these kind of standard results. So the first one is a well-known result that the discipline, the shortest remaining processing time, is optimal in a strong sense. And this goes back to the 60s. So this is, just think of an MG1Q. So sorry, we're thinking about an MG1Q. So this is optimal in a strong sense. And just think right now, it's stronger than this, but let's just think that we're minimizing the mean response time or the mean weight. Time or the mean waiting time in the system. So there's also a version called the shortest processing time. So shortest processing time just takes into account the actual processing time of a customer and doesn't keep track of the remaining processing time. That's another size-based scheduling policy that's appeared in the literature. And then, as I said, more in the computer science end of things, there are these class-based scheduling policies where classes are determined by size. Determined by size. And there's these things called Cytotype policies. But just keep in the back of your mind with class-based scheduling, just think if we knew the size of a customer, the small ones go into one queue and the big ones go into another queue. Now, I could have more classes than that, but just think about that setting where smaller in one, larger than the other. Okay, so I'm equipped. So there's already known these size-based scheduling policies. So, the optimistic view of this, so if we go back to our problem with our MM1Q, so why don't we just suppose that the estimator gives us the right size, right? So, the estimator, we don't know how bad it is, but let's just assume that it gives us the right size. So, what I'm going to do is I'm going to choose the optimal policy as if this estimator was correct. So, this is the attraction that is very straightforward. So, this is the attraction that is very straightforward. I'm implementing a known policy in this setting. So, the question is: what do we get? So, I'll give you an example. So, if you're interested in these problems, there's a really nice paper that I don't know whether it's been published yet, but it certainly was on the archive Michael Mitzenmacher wrote. And I'm going to give you one example from that paper. And this example is. This example is so there's much more general results than this, but just to give you a flavor of these problems. So I have an MM1Q with arrival rate 0.9 and service rate 1. So suppose that a customer of size X will have estimated size that is exponentially distributed with mean X. So the actual size is X, but I get an estimate that's exponentially distributed with mean X. So, you know, these estimates aren't Estimates aren't that great. In particular, you know, the estimate for large jobs with large X can have significant errors. But if I simply use the shortest remaining processing time discipline with the estimated service times, this results in a decrease of the mean response time by a factor of between two and three, as if you served the customers in first come, first serve order. Come for a server order. So, the moral of the story for at least this example is using such information can have huge payoffs. So, using such information, sorry, not just using such information, but using such information in a very naive manner, you know, just let's not even worry about what the estimation errors look like, then we can have a huge payoff, you know, a factor of two or three improvement in performance. Okay, so the pessimist in me says, is this always true? But I will comment that I think that if you look at these results of Mitzenmacher, basically if you're doing, for example, service time estimation and the service times have fundamentally low variance, then this naive approach works really well. So if you're interested in these kinds of things, you know, think interest in these kind of things, you know, think about that. But now I'm going to talk about the pessimism. Pessimistimy is going to talk about the limitations of the optimistic approach. So there is a body of work, as I said, this is more in the computer science literature. So some work way, well, way back, but in 2004, there's some work that now, if I start looking at service times that follow a heavy tail distribution, then we have to be careful. Have to be careful. So, you know, so using that naive scheduling only works well when the estimates and the actual job size are correlated. So, like a correlation coefficient of 0.75 or higher. And there's a couple other results, but essentially what I want to indicate is that there's a body of work that has looked at this, as I said, less in the more OR queuing-oriented literature, but more in the CS queuing-oriented. But more in the CS queuing-oriented. So, let me give you an idea first of the mechanism that could go wrong, that makes things could potentially make things go wrong. So, here is, you know, here's some insight into that. So, let's look at an MG1Q where I'm going to use the shortest remaining processing time discipline. Now, the shortest remaining processing time here, I'm using the estimated job sizes in place. The estimated job sizes in place of the actual job sizes because this is all I know. So, a consequence of this policy, so I'm going to use a really naive version of SRPT. You could raise all sorts of objections, but when the remaining processing time reaches zero, then, or this estimated remaining processing time is better, maybe a better way to phrase it, unfinished job or customer is not preempted until it completes. Completes. So this means that any customer that has an underestimate of its size, as soon as it hits that zero value of its estimated remaining processing time, then it essentially becomes the highest priority customer in the system and monopolizes the processing. Now, the objections to this, you know, people have looked at these kind of things in the literature. You know, there are estimates, there are You know, there are estimates, there are proposals that really what you should be doing is re-estimating the service times as you go. There are also works that say what I should do is when the remaining processing time reaches zero, I should treat those customers differently because I know something is wrong. But we're just going to continue naively here for the purposes of illustration. So this is a figure from a paper by Dellamico. By Delamico, and I don't have to spend much time on it. I think the intuition is pretty clear. So, on the left, so I have the top is I have three jobs, J1, J2, and J3. And basically, they show when these jobs or customers complete. And the remaining size is on the y-axis, and you can see how these things are executed. So, J1 finished. So J1 finishes first, J2 finishes second, and J3 finishes third, and that's according to their sizes. Now, if I overestimate the sizes, then things in some sense are okay, because what happens is if I overestimate the size, then this only hurts the customer whose size is overestimated. It doesn't hurt the rest of them. In fact, they get to finish a little bit earlier. So you only hurt yourself if you overestate yourself. If you overestimate your size, if you underestimate the size, the story changes. As you can see on the left, if I have job four, if all of a sudden it underestimates its size so severely, you can see that jobs five and six are significantly delayed. So, one of the challenges that people don't have, still don't have a complete understanding is, is how bad can this mechanism This mechanism in terms of underestimation causing severe problems for performance, how bad can this be? So let me give you one idea that has been used. A previous student of mine worked on this, and I won't spend too much time on this because I want to talk about what we're doing now. So, one of the ways to do this is instead of using sRPT, is just to approximate. Using sRPT is just to approximate it by essentially a system with K levels of priority, where each class contains jobs of particular sizes. Right, so that's what I said before. You know, you think if I have, if I did the small, small job, the small jobs or the small customers and the large ones, then you can think that that becomes much less sensitive to size estimates. So maybe if we just look at the bottom bullet point. Look at the bottom bullet point. Then, errors tend to keep a job within a class. Most of the jobs, you know, if I have an estimation error, it's just not going to cross the boundary. Or worse, it's going to change classes for those jobs close to the threshold. So the idea is that we tend not to have this happen as much, but that mechanism still happens. And as I said, the evaluation of these models is somewhat tricky. So there's some nice problems in there. There's some nice problems in there in terms of looking at these kinds of policies. But this is all this kind of thing about approximating sRPT by a priority Q with K classes. It's kind of like we're using sRPT for these estimated sizes. And now, you know, we don't know how well that works. And now we're approximating it by priority Q that we don't know how well it approximates SRPT. We have some ideas. We have some ideas. So, this is all starts to become a little bit ad hoc. So, one of the things we're looking at now is when I have these systems with estimated service times, what does the optimal policy for these things actually look like? So, in optimal in this case, I'm just going to concentrate in this talk on just minimizing the On just minimizing the mean waiting time or the mean response time in the system. So here's the model that I'm going to discuss for the rest of the talk. So I have an MG1Q. Service times are some random variable S. The estimated service time is X multiplied by S, where X is an error, not an error distribution, sorry, it's a random variable that A random variable that represents the multiplicative error. So we have multiplicative errors. And now I'm going to say that suppose that we know the distribution of x, so we have some idea of what our error distribution is, which is may or may not be problematic depending on the application. And when a customer arrives, we are given its estimated size, and I'm going to call that lowercase s hat. So one thing to note here is I'm going to assume that we. To note here is I'm going to assume that we know the distribution of x and that we have something that generates these estimated sizes s hat. In particular, I don't need to know the, in what follows, and it'll be clear why in a minute, is I don't need to know what the service time distribution is. All I need to know are the, well, not all, I mean, knowing the error distribution is a big deal, but I need to know the error distribution x, and I assume that I have these estimated. And I assume that I have these estimated sizes x hat. So, what we want to do here is we want to, as far as we can push it, determine the optimal policy if the goal is to minimize the mean response time. And the reason we want to do this is not necessarily that we want to implement an optimal policy because you'll see that these can become quite complicated, but we want to give some insight into how to develop policies that are. Develop policies that are near optimal, and one way to do that is to look at the optimal policy and see whether we can note anything in its structure. So I thought I'd go through this quickly because I'm not sure that people know this very well. So hopefully this gives you a little thing. So when I'm scheduling queues, there's a very scheduling MG1 queues, then there's a really nice approach. Then there's a really nice approach that's due to Gittens. So, those of you who are familiar with Gitton's work, you know, Gittens is famous for work on multi-armed bandits. And so, there is, in Gitton's work, there's basically the equivalence between MG1 scheduling and a multi-armed bandit problem. So, the idea is: I have a bunch of customers waiting in the queue. Each of those customers refers to the arm of a bandit, and I use these results to And I use these results to find optimal schedules. And so, people, if there are people who are interested in this, I find that when I've had students look at this work, that a better starting place for it is the work of Simuli Alto, Erzi Aesta, and Rhonda Reiter that has a summary of the Gittens approach and shows how you can use this Gittens index approach to get some structural results for schedule. Okay, so what does this approach look like? Okay, so what does this approach look like? So if S is a generic service time, and EI is the elapsed time in service of customer I, and delta is, capital delta is this assigned time to service. So I'm going to serve a customer for time delta. Then they define this thing called an efficiency function. So you know, it kind of has this fairly complicated expression. I'm not going Expression. I'm not going to break it down too much, but the thing to note here is this is less complicated than it looks like, is S minus EI is essentially the remaining service time, because EI is your lapsed time, and S is the total service time. So S minus EI is the remaining time. And that conditioning, S greater than EI, is just conditioning on the. On the customer still being in service or waiting for service. So that efficiency function is key. And what happens in this Gittons approach is, so for our setting, the interesting thing is, and I'll just maybe go back here. So the service time is this X times S. That's the estimated service time. So Time. So S itself is simply: I can plug in the estimate of the service time divided by X to be, that's essentially what the service time is of customer I. So I just plug that in through it. So the only thing I've done from the previous slide is just adapted this to our setting. And this is exactly how you can see that we don't need to know in order to figure out what the optimal schedule is, we don't need to know the distribution. Is we don't need to know the distribution of the service time itself, we just need to know the distribution of the error. And so this Gittens index is just taking the largest value for customer I. The Gittens is just taking whatever value of assigned service time maximizes that efficiency function. And then we get this index and we get Index, and we get an optimal amount of time to actually service the customer. And so the optimal policy in this case is at a decision epoch. So whenever I'm deciding what to do, select the customer with the highest Gittens index. The next decision, so I know which customer to serve. Now I just need to decide when I make the next decision. And it's very straightforward. The next decision I put is when one of the following three things happens. Is when one of the following three things happen. Either I apply the optimal units of service given by the Gittens index, or the previously selected customer departs from the system, or a new customer arrives. Then I recalculate. And so the issue with this policy, it's great. I mean, it's an optimal policy. But if I look back at this original thing, calculating these indices can be very, very difficult if I have arbitrary distributions. Distributions. But in our case, things maybe aren't as bad as we think. So I'm just going to talk a little bit and then conclude about an example. So suppose that the error distribution is such that the multiplicative error is uniformly distributed between A and B. And just for the sake of argument, let's assume that A is less than 1 and B is greater than 1. Then in this case, Then, in this case, no matter what the service time distribution, the Gittens index just consists of computing two values. So, the top one, 2 over S i hat minus A times EI, and the second one. You can see that the first one is until the elapsed time reaches this threshold of SI hat over B. I calculate the Gittens index by the top value, and after that, I use the bottom value. And in both cases, the optimal amount of time to services is that length. Services is that length of time at the bottom. So I know the optimal policy in this case. So if I happen to have this situation, I could actually implement the optimal policy. Now, we want to use this more for insight. So what I'm going to, so I will say that if you're interested, we are preparing a paper. And this insight here, this form of the optimal policy. Form of the optimal policy generalizes a fair bit, but just keep this example in mind. So let me go back here. So one observation is if I look at that first value, that 2 over S i hat minus A E I, if I divide top and bottom by A, I get 2 over A, and sorry, I should have written this out, but partially would have, but I didn't. 2 over A. But I didn't. 2 over a, si hat over a minus ei. So essentially, and that might be hard to see at the moment, but believe me, that that essentially corresponds to scheduling according to worst-case execution time, right? The worst, so if I underestimate a job by a customer with this factor A, then the execution time could be as bad. The execution time could be as bad, the service time could be as bad as SI hat over A. So we see that if A is known, if I have a lower bound on my error in this case, and it turns out that uniform distribution isn't important, then using sRPT with sizes S hat over A should work pretty well. And the intuition is using sRPT on the worst case service time. On the worst-case service time, avoids that problematic situation that I described earlier in those pictures. And one of the things we're looking at is: can we say more than pretty well? I mean, we have some simulation results, but we're looking for some analytic results. Now, one issue is that I might not really know much about my error distribution. And so it turns out that if I assume that I know nothing. If I assume that I know nothing about the error distribution, in particular, I don't know a lower bound at all. If I look at, well, maybe it's not so easy to see, but you can maybe believe me, that if I put A is equal to zero, so that means I could arbitrarily underestimate a customer's size, then the optimal policy becomes SPT, shortest processing time. And there's already some suggestions recently in the literature that suggest that this is a reasonable choice. So this is saying if I have. So, this is saying if I have estimators that really could cause me problems, then the optimal thing to do is SPT. So, in other words, don't use that elapsed time in order to promote priority. So, the last thing I'll say is what we're really working on now is we're trying to characterize gaps between these policies, this sRPT with worst case processing times, SPT, and the optimal policy. And we're trying to Optimal policy, and we're trying to see scenarios where we can really quantify what these gaps are between optimal and these maybe policies that are easier to implement. So I'll just summarize. I think that there's a kind of rich set of problems that are now arising from the fact that in these queuing problems, one is often faced with not just knowing more than the distribution. One is One has these often these estimates, and the question is: how can I best leverage this information without getting too greedy and causing myself problems? So I'll stop there. Nice talk. Oh, okay. Thank you very much, Doug. Any question, please? Please ask if you have any question. No question? I may have a comment. Yeah, go ahead. Just a comment, Doug. Yeah, go ahead. Just a comment, Doug. You know, you're talking about, let's take, for example, downloading as an example. Now, the file can be of a deterministic finite size, and the downloading rate varies, could be the one that is stochastic. You can have the reverse also, where the file itself is stochastic in size, but the downloading is deterministic, or both are stochastic. Now, the question really in queuing is: how do we define the distribution and service time? Is it The distribution service time is it the load transfer to the time it takes to download? No, because there are two variables here. I'm just wondering, this is a question I've been asking lately. Have you guys thought about that in terms of your scheduling? Well, I think that in that case where the, for example, the case where you said the size of the file might be random, but I might have deterministic bandwidth. Have deterministic bandwidth, for example. You know, that's kind of a like streaming app, you know, when I have streaming situations. I don't know how long the stream is going to last. And again, you know, there's, I guess the service time itself is the combination of those two things, right? But, but, and, and a lot of, I'm asking that question because there's a lot of papers these days on what they call vacation and working vacation, which means regular service time is that when you go on vacation, is that. Service time is that when you go on vacation, there's a reduced service time. And I'm saying, if a job is kind of a finite size and it kind of gets in between those two kinds of services, what's the true service time? I'm just asking the question, this, this, and that. I just thought it might be of interest to us on the Kieran. Yeah, but I mean, yeah, I think that the true service time, I mean, at least my take on it, is that you need to take into account both of those. Both of those things. But for example, I think in some of these, say, streams. Streams and these kinds of things is, I mean, there are people that work on, again, having predictors for how long these streams might last, right? So you might look at past data and get a distribution for these things, but you might also have a predictor that says, well, given that it's four o'clock in the morning, or given that these kinds of things that predict. So, yeah, I think both things are important. Are important is now having said that this work here is that if I have a predictor, I actually don't need to know my service time distribution. What I need to know, in order to do good scheduling, is what I need is a good handle on what the distribution is of errors in my estimate. Which is related to the service time. I mean, in order to quantify the error, I need to know what the actual service time is. I need to know what the actual service time is. But there's a few moving parts here. Okay. Thanks, sir. Any more questions, please? Okay, I'm going to stop recording down documents now.