A te amo pero rather than means I love you, dog. So I'm catching up on learning some Spanish here. I've been looking at count structures for about 15 years, and this is the first time I'm going to give this talk, but I want to give something. It's very, very simple, and I'm really, can I say, excited about this? This. But this is the first time I'm going to give this talk. And overarching, I hope it's the simplicity of this is what shines through at the end. The goal that we want is to construct a correlated time series, XT, maybe a spatial field, XS, a spatial temporal process, XTS, maybe a graph or something, a network. And here's what the statistician wants out of this. Statistician wants out of this. Okay, they want to have our account setup, you want to have gen-marginal distributions. Now, if you're working with counts, possibly it's Poisson, something like that. You might want an exponential distribution if you're working with continuous. The second thing a statistician wants is as flexible a correlation as possible. Okay, I'm going to show you here in a second that the radar. In a second, that the radar theory for Gaussian time series can be down some problem when you go and you build even a stationary time series with different margin distribution. The third statistician wants is if the margin distribution is important, you want a tractable, analyzable likelihood to do statistical inference with. Okay? The distribution matters, the likelihood method, feel that structure. Method feel that structure. Fourth thing you really want are easy ways to accommodate covariance in the setup. Okay, so I'm going to start this talk off, and I'm going to stay mostly in count time series. And then when we get to the end, I'll tell you about spatial statistics and space-time processes. Now, count time series started in the 1970s with discrete autoregressions and Discrete auto regressions. And then probably the workhorse model of today is called an integer auto regression. And I have written an equation, the first equation here, Xt is P, the circle means thinned X T minus one plus epsilon T. And that is a autoregression, an integer, autoregression of order one. And let me tell you, it was the first attempt in the 80s by McKenzie, and it mimics ARMA behavior. Mimics ARMA behavior or an AR1. And what you're trying to do here is you take everyone who has survived at time, well, everyone present at time t minus one, think branching process, and you take a probability p and you thin it by flipping a coin for everybody. And you add up Bernoulli random variables. All these, the bi's are independent of x t minus one, and these are the survivors from the last generation. Then you let epsilon t be sort of oops. Be sort of oops, another count type distribution, and you add that back in. So, this is a branching process with immigration right there, that equation. And this is called a integer autoregression of order one. There are integer autoregressions of other orders. There are even integer autoregression moving averages. Okay, so what do you get out of this class? Well, you can't. Well, you can't have any marginal distribution. You can have Poisson. That's one of them you can make. Harry Joe's papers tell you what you can make out of this, but it's not everything that you might want. One of the count distributions that you can't make is generalized Poisson, and that is rumored to be the most flexible count structure around. The other thing, likelihood inference, they claim is testy. I'm not sure why. I haven't really delved into that problem. Delved into that problem because it seems to me the integer AR1 is a Markov chain, so you ought to be able to do inference for that. But they say it's testy. Here's a big problem. Not only can you not have any marginal distribution, but you cannot get any covariance structure that you want. Now, here's where I'm going to give you a challenge. Okay, to show you how bad things can get. I want to make I want to make two random variables, x and y. And x is equal in distribution is equal to y. And I want them to both have a Poisson lambda distribution, but I want the correlation of x and y to be minus one. So if you don't feel like listening to this talk, try that out. See if you can do that, and we'll return to it at the end. And we'll return to it at the end. Okay. But you can because it turns out in an integer autoregression of order one, the autocorrelation function at lag h is p to the h. P is a probability. It can't be made negative. Well, you can try, but it doesn't work so well. Okay, so there's a lot. There's probably about 20 different count time series models that I've encountered in the literature. Okay, almost all of them have some. Almost all of them have some sort of drawback. And now I want to give you one that I think we can do a lot of work with. Okay. And the simplicity of this is amazing. Okay. Suppose that you want a strictly stationary, for the moment, stationary, count time series, and you want the marginal cumulative distribution to be F. I'm going to build that by taking our good friend a Gaussian random process. A Gaussian random process. Now, to do this, I'm just going to use a simple copula technique. Okay, we'll take the mean of the Gaussian process to be zero, its variance to be one, and we'll take the correlation or covariance to be same because it's standardized. We'll call it gamma or rho. And here's what I'll do: this is the whole point of this talk. Phi is the standard normal CDF, Z is standard normal. Z is standard normal, so phi of Zt by the probability integral transformation theorem is uniform 0, 1. F inverse, if you take the right definition of F inverse, which needs to be the quantile function, brings it back to something that has a marginal distribution F. Okay? So, first property: you can make any marginal distribution that you want. There's a construct. You want. There's a construct, a very simple construct to get a stationary series with any marginal distribution you want. You want a continuous one, exponential or something, or gamma, you can do that. The second one is coming out in a JASA paper, and this seems really simple, right? The copula doesn't depend on Zt minus one or anything like that. It's just a simple transformation of Zt. The second property. The second property is that this generates the most correlated possible structures. You will not produce anything more negatively correlated. This construct yields everything. Now, it's not hard to prove that. If you hit me over lunch, I can direct you how to do that, but it comes, it's a couple lines and it's amazing. So you don't have to work to see that you can get everything. See that you can get everything that you want in terms of distributions and correlations. So, then the question when we were messing around with this five years ago is like, well, why haven't people looked at this? Well, okay, for a count series, F inverse is a step function. Okay, phi is, if no closed form, so what do time series people want? The covariance function, right? And getting this covariance function. This covariance function is a little difficult. Okay? But a law of my co-authors, Vladis Piparus, is an expert on Hermite expansions. What is a Hermite expansion? It's a great way to analyze things like Gaussian processes and other things. It's an orthogonal basis of polynomials that do a lot of work for Gaussian processes. So here's what we do. It turns out We do. It turns out the third property is the covariance function of the transform series is just some function that I will call L of the covariance function of the background Gaussian process. Okay, the function L is a little bit tough. It is, if you do the Hermite expansion, you can get a power series form of this. And yes, that is a k factorial because the coefficient, the Because the coefficient, the little LK squared, has a k factorial in it. I'm sorry, the LK has a k factorial in the denominator. So when you square it, all the factorials work out. Here, the Hermite expansion is done with F inverse of phi of X. So you have to deal with what you want. You have to deal with F inverse of phi of X. Okay, that's what needs to be done. And for those of you who haven't seen the... You who haven't seen the Hermite polynomials? The first one is the constant one, the second one, well, H0 is one, H1 is X, X squared minus one. Anyone know the third? I don't. It's X cubed minus three X squared or something. Whatever. Okay. I want to credit Victor Dio Livriera as well for some work here on this in around circa 2010. All right. All right, what about those other two properties? The fourth one, likelihood. In general, almost all the count time series models you see in the literature today, the likelihood function is difficult to get. People are using composite likelihoods and estimating equations and not bypassing doing likelihood inference at all. Particle filtering works really, really well with this. Really well with this setup. For instance, if ZT is ARMA, you can run a particle filtering, which just generates a lot of particles and it simulates a likelihood that and you can optimize it. If you do the routines and the numerics in the right way, you get very accurate standard errors. And you can pretty much do, it works rapidly. So you can do inference, compute standard errors, and other things. The fifth property. The fifth property is you don't need stationarity. So, suppose you wanted to do a Poisson regression with correlated errors now. Just go ahead and let F depend on the covariates. Now, here is a lot of problems that I see in the literature when I go to ref papers is that people try to put the covariates in like the latent process, the Z, really, really bad idea. Really, really bad idea. You will get into all sorts of trouble. Okay, try to put the covariates in the marginal distribution. Just let, like, if you want to Poisson Lambda, let lambda depend on time t and do your Poisson regression. Okay, so one of the things is that now we can do a Poisson regression for correlated errors, not only a Poisson regression, but a negative binomial regression. There's an entire book on that by Hobe. You can do Toby. You can do a generalized Poisson regression, which I'm going to show you in a bit. Okay. Property six. If Z of T is M dependent, meaning two elements in the Z series separated by more than M units are independent, then so is X T, meaning if Zt is a moving average of order M, then so is X T. Unfortunately, no such analogy applies to an autoregression. applies to an autoregression. If Zt is an autoregression of even order one, it is not necessarily true that Xt is an autoregression of any order. Okay, the seventh thing I should tell you is that when you make this transformation from the Z series to the X series, you lose a little bit of correlation. Okay, and how much correlation depends on the marginal distribution F. Okay, uh and uh And well, like I said, there's optimality theorems and stuff. Well, and I think I've already told you: if F is continuous, the same doggone techniques work. Not only that, likelihood is even easier. You just use the change of variables for it, right? And it pops right out. It's just a little Jacobian determinant multiplied by the regular old Gaussian likelihood. So that's nice, right? So essentially, here. So essentially here, time series, now we can handle the marginal distribution aspect. I've always told my class when I teach time series that it's linear models for with a second moment, but now we can put a marginal distribution into that. Okay. Let me give you an example. Baseball. I don't know. I was coming to Mexico. Thought it would be good. Baseball's big here. Not bring stuff. Not brain stuff. These are the number of no-hitters pitched in Major League Baseball from 1893 to about 2016. I've got the most recent, I haven't updated it. Sample mean is 2.12, no hitters over a major league baseball season, and the sample variance is about 3.40. That is an ugly looking graph. Sorry. Never have there been more than eight no-hitters in a season. Eight no hitters in a season. And sometimes there have been zero. So this is a low-count time series. What is the best marginal distribution? How much correlation? Oh, and by the way, for you non-baseball people, a no-hitter is not a perfect game, but it's when a pitcher gets 27 outs and no one gets a hit from the other team. It doesn't happen often. Okay, now you might think because that's an extreme event, there shouldn't be correlation in there, but there's actually a good bit. I mean, Actually, a good bit. I mean, I will in a second. Okay. That's going to be a covariant. Okay. So the correlation between this data is not negligible, actually. And you might say, well, okay, the teams from the year to the year, you know, they have almost the same players. They play in the same stadiums and stuff. So it, you know, there could be a little correlation. It, you know, there could be a little correlation there. Okay. What about marginal distributions? If you worked in counts, two that come up again and again are the negative binomial and the generalized Poisson. Now, the rumor is the generalized Poisson is the most flexible structure and it tends to win two to one over the negative binomial. But here are the forms that I'm going to use for the probability mass function in the negative binomial. In the negative by nomial, I have some people write this differently, reversing one minus p and p, but I'm going to take this form for the probability mass function. There is an eta parameter in the generalized Poisson. If eta is zero, it's Poisson, okay? But that's the marginal distribution. If you haven't dealt with generalized Poisson, the mean is lambda over one minus eta, and the variance is lambda divided by one minus eta. Is lambda divided by 1 minus eta cubed. So it's over-dispersed, both of these. Now, covariates that I have. There's been differing number of baseball teams in the league. There's actually been changes to the number of games played, strike shortened seasons, COVID shortened seasons. That didn't come in here. So I have the number of games that they've played in a season. We'll call that covariate one. And also the pitching mount height when they make the pitching. Pitching mount height. When they make the pitching mount higher, the pitcher gets an angle and it's a little bit more advantageous. And I've had that over that covariate over the years. It's changed a few times. So in this data, the generalized Poisson marginal fit better than the negative binomial. So it was true to form and what the literature tells me on counts. The number of games played in a major league baseball season influences no hitters. Season influences no hitters. That covariant was significant, highly so. But the height of the pitching mound wasn't. It was almost, but not quite. I wonder if I put the most recent six, seven years data in there, whether it would work. A simple AR2 background latent ZT adequately models the correlation in ZT, and all of these parameters are significantly on zero. And again, I want to just state. And again, I want to just state that we did a generalized Poisson and a negative binomial regression with coordinated errors and computations like that. And, you know, it's time to go drink beer, I guess. So here's now what I want to say in extensions. This paradigm extends to spatial settings. Okay, what? Just take Z of S to be a standardized Gaussian random field. We'll take it standardized. We'll take it standardized means the mean is identically zero, the variance is identically one. We'll take it stationary. You don't have to if you don't want it, but we'll do the same tactic. You want Poisson in the F? Well, I just made a Poisson. If I'm modeling tornado counts in Kansas in the counties, this would be a pretty doggone good model. And so sometimes evaluating the likelihood in spatial context is harder, but with this, all you got to do is. This, all you got to do is be able to evaluate the Gaussian likelihood, and we'll optimize your likelihood for you. What about space-time? You want space-time and counts? Well, just take ZTS, a standardized stationary, spatiotemporal Gaussian process, mean zero variance one, and covariance now and correlations will be two-dimensional, do the same thing almost all. The same thing, almost all the stuff in time series is going to apply. Now, I was hoping coming here, people were going to be flashing counts all over the place. I'm just seeing Gaussian, Gaussian, maybe I need to change my plane flight. I was hoping to see some more non-Gaussian analyses. I'll stop for questions, but let me point out: Richard Davis and I, Rebecca, that's I, Rebecca, that's Costas Fociano, wrote a JASA review paper on count time series methods. This is where this paper is appearing. It's theory and methods. So they've got like about a three-year backlog in JASA. Believe it or not, my PhD student and I have already done the seasonal count time series. A lot of this work stems from our issues of modeling negative. Of modeling negatively correlated hurricane counts between the Atlantic and the Pacific. If you're interested in that, I'll refer you to an old Annals article, and I will stop and take questions here. Any questions for our speaker? Very nice talk. So in your F function, that does not depend on T. Function, that does not depend on T, right? You can make it depend on T. You just have to do a time-varying Hermite expansion in the thing, and it's not hard. The Hermite expansion goes really quick. But if you depend on T, you will not account for the dependence on F across T. Basically, you assume that independent. No, F will like think of Poisson, and it's think of F sub lambda. Okay, so now F sub lambda T, you just for each value of the covariates, you're going to. For each value of the covariates, you're going to have to do a Hermite expansion. But as you do a gradient step and search, that's not so hard. I see. Okay. Okay. Thank you. Oh, by the way, anybody get this? Why it works? Because you can make a fair draw of X of Z of T given. Of Z of T given the previous history X1 through Xn, you can make a fair draw of Z of N, and that means that that can get you back to the latent level. And particle filtering is wonderful. It's just like. Particle filter is the best way, best approach. Oh, I don't know if it's the best way, but you know, it's a way. I think in 10 years, we'll know the answer to that question. Years will know the answer to that question, but not right now. Okay, okay, okay. Wait a minute, what about this challenge? Who's got it? It's impossible, which shows you, because if you were going to do it with Gaussian series, you just take, say, Z and minus Z, and that has correlation negative one. But you'll have to, I'll take any proof later in this week. You cannot do that with Poisson, and that tells you all of the things. And that tells you all of the things that can go wrong with count stuff. It's a lot harder. Just regular non-negative definite covariance function, you may not be able to produce it. Okay. If I can ask a question, I was so motivated by some applications I've been working with with EMA data cell phone response. So say, for instance, older adults who are recently bereaved, we hit them up on their cell phone. On their cell phone once a day, and they check off the number of positive or negative things that are occurring, or have you been doing certain things? So we have count data, but we also have lots of missing data, and the missing data are informative. Do you see an easy way to incorporate informative missingness within these mechanisms? Depend on the missingness mechanism. If you just are missing a couple data, point. A couple data points, and it's like the process as normal. You instead of predicting Zn plus one from X1 through Xn, you would predict Z next observed value. So no, there would be absolutely no problem with that. If there's some other mechanisms going on where in the missingness, it could be an issue. Thank you. Last question before lunch: no pressure on you. Punch, no pressure on you. It'll be quick. So if you let Lambda increase for this problem, then if Lambda is large, as Lambda increases, then the Poisson gets closer to a Gaussian, and then you can asymptotically achieve this minus one. So does that mean that when Lambda is small, then are there any limits on how much coordinated correlation you can have? If Lambda is really small, you're going to be either zero or one. And how do you get highly negatively correlated zeros and ones? You're in trouble. It doesn't exist. Trouble doesn't exist. But you can have a sequence which is one, zero, one, zero, one, zero alternates, then that's, yeah, but you've got to keep the Poisson marginal. Try to prove it. No. No, I'm asking whether there's a limit. Maybe there's some negative correlation. As Lambda goes to infinity, you can do this. But if not, the most negatively correlated you can get as a function of lambda goes to zero as lambda goes down to zero. Got it. Got it. That's where I want to pick one. Rank correlation? I'm not sure I know what rank correlation is, but it sounds rank. Oh, I don't know. I'd have to think about it. Okay. Well, people say that, but it tends to work just fine. And we haven't moved away from the count time series people. Away from the count time series, people are still using it, just and it we have still haven't moved away from it. So, I don't know. You may be right. Great. Well, thank you, Robert, and thank our other two speakers.