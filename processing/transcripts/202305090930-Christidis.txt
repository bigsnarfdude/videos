For my thesis at UBC. So, just a couple of acknowledgments and disclosures before I start. So, in addition to being lucky to work with Gabby, for my thesis, I worked with Dr. Ruben Zamar, St√©phane Danaz from TO Leuven, and also I was unofficially advised by Doug Martin from Washington, the Department of Applied Math. And then, also for this talk, a couple of disclosures. Talk, a couple of disclosures. This talk is a lot more statistical than computational. I'm very new to the field of computational biology, so don't expect to get your socks blown off in terms of the application. But I still have one at the end and hopefully it just demonstrates the application for the method I'm developing with Gabby. Okay. So if we look at omics data from a statistical point of view, it's a very specific type of data, whether it's genomics, metabolites, or any type of Metabolites or any type of omics data is what we refer to as high-dimensional data. So, if we define the number of predictors to be P and the number of samples to be N, well, we typically have P much larger than N. For example, P could be in the thousands, even in the millions nowadays, and N is usually a limited sample. Another problems with high-dimensional data is we have groups of highly correlated predictors. For example, we could have genes that belong to the same pathway or biological process. And then we also have another problem which often Have another problem which often comes with omics data, which is data contamination. So, you could have, for example, technical problems in sample preparation, or you can also have molecular profiles. Okay, so we have in the literature a couple of methods that address these problems. So, first we have sparse methods. So, sparse methods address the high-dimensionality and multicollinearity problem, which is the groups of highly correlated predictors. The sparse methods generate a single The sparse methods generate a single predictive model. They're usually interpretable because only a subset of the predictors are selected in the final model. The level of sparsity is usually data-driven. So things like AIC have been used in the past, but nowadays cross-validation is typically the way it's done. And there's a lot of well-established theory on the estimators that you get, including asymptotics or non-asymptotic. And then we also have robust methods, which address the data contamination problem. And with robust methods, you can identify outliers, whether it's for locations. Identify outliers, whether it's for location, scale, or regression, or even correlation, which I forgot to add in my slides. And there's also some well-established theory, which was done more in the 60s, 70s, and 80s. And more recently, there's a more new area of research, which is the combination of sparse and robust methods. So they combine the appeal of sparse methods, which is the interpretability, with the robustness methods. Another type of methods, which is what I've worked on. Which is what I've worked on from my thesis, which is ensemble methods. So at the bottom, I mentioned that ensemble methods usually have superior prediction accuracy to sparse methods, but ensemble methods also come with a lot of drawbacks. And for those who don't know ensemble methods, I'm sure everybody knows what it is here. It's basically multi-model prediction, and then you pull them together to get a final prediction. So it's in essence really black box modeling. They're usually non-interpretable. The way you usually get ensemble methods, it's usually. The way you usually get ensemble methods, it's usually based on some sort of heuristics. And the level of diversity between the models is not data-driven either. And there is actually no proposals to incorporate robustness in enslavement methods at all in the literature. And on top of that, for Ensom methods, there's usually no theory because it's most heuristic. And I'll mention some examples later. Okay, so the overview of the talk is: I'll first do a brief review of sparse robust and enslavement methods very quickly. Methods very quickly. Then I'll introduce our methods, robust multi-model subset selection, which is sort of a combination of these three types of methods. I'll talk about some basic theoretical properties, although not too much. I'll go over some computational methods to generate these ensembles. I'll show a small genomics application. And finally, future work, which is robust dynamic predictions, which is an extension, a natural extension of using these types of ensemble models. Okay, so for this. Okay, so for this talk, I'll focus on the linear model, but everything presented in this talk extended to other types of models very naturally. And in fact, for my thesis, I also did it for classification. So I won't bore you too much with the details here, but this is basically just a new model with the classical assumptions. And then just to talk about sparse methods briefly. So I think most of you are familiar with best subset selection. So we try to minimize the squared loss under the constraint that the coefficient. Under the constraint that the coefficient vector has at most t predictors. So we penalize the or we restrict the L0 norm of the coefficients. Then we have regularization methods, which are essentially a relaxation of the best subset selection problem. So we just add a penalty, usually a convex penalty to the problem, although there are non-convex penalties as well. There have been two regularization steps methods, like the adaptive LASSO, the relaxed LASSO. And then more recently, starting in 2016, More recently, starting in 2016, but really in the last couple of years, there have been a lot of papers that appeared not only in statistics but in operations research and optimization. Papers that revisit the best subset selection problems because it usually has better prediction accuracy and variable selection recovery. So a lot of papers lately have focused on just going back directly to the non-relaxation problem. Okay, robust methods. So there's really two main approaches. So, there's really two main approaches to do robustness. So, the first one is to use bounded loss functions. So, this is just an example of the 2ke by square loss. So, we have m regression, s regression, and mm regression. And they're somewhat related methods. And the s regression estimator improved on the m regression estimator, and mm improved on s in terms of theoretical properties. But essentially, they minimize a bounded loss function. So, the outliers would not have. So, the outliers would not have such a huge effect on the loss. Another method is least trim squares, which kind of popped up around the same time as regression in 1984. And essentially, for least stream squares, we minimize the squared loss, but we only use a subset of the samples. And we can restrict the size of how many samples we use. So, for example, here in this case, we would use at least h samples, where h would be. H samples where H would be less or equal than N to minimize the loss, and the goal is to obviously try to exclude out of the minimization problem the contaminated samples. Okay. All right. Okay, now we have robust and sparse methods. So there were a few papers that appeared in 2007, actually from my advisor, which is a robustification of stepwise and the LARS algorithm, least angle regression. They're really just really algorithms to try to sequentially add. Algorithms to try to sequentially add predictors to a model. And then in 2013, sparse LTS, which is essentially the least trim squares loss, but now we add a regularization penalty to it. And then there was more recently MM Lasseau, and even more recently, Pensei, which is the method developed by Gabby and her PhD student, which is a combination of the MM regression laws with an elastic net penalty. And they did a lot of work on the initial estimator, which is a very challenging problem. Which is a very challenging problem in robustness because these are non-convex optimization problems, so they make the optimization very complicated and computationally intensive. And then more recently, as I mentioned, the best subset selection has been revisited because it has very nice properties, and that includes in the robustness case. So in 2022, there was a paper that appeared that combined the least trim squares loss with best subset selection. So we have a restriction on the L0 norm of the coefficient. Restriction on the L0 norm of the coefficient and also a restriction on how many samples we use for the loss. Okay. Okay. And then for ensemble methods. So the basis for ensemble methods is we want to have decorrelated models because if we decompose the mean square prediction error of an ensemble method, the bias turns out to be the average of the biases of the models in the ensemble. And the variance could be decomposed. And the variance could be decomposed as this interesting form, which is 1 over g times the average variance of the models in the ensemble, plus g minus 1 over g, the average pairwise covariance of the models in the ensemble. So we can see that as we increase the number of models in the ensemble, how decorrelated they are becomes way more important than how accurate they are individually. Okay. So, for example, if we have 10 models, then 90% of the variance of the ensemble comes from the pair. Of the ensemble comes from the pairwise covariances between the models. Okay, if you have any questions at any time, please let me know here. Okay, so ensemble methods, the way they're generated now, there's really two ways to do it, which was randomization. So randomization can be done in two ways, random sampling of the samples or random sampling of the predictors. So for example, in random forests, we create bags. So we do bagging to get different samples of the samples. Different samples of the different bags of samples. And then, after that, when we construct the trees, at each node, we randomly select certain predictors randomly, and then we use the best one. So there's both a random selection of the samples and random selection of the predictors to construct these tree ensembles. And then another way to do it is boosting. So adaptive resampling is sort of similar to bagging, but the way we select the second bag depends on how well. Depends on how well the samples were selected in the first one. And then that's how we construct the sequential bags. And then the way boosting is more done nowadays, it's more gradient boosting, which is essentially the sequential refitting of residuals. And papers like extreme gradient boosting have gained a lot of popularity in recent years. The problem with these ensembles is they're essentially ensembles of weak learners. So the models in these ensembles are only useful if there's a large Are only useful if there's a large number of them because they're inaccurate and they're only useful if we pull them together because recall that by the this formula if the models are inaccurate then we just want to make sure that they're at least decorrelated okay and then these are just a couple of papers that appeared recently in the proceedings of the national academy of sciences or nature that basically make the case which i think everybody here would Which I think everybody here would adhere to, which is that we shouldn't use black box algorithms. We should use models that are, yes, accurate, but also somewhat interpretable. So, how do we make ensemble models interpretable? So, for my dissertation, I developed a multi-model subset selection, the non-robust version. So, if you have p predictors and g models, we have a matrix of coefficients instead of a vector of coefficients. So, each column is basically the beta coefficient of a specific model. So, we have Of a specific model. So we have a p by g matrix, and the rows would basically be the values for the one particular predictor in each of the models. Hopefully that's not too confusing with the notation here. And then the goal in multi-model subset selection is to get accurate models, not just an ensemble, but accurate models. So we minimize the sum of the squared losses of the models under some restrictions. So the first restrictions is the sparsity constraint. Sparsity constraint. So, this sparsity constraint restricts the L0 norm of the columns, which is how many models appear in a single model. And then, this is the L0 norm restriction on the rows of this matrix, which is how many times can a predictor can be shared. Okay, so hopefully, that's clear. All right. So this is kind of a scary looking formula. You don't need to read the whole details, but basically, this is to count how many splits there are for this particular problem, right? So, for example, Particular problem, right? So, for example, in best subset selection, if we have 15 predictors and we want to, sorry, we have we want we have 15 predictors and we want to select 10 of them, there are about 31,000 possible subsets. If we just make it a little more complicated and say, okay, split 15 predictors into three models, and each model has at most 10 predictors, then it jumps to 171 million. So there's a lot of different possible splits of the predictors compared to subsets. Of the predictors compared to subsets, and best subset selection is already an NP-hard problem, so this is an even harder problem. And by the way, this formula is only if u is equal to one in here, which means that a predictor can only appear in a single model. So if we allow for predictors to be shared between the models, then this number goes into the trillions. And this is only for 15 predictors. So you can imagine that when you have high-dimensional data, these problems look a little scary. So one approach we use. So, one approach we use in my dissertation is a multi-convex relaxation, which is sort of a lasso-type problem, but now it's more of a like a multi-model lasso-type problem. And the advantage of a multi-convex relaxation problem is although this is still non-convex, if we just look at one model at a time, then this problem is convex. So, we came up with a lot of algorithms to optimize the subjective function very effectively. We developed a lot of theory, asymptotic. We developed a lot of theory, asymptotics for the multi-convex relaxation. And then in another chapter of my dissertation, we actually developed algorithms to actually directly optimize this problem. And it turns out that the direct optimization of this problem, unsurprisingly, is actually better than the multi-convex relaxation. Very much like best upset selection, if you can do it, is usually better than doing a relaxation like the last two or the elastic nets. Say again, sorry? Oh yeah, yeah, yeah, absolutely. Yeah, so you can only, yeah, you can only range, actually you can only take values one, two, three, all the way to G because it's how many, u is basically the parameter that determines how many times a predictor can be used. So, yeah. Be used. So, yeah. Is that clear? Yeah. Yeah, so your question is, how does this predict each of the models take a different predictor and still be very correlated? Well, the way these models are constructed is usually highly correlated predictors would end up in different models because the benefit of adding two correlated Models because the benefit of adding two collated predictors in the same model is not supposed to constrain model correlation. It does, yeah. But if I have a bunch of models and they all have distinct predictors, they're all correlated. I see what you mean. Yeah, it can happen. But yeah, that's a problem. But that's a problem that would happen if you only have just one model as well. So at least if you have multiple models, you can somewhat decorrelate it. But if you have multiple Somewhat decorrelated, but if you have multiple predictors that are highly correlated, even if you have just one model, that would be a problem. So, yeah. Okay, so how do we make robust multi-model subset selection robust? So, this is what I'm doing in my postdoc work with Gabby. So, now these ensembles are modeled without heuristics. They're interpreted. Heuristics, they're interpretable and accurate, they're resistant to outliers. The level of sparsity, diversity, and robustness are data-driven because we can actually select all these parameters by cross-validation. And we do a bit of theory and computation on these robust ensembles. And everything is implemented in C ‚Åá. I wrote a software library for it because these are algorithms to generate these ensembles are pretty computationally expensive. So, let me go into a little more details. So, now to make these. Details. So, now to make these ensembles more robust, we basically just replace the square loss by the least trim squares loss. And now the parameters t, u, and h can be selected by cross-validation. So again, t would be the size of the models. U would be how many times a predictor can be shared. So it ranges from one to capital G. Capital G is the number of models. And then H is the size of the Size of the sample sets we use to fit for each model. One thing you can notice here is that the sets of samples used to fit each model don't have to be the same. So the sets IG can be different. So you're going to trim different samples for different models. Okay, so for example, if a predictor is contaminated with a specific sample, then if a model doesn't use that predictor, then that's That predictor, then that sample would not be contaminated for that model. So, and this is related to something called sell-wise outliers, but I won't go too much into that. So, just one quick little neat result here. So, if u is equal to g, so recall that u is how many times a predictor can be used by the different models in the ensemble. If u is equal to g, then there's no restrictions on the sharing of the predictors. And it turns out then you can show pretty easily that each model would essentially be robust. Would essentially be robust best subset selection. So they would be g-identical models. Okay. And then also, if you go one step further and then you set T is equal to P, then you have least term scores loss. And then if you have one step further, if H is equal to N, then it's just best subset selection. So in essence, robust multi-model subset selection is a generalization of robust best subset selection, least stream squares, and best subset selections. These are special cases of robust multi-model subset selection. Okay. Okay. All right. Just a bit of theory, not too much. So, this is basically the definition of the finite sample breakdown points. And in a nutshell, it's basically up to what proportion of the samples can be contaminated without the estimator breaking down or becoming insignificant. And it turns out that both the individual models in the ensemble and the ensemble model itself retain the finite sample breakdown point of least term squares. Breakdown point of least two squares. So they have so basically each model can have up to n minus h contaminated samples and not break down. Now, if you have n minus what h plus one, then you break down. So that's for the individual models and for the ensemble model. All right, so how do we actually generate these robust ensembles? So we came up with a new projected subset log gradient descent algorithm. Log gradient descent algorithm and then we do local searches. And all of this has been implemented in the C library. It's still under construction, so if you download it, there might be some issues, but I'm still fixing them. I use an object-oriented design. Multithreading is available if you want to run it on Linux, for example. So it'll run pretty quickly. I try to make it as memory efficient as possible, although keep in mind that we're optimizing a lot of models at the same time and doing cross-validation. And the CRAN package will be coming soon. CRAN package will be coming soon. I'm hoping I can finish it by the end of this month. Yeah. Okay, a couple of more details for the computation. So we can recast robust multi-model system selection using auxiliary variables. So instead of using a sum over the samples, we're going to sum from i is equal to one to n. But now we have the auxiliary variables eta, which trim some of these samples. Okay, so you might have seen this trick used before. It's very common. trick used before it's very common for least trim squares and essentially the algorithm uh just the overview because i could spend an hour to talk about the actual algorithm to fit these uh these ensembles but basically we sequentially update the beta coefficient vector and the eta vector of one model at a time using this projected subset alternating block grid in the SIN algorithms the mouthful and then this gives you an initial grid of solutions and after you do that we use a neighborhood search so We use a neighborhood search. So, recall that we're going to generate these ensembles for different configurations of the tuning parameters. So, we have H, T, and U, the robustness parameter, the sparsity parameter, and the diversity parameter. So, if we have an initial grid of solutions, which is represented by this notation here, so this is for IHI, TJ, and UK. And now, what we're going to do is find all the neighbors for that particular solution. Neighbors for that particular solution. And basically, it's if we just change one of the parameters by one in the sequence, that would be considered a neighbor. So this fancy notation here, just basically say, for example, if you just move to the left or to the right in any of the tuning parameters, that's considered a neighbor. Okay. And then for each of these neighbors, run the projected subset alternating block grid and the SEN algorithm. And if the solution is improved, And if the solution is improved with this initial solution, then that becomes the new solution for that new configuration of IJK. Okay. And then we repeat this process for all IJK in the grid. So hopefully I didn't lose anyone here. Okay, just a quick application. So the application is using sort of an outdated data set. It's a data that was collected using microarray technology. So it's not really used anymore, but it's a data set that was used a lot in the Metallic. That was used a lot in the mid to late 2000s on sparse and robust methods. And the goal basically is to predict TRIM32, which has been identified as a gene highly correlated with Barde-Bidel syndrome. And one question might be, why are you trying to predict TRIM32 if you can actually measure it? Well, the goal in this paper was to see what causes this gene to be high to try to understand the disease better. If you want to have a better explanation, you probably know more. Have a better explanation, you probably know more about it than I do. And then if we use sort of a neat, very neat new method called detect deviating cells that was developed by Roussaint Busch in Belgium, it's a method that detects outliers in cell-wise outliers, which is a new type of outliers in the literature. The method identified outliers in this particular data set. And then, so we have 120 samples and 200 genes. And 200 genes. And to make the problem very high-dimensional, we'll use 30 as training and then 90 as tests. So we have 30 samples, 200 genes. Okay. And then these are the methods we'll compare just to run a quick little experiment. Actually, sparse LTS is not in the simulation. I apologize for that. I should remove it. But there's basically robust best subset selection, which is better anyway. And then robust multi-model subset selection with G is equal to five models. Selection with G is equal to five models, and then this one is actually not in there, too. I apologize for that. We'll run random force, which is the ensemble algorithm we'll use for comparison. Okay, and then we'll run it with g is equal to five models and also 500 models, which is the default number in R. And the reason why the default is so high in R is because the trees are inaccurate, so you need a lot of them to get a good on-sound prediction accuracy. And hopefully, they're decorated. Okay, we'll compare the MSP of the methods, and then this. The MSP of the methods, and then this is just a measure of the accuracy of the individual models in the ensemble. Okay, so just to see if the method actually generated ensembles that have accurate individual models. So this is the output. So as you can see, robust multi-model set selection is this one here. So the ensemble achieved the lowest MSP. And then, interestingly, if you notice, the accuracy of the individual models are almost as accurate, or in fact, As accurate, or in fact, pretty much as accurate as robust best subset selection. So the individual models are just as accurate as the single model sparse and robust estimator. Okay, and then you can see random forest didn't work particularly well for this method, but you can see that going from five to 500 models actually improved the performance a bit because what benefits Random Forest is a lot of highly decorrelated models. All right. All right. So the method is not only good for prediction. You can actually use robust multimodal subset selection to rank the genes by order of importance. So this notation basically, S sub G, S, sorry, S G is basically which predictors are in each model. And these are basically decreasing sets. So for example, A1 contains the model, the genes that appear in just one model. A2 contains the genes that appear in two models, all the way to AG. All the way to AG. So, if a printer is in AG, it means it's being used in every single model. So, these are sort of sets you can use to rank the genes. Okay, and then this is just an extension of this work, which is sort of the second step after we finish this first paper, hopefully, which is once you have these ensembles and you get a new test sample. In the literature, there's a lot about robust training, but there's nothing really about robust prediction. So, if you train a model robustly, it's kind of useless. Model robustly, it's kind of useless if your test sample is contaminated because it's going to give you a bad prediction anyway. So, something we can we're going to try to do with these robust ensembles now is when we get a new test sample, we're going to determine based on the actual values of this new test sample in which model is going to be used. So we can, for example, detect in this new test sample, are there contaminated predictors compared to the training data? If yes, we're not going to use the models that include these predictors. Include these predictors. Okay. And then, just as an experiment, what I'm going to do here is compute the prediction accuracy by cheating and doing what if every time I get a new test sample, I use the best possible model just to see if we get a significant improvement in prediction accuracy. And then we can see that. So dynamic one would be out of the five available models in robust multi-model set selection, I always use the best one. Selection, I always use the best one. And as you can see, the prediction error is almost close to zero. So it was very accurate. And then if I use the second best model, it's also a large improvement compared to using the ensemble. And even using the third out of the five best models is still competitive with using the ensemble. So if we can develop a method that can identify the good possible models, for example, the top three, which is better than identifying the best model, then we can. In fact, the best model, then we can get a good improvement in prediction accuracy by just doing that. And that addresses the problem of not only robust training, but also robust prediction. I hope I explained this properly. Anyways, so this is the end of the talk. If you have any questions, go ahead. Thank you. There was a lot there. But if you go back, maybe just two slides to the more to the importance thing, I guess I'm just trying to understand a lot more things with the AA. One more slide, I think. Oh, yeah. Things like random force are pretty explicit about what goes in a model and what doesn't, right? So it's kind of stochastic. So it's highly unlikely if you did random force that you'd find any variable that could ever be used in all of them. And other models seem to be more associated with, let's start with everything and try some, right? And so those ones you could get there. So it just seems like to me that this is slightly problematic because. Problematic because some mechanisms or model selection criteria use ensure that nothing is in AG, and others allow lots of things to be there. So just wondering your thoughts on, am I correct in my belief that that's what's going to happen? And then how do you think about that? If I understand your question, it would be that some of these sets would have no predictors, whereas if you use other methods, you can always get sets that actually use some of. You can always get sets that actually use some of the predictors. It's somewhat, yeah. Yeah, I'm trying to compare. Yeah. Well, it can't be that. I think the difference is how these sets, these genes or these predictors would be selected, it's still based on randomness. So I would say it's less meaningful than using something that actually optimizes the selection of the predictors. Not so much the sets themselves, but how is a predictor actually included in a set? Include it in a set. So, random forest is based on randomization, whereas this one is not. So, I would say that's the difference, if I understand your question correctly. Thank you. Yeah. Yeah. Hold on, Shaitan. Thank you. Yeah, I really like this kind of work. So, really nice talk. Quick question. So, everything, the loss part is all kind of a linear regression type loss plus the penalties, right? So, this might be a naive question because I don't do that much on the robust stuff. I do more on the other side. But one of the things I've kind of used in some. One of the things I've kind of used in some context is the median of means estimator, which I don't think is the same as MM, right? People call it like something I see. So, does that apply here, given that sort of in the linear case, the maximum likelihood is similar to mean estimation? It's very flexible. Basically, a squared loss, you can replace it with any type of model you want, as long as you have parameters. Sure. You can even do it in a clustering setup when you find different lower-dimensional representations of the data. Yeah, it seems like you can just do it with the clustering. Yeah, and the other thing that I was like, when you have something. Yeah, and the other thing though was like when you have something like the projected subsets stuff, that I mean, in theory, you can apply that anywhere. But in my experience, when I like replace the linear loss with something a little more involved with like more second order information, the projections can lose a lot of information. I don't know if you've played with that or just what is your experience with that? I think that's why using the neighborhood search is so important because as you say, these things are highly non-convex and in this case, non-differentiable. And my experience is don't rely too much on whatever this. Is don't rely too much on whatever this gradient descent algorithm gives you. Try to get new initial solutions and rerun the algorithm and try to improve on it. Because yeah, you never know. Thank you. Yeah, just to add quickly to that, Jason, that's why we are using LTS. Like there are many other ways of getting robust measures. One is to change the objective function, as you said, with the median. Or in my paper, the previous one, Pen Say, we used the S laws. Yeah. Yeah, but those give even more complex objective functions. This one is still the quadratic loss, but you just play with a subset of data. So yeah, so that helps with the robustness, but still it's a quadratic loss. Any other question here? Any other question in Zoom? No questions in Zoom. In general, you said that your method, the ensemble, is not transparent, it's a black box. So, can you interrogate or design ways to interrogate, for example, in this example that you say, why the best predictors of 30, whatever, who are the biological features that are doing that prediction in your model? Can you investigate inside or not? Investigate inside or not? You can, and that's probably the next step of this research: is try to see more the application. Do these different because each model is more accurate than if you used a black box. So they give you better prediction accuracy. And then the question is, does it also give you good variable selection? In my thesis, we did it for the non-robust case, and it did have very good model selection properties on par with the sparse methods. But for the robustness, I didn't really investigate this yet. But yeah, it would be interesting because obviously for all the Obviously, for all these articles that mention it's not just prediction accuracy, you also want models that are interpretable. It'd be nice to have maybe some theory or some investigation as to whether they actually have good variable selection. Yeah. And I think it goes back to Robert's question also about what are being selected in those sub-models. So we can investigate that for sure. Yeah. And this whole work was really to avoid using randomization or boosting, which is then in that case, it's not really meaningful to look at the models themselves because. Meaningful to look at the models themselves because it's the way they're constructed is you're not, if in boosting, you're not even fitting the whole data, you're fitting the residuals. And then for randomization, you randomly selected samples and predictors. Yeah. Thank you very much. Okay.