With a bunch of different people, but I want to especially highlight Bohan, who I worked very closely with on this. So Bayesian modeling, as we all know and are here today, offers a powerful toolbox for analyzing scientific data. So this talk is not about how great and powerful and useful that toolbox has been. And useful that toolbox has been, but rather about some of the dangers that come along with its success. So, this talk is going to be about robust phase, about what happens when you're doing Bayesian inference and your model might be wrong. And there are a lot of reasons to do robust phase, but I want to motivate this particular one particular reason which is relevant to today's talk, which is that. Talk, which is that as we've gotten better at building complex Bayesian models, applying them to large data sets, and making them easy to use, packaging them into nice software and reproducible workflows and all of that. They're getting used in all sorts of places, and people can just hit go on a Bayesian analysis. And there might not be actually a very good match between their model and the Good match between their model and the data that they're applying to it too. And so, as we become more successful at building Bayesian tools for important applications, and as we get more successful at making them really easy for the non-experts to use, this problem only grows. There's more and more probability that our models, however sophisticated they were and however good they performed on the test data sets we were examining, won't actually be appropriate for the specific data. For the specific data set that the users are applying it to. And so today's talk is about how we can ensure our inferences are robust and trustworthy, even when our model might be wrong. So I'll just fix my own notation first, and I'll say that we're going to be talking today about parametric Bayesian modeling. So what I have in mind here is parametric Bayesian modeling. Mind here is parametric Bayesian modeling, typically with strong scientific knowledge and assumptions baked into those models. And my concern, the type of robustness that I'll be concerned with is likelihood misspecification. And I want to start just by reminding you that your choice of likelihood encodes a strong prior belief. It says that there's some your the probability mass you're putting in the space of distributions. In the space of distributions, all of that mass is on the set determined by the likelihood. In other words, you're saying there's a probability of one in some sense that the true data distribution falls in this model class. None of us, we're all grown-ups here, and so we don't actually believe that even when we use parametric models. But we say, well, parametric models are still really useful, they're fast. They're fast, they're efficient, they're easy to understand, and they're often a pretty reasonable approximation of the truth. And so, why should I care? And so the starting point for this project is the question, well, you know, okay, this is something we all believe. We're all mature, we all can say, you know, okay, yeah, you know, when you're using a parametric likelihood, you don't actually believe that that parametric likelihood is correct, correct. Correct, correct. You know, it's just probably roughly true, good enough for the problem. And so the question we start about is, can start with, is can we encode this opinion mathematically instead of just having it internally? So we're going to start by asking, sorry, we're going to start by asking this question of like, how do we encode this subjective belief? And the intuition you should have, of course, is that, you know, we don't believe that the data actually Know, we don't believe that the data actually comes from the likelihood. We believe it comes from something close to the likelihood, you know, some delta away. Okay, so here's the roadmap for today's talk. I'm going to start by talking about non-parametrically perturbed parametric models, which is going to start to formalize this idea of how do we think about parametric models in a setting where we're not 100% sure the likelihood is. 100% sure the likelihood is actually true. Next, I'll talk about how you use these non-parametrically perturbed models, which I'll call NPP models, to do inference. This will all be fairly abstract. NPP models will be brutal to work with in practice. And so what we'll next do is start to understand. Well, actually, I should say, first, we're going to start to understand theoretically. We're going to start to understand theoretically how NPP models behave, what's the type of asymptotic behavior that they have, why it might be especially desirable, for instance, in comparison to conventional or standard non-parametric methods, as well as in comparison to parametric methods. And then finally, I'll make a very aggressive approximation. You might not even call it an approximation to these ends. To these NPP models using generalized base. And this will transform the problem from one that is a very challenging posterior to work with to something that can be almost done using sort of standard packages and without much modification to how you do inference in the parametric model. And my ultimate goal, what I will try to get to develop by the end of this talk, is. Developed by the end of this talk is a convenient method for packaging a parametric Bayesian model so that it can be safely applied to new data without expert model criticism and modification. And by packaging here, I don't mean packaging in, well, I sort of mean, I don't mean packaging in the software sense, but packaging in the statistical sense. How can we sort of add a statistical wrapper to our Bayesian models that will make it safe to use? To use. Okay, so what is an NPP model? So I gave you this notation for a parametric model in the IID setting. A non-parametrically perturbed model works like this. And this is abstract. This is hand-wavy. I'll make it a little bit more specific. But it communicates hopefully the basic idea. So in a parametric model, we sample a parameter theta that determines a likelihood. Determines a likelihood, and then we sample data from that likelihood. Oh, I didn't suppose to use this, right? In a non-parametrically perturbed model, we sample that likelihood, and then we sample a perturbation. So we sample, you know, you can think of this as a delta. This p does not have to actually equal p theta. This h is a general scale parameter. And so it just determines how much of a perturbation we allow. And then the data. And then the data is generated according to this perturbed version of likelihood. This perturbation model, which I'll get into, this pi sub pert, what you need to know about it is simply that it's going to be non-parametric. So it's going to have support covering all data distributions in some sense, or at least close enough for practical purposes. Close enough for practical purposes. And second, this scale parameter, the idea is that we're going to consider classes of perturbations that generalize. So this scale parameter, when h goes to zero, we're going to say that this concentrates at p theta. So you sort of, this h is sort of a tuning parameter, what level of perturbation you're going to expect. Okay, let's make that this concrete. Make that this concrete. So, I should say also that NPV models are not my idea. There are several examples of them in the literature dating back at least 20 years, but they're not well known and they haven't really been discussed all together as a class. So I think they're worth sort of studying because I think they're a very interesting class of model. So here's one that I think today is maybe the most, oops, maybe the most famous. And in fact, And in fact, mention of a related paper came up earlier today. And so, in this model, we start with a parametric model and then we perturb it according to a Dirichlet process. We start with the likelihood of our model, and then we draw a new perturbed distribution according to a Dirichlet process. You'll notice this has the properties I was alluding to previously, in the sense that when this concentration, when h goes to zero, corresponds to the concentration parameter going to infinity. parameter going to infinity and this Dirichlet process becomes a spike at p theta. You can also do a smooth version of this. So this actually, to my best of my knowledge, hasn't been published, but Jeff gave a series of talks on it back in 2019, motivated by work on course litigating Jeff Miller. And it is basically a smooth version of the same thing. There's also a classic paper, which I won't describe in depth because the I won't describe in depth because the polia trees today are maybe not as famous as they once were or well known. But this was a beautiful paper by Jim Berger back in 2001 that explained how you could draw a polya tree perturbation centered at a parametric model. We'll rely on that a little bit. There's also, you can do related constructions using, for instance, the Gaussian process density model as well. Density model as well. A lot of non-parametric models, of course, have a natural base measure. I'm going to make one modification to this recipe that others have considered, which is to make it adaptive. Well, actually, I shouldn't explain it like that. I'm going to add a particular prior, which is a famous prior, which is the spiked slab prior. So I'm just going to say I'm just going to say that the scale parameter, the amount of the perturbation, is going to be drawn from a sparse prior. The idea here is that I'm just trying to put mathematically in this belief that we all have, this subjective belief, that the parametric model is probably roughly true, right? And so that's the math that this encodes. It says that, you know, at the spike, the parametric model is exactly true. If the If the uh with the slab the parametric model is allowed to be roughly true. Um, and so the idea that you should have in your head is simply that we're going to, um, like what we're doing is we're encoding this idea that the model is probably roughly true. And it's, we're using a spike in slab prior because it happens to be a very powerful device with a lot of cool properties. Um, and in particular, uh, we'll, you know, Will, you know, not only does it sort of promote sparsity, but it has some nice adaptivity properties and some relationships to Bayes factors that I'm going to exploit. Okay, and this is the picture that I want in your head again. So, right, so we're thinking here about like the space of distributions. I'm showing the model class as a spike because, of course, like with the parametric model, it's lower dimensional than the space of all distributions. So, I'm sort of showing it as a. I'm sort of showing it as a 1D or a 0D manifold. And then the true distribution can be outside that model class. And what we're doing in a non-parametrically perturbed model is we're allowing for some spread. Okay. So hopefully you're happy with this. So the first question as you abandon parametric models is, well, what are you actually trying to do? Is well, what are you actually trying to learn? Usually, when we talk about, or at least when we analyze Bayesian procedures from a frequentist point of view, especially, you know, we can talk about like a true parameter that we want to do inference on. But of course, if the model is incorrect, we don't actually have a true parameter anymore. And so the first thing we need to do is to define our estimate. estimate. And we'll do that using what is becoming a standard approach in non-parametric phase and robust phase, which is to focus on functionals of the data. So we're going to say that the thing that we care about can be written in terms of some function of the true p0. And then when we do inference, we can simply push forward the posterior. We can simply push forward the posterior over distributions through this functional to get a posterior over the S demand that we care about. I want to say briefly that this is a particular type of robustness that I'm studying. This is robustness where the parametric model might be wrong in the sense that it doesn't describe the data distribution. But this is not robustness in the sense that the data distribution might Data distribution might be wrong. So, in other words, we're not going to consider situations where P0 isn't the P0 you care about, for instance, because there's a population of outliers. We're also not going to study situations where the target functional is wrong. So cases where this shows up, for instance, in causal inference quite a bit, but where you might be concerned that this functional of, say, a backdoor adjustment functional, which I'll explain in a second. Or adjustment functional, which I'll explain in a second, isn't actually the causal effect. So we're not dealing with that type of robustness. We're dealing with robustness that comes from this parametric model, not describing the true data distribution. And just to make this more concrete, here are some functionals. Those functionals might be summary statistics, means, variances, et cetera, of the true data distribution. They can be loss minimizers. So if you have a parametric model that's So, if you have a parametric model that's interpretable in some way, you can ask about the minimizer, oops, sorry, the minimizer of the expected loss under the true data distribution. And so that might give you a more, this gives you sort of a more general way of constructing an interpretable or useful or informative summary of P0. And then finally, And finally, many causal effects can be written in this way. Here I'm only giving one of the most famous, which is sort of the effect of a treatment setting A to some value in the presence of an observed confounder, W. Okay. But there are many more functionals like this, and many things can be recast as functionals in this form, though not all. Okay, so I've told you a little bit about an NPP model. I've told you how we might be able to do inference on it. So by switching our attention from a true theta zero to a functional of the true data distribution, we can simply push forward the posterior over the data distribution onto that functional and learn about the quantity that we care about. And so in this next section, I'm going to do a theoretical analysis of the frequentist asymptotic properties of NPP. Asymptotic properties of NPP models. I'm not going to go deeply into the assumptions here, but many of them are sort of standard, sort of Vandervaart textbook style assumptions. I'm just going to summarize those. So first, robustness. So of course, robustness has many meanings, but here we've been focused on this question of what happens when the parametric model is wrong. Will you get consistent inferences? In other words, will Consistent inferences. In other words, will your posterior converge to the truth? And of course, if the parametric model is wrong, it won't. It won't converge to the true data distribution. And as a consequence, the push forward onto the functional also, in general, will be inconsistent. If you instead use a non-parametric model, it doesn't have to be this NPP model, any non-parametric model, you can in general hope or expect that that will in fact be robust. That will, in fact, be robust in the sense that the posterior over the distribution, because it allows for effectively all possible distributions going to converge to the truth. And so, under some smoothness conditions, you should also get consistent posterior inference of this functional quantity that you're targeting. And PP models inherit the properties of non-parametric models in the sense that they also give rise to this consistency in the setting. In the setting where the parametric model is misspecified, and hence I'll say that they are robust. They're robust to models being wrong. Okay, so now let's turn to efficiency. So conventionally in non-parametric models, what you're told is that you pay a price for being for this robustness, for this general consistency. And the price you pay. Consistency and the price you pay is statistical efficiency. I'm not talking about computational efficiency, just talking how quickly does the posterior concentrate around the true value. And so there are various ways of quantifying it and so on. But the thing you can generally expect is that with a parametric model, you're going to get very fast convergence rates. And with a non-parametric model, And with a non-parametric model, you're going to get slower than parametric convergence, right? It's non-parametric. And this, my intention here really is just to provide some quantification, something to hold your head on in terms of why, also in terms of why parametric models and informed priors and so on can be so useful for practical purposes because basically they can get you closer to the answer. Because basically, they can get you closer to the answer more quickly in terms of the number of data points. And so, what NPP models have is despite their robustness, despite this non-parametric style robustness that they have, it turns out they are also very efficient. And so they converge at a parametric rate. And so, and they do so when, and I'm sorry, and they do so when the parametric model is well specified. So, this is the idea when the So this is the idea. In general, you can expect an NPP model to converge to the truth. When your parametric model is actually true, then you can expect it to converge quickly to the truth, just as quick as the parametric model itself. So in essence, if you replace your parametric model with an NPP model, you lose nothing. Approximately. You approximately lose nothing in the case where that parametric model was actually. Where that parametric model was actually right, and you gain something in the case where the parametric model actually happened to be wrong. And I won't dwell on it too much, but I will say for those of you who are causal interested in causal inference and know the frequentist causal inference literature, this set of properties was in part inspired by an effort to come up with some Bayesian approaches that had the same flavor as. Flavor as classical doubly robust style properties. And so, if this is rhyming with that, I'm very glad I can talk about it at the end or offline. But those also have this flavor where when models are right, they converge quickly. And even when models are wrong, they can still converge, if more slowly. But the machinery behind them is very, very different. Anyway, so here's the idea behind the proof. Behind the proof, it's pretty simple in some sense. The idea is to look, the idea is this behavior basically stems from this spike in slab prior that I introduced. It's a form of this adaptivity. And so the way you should think about the posterior in a NPP model is that there's two components. There's a component that comes from the spike and a component that comes from the slab. The component that comes from the component is the component. The component that comes from the spike, remember I said that the spike corresponded to just a delta mass at the true likelihood, right? And so then the posterior that you get is simply the straight parametric model, posterior, the same thing you were getting before. Whereas the flab gives rise to a non-parametric posterior that's going to, which I'm denoting with this pi plot. And then, and this is sort of the crucial. And then, and this is sort of the crucial part, there's going to be this mixing weight which trades off between them. And that mixing weight is determined by the Bayes factor comparing the parametric model, which is going to be up top, the marginal likelihood of the parametric model, the marginal likelihood of this slab model, which I encourage you to think about just as a more general non-parametric model. And so what's going on is what we're comparing. Going on is what we're comparing. This base vector is basically saying, well, how well does the parametric model explain the data versus the non-parametric model? And what you will expect is that when both models are correct, because the parametric model is lower dimensionality, the space factor is going to favor the parametric model. And so you're going to put a lot of weight on this, and you're going to inherit the properties of the parametric model. And the parametric model is incorrect. You're going to inherit the properties of this. Incorrect, you're going to inherit the properties of this non-parametric component. Okay. Hopefully, that sort of makes sense as a proof sketch. So, of course, like I've been hand-wavy about, I mean, I've given you some examples of non-parametrically perturbed models, but I have not discussed their details at any length. They seem, they hopefully, I mean, maybe not hopefully, but at least. I mean, maybe not hopefully, but at least on first blush, they seem pretty challenging to work with, I think. You have to do both barometric inference and non-parametric inference at the same time. And there's going to be challenges scaling it. There's going to be challenges adapting all your samplers to handle it. And so, by the way, I should say this is not necessarily true. There are settings where you can get this to work. Settings where you can get this to work more efficiently. But it is, in general, I think, safe to say that it's true. And so, what I'm going to study is a very aggressive, I used to call it, I previously labeled this section generalized Bayes Approximation. It might be best, and then I changed the generalized Bayes approach because it's, so it's really more like a piece of methodology that is inspired by this posterior than an attempt to get something that is close to it in some. That is close to it in some quantitative sense. Okay, and so the approximation method stems from this posterior decomposition that I gave you, where I decomposed it into the spike and the slab. And we're just going to stare at each component and see if we can come up with something easier. The first one, I'm going to assume, is just the parametric model posterior. And I'm going to assume. Model posterior. And I'm going to assume that we can compute that. So remember, I sort of motivated this by, I want a method for taking your parametric model that you've built a nice package for already and repackaging it to make it more robust. And so I'm going to assume that you've built that parametric model and you can do posterior inference. Then I'm going to look at this lab posterior. I'm going to say, well, that's kind of complicated. How about we just use how about we just use any non-parametric or semi-parametric model that's going to be consistent for the target functional. Turns out, it will turn out that we can do that and still preserve these properties I've talked to you about that the NPP model had. And then finally, and this is maybe the heart of the technical contribution here, we're going to adjust this mix. Going to adjust this mix, we're going to change this mixing weight. And this is more challenging. This is a remember that this mixing weight comes from two marginal likelihoods. It comes from the marginal likelihood of the parametric model and the marginal likelihood of the non-parametric model, both highly intractable in general. And so we're going to come up with an alternative, which is all think of as a generalized, which we can call a generalized base vector, which is going to have a lot of the same behavior. Okay, so we just heard about generalized phase, and specifically we heard about the type of generalized phase that I'm going to be considering today, which is you might call likelihood generalized phase. And this is inspired by this observation that hidden in your Bayesian posterior is a divergence in some sense, or at least the divergence. In some sense, or at least the divergence has to be constant. So, right, the average log likelihood of the data is related, of course, to an empirical estimate of the K L. And so, generalized Bayes, this branch of generalized Bayes, this flavor of generalized Bayes, is inspired by the thought that, well, maybe we can replace this KL approximation with another empirical divergence comparing the data to the distribution. In the approach that I'll follow, I'm going to be pretty agnostic actually. So, we've proved the results that I'll give for general results that apply to like general divergences of a particular class. Okay, so here it is. I will spend some time meditating on this expression. Expression. So there's a generalized Bayes factor up above. I'm going to encourage you to ignore rho for a second. And what you should be thinking about here is how does this generalized base factor reflect the properties of the previous base factor, where the key properties of this previous base factor was it went to infinity when the parametric model was correct. Was correct and it went to zero when the parametric model was incorrect. And so the term I want you to look at is first is, or maybe only in some ways, is this expectation. So this is the, I haven't defined this notation, so I will explain this in words. This is the expectation under the parametric model. Under the parametric model posterior of an empirical divergence between the likelihood of the model and the data. So you should think of this as a model criticism score. It's asking if we look at the posterior over model likelihoods. So this is related, of course, to the like the posterior predictive is the mean of this posterior over likelihoods. If we look at the posterior over If we look at the posterior over likelihoods, and we look at how well those likelihoods actually match the data according to this divergence, and then we average that mismatch level over that whole posterior. So we're looking at all the possible model likelihoods that the posterior is considering, and we're seeing whether they actually look like the data distribution according to a non-parametric measure of like the Wasserstein or MMD or KSD. KSD. All right. And the divergence that we're considering today are all ones where we have a true zero. And so that means that this quantity, this divergence is going to go to zero if p theta actually matches p zero. And what that means is that this expression on the bottom is going. Is going to go to zero. And so the whole thing will blow up to infinity, favoring the parametric model. It'll only blow up to infinity, though, if it's balanced correctly by this term, which I won't go into too much detail about, but there's basically this rate term here, this n plus 1 to the minus r. And we're going to set that rate. That r is a hyperparameter, but it's one we can set in this n plus 1. This n plus one factor that we've added on the end is going to drive it down to is going to drive the whole base factor down to zero. There's a few other things going on in this expression, these etas and this row function. The row, I won't talk about the row too much, but the construction here is set up such that when you have zero, Set up such that when you have zero data, or at least sort of in the limit where you have zero data, you should recover the prior probabilities on these two alternatives. Okay. So this is the expression that we came up with. And here's the result that we got with it. So this thing is carefully designed to match the behavior of the conventional base factor such that we Base factor such that we recover the robustness and efficiency properties of the original NPP model. And so, regardless of what the true P0 is, this, we're going to call it a GNPP for generalized Bayes non-parametrically perturbed model. This GNPP posterior is going to converge to the true functional that we care about. And if the model is correct, it'll converge efficiently. I realize that I'm running out of time. Running out of time. So, I'll briefly give you some simulations, which I won't talk too much about the setup, but give you some flavor of the idea. So, here's the misspecified setting, and the non-parametric model and the NPP model are just on top of each other. The NPP model is matching the robustness of this non-parametric model. And when the model is well specified, the NPP model matches the behavior. NPP model matches the behavior of the parametric model. It's just as efficient. Oops, sorry. The GNPP model inherits that behavior despite this funny generalized Bayes factor construction. And although our proofs around coverage are a little rough, it's challenging to prove coverage for non-parametric models. Empirically, the coverage looks very good. So the non-parametric, the generalized The generalized non-NPP model still achieves nice coverage in both the well-specified and misspecified settings. Okay, so what I've told you about today is a new method for packaging the parametric vision model in a statistical way to make sure that it can be applied robustly to new data sets. And although the Although the derivation might seem involved, I hope that the intuition is clear. The intuition is that you take your parametric model, and then you take a backup non-parametric model, which is guaranteed to be consistent, and you mix the posteriors of the two. And you mix the posteriors of the two according to a score, which depends on a divergence, which looks at the mismatch between the parametric model and the data. And our hope. And our hope is that this can provide a step towards producing robust and reliable and efficient inferences. So not losing any of the advantages of parametric models, but gaining robustness even when the models might be wrong. There are many related ideas, which I don't have time to go over, but we were certainly inspired by previous work on NPP models, even though they hadn't studied this adaptivity question. And we were also inspired by And we were also inspired by recent robust Bayes work and by trying to achieve some of the flavor of results in semi-parametric Bayesian causal inference, which itself builds on theories of double robustness and influence functions and so on. So thank you very much. Maybe in the interest of planning, they can't be able to do it.