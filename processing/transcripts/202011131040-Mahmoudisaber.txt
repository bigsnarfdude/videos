Thank you very much, Leonid. Today I'm going to talk about my project on estimation the feature selection accuracy of machine learning models in bacterial genetic framework mapping. That is, this project is being done at Shapur Lab at Montreal University with collaboration with Lomajinderovich at Royal College and Simon Fraser. As previously highlighted, As previously highlighted a few times in this meeting, in this symposium, the mapping, the relationship between genomic continental bacterial pathogen, and the front are essential to precision medicine. However, this process is confounded by multiple, by multiple confounding factors, such as population stratification, genome-wide linkage, disequilibrium, and some others. And so far, we have two main approaches for this bacterial genotype phenotype mapping. First is genome-wide association study or GBAS and GBAS pay. Study of GBOS and GBOS-based polygenic use score analysis. And we also recently have machine learning models. However, since the GBOS approach explicitly controls for some of these confounding factors, machine learning models often do not explicitly adjust for them. So here our question was that whether the machine learning model are able to identify and adjust for these confounding factors or they are just randomly identifying the patterns, the random pattern that is available in the machine. Patterns, the random pattern that is available in data, and thus producing ungeneralizable results. To evaluate this hypothesis, we develop a bacterial genotype phenotype simulation framework. And using that, we benchmark the multiple known machine learning-based GWAS tools. And bottom line was that the Lanyard Mix model implemented in GIMA was the best performing method. So we use this Lanyard-Mix model as our reference to compare different machine learning. Compare different machine learning tools. So far, we evaluated a few standard machine learning models, including regression-based models, such as logistic regression, linear support, vector classifier. And you also have ensemble learning models, such as random forest, and you also have a gradient boosting model, such as extreme gradient boosting, and light gradient boosting model. And for the feature importance estimation, we have for regulation-based models, we use the coefficients and for. Coefficients and for ensemble learning model, we use the Guinea important, that is the normalized reduction in impurity brought about by each feature across all the trees in the model, and did a small modification that was also applied to gradual boosting. And the result, we first evaluated whether these models are capable of correctly ordering the 100,000 features that we have in each data set, in each stimulated data set. Each stimulated data set. And as you can see here, in small sample sizes, such as samples of 400 and 700, specific machine learning models, such as light-garden boosting model, logistic regression, and Linear SPC can perform better than that of GIMA. Sorry, to interrupt here. You are not actually changing the slides, or they're not changing for me anyway. I'm not sure if everybody else has this problem. Problem. So, can you have this slide right now? Yes, it's okay now, yeah. It's okay. Yeah, as I was saying, as I was saying, for a small sample size, this certain machine learning model outperformed our best performing linear mix model implemented in Gamma. Model implemented in Gamma. With increasing the sample size, the mix model implemented in Gamma reaches in performance to data linear SPC and light segregation. However, our light gradient boosting model still has a better performance. But this shows that the machine learning model can potentially identify and correct for the confounding factor as good as or even slightly better than that of best for morning mixed models. For more in mixed models. We also next check whether these models, whether there is a bias in identification of the causal marker with different effect sizes through different models. And for that, to evaluate this hypothesis, we tested the power of these different models in different models in identifying the causal markers with different effect size in the top 50 ranked features. And as you can see here, I mean, we have a range of simulations, so there is a variation in the power. Simulation: so there is a variation in the power, but there is a pattern, there is a general pattern, and that's it. That in small sample sizes, the most the certain machine learning model, such as again LGBM, binary SDGs and logistic regression, performs equally or even better than Gamma in identifying both a small effect size and high effect size causal variant. With increasing the sample size, the linear mix model in Gamma reaches in performance in identifying causal variant with high effect size, but in for small effects. Sizes, but in for small effect sizes, still the machine learning model has a higher power in identifying the causal markers. So, this is this project undergoing, and next we are going to test additional feature selection models, such as Lasso regression and L-1 regularized garden-boosting model. We also plan to try a deep learning model, and for that, we need to develop a computationally scalable permutation-based feature selection method. That is so far, we could not scale it using CPU, so maybe GPU would be an answer. CPU. So maybe GPU would be an answer. And we're also going to try the effect of evaluate the effect of recombination on feature selection and also replicate the result on an independent data set. Thank you.