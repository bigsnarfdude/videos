To introduce the first speaker, and he's going to talk to us about the dynamical easing cats model and the fact that it converges to pi four in three dimensions. Thank you very much for the introduction. Thanks to the organizers for inviting me here. So, I'm going to talk about my joint work with Paolo Grazievsky and Henrik Weber. So, Paolo is a graduate student at the University of Bas who is going to Of bath, who is going to defend in a couple of weeks. Yeah, so in our work, we proved quite a well-known conjecture about the convergence of the Easy-Cause model in three dimensions. So let me start with the integrities in the model. Okay, so I will start with the general model in any dimension. So D will be the dimension. So we look at a grid, integer grid, and we integer grid and we periodize it so just it will be much easier to look at the periodic setting so we look at the discrete discrete torus discrete cube so that the size of the side of the cube is 2n plus 1 okay so for each grid side we associate a spin value so sigma so so lambda is our grid so for any Is our grid. So for any site on the grid, we associate the spin value plus or minus one. So on the picture, you see just a realization of a spin configuration in two dimensions, a two-dimensional grid. So we want to introduce an interaction between the different spins, and we use the so-called Katz interaction. So, what it means is the following: we take Following: We take a smooth, compactly supported function k, so it needs to satisfy certain assumptions. It is a rotation invariant, it is zero at the origin, and we rescale it by some positive parameter gamma. Okay, so we call it kappa gamma. So, this function kappa gamma will describe the interaction between different spins. So, more precisely, we can define a Hamiltonian in Hamiltonian in the following way. So you should read in the following way. So you have two sides k and l on the grid, and the interaction between the spins sigma k and sigma l is given by this kappa gamma function. Okay, so in other words, the spin at some site on the grid looks into the neighborhood of the radius one over gamma. One over gamma, and it interacts with the spins in this big neighborhood. Okay, so given the Hamiltonian, it is natural to define the Gibbs measure. That's in the usual way, exponential of minus beta on the Hamiltonian. And beta is the inverse temperature. So, depending on the temperature, depending on the value of the temperature, we observe different pictures. So, more precisely, when the temperature is high, so maybe. The temperature is high, so meaning that the inverse temperature beta is small, we observe a very chaotic picture. So the spins behave like as if they're independent. On the other hand, if the temperature is low, meaning that the inverse temperature beta is large, so the picture looks more frozen. So we have big islands of plus ones and minus ones. These islands are moving slowly. So somewhere in between, So, somewhere in between, there is a critical temperature where the phase transition happens from one picture to another. And okay, if we define our model correctly, then the phase transition happens at the critical temperature one. So, we want to study the random fluctuations of the easing. So, this model is called Ising-Katz model because of this Katz interaction, and we want to study the random fluctuations of this model around the critical temperature. Model around the critical temperature. Okay, so near the temperature one. So this is a very natural question in statistical mechanics. So what happens is that if the phases are stable, like this one or this one, then one typically expects to see the analogs of the law of Leische numbers and the central limit theorem. So in other words, the rescaled random fluctuations in this phase. Random fluctuations in these phases should be Gaussian. On the other hand, near the critical temperature, typically the fluctuations are significantly more complicated, something non-trivial. And very often these fluctuations near the critical temperature are described by solutions of stochastic partial differential equations. So, we want to study the dynamical version of the Isimka's model, so we need to introduce. Version of the Zimcast model, so we need to introduce time. There are several ways how to define the dynamical version. We use the simplest one, the Glauber's dynamic. So the generator of the Markov process is given by this formula. So what it means is the following. Given the configuration of spin sigma, the spin at site j changes its value with this rate. So this rate, sig gamma. So, this rate C gamma depends on the configuration and on the side of the spin which wants to flip. So, we take these rates in such a way that the Gibbs measure is invariant for the Markov process. So, more precisely, we define the flipping rates in this form via the Gibbs measure. Okay, then it is not so difficult to see that this dynamics defines a Markov process. This dynamics defines the Markov process, and the Gibbs measure is stationary for this Markov process. Right, so rather than studying the limiting distributions of the Easen-Cas model, which I introduced, we are going to study the limiting distribution of this dynamical version of the Easen-Caus model. And more precisely, we will look at the local averages of the spins. Averages of the spins. So we define the quarter-grained field H gamma in the following way. So we just take the local average of the spins at time t, and to average them, we use the function k gamma. This is the same function kappa gamma, which was used to define the interaction between spins. Okay. Then one can see that the Then one can see that the flipping rates of the model can be written in terms of the function h gamma. So, more precisely, we can write explicitly the formula for the flipping rate. And if we expand the hyperbolic tangents into the Taylor series, we observe the powers of h gamma. First, third, fifth, and so on. So, what is remarkable? So, what is remarkable about this model is that it is a mean field model. What does this mean? That if we write an equation for this function average spin field h gamma, then the equation will be written only in terms of this h gamma. So, the pure spin field sigma will never appear in this equation. So, more precisely, we can just write the usual Martin-Gabe. Just write the usual Martingale problem for this Markov process, and we get this equation. So, the evolution of H gamma with a step to T is given by this equation where we apply the generator, the Markov process, to H gamma. Okay, so the driving noise here is a Cadlock Martingale, and we can write precisely what the bracket process of this Martingale is. Yeah, so first of all, let me write what the generator is. Of all, let me write what the generator is. So, if you apply the generator to this function h gamma, it can be written in this form. So, I use the Stellar expansion of the hyperbolic tangents. So we see the powers of H gamma, first, third, and so on. And we see the linear part, which looks like an approximate Laplacian. So, after appropriate rescaling, it is expected to converge to Laplacian. Yeah, so we can. Yeah, so we can also write explicitly the bracket process of the martingale. So that's the general formula looks like this. And again, if we use our formula for the flipping rate C gamma and ignore some higher order terms, the first leading term of the bracket process will be this one. Again, okay, so this is already good. Any questions so far? Any questions so far? So, lambda is a still finite function. Yeah, so lambda is a parameter. So, in the end, we want to send it to zero. But before sending it to zero, we are going to rescale this process. I will show you right now. So, we take three parameters: delta, alpha, and epsilon, and rescale the average. The average spin field. So we call new process x gamma. And our final goal is to send all the parameters to zero. So all these parameters, delta gamma, alpha epsilon will go to zero. But in order to get some meaningful limit, we need to choose appropriate relations between these parameters. So if we plug in this rescaled field x gamma into our previous equation. X gamma into our previous equation, then we will get the following one. So this is the equation for X gamma. So the martingale here will be the appropriately rescaled martingale from the previous equation. So this new function k gamma will be just the rescaled interaction function kappa gamma. So So now you can see that, right? So, what we get in this equation is we get an approximate Laplacian here, we get the first order term, third order term, and so on. So, first of all, if you want to get a stochastic PDE, we want this term to stay in the limit. So, we need to choose the parameters in such a way that, well, we should just take this expression to be equal to one. On the other hand, we also want to see. On the other hand, we also want to see something non-trivial in the limit. So we want to see the third power of the solution in the limit. So we also take this expression to be one. On the other hand, we also want the Marten-Gale to converge to the cylindrical Vino process. So if you write the bracket process for this Martin-Gale, we get this expression. So in the limit, these functions k gamma, they behave like delta functions. They behave like delta functions. So, in the limit, this integral will give us, what is inside the integral will give us a delta function. And in order to make sure that it doesn't vanish and doesn't blow up, we also need this ratio to take to be equal to one. So we have three relations, one, two, three. We put them to one. And we also need to make sure that this one doesn't blow up. It may go to zero, but we don't want it to blow up. So here you can see that. So, here you can see that the temperature one plays a special role. So, well, so if you take beta to be not equal to one and send alpha to zero, then it seems that this term will blow up. So, the only hope to prevent this blowing up, we need to be at the critical temperature one. So, more precisely, in order to get all these relations which I described, we take the parameters in the following form. Defined, all of them are defined in terms of gamma. So, these computations are pretty standard, and probably they have been known for quite a lot of time. So, I learned about this conjecture from the review. About this conjecture from the review paper of Jacomin Lebois-Prussuti in 1998. So they performed all these computations and they conjectured, okay, if we choose the parameters in this form, when we send gamma to zero, meaning that all these parameters will go to zero, then in the dimensions one, two, three, we should observe the solution to this stochastic PD. Okay. Okay, um, so this is the stochastic PDE driven by the spacetime white noise, and the nonlinearity is given by the third power of the solution. Right. Sorry. Yeah, so if we take beta to be one, then this seems to disappear. But then I take beta to be one plus alpha c, and then this one. A C, and then this one will give C times this expression. Yeah, so I take beta to be beta minus one to be C alga. Yes. So at the beginning, you said this is the fluctuation around the phase transition. So this are yeah, this is random fluctuations near the critical temperature. So what you are doing works in that area. Yes. Not somewhere else. Yeah, you see. Not somewhere else, yeah. You see, beta should be close to one. It's like either it is equal to one or it should be one plus something small. So we are near the critical temperature. Yeah, so the difficulty with this limiting equation is that it is well defined, classically well defined, only in one dimension. So in the one dimension, the solution to this equation is a function. So the product, the third power, is well defined. The product, the third power, is well defined. On the other hand, in dimensions two and three, if we ignore the nonlinearity, the solution to the Stochastic heat equation is a distribution. So when we take the third power, we need to take the third power of a distribution. So the classical theory doesn't work to solve this equation. So as I'm going to tell to talk about, so as I'm going to tell you later, so in this case, in the dimensions two and three, This case in the dimensions two and three, the nonlinearity should be renormalized. So, what it means is that we need to subtract a divergent constant from this nonlinearity so that in the limit, the equation makes sense. And the effect of this renormalization is a little shift of the critical temperature. So what we need to do in order to get this limit, we need to take the critical temperature of beta to be one plus something small. So this constant can be arbitrary constant. It will just give a Arbitrary constant, it will just give a linear term in the equation, but this constant sigma should be the randomization constant of the nonlinearity. Okay, so in other words, we look not exactly at the critical temperature one, but we are close to the critical temperature one. So, this randomization constant C gamma, it diverges when gamma goes to zero, but it diverges slower than alpha. So, si gamma times alpha goes to zero. Alpha goes to zero. And so you can already see in this scaling that something happens when the dimension is equal to four. When the dimension is equal to four, these formulas don't make any sense. And in fact, one doesn't expect to see anything non-trivial in the dimensions four and higher. So one of the explanations is that, well, there is no solution theory. So there is no solution theory for this equation in dimension. There is no solution theory for this equation in dimensions four and higher, and one doesn't expect to see some non-Gaussian fluctuations after normalization. Okay. Yeah, so in the dimension one, this conjecture was proved by many people in the middle of 90s. And well, the conjecture stays, so the result states exactly what the conjecture is. The result states exactly what the conjecture is described. So we take the scaling parameters depending on gamma, and in the limit, we obtain this stochastic PDE. So the proof of this result in one dimension is relatively easy because there is just a classical solution theory for this equation. And yeah, so as I said, the solution to this equation is a continuous function, and there is no problem of taking the There is no problem of taking the third power of the solution. So, in the dimension two, this conjecture was proved by Mura and Weber significantly late in 2014. And the reason was exactly because of the complications with the equation. So, already in dimension two, they needed to use randomization. And the whole proof of the convergence is highly non-trivial. So, more precisely, So, more precisely, we choose a scaling parameter as described in the conjecture. We take the little shift of the critical temperature by the randomization constant. In this case, the randomization constant C gamma diverges logarithmically. And so we can write explicitly the formula for the randomization constant. And so they prove that indeed in the limit when gamma goes to zero, they obtain the solution to this stochastic PBE. To this stochastic PD. Yeah, so as I said before, the solution to the stochastic PD in dimension two is a distribution. So it is a Bessel distribution with regularity mu strictly smaller than zero. So there is a problem with taking the third power, and that's why we need the randomization. Yeah, so we prove this conjecture in three dimensions. So again, we take the scalar. So again, we take the scaling parameters as prescribed before. We take the little shift of the critical temperature, and in the limit, we get the solution to this stochastic PDE. So in three dimensions, the solution to this stochastic PDE is much more irregular. So it is in the best of space with a regularity, strictly smaller minus one-half. So in other words, So, in other words, making sense of the third power is significantly more complicated. And as a result, the randomization constant which appears in this equation is significantly more complicated and it diverges as gamma to the minus three. Right. So, let me show you what the randomization constant is. Constant is. So it has three bits. One diverges with gamma to the minus three, another one logarithmically diverges, divergent, and one is constant. So we can write explicitly quite nicely the divergent parts, gamma minus three and logarithmical. So they're defined in terms of the Fourier transform of the interaction function k gamma. function k gamma and the constant term so we can write explicitly but usually the formulas for these constant terms are very complicated so we can we can show that when gamma goes to zero this randomization constant converges to some finite value um right so i'm sure you many of you have So, I'm sure you, many of you, have seen how the regularity structures work, the main ideas behind the regularity structures. So, I want to go quickly through the main ideas. So, we can perform a simple power counting of the terms in this equation in three dimensions. So, the drive and space-time white noise has a regularity minus five-half. So, it means that when we mean that when we apply the Schauder estimate, the solution to the equation is expected to have the regularity of the noise plus two, which is minus one half. Okay, so and as I said before, when we try to take the third power of the nonlinearity, it doesn't work. The naive approach would be to modify the noise, take some smooth approximation of the noise, solve the equation driven by this smooth noise, and take the limit when epsilon goes to zero. But in this case, what happens is that But in this case, what happens is that we get a trivial limit. So, as it was discovered by Martin Haar in 2014, the correct way to make sense of the limit of this modification is to renormalize the nonlinearity. So instead of writing this equation, we need to consider this equation where we subtract a divergent term from the nonlinearity. Okay, so when epsilon goes. Okay, so when epsilon goes to zero, the third power diverges, but also this term diverges. And these divergences compensate each other so that the limit makes sense. So how do we proceed? How do we compute this rimmarization constant? So we do the following. We perform the Prata-Debush trick. So we have our stack. So, we have our stochastic PDE, and we try to get rid of the worst term on the right-hand side. So, we want to get rid of the driving noise. For this, we define the just simple stochastic heat equation driven by our noise ψ. So, the solution to the stochastic heat equation, I call in this way, pi epsilon of a this diagram, like a lollipop. So, what happens now is that if we look at the difference of psi. difference of ψ epsilon and the solution to the heat equation we call this difference v epsilon this v epsilon solves the following equation so in this equation there is no noise anymore this noise disappeared after subtraction and we can compute the regularities again of the terms in this in this equation so the the solution to the stochastic heat equation has a regularity minus one half so if we One half. So, if we make sense of the second and third powers, they should have regularities minus one and minus three half. So, it means that now the worst term on the right-hand side is this one. Before it was a noise minus five-half, now it is this term, which is minus three-half. So, again, the shadow estimate tells us that the solution d epsilon to this equation will have the regularity minus three half plus two, which is one half. So, which is one half. Okay, so how do we make sense of this product? So, if you just take the second power, the solution to the Stochastic heat equation, it diverges when epsilon goes to zero. So, one needs to renormalize both of these products. And the renormalization constant is given exactly by the expectation on the second power, so it diverges. On the second power, so it diverges as epsilon to the minus one. So, in other words, if you want to take the limits of these products, we need to look at the randomization, renormalized versions. So, the solution to the stochastic heat equation converges. There is no problem with this. But then the second power would be normalized. We subtract the normalization constant. So, I denote this new term by pi epsilon of a cherry. One of a cherry, and this cherry has a limit in the respective space of Beso distributions. So, respectively, the third power should be normalized by subtracting this term. So, I call this pi epsilon of triple cherry. Then again, when epsilon goes to zero, we get some non-trivial limit. Okay, so before we because we renormalized all these products. We renormalized all these products, a new term appeared in the equation, right? So, and this is exactly the term which is cancelled out by randomization of the nonlinearity of the equation. But still, after doing this, we cannot make sense of this equation because the products which you see here are still undefined classically. So, both of these products. So, both of these products need again to be renormalized. So, we performed the Departed-Debush trick once again. So, now we get rid of the worst term on the right-hand side, this one. Yeah, so we define this stochastic heat equation driven by this term. And again, we perform subtraction. In this case, we need to add the epsilon and the solution to this new equation. New equation, then the new process W epsilon solves this equation. Okay, so we continue this procedure just step by step until we are able to make sense of all the distributions on the right-hand side. And in fact, there are not so many steps to be done. Okay, so I don't have much time. Let me just tell you. Yeah. Yeah, so if we now go back to our solution x epsilon of the initial equation, then we can write it as a local expansion of all these terms which we defined. So we can write in this form. So there is the first term given by the Salesforce-Sergei heat equation, then there is a constant term, then there is a term given by more complicated objects of Helder regularity one-half, and so on. So in order to make to solve this. To solve this equation, it is enough to take only this how many four terms. So, in other words, if you want to solve the stochastic PDE, we need to look at the solutions of this form. So, we need to look at the solutions given in form of kind of Taylor expansions where the monomials are replaced by some more complicated distributions of functions. Yeah, so the theory of regulatory structures developed by Martin Hayer allows us to perform all these computations. So it tells us how this object should be renormalized to make sense and then how to construct a solution in this form. So, if you now plug in this expansion into the equation, then we have. Equation, then we have the third power on the right-hand side. So, in fact, the regulatory structure which is required to describe the solution to this equation contains more objects than what we see in this equation. So, the whole list of the objects which we need to consider is here. This is highly notation, I think. Yes, this is his notation. Yeah, and so what you should understand from here is the following. What you should understand from here is the following. So we need to make sense of the limit of pi epsilon applied to this symbol. And then it will be the limit will exist in the best of space with this regularity, which is given here. Okay, just very quickly. Yeah, so the theory of regularity structure had a huge progress since it appeared. And basically the four papers which you see here, they give you a black box result. Black box result. So, given an equation driven by noise, as soon as the equation, the noise satisfy certain properties, there is a general result which tells you, okay, there is a randomization constant so that you get the solution to the stochastic PDE after the proper randomization. So, in our case, we consider a discrete model. So, our easing customer. So, our easy cast model doesn't fit into this framework. So, there are three main problems. First of all, our setting is well, first of all, the regularity structure is more complicated because when we expand the hyperbolic tangents, there will be more terms on the right-hand side. And we need to control also these terms to make sure that they go to zero. Okay, so it happened that only we need to control only this term. We need to control only this term. We need to show that this term converges to zero in a suitable best of space. Then the other terms they can just bound uniformly and it's not hard to control them. So on the other hand, the bracket process of the martingale is also a bit complicated. So first, before I showed you only the deterministic part of the bracket process, but it also has a random part. And the random part depends on the pure spin field. pure spin field and and the solution x gamma so we also need to control this product which also needs to be renormalized okay and what is interesting what we observed that in three dimensions this renormalization of the product in the bracket process influences the renormalization constant which we use in the equation in the initial equation this is some phenomenon which doesn't appear in two dimensions yeah so the uh Yeah, so the equation is discrete. We need to work in the discrete setting of regulatory structures. So, this is this has been done. This theory has been developed first by myself and Heier in 2016 and then by Erhardt and Heier three years later. And then, as I said, the driving noise is Martin-Gale, which needs this probably the hardest part of our work to develop, to prove all these limits of all these objects. Limits of all these objects which you saw before in the case when they're driven by Mars and Gaels. Okay, so I don't have much time. Okay, so our regular structure looks a bit bigger. It contains four extra terms. There is some technical difficulty to control these terms, but this is probably the smallest difficulty of our work. Just very quickly, so what is the problem with martingales is that when you want to control the piece. Is that when you want to control the peace moment of a martingale? So, in it, so when you prove convergence, you always want to prove tightness. So, when you prove tightness, you always want to control moments. So, the best way to control moments of martingale is to use the Brookholder-Davis-Gandhi inequality. So, in the case of continuous martingales, you have only this term on the bound. You control the peace moment of the martingale by the power of the bracket process. In the case of Of the bracket process. In the case of Cadlak Martingales, you have some error terms which is defined in terms of the jumps of the martingale. And now, imagine if you have stochastic integrals with respect to martingales driven by martingales of higher orders, like second, third, and so on, then all these errors which come from different martengales accumulate. So we need to develop some very hard generalization of this Brook-Holo-Diasin equality. Of this inequality, Brookholder Davis-Gandhian equality, which controls all these error terms which appear on the right-hand side. Okay, so I think it's better if I stop at this point. Thank you very much. Are there any questions or comments from the audience or anybody online? Yeah, they'll. Yeah, so where do the entire order trees in addition to the usual pipeline? So where do they come from? Yeah, so the nonlinearity of the discrete equation looks like this, and there will be higher power seven and so on. So the powers seven and higher, they can be bound uniformly. It's very easy. Formally, it's very easy. So, basically, the divergence of the seventh power is controlled by the power of the gamma which appears in front. But then this one, we cannot bound uniformly. So we need to... So this term converges to zero, but it converges to zero in some suitable space of distributions. So this fifth power gives us these extra threes. So, this is the fifth power, but then, yeah, so this is the fifth power, then this fifth power multiplied by the power of gamma. Okay, so the real thing gets the last two. Right, right, right. We need to take the limit of these two, but eventually they disappear. So, yeah, they disappear because they multiplied by a power of gamma. But to prove that they disappear is non-trivial. Right, right, right. Is good right, right, right. Are there any more questions? I have actually elementary question for you. So, when you use this the black or the mute approach, basically you use it to find the number of terms you understand. Later, you still refer them to regular assumptions. Well, because this is what a regulatory structure does. So, the regulatory structure, the still. So, the regulator structure, the theory of regulatory structure is some automatic machinery which performs these brata-dependent tricks. Yeah, basically, you just this is this trick is helping you to know how many terms you have there. Yes, exactly. So, for this equation, we can do it just by hand. And I think we need to make only two steps so that yeah, as soon as yeah, so we do it second time here and then we get a solution in. We get a solution in this form, and this is already enough. Okay, we should probably move on to the next door because the next time I'll put it on it, one minute I'm going to go to our next door.