Thank you very much, Lilia. And I would like to thank the organizers for the kind invitation to speak at this conference. I initially planned to be there in person and I'm sorry that this was not possible. Well, so I'm going to talk about work, part of it. Work part of it completed, part of it still pending jointly with Outside from Puki Hio and with Pedro Duarte from University of Lisbon. And let me start the talk with the original motivating question posed to us by Jiang Gong-kyo during a conference in Nanjing before the pandemic. Before the pandemic. So his question was: given the discrete quasi-periodic Schrodinger operator, which needs no introduction in this setting, the question is whether the Lyapunov exponent of this operator is stable under random perturbations. So this question, besides being interesting in itself, I think it's relevant also in computational. It's relevant also in computational problems. Lyapunov exponents are asymptotic quantities that are computed approximately in an inductive process that incurs errors. And if you model these errors by random noise, the question is whether the result of your computation resembles or not what was intended to compute. So, my goal is to phrase this question in a more precise manner. And before this, let me introduce some of the notations and terminology. So, firstly, consider the associated discrete Schrodinger equation, solve it formally by means of transfer matrices. So, the end transfer matrix is this product. Then, Then we can regard, we can interpret the transfer matrices as iterations of a dynamical system of a linear co-cycle. So defining for an energy parameter E, this matrix, SL to R valued matrix, this matrix determines a skew product transformation over the Taurus translation, right? And this skew product is called a Schrodinger linear co-cycle. Schrodinger linear co-cycle. Iterating this new dynamical system in the first component is simply the nth iteration of the Taurus translation. And in the second component, we have a product of n matrices, which are obtained by sampling this Schrodinger matrix along the orbit of the Taurus translation. So these are exactly the end transfer matrices used to solve the Schrodinger equation formally. The Schrodinger equation formally. So these geometric averages of the end transfer matrices converge almost surely to a quantity which is called the maximum Yapnod exponent, which depends on the data, but I'm going to emphasize its dependence on the energy parameter. So going back to the original question, it's well, It's uh well, it's it's a great question, and we are grateful for it also because it's open-ended, it's subject to various interpretations. So, the question is, what does it mean to randomly perturb this system? So, I'm going to indicate some options. One is you want to perturb the potential function, right? The potential function, right? Adding random variables. Another way of perturbing the system is randomizing the frequency. So instead of at each step translating by the same frequency, you can translate by a sequence of frequencies that are chosen randomly according to some distribution, right? And there are other ways you can do both. So let me start with the first way of randomizing this. Of randomizing this, randomly perturbing the system. So, I'm going to describe what we call mixed random quasi-periodic Schrodinger operators. So, begin with the sequence of real IID random variables with the common distribution, completely supported random distribution, completely supported common distribution mu. mu and consider the Schrodinger operator, the discrete Schrodinger operator with potential given by a quasi-periodic part plus this random IID random variables. Considering the eigenvalue equation, again, the end transfer matrices will have this form. So it's the potential which now has these two components quasi-periodic and Two components, quasi-periodic and random. The geometric averages of the transfer matrices converge almost surely now with respect to the product measure to the Lyapunov exponent of the system. So a more precise interpretation of our motivating question would be in this setting, the random This random sequence is a priori fixed. So now let's think of it as you may think of it as a random perturbation of the quasi-periodic system, right? So then assume that this amount of random noise is small. And we do this by multiplying with a small parameter epsilon, right? So then considering Considering a randomly perturbed quasi-periodic Schrodinger operator, the question is: what happens with the Lyapunov exponent of the original quasi-periodic operator as the size of the random noise goes to zero? In other words, is this function, the Lyapunov exponent, as a function of the size of the random noise continuous at epsilon equals to zero? That's one way of that's that's one way of uh of uh of formulating this question of course another one is uh considering noise in the choice random noise in the choice of um of uh of the translations um this second question uh is less we understand at this point we understand it a bit less so i'm going to focus on on this one now i want to for for a while To for a while forget about the mathematical physics background, the Schrödinger operator. And this is a dynamical systems problem. It's about the continuity of the Lyapunov exponent of a certain system as it converges to another one. So I want to introduce a terminology that allows us to study this question in this setting. It was actually introduced yesterday by Rafael. Yesterday, by Rafael in his talk, but let me review some concepts. So, first of all, the notion of a linear co-cycle, you have a base dynamical system, ergodic, it's assumed to be ergodic, and you have a matrix-valued function, A. Then this then they determine a new dynamical system, a skew product map on X times. Map on X times Rm, which in the first component is the Bay dynamics, and in the second is the linear map induced by the matrix value function A. Iterating this new dynamical system in the base will be just the iterates of the base dynamic. And on the fiber, you multiply the vector P by the matrix valued function A. The matrix value function A sampled along the orbit of the Bay transformation, right? So we usually, but not always, fix the base dynamics and it identifies this system with its fiber action. Sometimes one important component of this dynamical system is this measure mu on the base, and this may be allowed to vary depending on the context. Okay, so. Okay, so considering the iterates of the Co-cycle on the base, the geometric averages of these iterates converge almost surely to what we call the maximum Lyapunov exponent of the system. There are other Lyapunov exponents, there are M of them, but let me not define them. Anyway, in this setting, the maximum Lyapunov exponent is non-negative. is non-negative right so uh the the examples of examples of such systems that are most relevant to to to our questions are deterministic or quasi-periodic linear co-cycles they are co-cycles over the torus over a torus translation where the fiber action would would have to have some regularity the second relevant example The second relevant example are the random linear cocycles, where the base is a Bernoulli shift over a probability space of symbols, right? Could be finite or infinite compact, and the shift could be unilateral or bilateral, invertible or not. On the fiber, we consider locally constant Functions in the sense that they depend only, so it's a function that a priori depends on the sequence, but it depends only on the zeroth coordinate. This is because this is the model that encodes IID products of matrices. For instance, the linear co-cycle associated to the Anderson model fits in this setting. Of course, it makes sense to consider. Of course, it makes sense to consider a fiber action that depends on all the coordinates in some sense. This has been considered by us recently. But well, more relevant, for instance, to the Anderson model would be this when the cos cycle depends on the zeroth coordinate only. So these types of cos cycles have been studied for a long time. For a long time, starting with Furstenberg and others. Well, so we want to model these transfer matrices or iterates of cos cycles associated to a Schrodinger operator that has a quasi-periodic and a random component. And we do this by defining a linear co-cycle over a base. Co-cycle over Bayes dynamics that is mixed in the sense that it has Taurus translation and a Bernoulli shift in it, right? So I'm going to introduce what we call mixed random quasi-periodic base dynamics. So we start with the space of symbols. We consider the Bernoulli shifty over the space of symbols. And then our base dynamics will be a skew product map itself. Map itself, which is on this product, the space of sequences times the torus, which is the shift in the first component. And in the second, it's a translation. And now this translation could be by a frequency that it's chosen and fixed, right? And in this case, we would have just the product between the shift and the translation on the torus. Translation on the Taurus, but this translation may also be random. And this can be encoded by this or described by this iterated function system of Taurus translations. So this is also an interesting model to consider. Now, in the special case, which I'll focus more on this case of More on this case of when the frequency is non-random, the system is obviously already ergodic. But when it's not, then even the question of ergodicity of this map is not exactly trivial and other related questions are not exactly trivial. So we began our study with this question, understanding better the base dynamics. Okay, but I'll focus more on this when the frequency, in this talk, when the frequency is. In the stock, when the frequency is deterministic. Okay, so now we want to define a linear co-cycle over the space dynamics. So this would be a function defined on the space with values in the group of matrices. And to maintain the same analogy with the products of random matrices, we make this. We make this cos cycle depend only on the zeroth coordinate of the sequence, right? So it's a function of two variables. The two variables, the two components, one component is the random one. It's the sequence of random variables. The other is the quasi-periodic component, right? And we make it dependent only on the zeroth component of this. And when we iterate this co-cycle, what we get on the fiber. Cycle, what we get on the fiber are these products or compositions of quasi-periodic cocycles. Let me, I want to describe this in a slightly more abstract way, what we are, a more general way that will include what I just described. So, one way to see this model is the following. Following. Consider the if you think of quasi-periodic cycles, right, they are determined by a number, by a frequency and a function, right? The frequency is an element of TD and A is a function on the torus with values in some group of matrices. And so let me call G the set of all these pairs. The set of all these pairs, right? These are the quasi-periodical cycles. And this set G is a group naturally, right? It has a group structure. Hafaiel showed this, how to compose quasi-periodicocycles yesterday, right? So the way, so what you can do is, and this is what we do, we consider a measure on this group. On this group. And then we consider a sequence of random variables omega n in this group. So a sequence of a random sequence of quasi-periodic cycles chosen according to this distribution nu. And then consider the products or compositions of this. Of these random quasi-periodic cos cycles, right? This is what we have, this in particular is what we have here, right? When we study Lyapunov exponents of cos cycles, then one could actually look at the map that assigns to this measure that determines this multiplicative system in this group of quasi-periodic. mean this group of quasi-periodic cycles. So the map that assigns to this measure the Lyapunov exponent of this measure. So if you study, for example, the problem of continuity of the Lyapunov exponents, if you studied it in this setting, if you allow this measure on the group to vary, basically you are allowing everything to vary. If you go back, for instance, to the to the let me go back to the sorry about this, to the Sorry about this to the Schrodinger situation. So, this is as if you are allowing all the data to vary, the potential, the underlying measure that drives this sequence of ID random variables, the energy, everything. So, this is actually the setting in which we studied this problem, where we allow the measure on the whole group. The measure on the whole group of co-cycles to vary. Okay, so this is the setting of random quasi-periodic, mixed random quasi-periodic linear co-cycles. And now we were going back to the question, what does it mean to study the perturbation of a quasi-periodic co-cycle? Well, so let's fix a quasi-periodic co-cycle and Cycle and consider a measure, a probability measure on the Lie algebra, the matrices of trace M, zero, M by M matrices of trace zero, and let sigma be the support of this measure. So we want to randomly perturb our Co-cycle A. You can either add a small matrix or multiply with a matrix that is close to the identity. matrix that it's close to the identity right so that's what we do here right this is the this is a random perturbation of uh quasi periodic cost of the quasi-periodic co-cycle a right and so then the question the extended question of uh that was originally posed by uh by zhang gong is uh what happens with the lyapunov exponent of this of this cos cycle so now this question is in higher dimension So now this question is in higher dimensions, and it's not just for Schrodinger, it's for any kinds of cycles. What happens with the Lyapunov exponent as epsilon goes to zero and possibly as other input data vary, right? So in other words, is the map that assigns to A to the measure μ that we took here on the Lie algebra SLM and epsilon, is this continuous and specifically continuous when Specifically, it continues when epsilon is near zero. So, well, it turns out that when you start working on this problem, and we started this in the end of 2018, when you start working on this problem, you quickly realize that before you even take epsilon to zero, you need to understand To understand these mixed systems, linear co-cycles over mixed random quasi-periodic base dynamics. And these are not very simple dynamical systems. So before anything, well, let's go back. Let's forget about epsilon going to zero. Let's fix the randomness, right? Don't treat it as. Don't treat it as noise, but fix it. And let's try to describe some relevant questions that will be also important in later understanding what happens with the system when epsilon goes to zero. So again, our mixed random quasi-periodic linear cycle is a skew product that on the base you have this shift translation. Shift translation, possibly a random translation, and then it's the linear action. And there are some very natural questions about the behavior of the Lyapunov exponents for these cos cycles. So one is proving under general conditions the positivity of the maximum Lyapunov exponent and its implicity, right? And its simplicity, right? These are questions that go back to the work of Furstenberger and others in the IID setting. Then there is the question of the continuity of the map that assigns to the co-cycle the Lyapunov exponent, and then the modulus of continuity, whether this is Hölder, for instance, or not. So this is for, again, the randomness. And the randomness is not treated as a noise, it's fixed. And then these are problems that we already considered. Well, the simplicity, not yet, but the positivity and the continuity. They are already on the archive. And this is we are writing it. But let me just say before we get into what is expected to hold for these systems, let me just For these systems, let me just say the obvious for many in the audience that the continuity of the Lyapunov exponents is a delicate matter. And I'm going to illustrate this in the quasi-periodic setting that is again familiar to many of the people in the audience. Why is a delicate matter? Well, for instance, by work of Burgundian, Golstein, Schlach, Of Burgundian, Golston, Schlag, Christoph Marx, Avila, Gitomirska, Hasardel, Duarte, myself, and many, many others, we know that if the co-cycle depends analytically on the base point, and if the translation number satisfies some reasonable arithmetic conditions, right? So, at least in this setting, the Lyapunov. The Lyapunov exponent is continuous, even Herder or almost Herlder continuous under appropriate assumptions. On the other hand, if you leave the analytic class and you look for smooth quasi-periodic cycles and take your best translation number, the golden mean, Wang and Yeo showed that there are points of discontinuity of the Lyapunov exponent. Upnov exponent. And to make things even more interesting or confusing, Avila, last Chamis, and Tsijou showed that if you maintain the analytic setting, but you take your translation number to be, well, a Louville number, then the modulus of continuity can be very weak, right? So this is the So, this is the general picture in the quasi-periodic setting. On the other hand, in the random setting, for purely random cos cycles, at least in the case when they depend only on the zeroth coordinate for locally constant, that corresponds to, again, to the Anderson model. And this is the better understood setting. Better understood setting. So, at least in this setting, the behavior is much better. So, we have the positivity of the maximum Lyapunov exponent in a very general setting. This is Furstenberg's theorem. And this setting automatically applies to Schrodinger co-cycles. And we have the continuity or even Hölder or nearly Hölder continuity. Or nearly Hölder continuity. Again, if you don't want to think about co-cycles in general and just want to work with Schrodinger operators, then the classical theory of Furstenberg-Kieffer and Lopage are enough because they imply the Hölder continuity and all the properties that you need of the Lyapunov exponent. If you want to go outside of the Schrodinger class and study Study co-cycles that are out of the reach of Furstenberg's theory, then more recent results of Carlus Boker and Marcel Viana, Avilaishkin and Viana, Eduarda and myself cover also these basis. Okay, so this is the random setting. So what we should expect in this mixed setting. In this mixed setting, in the mixed setting, we expect that the random component be if the random component is non-trivial, then the expectation that it should be dominant over the quasi-periodic component. In other words, the system should behave as if it were purely random. And I have to admit, I have to... Admit, I have to confess that this is more based on fate than on this sort of expectation. Okay, a posteriori, this is true, but a priori, I think it was just fate that, or perhaps the Anderson's original intuition of the behavior of regarding the behavior of the Anderson model. I don't know, but we expected this to happen, that the randomness would be. happen that the randomness would be uh would dominate the the behavior of the of the at this level of the of the cos cycle of the lyapunov exponents of the cos cycle okay so um uh let me uh mention some results uh regarding uh regarding this um these systems mixed systems so last year uh jamerson beseja and mauricio poletti both former students of Both former students of Jana at IMPA, they considered mixed quasi-periodic random co-cycles where the randomness is driven by a finitely supported measure. And they look at two by two co-cycles and they prove that generically, in other words, for a C0 open and CR dense. C0 open and CR dense set of such cos cycles, you have positivity and continuity of the maximum Lyapunov exponent. But this condition is not that explicit, right? It's a generic statement. Outside, Duart and I showed that, well, in the setting that I describe, the setting that I describe, right, that it's that I describe here actually, right? So, which includes all the others, we showed that assuming some conditions that are, if you want, the analogs of the conditions in Furstenberg's theory for random products of matrices, right? But these are the analogues of them adapted to the setting. So we prove that under these conditions, the So we prove that under these conditions, the Lyapunov exponent is strictly positive. These conditions are generic and they include the Schrodinger coscorus. They are immediately applicable to Schrodinger cos cycles when you look in dimension two. And moreover, in this open set of cos cycles that are irreducible and non-compact, whatever that means, the Lyapunov exponent is continuous. And there are other There are other results in the spirit obtained a while ago by Kiefer, then more recently by Furman and Monot, and much more recently by specifically on the issue of the positivity of the Lyapunov exponent, or in other words, Fustenberg's theorem in this setting, obtained by Anton Gorodetsky and Viktor Klepsin, who actually announced this, interestingly enough. This, interestingly enough, I announced these results in a previous BAMF workshop. So there is an interesting intersection between their results and ours concerning this positivity. Theirs in some sense are much more general if you don't think that the frequency is allowed to vary, to be randomized. Okay, so what about stronger So, what about stronger continuity properties, such as Holder continuity of these Lyapunov exponents? Well, we have a tool which we use as a black box that says the following. If in your space of cosycles, whatever that may be, whatever kind of Bay dynamics, you manage to prove a certain type of statistical. Of statistical property called a large deviation type estimate of a uniform type, uniform in the sense that as you perturb your data, the estimates do not explode. So if that's the case, then the Lyapunov exponent depends continuously with the modulus of continuity, which depends explicitly of the strength of your large deviation. So this is the black box that. Black box that we employ in the study of these problems. And this is especially useful in degenerate situations. For instance, well, I mentioned this question of letting the noise go to zero. So having to study situations, well, of boundary between a kind of system and another. Another way situation. Another situation in which this tool is especially useful is when you have, in our extension of Lopage's theorem on Holder continuity of the up node exponents for random co-cycles from ones that satisfy Furstenberg's assumptions and ones that don't. So these degenerate situations can be treated much better when you have these kinds of When you have these kinds of estimates. So, let me just say what these estimates mean, although everyone, I guess, or most of us are used to these kinds of properties. So consider such a linear posycle and look at these geometric averages on the base. They converge almost surely to the Lyapunov exponent. So a large deviation type property is. type property is a more precise convergence in measure of right it's a convergence in measure but with a rate and this rate ideally would be would be exponential I mean this this this exceptional set would be exponentially small ideally which would ensure helder continuity of the of this exponent the up node exponent right so what did we get for our mixed models we We showed the following, we proved the following. So, again, in this setting, the frequency does not vary, right? So, the randomness, if you want to think just of the particular example of the Schrodinger operator, the frequency stays fixed and then we add random variables to the co-cycle. We have work in progress where we also will allow the frequency to vary, but this is. Will allow the frequency to vary, but this is less certain for now. So, assume, so this is the setup, we have a space of symbols, we have a rational independent frequency, the function that determines the cos cycle. It's continuous and in theta in the quasi-periodic variable, it's C1. It's C1. This is enough. Then assuming that the Lyapunov exponent, the maximum Lyapunov exponent is simple, and that this co-cycle satisfies an irreducibility condition, a la irreducibility condition in First Amber's theory, but adapted in this setting, which is generic. Then we have uniform large deviation estimates. And this implies Holder continuity of the of the maximum Yapnod exponent. Let me apply this, because this will be easier to digest. Let me apply this to our original setting of a random quasi-periodic Schrodinger operator. So you consider this operator and you assume that the potential function that is That is responsible for the quasi-periodic part is C1. Assume that alpha is rationally independent and assume that the sequence of IID random variables that you add has a non-trivial common distribution in the sense that the distribution is not support, it's not a one D rack delta, its support has at least two points, right? Has at least two points, right? Then everything I said applies and before applies, and then the Lyapunov exponent is strictly positive for all energies. The corresponding host cycle satisfies uniform Lash deviation estimates, and the map that assigns to the data the Lyapunov exponent is locally helder. And the data, what you can vary, you can vary everything. You can vary the energy, you can vary the underlying. Energy, you can vary the underlying distribution. Well, in the weak star topology, yes, this is important because you could vary it more trivially in the total variation norm, which is, yeah, would be too strong. So we vary it in the weStar topology. And you can vary v if you want, right? So this function is locally held. And okay, so this is the okay, this is a consequence of this more general result. And I have to say that so far we haven't taken epsilon to zero, right? The random part is fixed. I mean, it's not treated as noise. Eventually, I will discuss that. Eventually, I will discuss that problem also. But let me just say that this result has proven extremely technical to establish. We had to create new technology to deal with this issue. A priori, we thought that since we understand how to study the continuity of the Study the continuity of the Apnov exponents and large deviations for quasi-periodic systems and for random systems, we should be able to handle a product of such systems on the base. However, the methods used to study these kinds of problems are completely different. So it was not immediately obvious. Was not immediately obvious which approach should be the dominant one. It proved to be that the approach, the general approach used in the random setting, was more flexible. However, just briefly, I'm going to just say very, very, very, very quickly how large deviations are proven for products of IID matrices. So, this is a general There is a general method that comes from the work of Nagaev that studied Markov chains, statistical properties for Markov chains. This method was then refined by the French School of Probabilities based in Bretania, people like Lopage, Bougerel, Egnon, Hervé. And this method consists of the following. So you have a product of You have a product of matrices, right? So these GNs are IID random matrices in some group of matrices, doesn't matter, SLM, right? So this is a multiplicative process. Then you turn, first of all, you multiply here by a You multiply here by a vector. So you actually prove something more. You prove large deviations for the components of these matrices more than just for the matrices. So you turn this multiplicative process into an additive process. So this is the first step. But this additive process, it's on a bigger space. And instead of being IID, it's going to be a Markov process. So the problem reduces to proving large deviation. To proving large deviations, for instance, for a certain type of Markov chain driven by a certain Markov transition kernel. If you manage to prove that this particular transition kernel is strongly mixing in an appropriate sense, which is a replacement for the lack of independence, right? So if you manage to prove this, then you get large diploma. get you get uh you get large deviations you get central limit theorem and and other such limit laws so the goal is to prove that uh that the the kernel that it's uh it's um uh associated to to to to to this multiplicative system is uh um strongly mixing i can actually say what this kernel is it's uh basically the what the markov chain is you you start Chain is you start with the matrix and the vector, and let's take the vector to be of norm one, and then the matrix is in SLM, and then the transition will be for another matrix which is chosen randomly, right, independently of G0. And then you multiply G0 with V, and then you continue, you choose another one randomly, and you. Another one randomly, and you multiply the previous one: G1, G0, V. So, this encodes this product here, right? So, this is the Markov chain, and this Markov chain is, you can describe explicitly the transition kernel, and then the goal is to prove that this Markov chain is that this Markov kernel is mixing, strongly mixing in some sense. Sense. And okay, and this will hold not always, it will hold under these assumptions of first and that theory, which are applicable to when these matrices are Schrodinger matrices. Right, so the method consists, it's a functional approach. Associated to this kernel, you define a Markov operator, which is an operator. Markov operator, which is an operator that acts on the space of functions, which are functions of, if you want, SLM times S1. So they are continuous functions, maybe Holder continuous functions here. And then the objective in order to prove that this, so this is the Markov operator associated to this transition kernel. So then the goal is to show, in order to show that the kernel is strongly mixing, you have. Show that the kernel is strongly mixing, you have to show that this Markov operator is quasi-compact and simple. What does it mean to be quasi-compact and simple? The spectrum of this operator consists of a point, an eigenvalue with multiplicity one, actually it's eigenvalue one, and the rest of the spectrum. The rest of the spectrum will be contained in some ball of radius sigma, which is less than one. So you have a spectral gap in the spectrum of this Markov operator associated to the transition kernel that drives the system, right? So this is the, and because you have this, because the spectral radius of this restriction of the operator here is less than one. Here is less than one, you get that its powers converge exponentially, and this leads to the strong mixing of the kernel. Well, so this is the classical setting of random co-cycles. For mixed systems, random times quasi-periodic, the corresponding Markov operator, it will not be anymore like this. Its spectrum will not look like this anymore. Its spectrum will not look like this anymore. This part, you will still have, we managed to show that there is this spectral gap, but instead of having one eigenvalue of multiplicity one, the spectrum will consist of this entire circle. This is because of the quasi-periodic part and some part of the spectrum which is away from the circle, right? It's contained in a disk of radius strictly less than one. Right. So we so because of because of this, the argument, it's required a revision of the technology that existed previously. And in a way, this led actually to a simplification of the arguments used even for the classical setting. Classical setting. Okay, so moving forward, so we have whatever the method, we have this result. So let's go back to the mixed random quasi-periodic Schrodinger operator. I said already that the random path. The random part is dominant over the quasi-periodic part, at least when it comes to the study of the properties of the Lyapunov exponent. And let me actually, I forgot to emphasize a fact, going back to what happens in the quasi-periodic setting that I mentioned, the results of Wang Yo, of Adila Last, Shamis, and Xi Jo. And Sijou. So, if the potential is not analytic in the quasi-periodic setting, bad things could happen. If the frequency is not good enough, it's not diophantine or even, then bad things could happen. Well, if you add randomness to it, nothing matters anymore, right? This becomes dominant. Anymore, right? This becomes dominant. So we don't require the potential to be anything. C1 is enough, probably less is enough. We'd have to take some derivatives at some point. And we don't need arithmetic assumptions on the frequency also. That's all, right? So we add a bit of, we add randomness as long as this is non-trivial. The behavior of the Lyapunov exponent is as good as it was in. Is as good as it was in the random setting. What if your smallness is very small? No, now it's not, now it's big. I mean, it's fixed, right? It's the this is fixed. Epsilon didn't go to zero yet. No, no, no, no. But I'm saying, let's assume it's big, but it's very, very small. Well, then you have to iterate more the system until you feel the randomness. And then so. And then, so, for instance, I said that you have uniform large deviations. Well, they are meaningful after a certain number of iterates, and this is it's uh it's it would be related to the amount of noise. Actually, this is a very relevant question for the second, for when epsilon goes to zero. But for now, this is fixed. Yes. So then the next question would be, do random so Do random, so I've talked about the Lyapunov exponent. Now let's look at the operator. The question remains: do random impurities still turn a conductor into an insulator? In the sense that, well, this operator, since the Lyapunov exponent is positive, it cannot have AC spectrum by Ishi Pastur Kotani. So does it have pure point spectrum? Does it have pure point spectrum? Probably. Does it satisfy Anderson localization? So, does the randomness indeed dominate completely the quasi-periodic part? We expect it does. We haven't yet proven this, but we have the ingredients. We have ingredients for doing it. We have the Fustenberg's theorem, we have the large deviations. Theorem, we have the large deviations, we have the continuity of the exponent. And well, we have new methods for proving localization in the random setting devised by organizers of the workshop and their collaborators. So it's likely that some of these methods would be applicable with the ingredients that were already created, that I already mentioned. Okay. Mentioned okay, so this is something to consider later. So to now let's go back to the original question that motivated this series of projects and see what happens when the noise goes to zero. Put a parameter epsilon that is presumably very small in front of this sequence of random variables. And but the question is again if the Lyapunov exponent is continuous at epsilon equals zero. So we have a result with the outside Pedro Duarte and And that should be available soon that says that, well, we have to assume that V is analytic and that the Lyapunov exponent when epsilon is zero is positive. Well, of course, by Sorrette Spencer, this can be ensured by throwing in a large enough coupling constant in front of V, right? So assume this and assume that. Assume this and assume that alpha is that the translation is satisfies some arithmetic condition, for instance, is diophantine. In principle, these conditions are necessary, not exactly for in themselves, but to ensure that the unperturbed system satisfies large deviations. It's essentially It's essentially this. Well, there is a part of the argument where we actually use semi-algebraic sets where so perhaps V should not be too rough. But isn't it enough for V to be shaped? Yeah, yeah. Okay, this will work also. Yes, yes, this will work also. Yeah. Yeah. And now assume that now the assumption on the noise, it's that the noise is non-trivial, right? But in a stronger sense. So what we assume is that the distribution, the common distribution mu of this IID sequence of random variables has pointwise positive Hausdorff dimension at each point in the sense that for In the sense that for any ball of radius r, the measure of the ball is r to one-half, let's say, r to something greater than one at most. For instance, if the measure were absolutely continuous with respect to Lebesgue, this would work, but doesn't have to be, right? This is enough. We are not sure that this is sort of a preliminary assumption that works, whether Works. Whether it can be improved or not, we don't know. It leaves out clearly, it leaves out perturbations that have finite support. But anyway, so this is a condition that it's on the noise. Then the map, where we perturb whatever we perturb whatever we we we we want so the energy the the measure mu but we need to perturb the measure mu well uh ensuring this uh this positive housing dimension condition right and epsilon so this map is locally weak holder continuous specifically when epsilon is zero right at epsilon equal to zero right so this is the um this This is the result. This is the result applied in the Schrodinger setting. We actually have a more general result that applies to that extended question that I phrased. So it's higher dimensional. The condition is not so elegant on the measure. It's easier to phrase it in this setting. This condition on the measure, what it does. What it does, it ensures a certain spreading property of the measure. Let me try to explain it. So this condition that we say that the measure of the interval, let me put it as a ball is less than. Is less than r to something for every x and r positive. So this implies the following. When you iterate the cos cycle, right, a number of times, and then you multiply with some vector, p and actually we work in the okay, yeah, so p is a vector in s1. S1, the probability that probability in omega, right, that this will get stuck near some other point on the unit sphere. This decays. It's something like this. This is what happens. So as you iterate your system, You get a spreading of the measure. And in the Schrodinger thing, this happens after two iterations. Actually, it happens after one or two, depending on the point P that you start with. And then if it happens, then it stays like this. This property continues to hold. And this happens for every theta, every P, and every Q. So you start. So you start, it's very important for us to start our process with any point. And then you start with any point, you iterate for a while, then you are not going to get stuck near any other point, which this allows us at some point to apply the avantage principle. So you are not getting stuck near any other point for a long time. Time with the with the or or this could happen but with very low probability. So this is actually the condition that we need and this this this condition here it's implies immediately in the Schrodinger setting, it implies the spreading property of the measure. Okay, so let me just stand by saying that well to to to to to to to prove this result we threw at it everything we we had everything new we knew because it um it was not um okay if even even understanding the the behavior of uh of the mixed systems the problem when epsilon goes to zero is that you are trying to uh to join two comp two completely different types of dynamical systems Completely different types of dynamical systems. One, which is the mixed system, which, if you iterate it enough, it has traces of hyperbolicity, right? And depending on how the number of iterations depends on the amount of noise epsilon. And when epsilon is zero, you have the quasi-periodic one, which has none, no hyperbolicity. So joining the So joining these two, let me say it better. Our goal, our objective was to prove, in order, to prove this continuity, was to prove uniform large deviations, right? And then applying the abstract continuity result, uniform large deviations imply Perlberg continuity or something like Terbler continuity. So we had to use, to prove uniform. To use to prove uniform large deviation, a statement that holds for at the same time with the same constants, with the same number of iterations for the unperturbed quasi-periodic system and for perturbed mixed system. So then the method is, and this is related to the question that Mira asked, what is the role of the random? Of the randomness or of the size of the randomness. Well, you have to iterate your system a certain number of times for you to feel this randomness, for it to have an effect. So if you, on the other hand, if epsilon is extremely small, then your system is for a relatively small number of iterates, your system is more like the quantum. Your system is more like the quasi-periodic one. So, what you really need to understand to prove this uniform Large deviations is you have this is the time, the number of iterations, right? So if you starting with one, then if you up you give an epsilon up to a certain scale, the system, A epsilon. The system a epsilon behaves more like a zero or the iterates up to a certain scale. After a certain larger scale, n1, a epsilon feels, the iterates of epsilon feel the randomness. Now the question is to relate, is to describe how much you have to iterate in order to feel this randomness. And then it turns out that you And then it turns out that you start feeling this randomness, whatever that means, after a scale which is of order log of one over epsilon to some big power. And then this scale n0, where your system is more like the quasi-periodic one, is of order log of one epsilon. And it's extremely important to prove that you feel the randomness after something that is over the. After something that is over the order log of one over epsilon, if it was polynomial and not logarithmic in one over epsilon, the argument would be dead. But because it's logarithmic, the argument works. So, well, let me yeah, let me stop there then. So, let's add a few to those for next talk. Questions? I have a couple. I have a couple of questions. So, first of all, weekly holder, you mean logarithmic holder, right? No, no, sorry. Weak holder, we mean this like in Goldstein Schlach on the Taurus TB. So it's almost Hilder, except that it's I see. Okay, and then what do you mean by Hilder and Mu? you it's uh um you you put on on the on the set of on the space of measures you need to put a metric and we work with the vasserstein uh or rubinstein kantorovich metric which is uh which which metric met metricizes the the weak start topology so i i i can i can describe the the the the metric if you want it's The metric, if you want. It's X star was not metrizable actually, but no, no, it is by various by various metrics. There is one that is not very useful. I forgot what it's called, but it yeah, it is. All right, and but and you do not But, and you do not get continuity in alpha, do you? In alpha, no, no, no, no, we don't study this. Well, this is a subtle problem, as you know. But yeah, we don't, okay, we don't, yeah, we, for now, we fix alpha. But then there is another question of considering the system where you You take a sequence, an IID sequence of alphas, and then you translate by, at each part, at each step, you translate by another frequency chosen independently of the previous. It's not what you asked. But I know what you mean. It's the continuity in the frequency we don't study. Right. Okay. Okay, well, yeah. Finally, so for localization questions that you mentioned, does the result follow from Gorodetsky Kleptsyn? Actually, that's one I wanted to ask also, or rather say. So basically, it's in their second paper, which hasn't been released yet, but it's plain. So I don't the question would be, I don't know. So yeah, so there's a second paper by Gov. Okay. Okay. Okay, okay. We we had a very good conversation with them, uh, with Anton and Victor in Lisbon during our recent workshop about this. So now we are a bit more aware of their results because we knew nothing about them. It may, it may, they may have proven this. I don't know. No, no, I think they're officially claiming it already. Yeah, so okay. So, I mean, if you look at yeah, sorry. I mean, if you look at yeah, sorry, I just to uh so if you look at the paper that I wrote with Anton end of last year, I don't I don't know if it was November or December, but actually it's explicitly claimed there. I just don't know when the paper will be released, but I think they have the proofs. It's just a matter of finishing the writing. Okay. Yeah, well, it's still this model would not be this is this is another model that that we are. Another model that we are interested in. No, it's for the additive model. So it's basically any bounded background plus IID is localized. Okay. Yes. All right. Okay. And as I understand, it was also claimed a long time ago by Golshade. Is that right? Yes. Okay. But the proofs never. But the proofs never appear in the understanding right? No, there is part of the proof that appeared already. There is a paper on our guide in which they're part of these results. Well, I think that even when they appear and when everything appears, it would still be interesting to see other proofs. Recently, the classical issue of The classical issue of the Anderson model was studied by various teams with various approaches. At least when it comes, I'm sorry, I'm just answering Ivana's question. At least when it comes to the results of Glutton and Gorodevsky, the parallel results of Ilya, I think they already are not the unpublished results of Gorodsky content, but what they already published. But what they already published about the product of different independent matrices. Yeah. All right. Yeah. No, certainly this large deviation-based approach could also lead to dynamical localization. Usually, this proof is doing almost immediately. Antoine says they also have dynamical localization. Oh, I see. I see. Okay. I see. Okay. Well, uh well, presumably not strong, right? Well, it would have been nice if he had given a talk, but he says he couldn't. Yes, but so well, there is this purely random result, right? It wouldn't fit into this conference. No, but it applies to this model, I mean, the additive model. Well, yeah, right. But so, presumably, within their approach. So, presumably, within their approach, it's not so easy to average, right? Well, for a large deviation approach, it allows to average just by default. So, I wouldn't expect that Gorodetsky collapsing gives strong dynamical localization while this approach should lead to it, for example. But anyway, okay, thanks. I I have one more question. Sylves, can you put the slide with your theory? This slide with your theory, um, which the last one, the last one, yeah, that one. So, I have a question. So, so here you basically say to me that your assumptions are that without the noise, you are in localization setting. So, your level of tone is positive. Does this apply to this? So, somehow I want to erase the direct analog. To erase the Darfi-Town condition, so you will see where I'm going to. Will it be so? Can you apply your theorem in the setting where the Lambda response is positive, however, you have singularity in this particular we need the arithmetic condition and we need it, we don't need And we need it, we don't need it just so we claim that we have large deviations. It's not just there. I cannot replace this list of assumptions with consider a system that satisfies large deviations when epsilon is zero. This is not enough. No, I cannot. I need this. And this makes an appearance in a subtle way in the argument. So, how does alpha is the Google? Well, if alpha is if alpha is, well, you know.