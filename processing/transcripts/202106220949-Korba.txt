 Okay, so let's start. So, our second talk is by Anna Kropel, and she will tell us about Wassenstein Prosimal Gradient. Ena, please. Hello. So, firstly, thanks for the invitation. I will talk about a recent paper called the Research Training Proximal Gradient, that is a joint work with Adil Salim from Coast and Julia Rusev from University College London. And it's not exactly about And it's not exactly about entropic regularization of optimal transport, but we consider a closely related problem. So I hope you will find it interesting. And yeah, and do not hesitate to interrupt me with questions during the talk. So we consider the following problem. Let P2 of Rd be the set of probability distributions over Rd with finite second moment. a moment and let v a function from rd to r and h a functional defined on a p2 of r d we consider the following minimization problem of a functional g on a p2 of r d that is the sum of a potential energy e v that is an integral of the function v with respect to mu and the functional h and this problem is motivated by two Motivated by two applications in machine learning. The first one is the application of the task of sampling from a target distribution pi. So in this case, the functionality that could be considered is the callback Heybler divergence. So this point of view has been considered in many papers recently. And another example motivating this problem is the optimization of over-parametrized. The optimization of over-parametrized shallow neural networks. In this case, one could take g to be the MMD between mu and pi. So the MMD that Gabriel mentioned in the previous talk, but I will recall what is this functional. I mean, this case where pi is the optimal distribution over parameters of the network. So I will give more background on these applications, the motivation, but yes, this is a problem you consider. And the contributions of our paper are the following. So, the previous problem is a free energy minimization for which Weisserstein gradient flows are well understood continuous time minimization dynamics that were introduced in this seminal book by Ambrosio, Gilly and Savari. And various time discretizations have been considered in the literature, either to approximate this flow or as optimization algorithms on the space of probability distributions. Space of probability solutions, such as the famous JKO from the name of the authors of this paper, Jordan Sindalero Oto, that I will recall as well. And here we will propose a forward-backward discretization scheme that can tackle the case of functional G as before, which is a sum of a smooth and a non-smooth term. And we will show that this scheme has convergence grammar. This scheme has convergence guarantees similar to the analog scheme in Euclidean spaces under mild assumptions on V and H. So I will first recall some background on Vassar chain gradient flows. So we will work on the Vasar chain space. So I denoted P2 of Rd is a space of probability measures with a finite second moment, which is endowed with the Vassar chain 2 distance. Vaser chain 2 distance that you all know here. And I need to recall some definitions as well, such as the notion of a push-forward measure. So if I take a measure mu in P2 of Rd and a measurable map T from R D to R D, the push-forward measure denoted as follows is characterized by the following. So if you have a measurable set B, T push forward mu of B is mu of T minus one of the image measure. minus one of the image measure or you can see this as if you have a point x distributed as mu t of x is distributed as t push form and in our work we were really using this famous theorem by Jan Brignet that says that for two distributions in P2 such that sorry such that the first one has a The first one has a has a continuous uh has a sorry has a density with respect to the Lebesgue measure. Then there exists an optimal and unique transport map pushing mu onto mu and a convex function g such that the this optimal transport map is the gradient of this convex function mu epsilon everywhere. Also, this optimal transport map attains the Wasserstein to distance between mu and mu, which is the L2 norm between identity and this optimal transport. norm between identity and this optimal transport map and if the second distribution if the second measure has also a density with respect to the lebesger then you can consider the reverse optimal transport map to recover identity so this would be a result that we use a lot in our proof and it's also particularly convenient if you want to write explicitly by certain two geodesics for instance if mu For instance, if mu verifies this condition, the geodesic between mu and mu writes as follows. It's the average map between identity and the optimal transport map from mu to mu, push for new. So it's a vertical, sorry, horizontal displacement, like on the right, instead of vertical displacement, such as in a mixture. And so for research and gradient. So for research and gradient flows, I need to introduce the notion of continuity equations because we'll see that they take the form of such equations. And continuity equations are defined this way. So if you fix a horizon of time t and you consider a family of distributions indexed by time, this family is set to satisfy a continuity equation if you have a sequence of vector field vt such that at each time vt belongs to a L2 of mu t, which is L2 of mu t, which is the set of the space of squared integrable functions with respect to mu t. And this equation in orange holds in the sense of distributions. It says that the derivative of mu t respect to time plus the divergence of mu t v t times vt is equal to zero. So it holds if you if you in the sense of distribution. So if you integrate this with a respect with um if you integrate if you If you integrate smooth functions with compact support. And this equation describes the evolution of the density of particles XT and R when these particles are driven by a vector field Vt. So at each time they satisfy this equation. And there is a Riemannian interpretation of such continuity equations in the sense that this vector field can be seen to This vector field can be seen to roughly belong to the tangent space of P2 Rd at mu t, because this tangent space is included in L2 of mu t. And the surface chain gradient flows that were introduced in this book actually take the form of continuity equations. Because so if you consider a functional G, as I introduced in the first slide, the one that we want to minimize, that is a defined on p2 of Rg and that is a Defined on p2 of Rd, and that is regular enough. If you consider the differential of this functional, evaluity that mu, it's a function from Rd to R defined as follows. So for you take two distributions in P2, such that their difference is also in P2. And if you look at the limits of when you perturb G by this measure, then you recover the Then you recover the differential here integrated with respect to this difference. And a family of distribution index by time is said to satisfy a Vassarstein gradient flow of G if it satisfies this continuity equation where the vector field is the Wasserstein gradient of G. And the Vassarstein gradient of G is, in this case, for mu and G regular enough. U and G regular enough is the gradient of the differential of V. So you have a functional G, its differential is a function, and you can take its gradient and you recover the Vassarian gradient of V. So that holds for when, for instance, the densities are C1. And this function belongs to L2F. And in particular, if the functionality that we consider Functionality that we consider is a free energy. It's a Vasser-Shank gradient flow has a form that is quite classical. And what is a free energy? It's the sum of three different energies. An internal energy, that is a function evaluated at mu, integrated, a potential energy, which is the integral with respect to mu of some function v, and an interaction energy, that is the integral of a function w. The integral of a function w integrated with spectrum. In this case, the Wasserstein gradient flow writes as follows: where you have that is mu t gradient of H prime of mu t plus V plus W converted with mu t. So in this paper, we consider the case where the function functional to be minimized g is the sum of a potential energy E V of mu plus some plus some functional that could be actually an internal energy or interaction energy and we study an unbiased algorithm or said in another way a specific time discretization of the vasarshan gradient flow of d to minimize this functional so i will now talk about two main motivations for this problem that i mentioned in the beginning In the beginning. And for this, I need to introduce. So, I said there were two main applications in machine learning: the one of sampling, where G will be the Kullback-Leibler divergence, and the one for optimizing neural nets, where G will be the MMD. So, what is the Kullback-Leibler divergence first? So, if you take two distributions, mu and pi and P2, the Kullback-Steibler divergence of mu with respect to pi is defined by is defined by this integral of the logarithm of the Radon Ecodine density of mu with respect to pi dÎ¼ if mu is absolutely continuous with respect to pi and is plus infinity otherwise. And if you fix pi, the target distribution, you can consider the functional that computes the Kullback-Liber divergence of mu with respect to pi. And this functional has functional has a differential that is a non-inclusive form. It's the function that is the log of mu over pi plus one and hence its gradient, its Vassertian gradient is the gradient of this differential, I'll recall, and it's simply gradient of log mu over pi. And why is it useful in sampling? So the motivation of sampling in machine learning is for instance. In machine learning, is for instance in Bayesian statistics, where you have a data set of observations, WIYI. For instance, in a regression setting, you want to regress Y over W. You assume an underlying model parametrized by some theta in Rd. For instance, your P of Y given W and theta is Gaussian. You can compute the likelihood of your data set given theta. And furthermore, you assume some prior distribution P. Some prior distribution p on the parameters. Then, by this rule, give you the formula for the posterior distribution of the parameter, which is p of theta given d, which will be the target distribution here, pi. And this posterior is proportional to the likelihood times the prior. And this target distribution that is needed for Bayesian inference is actually only known up to Is actually only known up to a constant because this normalization constant is untractable. So we know the density of pi up to a constant. And it's hence a big question in this literature to be able to sample from such a distribution. There is a vast literature on MCMC methods that I won't detail here, but basically they construct Markov chains whose law converge to pi. And there is also this also this point of view of sampling from pi and approximating this distribution by optimizing the Kuhlbike-Lebler divergence. Because you can, it seems trivial, but pi minimizes the Kullback-Leibler divergence of mu relative to pi, assuming that pi is in P2. And we've seen in the slide before that the Vassar ingredient of the Kuhlbike-Leibler divergence requires two in requires to compute it's here gradient log pi and if you know pi up to a constant you know gradient log pi which is called the score function so this is one main motivation and many methods are are now considering this point of view including our algorithm a second motivation that i mentioned in the beginning is the optimization of a neural network that could be seen as optimizing the mmd so MMD. So I will recall what is MMD. If you consider a kernel K defined on Rd times Rd, positive semi-definite, it's a function that compares two points in Rd ZZ prime as the scalar product between the representation of Z and the representation of Z prime, where the representation is a map from R D to a Hilbert space. So if you have such a kernel and that the Such a kernel, and that the application that maps a distribution Î¼ to the integral of the kernel with respect to this distribution is injective, then the MMD defines a distance on P2. And here I consider the 1 over 2 MLD squared because it's more convenient for my notations, but this functional writes as three terms. First one is the integral of the kernel with respect to mu two times. The second one with respect to mu. The second one with respect to pi two times, and the last one is a cross term between mu and pi. So, similarly, if we look at the differential of this functional, when we fix pi, the target distribution, evaluated at mu, this differential is the function that is the difference between what is called the mean embedding, so the integral of the kernel with respect to mu and the integral of the kernel with respect to pi. And for k irregular enough, the vasertion gradient of the enough, the Vasarchein gradient of this functional is simply this time the difference of the integral of the gradient of the kernel. And why does it appear in machine learning and in optimizing neural nets? So I will try to illustrate with this slide. So we again consider a regression setting where we have a point here named xy. We want to regress y over x that are distributed according to Are distributed according to distribution that is called here data. And assume you want to solve this regression problem by using a one-hidden layer neural network. So you can represent this as follows, where the input X here is four-dimensional. You have capital N neurons in the hidden layer, and the output of the neural net is denoted y-hat. Here in this setting, with one hidden layer, the output of this the output of this neural net can be well hat can be written as the average of the outputs of each neuron where the output of each neuron i is phi the i of x for an input x and so if you want to find the best parameters you can minimize the squared error so you the expectation of a this distribution data of the true output y minus the output of the neural net and you're looking for the best and you are looking for the best capital N parameter Z over R D. Now in a theoretical setting you can consider the case where the number of hidden neurons go to infinity and in this case this output of this neural net writes as an expectation of the output of each neuron phi z of x. The neuron is parametrized by z, where z follows a distribution u. So now you are not looking for the best capital n parameters of Rz. best capital n parameters of Rd, but for the best distribution nu on P2 of Rd, so the best distribution over the parameters. And in the well-specified case, which is written in this assumption, this loss, this loss on the space of distribution corresponds to the MMD. Because if you assume that there exists some pi over, so an optimal distribution pi. an optimal distribution pi over the parameters of the network such that you can write the conditional expectation of y given x as the output of your neural net when the parameters are distributed as pi then the loss that we could see on the previous slide the one that we're minimizing were minimizing the squared error by developing the squared the squares and rearranging expectations it writes as an MMD so here we can recognize So here we can recognize we are minimizing the sum of three terms involving two different distributions, the optimal one and the one that we are looking for. And here it's with a specific kernel that actually depends on the data distribution and the activation function. But we are minimizing the name. And furthermore, so these two dissimilarity functionals, the backlibre divergence and the MMD, are And the MMZ are highly considered in machine learning currently because of these two applications. And one thing that should be noticed is that they write as free energies. So that I've presented before. Indeed, if you look at what is the scal between u and pi, you can write this as a sum of an internal energy and a potential energy where, so sorry. Where, so sorry, it's not written here, but if pi is proportional to x minus v, here it corresponds to minus log pi d mu of x. And here it's the this term corresponds to integral of log mu d mu. So, and you can notice as well that this is a free energy without interaction energy. In contrast, the maximum indiscrepancy also writes as a free energy, where you have a potential energy Where you have a potential energy which is this one, where the function v here is the integral of the kernel with respect to the target distribution pi, and you have an interaction energy also depending on the kernel, but here there is no internal energy h. All right, so I'd like now to talk about the specific case of the relative entropy. That will be the last motivation before introducing our results. Our results. So here I focus on the case of the Kullback-Heber divergence. So we've seen that it can be written as a composite functional where you have a potential energy, EV of mu, and the negative entropy, where pi is proportional to x pi in the sp so it comes from this term here. And the Wasserstein gradient flow of the Kubank-Cyborg divergence. Of the Kuhlbeck-Cyber divergence. So we've seen it can be written as this way, where the time derivative of Î¼t is equal to the divergence of Î¼t times the Vasser-Thang gradient of the Kubek Labeler. Here it's for the Kubek Label divergence is gradient log Î¼t of the pi. And if you develop this term as a gradient log Î¼ t minus gradient log pi, the first term will give you Laplacian of mu t because you will simplify with mu t and you have a divergent. simplify with mu t and you have a divergence of a gradient of mu t. And the second one, mu t gradient log pi minus gradient log pi gives you this term. So it's a famous equation associated to the Langevin diffusion process in Rd that is defined as follows. So if you have a particle evolving as this way, where dxt is minus greater than dxt, so it follows the potential plus a one The potential plus a Bronian motion, then its distribution satisfies this Fokker-Planck equation. And if we focus on the second part of the Fokker-Planck equation, which is just the heat equation, so it's the gradient flow of only the negative entropy part in the Kuhlback-Leibler divergence. So this is the heat equation whose solution is known exactly if you start from an initial distribution zero. initial distribution mu zero. At each time t, mu t, that is, mu zero convolved with a Gaussian scale by 2t verifies this equation. And in space you can simulate this by in R D by just adding a Gaussian noise to your particle at each time. So Z here is just a standard Gaussian noise. And the famous discretization of the The Fucher-Planck equation that we've seen before that is so if you pick a discretization, you can also see this as choosing an optimization algorithm or a sampling algorithm. One of the maybe the most famous discretization is the one in the unadjusted Langeman algorithm. Here, at each iteration n, the particle at time n plus one is the particle at its previous position. Is the particle at its previous position displaced by minus gamma step size greater than V, so the potential related to pi, plus a Gaussian noise. This discretization is biased. And if you run this and for a fixed step size gamma, in the end, the distribution of, let's say, x infinity will be pi, that I index here by gamma, that will be different from pi. So it does not converge to the right target distribution. not converge to the right target distribution and the reason for this is related to what i've explained before is that you can write this this step you can decompose it in two steps where you introduce an intermediate point yn plus one so you do a gradient step on the potential v and in the second step you you add the gaussian noise and this first step it's a it's a it's a gradient distance step or it's a forward discretization of the gradient Discretization of the gradient flow. So there is a discretization error. While in the second step, we've seen that adding a Gaussian noise exactly solves the heat flow. So there is no time discretization error. So this equation, the Euler algorithm, can be qualified as a forward flow discretization. And this is the reason why it's biased. In the space of measures, you can write this first step or the This first step in RD translates as follows. So you do a gradient descent step for the potential energy by pushing mu n by identity minus gamma gradient v. And for the second step, where you add Gaussian noise, you convolve the intermediate distribution by a Gaussian. So this discretization is known to be biased. It's very well explained in this paper by AndrÃ© de Pizzolot. And if you want to derive non- To derive a non-biased algorithm by discretizing in time of a certain gradient flow, at least in the case of a composite functional, you have three solutions, three possibilities. Either you consider the forward discretization that use the gradient of the functional G of interest, so here the callback labeler directly. So you can consider either this discrete. You could consider either this discretization, which is called the forward discretization, which is the analog of gradient descent in RD, or you could consider the backward discretization, where at each step you compute what is called the JKO, which is the analog of the proximity operator in Rz. So, what does the JKO evaluated at mu n of the callback labeler? It looks at each step for the distribution mu that minimizes. Distribution mu that minimizes the functional that we want to minimize plus a penalty in terms of the vast entry distance between the current distribution and the previous step. So either you choose a complete forward or a complete backward discretization, or you can use a forward-backward discretization. And this forward-backward discretization, so it takes a gradient step on the first term, and it takes a JKO step on the second term. takes a JKO step on the second term in the composite functional. And this discretization is unbiased in contrast to the unadjusted Langeman algorithm discretization that we've seen, which was a forward flow discretization. This one is unbiased because the forward and backward operations are adjoint, so the minimizer is concerned. So all these discretizations or algorithms that are derived from here will be unbiased. Will be unbiased. So, this motivated us to study the Vassar-Chain, what we call the Vassar-Chain proximal gradient algorithm, which is actually the forward-backward discretization. So, this scheme was introduced in Andrew Bizono's paper, but it was not studied as an optimization algorithm. I see that I have a question. Why is the forward unbiased? So, so it's why is the forward unbiased bias? Yeah, so why is the forward discretization unbiased? So here yes, so it's unbiased because for instance, if you look at the proof of gradient descent like in RD for either a convex function or a convex function, you can prove that the function that x minus x star will converge to zero. So you attain where x star is a minimizer of short. Where x star is a minimizer of your function of interest. Sorry, something I don't understand. So the JKO scheme is just the backward Euler iteration which is done in the Euclidean spaces. And we know that the forward Euler iteration, for example, does not converge or it may not converge. But maybe your forward expression is something different. Right? So even in the Euclidean spaces, one can have backward Euler iteration or forward Euler iteration. And the backward. Iteration and the backward Euler iteration is a JKO scheme, which is known to converge in the Euclidean case. In the Euclidean case, I know that the forward one may not converge. In fact, it can behave pretty badly. So I'm not entirely, maybe I just don't see why the forward one here should converge or should be unbiased. Maybe I'm missing. At least in the convex case, you can prove that it's converging. So it has to do with convexity. So it has to do with convexity of this function. So it's better explained in Debizono's paper. There is the appendix on this. I don't remember every detail on this, but at least I have in mind the proof of a gradient descent for a convex function where it works. So I'm I wouldn't, yeah, good question. I wouldn't. May I say something? So yes, just to react. Yes, just to react, there are two things. First question is if the algorithm is converging, and second question is what is a stationary point? Okay, these are two different questions. If we take, for instance, in the Euclidean space, the gradient set, there are conditions under which the algorithm is converging, and there are conditions under which the limit is a minimizer. If you want the limit, If you want the limit to be a minimizer, typically it will require, for instance, convexity assumption, because in that case, you know that the algorithm will converge to a zero of the gradient, and the zero of the gradient will be a minimizer thanks to the convexity. And if you want the algorithm to converge, if you want it to converge, just to converge to something, it will require other assumptions, say, for instance, smoothness. Other assumptions, say, for instance, smoothness of the objective functions. So here it's the same. If we take, for instance, the forward algorithm, conditions under which it will converge. Actually, I don't know because you will see later that KL is not smooth and so on, so I don't know. But if it converges to something, if it converges to something, well, you can look at if you. you can look at uh if it converges to something typically it will be a fixed point okay so you will you will you would have mu n equal to mu n plus one so okay loosely speaking okay you will have something like mu n equal to mu n plus one so roughly you will have the gradient the vast gradient of kel which is equal to zero and when you look at the vast gradient of kel equal to zero uh you should have something like mu n equal to pi okay right right i see okay so it'd be not something like that So it may not be it will converge to the right thing. I see. Okay. So just to clarify, there are two questions. Is it converging and then is it converging to the right thing? So yes. So just to clarify, so for a fixed learning rate gamma, if it converges, then it converges to the right thing. This is what you mean. At least it converges to the right thing on some conditions of the functional, such as the convexity and smoothness of the function. To converge to the writing, to converge to the minimizer, right? We can discuss it at the end of the talk. All right. Yes. So in terms of fixed points, if you consider a fixed point for this the the this these um two operations with uh conserve with uh Operations with a conserve with a conserve the fixed point, actually, the forward and background. So maybe this is the right way to think about this. And yes, so we wanted to study this algorithm for this reason that was introduced in Andrea Lebison's paper, but was not studied as an optimization algorithm and whose non-asymptotic properties were not studied. So that's what we did. We want to analyze this algorithm that is forward for the potential energy that takes a gradient state. Potential energy that takes a gradient step and backward for the internal energy or any actually could be an interaction energy, but at least a non-smooth, a non-smooth functional. And here we managed to prove convergence results for this algorithm. And the main tools for the proof, so I don't detail the proof here, but in the time for questions, we can discuss about this. Questions we can discuss about this, but the main tools for the proof are to identify optimal transport maps at each step. For instance, here we need smoothness of the potential function G, V, sorry. And in this case, if you assume that V is smooth, defined by where smoothness is quantified by a constant L, and if gamma is small enough, smaller than one over L, this will be the greatest. Over L, this will be the gradient of a convex function, and by Bruni's theorem, we know that this is the optimal transport map between mu n and mu n plus one. So, this is the kind of arguments we use. So, we need the smoothness of V. And also, we will need, for the proof to work, to use geodesic convexity. So, we will need convexity of V to get the rates of convergence, as in a Euclidean optimization, but more importantly, generalized. Importantly, generalize geodesic complexity of the functional H. So, this is what we do here. So, to obtain a descent lemma, for gradient distance in Euclidean space, you need smoothness of the function you optimize, which is basically, which is roughly V here. So, that's why we need also these assumptions of V being L smooth, which I recall here. But additionally, here, we needed this assumption on Here we needed this assumption on generalized geodesic convexity of the functional H. And what is generalized geodesic convexity? If you pick three distributions, mu, mu, and mu star, mu star will be a, sorry, here it should be a mu star that is absolutely continuous with respect to Lebesgue. Mu star will serve as an anchor. Then if you look at the path, this path where the anchor distribution is mu star and you Distribution is mu star, and you push it by the average between the optimal transport map between mu star and mu, and the optimal transport map between mu star and mu, then the functional h should be convex along this path. And yes, under these two assumptions, under a generalized geodesic convexity of h and the function v being L smooth, we could obtain what is called a decent lemma, which is a classical result. Lever, which is a classical result, in at least a cornerstone result as in optimization in Rd, that's that says that the functional we optimize g decreases at each iteration, because here at iteration n plus one, we have g of mu n minus a positive term as soon as the step size is small enough. Right, and to obtain rates of convergence, as in optimization of Rd, we needed convexity. So if you have needed convexity so if you assume furthermore that the potential v is strongly convex lambda strongly convex which is recalled here then we could obtain what is called an evolutional variational equality that is here and that implies rates of convergence so for a step size small enough if you start from a well-derived distribution u0 we could show that the Could show that the so in the convex case when lambda is equal to zero, we could get a one over n convergence rate in terms of the objective function g. And if lambda is strictly greater than zero, we could obtain a linear convergence rate in terms of the Vasser chain distance. So these results were, so this is our main result in the sense that what should be noticeably that it's the same rate as the proximal gradient, which is the Than the proximal gradient, which is the analog of this forward-backward scheme, but in a Euclidean setting. And it's much faster than the unadjusted Langevin algorithm that I've shown before, because EULA gets a one over square root of n instead of one over n for lambda equals zero, and one over n instead of linear range for lambda strictly greater than zero. So I don't know how much time I have yet. Yes, so here are around three more minutes. Yes, around three more minutes. Three more minutes. Okay, perfect. So here will be the main connection between the subject of this conference and our work. So if you fix G to be the Kullback-Lebler divergence, which is one of our main applications in mind, in this setting, the non-smooth functional is the negative entropy. So in the forward-backward scheme, we need to compute the JKO of the negative entropy. GKO of the negative entropy to have an efficient algorithm. So in the literature, some subroutines exist to compute generic GKOs. So you can find it in Santran Beauge's book or Gabriel Perry who was talking before, had a paper in 2015 proposing JKO not with respect to the Vassar Chain 2 distance, but with the entropic regularized Vasarchain 2 distance with algorithms similar to what is done for What is done for Syncor. But yes, but here we wanted to really focus on the JKO of entropy. And you can notice that it's very close from the entropic regularized OT problem because if you write, so JK of entropy is this minimization problem. You want to minimize the entropy for mu in P2 plus the penalization in the search engine distance. So it's a bit thinking in a reverse way by writing by Writing by involving the minimization problem in the Vasertra into distance, in the end we get a minimization problem that looks like the entropic regularized problem, but the entropic penalization is not on the coupling S, but only on its second marginal. So we try to come up with an algorithm tackling directly the GKE of the entropy. Of the entropy by considering a discretization in the spirit of what wasn't for St. Corn, but we didn't manage to do so. So that's an interesting open question for us. But yeah. And in the Gaussian case, fortunately, there exists a closed form for the JKO. So we could try this algorithm in practice. So if you start, if your target is Gaussian and you start from a Gaussian distribution, Start from a Gaussian distribution. Basically, you can. I'm a bit off of time, so I won't detail the equation here, but you can run this algorithm by updating the mean and covariance of your distributions. Let's take Gaussians along the algorithm. And yeah, we tried it in practice. If you have a Gaussian target like Hermu star in orange, it works. And the rates that we... The rates that we found are verified in practice. So, yeah, just to conclude, in terms of contributions, so we proved that this forward-backward scheme is faster in number of iterations compared to EULA. So the rates are better, but at the cost of a higher iteration complexity. Because I recall that EULA is only adding a noise while we are computing a JKU. One thing to be said is that our proof One thing to be said is that our proof works for any functional H, so the non-smooth functional that is convex along generalized geodesics. So we prove, I insisted on the case of an internal energy, but you can think of potential energy or interaction energy that is non-smooth. And yeah, as open questions, we are really very motivated about finding efficient subroutines for this GKO of entropy, because as I explained, it would because as I explained it would able it would it would enable to to to sample from any distribution target distribution pi that is proportional to exponential minus v. So we really think this would be a great finding. And an open question that remains is can we obtain results in the non-convex case like rates of convergence where maybe V is not convex but we have some functional inequalities that we could use. Inequalities that we could use, but I can discuss this further in the questions. So, thank you for listening and happy to take questions. Thanks, Anna, for the talk. Any questions? So, about the callback label divergence, so as soon as V is convex, so let me talk here. So, when V is convex, the Kullback librarian. The Kullback Leibla divergence is geodesically convex. So I don't know if this answers the question, but the results we have, depending on the convexity of V, if it's lambda convex with lambda equals zero or greater than zero, we have a faster rate. So I see that Jonathan has a question. Yeah, thanks for the talk. I missed one thing, which is on the I missed one thing, which is on the when you were talking about implementing the JKO step. Is it the case that you have for the general case rather than the Gaussian, is it the case that you have an algorithm that works in practice, but you just can't prove convergence? Or is it really unclear how to do this even empirically? So the algorithm is this one. So this step is made by pushing particles by identity minus gradient. By identity minus gradient v. So, this corresponds to this step here, basically. So, the gradient step is easy and the JKO step is hard. What I was saying is that the JKO of the negative entropy, we don't know how to compute it right now in the generic case when you start from when the previous iterate is not a Gaussian distribution, for instance. For instance, in the case where the target and all the iterates are Gaussian, which is the case as soon as the initial distribution is Gaussian, then there is a closed form for the JKU. You can write the update at each iteration. So I think you mentioned, can I follow up with a question? So you mentioned some numerical algorithms for the JKO. So do they work in, let's say, high dimensions? So we didn't try these generic approaches. try these generic approaches um so here uh so there are a few i can i can mention some of these uh so among uh known routines for jko there is apparently the one from a quantum merigo that uses the techniques from a semi-discrete optimal transport this one i i didn't try uh but this one uh based on uh on uh so it's a gabriel pere's paper i think it's entropic approximation of the Sauchian gradient flow or something like this. Versus an gradient flow, something like this. Here it's relying on algorithms similar to SINCORN. So the same thing, the complexity will depend on the parameter. So this would behave similarly to entropic rigorous transport solving. Are there other questions, Schmeik? Shmik, uh, Leonard. I sorry, I have a comment. I just want to advertise this beautiful but somewhat less well-known paper by Adams, uh, dear Pelletier and Zimmer, who actually rigorously established this connection of entropy regularization and the GQ. And what they prove is that this entropy regularization, essentially, you know, for small temperatures, actually converges to the GQ algorithm. So maybe you're familiar with it, maybe you're not, just it's an absolutely beautiful paper. It's an absolutely beautiful paper. We should probably take a look at that. The authors, I'll put it in the chat. The authors are. And you're saying, what do they show exactly? Sorry? So what they show is that if you do, so let me recall. So for small temperatures, if you do the entropy regularization in step by step, then it converges to, as the temperature goes to zero, it converges to the JQO iteration. Converges to the JKO iterations, it converges to the gradient flow. Yeah, you mean the JKO, to the true JKO with respect to the true position distance? Exactly, yes. So they give the approximation of entropy regularization to the gradient flow. Yes, yes, and there's also a paper by Guillaume Carrier on this subject as well, on the considering PKU with respect to the cable convergence. Yes. cable convergence uh yes but purely theoretical it's it's it's uh it's uh at least it's it's uh it's motivating the use of gko with respect to entropic regularized optimal transport because if you yeah if you use a small temperature you will it will be close to the true gko which is what you want to do but uh but yeah i think this is right anyway it's uh i love that paper because it gives a very physical interpretation of what the jko is doing anyway JKO is doing. I don't know. And I have just another small question. So, in your generalized geodesic, can you interpret it in terms of the original W-2 geometry? Can I interpret it in terms of the... So here, it's not. These are not actual geodesics in the WH2. It's not an actual geodesic. The actual geodesic could be a mixture between. The actual geodesic could be a mixture between identities, the push forward between new and new. Here, this is a path from new to new, which is not the visor chain to geodesic between new and new. It's another path that involves the optimal transport map from with this NCOR distribution. But it's yeah, there are illustrations in the in Ambrosio and Jeanie Savarez's book. In Ambrosio and Julie Savari's book, well, you can, yeah, maybe the drawing is easier to spot, but it's a kind of triangular path you use to interpolate between these two. But it's not the research into geodesic convexity along a different path. Yes, yes. Okay, maybe let's thank Anna again. Thank you. So the next talk is by Jonathan. By Jonathan. And so the next two talks will not be recorded. So please stay tuned.