Thank you very much. Yeah, first, I want to thank the organizers, especially Fung for the kind invitation. It's my pleasure to give a talk here. I was actually in Oaxaca in 2018. I love the city very much and would love to be there again. But I have just moved to Sydney. This is the first week for me to work here. So I have to give. So, I have to give the talk via Zoom. So, I want to talk about the approximation theory of structured deep neural networks. And here is the outline. I will review the classical theory of neural networks and then turn to the recently interested deep neural net reload neural networks and then turn to some review of deep learning with structured deep neural networks. works and then we'll describe you mathematical theory of deep CANs and show why CANs are better for some purposes. So let me first review the classical theory of neural networks. Most of the time people consider first shallow neural networks, meaning that for an input vector X in R D, usually the dimension D is large. The dimension D is large. And then you take an inner product of a connection vector w with x, which will give you a dot product w dot x, and they adjust it by a bias constant b. This will give you the linear form in one hidden neural, which will be activated by an activation function sigma. Advation function sigma. And if you have n such hidden neurons, linear combination will give the output of a shallow neural network. So this is what we have as an output function fn. N stands for the number of hidden neurons. So the hypothesis space consists of all such output functions and the parameters in this hypothesis. The parameters in this hypothesis space are the connection vectors Wi in R D, bias constants BI, and the coefficient Ci. So the challenge in dealing with big data, meaning that the dimension here D is extremely large, is that when the dimension D is large, the number of free parameters to be trained in the hypothesis space is N times D plus 2. Space is n times d plus 2. So if d is extremely large, then this number of parameters to be trained is also large, which is very difficult for real implementation. So the classical theory says that if sigma is not a polynomial, then for any compact subset omega bar d and any continuous function f on omega, the function f can be approximated by an output. By an output function from the hypothesis base with n hidden neurons as accurately as you want when the number of hidden neurons is large enough. So the classical literature on this have given a very good study of this topic, in particular, rates of convergence. Particular rates of convergence, I mentioned a result, a typical result of Lushikish mascara says that if sigma is approximately u to the L where U is large for L not equal to one. Okay, so this is different from RiLU. And also at some point, B, the derivatives of sigma at B is not equal to zero for all orders of derivatives. Then the resolution. Derivatives, then the rates of convergence is n to the minus r over d. r is the smoothness of the approximated function, d is the dimension. Notice that if d is very large, this rates can be bad. So we are interested in real activation function, meaning that first constant on the negative side and then linear on the positive side. The advantage is that it's very easy to implement. And also, And also, if we use sigmoidal type classical activation function, we will have the problem of gradient vanishing when we use stochastic gradient descent algorithms in training parameters. So, with this wheel, then that difficulty can be overcome, and practically it is very efficient. But the main problem is the real activation function. The main problem is the real activation function is not C infinity. It does not satisfy the asymptotic condition, which is asymptotically u to the L when L is one. So because of this, recently they have been theory talking about approximating functions by VLU neural networks, even with many, many layers. So equation one says that if we define a neural network, That if we define a neural network in this way, where TJ is a fully connection matrix, and then you iterate, that will give you the Lilo neural network with sorry. Let me just. Just delete. Okay. I don't know how to get rid of this. Sorry about this. Can we just okay inside? Oh, actually. Sorry about this. Suddenly this thing is a new iPad I just got here. Let's try. Okay, let's try. Okay. So I don't know how to get rid of this. Sorry about this technical question. I'm sorry, it's a new iPad. What I can do is I switch to my notebook computer and what if you just press continue again, continue again, it does not work. It does not work. Yeah, I think I will use the other notebook computer. Okay. Okay, just end up this. Yeah.  Can you see the screen? Yes, we can. Okay, sorry. Sorry about this technical Difficulty, yeah, I've just arrived here in Sydney, so it's a new environment. Um, and they ordered a new iPad for me. I haven't practiced, okay. So, let me use full screen. Okay. Okay, so I think it should work now. So, this is the iteration relation for a real neural network with J hidden layers for the number of layers J to go from one to capital J. And TJ is a full connection matrix. It's DJ by DJ minus one, and Bj is a bias vector. So, theorem one says that says that if the function is from the soft space hurdle space wi infinity on dimension d cube then for any accuracy epsilon we can find a real neural network of depths log one over epsilon and parameter numbers epsilon minus d over r log one over epsilon to achieve the accuracy epsilon that's uh the main result in the paper of yerowski now Now, then after this paper, a lot of work has been done. I didn't mention here, I just list some names. Now, the third part I want to review a little bit about deep learning. And we know that because of big data appearing, deep learning has been very successful for speech recognition, for computer vision, and the natural language processing, especially the last. Language process, especially the last part is less complete. The first two parts are relatively complete. So, but compared with the practical success of the mathematical theory is way behind. That's what we want to develop. So, as I said, this is the fully connected neural network. See, yeah, should be correct. This is the fully connected neural network. The fully connected neural network, and a large literature in deep learning studies is about the stochastic blades and descent algorithm, meaning that how do we train all these parameters in a four connection metrics, the bias, the coefficients. So there are three kinds of parameters, T, B, C, for minimizing this empirical least squares error. And people have shown how the How the SGD algorithm converges locally or globally, that's a large literature. So, in practice of deep learning for dealing with big data, actually neural networks often have structures, and I will explain to you why. And the architectures often used are CNNs, convolutional neural networks, and RNAs, recurrent neural networks. Recurrent neural networks and more combining together you have gas and some other more complicated structures. So, why CNN is so important? Let me just demonstrate with this AlexNet. So there is a competition called ImageNet Large Scale Visual Recognition Challenge. And in 2012, Deep learning algorithms actually won the prize. And in the following years, people always use deep learning algorithms to win the competition. So the AlexNet, the structure is given here. So the input image is a color image with three colors. So here you can see three because we have three colors, and each color is a matrix. It's a 200. Is a matrix. It's a 224 by 224 matrix. So it's a large matrix and there are three. So that's the input of a color image. And for this input color image, people have used small matrices, 11 by 11, to get local features by inner plot, okay, by dot plot. So dot plot, you get one linear. Input and then activated by reload, you get one number, which and then you use a stride four, meaning that you don't need to take all the samples, you just down sample by four. So the original 224 by 224 matrix becomes 50 by 50 matrix, but we use 48 matrices. Okay, so then you do the same with a five. Then you do the same with a 5 by 5 matrix. You get one number, but the size is reduced by 2. So it becomes 27 by 27 matrix and number of channels is 128. And so. So at the end, you get 13 by 13 matrices, altogether 128 matrices. And finally, reduced to a vector of length 2048. 2048. So, the main advantage of deep learning of deep CNS in this process, the original input dimension is 224 by 224 by 3, which is about 150,000. But at the end, the dimension is reduced to about 2,000. So that's a large reduction. The second is because of using many, many channels, the main features of the original natural image can be kept. Natural image can be kept. And that's why CNNs are very efficient because it can extract the main features of the original image. On the other hand, the size has been reduced from 150,000 to only 2,000. So with 2,000, then it's possible that if we use GPUs, we can train the parameters efficiently. So that's the main advantage of CNS. Of CNNs, reduce the dimension, and keeping the extracting the main features. Now, turn to the mathematical theory of deep CNS. As you can see that CNNs are the most important and the first family of structured neural networks used in deep learning, especially for speech recognition and for computer vision. Now, what I'm interested in is one-dimensional convolutional CNA. Convolutional CNNs, which are mainly for speech recognition, for dealing with signals. So the input is a sequence, k from minus infinity to plus infinity. Most of the time, it's finitely supported. Say, if the length of the vector is d, then x is a sequence with index from 1 to d. And suppose we have a filter, w. Which usually is a sequence supported on a very small interval, say s equals to 2, then only supported on 0, 1, 2, meaning that although it's a sequence on z, actually only three entrants are non-zero. So it's a short supported sequence. X is relatively largely supported sequence. And then the convolution is given by this form. Everyone knows about this. No one knows about this. If we express this in terms of explicit summation, you will see a coefficient matrix appeared here in terms of the input vector x. So if you list x1, x2, into xd, then the input-output relation between the input and the output-convoluted sequence is given in this way. So, what's important is this matrix is given by these coefficients. It's a top-list type matrix. So you can see that W0 is here, W1 is here, Ws is here. So although the size of the matrix is D plus S by D, it's a very large matrix, but there are only S plus one unknown parameters, which is W0, W1, until WS. So that's the large reduction in neural networks, because if we use fully connected Because if we use fully connected neural networks, the size of this matrix D plus S by D, so there are altogether D plus S by D parameters to be trained. But here we have only S plus one parameters to be trained. So because of this, the number of parameters has been largely reduced, and we are able to use many, many layers for the neural network. For the neural network. So when I was asked to write a review article for EEEE Society on CNNs. It's an encloped article. I reviewed many literatures, found that in 1989 and 1990, Hinton and some other people have already introduced Some of the people have already introduced these types of CNNs. So, the parameters coming from the filters, as I said, if you have J layers, then all together you have J sequences. Each sequence consists of S plus one parameters. And that's the neural network iteration relation. Starting from the input vector, H0 is just X, which is the input vector. X, which is the input vector in Rd. And then once this is X, you'll get H1, and then you'll get H2, and so on. So, what's difference here is this TJ is a convolutional matrix, which is a topless matrix having very few training parameters. And that's why we can allow J to be large, because the number of the reduction of the number of The reduction of the number of parameters. But the hypothesis base is the same as before. You take linear combination of the last layer. And when we understand this hypothesis base mathematically, we found that because of the parameters, W, B, and C, so all these parameters are flexible, meaning that for each fixed WB, this is a finite. A finite dimensional space. But when WB changes, actually, there are many, many finite dimensional spaces. And the problem is the union of these subspaces, these spaces. So because of the union, the structure is actually very difficult mathematically to be analyzed. So computational advantage is that the number of parameters has been largely reduced. Reduced. It's 3s times j minus 1 plus 2d plus 2j. J is the number of hidden layers. And if s is 2, then j can be as large as d over 6, for example, okay, which can be very large. But the challenge, as I said, is the hypothesis space is the union of many, many function spaces. Therefore, people often complain that deep learning That deep learning lacks robustness and explainability. Okay. And that actually also gives the difficulty in analyzing these deep learning algorithms. So the first theorem I obtained for CNA is this one, it's universality. Says that if the filter length is between 2 and D, usually we take simply S to be 2, then for any compact sub. Therefore, any compact subset omega var D and any continuous function f on omega, there exists a sequence W of filters, bias vector sequence B, and an output function from the hypothesis base with this fixed W and B. As I said, these need to be trained. Okay, then the universality says that we can always approximate the function f well, as well as what. Well, as you want. That's the paper published in our chart two years ago. And the key observation is that if you have a sequence with large support from 0, 1 until M, M is extremely large, they actually can find short supported sequences, filters, such that this capital W can be written as. Can be written as the convolution of these shortly supported filters. That's the key idea. And we actually prove the statistical consistency for the CNS in this follow-up paper. Rates of convergence is suppose I assume that F is from the sobriety space HR with R to be greater than Which r to be greater than half d plus 2 for technical reasons. Then the function can be approximated by an output function from the hypothesis base at the rate of 1 over square root of j, where j is the number of hidden layers. So we can see that if you have, if you use enough number of layers, actually the function can be approximated well enough, and the order is one over square root of j. Is one over the square root of j. There are some technical advantages of this result. Now, in the second part, I want to explain why deep CNNs are better than other neural networks in certain sense. So, the first result shows that actually CNNs are at least as good as fully connected neural networks. So, I use the concept of So I use the concept of downsampling from WebIts, meaning that if you have a sequence in R D, I take one out of each M entrance to get a sequence formed by Vm, B2M, 3M, and so on. So this is downsampling, very simple. And we apply downsampling at certain layers. Then the theorem says that if Then the theorem says that if you give me a fully connected neural network with L layers, these are fully connected neural networks, then I can always give you a downsampled deep CNN such that the output functions are the same. So they produce the same output functions, same last layers, therefore the same output functions with at most eight times three parameters as. Parameters as that of the fully connected neural networks. So it shows that CNNs can be as good as fully connected neural networks. Again, the paper was published two years ago. Then you may ask, can we do better? Actually, we can if the function, if the approximated function has some spatial structure. So the first result is suppose the approximated function is a rich function induced. Is a rich function induced by an unknown vector C in Rd and a univalued function G. So if we assume that the univalid function G is Lipis alpha with alpha between 0 and 1, then the rates of approximation with this number of layers, J, which is determined by the dimension and the filter length, is Of the filter length is n to the minus alpha, which is the rates of convergence by shallow neural networks for approximating univalid functions of Lipitz alpha. Okay, so this is joint work with Han Feng, who will speak tomorrow, and with our students, So Huang, and my former student, Xing Fuang. The second evidence to show the advantages of using CNs is when we consider approximating radial functions. So these are functions given in the form of radials. So although it's a function R D, but actually it appears in this function only in terms of the norm of the input X. So it's a radial function and we require this radio function. And we require this value function is Lipitz, a Lipitz function. Okay, Lipitz one. And we use not only shallow neural networks, but allow this rich function form where this activation function may be different at each hidden layer. If they are the same, then that's the shallow neural network. And we use Hostov distance to measure the distance between The distance between two functions set. And our theorem says that if you consider approximating the ball of the Lipitz radial functions in Rd by the CNNs, then actually the rates of convergence can be of n to the minus half. So, which is very good because. Is very good because these rates do not depend on the dimension D, although in coefficients there is a constant depending on D. But if you use the rich function space to approximate it, the rate is at most n to the minus 1 over d minus 1. So you can see that the exponent is 1 over d minus 1. If d is large, this exponent is extremely. This exponent is extremely small. Meaning that if you use shallow neural networks, the approximation is pretty bad for large dimensional functions, but can be very good by CNAs. So that's another advantage to show why CNAs are better. This is joint work with my two former students, Tong Mao and Shongjie Su. Now, we have also done some generalization analysis to show that whether actually we can accommodate noise in learning algorithms. So, we restrict the parameters by some constants R, where R can be large. Then we use the so-called empirical risk minimization, meaning that we minimize this empirical error, least squares empirical error, over The squares empirical error over this hypothesis space. So, in this hypothesis space, we take all possible output functions of the CA algorithms, but restrict to the coefficients by the norm, restrict the filters by the maximum norm, and restrict the bias vectors by the infinite norm. And there is a fully connected layer which is bounded by this. Right this. Then, because in practice, quite often CNNs are combined with fully connected layers. So we have here we add one fully connected layer with special structures. And then turns out that if you use this evidence, typically least squares error is for regression task. So we want to learn the conditional mean, which is called the regression function of some. Of some probability distribution rule on this product space B product with minus mm. And if we take m samples, then we get this empirical error. And we minimize in this empirical error, least square empirical error, we get the output function from this neural network. So because the distribution is supported A distribution is supported, is supposed to be bounded by m. So, naturally, if you have an output function, since the y, the output is always between minus m and m. So, naturally, we bound, we truncate this output onto the interval minus m. And then, yes, I'm sorry, your connection seems to be a little unstable. If you could turn off the camera, maybe it will be better with your printing. Maybe it will be better. Okay. Thank you. No problem. Turn off the camera. Okay. Stop video. Okay. Yeah, thank you. So now is it better? Hopefully. Okay. Yeah, I don't know why. It's a new office for me. This is day five. Okay, yeah, go ahead. Okay, now it's okay. Let's go. Okay, now it's okay. That's good. So then the result says that if the regression function is radial and its leap is alpha for some alpha between zero and one, then the output function can approximate the regression function in our two norm in expectation of this order. In particular, if we choose the parameter in the fuller kinetic neural network by this, and if we choose the number of layers by this, then the rays are. This, then the rates of convergence is m to the minus alpha over one plus alpha, which does not depend on the input dimension d. Okay, it's pretty nice. Yeah, you're right. I don't know how to turn to okay, so it's here it is. Okay, so then uh Okay, so then yeah, just mention some other results along this direction approximating by Lilu deep neural networks with structures. Recently, we have studied this for classification problems, but only on spheres with my colleague, former colleague Feng Han and our student Seoul Huang having this paper to be published by To be published by IEEE Transactions on Neural Networks and the Learning Systems. And also, we considered real neural networks with logistic loss, not only with these serious loss. And yeah, this is a joint paper with my former student, Lei Zhu, and our jointly supervising student Zhu Han. It's a paper. It's a paper we are revising for Journal of Machine Learning Research. Should eventually be published. And we have also worked on benign overfitting. This is a hot topic dealing with over-parameterized neural networks. Also, the paper is revised for journal machine learning research. And we have also considered learning functionals and maps, nonlinear. And the maps, nonlinear functionals, and nonlinear maps from infinite dimensional spaces. It's a joint work with my student Inhao Sung and my former student, Trin Fan. And yeah, it's a paper we are revising for JFAA. So there are a lot to do along this direction, but I only want to stop here. Only want to stop here and later on, if we have another birth conference, I will be happy to share the results with you. Thank you very much. Maybe I stop here. Thanks a lot. Are there any questions about connection? But connection in the audio, yeah. Bad connection now, it's better either in person or yeah, okay, doesn't seem to be probably every okay. Uh, let's uh thank yeah, let's thank Ding Sean again and let's thank all the speakers of this afternoon. Thank you, thank you. No, no question. I'd like to. I'd like to no, okay. Sorry, I didn't realize. Dima, yeah, I have a question. One personal question to okay, I think the speaker left already.