Thank you so much. So I apologize for being the organizer who also gives a talk. It's slightly awkward, but I was really excited about this result, so I wanted to share it. A couple words about Gerrard. Gerard is my mathematical brother. We had the same advisor. He was the first person to ever invite me to give a seminar talk. He was the first person to ask me to help solve a problem. Help solve a problem and the first person to make me take analysis seriously. People used to ask me, like, What Soblev space are you working in? And I would give this answer, all of them? So to the extent I feel like a serious mathematician at all, it's thanks to Garrett. So happy 60th birthday, Garrett. I mean, happy 61st as well, but more importantly, happy 60th. And thank you for everything. Thanks a lot. For everything. Thanks a lot, Steve. So, this talk is joint work with Martin and Justin Valletta, who's Martin's PhD student at Florida State. So the problem is to study the EP diff equation, which is a PDE of the form written most easily in terms of omega, but basically. Omega, but basically in terms of a velocity field u. So omega is defined in terms of u by some operator a, which is a differential operator, maybe pseudo-differential operator, in general of order 2k, and k is at least 1. So I'm thinking about the radial case, so r is greater than 0, and t is greater than or equal to 0. So the basic question is, we have a smooth initial condition, u0, that generates, of course, the omega. That generates, of course, the omega as well, and you ask whether u exists for all time and whether it remains smooth as a function of r for all time greater than or equal to zero. So there is a sort of well-known standard method of trying to prove breakdown for PDEs like this. If I'm assuming radial solutions, then most likely the worst case is right at r equals zero, the origin. zero, the origin. And so I would probably want to do something like find an ODE for the spatial derivative. UR, the C1 norm controls nonlinear hyperbolic PDEs in general. So if you can figure out what happens with the C1 norm, or at least the derivative at a particular point, then you should be able to say something about the entire solution. So the basic method is try and derive some ODE for this. Derive some ODE for this. So for this equation, what that looks like is time derivative of ur looks like some negative constant times ur squared plus some terms. Positive, they're certainly positive. You hope that they're bounded. If they're bounded, then this looks like a very standard ODE. You could give an undergrad and have them solve it and report back the solution. You'll see it blows up in finite time. So you show that UR of T and 0 approaches. UR of t and zero approaches, in this case it approaches minus infinity, but that gives you a blow-up proof, and you're done, and you're very happy. So the problem is, and I'm embarrassed to say how long I spent trying to figure out this term with a question mark, it's one of those things that looks like it's almost going to work. And so you can spend a long, long time trying to just fill in that little tiny gap and fill. Tiny gap. And finally, I'm sitting in an Austrian prison cell and realizing: like, what have I been doing with my life that I'm spending all this time on this thing that's obviously not going to work? So I came up with finally this ODE approach to PDMEs that often works for local existence. I realized, like, what if we tried something different that's probably not going to work? We tried something different that's probably not going to work, but maybe it does. And so we did something completely different, and so that's what I wanted to show you. So the geometric interpretation essentially turns a PDE into an ODE on some infinite dimensional space. And not every PDE turns into an ODE on an infinite dimensional space. There's something special about Euler equations that makes that sometimes work. And so So, the Bonach space that I'm considering is the space of positive continuous functions. Those are the derivatives of diffeomorphisms that have in mind. Then, one of the nice things about ODEs is you have a comparison theorem. So if you know how a simple ODE behaves, then anything else that's related to that should behave in essentially a comparable way. So, we do that, an infinite-dimensional version of that. Most important thing is. Most important thing is finding an ODE that has a simple exact solution in the same sense that this has a very simple exact solution if I know a bound on this. So the Louville equation is what we end up using. And so the difference in what we're talking about now is while ODE techniques give very good local existence results, they don't typically get used to prove global existence results. Prove global existence results. And so, this is, I think, one of the first times that we've been able to do this. All right, so let me summarize the EP diff equation. So, consider the group of, so we're doing a simple case to make things work out pretty explicitly. Consider the group of all smooth diffeomorphisms of all of Rn with good enough decay conditions at infinity. And the Lie algebra is the vector fields that also decay well enough at infinity. At an infinity. And so I take a Riemannian metric defined at the identity by a Sobolet hk. So if sigma equals showing everyone else how to use the pointer, I can't have one. So consider the HK metric. HK dot is when sigma equals zero, when it's degenerate. It's slightly degenerate, it's not so bad. But when you consider for some When you consider, for some k greater than or equal to 1 to make this well-posed and well-behaved, so sigma is either 0 or 1. And we extend this to a metric on all of the diffeomorphism group to make it right-invariant. And we know, as Cornelia explained yesterday, and several other people have said, we get a geodesic equation that splits into two different equations: one in terms of the flow on the flow. One in terms of the flow on the diffeomorphism group, and one in terms of the Euler-Arnold equation. So, this is a general sort of setup, but in particular, in our case, for this metric on the full diffeomorphism group, the Euler-Arnold equation looks like this. And omega is a sort of momentum term given in terms of the velocity field by some differential outer. So, the simple example to really see this is if n equals 1. is if n equals one-dimensional and Sovalov H1 metric, then you get an equation that's pretty well known, either the Hunter-Saxon equation with omega equals minus uxx, or the Kamaso-Holm equation with omega equals u minus uxx. Those are both very well-known examples of this situation. What we want to do is consider essentially the same thing, but possibly raise the. So we considered k equals one, but also k equals two, and more importantly, consider. And more importantly, considering higher-dimensional situations. So, we have a flow equation in terms of the group of diffeomorphisms. And the breakdown condition for equations of this sort with k big enough is that generally when things go bad, it's because the velocity, the derivative of the velocity, goes to plus or minus infinity. For these equations, For these equations, the typical breakdown mechanism is that ux goes to minus infinity, corresponding to the derivative of what's supposed to be a diffeomorphism going to zero. So you might be remaining a homeomorphism, but you're failing to be a diffeomorphism, and that corresponds to particles colliding. In a sort of casual sense, not like a disastrous sense. Disastrous sense, you can, in principle, extend solutions beyond such a breakdown, but it does break down your classical sense of solutions. So you can see this working really explicitly in the Hunter-Saxon equation, which is the most famous, deservedly, example. You're probably more familiar, if you're familiar with it, with the Hunter-Saxon in the periodic case. This is Hunter-Saxon in the full real-line case, which is slightly different. Slightly different, and that's closer to what we're dealing with because we're working on Rn. So, what I want you to see in this formula is that if you start with some initial velocity field, u0, if that derivative is ever negative, then of course you can make this whole thing equal to 0 at some time. If you can make this whole thing equal to 0, then gamma x must go to 0. So, gamma fails to be a diffeomorphism. Typically, it becomes just a homeomorphism, which is still pretty good. Just a homeomorphism, which is still pretty good, but it is something that you don't classically want. And it will result in the corresponding velocity field, if that's what you're paying attention to, going to minus infinity. So here's a picture of how that looks. Hunter Sachson with specific initial data. So here I'm plotting what's supposed to be a diffeomorphism, and you're seeing how it flattens out. Here is the x derivative of that diffeomorphism. X derivative of that diffeomorphism. You can see how it goes to zero. And I did it, continue it past the breakdown time so you see what it would look like if you try to continue it. And here's the corresponding picture in the velocity field where, of course, you get this extremely bad-looking behavior. That behavior is not so bad if you look at it from the Lagrangian point of view, but it's not a classical solution of a PDE. But it's a weak solution, right? Yeah, you've got. Solution, right? Yeah, you could absolutely consider it a weak solution. It's still energy conserving. So if you're willing to make more allowances, it's fine. But if you just want your velocity field to be a nice C1 function, it's not. And that's all the breakdown results that we have here are essentially of the same form. I think you can continue them as weak solutions, but they just won't be C1. Okay, so some things you get for free if you. Okay, so some things you get for free if you have, we're considering HK metrics. HK is contained in C1 if K is big enough. So for example, in one dimension, if we were to consider the H2 metric, for free, any H2 velocity field is also in C1. If you're in C1, then you'll have your global existence. C1 controls everything. So C1, so the H2 metric in one dimension, H2 metric in one dimension, everything works out very nicely due to several quite general results from Martin, from FX, and several other people. So in particular, you never have breakdown, but we wondered what happens in higher dimensions. Two dimensions is critical. So in two dimensions, H2 is almost, but not quite, embedded in C1. And so that's where everything does. One. And so that's where everything does get quite difficult. Once you're in dimension three, things are much more comfortable and easier. So when you're able to prove in three dimensions, you do get breakdown in finite time. I suspect we've got a partial argument that in the critical case, solutions do exist globally. We don't have it fully proved yet. So the general conjecture is the critical case k greater than or equal to n over 2 plus 1. Greater than or equal to n over 2 plus 1. So, as you probably know, if k is strictly greater than n over 2 plus 1, then hk implies c1, and c1 is the important thing. The critical case does not imply c1, but it still seems to work in all cases that we know. So our conjecture is you always get breakdown, or you always can find an initial condition that leads to breakdown if k is less than n over 2 plus 1, and you always get global solutions. And you always get global solutions if k is greater than or equal to n over 2 plus 1. So here are the known cases. So I would say the most important results so far is k equals 1 for any dimension. So that's generalized to Camaso-Holm, and that's due to Dong, Yu, and Zhai in 2013. Our technique is kind of similar to theirs. They also used radial solutions. But we couldn't make their technique work in k equals 2, so that's why we came up with something different. And it also gives, I think, Different. And it also gives, I think, a somewhat easier explanation for what they found. But they found that first. The result I'm talking about is k equals 2 for any dimension larger than 2. Breakdown we showed a while ago in k equals 1 half. That is also a special case of the generalized Constantine-Lax-Maida equation in the sort of easy parameter range. So it's not a huge deal. It's not a huge deal, but the special case that corresponds to Euler-Arnold is that k equals one-half for one dimension. For global existence, as I mentioned, these general results for k strictly larger than n over 2 plus 1, when it becomes a strong metric, then things get easier. Not easier, but things work out more nicely. For any dimension, also works for non-integer k, which is substantially harder than integer k, as you might imagine. And finally, global existence. Dimension, and finally, global existence for the critical case in one dimension, which is what makes me think this critical case works in any dimension. Critical case in one dimension is three halves that we did a few years ago. So, I'll sketch the basic ideas of this proof. The main thing is using momentum transport. So, any Euler-Arnold equation, but more generally, even equations. Equation, but more generally, even equations that aren't Euler equations have some sort of momentum transport law. And if you view that as fundamental, you can get pretty good existence results just using that. So what that looks like in general on any lead group is that the dual of the group adjoint operator conserves the velocity field. Conserves the velocity field. So basically, this formula. So if gamma is the flow and u is the velocity field, then the add star operator is constant. And you can use this to solve for gamma. So normally the geodesic equation is a second-order differential equation. Exactly like this. d gamma dt is something and then something in terms of u and then du dt is something else. And then du dt is something else. But using this conservation law, I can solve for d gamma dt. That's the right translation of u of t. I can solve for u of t using this in terms of gamma of t, and now I've got something that depends only on the initial condition u0 and the diffeomorphism gamma. And typically people don't like this because this looks superficially a lot more complicated than this Euler equation, but it is actually a lot easier in the sense that In the sense that this is never an ODE, but this is, in the best cases. So this is not exactly what Evan and Marston did in 1970, but it is a sort of consequence of what they did. What Evan and Marston showed is that the full second-order differential equation is an ODE in infinite dimensions. And you can also directly show that this is an ODE. Normally, a PDE doesn't. Normally, a PDE does not turn into an ODE just because you say, I'm infinite-dimensional now. You get just very lucky with the special properties of this equation. Essentially, what's going on is, if you'll recall, add star involves the metric. The metric involves some differential operator, which is order at least k, which is order k, and k is at least one. And so, what that means is what's going on here is the add. Going on here is the add star involves inverting that first-order or higher differential operator, and so you're getting smoothing for free. And that smoothing is what makes this work as an ODE, where the right side is bounded as an operator in a bonic space. Normally, that does not work. So it was the great insight of Evan and Marston that makes that work. So, as Gerrard was saying in the previous talk, you can use this technique to get a really solid local existence. Needs to get a really solid local existence theorem. If you start with an initial condition in some good enough Sobolev space or in C1α for alpha strictly positive, then you get local existence on some time interval. And the solution gamma depends smoothly on u0. Once you have gamma of t, you can solve this equation to find u of t. You also get dependence of u of t on the initial condition, but it breaks. But it breaks. And so you get really poor results in terms of the continuous dependence because this is a bad thing to do, essentially. And diffimorphism groups is not a smooth operator. Okay, so that's the first idea of the proof, working in terms of an ODE. Second is considering radial solutions, which reduces it, of course, to a one-dimensional problem and makes things much, much easier. So you get a fairly explicit formula for. Fairly explicit formula for how to invert the differential operator that gives you omega in terms of u to get u in terms of omega for some function delta, some green function. So your momentum conservation formula that looked complicated before is now fairly concrete in terms of the diffeomorphism gamma, the momentum function omega, and the initial condition. And so plugging this in, you get an equation for the diffusion. For the diffeomorphism γ directly in terms of its spatial derivative and the initial vorticity. So I've written this in sort of a weird notation. This all depends on t, but I wanted to make it look like an ODE, because that's what it is. And that's the whole point of how this works. If I take a spatial derivative, you might think things go bad, but they don't. You get lucky. And gamma prime still just depends on gamma prime. And gamma prime still just depends on gamma prime, not on higher derivatives. And that's what makes this whole thing work: that it's a really closed ODE system. So on the space of Banach C1 diffeomorphisms, gamma, this is an actual ODE, which is a little bit surprising because normally when you're working in higher-dimensional spaces, you can't do C1. C1 is the space. Can't do C1. C1 is the sort of gold standard. Of course, you want a diffeomorphism. That's the optimal space. Normally, you need to go to something slightly higher than C1 to make your inverse Laplacians work and such. Because we're working with radial solutions and we have a pretty explicit formula for the green function, we can actually get C1, which is a little bit better than you might expect in higher dimensions. So here's a concrete example of what this looks like in terms of what I call rho, which is the R-derivative. Rho, which is the R derivative, notice that gamma is completely determined by rho. So while this looks like two different functions being involved, gamma is really just a function of rho. It's just all infinite dimensional, so we're dealing with non-local stuff. But once you make that sacrifice, the behavior is otherwise very nice. So you get a theorem that if omega 0 is L1, then you get a unique continuous solution rho. If you have a continuous rho, If you have a continuous rho, that's the R derivative, then you get a C1 gamma and a C1 velocity field. So that's a nicer theorem than you might expect, especially since this allows for very rough sort of solutions, right? Like omega 0 might be a delta function. You might get pecans and such, but you can still get well-behaved enough solutions from this. With k equals 2, you get a slightly weirder condition, which I still don't fully understand. Which I still don't fully understand, but this also works. This is enough in the higher smoothness case to get the same local existence in C1. Okay, now the main point is, how do we actually use the ODE technique to solve the PDE? So the simplest possible case is this, which is where gamma doesn't appear at all. Gift morphism. If I were to differentiate, If I were to differentiate this with respect to R, you get this equation, which you might recognize as the Louisville equation, which Louisville solved in a beautiful two-page paper from 1853. The first page is the solution of the equation. The second page is an alternate derivation of the solution of the equation. The most efficient paper I've ever seen. It's in French, but it's just deriving equations, so it's very easy to translate, even if. To translate, even if you don't speak French well. So essentially, you get a very explicit formula for the solution, which looks like this, and you might recognize this from earlier in the talk as exactly the explicit solution for Hunter-Saxton. So the connection between these was pointed out by Saria and Saxton in 2015. And so the point is: once you have an explicit solution of an ODE in a bottom space, Of an ODE in a bonic space, you can compare anything else to it. If it works, then you get a similar blow criterion. So here's what that looks like in the well-known case of Camaso-Holm. Here I have an actual system, gamma and rho, sort of connected in this complicated way, this for odd solutions of Camaso-Holm. When you make this combination of formulas, you get an equation that basically eliminates the gamma and just involves. Eliminates the gamma and just involves q, except for this term, right? So this is also your q. And so if you use a comparison theorem, if you know that omega 0 is negative, and you eliminate this by saying it's greater than or equal to 0, then you get a comparison result. And so you say cube goes to 0 at least as fast as the solution of the Louisville equation, which I already know, and you get a breakdown result. That's all. So it makes it very easy. And so this is a special case of the general idea we did. So here's the general Louisville comparison theorem, or sorry, general comparison theorem for ODs on Bonach spaces. If you are so lucky that you have whenever v tilde is greater than or equal to v everywhere, then the velocity field is also greater than or equal to the corresponding one everywhere. Then of course you will get the solutions are. You will get the solutions are everywhere greater than or equal to. So you only have a partial inequality, but if you're so lucky that it works everywhere, then you get what you expect. All right, so the main theorem looks like this. The kernel delta is what inverts the differential operator. We write it in terms of some function phi to make things a little bit easier. And our conditions on the domain s greater than or equal to r, greater than or equal to zero, are that phi is everywhere positive. This kind of weird one logs. This kind of weird one, log supermodularity, which happens to work for the cases we know. And finally, this extra weird one that whatever this function is of r, it's greater than or equal to a constant. So happen to have this for the cases we know. And okay, so assume that your kernel looks like this, satisfies these basic properties. Define a function capital Q in terms function capital Q in terms of the green function and this Q is your comparison function which we'll use to compare to the Lugal equation. Notice it looks like some blah blah blah times gamma r. If Q goes to zero then gamma r goes to zero you leave the videomorphism here. All right then this can be compared to a Louville equation and you get breakdown in finite time. So roughly speaking this is the Jacobian determinant. It is exactly the Jacobian determinant in It is exactly the Jacobian determinant in the simplest case. In general, it's some slight modification of it. And the very, very rough sketch of the proof is you derive an ODE for this equation using the equations I gave you before. I rewrite this initial momentum slightly. The log supermodularity tells you that the only things you need to consider for general S and R are the endpoints. And R are the endpoints. That's what it's there for. Just to reduce this whole interval thing to something happening on the endpoints. And the C, the weird third condition, comes up to get me this term being bounded by this. So notice what's happening here is the time derivative of the quantity I care about only involves an integral from r to infinity, so it's essentially things happening out at infinity telling you what's happening at the origin. And so that gets you the comparison theorem. So you see this going to zero, and here are the actual applications. To prove the condition, the log supermodularity and that constant condition, in principle it's fairly easy, but when you're dealing with Bessel functions, it's harder than it may look. So I'm skipping those details, but that's like, surprisingly, that's where a lot of the work comes in. A lot of the work comes in. Just checking those conditions. Here's what the graphs look like for the comparison functions that I'm using. I needed some color in the slides at this point. I'm not sure you care about this though. Okay, so finally the future directions. I would like to do this for any initial momentum, not just assuming the momentum is negative everywhere, because I think it doesn't matter. We only did k equals 1. We only did k equals 1, which was already done. k equals 2 is new. Higher integer k should be relatively straightforward, but I would like to do that. In generalizing this to any value of k, so any Sobolev metric, possibly non-integer value, we've only done, so for very large k, it doesn't matter, but for small values of k, it's very hard. So we haven't figured out how to do it yet. How to do it yet. Even in one dimension, if k is less than 1, the green function becomes submodular. Remember, supermodular was the condition. The behavior genuinely changes. We need a genuinely new theorem. We haven't gotten that to work yet, but I think it works. Then the general conjecture that I gave you that k greater than or equal to n over 2 plus 1 is necessary and sufficient. In particular, the critical case is the hardest. Particular, the critical case is the hardest part. I think it's probably true when n equals 2. We don't have a proof yet. We know it for n equals 1. For higher n, no idea, although I think it's true. Non-radial solutions, obviously harder. Radial makes it one-dimensional. You can prove blow-up in the radial case, you can't prove global existence in the radial case. Because it doesn't tell you the general question. But I think the radial is the worst case. Other domains should be not so hard. Compact domains should be easier. We had some trouble with the decay conditions. But the ball in the sphere would be probably easier. I know, I know. And finally, almost finally, you can do this for non-metric geodesic equations as well. So things that don't come from a Riemannian metric, but do give you a higher generality in the family of equations. Family of equations. And last, and probably very far away, is doing this in the volume-preserving case, which is the one, of course, we care about the most because it's fluids. But since we're dealing with Jacobians collapsing to zero as our breakdown condition, of course, that's not going to be what shows up there. But possibly some of the techniques also generalize. So that's all. Thank you for watching. So for fixed K, do you expect that you take M to be sufficiently large? So proving breakdown should be much easier? Much. Do you see that maybe in your proof? No, so in the proof it really is the same for all dimensions. Okay, so you do not see the advantage by taking n submission large? Not really. I mean there's a trend. Not really. I mean, there's a transition between when things work and when things don't work. But once they work, they all work the exact same way. So for k equals 2, as soon as I get to dimension 3 versus dimension 17, there's absolutely no difference. So I don't see that happening in this situation. In principle, it should be easier in higher dimensions, though. This is maybe a very nice question, but like in the new method, now you have some estimates. Can you then go back and translate them into this differential inequality that you were struggling at the beginning and sort of learn something about the things that need to be positive based on this analysis? I mean, all of them should be an equality, right? So so then maybe once you go back, maybe there is some hidden identity that you can uncover based on like a posteriority, of course. Like a posterior, of course, but on the analysis and incorporate. So, honestly, I don't think so. And I did think about that a few minutes before I came here. How am I going to answer that question? What happens is, so not only are they opposite sign, and not only is one of them hard to bound, but when I thought about how you might handle that positive term that's supposed to be bounded, I convinced myself it was unbounded and that it was of the same order. And that it was of the same order. And so, but I have not gone back to see, you know, now that I know how it works, exactly what that looks like. I still don't feel like, yeah, I don't believe that works in general. I do believe in the critical case, all right, because uh essentially what you'd want, you get one bound for free, which is energy conservation, 'cause it's a the G Desk equation. Because it's a G-description equation. You get this extra term, which you would very much like to bound in terms of the energy, because then it's constant. And you can do that, I think, probably in the critical case. But beyond that, they're just not comparable. And so even still, even if I tried to take all this and go back to the more standard method, I still don't think it would work. I don't see it working. I don't see it working. It's entirely possible someone more clever than me could see a way of making it work, but I couldn't. For the metric to work, do you need the existence of this Newtonian conservation law that is for that star of gamma U2? Yes, so just to even say Yes, so just to even set things up, that's the important thing, the momentum conservation thing. The energy conservation is not important at all. So we basically never use that. Although it's important for Euler-Arnold equations and it's true, it's not helpful. The momentum conservation is the crucial thing to get the ODE directly on the Dippy morph. Does it mean that the equations that these covers are equations that come from come from right invariant metric. So because in order to derive this conservation law you need you need right invariant metric on the group of people markets. Although the conservation law is true for other equations that don't come from geodesics. So the whole family of Proudman-Johnson equations, only one of those parameters is a geodesic. The Konstantin-Laxmein. The Constantine Laxmaida general family, only one of those comes from a Romanian metric. But you still get the momentum conservation law in all the cases. And since that's the important thing, I think this also works for those as well. In your comparison, I haven't really understood where the conditions on the N and the S really play a role when you check the conditions on the log supermodularity and stuff like that. Because it's somehow hidden in the file here, is it not? Somehow hidden in the file here, is it not? Yeah, it's little. Yes. Yeah, it gets a little technical to actually compute those things. So, for example, log supermodularity is a thing that happens when k is large enough compared to n. And it changes. So, in the same way I said here, even in the n equals 1, k equals 1, that's a transition point for logic for modularity. That keeps happening in higher dimensions as well. In higher dimensions as well. So I wish I had, and I suspect probably there is some general principle that says you can prove this easily without having to dig into formulas for Bessel functions that are hundreds of years old. There must be an easier technique, but you do see it coming up, those conditions, when you try and check the conditions. You said that you can you expect that you can generalize this to domains. Do you expect the result so take any compact domain, for example, any compact manifold, do you expect the same result? You have a field. I would be very surprised if that's not true. Okay, yeah. And I think on a compact domain, things should be easier. The one thing that might change is the critical index, because you never know when in that case. You never know in that case. But for example, we have trouble even with some of the degenerate metrics, like sigma equals zero, right? The hk dot metrics. Those are badly behaved on the full RN, much better behaved on a compact domain. So you can get more results, I think, on BN or SN. But I strongly suspect all the same stuff will work. We only really have something for radial symmetry. So I can't say general domain. So I can't say general domain. You need to adopt the technique somehow if you're working on a general manifold with some of those. Yeah, you need to break out of radial, obviously. Yeah, I think I have one comment. So if this first of all, we start at starting a blowout. If you have something focusing blowout, then it can put to any like a compact manifold because the blowout zone has one point. Then when it's zooming, it can be flat. Right, yeah, so that may be possible. Like, if the manifold is irradial, symmetric, it's the irrigation. Right, probably the influence of anything that's not at the origin should be negligible. It does not, I mean, as I mentioned, I couldn't see a way to really localize the result and just get something that's happening there. So the contribution from outside matters, but possibly maybe it's not that important to have the symmetry. Can you comment a bit about this? Especially, can you comment a bit about the assumption you impose on the Greens function? You said you impose something, uh how restrictive it is, why you need that function? The delta, you call it delta, right? Yes. So if you can comment about that? Funny enough, it's so the main thing was dealing with this, trying to reduce it to something that's happening only on the endpoints. So we needed something to say why only the endpoints matter. Endpoints matter. And so the log supermodularity showed up in order to give you that, and the condition, this weird-looking condition, showed up in order to be that same C that shows up here and therefore here. So how did we come up with those conditions? Weirdly enough, you might think it would be easier just to work directly with the H2, because that's what we're trying to deal with, right? H2, so some Bessel functions. We couldn't Some Bessel functions. We couldn't figure out how to deal with the Bessel functions specifically, so it was actually easier to try and do this much more general version and then check later that the Bessel functions actually satisfy those, which is not normally how it works, but just conceptually, it made it much clearer how things, or why things worked. But it was really specifically to eliminate these integrals one by one. Well let us thanks stupid general. And now we have quite a great rate for 35.