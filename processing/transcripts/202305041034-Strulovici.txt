This is joint work with Theo. I don't have a picture of him, but you can see him live. And he's another contestant co-author who has taught me a lot. So as usual, if you have a hard question, ask him. That's typically what I do. So the idea here is: as an economist, I often need to use some sort of smoothness of value functions. And I will give you some when I look at When I look at stochastic control problems, and I will give you an example, a motivating example to get some monotonicity of optimal control. And I believe that Theo, when he gave his talk earlier this week, actually was also using some of the results that we have here. So we want to have some sort of general result for the smoothness of value function. And so what we're going to do today is to talk about this concept, solution concept for solution. Concept, solution concept for solutions of LGB equations, which are the LP solution, which is sometimes called in the literature a strong solution or sub-left solution. And we will provide conditions, weak conditions, on the primitives under which the value function is an LP solution. And I will again motivate why we think it's useful, especially for economists who are not very well versed in viscosity solutions, to have this kind of concept. And this will imply This will imply that the value function is C1 in space, almost differentiable in time, and almost everywhere twice differentiable in things. Let me start with a small motivation. So consider the following control problem. It's a pure control problem for simplicity. There is a state variable X and there is a discount rate R that I emphasize. And you control the you control the state according to this equation. Control the state according to this equation. We're going to make two assumptions. The first one is that the flow payoff is increasing in the state little x. And the second assumption is that the drift is increasing in the control over x. And the question, simple question, how does the optimal control that you apply here at any time is going to vary with the discount rate? Okay, so that's a simple question. So if you don't have all the So, if you don't have all the tools that many of you have, but you want to apply something simple, you're going to be able to read it. Well, the one that I mentioned is certainly important, but I mean for example. No, it's for simplicity. Yeah. So, we're going to have this result that if the value function solves the AGP equation in a classical sense, then we can show easily using some comparative static techniques. Comparative static techniques that the optimal control that you apply at any time is decreasing in the discount rate, which means basically that the more patient the agent is and the more effort the agent puts in raising the state. But we assume that alpha star exists as well. Yeah, so here this is just a motivating example. I'm waving my hands. In fact, the compartment static results. In fact, the comparative statics results that are used often do not even assume this. They say there is a sort of a comparison of the RMAX that does not so it does not assume that the RMAX is normality. So this is an instance of comparative statics. So how many of you are familiar with the expression comparative statics just to see? Okay, so most, most, okay. So comparative statics basically describes how optimal variables change with parameters. How optimal variables change with parameters. So, in particular, the theorem, sort of basic theorem, again I'm stating a very simple version of it, is the following. So, suppose that you want to optimize the subjective h with respect to some action alpha, and there is a parameter theta. Again, for simplicity, the thing is in R. And suppose that the subjective function h has decreasing differences, which means that whenever you compare two actions, alpha You compare two actions, alpha prime greater than alpha, the difference in the objective is decreasing in the parameter. Okay, so that's what decreasing differences means. When H is differentiable, that's characterized by having a negative cross-partial. The theorem is that if H has decreasing differences, then the set of maximizers of the problem is decreasing in the parameter in the so-called strong set order, which I'm not going to define here. Which I'm not going to define here, but when the maximizer exists and is unique, it means basically the maximizer is decreasing on theta. So now suppose that we want to use this result to our problem. If the value function is a classical solution to the LGBT equation, it means that you can literally take this equation, I mean you can take it literally, and so it means that the control alpha, the optimum control alpha, Control alpha, the optimum control alpha at any time maximizes this objective. So if you can show this objective has decreasing differences, you will be done, right? In alpha and R, Roy checks. So here what we can do is to check this cross derivative respect to alpha and R. And you can see very easily that the cross derivative, again waving our hands a little bit, is equal to mu alpha times VXR. And so our assumption that mu is increased. So, our assumption that Î¼ is increasing in the action implies that if we can show that V has decreasing differences, the value function has decreasing differences, then this whole objective has decreasing differences and we can conclude that the optimal control is decreasing R. Make sense? Yep? The video is also indones. Absolutely. Absolutely. So, in fact, So, in fact, you can show that P has decreasing differences, but using completely different techniques. So, you just compare different control paths, and you can show from first principle, basically, that it is true. And basically, what decreasing differences means is that the value of being in a higher state, so that's the v with respect to x, is greater as one becomes more patient. Becomes more patient, okay, because you enjoy this higher set basically for higher weight. Okay, so that's true if V is a classical solution. Now, in fact, even if V only solves the LGB equation almost everywhere, we can get a similar result, which is that the optimal control is going to be decreasing in R for almost every X. So that's the kind of reasoning that we want to do. Kind of reasoning that we want to do, and as economists, it's kind of simple to do this. There are possibly other techniques using these cost solutions, but that's the kind of result that we want. So, there are lots of results in the literature, but not that many for the kind of setting that we have in mind. And so, what we want to do is to relax the usual assumptions and to get a strong result. So, in particular, the wish list for For is to have the simplest assumptions. In particular, we want to relax the assumption that the payoff and the drift in the control problem are continuous in the state variable. One reason to do this is because oftentimes the problems that we have faced as economists are games, and when they are games, basically there are other agents who may react in different ways, and that breaks the conclusion. Different ways, and it breaks the continuity of the pair, for example, in the sector. Okay, so we want to relax this continuity assumption. We also want to have control and stopping at the same time, because there are many problems of this kind. And we want to have unbounded double operator. As I mentioned, we want to have a strong conclusion, we want to have a strong solution or LP solution instead of a viscosity solution. Instead of a viscosity solution. And the other thing that we care about is to have, we want to look at controls, we want to define the value function respect to controls that are admissible in the sense that they generate a strong solution to DSD. So, and the reason we want to do this is because if we think, if you think in discrete time, an economic model, in discrete time, you have some sort of state variable to take some action, there is some exogenous uncertainty that's going to affect the state in the next period. That's going to affect the state in the next period. And so, if I tell you the realization of this exogenous uncertainty, and if I tell that you have in the model, and I tell you the actions, you can predict exactly the state in the next period. And so, the continuous time equivalent of this is to have this strong solution to SDEs. And that's why we want to insist on this particular. This is very, very arguable. Well, point of view, this point of view is very arguable. Because if you observe the state, then you never observe the law. The state, then you never observe the noise. Noise can be dependent on the state, that's with solution. That's right, but so when you write your model, when you write your economic model, you're going to start with some description of exogenous uncertainty. So, for example, you're going to say there is some brain motion, there is some action that you can take, and then what I would like is that g if you tell me the realization of uncertainty, meaning the path of brain motion, and you tell me the control that has been used, I won't be able to say where the state is. To say where the state is. But you never observe the brown emotions, my point. That's right, but that's also true in discrete time. And nonetheless, we insist that whatever exogenous uncertainty we modeled, when it is realized, predicts the state in the next period. So let me tell you about the setting. So there's going to be a combined So, there's going to be a combined control and stopping of a diffusion. The domain is going to be, so this horizon here could be finite or infinite. And the state space is going to be some open connected subset of Rt. So, the interior of the domain that we consider is this Cartesian product. And the decision maker is going to choose some stopping time and some control process. So, let's look at the problem in more detail. In more detail. So that's the control stopping problem where you have a running payoff F that depends on the control, the time and the state. And you have, when you decide to stop, if you are actually deciding to stop yourself, you have some uh some free boundary pair of little g. If you hit the boundary of the domain, you get some uh some pair of little g. And you have this dynamic equation. And you have this dynamic equation. As I mentioned, we want to focus on admissible controls, meaning that they generate a unique strong solution to TSD. The corresponding HGP equation is given by this. So there is this stopping payoff on the boundary of the domain. You can also decide to stop endogenously, in which case you get spare of G. And you have the running payoff of the question on the Any question on this? Is it important to consider both the control and the timing? Because already looking at the timing by itself is interesting. Yeah, so here the idea is to provide some sort of a Swiss army knife that people can use easily with both. But yeah, we we could focus as well. Any other question? Yeah, I don't see that. So we are going to give you the assumptions. Yeah, yeah, in detail. But so far, yeah, so we do have these two things. We wouldn't, you know, we could look at a pure, you know, like we could look that without the stopping and so on, we want to be able to do everything else. Okay, so the solution concept is that we can see that uh uh a function u is a solution to uh the AGB equation. To the AGB equation, an LP solution if it is in the subordinate space W1 to P, or a value of P that will take to be greater than the dimension of D plus 2. And it solves this equation almost everywhere. So it's in the sub-space for the interior of the domain, and it's continuous over the closure of the domain. We call them LP solution. Okay, we call them LP solution. There are different names in the literature. Sometimes they're called strong solution, also delivered solution. Now, one thing that's important to note is that an LP solution is always a viscosity solution. So when we show existence of an LP solution, we also show existence of a viscosity solution. Can I ask a question here, Penny? Yes. So yt is the interior of the domain. So let me go back here. I'm going to give you everything. I'm going to give you all the assumptions. I'm going to give you all the assumptions. Yeah, yeah, absolutely. Absolutely. So, for now, just note that it's an open, open connected subset. But I'm going to give you more assumptions. Because the equation, I mean, it's like meaningful regardless of the collection. That's right. So let me get there soon. So that's the construction concept that we consider. Many of you are familiar with the serverless spaces, so I'm not going to go through this, but To go through this, but let me state the theorem. So we stated, you know, for any p, we take p to be greater than d plus 2 because that's going to give us the smoothness that we want. Under the assumptions that we state, that includes both the assumption on the domain x and on sigma and so on. This is the unique LP solution of the LGB with linear growth. And in particular, it is also a viscosity solution. And you have And you get all these consequences, meaning that the space derivative exists everywhere and is alpha button or continuous, is twice differentiable almost everywhere in space, and once differentiable almost everywhere in time. Let me turn to the assumptions. So the first assumption, so that's on the set for the state. We assume the frontier is locally leap shift. And I will note that this is. And uh I will note that this is an assumption that is uh satisfied by all convex domains. And in particular in economics we often have a state that's basically lying a positive orthant, and the positive orthant satisfies this equation, this condition, even though of course it is not a smooth domain. The second assumption is that the control is takes place text values in a compact metric space. And now let's turn to sigma. Let's talk to sigma. So we assume that the volatility is measurable. It's Lipschitz continuous in Xt with a uniform Lipschitz constant. It is continuous in the control. And it satisfies this growth condition. Okay, so that this this is the first set of assumptions. So again, did you say something about the lower point for sigma sigma? I'm going to that's the next slide. Yeah. I can see some impatience. Alright. So let's move to the next slide. What if we move to the next slide? So the assumption is that for any compact subset of the domain, there is a lambda that basically that gives you this lower bound on the on the the following. Okay? On the product. And so basically, what we are assuming is that on the continuation region, when you do not stop, you do have this uniform ellipticity. The problem itself is not uniformly elliptic because we do have stopping, but on the continuation region we have this. A small point perhaps of interest is that this lambda, for any compact state to be strictly positive, the infimum, but this could go to zero. This uh this could go to zero as you go towards um the boundary of the of the of the domain. If the domain is unbound yeah, but then you have a C12 solution sorry, you have a C12 solution then. No, no, but you are uniformly elliptic or compact sets I didn't give you the assumptions on the payoff. Yeah, but the function is continuous itself. No? No function is not continuous? No. Okay. No, okay. But in fact, even if you just assume continuity, I'm not sure that I don't think we have this result, right? I think we need C12 to get C1C1. Yeah, the relationship that F and new are measurable and not anything. Yes, so for now we don't we're not going to impose anything. So let's talk about So let's talk about the drift and the depth. So we're going to assume that this Î¼ here is measurable. So in particular, we do not assume any continuity. I mean, we assume continuity with respect to the control, but not with respect to TMX. We impose this growth condition for the drift, where this mu2 is less than this count rate, of course, to get a well-defined problem. Similarly, for the Similarly for the pair function, we assume that the pair function is measurable and it's continuous in the control alpha, but we do not assume that it is continuous in timpt. Okay? And similarly, we have some linear growth condition. Now, let's talk about the stopping payoffs. I'm thinking about strong solutions and your view is only. Strong solutions and your view is only measurable. Yes. That's fine. Is it a possible administrative model can show us calculate the actual constant? Yes. But even if age is a constant, it's not guaranteed. I think we we have So I think we no, I I don't think we need uh we need this uh the the set of uh of strong control I think is uh is not empty but I I mean with other admin conditions or other coefficients once under control solution On sigma, we do impose this condition that uh you get each continuity. Well but as a given control, the strong solution may not be unique. Maybe one question is what if suddenly you tell you say I don't care what's also we should send more and help you with weak plans change more, right? Yes, you can get you can get the result, but we Yes, you can get you can get the results. But we get more. Do you want to stop? So let me give you the assumptions on the stopping pairs. So for the the L D that you get with uh and a stopping, it has to be uh C one two. Be C12 over the interior of the domain and continuous over the closure. The pair function B has to be continuous over the closure of the domain, you get linear growth, and the pair that you get when there is exogenous stopping at the boundary has to be greater than the endogenous space. Okay, so this is the theorem that I gave you before. So, now let me tell you about the proof. So, the first step is So, the first step is to show that the LGB equation has an LP solution. The second step is to show you that this solution or any of such solution is equal to the value function and therefore you get uniqueness. And the third step is to derive the regularity of the LP solution. Now, the first two steps are the substantial ones, and the last one is actually follows from the Maurice-Obels analytic. Let's start with the first step, which is the existence of an LB solution. So we're going to show this existence, and we're going to do this, and we have to face a couple of challenges. The first one is that we have a stopping problem in addition to a control problem, so the IJP is not uniformly adapted. And the way we're going to deal with this is by some kind of penalization method that I will show you. And then we also have a general domain. And then we also have a general domain, which is lip sheets, but not necessarily smooth, and it could also be unbounded. So we're going to take some sort of a limit argument. So let's start with the case in which the domain is smooth and bounded. We're going to show the existence of an LP solution for this case. So, as I mentioned, the difficulty is that we have disturbing problems, so we don't have Distopping problem, so we don't have a uniformly elliptic problem. So I'm going to show you this slide to give you the idea. That's not exactly something that we use because there is an indicator function here, but we're going to transform the problem of control and stopping into a problem of pure control. And again, that's not exactly what we're doing, but just to give you the idea, here we basically dropped the stopping part of the AGB equation, and instead we replaced. Equation, and instead we replace it with this right-hand side. So, what does this right-hand side tell us? If u is greater than g, so the solution is greater than the stopping value, then this term is equal to h plus, so this h plus term, so you get h plus 1, h plus, so you get zero. And basically, you get the Benman operator equal to zero, which is the first part of the, of the, uh, of the uh Of the AJP equation. Now, suppose by contradiction that little u was strictly less than g. In this case, we get capital G is equal to this minus H plus, and H is given by this formula. So, I'm going to simplify a little bit, but basically, you get this big G of little U is equal to big G of ital G. And then basically, you can use some sort of comparison principle to get Comparison principle to get uniqueness, and you can show that in this case, little u is equal to g. And so, what you can show by contradiction is that little u can never be strictly less than little g. And so, that's how basically we embed the stopping problem into what looks like a pure control problem. But this is formal, literally, because u has to be somehow regular for this to happen. Sorry, u has to be at least continuous for u equal to g and then you replace the. You replace the invertis. You mean like to apply the comparison principle? I don't know what comparison principle you're using, but I mean, at least continuity is needed because the region on which u is equal to g could be very, very bad. Right? I mean, and if u itself is very bad, then the reverse is, I mean, I don't know how analytically it might be a bit difficult. Right, so here, let me just go back to the segment. Just go back to the segment. So here we start with a little U that's going to be. We assume it's discontinuous. This is what I said. Yeah, that's the existence. I mean, I have to show existence to this one, but I have to go to the other one. So we are not using this particular representation because here you have an indicator function that breaks the continuity of the operator. So instead of using this specific form, we're going to Using this specific form, we're going to use the same thing, but with a phi epsilon here, where phi epsilon is C infinity, and basically approximate the indicator function from before. And so for each epsilon, and phi epsilon is going to converge to the indicator function. And what we do is that, so for each epsilon, we're going to have an L P solution of the previous problem T epsilon. And then we have to show that this solution converges. This solution converges to some limit q that solves the problem as epsilon goes to zero. So, to do this, we need to show that the estimates that we have are valid independently of epsilon. And so we use the fact that the estimate that we get of the domain are independent of epsilon because these estimates depend only on some sort of Gipsy. On some sort of Gipschitz parameter of the domain that we consider. So we can use some existing results that give us the estimates of the solution for each epsilon and show that these epsilons are independent of epsilon. And then we need to take the limits and that's sort of a more standard standard argument. Okay, so now Okay, so now once we have done this for a smooth bounded domain, we have to take the limit when we want to have a general domain. And here we use a result by Doctor that says that if you start with a Lipschitz domain, you can obtain it as a limit of smooth bounded domains that have the same Lipschitz parameter throughout the the sequence. So when you do the approximation again, you can get some estimates that are independent of the sequence that you take. You get some nice convergence. Tech, and you get some nice convergence. So, now let me tell you about the second step, which is to show that the solution, the LP solution that we have, coincides with the value function and is therefore unique. So, these are the steps that we use. So, first, that's an easy step. An observation that any IP solution, U, has to be above the value. U has to be above the value function. That's a very standard result. Then I'm going to do a step that maybe Niza would say is sufficient, and then you can just stop there, which is to say that you can just look at weak control, meaning controls that generate a weak solution to the SD. And then we know that there exists an optimal weak control. And so if we call w the value function of the problem Of the problem with a weak control, we can do the verification of the standard verification argument with this optimal weak control, and we get immediately that the solution, the LP solution, has to be equal to the value function obtained for the weak control. Now, whether or not you're interested in a strong solution to SDs, you can be interested in the proof as to why this works also when we have a solution to strong SDs. Strong solutions to SD. So we're going to Strong solution, so easy. So we're going to do this in two steps. The first step is to say: suppose that the pair function f is continuous in T and X, and then we're going to show that the optimal recontrol can be approximately well approximated by a strong control. And so in particular, this will imply that the value function obtained when you just look at admissible control that generates strong solutions, so as Ds, is actually equal to the value function when you allow yourself to have weak solutions. Okay? And from the previous step, And from the previous step, since the DLP solution has to be equal to the value function with weak controls, we conclude that it also has to be equal to the value function with a strong control. So that's the interesting set. So let me tell you a little bit about this. We're going to do a two-step approximation. So let's first assume that the control set is finite. We can also take a limit respect to this. So with a finite control set, So with a finite control set, we're going to represent the control as a distribution. So in particular, suppose that the control set was binary, so you have a low action or a high action. Instead of saying that you choose A lower bar or A upper bar, you can say that you're choosing a weight that you're putting on these two things. So of course, if you're choosing a pure control, it's going to be either all the weight on A lower bar or all the weight on A upper bar, but in general you're choosing some kind of distribution over this. Some kind of distribution over this thing. When you do this, you can rewrite the volatility in the ESD as this sum. So that's the weight that you put on X times, sorry, on A, times the relativity for this particular value of A. So you can always do this. In the original problem, these alphas were values were values of 0 and 1, but in general you can approximate any control by some Lipschitz control in X. Okay, so for example suppose that the original control was putting weight 0 on A opposite bar for X less than 0 and weight 1 on A opposite bar for X greater than 0. We can replace this step function by a Lipschitz function, a weight function, that increases very quickly around 0. Very quickly around zero. So, by doing this, we basically obtained the Lipschitz control. And now we have an assumption that the original relativity was Lipschitz in X. And this alpha bar, the approximation is now also Lipschitz. So the combination sigma bar is also going to be Lipschitz. And that's going to give us the existence of a strong solution to DSV. Doesn't go for standard results. Standard result that tells you that in problems, value functioning, relaxed control, weak controls, strong controls, they are all equal, provided that the rewards have a little bit of some continuity. So mu is not regular or? So, I mean, here we are assuming that the p uh f is continuous. We do not assume that mu is continuous. So that's the first approximation that we make. Now, in fact, of course, this is not an admissible control because we sort of introduced this mixture. So now we have to replicate the mixture with something that actually works. So we're going to purify the convexified control following some techniques that are in the literature. In particular, we're going to The literature. In particular, we're going to look at a simple control process that is piecewise constant and that approximate this mixture. So, for example, if I was supposed to put weight one half on A lower bar and A upper bar, you can replace this by switching the required between these two things over time. And you get this strong solution. And as you refine the approximation, basically, what you get is that the value function for the when you restrict attention to strong controls is equal to. To strong controls is equal to the one with the weak controls. Okay? Take my time here. I'm almost out of time, right? Okay, great. So then you have to do an argument, because this was only for the case in which the control was finite, so you need to take the limit when you have a general compact control space. And again, you can use some sort of diagonal. And again you can use some sort of diagonalization and you need to use some sort of stability principle or some pervert continuity of the operator. Now, in fact, we show something that's stronger. So we have uniqueness of the LP solution because it has to be equal to the value function. We can also show It has to be equal to the value function. We can also show the uniqueness of the viscosity solution using some comparison principle. And the comparison principle that we use is a comparison principle that applies to unbounded domains because that's the domains that we have. And it also applies to discontinuous functions f and mu because we also have this just this assumption of measurability. And using this comparison principle, we basically show that this was. Show that the viscousity solution has to be equal to the LP solution, and we get uniqueness of a viscousity solution. Any question? Okay, so a consequence of this is that the value function is an FP solution, so it is a subof, it is in the subof space. In the subway space. So now we can use the embedded theorem to conclude that the value function has all the smoothness properties that we claimed at the beginning. In particular, it is continuously differentiable in space and the derivative is alpha order continuous. It is twice differentiable almost everywhere in space and it is once differentiable almost everywhere in space. And it is once differentiable almost everywhere in time. So P, yeah, it's NEP that we we it works for NEP, but we assume that P is greater than T plus 2 in order to be able to do the embedding. Okay, so let me conclude here. So the value function is the unique LP solution. So in particular, it solves the LGB almost everywhere. And so we can use, we can treat the LGB as an actual equation. We can treat the AGP as an actual equation for this compactive static analysis. We get smooth pacing in space, and we could impose stronger assumptions on the primitives in order to get unique classical solutions. That's related to an earlier question. I think what we need to assume is C12, right? So the So the same technique that we have could also work if instead of having uniform elasticity for the state variables, we had some state variables that are deterministic that basically you would play the same role as time. And we could also allow the discount rate to be Markovian in the state. Okay, so this is basically my account of this. So if you have any questions, I'm happy to take them. If you have any questions, I'm happy to take them. Yep, I mean, it is known that in the PD literature, you didn't start with, I wish you did, to show us what PD people knew and then what you were improving upon. If every I mean, in the P D literature they seem to take everything smooth and prove C one one, which is which is Better than that. And so, if I understand it correctly, the main thing over here is to do the same result with assumptions of the coefficients. And the main, I mean there are a lot of beautiful things going on, but the main regularity result is the part that you didn't show us, which is the uniform W two P estimates and and modules of continuity, right? Modules of continuity, right? I mean, that is the Achilles of Hill of the existence of a LP solution, as you call it, is the main thing, and it comes from the known results, and then you get some uniform estimates as to the other things. I mean, verification and all this stuff is, of course, has to be done, but the main thing is that once you get the at-peace solution, you can do many, many things. That's the existence. Yeah, a lot of these things are basically folk knowledge, but we couldn't find any sort of result that exactly corresponds to this. But I mean C11 regularities, I guess, exists everywhere, right? Starting from C11, regular solutions, and then you do approximation and then you get the uniform estimates. It's this one that you were referring to, a paper by Crandall and somebody else, also. Right, that is the part that is the technical. Part that is the technically the difficult part is that want to get uniform estimates. And what makes the uniform estimates work is the structural assumptions you deal with, I guess. You mean on the on the domain? No, domain is Lipschitz. Domain again is