Yeah. And the idea, I guess, from this whole week is to give some ideas, to cross-fertilise between the groups and so on, and also to throw out some problems that we haven't yet solved. So I'm a chemist, just to give my background so you know where I'm coming from. And what I wanted to do in my talk was, so while the title is, mentions Gaussian wave packets, I will come to that later on, what I actually wanted to do was to do sort of an overview of the families of methane. Overview of the families of methods that we've been developing over the years to solve the time-dependent Rhodian equation. And give some ideas and some of the sort of the details as to how it works, what we're trying to do, and a lot of the properties of the methods. So just as sort of background, what we're obviously trying to do is, certainly my thing, we are simply trying to solve the time-dependent Schrodinger equation. So this is it from sort of chemist, chemical physics perspective. Chemist chemical physics perspective. And yeah, so we've got a wave function, a Hamiltonian, and that gives the time derivative, obviously, which we can move forward in time. And by being able to do this, well, this is just a sort of little example. Why do we want to do this? We always have to give our motivation, at least in sort of chemistry, as to why we want to do something. And there are actually sort of a whole class of problems that you can solve when you solve the time-pen-Schr√∂dinger equation. This is not all of it, but particularly at the moment of interest for me. Particularly at the moment of interest for me are things like time-dependent spectroscopy. So, just as an example, this was sort of one set of experiments that were done by a colleague in Wurzburg, Ingo Fischer, on cyclobutadiene. So, they'd measured the time-resolved photoelectron spectrum. This is just the sort of the photoelectron spectrum, but they weren't entirely sure whether they'd made cyclobutadiene because it's very unstable, and in fact, it didn't really match anything in the literature. So, we did some calculations solving the time-penetrating equation. Solving the time-penetrating equation, and our calculator spectrum, which is the blue line here, matches the experimental spectrum very well. So, they were actually quite happy after that that they had indeed made what they wanted. So, it's really quite a sort of support for experiments in modern ways. Just clarify the notation of what QR are. Ah, I will come on to QR. Yes, DrudgeNet. So, QR. So, this is, yeah, that's true. This is one of these things. It's important. So, please do ask questions on the way through because obviously we have our implicit notation. Because obviously, we have our implicit notation that we assume people will know. So, general, so that the wave function, at this point, this is a wave function, contains all the particles. So, Q are the nuclei and R the positions of the electrons. This is a sort of full system. Okay, so just briefly for how we sort of go down, because we started out, we just have the full Schrodinger equation, all particles in the system. It's a lot of particles, so we want to sort of obviously simplify things. And the first thing that we actually do. Things and the first thing that we actually do is we get rid of the electrons because there's a lot of those and we want to focus on the nuclei. So we start by saying, okay, this is our full Hamiltonian, so this is our Hamiltonian of the nuclea and the electronic coordinates. We sort of pull out the kinetic energy for the nuclei, and then we have the electronic Hamiltonian, and this contains the kinetic energy of the electrons and all the Coulomb interactions between the particles. And so to solve this, to simplify things, we use what's called the Born representation. We use what's called the Born representation. So we write our wave function in terms of, well, a sort of set of nuclear functions or functions of the nuclei and a set of functions of the electrons. And we talk about these being parametrically dependent on the nuclear coordinates because we obtain those by solving what's called the Clamp nucleus Hamiltonian. So this is the electronic Hamiltonian with the nuclei in one particular configuration. So this is the So, this is the classic thing. There's a whole branch, quantum chemistry, as it's called, electronic structure theory, which is all about solving this problem. So, for a given Hamiltonian, so take our nuclei in some configuration, you can then write down the Hamiltonian, and we can solve that to get the eigenfunctions, which are the electronic wave functions at a particular geometry. And so, what this then does is it gives us potential energy surfaces for the nuclei to move over. Like to move over. So if you take this ANSATS, put it into the time-dependent Schrodinger equation, and you integrate out the electronic degrees of freedom, you then get it in this form here, where we've got this kinetic energy operator form, we've got this extra term here, up at the moment, and the potential energy surfaces. So your big R is the same as Q? Ah, yes, sorry, that's that's yes, sorry, that's a Q. Yeah, obviously. Yeah. Obviously, sort of when rotation comes first, this big R is Q. That's meant to be. Thank you, Tucker. Yeah. Right. So this gives us, so what is this F? And this F here is, well, this is the electronic wave functions, and this is the derivative of them with respect to the nuclear coordinates. And this is something we call the derivative coupling vector. Okay, so this is effectively the coupling between the electrons and the nuclei. The electrons in the nuclei. And this is sort of one of the important steps implicit in just about the whole of chemistry, in a way, is that we assume that F, so we've got M here, this is the effective nuclear mass, so this is a big number because the nuclei are about 1800 times heavier than the electrons. So we've basically got F over M. And so if you take the approximation that, well, F over M is very small or close to zero, you can throw away that term. And then we get what we call the adiabatic approximation. What we call the adiabatic approximation or the von Oppenheimer approximation. Different people use slightly different words for what the exact approximation you use, where the coupling between all of the electronic states disappear, and we just have this picture with the nuclei moving over a potential energy surface. And that's the sort of the standard thing in chemistry. That's how we describe molecules. This is cool to say, adiabatic surfaces. However, we have this problem that these derivative coupling vectors, you can actually write. Derivative coupling vectors, you can actually write them in this form. And so we've got a one over the difference in energy between the potential surfaces. So when they become close, or worse still, if they actually become degenerate, this thing obviously becomes the same. And this leads to things that are called conical intersections. So this is an example of some potential energy surfaces for a particular molecule, doesn't matter what, that's your buttotrie, doesn't matter, in two coordinates. And you've got this point here where these potential surfaces meet. Here, where these potential surfaces meet. So, this is the point where this thing becomes singular. So, this is obviously a problem for any sort of simulations, or even any sort of calculation. So, we want to get rid of those. And the way we do that is, well, we sort of separate things out and we try and rotate away these singularities. So, firstly, we separate out a group of states. So, you've got your Hilbert space of all of the states, and you want to say, okay, if I can take that a certain set of coupled, and the rest of the state. That a certain set are coupled, and the rest basically can ignore the couples and its high-like states. It's quite an important sort of point, but we sort of assume that we can do that. Then I can take this, what we call the group-born Oppenheimer, and then we can rotate the states, just a unitary transformation of our electronic wave functions to a new set in such a way that you get rid of those singularities. This is then called the diabetic nature. And this is the condition for that to be the case. Now, so we've got that for this transformation matrix. For this transformation matrix multiplied by these derivative couplings, it gives you a derivative of that matrix. So we can sort of find what that transformation matrix is throughout space. That's the idea. And when you have that, you then end up with this form here. So these are the vectors of, you've now got electronic functions on all of the different potential surfaces. This is now our potential matrix. So we've got couplings and surfaces, if you like. So these are like the diabetic surfaces. If you like, so these are like the diabetic surfaces. These are actually the same surfaces as the ones before. It's just in the other representation where you don't see the coverings. Seems sort of sensible enough. The problem being that you can't explicitly, or you can't actually do this exactly because of this approximation here, because we've obviously truncated our space. So if that's a reasonable approximation, we can do it within this subspace. But still, when we can do this, we now have the Hamiltonian really that we want to solve, this one here. Want to solve this one here. Sorry, the Schrodinger equation. Okay, so how do we actually then solve the, take the next step and represent our wave function and solve that to propagate it. So to do that, we take the electronic, so the nuclear wave functions, just the nuclear coordinates for all of our f degrees of freedom, and we expand it in a direct product. So we just have a set of basis functions for each degree of freedom. For each degree of freedom, this is the sort of a starting point, with time-dependent coefficients. And then we use the Dirac-Fracle variational principle, Takey's variational principle, which is just saying that variations in our trial wave function have to be orthogonal to the Schr√∂dinger equation effectively, and we can solve that using this sort of trial function to get equations of motion for these coefficients. And so this gives us this picture. This is sort of the chemist's picture. Is sort of the chemist's picture that you have these basis functions then form a grid, and the coefficients then here are the amplitudes of the wave function on that grid. And they just propagate, you can propagate all the time. And it's very sort of straightforward, and you can just sort of put it in the different forms. So this is just in nomenclature. If I take one of these sets of products of the functions, that's a configuration. So that's just one of the configurations. And of course, we can take that down a step further. It's as simple as we've got a matrix. This is our Hamiltonian matrix. This is our Hamiltonian matrix in our basis set. We've got this vector of these expansion coefficients, and that gives us a very simple sort of set of equations to solve. It's actually sort of not that hard to do. It's all down into the sort of basis sets you choose. Short Chuck will be telling us more about other ways of doing these things. I'll come to a moment for more details. The big problem is, though, that this thing scales exponentially. This is this exponential scaling that underlies quantum mechanics in general because everything is coupled to everything. General, because everything is coupled to everything else. So, if I've got n basis functions per degree of freedom, I've got n to the f coefficients, and that means that you can't do very large systems. So, to get around that, or at least to move to larger systems, what we do is, well, we take a similar looking ANSATS for the wave function, but now we make our basis functions time-dependent. So, these time-dependent basis functions that we refer to as simple particle functions. We refer to as single particle functions, are in turn expanded in these time-independent functions. So we have still this underlying time-independent grid representation, and we have these time-dependent functions on top. And the idea of doing that is, well, we can again use the Dirac variational principle to solve now for not only the coefficients, but also for these functions, so that we have the optimal functions, so that we have an optimal basis. This is a contraction. On the basis. This is a contraction. So if we need large n grid points, we need small n basis functions, and this is the saving of time. Okay, so this is the multi-configurational time-dependent hard tree, the MCTK. And that actually works really quite well. The only problem now is that we have, of course, more things to solve for. We don't just have the coefficients, we also have to solve for the basis functions. And, well, so what do we have here? Well, for a start, we have these non-linear. Well, for a start, we have these non-linear equations of motion. So they're harder to solve because the original time-independent base is a very simple problem. You just calculate the matrix of the Hamiltonian, and then you just do matrix vector to propagate forward. Whereas now here, we've got these sort of coupled equations. We have things like, so there's a projector, this is projecting out of the space of the functions. I haven't actually picked that up as performed, but it's just the projector onto the space of these basis functions to ensure that. These basis functions to ensure that basically they only change as required. This is a sort of outbush. This is basically a density matrix, which is about how populated these functions are. And this is then a mean field operator, which is coupling all the different sets of functions. And so now we've sort of taken our memory, the amount of effort we could save out to small n to the power f plus a certain amount for these functions. And that's actually really, yeah, a sort of good thing to do. So just as a few notes on. A few notes on what's in here, these various points. This is sort of a mass meeting, so I can show some of the details. So you can write all of these things, and we'll sort of come to maybe some of the other points when I get on later in the talk. So we define these things called single hole functions, which is basically the wave function for just ignoring one of the sets of basis functions. So you make a hole in the wave function. That's this thing here. We made a hole in this one degree of freedom by not including it in the wave function. Not including it in the wave function. And that's how we write these mean field operators. So they're the Hamiltonian in the basis of these single whole functions. So it's an operator of the one degree of freedom. And in a similar way, we get these density matrices. They're just defined in terms of these single whole functions, which you can relate to in a way. The populations, the importance of the basis functions. It gives us a degree of how good our calculations are. We now have the problem. We now have the problem. Well, in the original sort of version, we had time-independent functions, we have time-dependent functions, and so our Hamiltonian matrix is now also time-dependent. So you've got to calculate it many times rather than just once. So you want to do that efficiently. And to do that efficiently means that we require the potential to be written in a particular form. So it's in this, what's called a sum of products form, also often loosely referred to these days as the LCTD. Often loosely referred to these days as the LCTDH form. So, as long as it's in this form, we've got low-dimensional one-dimensional operators, in a sort of products, in a sum of products form, then you can do these all in small low-dimensional integrals to relatively efficient sort of requirements. Okay, yeah, that's just, well, so how do we actually do it in practice? This is just some words. So, we have these matrix elements that we want to solve the whole time. That we want to solve the whole time. This is obviously a big matrix because, in principle, I might have reduced this down in size, but I still had to use this full space on the full grid to do these matrices, and that's why we want to bring them down to those one-dimensional products. And then we use these sort of different basis sets, discrete variable representations, collocations, and various other things, to actually do that efficiently. But you want to do it also not only efficiently but accurately, and these discrete variable representations. And these discrete variable representations is basically a quadrature that you're doing a quadrature for these integrals. Okay, so just to sort of some properties. So we've got some nice properties to these equations. So the one thing is that because it's a variational solution, so by construction, we've got, in principle, the optimal basis set, it does converge on the exact result. So as we add more basis functions, we converge on the exact result, which is nice. The exact result, which is nice. The norm's conserved, the energy is conserved, which is important. However, and this is sort of one of the problems which has been dealt with actually partly in the meetings that we've had in the past. So things like we've got this density matrix, well, that is actually singular, usually at the beginning of a calculation. So we've got all of this sums of all of the different configurations, but at the start of the calculation, obviously this is an initial value problem, so we have to set what our initial weight. So, we have to set what our initial wave function is, and this is effect setting what's the experiment that we are simulating. So, you'll do something like you have that at the start at time t equals zero, we're in some configuration, and we have a ground state wave function that appears, some particular eigenstate, and then we'll do something to it, and then dynamics happens. But because we're only in one configuration at the start, then of those expansion coefficients, one has a value of one, and everything else is zero. So, of course, all those density matrices, elements, are zero, except for one. Are zero except for one. So you have a problem that we are doing. We have to invert that. So, of course, if it's singular, then how do you deal with that? And traditionally, we just deal with that by regularizing it. So you get the eigenvalues of that density matrix. You regularize them by adding small numbers. You could use single-value decomposition or whatever. And in general, And in general, this is the sort of the difference. We assume that actually we're only affecting by adding small numbers by doing this great linearization or SPD or whatever. All you're doing is you're affecting the numerical behavior of functions that don't actually have any population, so they're not important. So we're just making some sort of minor error. And of course, that doesn't bother us so much because, hey, you get a result, but obviously it's something that, particularly, mathematicians don't like these sort of things when we say things like, well, it's probably not doing much that's particularly important. Probably not doing much that's particularly important. And it has actually been investigated quite a lot, particularly by Christian Lubisch, Hans Lee Temeyer, and others. And they've developed other integration schemes because in general we use just sort of standard integrators. But they have, let's say, Christian has developed a number of integrative integrator schemes that directly get around this problem of this inversing matrix. Okay, so there's one. So, there's one thing there, or the next step. So, that's all well and good, but we're still not able to get to really large systems because while we might have come from going from n to the power f, we still have this exponential scale. How do we get past? I mean, the exponential scaling in the original version, yeah, you could do maybe three degrees of freedom because about 100 basis functions per degree of freedom, so it gets very big, very fast. These days, of course, there are. These days, of course, there are nice ways of doing that, so you can get up to five or six, maybe, but it's still not great. Size, and likewise, when I sort of showed there, so MGTDH as written here, these are all one-dimensional basis functions. So we still have this exponential scaling with the number of degrees of freedom. We can reduce that, and the first sort of step, and this is the sort of conceptual step that will eventually get us to the Gaussian wave brackets, is that we can make these basis functions multi-dimensional. They don't have to be one-dimensional. Dimensional. So that rather than having this as a set of sums over all f degrees of freedom in the system, we can reduce it by having some of these be two or three or maybe even four dimensional. So I've got fewer dimensions effectively. That's just an example here. If I have a three-dimensional wave function, I could write it with three one-dimensional sets of functions, but I could also have a two-dimensional set. And then, of course, I've only got two indices here, so I've got squares. So this is reducing. These then become. So this is reducing. These then become harder to propagate because they're on multi-dimensional grids. But I've got fewer of these, so you can balance the effort. And by doing that, you get a saving in memory, sort of fairly clear in a way, just because we go from n to the f to n to the p, so the number of these sets of functions that we have. But you have to balance that because this term and this term, you know, as this gets smaller, that gets bigger, so you want to sort of balance those. But anyway, that allowed us. Sort of balance those. But anyway, that allowed us to do these, sort of, allows us to do in this way with entity TDH around about 30 degrees of freedom. That's actually relatively straightforward. And then we can take it a step further, and this is the sort of state of the art at the moment, is that you can take it to the sort of the next step of logic and say, well, we've got our wave function, we're expanding it in these multi-dimensional functions. Well, we could, in turn, expand our multi-dimensional basis functions in terms of a number. Functions in terms of another set in this sort of same MCTDH expansion. And likewise, these can then be expanded on down. So you've got this layers. This is called multi-layer MCTDH. So we're getting a layer of functions. So these are high-dimensional, getting down to low-dimensional, getting down to low-dimensional still, until you can actually handle the calculation. So we have these recursive sets of variation equations of motion. And this has particularly been driven by sort of Uganda Manta and Hauking Wang in particular. And Haobing Wang, in particular, sort of really Hau Bing Wang sort of pushed this forward and had thing. Hau Bing's latest calculations do of the order of two to three thousand degrees of freedom. So these are huge calculations that you can do. And you can represent these things in terms of trees. So this is how you can imagine breaking this down. So we've got here, this is our sort of overall wave function, and these nodes, the different layers. And these nodes, the different layers. So we've got the top layer, the next layer, the next layer, the next layer, and so on down. And at the bottom here, these are still our one-dimensional time-independent grids. So we've still got that exact same representation at our bottom. And then we layer these things up. So we've got one-dimensional functions, three-dimensional functions. These are now, what have I got here? Eight-dimensional functions and so on, building up on this tree. And this is basically a huge tensor contraction of the underlying grid coming up to build the whole wave function. Be coming up to build the whole wave function. And it relates to there are people in physics, in the physics world doing tensor network trains, DMRG, these are all different variants of this idea. And so they're really very powerful. So we can represent a wave function. We can propagate the wave function. Great. So we then have, this is just a little example of the sort of, you know, you could say accuracy or the sort of things that we can get out of that. So just a brief bit of chemistry. Bit of chemistry, probably aren't too many chemists here, so we put a bit of chemistry into the room. So, this is just a little study. I was involved in this with the group of Rachel O'Reilly in Birmingham. These are these maliumides, so this ring, don't mind about the X's, they're just substituents, and by changing these substituents, you can basically tune it and get any colour you'd like. So, they're really quite useful molecules. So, we can, I haven't talked at all about how we actually get the Hamiltonian, but then we actually need, of course, for our molecule, and that's why it's all very well to be able to sit. It's all very well to be able to simulate 3,000 degrees of freedom, but you obviously need some sort of operator that you're going to put in there. And for molecules, you have to for each molecule. This is really one of the sticky points. I mean, Halvin works a lot with models like spin boson models and so on. You can parametrize them. You can make those as big as you like relatively simply. But if you really want to look at a molecule of something like this maliamide, you have to actually calculate the potential surfaces. And I'm not going to go into how we do that. And I'm not going to go into how we do that, so we have to get them into this diabetic form and stuff. So, we basically, the points here are all calculations from quantum chemistry calculations, looking at the different electronic states, and then we fit to these to give our diabetic potentials, and that gives us our hand-to-be. And then from that, we can calculate a spectrum. So, this is an example of a typical calculation. This is the sort of the scheme as we imagine it. So, we start, this is our ground state potential with our ground state wave function, and we excite the molecule. Function and we excite the molecule, and then the wave function propagates in time, it evolves in time, and from that we can calculate the spectrum. It just comes from the Fourier transform of the autocorrelation function. So the evolving wave function overlapped with the initial wave function, you Fourier transform that you get a spectrum. And so these are the experimental spectra when it excites two particular states, and these are the calculated spectra, because I actually agree really very well. The sorts of things that we can do. Okay, so these are the sort of the problems at this point. This is our sort of our sticking point effect. Well, the scaling, so we've got these powerful methods. Yeah, scaling is a problem, especially for the standard MCGH method. And if you're doing multi-der MCGGH, it's actually just how you build this tree. Because we've got this tree structure here, and how you actually build. And how you actually build it, which of these modes you put together and what point on the tree makes a huge difference to the actual efficiency of the calculation. You can change calculation times from hours to days just by doing it in a different order. Because all these different degrees of freedom, they couple differently. They have different characters because of the way, you know, this is a mode where, okay, when you're sort of used to doing these things, you can sort of see here if I excite onto this surface, well, you're going to clearly do some dynamics. Well, you're going to clearly do some dynamics, which is crossing with this surface, so you're going to get something complicated going on here. Whereas, if I, for example, excite up here, it's not much, it doesn't really couple to them. So, this is a mode that doesn't need to be, it doesn't need as much effort to describe it. So, how you have those and how you fit them together makes a big difference. Right, so that's the sort of the problems there. We also have, well, yeah, you have, of course, an error during the propagation. In general, we try to ignore this. In general, we try to ignore this fact, but of course, we are doing an integration. So you start with some initial value and then you use some integration scheme to go for all your time. This has a natural error in the X. So if you propagate for too long, you have an unknown error by the end. In principle, if you propagate back to the beginning, you should come back to where you start. That's never actually the case. We will have drifted quite away. So these calculations are only valued really for short times. And then this is the third point, and this is really what I will get. And then this is the third point, and this is really what I will get onto here because this is the bit that I'm most occupied with at the moment: is how we actually obtain the Hamiltonians for systems without spending about two years of work just for doing one molecule, which is quite that way these days, but still one of the big efforts. Okay, so one way that we can go to the next stage is that, well, we've got this grid-based representation, and they have certain restrictions. You have to have the Hamiltonian in certain forms. To have the Hamiltonian in certain forms and so on. So the development route we're going down, and this is particularly together with the age burkhart. What about if we, rather than having these grid-based functions, we take some parameterized functions. We don't have grids, we have parametrized functions. And then the obvious sort of set of parameterized functions are Gaussian functions. Gaussians are used everywhere. They're actually particularly horrible things to work with at the moment. But anyway, they have sort of nice properties. Everyone knows about them and they're used for everything. So if I take For everything, yes? So if I take my set of functions, so these are these, as it happens, you can mix and match. So you can have some functions on grids, and some of these parametrized functions, whatever you want at this point, this is the GMCTDH method. And so we have our functions, and these just have a few parameters describing them. So you can again just work out the variational equations of motion for these parameters. And so I now have, you can see the change, this is the same. The change, this is the same idea for these expansion coefficients. So that was the original part I've got. I have to now worry about the fact I've got this non-orthonormal basis because Gaussian functions is a non-orthonormal set. So I have to worry about, I don't know why that's kind of the different font, they're just the overlap matrix, you have to worry about the overlap between these functions with the universe, and you have to worry about the time dependence of that overlap. So if I had some of these functions, they have exactly the same form for the Exactly the same form for the propagation they had before, and I'll go into a little bit more in a moment. The equations of motion, this is just all of the vector with all of the parameters from here brought together, and it just has this form, which I'll be going to in a moment. The imaginary unit in the expirational complex numbers. Yeah, sorry, I should have said that. Yeah, sorry, I should have said that. Yeah, these are all, these are all, yeah, these are complex numbers, so they're all three. So I've got a, at the moment, this is not written, I'll come on to the sort of the standard form of the Gaussian wave pack in a moment, but this is just in a very general form where I've just got the bilinear, linear, and stained parameters, and they're all confidence. So whatever, yeah, that's it. So is it transpose or transpose convent? Is it T or star? Uh that's T 'cause these are the coordinates, so they're real. Yeah. Coordinates, so they're real. Yeah. Okay, so this is what we have in these two, so this matrix was actually a tensor and this a vector. So we have these rather complicated-looking things. So this part here, this y, this is the bit including the Hamiltonian. And what we have here is always, you see here, it's a projector. So again, we have this projector out of the space that the Gaussians are in at the moment. Yes? That's part of the. Are in the moments. That's part of the basic part that comes from being a variational optimizer. And then you always have these parts here. Where we have this is from the variational character, the derivative of the Gaussian with respect to its various parameters. So that's what we have in here. And this density matrix comes in again. So this is the one with the Hamiltonian, and this C is the one without the Hamiltonian, effectively. This is just then to do with the overlaps between all the various Gaussians. So we have, you could say, an extended overlap matrix. An extended overlap matrix, so it's not just a Gaussian with a Gaussian, but the derivatives of the Gaussians with the parameters. And that then comes down to, as you can see, you can write all of these as I've just written the first one. These then just come down to being moments of Gaussian functions, because you've obviously got the derivative with respect to one of the parameters. That's going to give you an x or x squared or whatever. And so these just become Gaussian moments. And these then become the Hamiltonian with extra parameters as well as, yeah, but. So that's, yeah, but this is sort of something not so nice. So do you go back to the previous page? Is the S matrix matrix you have here, is it the same S matrix? Is it different? Which means? This one. They are the same. If I put, so if I have the top line, the top line is overlap matrix of it's the same here. It's the same here. It's just if I wrote that with a 0, 0, this was Evain's nomenclature. If I had a 0, then I'm just taking the derivative with respect to the scalar parameter, and that just becomes the Gaussian. And if you go back to the previous page, is the S also just for Gaussians? Is the S on this page, say that S, it's just Gaussians? Yes. Yeah, it's just the Gaussians, yeah. So then, so your phi, the phi H phi, so an equation for A dot. So in the equation for A dot, the phi K H phi, that phi. Or this one. Yeah, so it's a multi-dimensional thing. This is just one of the configurations. Right. Yeah, so the label K there labels a configuration. That labels a configuration, yes. But in the S it doesn't? It will be. Yeah, yeah, it's just an overlap. So it's an overlap of a configuration with a configuration. These parts, if you have them, they're orthonormal and these aren't, but you can still just build the full overlap between the configuration. Build the full overlap between the configurations. So that does imply, though, that S is not the same as the S? You're right in that sense. Okay, yes, yeah, yeah, yeah. That S is then all of these multiplied together. Yes, it's the direct product of all of the low-dimensional ones. Yeah, that helps. Yeah. Yeah, so in that sense, absolutely. The magnitude otherwise it gets horrible when you have URL. That is the overlap between the configurations and this one here. We're talking about the overlaps between just the gap. About the overlaps between just the Gaussian functions. Sorry, Graham. So I guess this matrix is computed analytically. Yes, you mean all of these, yeah. All of this is analytic. So you have some assumption for the age? That's what I would come to. Absolutely. Yeah, because in principle, of course, these you can all do analytically, they're just Gaussian moments, but then yes, if you could do that fully, but we will have to make some assumptions so that we can do it simply, and then we will end up still being able to do it analytically, but approximately. But approximately. One of the problems to be set, you see, problems and questions, right? So, this is sort of where we are from this. Well, the advantage of this, in a moment, sort of connect it to various other sort of Gaussian wave packet type methods. But because it's variational, again, by construction, it conserves the norming energy if we have the Gaussian functions or not. And these Gaussian functions, they're spatially unrestricted. We haven't had, say, a grid, which is nice. They can sort of go where they are. Go where they are. Well, you could say we're going to need more of these functions than these functions. You might think this is not good, but we've got this small set of parameters and you can make them, therefore, more multi-dimensional. That was the original idea. Why do we need more of them? Well, we need more because they're restricted. They're not quite as flexible to get the same accuracy. So these then are problems, if you like. Well, we have this non-orthogonal basis set, which Basis set, which brings general issues. Yes? Big problems, you get linear dependencies happening, so this inverse of this, these overlap matrices become singular. And then we have the, you could say, the compounded problem on top of that, that if that becomes singular, well, I have to invert this thing as well. So I'm actually starting to go, I'm inverting something that's numerically horrible, which is not great. You then also have the problem. This is something that took us quite a long time to work out what was going on, but it actually. A long time to work out what was going on, but it actually has a slightly pathological character in that as we go towards completeness, so as your basis set gets bigger and bigger, and actually doesn't really need to do very much, this C matrix goes to zero, which is a problem. In a way, it makes sense. It makes absolute sense, by the way, because you can see it goes to zero because as you approach completeness, well, the projector becomes zero, so the whole thing just disappears. And it makes sense because basically. And it makes sense because basically when you've got a complete basis, this thing is totally undefined, they don't have to do anything. It doesn't know what to do, because the various you have the fues that you need, so why move it? Which is a problem when you've got a method that you want to take from doing something. So it works relatively, or really, initially, really quite well when we had a very small basis set, but as you started to improve things, it just didn't really propagate very well. So we get around that in a rather crude way. In a rather crude way, you can look at this, you basically just include the part of that matrix that is non-zero. Anyway, this is our sort of list of things. So, these are the sort of problems we're still dealing with. And then the other problem then comes, so this is the sort of the numerical instabilities, which you can get around with, you know, you can do things like SVD and regularization on that to solve that part. As I say, we get around that by just you take out, say in a moment, as to what that means, at least in my head. What that means, at least in my head, as to why that's allowed, just by ignoring the part of that matrix that you don't want to invert. So then the second problem is the integration. I'll come back to Paul's comment here, because for some sort of efficiency, we've got to do these matrix elements again, and we're going to be doing multi-dimensional matrix elements, integrations. And so to do this efficiently, we use what's called a local harmonic approximation. So you take your potential and you expand it around the center of a Gaussian function. It around the center of a Gaussian function to second order. And then, of course, we can do everything analytically, which is sort of straightforward, but you're, of course, making an error there in your integral. And what does that mean? Yes? And you see the energy is no longer conserved, whatever. Yeah, and that was actually a question. I think you pulled this question on New Jersey. What's actually conserved if we do in the NHL? I've no idea if it would be nice if we could show you what is actually being conserved. Do we have anything when we take that approximation? When we take an approximation um that we can one half oh yes yes before you need it in the gaussia about here you'll just move on and take it out to become slightly more practical time-wise. So if we just have the Gaussian functions, yeah, so we get rid of all of our sort of So we get rid of all of our sort of MCGDH-like equations. We say, okay, I've only got Gaussian functions, multi-dimensional Gaussian functions. Then this is just a Gaussian wave packet method then. But you have the same equations of motion for those Gaussians that I just showed. Although now the S's doesn't make any difference. The configuration is the set of functions. We call that the variational multi-configurational GWP method. And yeah, this has lots of people working on these types of methodashes too. Of people working on these types of methodologies, different versions. This isn't the only one, but the difference is that so things like multiple sporting that Todd Martinez is working with, and Dimitri Shalashilin, coherent coupled states, they're all sort of similar. And Jerry's going to show his sort of versions of Gaussians as well in a bit. And they're simple, you know. I think this is something, because certainly when you're trying to show results and talk about them with my chemistry colleagues, they don't really like wave packets because they're just these big delocalized. Wave packets because they're just these big delocalized things, sort of what you see in it. Whereas, as soon as you have things like Gaussian wave packets, you have the trajectory at the center of the wave packets, and that gives us some sort of nice chemical pictures of molecular structures moving in time. It's much easier to show things from, actually. It's easier to analyze in a chemical sense. So, the one main difference between multiple spawning and the BMCG method for the equations of motion is that Todd basically has his functions followed. Has his functions follow classical trajectories. So, this is to bring it down to link our method, which I've written in that general form for these Gaussian functions, to the sort of the Heller form. So, this was sort of Heller-Helle, at least in the chemistry world, is credited with bringing in the we take a wave function expanded in a set of Gaussian functions, and he wrote them in this particular form. So now we have this form where we have a center and a set of momentum at the center of the Gaussians. And a set of momentum at the center of the Gaussians, a bit of width, and then a phase. And the nice thing about these sorts of methods is: well, it's known, the Gaussian is the exact solution for a harmonic potential. And if you take the equations of motion for these Gaussian parameters, just from the time of Penrhymi equations, this is what Heller did, you can basically show that if you have frozen Gaussians, i.e., these widths don't change, just to keep things a bit simpler, you can make the widths change, you don't just make things just more awkward. Make things just more awkward. Then you end up with classical equations of motion for this center and momentum, and then you have the phase. So they're very nice, very sort of simple. And we can actually show that we have a similar, in a way, or a related structure. So if we sort of look at these linear parameters that we have in our Gaussian functions, they can be related to the Heller Q and P, has this relationship here. And again, if we use. Here and again, if we use frozen Gaussian, so we don't change the widths, and we can actually set them so that they're normalized and that the phase can be kept to zero. We can set the phase to whatever we like because of the expansion coefficients. Heller originally didn't use expansion coefficients, whereas we have these expansion coefficients. This means if I've got a phase here, well, I can actually put it into there or whatever, so I can, which makes things somewhat nicer. You have to set the gauge between the two parts, basically. And if you do that, you can actually show. And if you do that, you can actually show that for our Gaussians, you get the equations of motion. Well, they have the classical part, but then they also have this variation of coupling between them. This is the Y, where we've taken out the classical parts effectively. Okay, so that's, in a way, that's a rather nice thing. And then, how we imagine that we can then do this or the relationship is if you have the classical parts basically, you're just ignoring this part here. And all we're doing at that point is, well, obviously, our Gaussians are no longer variants. Obviously, our Gaussians are no longer variational, so they're no longer the optimum ones, but they're giving us maybe a sensible basis. This is what following says, it's giving you a sensible basis that's following the classical trajectories, and then all the variational parts go into the expansion coefficients. So you still have a potentially exact solution. Then the difference is, well, how well or how important then are these variational paths? And this is just as a little example. So if we take a double well potential, Potential, this is actually an asymmetric double well, which is meant to be from this proton transfer and this molecule. This is the flux, this is the calculated flux through the barrier. So, if we start the simulation where we're just below the barrier on the low side, and then effectively stretch the bond, compress the bond a little bit so that you give it some energy, and then you sort of basically let it start to vibrate. Do you get tunneling? So, this is a tunneling problem. So, that's the flux through the barrier, the exact quantum flux. Quantum flux, and that's what you get when you use about 16. Well, it's 16, 32, basically, you get the same results. So, with 16 of these Gaussian functions, these are the variational functions, you get the right sort of results. They converge pretty quite well. And I didn't show the result, but if you use classical Gaussians, you need far more of them, that's the point. Because they don't, you know, they sort of have well-defined trajectories. Trajectories, you have to put them in the right places. So these are the sort of looking at the coordinates for 16 Gaussian. So these are the variational Gaussians, these are the classical Gaussians starting in the same place. And we can see, of course, that these classical Gaussians, they have high periodic orbits, all very nicely defined. Whereas these variational Gaussians starting from the same place, they actually spread out to cover the space. That's why they converge much quicker. So that's one of the nice things. One of the nice things, and you know, we obviously want things to converge quickly. So, could you go could you go back one page? So, you've got two coordinates here, so one of them is being treated with Gaussians? No, but it's a two-dimensional Gaussians. These are two-dimensional Gaussians on these two coordinates. So, you're solving the full-dimensional problem. Well, a two-dimensional problem. Yeah. And so, when you use Gaussians, everything is Gaussian. This is with these are with it, it's with a set of two-dimensional Gaussians. It's just a two-dimensional problem. Of two-dimensional Gaussians. It's just a two-dimensional problem taken on this effect. So it's just a two-two-dimensional Gaussian. So no SPFs. There's no SPFs or anything like that. We obviously could do, and that would be the way you make it more efficient if you do an SPF for the tunnel mode and a Gaussian for the other. Okay. Yeah, so this is, in a way, that would be that, but how do you move to larger systems? Well, these things scale, you know, you can have, you've got whatever, a 30-dimensional system, you do 30-dimensional Gaussians. Well, that actually is. Initial Gaussians. Well, that actually is actually quite expensive for the VMCG because this C matrix scales, you've got a number of degrees of freedom times the number of functions, and you have to invert this thing a lot. And you also start, because we're using this LHA, if you've got large dimensional functions, you want to get large numbers of second-order terms that you have to put in. And so you can actually again use- go back to the GMC-TDH idea and start to partition your system into the different bits so that you can break it down into, as we did originally, smaller dimensional Gaussians. Originally, smaller-dimensional Gaussians, whatever. So, if you've got a 30-dimensional system, you can do three sets of 10-dimensional. It's actually cheaper for the Gaussians. But now, of course, there's no obvious no, and there's never anything that is really, it's no free lunch, etc. You now have mean fields you have to calculate, you have many expansion coefficients, and so you're merely just moving your effort from one part to another. And so, the optimal part step. But the nice thing about this idea, and this is what we're looking at for practical calculations. Looking at for practical calculations, is that you can actually think of: well, I've got my system partitioned. Well, we think of this generally when you have a chemical system, you've got your key modes, you've got spectator modes that might be important, but they don't do that much. Then you've got a bath. Now, that's the usual just the way that we think of things. And then you can do those at different levels of approximation. So you can start to move to really big systems, which is something we're looking at at the moment. So you can have the modes that are the most quantum, so those that are doing tunneling, or if you've got a common intersection. Or if you've got a common intersection, those that go through the intersection, you want to do those fully variational so that you really get the ideal best Gaussian symbol. And then you can have the other, the spectator modes, they can probably just follow classical trajectories and this seems to work very well, much cheaper, but they are then driven or following the exact wave packet in the really quantum part of the system. So that's the last we did. So that was just one step. Last week then, so that's that was just one state. Well, we have to worry as well now about I want to do things with excited states. We've got our multiple, our manifold of states. How are we going to do all of them? And well, we want to get, we have to do this in a diabetic representation. So people, you know, if you're doing surface hopping or doing AIMZ, so multiple spawning, they can do it on adiabatic surfaces. We can't because we're doing this thing with the LHA, so you obviously want to go to second order, and therefore you need things to be fairly smooth. Second order, and therefore you need things to be fairly smooth. If you do an LHA on an adiabatic surface, if you have a conical intersection, so where the states intersect, you have cusps, and then you just get, of course, it's a very poor approximation if you do an LHA over a cusp, you just get bad results. So we need diabetic states that they're smooth. And then you have the problem: well, how do we form diabetic states on the fly? And at the moment, what we're doing to form these states is we're using something we've called diabetic states. is we're using something we call diabetization by propagation. So this is the relationship between this adiabatic diabetic transformation matrix. So by knowing the non-adiabatic coupling vectors, derivative coupling vectors, you can form the derivative of this matrix and we can propagate that transformation matrix. Yeah, unfortunately this is only exact for this complete set of states. And in principle, yeah, you so we we're introducing an error somewhere by doing this for it's the best we can do at the moment. It's the best we can do at the moment. So, anyway, so this gives us a sort of a scheme. And the fact that it works is just sort of as a demonstration. This is the allene molecule, it's a small Yarmatella system, the cation in fact, and you have these two states, and these are the states that we get out. So, I have a thing about direct dynamics. So, you can basically propagate these Gaussians, this one. These Gaussians swap. Oh, obviously you removed a slide by mistake. Okay. So, what do we do to get these surfaces? So, one of the reasons we wanted to do this is we want to get potential energy surfaces. And the way to do that, rather than parametrizing everything beforehand, is to do them by what's called on-the-fly. So, you want to propagate the Gaussians and calculate. You want to propagate the Gaussians and calculate your potential surfaces as required where the system goes. And we can do this because, with our recipe, which I thought I had a slide on the recipe, but I also don't, so I'll just have to say it, what we need is what we need at each point, at the center of each Hart-Gaussian functions, we need the energy, we need the gradient, and we need the Hessian, which are all things that can be provided by quantum chemistry programs. Very straightforward. Although we don't calculate the Hessian every time, because that's expensive, but there are Hessian updating schemes that you can. But there are Hessian updating schemes that you can use. And so we propagate Gaussians. We don't calculate at every step. We calculate every so often when a Gaussian has moved far enough. And then we collect these things up in a big database of points. And then we are using shepherd interpolation between the points to provide the surfaces. So that's the recipe that we're doing. But in any case, we are therefore able to generate the surfaces on the fly. And so this is the example of the surfaces that came out from a calculation on this Alley calculation. A calculation on this allene cation. So, if you take the allene cation, you basically ionize the molecule, you end up in this really strongly Yarn-Teller coupled set of states. And so, here, this is the adiabatic representation, and this is the diabetic representation that comes out from our propagation diabetization. And we even get, we can represent the vector field for the non-adiabatic coupling and all the sort of the phase changes and so on that's in there. And yeah, so it seems to work very well. At least we can only test. It seems to work very well. At least we can only test this for two states. If you have multiple states, it's actually you can't do these plots to check how good your results are, just because you're always doing some cut through surfaces. So it's tricky. Anyway, we're assuming that it works, which is not great. Okay, and yeah, really just as an example, because this is one example, this is a recent bit of work, where, okay, why are we even in a way worrying about this? Direct dynamics, most people at the moment are now doing direct dynamics using surface hopping. Surface hopping. Most people probably know, at least in the field, know what surface hopping is. So, in surface hopping, you basically represent your wave packet by a set of trajectories. So, you discretize into a set of trajectories. And, for example, if you have two surfaces, so your trajectories will go on the upper surface. And then, whenever you get to a region of coupling, you calculate a probability for hopping to the upper surface. And this is a set of models, that the Tully models that John Tully. So, the Tully models that John Tully set up a long time ago for testing surface hopping to see how good it is. They're all just one-dimensional surfaces. And recently, so Basil Fircher and Le Ibeler came up with a set of molecular versions. So, this is our sort of new set of benchmarks, which is ethene, DMA, BN, Fulvine, to try and get some sort of, because one of the problems we have is, well, we have to benchmark our ideas, see how well they work, how can we compare them. So, we have these three systems. So, we have these three systems, and they actually have different characteristics. So, these are just cuts through the surfaces, and they have different types of intersections. So, the key part is always when we have these surfaces, where they cross. Even though these two look similar, they're actually very different. It's difficult to sort of explain the scale. But this is actually very close to where the wave packet starts. We start at zero, and so this is actually more or less under the initial wave packet, whereas this intersection is a long wave packet. Whereas this intersection is a long way from where we start, so we actually start over here. We've got to move a long way to get to the intersection, it's much of our own energy. And in ethene, it's very, very different again because it's all by the torsional motion in the ethene. And so, when we go along this torsional mode, you excite, if you excite this one, you come across, and then you basically have an intersection up there. So, it's very different, sort of different sets of ways of crossing. And if we look at the state populations, I'll sort of very briefly sort of go through these, you have this very Briefly, sort of go through these. You have this very different character for how the system is responding. So after exciting, you're getting this relaxation from the one state to the other. And the different characters of DMABM, because as I said, the intersection is right there under the Frank Condon point. It initially just immediately relaxes. It's just getting a crossing from one state to the other. Very simple. Whereas full V, you get a delay because you've got to move to the intersection and then you relax. So this is the diabetic picture that you've got. So, this is the diabatic picture, these are the adiabatic picture. And so, the adiabatic picture, this is again the sort of the signal, it's really fast, just going through the adiabatic intersection. This one for the full vine, it's sort of slow, but we get this stepwise transfer that goes across and back and forth. And then in ethene, which is sort of quite different, so we start in the S1, and we get crossing, we've got an S2 and a little bit of S1. And if we compare this to surface hopping, which is what we're trying to do here, well, these two models, they're sort of Well, these two models, they're sort of, they're not identical, but they are sort of similar in behaviour, similar in time scales, whereas the ethene is completely different. And that's because you see here, the ethene, it actually predicts that it will all end up on S0, whereas we predict that it will all end up on S1 and S2. So it's quite a different prediction. This is the first time you've really come across a completely qualitative description using surface hobbling or Gaussian functions. And the reason for that. And the reason for that, we think, looking at it, this is just looking at the densities. And again, I don't expect anyone to follow everything that's going on here, but this is looking at the density along the torsional mode. So we're starting here in S1, and you can see that here, this is very sort of, it's just, this is the variational multi-configuration of the Gaussian calculation, and this is the torsional mode, the wave packet that is staying coherent, so it's just rotating and back, so it's just rotating backwards and forwards, and it stays coherent. Whereas the surface hopping, the classical trajectory, they lose. Surface hopping, the classical trajectories, they lose the coherence and they just spread out. And as a result, they are staying in the region over here the whole time, which is where the surfaces all come close together and it pops to the ground state. Whereas for a long time, for example here, in the VOTG, you're nowhere near the bits where you can cross, so we don't actually end up crossing to the ground state. So you get quite a different behaviour. So yeah, that's just an example where no one actually believes that we're right because everyone believes the surface. Believes that we're right because everyone believes in the surface hopping because it's the known result for 20 years. So we now have to work that out. Okay, so at that point I will finish. And what I've tried to do, because I've sort of done this, is so we'll be working on these methods. These are all linked methods for basis sets, developing basis sets to solve the time-dependent Schrodinger equation. All based on using the direct variational principle, very simple sort of, let's just solve the. Very simple, sort of, let's just solve the equations, and this comes then down through these sets of hierarchy of methods coming from this time-independent basis set through MCTDH, multilayer MCTDH, which are all based on the same ideas, just able to access large systems, getting more complicated the whole time. So harder to use, but more powerful. And then going down this Gaussian route, so introducing these parameterized spaces functions brings an extra flexibility to the whole problem. If you're Problem. If you were doing something like a very simple model, you're probably always quicker doing MLMCGDHTV Gaussians, actually, at the moment. But they are more flexible, they have some really nice properties to them. These variational Gaussians, they converge quickly, which, especially if you're doing direct dynamics, you obviously want a method where you don't need many functions. And I think this is one of those things that actually, when we get it working properly, will mean this will be quicker than doing surface hopping, because surface hopping you need thousands of trajectories really. Thousands of trajectories is a really good result, whereas we seem to only need about a hundred functions for many systems or less. So that makes sense faster. Let's say, yeah, the problems that we have, the practical implementation reduces the accuracy because we start with a method that at least we know that if we can solve it fully, we have the full answer. And that's basically where we are. So the present state of the answer, these direct dynamics. Okay, you have the whole problem now. The electronic structure is usually terrible. The electronic structure is usually terrible, whatever. So, whether what it means is relating to experiments, but it's a starting point, and at some point, we'll be able to do it. Okay, so at that point, I'll just leave the, yeah, these are just so it's not all just me doing this, lots of people who've been involved in this various development, and so just some of the people here, because I'm telling you this at the moment. Anywhere, and we have this program that all of this has been in, so our quantities program that sort of starts this big sort of Cosmos project. This is to try and actually push these Gaussian wave. To try and actually push these Gaussian wave packet methods, but this also does MCTDH and all requests as well as comparisons. So, with that, thank you, and I'll turn it over to you. Thank you very much for this talk. Yes, yes. Could you speak a bit more about how some of these solvers are preserve energy and Hamiltonian? Because I I was a bit confused at the point 'cause you you I was a bit confused at the point because you mentioned using some predicted corrector methods with adaptive step sizes and run-cutter methods, and that doesn't seem to chime well with the idea of preserving energy and how much you well they okay. That's what I did say you get the drift. You don't get that's the in principle it conserves energy, yes, because by construction it conserves energy, but obviously that's what I said when I meant what I meant when I said as soon as you use an integration scheme, you get this error that grows. That grows. So they actually, I mean, basically, we just monitor it. So you have to use the adaptive time step. So adaptive time step predictive corrector is better than the cutter. There are other integrations to do, of course, as well. I think Christian's 1 is meant to be even better in a way, because it doesn't have this regularization problem. And we just watch, you monitor the energy, and we just say you can take smaller step sizes to keep the energy as constant as possible. So in that sense, we aren't conserving it. And the norm tends to be conserving. And the norm tends to be conserved by construction because you actually propagate, in practice, the coefficients get propagated by Lanchos algorithm, so it's conserving the norm on the way through and the single particle function. So you're, again, you're losing some error on the basis functions, sorry, you're gaining an error on the basis functions, so they're not optimal, but they're still going to be very good. And you're retaining through the Lanchoff scheme on the A coefficients as much as possible the structures there. The structures there. Now, this is sort of more general remark. We don't expect you to give a complete heart space, otherwise all you deserve to visit. All these calculations and many other calculations are essentially contradictory to the standard, in my opinion, wrong answers. Wrong answers in Berkeley computations, which is Berman scales and paper should have. Namely, that you have a very high-dimensional problem with a minute and mathematical number of degrees of freedom rather than special proportions. And the number of the volume of computations increases exponentially with dimension, and hence highly dimensional computations are virtually impossible. Virtually impossible. But in principle, we know that they are possible. And somehow, in a naive context, from a naive point of view, at this moment, every sort of the standard practice that actually we can get good results departs from the standard logic, the number of degrees of freedom increases exponentially. We replace it by a much, much lower dimension, perhaps increasing linearly, but still it works. This is linear only, but still it works. Well, the answer is that there is a branch of mathematics that explains it. It is a concentration of measurement. The branch for which Michel Talagram got the other part of the CCM. And we know that inside very highly dimensional structures, there are low-dimensional structures that explain them to an incredibly degree. To a very ver to an incredibly high probability. And the question is: how does it com uh translate to computation? Because there must be something going on in this computation and many other computations of this kind. Something going on that allows you to do it. And I believe that once we understand it mathematically, probably the root to all that is concentration of plan. We'll be able to construct Will be able to construct incredibly better algorithms, building upon it. And people have done it in several areas, like the Johnson Windelfranz webinar.