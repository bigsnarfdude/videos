Okay, so now it seems that we're recording. Thank you, everybody, for coming, and please feel free to interrupt me if you have any questions. Before I get all excited and talk too much, I do want to remember that this is work based on three joint papers that are currently in progress. So I wish I could point you to archive links, but actually the papers are in progress, but it's But it's in different combinations is joined work with Frederick Behan, Jens Vorsgaard, Winter Neese, Kayla Philipson, Erica Rafslund, Corbin Rusik, Lisa Sewell, Lisa Sol, and Joseph Rennie. And I'll mention other works in progress as well. So today what I'd like to do, I want to talk about counting the number of kinetic components of real serosets of arbitrary polynomials from a particular point of view. And just to make things easy to not say kinetic. To not say connected components over and over again, I just want to say pieces just to make it easy. And I want to understand, it's good to understand the fine-grained complexity of doing this. So fine-grained complexity, meaning that we're going to look very precisely at certain parameters in this problem and how the complexity grows with the parameters. And ultimately, we will be very general, but I want to show speedups happening in certain special cases. Cases. Now, of course, we should justify why we're going to do this, and also I should explain how we're going to do this. So, let me spend a little bit of time with that. So, first off, certain dynamical systems, if you study flows and differential equations and this sort of thing, they have attractors that are actually pieces of a real hypersurface. This is one setting where you might be interested in knowing something about hypersurfaces and the number of pieces. And in particular, the special case of And in particular, the special case of plane curves, this already shows up in Hilbert-16 problem. So in Hilbert-16 problem, the meat of the problem has to do with certain flows, dynamical systems in the plane. And then a portion of Hilbert-16 problem has to do with how complicated can algebraic plane curves be? How can the ovals sit with respect to each other? And so these are very interesting questions and very hard questions. Questions and very hard questions, and a lot of them are still open. I think for degree eight or degree nine, there's still many, many open questions about how many ovals can be inside of each other and how they sit. From another point of view, counting roots of polynomial systems, that's very interesting to biochemists who want to count equilibria or consider how many equilibria there are for certain kinds of reaction models. And that's something that several people in the audience know a lot about. Several people in the audience know a lot about, so I won't dwell on that anymore. And just another, it's actually kind of a trivial observation, but it's a nice one to make sometimes. If you consider tons of squares, then if you're looking at systems of polynomials, over the reals, you can always reduce many equations to one equation. And of course, this is very inefficient, but it's a useful theoretical fact to be aware of. So if you have many equations, you just take the sums of the squares of the equation, set it equal to zero, and that's one polynomial. Equal to zero, and that's one polynomial in the same number of variables, but you do blow up the number of terms a bit. So, in any event, studying one polynomial is actually pretty good, and it's already quite intricate, as we'll see soon. Okay, so yet another motivation is to go beyond Smale 17th problem. So, let's recall, in 2017, Carol Ray, following earlier work of Beltran and Pardo and Peter Bergriser, who could And Peter Bergreiser, Fluke Cooker, Mike Streef, and Ms. Dave Smale proved a really beautiful result. He proved a conjecture of Smale from about the late 90s, and the conjecture was his 17th problem. It was finally solved positively. And so the problem states that, okay, you want to find a complex zero, event complex polynomial equations, and then unknowns. And you want to find the zero approximately. So let me back up a little bit. So let me back up a little bit. Smale 17th problem deals with the complexity of solving polynomial systems over the complex numbers. And the problem is stated in a very simple, a deceptively simple way. And there's many little subtleties. So first off, you should notice that in Smale's 17th problem, he's asking for one zero. He's not asking for several. And there's a reason for that because, first of all, the number of zeros can be exponential in the input size by Bezoo's theorem. By Bezou's theorem. So, already that's a smart thing to do. Notice that in the statement I had here, I'm saying approximately, and that's also important because, I mean, Gawa theory, and there's many other ways of looking at it. When you're solving a polynomial system numerically, you're never going to get it exact infinitely many rational operations. The best you can hope for is to get some approximation, and even doing that is subtle. And also, the problem asks for this being done on average, and that's important. And that's important because you know that a system of equations can have infinitely many solutions. So if you average, then you get rid of that discriminant locus most of the time. And the offshore is that you can do all this in polynomial time with a uniform algorithm. So a uniform algorithm, that's more of a technical part. But the point is, it's an algorithm you can actually implement and actually use. And there's a lot of work nowadays. Pierre himself has actually done a lot of improvements and he's trying to get this to work more properly. And he's trying to get this to work more practically. Many other people are working on this as well. So, to make it a little bit more precise, if you assume the input equations have maximum degree d, then Lara's algorithm, at least one of the ones that he proposed, uses d plus n to the something on the order of the minimum of dn field operations to get this approximate solution. So observe the following. Observe that if you let d grow and fix n, Grow and fix n, then you get a polynomial, an algorithm which is polynomial in d, and the exponent is on the smaller number than n, and vice versa. If you let n grow and you fix d, you also get a polynomial time algorithm. So it's a rather amazing result. And it took quite a bit of work. So this is just for the complex numbers, and doing it over the reals is actually a good deal more subtle. So what about the real case? About the real case, so first off, the input size is going to be a little bit tricky, and also the averaging is going to be kind of tricky too. And let me clarify that. In the way that Smale's 17th problem is stated, in the way that LeRay solved it, there is a little twist. So Smale left a lot of the problem open, and he didn't really get so precise about the output size, but I think most people said, okay, let it. Uh, most people said, Okay, let it let's let it be the number of monomial terms. And if you look at polynomials of degree d, then the number of monomial terms you have is going to be d plus d plus n choose n. And so actually, Le Ray's algorithm works in time polynomial of the number of monomial terms. Okay. That's one way to state it. Assuming the polynomial is dense, assuming that every possible monomial term, or close to all those terms, show up. But what if the polynomial system is sparse? Well, that's. System is sparse. Well, that's that's where it gets a bit more subtle. Um, over the complex numbers, sparse systems can still have many roots, but over the real numbers, sparse systems have fewer roots. So this is why we might hope that things go faster over the real numbers. Also, the averaging is more subtle because over the complex numbers, with probability one, you will have exactly d to the n roots anyways. Whereas over the real numbers, with positive probability, With positive probability, you can have any, you can have several different fixed numbers of roots. Okay, so the point is that generically, the number of roots is constant over the complex numbers, but this is not true over the real numbers. So there's a subtlety. Maurice, there is a question. Can you comment if in which way the allowed size of the error plays a role for the number of operations necessary to be an algorithm? Yeah, that's a very good question. So to answer the question, To answer the question, the way the size of the error comes in is a little bit tricky. So, Smeal has this very elegant definition of approximate root. So, the definition is so elegant that it makes the precise statement of the error evaporate. In other words, his definition of approximate root is a root which, when you throw it in a Newton's method, converges doubly exponentially fast. Okay, so that is the gold standard for. Is the gold standard for solving? So, for an individual polynomial system, you can certainly say, okay, you need roots of, you know, with an accuracy 0.001 to converge. But the more intrinsic notion is just, okay, just give me something that'll make Newton's method converge fast. So that is the definition of approximate root. And that's how he gets around having to quantify the error. So that's it's a very beautiful. It's a very beautiful approach. So it's somehow intrinsic and it adapts itself to whatever polynomial system you're looking at. And again, the averaging, the averaging comes into play, because you can imagine for certain polynomial systems, you need high accuracy. For certain polynomial systems, you don't need that much accuracy. But on average, it all works out. So that's also another reason for the averaging. Okay. So is there another question? Okay. So So, to get a real analog of SNOL 17 problem, this actually goes back to 2005. So, I wrote a paper with Yin Yu Ye, and we stated an explicit version. So, I'm paraphrasing a little bit here. So, consider n by n systems of real polynomials of degree, let's say it must be. And let's say you fix the exponent vectors that are going to show up. So, there's n plus k distinct exponent vectors. The question is: can we find all the real roots? That is, I'm saying all the real roots, not just some of them, approximately. Of them approximately using this many arithmetic operations on average. Okay, so again, this is sort of clearly in the spirit of Smales problem, but notice that I've changed the complexity bound. So you notice that the complexity bound I'm asking for is exponential in the number of variables. And yeah, that's a poor bound. But notice that if you fix n, you're polynomial in the number of terms, polynomial in the log of the degree. And that's the key place. Of the degree, and that's the key place where you're going to win. It's log of the degree. And so, okay, this is a crazy conjecture to make, but there is some, you could argue in favor of the conjecture, and that's what we stated. It remains open, but that is a real analogous male 17 problem. But here's the thing: this actually includes counting already, because you can't solve for all the roots unless you know how many roots there are. So that's a sub-problem. Sub-problem. So clearly, maybe we should first solve this particular sub-problem. And the sub-problem is this. In the notation above, let's limit ourselves. Can we at least just count exactly the real roots, the other real roots, of most such systems using the many arithmetic operations? Okay, so now in this boxed statement, I'm simplifying the problem a little bit. And I'm not even asking for on average. I'm saying, okay, maybe most systems or a lot of these systems, can we at least? Or a lot of these systems, can we at least count the roots? So forget about solving. Okay, so let's focus on that problem. And that's an important problem already. So there actually is positive evidence in two sort of limited settings. One setting is when you have one variable and the polynomial is sparse. And so there is some work in that direction. I'll mention the references in a moment. The other setting, I'm sorry, this notation looks funny, is k less than. Looks funny, is k less than or equal to 2 less than or equal to n. So, this is another setting. This is where you have systems of equations, and they're very sparse. You have just a little bit more terms than the number of variables. Okay. And the references for this, there's work of Sagrilov. He's done some work with some other co-authors on sparse polynomials in one variable. There's work of myself and Kirkhorse Paris and Caitlin Philipson. We look at binomial systems, and you can actually solve this so-called analog in Solve this so-called analog in exactly, but it's very limited, binomial systems. There's work of Tonaliqueto. So I confess I've not looked deeply at the work, Tonaliqueto and Nenci Garidas. They did a very nice probability, some results in condition numbers, and they found that an earlier algorithm, the Sager law for sparse polynomials, works very well on average. Okay, so that's also another very positive result. Another very positive result. And another recent result is certain systems where you have n plus two terms, n plus two distinct exponent vectors, you can actually deterministically solve, deterministically count all the roots. So it's actually a refinement of the multivariate Descartes rule. So you can actually not only estimate, but also exactly count. And again, the complexity is polynomial and log of the degree, which is nice. Which is nice. Okay, but we'd like to do this in greater generality. Okay, so why are we limiting ourselves and why are we being pessimistic? Well, here's the problem: there's a complexity lower bound already. So this is from a few years back. So if you fix any epsilon greater than zero, and let's suppose there's an algorithm that just for one polynomial, one polynomial, n variables, and let's say the polynomial has a little bit more than n terms, say it has n plus n of the epsilon terms, you know, n plus the square root of n. You know, n plus the square root of n, maybe n plus n to the one over 100. The algorithm that decides that the polynomial has a real root, and it uses time polynomial in the bit size of f. Okay, so in other words, bit size of f, what is that? Well, it's, you know, log of the degree, heights of the coefficients, you know, this sort of thing, logarithmic height of the coefficients. So if such an algorithm existed, then p would equal to np. In other words, equal to np. In other words, just detecting real roots and forget about counting connected components, just to know if there's a root or not, for one polynomial, n variables moderately sparse, that's already an NP-hard problem. Okay, so there's a relatively simple reduction you can do. You sort of embed satisfiability into this problem and these tricks for doing it. Of course, this is a limitation, but you can still get around it. But you can still get around it. Maybe if you look at n plus one terms, n plus two, n plus three, n plus four, n plus five, maybe you can do it in polynomial time. Maybe. Okay. But as soon as you go to too many terms, n plus n to the epsilon, then you're in trouble. So that's a limitation to be aware of. So again, to clarify, most of this talk I'm going to be using the Turing computational model. I'm going to be talking about bit complexity and the size of a polynomial. And the size of a polynomial, that's going to be the total number of bits in the coefficients and the exponents of f. So this is why I'm interested in log of the degree, not just counting polynomial and d at the n. The case of n plus 1 terms, that's sort of the opposite extreme. It's a cute exercise. So you can actually get an O of n time algorithm that's clearly linear in n. And assuming the exponents are affinitely independent, you can do it. And I'll return to this in a moment. Return to this in a moment. I'll actually give a little proof for you. But again, the question is: what about n plus k terms? Okay, and just a little comment. The reason I'm looking at n plus one, n plus two, there's a technical reason for that. Basically, if you do a simple monomial change of variables, you can always assume it's n plus k terms, where k is positive. It doesn't make sense to look at n minus two terms or n minus three terms, n plus k. n plus k okay and also uh another thing we should be aware of i mean what kind of bounds are there so if you have a one polynomial sparse n variables uh what is the maximal number of kinetic components you can have as a function of n and k what is that okay so i want to get to the main results i just want to state the main results briefly and the first result has to do with And the first result has to do with an upper bound. So, this is joint work with Jens Vorskard and Lunar Nies. And you assume the following: you have one polynomial and variables, assume it as n plus k polynomial terms. And you should also assume that the exponents are affinitely independent, technical assumption. And you assume the positive zero set is smooth. Okay, that's the Jacobian is a full rank. Then the positive zero set. Then the positive zero set, that's my notion for the positive zero set, has no more than this many pieces. Okay, so take a look at the band. The bound is exponential in k, but polynomial in n if you fix k. And this bound is actually quite nice and it refines earlier work by many other people. I want to point out something. There's a beautiful band by Frederick Bihan and Franks Atiel, and we managed to beat this band, but I should be. Managed to beat this bound, but I should be clear: their bound is for a larger quantity. So they actually bound the sum of the Betty numbers, which theoretically is actually, could be bigger than the number of connected components. Undiscounted connected components, Bihan and Satiel and their work, they look at the sum of the Betty numbers, which is actually a larger quantity. Second main theorem, this is a joint with Lisa Sewell. Lisa Sewell was one of my RU students from last summer. In this notation, if you assume In this notation, if you assume further that the coefficients of the polynomial all lie in this range, they're just integers, and they can be the most H and absolute value. And if you assume the k is less than or equal to three, okay, so that's very particular. Then as the coefficients get bigger and bigger, you can count exactly the number of pieces in time polynomial log dh. Now, of course, it's exponential in n, but you can do it that fast for a fraction. Asked for a fraction of these inputs. So the fraction, I'm afraid I'm being vague about the fraction. I'm being vague on purpose. The fraction is about one half plus epsilon, and the epsilon I'm still working out. Okay, so this is work in progress, but what I'm going to do today is focus on that second theorem. So I'll mention something about the first theorem, but I'm trying to head towards the second theorem. Okay, so before I go on, are there any questions? I will spend the rest of my talk sort of explaining. Spend the rest of my talk sort of explaining the background behind these main results and saying something about these main results. I still have a little more background to go through. Okay, so if there's no questions, I'm going to continue. I do want to contrast this with the classical setting. So if you just look at curves, polynomials and two variables, and if you look at the bounded degree case, so if we forget about sparsity for a moment, just assume you have some polynomial of degree D, maybe all. Some polynomial of degree d, maybe all the terms show up. There's this classical band of Hardnack, which says that the number of pieces for a projective degree D real algebraic curve is something on the order of D squared. So that's how many pieces you can have at most. Interesting, an interesting result is you can look at the isotopy type of the curve. So what I mean by isotopy, well, it's I'm being vague and not defining everything, but basically, if you look at deformations of the curves, under Under a family of diffeomorphisms, then how many different isotopy types can you have? By the way, this corresponds to disposition of ovals. So, for example, if you have 10 different ovals and they're not lying inside of each other, this is a different situation than having 10 ovals all progressively nested inside of each other. These are two situations that you cannot deform into each other. So, even though you have 10 kinetic components, there's many different ways as 10 kinetic components. Components, there's many different ways these 10 kinetic components can be inside of each other. Okay, so it turns out that there's many, many possible isotopy types of the deformation. So it's something exponential and constant times d squared. It's something of this nature. Theta, I'm using computer science notation. So banded above and below by constant times that. Okay, so the results that we have imply much simpler behavior in the sparse setting, even though you can have huge degree. And then this is kind of interesting. So if you fix n. Interesting. So if you fix n, but you let the degree be massive, our number of isotopic types is pretty small, number of kinetic components remains bounded, rather different. Okay, so let me return to this observation on m plus monomials. This is simpler than the setting that everybody else has been looking at. I was very happy to see Reich talking about circuit polynomials and other. About circuit polynomials and other people talking about circuit polynomials. This is even simpler. So just look at one polynomial, n variables, n plus one terms. Okay. And so assume the exponent vectors don't all lie in a common affine hyperplane. Then the VAT is the following. The positive zero set is unempty if and only if the coefficients of f differ in sign. That's actually pretty easy to prove. And you can say more actually. In which case, the positive zero set is isotopic to Positive zero set is isotopic to a hyperplane. So you can form it into that. So this is a nice exercise to give to your students, and there's lots of ways of proving it. I'll just give you a MyMade three-line proof. If you change to, if you make an exponential change of variables, you and if you deform the exponents, here's what you can do. After you reorder the terms, the exponent vectors form a signed basis of Rn. Okay, so then it turns out that the general linear group, at least the positive. Turns out that the general linear group, at least the positive part of it, is path connected. And what this means is that you can find a path from one basis to the standard basis, is basically like a homotopy. And you can rescale the coefficients too. And you basically deform to this case, the case of plus minus one, plus minus e to the y one, et cetera. And that case is pretty trivial. That case is very easy to see that if all the signs are the same, empty set. And if any sign differs, then you have something isotopic to a hyperplane. Okay, so that's that. Plane. Okay, so that's that's that. So it's a four-line tree. Um, okay, that's easy. And my point is that when you have n plus k nomials, as you increase k, things get a lot more complicated. So this is the super easiest case. By the way, consequence for this particular limited setting is that there's only two possible isotopy types. It's either empty or hyperplane. There's two types. And also, you can decide which one it is in time of event. So it's a ridiculously simple algorithm. Just look at the coefficient. Simple algorithm, just look at the coefficients. It doesn't even matter what their exponents are as long as they're affinely independent. Just look at the coefficients and see if there's anybody of a different sign. That's very simple. Okay. n plus two numials. So you go back a few years and so this result is the following. Let's go to n plus two. So assume that f is an honest n-variant n plus unomial. Okay, what do I mean by honest? That's shorthand for the exponents. Shorthand for the exponents not all lying in a common alphine hyperplane. Assume the sky has degree D. Then, first off, the positive zero set has at most two pieces. So just that fact alone, this goes back to Frederick Bihan's thesis. If I'm not mistaken, he proves it in his thesis or maybe a habilitation shift. But that's an older result, but we can say more. The deeper fact is that you can actually classify the isotope. Fat is that you can actually classify the isotopy type of the positive zero set. And here's where it gets cute. You don't have to talk about Veraux diagrams, you can if you want to, but the positive zero set is isotopic actually to the real zero set of a quadric and of a very, very special quadric. So you can actually just look at quadrics with plus plus minus one coefficients and the constant term, this is just quadric terms, square terms. Square terms, a constant term, and the constant term is zero or plus or minus one. Okay, so in particular, this result allows singular zero sets. Okay, so you can completely classify all possible isotopy types, even the singular ones, and that's a theorem. And in fact, if you assume the coefficients are integers and they have absolute value of most h, even farther, you can do all this. You can determine exactly jk and epsilon. You can determine the isotopy type in time log of. type in time log of dh to the O of n. Okay, so this is a pretty nice complexity done because if you look at the literature, there are some really beautiful papers, very, very general. There's work of Basu and Muba and Pollock and many other people, other people who joined the game, Safi al-Din and many, many authors. Forgive me for not remembering all the authors. There's been work on computing homology groups. Homology groups and computing many numbers for arbitrary semi-algebraic sets. That's a completely general setting, and people know how to do this. But all those algorithms are very, very strongly exponential. So they're exponential in basically the exponent, the term of the exponent is something along the order of n squared. And it depends what problem you're asking. And what's nice about this particular complexity bound is that your polynomial and log d. There's really no other algorithm. Log D. There's really no other algorithm that goes as fast. Again, limited setting, but you are going very fast. Yes, you're exponential in n, and you could ask if that's possible, if it's possible to lower that exponent. I'll say more about that later. Okay. So a consequence of this is that if you look at these kinds of F's, then there's actually only about on the order of n squared possible different isotopy types. Okay, so you know, you're making a slow jump. Okay, so you know you're making this slow jump. You go from n plus one nomials, n plus two nomials, and things are getting more and more complicated. Okay, so it's natural to try to go to the next case of n plus three nomials. But let me say a little bit more about n plus three nomials. I'll say a little bit about the techniques. So for this result, let's just look at n equals one. n plus unomial, that just means a trinomial in one variable. And so this is just a very simple, basic calculus observation. Just a very simple basic calculus observation. If you look at the graph of that function, that trinomial, then for C small, you have an empty positive zero set. For C big, you're going to have two positive roots. So clearly, the dividing line, what separates these two click cases, is when you have a degenerate root, a single degenerate root. And so if you just do a very basic calculation, you know, you set f equals f prime equals zero, you get that this vector has to be a right null vector for this matrix. So this is actually like a linear algebra exercise. Like a linear algebra exercise. You know, it's given to your students. And so the point is that you can turn this around and get that there is a function. So there's this rational function. This is the A discriminant. Now, okay, I'm lying here. This is actually the A discriminant with a monomial factored out. Okay, but this is an example of an A discriminant. And this is exactly the condition you need under these assumptions to get a degenerate root. To get a degenerate root. Okay, so that's Q. So you're lucky for this particular one trinomial, one variable. You can determine exactly when you have a degenerate root. It's very easy to derive. And what happens is the following. You can count positive roots by looking at where you are in the coefficient space. So the coefficient space can be thought of as a ray. And I look at the discriminant, it's going to be this blue dot here. Anybody. Anybody who's a connected component of the complement of the discriminant, I'm going to call that a chamber, a reduced discriminant chamber. And depending on what chamber you're in, then you see that you have one number of roots or another number of roots. I've drawn these other things here. I don't have time to explain them all, but there's a very natural correspondence between triangulations of the support and these different chambers, at least in this limited case. Okay, so you see when you have no roots, somehow. So you see when you have no roots, somehow I have some correspondence with a trivial triangulation with one cell. When I have two roots, somehow I have a correspondence with a more complicated triangulation with two cells. Okay, and there's a transition in the middle. Okay, so it seems that all you have to do to count roots for a trinomial is determine the sign of a discriminant. That's all you have to do. So let's go a little bit more, a little bit higher up. If you look at it, let's say a three plus trinomial. You look at it, let's say a three plus two nomial. I'm jumping up in complexity here. So, here's this polynomial in three variables. It's got five terms, you know, three plus two. So that's high degree. And if you run through the theory, you can derive that there's going to be some kind of a discriminant. And the discriminant is again going to be a binomial. Now, I'm going to do a little more work. If the discriminant is a binomial and I care about the sign of it, this binomial can be. sine of it, this binomial can be turned into a monomial equals constant. And then I want to see if the monomial is bigger or smaller than the constant. And then what I could do is take logs. So actually when I consider the sine of a monomial, that's the same thing, if you take logs, as looking at the sine of a linear combination of logarithms. Okay, so you see here it looks a bit more complicated, but I claim that this is actually kind of simple. So here's what's going on. This polynomial has degree. This polynomial has degree 2009 plus 6027 plus 18,000. So the degree is, I don't know, 26,000 or something. And you look at the coefficients that are occurring in this linear combination of logarithms. The coefficients have about seven digits or so. So degree 26,000, the coefficients have about seven digits. This looks complicated. But it turns out if you know some diphantine approximation, these kinds of linear combinations can be kinds of linear combinations can be can have their sign determined relatively efficiently okay so that's that's what's going on in the background you have to look at linear combinations logs and depending on the sign you're going to get three different isotopy types so on the left this is the isotopy type of a cylinder in the middle is when you have a singular case and it turns out to be isotopic to a double cone cone with a single singularity and then on the other side you have two You have two hyperplanes. Okay, so it turns out that for these guys, if you vary the coefficients no matter what, and I apologize, there's a sign restriction as well. I didn't tell you about that. So basically, you need, if you assume C1 is greater than zero, C5 is greater than zero, and you let these other middle coefficients be negative. So if you make those choices of sine, then there's only three possible isotopy types. Okay, for this particular example. All right, so my point is that. Right, so my point is that as you increase the number of variables, things get a little bit more complicated, but then the structure is not so bad. And you see here: oh, wait a minute, all these guys happen to be, up to isotopy, quadrics. A cylinder is a quadric. A double cone is a quadric. And two hyperplanes, well, it's like, you know, a hyperboloid with two sheets. So yeah. And by the way, what is this picture here? Yes, I've drawn Verro diagrams. I haven't defined what a Vero diagram is. Diagrams. I haven't defined what a grow diagram is, but let's not worry about that. Okay, now that fine-tuned approximation does enter and enters for circuit polynomials. So here's a comment I want to make. If you're deciding equality of two monomials, you can do it in time, in polynomial time, via a technique called QCD-free bases. So this comes from number theory, and it's somehow an augmentation of the extended Euclidean algorithm. And so this is assuming that. And so, this is assuming that the AIs are rational numbers and the numerators and denominators no bigger than h in absolute value. And you assume the integer, the exponents, are no bigger than plus minus h. Okay, but here's the problem. Deciding an equality, you know, which one is bigger, that is not known to be doable in polynomial time in n. That's a very, very weird subtlety. So, what can you do? There is a theorem. Do there is a theorem of Baker. So, in fact, Baker got a Fields Medal for this theorem and related work. So, he precisely looks at linear combinations of logarithms, and he observes that if you look at linear combination of logarithms, then if the linear combination of logarithms is not zero, then it is bounded away from zero. Okay, it can't be too close to zero. And how close you are to zero is bounded explicitly here. So, if you do some work, So if you do some work and recall that logarithms can be approximated efficiently, then you can decide the sign of a linear combination of logarithms in polynomial time if you fix n. That's one of the key tricks. So to recap, when you're looking at the circuit case, n plus tunomials, the discriminants you see can have really high exponents, but the sign of these discriminants can be decided efficiently. Of these discriminants can be decided efficiently by using Baker's theorem. The only trouble is that you have to fix n. So, this conjecture is, you know, in fact, there's even a refinement of the ABC conjecture would imply that you can bring the dependency down to polynomial in n. Okay, but that's that's very, very speculative and very far away. Okay, I'm getting short on time. I think I have about five minutes, right, Alicia? Six minutes. Six minutes. Okay. Six minutes. Six minutes. Okay, well, I'll uh I'll see where I go. So, uh, goodbye patchworking, and uh, this is going to happen for n plus three terms. So, here's the thing. Honest n-variate n plus two nomials are the last sparse setting where Spiro's patchworking completely determines the isotopy type of C Z plus F. The way I stated the theorem for the circuit case is that you're isotopic to a quadric. And the other side is that, yeah, it turns out that you can use Vero's method as well if you want. Rose method as well, if you want. And the one complication is that you have to talk about triangulation, but it actually works in this case. But when you go beyond, it fails. There's these counterexamples. So yes, you can use Burroughs patchworking, but you also have to take into account which sign of the discriminant it is. And let me prove for you that Burroughs method fails for n plus 3. So here's a concrete example. I mentioned this earlier in the week and in chat to when Federic was talking. So look at this. Talking. So look at this particular five nomial in sorry, yeah, so five nominal two variables. This positive zero set happens to happen three pieces. I'll draw it for you later. But on the other hand, if you look at the triangulation of the Newton polytope, there's exactly five triangulations. And no matter how you attach signs or how you draw your Vro diagram, you're never going to get more than two pieces. So you're basically dead. So I apologize, I never define what a Vro diagram was, but basically. MROS, but basically, if you look at triangulations of Newton polytope and you draw a little polyhedral complex dual to this, making certain choices, that's essentially a Varro diagram. Okay, so this example shows you that Varro's method basically fails pretty early on. I mean, it doesn't mean you can't use it, but it just means that you can't use it in complete generality. All right, so what do we do? Here's our refined results. If you look at Honest M Plus Look at honest and plus invariant implications, then the positive zero set has no more than quadratically many pieces. Okay, so this is a refined bound of an earlier result that I stated. Okay, so you see things are getting more complicated. n plus one nomials at most one enacted component. n plus two nomials, at most two kinetic components. n plus three nomials, now we go to quadratic and n. Okay, so this refines an earlier bound. There's an earlier bound which is exponential by behind. Which is exponential by Behan and Satiel, but remember their bound counts Betty numbers, some sum of Betty number that's bigger. And the key tool is the generalized A discriminatory contours. So I'll try to explain some of that. And in particular, the reduced chambers now are cells of dimension two. So in the circuit case, basically everything can be reduced to the line. But now I'm going to have to work in the plane. So let's return to that example that I showed, this five note. That example that I showed, this bibnomial and the two variables, the degenerate polynomials in this family, you can reduce them to coefficient vectors of a certain form, 111ab, satisfying some polynomial. And this polynomial is kind of unwieldy. So here's what the discriminant looks like. So for circuit discriminants, you can always get it to a binomial. But when you go beyond, the discriminants get big, coefficients are big, and now you get into trouble. And now the question is how do you get around this trouble? Is how you get around this trouble. So it's too unwieldy, but what you can do is look at contours. So it turns out you can, even though the polynomial is very complicated, you can still look at its amoeba, and you can still look at the real part of the discriminant. And in fact, you can look at the log of the real part of the discriminant variety. It sits inside the amoeba. Okay? And more to the point, if you want to understand the positive zero set, you also have to take into account the signs of the coefficients. The signs of the coefficients and use signed contours. So let me go back. That picture there, it's like smashing everything down via the absolute value map. And you're forgetting which the signs of your coefficients. If you unwrap that, these five separate curves, when you put them on top of each other, you get exactly the contour that I just showed you. And I'm showing you the signs for these different settings of coefficients. Okay. So So, how do we get these sign contours? And what is happening? Well, what's lurking in the background is a basic result. There's a theorem called Hart's triviality theorem. So, Hart's triviality theorem from the 60s, it tells you that when you're in coefficient space, as long as you're in a connecting component of the complement of the discriminant variety, your topology is constant. Okay, so now the problem is the following: I want to know how many pieces my zero set has. Well, Well, I can solve this by seeing what my sine vector is and then what side of the discrete variety I'm on. That's what is going to happen. And observe that this guy only has two sides. This guy only has two chambers, two chambers, two chambers. But here, uh-oh, there's a problem. I have this kind of exotic curve, which seems to have one, two, three reduced chambers. So there's a small chamber in here. Okay. Sorry, Marty. Okay. Sorry, Maurice, how do you assign these signs plus and minuses? Oh, it's literally just the signs of the coefficients. So, you know, C1 is positive, C2 is positive, C3 is negative, etc. So, okay, you asked a really good question. So there's a calculation you make related to the Horn-Component uniformization, and you observe that certain signs matter and certain signs don't. Okay, so you're right. There's actually 32 possible signs, 16 if you quotient out by negatives. You quotient out by negatives. And so it seems that there are 11 signs missing. And these 11 signs, basically, the isotopy type is constant. The discriminant variety doesn't hit this orthet. Okay. So those, the topology can be computed easily, and I'm going to ignore those. So this is where the action is. Okay. So you're right. There's certain orthons that matter, certain that are more trivial. All right. So. All right, so I'm basically applying the Horn component of uniformization with a little bit of more work. And okay, there's definitions. I'm already at the end of time, but there's formulas here. And okay, I've drawn these contours again. And maybe see, I'm already beyond time. Okay, I'm going to maybe stop with a little picture. So I'll stop on this picture, actually. So the point is that there's going to be inner and outer chambers. Going to be inner and outer chambers. So, this little bounded chamber, I call it an inner chamber. These other unbounded chambers are known as outer chambers. And there's going to be a way that you can approximate the membership problem. So actually, maybe I should go to a different picture. Here we go. There we go. Okay. So here's where I want to end. So basically, there's pieces, these different contours. When you're deciding what side you're When you're deciding what side you're on, you're in trouble because you're trying to evaluate which kinetic component of the complement of a very complicated zero set you are. And that has pretty bad evaluation complexity. But look here, the discriminatory curve is the blue one. There's a simpler approximation to Hornkevranov, which is this green curve, which is very, very close. And this red polygonal guy, this is actually some kind of modification of the tropical discriminant. And my point is that this green curve is very, very. And my point is that this green curve is very, very useful. And you can use them to approximate and solve the membership problem faster. Okay, so this reduces you to something which is tractable. Whereas, if you work with this massive polynomial and try to figure out where you are, then it's going to be impossible. Okay, so that's a rough idea of what's going on. I'm afraid I have to stop here. I'm sorry I went over time, but thank you for listening. Thank you very much. So there was a previous question by Paul. He asked, I Paul, he asks. Can you comment if, no, sorry, is theorem one true if we allow singularities? I honestly don't know. So in theorem one, I specifically concentrated on the smooth case. I would guess that it's true. At least in other situations, the number of degenerative roots or the number of connected components, it can be a little bit nasty in the singular case. I don't think it's going to blow up that much. It's going to blow up that much. Okay, the truth is, I don't know. Okay, I would guess that the band of complete generality probably will be of the same order, not too much worse. That would be my guess, but I don't actually know. I've not handled a singular case. Thank you. Thank you for the talk as well. You're welcome. Okay, I have one question. So, Maurice, could you show me again the Could you sh show me again the bound you have for the number of connected components when the number of monomers is n plus three? Yes, yes, yes. Here we go. Oh, there it is. Yeah, there's the band. So as far as I know, I mean, maybe there's a more recent bound, okay, but I'm going off your bound from 2008. Your bound from 2008, I think, is exponential in N. But if you separate it into compact components, If you separate it into compact components and non-compact components, then for compact components, the bound is much better. Your bound is better, and I know that. Yeah, yeah, exactly. What I wanted to say. Yes, exactly. So the trick here. Yeah, I mean, there's a little bit of trickery going on here. So the number of the bound we have in the compact test is just the 5n plus 1 divided by 2. It's compact or the yes, I just checked. Exactly, exactly. Exactly, exactly. So, the improvement that we make is to the non-compact components, and we use a different technique. And so, the key trick is actually making use of the A discriminant. If you make use of the A discriminant and a nice result of four scouting cats, then you can actually show that the number of non-compact components is not too far off from what you would get from Varro's method. So, Vero's method doesn't work, but you could show that. But you could show that as you go through the discriminant variety, in the worst case, you're going to add quadratic error to Veraux's method. So that's the key trick. And so that's how we knock down that exponential term to quadratic. Thank you for asking. Do we have any other questions? Yeah, I would like to ask in the very end. I would like to ask in the very end. First of all, thanks for the talk, Maurice. And in the very end, you showed this green curve, and you said, like, in terms of basically the underlying computational problem, it becomes much easier. So can you tell a little bit about where you obtain this from? Yeah, yeah. So here's the thing. There is this issue with determining what connecting component you're in. So sometimes a similar thing. So sometimes it's a matter of just determining the sign of a polynomial, but already determining the sign of a polynomial, unless the polynomial is very special, it's a very, very hard problem. So the key trick with the Horn-Kapanafenoformization is the following. In general, the Horn-Kaponifenoformization in this case can have a lot of terms. It's going to be a linear combination of logarithms. Observe the following. If you look at the amoeba of 1 plus x plus y, that one actually has a Horncaponiforma or something like a Horncomponiform. Or something like a Horn-Cochrano phenoformization. It's a very simple linear combination of logarithms. So observe the following. Observe that if you have an amoeba one plus x plus y, it's like an amoeba with three tentacles, you can hit this amoeba with a shear and you can move around the tentacles. So why don't you take this amoeba and move the tentacles and so they match up with the tentacles of the more complicated guy? So basically, I just replaced Ronkarpanov with a carefully chosen simplified linear. Simplified linear combination logarithms. And it seems to work really well. I mean, I've done some experiments, I've implemented this in MATLAB, and that's part of the work. I mean, right now, I'm trying to understand exactly how good this is. But you can make this modified Horn-Kapranov agree with the tangents at infinity. And in between, miraculously, it seems to work pretty well. Okay. This has to be quantified. That's where I have the one-half plus epsilon. Oh, this is super interesting. Oh, this is super interesting. So, and can you can you estimate the error? That's what I'm working on right now. Yeah, yeah, but it's actually a pretty easy formula. And so it's nice because, you know, Horn Kropanov, I mean, I love Horn Kropranov because, you know, you have this horrible polynomial, but the zero set is parametrizable. And so Horn Kropanov is a very elegant formula. But then you can even simplify it further. And so you basically just get a linear combination of logarithms. Get a linear combination of logarithms involving just true logarithms. You just do that. And there's a way you pick the coefficients, and it seems to be a really close approximation. And then that guy, then you can determine sidedness much more easily. You're still not out of the woods. I mean, there's some issues. I mean, I'll mention a technical issue. There's this issue of trinomials. Okay, so look at this first bullet. Deciding the sign of a trinomial with rational coefficients of one variable in time. Rational coefficients of one variable in time polynomial in the input, that's actually an open problem. It's still an open problem. I have joint work with some students of mine, and we're getting close to solving it actually. But it's not a trivial thing. Even just like one polynomial, three terms, determine the sign of that. Determine the sign of that at a rational point. Not trivial. And thanks to work with Eric Boniface and Wei Xin Ding. And Weixundeng, we're close to getting an algorithm for that problem. So, this is very much related. So, this is one of the things that's underlying these techniques. Thanks. Okay, thank you, Tima. Thank you. Do we have any more questions, remarks?