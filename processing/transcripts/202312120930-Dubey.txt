Although today my talk is on two-sample inference for random objects, and we just saw in Professor Jin's talk that indeed, I mean, it may have a lower rank in terms of the scores when you do the topic modeling. But I'm going to try and propose this new tool, which could still be useful for analyzing samples of random objects. For analyzing samples of random objects. So random objects are. Okay, so first I'm going to describe what are random objects. We start with a separable metric space omega d. And often we have observations that are taking place, taking values in such complex spaces. And in a minute, I'm going to show you what could be some examples of those. Examples of those, but for now, just to set some notations, let x be an omega-valued random object which is generated according to a law of p, and x1 to xn are the observations that we get out of this distribution. So a very common form that we come across all the time is when your space is Rd and when the metric between two data points is just the Euclidean metric. Euclidean metric, but we are going to try to move beyond those and consider observations that are taking values in general metric spaces. So what are some examples? First, it's very common these days to observe distributional data. So suppose you have, it's going to come up later as one of the examples where we have a number of countries and for each country, we have their age at We have their age at death distributions. In that case, we observe a sample of distributions. And in this case, in this particular case, we are looking at univariate distribution. And one very popular metric to model them is the L2 distance between their quantile functions, which is also the popular 2-Wasserstein metric. So, I mean, there could be other choices, but because this is the metric that I'm going to look at later. This is the metric that I'm going to look at later. I'm just coupling omega and d according to the examples that are going to appear later. But these are just some examples. So, this kind of data could be equipped with many different kinds of metric. We could come across matrix value data where each data point could be an SPD matrix or a network in itself. And this is very common in neuroimaging studies where you might have for each subject their brain connectivity networks. And there could be multiple choices of And there could be multiple choices of metrics in such spaces. And in the example that we'll talk about later, I'm going to use the probenius metric. Another popular data that's very common these days also appears in many, many forms, but especially in microbiome data analysis is when you have compositional data, where each of your observation is in the form of a vector with non-negative entries that add up. With non-negative entries that add up to one. And again, there could be many possible choices of metric in such spaces, but we look at this, we project those vectors on a sphere by taking the square root and look at the geodesic distance between them. So what I just talked about are some different possibilities of data which are taking values in a metric space where we do not have vector space operations like a dish. Vector space operations like addition, subtraction, or scalar multiplication. For example, like if for each subject you observe a distribution, you cannot multiply those distributions with a negative number and end up with another distribution. Or you cannot subtract two networks and get another network. So these data points are not taking values in a vector space which are closed under these operations. And neither do they have canonical ordering. But what we do have are the pairwise distances. Have are the pairwise distances between them. And we are going to use these pairwise distances as a tool to draw inference about this very complicated distribution, P, that's generating such data. And we are going to take an umbrella approach that works for many, many different kinds of spaces and metric and not tie it to one single form so that this can be an omnibus test that can be applied to many different problem settings. Settings. So, the key tool that I'm going to use is something called the depth profile. And it's a very simple thing. It's just so to describe it, it's just the distribution of distances around every point. That's the way to say it. So, if you're at a point omega, the depth profile of omega is the distribution of distances of the Of distances of the random variable around omega. So, one way to imagine this is that you draw balls of increasing radius around omega and then you find the number of observations contained within each ball. And why is this important? Because suppose you want to guess the centrality of any point. And if that a point is a central observation, then distances. A central observation, then distances around it will tend to be smaller, which means its depth profile will have more mass on smaller distances as compared to a point which is on the outskirts, in which case its depth profile will have more mass on the higher distances. So, this simple device, which is just looking at the distribution of distances around every point, can give you a lot of information about how your complex data is. How your complex data is distributed in general. And now, looking at it in a more like formal way, can these very simple depth profiles in any way characterize the complex distribution P that generated your data on the metric spaces? So, from very classical results, it turns out that if you suppose you have, if this measure is a boreal process. You have if this measure is a borel probability measure, then if you know its value on the borel algebra generated by the open balls, then you can uniquely characterize this measure. So what this really means is if you know a measure on all open sets, on all open balls, their finite unions and intersections and complements, you can uniquely identify the measure. But, and which translates to this statement that if you knew the law of this stochastic program, If you knew the law of this stochastic process as a process in omega, then you uniquely can determine p. But what we are asking is a more ambitious question. Can the collection of all the depth profiles uniquely identify P? So the depth profiles are much simpler than the law of the stochastic process. In fact, those are the one-dimensional marginals of this stochastic process. So what it means is. So, what it means is you just know the measure on all open balls. Then, can you uniquely identify the whole measure? And like, quite intuitively, we would think this is true in R D, but unfortunately, the answer is no in more general spaces. There are counterexamples. So, I'm going to leave this question open for now and come back to this later when I'm going to discuss in depth why we need to understand this. Uh, to understand this, so going to the problem that's going to be the main focus of this talk, suppose you have x1 to xn being generated from a distribution p1 and y1 to ym being generated from another distribution p2, we want to test p1 equal to p2 versus p1 not equal to p2. So this is the age-old classical two-sample problem. The twist is that x1. The twist is that x1 to xn is not in a typical vector space but rather in a more complex metric space. So, p1 and p2 are really very complex distributions. And the idea is we will compare the depth profiles of each observation with respect to its own group versus with that with of the with respect to the other group. So, here's a toy example. Here's a toy example. Suppose you have two groups: the orange one and the blue one. And focus on this one observation, which I have highlighted here. What you see as the blue curve is its in-sample depth profile, which means it is the distribution of distances around it with respect to its own group. And the red curve is the out-of-sample. Is the out-of-sample depth profile, which means how it stands with respect to the other group. And you can see for this particular observation, the two curves look very different. And then we can find such pairs of curves for every observation. So the goal is to design this test statistic is to combine all of the two curves that you can get out of every observation to see how different they are. How different they are. So, this is the test statistic. So, for each observation Xi, we will look at its depth profile with respect to its own group and that with respect to the other group. And we'll do the same thing for every Yi as well. And then we will look at these profile differences over all U. So, these are curves, right? So, you look at the differences for every U. So. So this thing TNMU is a function of u, and u is the radius around each point, like to capture how many observations you can get in it. And to aggregate the information over all you, you can either integrate it with respect to you or take supremum, or like you can use your favorite way of integrating this. Of integrating this. But what we will show, so this is just the idea of building the test statistic. But to take it from here to a viable option, we need to work hard to first find what could be the null distribution of this test statistic. And as you can see from this form, that this is not an easy task because each of these depth profiles are estimated using the same data and then. Data and then the whole statistic is also evaluated using the same data. So these are heavily dependent on each other. And a lot of work needs to be done to basically cancel out all the dependencies and retain what's important in the null distribution. So here is how the null distribution looks like under certain regularity assumptions. I'm going to talk about those in the next slide. Next slide. Here I'm going to describe the null distribution of aggregating the information of TNMU in the form of an integral, but such results will carry over also to the supremum because we have the whole process convergence. But suppose you are just integrating this, then the test statistic TNM will converge to an infinite mixture of chi-squares. And this is something that's And this is something that's quite familiar to us because you can see from the form of the test statistic that it's some form of grammar-von Misses type statistic comparing these distributions, although it's in a more complex, it's represented in a more complex way here. So the distribution quite naturally is a mixture of chi-squares in this case. And these mixing weights, however, are data dependent. Data dependent. So even though that's not the most encouraging result, yet it still gives us a lot of it gives us a lot of tools to justify how we are going to implement this test in practice. But for now, we can derive a concrete asymptotic distribution for this test statistic. Now, going, so this null distribution is derived under. Null distribution is derived under certain assumptions. And what do we need? So, what do we need for the null distribution to work in practice? For now, we assume that the depth profiles are continuous. So, the complex data are not in a discrete space. So, some hard work can extend this to discrete space, but we are taking it in this continuous framework where we need the depth profiles to be absolutely continuous with positive density. With positive densities that are uniformly upper bounded. And a restriction that we have on the space is in terms of its covering number. So n epsilon omega delta omega d is the covering number of the space omega with balls of radius epsilon. And we need, and log of the covering number is the metric entropy. And we need it to satisfy this. Satisfy this. So, as epsilon tends to get small, the metric entropy tends to explode, right? Because if you want to cover your space with smaller balls, then you will need a lot of balls. So, basically, this just puts a condition on how fast your metric entropy can increase with respect to epsilon. And this is a very flexible requirement, which allows us to play around with a lot of metric spaces. lot of metric spaces because even for the Euclidean case, the metric entropy is polynomial in 1 over epsilon, which means log of that is of the order of log of 1 over epsilon. So this condition is easily satisfied, but it's also satisfied for many other object spaces that we are looking at in this talk and also not looking at in this talk. Like many, many examples will fit here. So. So, just a key result that helps us establish the null distribution is the uniform convergence of these depth profiles as a process in omega and t, which we can show under these assumptions to converge to a Gaussian process, which is zero mean and has covariance of this form. So, I'm going to not focus too much on this, but move on to now discuss the other aspect of this test, which The other aspect of this test, which is how does it do in terms of power, and as I discussed earlier, this final asymptotic distribution is still data dependent. So as a like keeping with the tradition, we run a permutation test to get the critical value. But can we, how good is this permutation critical value? Does it satisfy? Critical value: does it satisfy the like? Does it keep the type one error of the test? And how good does it do in terms of power? Now, to quantify the power of this test, we are going to look at this quantity dxy, which is the population target of our test statistic. So, if you can see how the test statistic looks like, we basically look at the average. Differences in these depth profiles across all the observations. And that's what this quantity is doing. And in fact, under some conditions, dxy is a divergence between the distributions p1 and p2. So of course, under the null, dxy is 0, but under the alternative, like if dxy is bigger than Like if dxy is bigger than zero, under some conditions on omega d and p1 and p2, one can say that p1 has to be not equal to p2. So in some way, dxy captures your alternative. And here we are going to look at a class of alternatives that shrinks slowly to the null. That means we are looking at the collection of p1 and p2 such that dxy is an m where a nm goes to 0. Where A and M goes to zero. And under such alternatives, we are looking at the power of the test, which is operated under the permutation critical value. Then we can show that even with the permutation critical value built in and under this challenging alternative that shrinks to the null, the power of this test will converge to one as long as you're not too far away from the null. So the condition we need is n. null so the condition we need is nm by n plus m of an m goes to infinity so a n m cannot decay to zero too fast so what this really means is if you run this test as long as the distributions that generated your data are like within this much away from each other captured in terms of dxy you can detect the difference uh now when is this When is this DXY really a divergence? Like, when can it detect? What all alternatives can we detect? And here I'm going back to the slide that I showed at the very beginning of my talk. And this one, basically, I asked the question like, which are the distributions P that are uniquely characterized by their depth profiles? And this is exactly the class of distributions where this test. Where this test will work to detect how different they are. Now, this has been a very, like, this has been a research topic that has existed since a long time. And it has appeared in different forms. People have figured out sufficient conditions to say, okay, if this and that happens, then the debt profiles will uniquely characterize P, like measures will be uniquely characterized by open balls and so on. Balls and so on, but I was not able to find an if and only if condition. But what I could at least show is like, so I'm trying to compare them with all other distance-based tests that exist. And a common assumption that's taken there is if your space is of strong negative type. So, in the interest of time, I'm not going to describe that in detail, but if But if you're operating in the setting of existing non-parametric tests based on distances where they assume that the space is of strong negative type, which includes all these cases like Euclidean spaces, separable Hilbert spaces, hyperbolic spaces, and so on. Then as long as your metric that you are working with in your example is of strong negative type for some power gamma, then the depth profile. Then the depth profiles will uniquely characterize the two distributions P1 and P2. So, basically, at least if we are in the setting of existing tests, then we can certainly detect the differences, but we go beyond because even if the space is not of strong negative type, but if you satisfy certain assumptions like this, the test will manage to catch differences between the distributions generating your data. Data. So now I will first show some examples and then I will discuss the other methods that I'm comparing my test against. So the other methods are actually all distance-based or graph-based where the graphs are derived from the distances. And in this case, since we are looking at mean difference, I've also included hoteling. So we are looking at normal data. So this is just a Euclidean. Data. So, this is just a Euclidean example where your underlying space is Rd. This is something we have seen and worked with and know what's going on. So, we have 100 observations in each group. The first group is normal 0 sigma and the second group is normal delta sigma, where delta is along the leading eigenvector of sigma. And in this case, you can see for different dimensions, we are doing pretty good. Of course, in this case, we are not the winner, but Are not the winner, but we are doing quite competitive with respect to the other methods. Although Energy Test is the winner for detecting mean differences, and somehow it's too hard to beat. But if we are now having scale differences, then we can see that the new test does pretty good and outperforms the other methods. And the differences here are in the scale parameters of the two groups. Scale parameters of the two groups. Here is another hard-to-detect scenario where we have the null distribution to be a single component. That's the normal distribution with zero mean and identity matrix. And the alternative is where this single component breaks up into two components where the means are negative mu and mu, with mu looking of this form. Looking of this form, and the distribution is basically a mixture of those two components. And in this case, we are doing very good to detect this kind of differences. In this one, we compared normal versus T. So it's like comparing normal with other heavy tail distributions. And here also, we are doing much better than the other existing tests. And finally, a non-Euclidean example. So in this case, So, in this case, we are having two samples of networks where the networks have 200 nodes each and are generated according to the preferential attachment model. So, the generating mechanism is that the nodes keep getting added to the network proportional to degree power gamma, where we set gamma equal to zero as our null, where there is uniform attachment. So, no preferential attachment. And under the alternative, we set gamma. We set gamma, we basically vary gamma greater to be taking values greater than zero. And in this case, we are doing much better than the other methods. So that was for the simulations. And now we tried our test on a few real data examples. So this is the human mortality. These are distributional data derived from the human mortality data set where we are looking at. Where we are looking at age and death distributions in 11 Eastern European countries. And you can see for the females, so basically we have 11 age at death distributions in the year 1990, which is our first sample, and in the year 1993, second sample. And then we look at this for both females and males. And visually, you can see that. That the curves are not too different for the females across the years, but for the males, they could exhibit some differences. And here are the p-values that we got by evaluating our test and the others. And we do catch the differences. And this is quite explainable because this is a period of political turbulence in these countries where males resorted to a lot. Resorted to a lot of drinking and had early deaths in their 40s, and was not a very distressing period related to male mortality in these particularly in these Eastern European countries because of the political situation. The next example is where we have samples of functional connectivity networks in different age groups. So the baseline age group is 55 to 70. Baseline age group is 55 to 70. And then we have samples from 60 to 75, 65 to 80, and so on. So age is increasing along the x-axis. And then we look at samples of these functional connectivity networks across 264 regions of interest and compare these samples of networks to look at their empirical power curve. And our test is doing much better than the others. Test is doing much better than the others. So we also compared the functional connectivity networks between normal subjects and Alzheimer's patients. And so we can get significant differences between them based on this atlas. And this atlas is actually quite popular in Alzheimer's disease literature, where it was designed to catch differences between, like, of Alzheimer's disease patients from other groups. Alzheimer's disease patients from other groups. So, what all checks out is in this test, we have basically established a non-parametric distance-based test, which has guaranteed type 1 error control and consistency of the test, even under alternatives very close to the null, and using permutation p-values. So, all of these theoretical guarantees are operated using permutation p-values. We do not need tuning parameters. We do not need tuning parameters, which is a big thing because most often, like even though, like, it may all of the theory may work when doing simulations, tuning parameters could be a hard thing to decide. We do not need sample splitting. And if I've convinced you, empirically, the new test looks quite promising. So, now I'm going to discuss the other methods that I compared against, which includes a graph. I compared against, which includes the graph-based tests and the kernel-based MMD tests and energy tests. Of course, this is not a complete literature on existing distance-based tests, but some that I have compared against and most popularly used. And with respect to all of this, none of these methods will actually check out all four. So either they are tied to tuning parameters or they may not have all the theoretical guarantees for. Theoretical guarantees for consistency of the test. Sometimes they may need sample splitting and so on. So, with respect to all of that, I'm hoping that this new test can still be effective. And thanks to my collaborators, Hans at UC Davis and Yaching at Rudgers. And thank you for your attention. And this is the archive link to the paper that we just put up. 