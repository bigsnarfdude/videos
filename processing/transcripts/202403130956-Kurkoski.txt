Okay, great. Thank you so much for the invitation. I'm really happy to be here. Yeah, so this is joint work with Shin Ki Guan. He's a graduate student at JIIST. That's the Japan Advanced Institute of Science and Technology. And Lei Liu. So he was at JIIST, and then he moved to Zhejiang University last year. And so, yeah, honestly speaking, Lei Liu did the heavy work on heavy lifting on this. And this is my first time presenting it. So apologies for anything that's not very smooth. Not very smooth. And then this morning at breakfast, Way asked me, Brian, where's your abstract? And so I sent it to him, like, I sent it to Way like five minutes ago. And it's already on the website. What's the abstract? So I'm going to talk about, mostly I'm going to talk about memory AMP, which is sort of a framework for AMP-type methods that uses memory, that uses memory, and then through That uses memory, and then sort of a specific realization of that that uses gradient descent. It's basically in the family of the OAMP and VAMP algorithms. And then I'm going to talk about two sort of specific implementation details of this memory AMP that we found dealing with the problems of overflow and getting the complexity down a little bit. Okay, so here is the system model. So the standard problem is that we want to estimate x in the linear system in the presence of noise. We have this matrix A, which is an M by N matrix, and we have Gaussian noise and all these A, Y, sigma squared, and also the distribution on X is known. We're using this large system assumption, which is usual, so M and N can be very large, but the ratio between M and N is fixed. But the ratio between m and n is fixed. A is right unitarily invariant, and the x is id with these assumptions for convenience. Okay, so we want to find an estimate of x, and so what's a good estimate of x? Well, minimizing the mean squared error is very reasonable to do. So if x was Gaussian, then we could use standard linear algebra check means linear M and S C in order to find x. We find x. But for non-Gaussian x, for example, if x is sparse, then solving the problem is generally np-hard. And so that's why this class of algorithms called AMP have been getting so much attention because they can solve this problem efficiently. So this AMP class of algorithms, they iterate, they operate between two estimators, a linear estimator and a non-linear estimator. Linear estimated. In the original version of AMP, sort of the big insight was that they had this on-sigran correction term, which makes the error Gaussian. And there's this nonlinear element, which yesterday Wei gave a very nice explanation of how the Berkeley service AMP correction hired this AMP denoiser functioned as a soft d mapper. Function as a soft D mapper. And so this is Bayes optimal and it's relatively low complexity because you just have vector matrix operations, but it requires an A, the analysis requires an IID matrix A. But in practice, you might have matrices which are not IID, they might be correlated, and A and P may perform poorly in these cases. In these cases. So there were two papers. One is called OAMP for orthogonal AMP, another is called vector AMP. It seems like many people are in one camp or the other. So Lei Liu did his postdoc with Li Ding, so we kind of followed the orthogonal AMP approach. But the idea here is that for the linear estimator, for the linear estimator, you perform a LMMSE. A LMMSE operation. In addition, there are the orthogonalization parameters so that the error term is orthogonal to the message. As a result, this is Bayes optimal, but it works for not just IIB matrices, but for unitarily invariant matrices A as well. But there is a matrix inverse right here, and so that makes this high complexity. So that makes this high complexity. And so when we come to memory AMP, we're going to see that our goal is to get rid of this matrix inverse. So convolutional AMP, I guess we don't call this CAMP, although it's very tempting to pronounce it as CAMP. It's more in the framework of the original AMP. So it uses memory. It uses memory in the correction term. Memory in the correction term. And so rather than using just the most recent iterate, it uses the history of all the past iterates to construct the correction term. And so if this converges, it is Bayes optimal and converges for unitarily invariant matrices A. It's low complexity because we don't have this matrix inverse, but actually it converges quite often, but we found It converges quite often, but we found that it fails to converge for A which has high condition numbers. And so, in order to address this problem, we developed memory AMP. So, memory AMP is, you can think of that as a framework. So, the linear estimator is sort of linear combinations of the current iterate and all of the past iterates as well. And then in the And then, in the most general case, we could also have the nonlinear estimator would have access to the current value of R and all the past values of R. So we have our fogonality conditions to be satisfied. So this first one, all right, so the point here is that we have memory. I guess I could use a pointer. So right, so we have the history, the current iterate, and all the past ones at the end. Iterate and all the past ones at the input here, and the current iterate and all the past ones at the input here as well. So we have orthogonality condition. We require orthogonality between the error at the input here and the signal. And so this is present in OAMP. So AMP has this orthogonality condition. And then additionally, we require new orthogonality conditions on this F matrix and this error term and the And this error term, and the G matrix, and the F terms. So the idea of memory AMP is that AMP, OAMP, VMP, and convolutional AMP can all be described using this framework. But that's not an algorithm, so we need to move towards an algorithm. And so the idea here is that the linear estimator has memory, and it consists of a local estimator and orthogonal estimates. Of a local estimator and orthogonalization. And so the idea is that this local estimator should try to approach this term which appeared in the OAM V. And that the error of R and T is orthogonal to X and to the errors in the iterates. We add a damping term. So the damping term is selected to minimize the variance. So it's analytically optimized to guarantee convergence, and it also improves the convergence speed. Okay, and then for the non-linear estimator, so in the general framework, in the general framework, you know, the non-linear estimator has access to memory, but here we don't actually need memory. We don't need memory. Okay, so here is memory AMP with gradient descent. So we have an expression for the linear estimator. We have an expression for the linear estimator and an expression for the nonlinear estimator. So the linear estimator has two steps, it's u sub t and r sub t. So the idea that u sub t is an estimate of this term from the OEMP. And so as we iterate, we get closer and closer to this exact expression. And so there's a lot of details which are omitted. So there are these parameters theta and c, which are optimized parameters, and the Are optimized parameters and the p and epsilon are chosen to ensure orthogonality. So all these computations, I haven't shown them, but we're going to talk about them more later. The theta is the optimized damping vector, and then this denoiser is the same as in the OAMP case. Okay, so the idea then is that we're going to use gradient descent to approximate this term from OAMP by U sub t. Use of teeth. Okay, so why does memory work? So, I want to give you some intuition. Again, we want to eliminate that matrix inverse in OAMP. And so a little bit more generally, just going back to first principles, what we want to do is to solve w times u equal to b without explicitly finding a matrix invoice. So there's actually two different ways to do that. One way is that this underwriting is a very good. Is that this unconstrained quadratic optimization problem has a solution of W inverse B. So one approach is simply to find a solution using gradient descent. So the simple gradient descent is just like this, and so we can find the gradient of f explicitly. And so if we iterate on this, we'll get w inverse times v. And this is use of i's will correct. And this of this use of i's will correct approach the correct value for u. And then choosing the step size to be 2 divided by the sum of the max and min eigenvalues is close to optimum. Okay, so that's sort of an intuition about why gradient descent, sorry, that's why memory might be useful, the connection with memory and finding this matrix inverse. I thought this was a little bit funny. I thought, well, maybe ChatGPT could help me explain why. GPT could help me explain why this is useful. So I asked ChatGPT: Is it possible to find a matrix inverse using gradient descent? But ChatGPT was feeling a little bit snarky and just said, well, there might be unconventional methods or iterative approaches that can find the matrix inverse, but it's recommended to use established linear algebra techniques. So actually, apparently what we're doing is unconventional, and well, it's definitely intricate. There's another intuition, which is intuition which is turns out to be identical. So if we let rho of C denote the spectral radius of C, if the spectral radius of C is less than 1, then the Newman series, so the Newman series converges and we have I minus C inverse can be computed using this series. And so if you choose C is equal to I minus W, it may well be the case. It may well be the case that a spectral radius of C is greater than 1, so we define C prime, choose C prime and theta so that spectral radius is less than 1. Then we have a new set of iterations based upon the Newman series, which also approaches W inverse times B. And it turns out that this iteration is identical to the gradient descent of the second idea. So basically, there's two different ways to look at why memory should give us the correct matrix inverse. Okay, so this is an overview of the OAMP type algorithms. Just to review what I said, so the original AMP is valid for IID matrices, but may not converge for correlated matrices. Correlated matrices. This was solved by OAMP, but it has high complexity. Convolutional AMP has lower complexity, but it diverges for high condition numbers. And then memory AMP, which we're describing here, it can tolerate much higher condition numbers. Okay, so this is just a numerical performance of these different algorithms. So, first of all, So, first of all, so this matrix, so the test condition here is a unitarily invariant matrix, and so AMP performs very poorly in this case. The remaining algorithms of convolutional AMP, memory AMP, and OAMP, they all converge to the same fixed point. OAMP has the fastest convergence, but it has high complexity. And then the convolutional AMP has low. Evolutional AMP, it has lower complexity, but it has slower convergence. It also appears there's some problem with its state evolution. And then the memory AMP has lower complexity and faster convergence and correct state evolution. Okay, so for the rest of the talk, I'm going to talk about a couple of implementation aspects of the gradient descent AMP. I tried to make this interesting so that it could relate to something that you might have seen before. Implementation details aren't necessarily that easy to present. So the first one was we had an overflow problem, and probably almost everybody has likely had an overflow problem. So just to remind you, usually, you know, when you're working in MATLAB or Python or something, they're using double precision numbers, which are based upon the hardware that we're using. And you can represent a number. And you can represent a number as large as 10 to the 308. But if the value is too large, then you get the dreaded not a number will appear. And so we were working with this gradient descent algorithm, memory AMP, and indeed, particularly we wanted to show that it would converge for high condition numbers and it was going in the right direction, but then suddenly an overflow error occurred. And so the first thing that we tried was MATLAB has a variable precision arithmetic. So you can turn off this double precision and make the precision as big as you want. And so you can, besides increasing the precision, the maximum value gets larger. And so that solved the problem, but it's very slow to converge. And so it requires additional computational overhead. So that wasn't really the right solution. So most people, Solution. So, most people they would just go into their code and tweak it or something. In our case, we thought this was really interesting and found a nice solution and made an ISIT submission out of it. Okay, so which intermediate variables cause overflow in GD in the gradient descent, MAMP? So again, there's all these additional parameters which need to be computed, and I tried to vary the details. To vary the details, but I can't quite vary all of the details. So we need to compute these internal intermediate variables, b of k and w of k. So b of k is defined as the trace of this matrix, b to the kth power given here. But it can be computed if the eigenvalues of a, h, Hermitian are known. And so if the spectral radius is If the spectral radius is, you know, if the eigenvalues satisfy this condition, the spectral radius is greater than one, then this negative two k increases exponentially, and then usually the wk's also increase exponentially, and everything blows up. And so this is just a graph. This is the graph that we made showing k, the number of iterations, versus the internal parameter w sub k. Parameter w sub k, and it's increasing exponentially because this is a log scale. And here's the maximum of 64-bit floating-point numbers. Okay, so it's definitely a problem. Okay, so this is what we want to compute. This is what we want to compute, and this wk is increasing exponentially, but it always appears in a product, theta times wk. And this theta is, this product is bounded. is this product is bounded. That is, this theta is small. This theta is small. So what that means is we want to be able to compute the product without having to compute the product without having to compute the two terms individually. And so this expression gives the direct computation. If we know all of the eigenvalues of B. So if we know all of the eigenvalues of B, then we can compute. Values of V, then we can compute this product without having to compute these exponentially learned values. Okay, so honestly speaking, I think this is a little bit detailed and not so meaningful to present it. I think it's just, yeah, it's speeding up the computation a little bit. Let me skip that slide. And then also we have a theorem here. And so the question is: does it make sense if I didn't present the one before? Doesn't make sense if I didn't present the one before. But it started at 20, right? We've got like 10 minutes longer than so many. Oh, okay. Well, okay, well, the central idea here is that computing this requires order m computations, and we have to compute them t to the cube times, t to the cube times, or t is the number of iterations. So the overall complexity is m to the cube. So the overall complexity is m to the t cubed, and we can do some complexity reduction simply by defining this c of chi of k, right? The student likes to, was running out of letters, Greek letters, and I think he used every letter in the Greek alphabet. And so we pre-compute these terms before the iterations begin. And so this term can be basically computation of. Basically, computation of this theta times WK can be reduced to some scalar operation. So that means if we pre-compute these, it costs order mt, and computing the terms involved in wk costs, still costs of t to the cubed, so the overall complexity is reduced. And then the point here is that we need to make sure that this new intermediate term is bounded, and indeed it is, and so that there's no risk of overflow. Of overflow. Okay, so that assumes that the eigenvalues of AAH are known, but in a large-scale system, the eigenvalues may be impractical to compute. And so, yeah, in our paper, we have a method to compute the maximum and minimum eigenvalues. And so we can perform, we can approximate this chi. Approximate this qi by the following approximation. And in this case, we don't need to know all the eigenvalues, just the maximum and minimum one. So this lambda dagger plus is just the average of the maximum and minimum eigenvalues. And interestingly, this recursion is initialized by noise rather than by zeros. Okay, so how does that work? So again, here is a matrix with a high. Here is a matrix with a high condition number. And so here's the number of iterations versus MSE. And so if we don't do the overflow adjustment, then the iterations progress and then suddenly they just stop here when we overflow. But when we continue doing, continue the iterations using the overflow compensation, then the MSE progresses towards the MSE progresses towards the fixed point. In this case, we also showed the result if we don't know the eigenvalues and just approximate the min and max eigenvalues. And in this case, it shows that it's a little bit better, but this is just one sample. And so in other cases, it might be a little bit worse. But anyway, approximating the eigenvalues seems to give very similar performance to knowing the eigenvalues directly. To eigenvalues directly. Okay, and then the final section is complexity reduced MAMP. So we got rid of the matrix inverse. We got rid of the matrix inverse. So what does the remaining complexity? So the remaining complexity is dominated by matrix vector products. And so if you naively look at the iterations, At the iterations steps, it looks like that there's four of them. One is that we have to compute this a times phi to estimate the covariances. This is hidden in the computation of the zeta function, so it's not written up here. Computing b times u requires 2, because we multiply u times ah, and then that vector times a. And computing ah times ut requires 1, but oh, actually, we're computing ah times. Oh, actually, we're computing AH times UT two times. So, actually, we only need three matrix vector products. So that's kind of trivial. So, what's less trivial is that we're going to get rid of this matrix vector product. So, estimating this damping vector nominally requires one matrix vector product. So, I just listed the steps here. There's the matrix vector. The steps here. There's the matrix vector product. And so what we can do is that we move the damping from the NLE to the LE. So I omitted the details. So that actually eliminates this computation. But actually, this C normally depends upon Zt and this V, which is the covariance matrix. But we found that approximating C, we could actually approximate to C. We could actually approximate to C, and it gave little to no performance loss. So, this C tilde is just given by this term. And as a result, we reduced the complexity of this gradient descent, MAMP, to two matrix vector credits per iteration by making a small approximation. Okay, so here are the simulation results. So let's hear what do we have here. We have condition number of 1,200. Of 1,200. And so, this okay, yeah, so I think last night Shunki added this for me because we were talking about it. Right. So, the point is that this is the number of iterations. So, the complexity-reduced version, there's this approximation term added, and so there's a few more iterations are required. We still reach. Are required, we still reach the same fixed point. But when you look at the number of matrix vector products, so here the horizontal axis is the number of matrix vector products. Now the complexity reduced version is actually much better in terms of complexity. So what we did is that we went from three to two matrix vector products. And so the improvement here isn't quite two-thirds or one-third reduction, but it's close to it. One-third reduction, but it's close to it. Okay, so let me wrap up. So, gradient descent MAMP, it's a memory EMP algorithm that converges for unitarily invariant matrices. It has low complexity, avoiding the matrix inverse, and it converges for A with high condition number. This is overcoming some weaknesses of other algorithms. And then a little bit more specifically to solve the overflow problem, we gave a Problem, we gave an overflow avoiding variant, which when the eigenvalues of AAH are known, the algorithm is actually the same, but otherwise if we approximate the eigenvalues, then we can achieve nearly the same performance. And then the original algorithm required three matrix vector products per iteration, and we reduced that to two by making a couple of approximations in the algorithm. Of the algorithm. Okay, thank you very much.