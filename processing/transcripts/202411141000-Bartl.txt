is I explain the problem, then the mathematical difficulties, and then give you a result. And coincidentally, there's also just three slides for this. Good. So at any point, if you want to ask questions, please feel free. Yeah. Okay, so what is the setting? So we have a convex function or a function f that actually depends on two things. One, One variable x, which we let's call it action, then on some random input xi. And our goal is simply to search for the best possible action. So x star just minimizes this expectation, which you can think of as expected cost. Right. So there's tons of examples like in statistics, obviously, there's, for instance, linear. Obviously, there's, for instance, linear regression you can write as this form. So, here ψ would be like a d plus one-dimensional vector, and you just search for the best linear approximation of the target y by linear combinations of x. Or how I came to this problem is like portfolio or utility maximization problems and then math finance. Very good. So, and the goal of this talk. The goal of this talk, or our goal today, is to estimate this optimal action X star based on statistical data. We don't have access to the distribution of psi, so in the stock, for instance, but we only have psi one to psi n, so capital N many, let's say, IID data. And using this data, we aim to find a guess for. find a guess for the optimal action x star cool so there's a quite natural way to do do so simply we remember um well back into the first second year of of the bachelor that the sample mean um so this guy here approximates the true mean like there's law of large numbers and so forth central limit theorem and as it happens if you additionally have this option You additionally have this optimization problem on top, it doesn't destroy this convergence property. And what I'm butchering here a bit is the central limit theorem for this optimization problem. So of course, what I mean here is that this sample estimator minus the true optimal action, if you rescale it by square root n, it exhibits asymptotic normality. Yeah, but in order to write it a bit, yeah. Bit more concise, I'm using this notation. And I mean, if you have such a result, like if you have that this distribution of the sample average approximator essentially highly concentrates around the true optimal action X star, then you automatically get confidence intervals, you get like prediction properties, like so out of samples performance of this estimator and so forth. So, this is a very desirable thing to. So, this is a very desirable thing to aim for. And right, so this is what we want. And the questions that you can ask or are quite natural in this context are at least the following: three questions. So, firstly, like, yeah, the central limit theorem is obviously an asymptotic statement. So, as the sample size goes to infinity. As the sample size goes to infinity. So the question is: does it hold approximately for finite n? How close does it hold? So, how large do I need a sample size, the data that I need to get such a result? What happens if the randomness is more heavy-tailed and all the problems in math finance, for instance, are essential? I mean, are never like Gaussian, they're always to some degree heavy-tailed. Some degree heavy tailed. So, what happens to the classical results in such settings, and also what happens if some data are corrupted. So, I will give an example to the corrupted data in the next slide. But I mean, what you can think of is maybe, yeah, think of this as a classification problem, as the data as images together with the label. And so, ψ1 is the dog image of a dog with the label dog. Dog image of a dog with the label dog and so forth, and corrupted data simply means that some of the data are false. So, there's a dog here in the data that is labeled as a cat. So, what is the effect of such mistakes in the data? Yeah. And yeah, what turns out that all these questions are like really intimately related, but my feeling is that for this talk, let's just think about the corrupted, focus on the corrupted person. Corrupted data. Good. So let's see what happens under corrupted data. How much does it destroy or doesn't destroy this convergence result somehow? And this actually for this, let's consider a very simple case. F is just subscribe, say, one-dimensional random variable. F is just. Is there a question? Is there a question? No. F is just the square distance. And then, of course, like the minimizer, which minimizes expected expectation to xi square is just the mean of this random variable in this example, right? So in particular, what we actually do, we want to estimate the mean. And it turns out that this sample average approximator, of course, is just the empirical mean. Measure, of course, is just the empirical mean. And the problem is that even a single corrupted data point can arbitrarily destroy this convergence result. So to give you a concrete example of this, so this actually happened after I got involved in this type of math. But someone here in this research group of Matthias wanted to estimate Wanted to estimate interests of Austrian banks. So there's, say, 100 banks, and like the interest average interest rate is roughly like 3%. All of them are more or less 3%. So you have 100 IID data where you expect roughly 3%, but one of the banks actually mistakenly didn't enter 0.03, but like 3. So it essentially entered 300%. So what happened? Percent. So, what happens in this case that, yeah, if you have 100 data points and one of them is like 100 times larger than you expect, in this specific example, you like get an empirical average of 6% returns. But I mean, this is just to say that if you have outliers or even a single outlier, then this empirical mean or sample mean is just really a poor estimator. And I mean, if you have ever heard of robust statistics, then this is exactly what all of this is about. And the takeaway here is that, well, the sample mean is not robust, and you should aim for or you should construct and consider possibly different estimators. So, how does this work? So, still, we're in this one-dimensional setting. Then, what you can think Setting, then what you can think of: okay, the example that I gave you is why the sample mean has poor properties is because there are really some huge, in the data, there are some huge outliers, so which take abnormally large values. So one of the ideas maybe just truncate all data at a certain level and just use the sample mean of the truncated data, right? This is sort of a very natural idea, or use something based on. Or use something based on quantiles or so forth. But whenever you do that, I mean, of course, you also introduce a different type of bias because I just replace all random variables by just different random variables. So the question is, the key question is, so how to design different estimators, so in this context of the mean even, that have like the least amount of bias while still being as Amount of bias while still being as robust as possible. And what is actually what turns out, this is in this first peak of robust statistics, is that actually what you can do is you can design an estimator for the mean now of a random variable that even if you have heavy-tailed and corrupted data that exhibits the exact same behavior as suggested by the central. As suggested by the central limit theorem. And if you take a second and think about this result, then I think it's really amazing and surprising. Like you have data that could contain really abnormally large outliers, whatever. This estimator just doesn't care and gives you the best possible estimates whatsoever. So, yeah, when I first saw this result, So, yeah, when I first saw this result a couple of years ago, I was really like, I was positively shocked for a while. Yeah, but in any case, so this is in the 80s and this concerns estimating the mean of a random variable. So this is kind of not that interesting. Of course, I mean, the next natural step is to consider mean estimation in multi-dimensions. So, I mean, multi-dimensions, I mean, high-dimensional, so where the dimension is really like not two or three, but like 500 or so. And in this setting, like there have been many, many estimators, but this was only solved like very recently, like 40 years essentially after the solution in the one-dimensional case. And for a linear regression, this was solved essentially at the same time, which is even. Which is even harder, of course. Right. So, our problem, our general problem, if we have like a general convex function f here, is sort of obviously more complicated than these two settings, simply because I can include both settings as for special cases of F. And as it turns out, then there were no known optimal solutions. Known optimal solutions. So you, their no estimator had somehow this property that you get the same as the central limit theorem would suggest. So either the error was way, way, way larger than by the central limit theorem, or you would require like exponentially many data points to say anything, which both things we obviously want to avoid. Right, so that's a bit of the history. So, the result is now actually, yeah, you can do it. So, what turns out is you can construct a statistically optimal estimator in the following sense. So, this guy here is an estimator for the optimal action of our original problem. It exhibits exactly what the central limit theorem would suggest: the same variant, same everything. Everything. Even if the data are very heavy-tailed, even if some of the data are corrupted, even adversarially corrupted. And the only thing you need is the sample size more than dimension. Which for sorry, for the sample size. For sorry, for the sample size. So, this is kind of obvious. Like, you have an D-dimensional optimization problem. If you have less than the sample points, there will obviously be an L space. So, you can't say anything and you can formalize this. Yeah, but I will cut the discussion here. Just maybe a bit more recent results. So, with Stefan, we have started extending these methods to Extending these methods to risk measures, so where instead of the expectation, you have something non-linear here. So, building on prior work with Ludovic and to end this talk with an image. So, one of the things that I'm doing at the moment is in the context of statistics in like nonlinear spaces. So, for instance, one of the prominent problems that you have here is the barycentric problem. Here is the barycentric problem, where essentially we do the same what I said before. You minimize a square distance, but now not in a linear space, but in a space with curvature. So like in, for instance, in image analysis, this is very, very prominent. So here, if I don't know if you can see this, but these are like images of sixes drawn by different people. If you use Euclidean averaging, then you get the nonsense. If you use like a non-size, Sense if you use like a non-Euclidean metric, in this case, the Wasserstein distance, you get something very reasonable what you would get, suspect, and hope for. And for, yeah, here you can also do these kind of methods as described in Ethereum. And that's everything that I wanted to say. Thank you very much, Dan. So we still have like a two minute.