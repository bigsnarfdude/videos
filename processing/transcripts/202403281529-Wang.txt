Also the last formal session for this week. There are three speakers. The first one is one from Boston. Thank you, Eugene. First of all, I would like to thank all the organizers for having me here. That's been really pleasant, really nice, really fun. So I I think I should apologize for uh this is already Thursday afternoon and This is already Thursday afternoon and everybody's tired, but I planned a mass talk. So it's probably going to be a little bit painful for some of you. I hope you had enough coffee. That's not. But there's a reason. So I have this math paper that I think is perfect for this audience. So I cannot resist to talk about it. That's why I thought about it and then I thought, okay, I still want to present this one. Okay, we're all at coffee. Yeah, so that's good. Coffee. Yeah, so that's good. Wait a year. So the talk is very simple. It's based on joint work with Masinus from Columbia and Jamie John from Stanford. The talk basically is one thing. I want to prove this theorem. This is the good thing about Math talk. I can present the main result in the first theorem and I will do it. So I think we all know what complex order is. It's very simple. Commax order orders 200 variables by their expected value of a complex function. And what we want to prove is that what we call a refinement of What we call a refinement of the Strassen theorem. That is, on an automatic probability space, the comma is equivalent to an equality in distribution where x is smaller than the line in common-order means that x is equal in law to a control-spectral one. For some of you, you might think, oh, this is trivial. I mean, you have seen something like this before, but I have to tell you that it's probably not the case. So there's something tricky about it. For this result, if you go from the right-hand side to the left-hand side, it's true. side to left hand side is true. This is just a C equality. So we can all do that. The non-trivial part is from the left hand side to put the right hand side. And there are some versions of this that exist in the literature but not with this generality. So the point of the talk today is try to prove this and some other results with the generality that we want. Is that clear? Somebody already know how to promise? Okay, that's good. So my deepest fear is that after I present this on the first page, someone take a paper, start writing, and then after five minutes, the presentation approved. And then, after five minutes, the person says I approved this. I don't want to listen anymore. Hopefully, this is not the case. So, I will tell you what are the tricks in this and how does it connect to optimal transport. Actually, this result is larger in use in marketing as well. So, let me jump to the market transport formulation. So, a marketing optional transport is formulated this way. The usual transport is that you have a marketing structure nu and a market screen nu, and you try to couple them, and we might expect both. The marketing document transport. The Martin-Dockman transport differs by adding an extra constraint that we want is x and y form a conditional expectation equality. So in another word, you can say that x and y form two period money. Equivalently, you can write this in the kernel formulation. This big K is the other probabilistic kernel, Markov kernel from to mu. That means, I should probably definition, that means that all the kernels such as That means that all the kernels such as you integrate over the first marginal, you get the second one. So that's the equivalent formulation of computer two. And the additional constraint of marginal becomes that when you each drop this kernel, the mean of that kernel should be equal to the point where it is supposed to transport. So we denote by m the set of all multiple transports. Right now I don't need to assume the dimension of my problem, but soon I will have to restrict to the real one. So one of the motivations of the market transport is the pricing of Motivation of the market transport is the pricing of Asian options. This is pretty much well known. So you want to look at this is the risk that all of these properties are evaluated under the risk literal problem matter, the pricing matter. And by looking at two different offering prices with two different maturities, you can calibrate the marginal distributions of the same asset at different maturity. But then, if you are looking at the Asian option, then you need to talk to the kind of structure between the two periods, which is not. Where which is not available from market prices traded on the market. And then what you need to do is you need to look at supremum or minimum or listening with extra condition that this is marketing or this. So that was one main motivation for marketing local transport and that's why market local transport was studied a lot in massive finance instead of the probability literature. But of course now there are more people who are interested in marketing transport and probability as well. So one feature of marketing of transportation So, one feature of marginal transport is that if you look at the quadratic form of cost, then the marginal transport actually all have the same cost because the marginal condition already tells you what the value of the cost term. So, that means if you start to think about interesting cases for marketing transport, you should go at least to the third, not the first to that of the number. This will reappear in the talk, so that's one of my things here. But of course, this works on real line. So, if I'm moving away from real line, then things get a little bit more complicated. A little bit more complicated. Okay, so now I want to mention one concept that was not usually considered. That is the moon property of Martin Lee's concept. So you can start from the very simple case. You look at the x equal to the expected value of y given x. And if you further assume that y is a function of x, then y must be equal to x. This is very easy to prove. So in the fourth direction, we go from the x to y, y has larger complex order. In that direction, there's no way to find In that direction, there's no way to find a mount map unless you are in a trivial case where x is right. That is because the particle is supposed to spread out, so it cannot be a function and still spread out unless this function is identity. So, this is not interesting. That's also why in the marketle transport literature, the mount property, meaning that you transport with a map instead of a kernel, is not studied. Or basically, there's no study on that. But actually, if you go from the opposite direction, you go backward. The opposite direction, you go backward, this is possible, right? So, if you look at x is equal to continuous spectrum y, and you want x to be a function of y, so from the larger distribution, not just a smaller distribution, common sorder, you can actually build a function. You can have a measure of a function. For instance, you can think about y being uniform between negative 2 and 2, and x is a sine of y, and then you form a martingale, actually x is a function of y. So, this is possible. So, in this talk, I will talk about this martingale transport at MMT. Transport MMT, I would omit backward because it can only be backward. The full direction is triggered. And then this is MM to be the set of MMT. And this MMT can be useful for identifying worst-case probabilities, as we saw in the talk on Monday by Yaw. And also, there are some settings of natural problems, like for instance, marriage or worker job imaging in labor market, where you need a function, you need a map. You don't want it. You don't want it. But of course, here I want to omit all these applications. I want to go directly to the result. And I will tell you why this relates to the problem I mentioned in the first lecture. So from now on, I have to focus on R. I will discuss how and why we didn't manage to go beyond R. And then this coise order is the same as on the first page, but now they're probably looking for metrics. These are random variables. And there is a Strassen theorem, this is one version of it, to say that the coise order holds even. That the coin order holds even only if the market goal transport is not empty. That's one way of saying it. And this also relates a lot to the increasing in risk concept well established in terms theory. So the first result, this result is probably the most important protocol. That is, you have convex orders, that means your modical transfer exists. Then we can say that there exists the model transfer that such as the further properties. And the password property is that it is supported on two. That it is supported on two possible regions. One of the regions is this region we call a right reverse graph because the first component is like refunction of the second one. And the second component is an atomic component. So the first one you can see as a Monj map because you're mapping from the second to the first one. That's the one that we like. The second one is the atomic part, which we cannot handle because we all know that there's an atomic part, we cannot really have map even in the plastic case. So, but that part will have to deal with. But in particular, if you assume that you don't have a tool part, so that the mu is actually complex, then you actually find a mong marginal constant. So you have an MMT. So the result is that if you have an atomic problem and you have complex order conditions, then the MMT will exist. That's the thing we try to prove. So, how do I do that? We need this concept of left curtain transport. And this is unfortunately. Curtain transport. And this is unfortunately a little bit complicated concept. It's a little bit involved, but I want to spend a little time on it. And that was developed by the Wiener in a very nice paper several years ago. First, we define a partial order that is mu smaller than u and e, and that is a positive version of comet's order. So here, mu and mu are no longer probabilities. They don't have to have the same math. And then And then this condition is the same as the convex order that we have seen before, like complex order. But the difference is that here I require the convex function to be the negative. And if mu and mu have the same probability math, then this would actually just be common squared. But you could have different cases. For instance, if mu is actually larger than mu, yeah, I don't require them to have the same total math. In that case, actually, mu would always be smaller than this E or because this is negative. Because this isn't a negative. So you have a lot of probability, you have a lot of total math than that. And then you're given that mu and nu having this property, the shadow of mu in nu is defined as the smallest object in commission order that is dominating mu in commission order and also still covered in the set inequality by mu. And it was proved in this paper that this formula exists. Of course, you look at a medium. Problem exists. Of course, you're looking at a minimum income order, so it may not exist. But for that paper, one of the results is to show that this actually exists. So this formulation is actually fine. You can actually have this shadow built. And then the left current transport is the transport such that for the conditional, you look at the conditional distribution on mu from neck to infinity to x, so you look at the left part of the distribution. Now the right part, we forgot about the right part. When we look at the left part, we map it to its shuttle. So this looks a little bit complicated though. So, this looks a little bit complicated, I know. So, in one slide, this is difficult to explain. So, I'll show you one picture and try to help to understand what this current topology is. So, here I have two distributions. They're in common order, obviously. What happens is you first look at the gray part, you start from the left, right? The gray part, you just map it down to its shadow. Sorry, just a quick question. On the previous slide, the shadow, if mu and mu are measures, the shadow is not anti-left. Can you say that again? If this S would mean an empty one, they're both probability measures today, or at least they have the same total mass, correct? I think there's a result like this of the market. Well, you said that if these two are in if these two are in comet order, right? Yeah, but I mean, and you have a strict also in betweenness. You know which result I'm talking about? Sorry, not. Okay, I can talk about it later. Well, if these two have the same total math, then this is common sorder, this becomes common. And this is comma's order, this becomes comma's order, and this will be just taken by view. Yeah, but there will be a strict separation to make one that we can look at. Okay, sure, yeah. So we start with the gray part, and the gray part will be mapped identically to below, because this will be the smallest that dominates the gray part in comet order and still contained in me, in the new, in the new, right? Because as long as you can do the identity, you want to do the identity. Because identity would be the smallest in this sense, because you have to automate it. And when you go to the blue part, you started to make the map started to look like two legs. Map started to look like two lengths. Because each point here, the blue is more than the corresponding density here, so that you have to spread a little bit to the left and to the right, but still you want to maintain the minimum convexity appropriately. So you want the smallest convex order, the dominating measure that dominates mu and covered by mu. And eventually look at the right part and you gave it to the remaining places and the convex order is to hold. So we can always do this regularly and the complex order always holds on each of the local part and also global. Part and also below. This is the construction. And I think, oh, this is my personal interpretation why this is called left curtain. You look at this picture generated by ChatGPT, and this map looks a little bit that you are folding this part, and then you still have something at the back. So it's like a two-layer curtain. So I find this only myself. I didn't see this explanation in the paper, but I guess this was the reason. They draw these kind of transport. By the way, I draw this, it took me hours. It's not so easy, I'm not so good in that. When I draw this with LaTeX, it took me hours. I draw this with LabPak between ours. But it's quite accurate in the presentation of those types of LabPer. The LabKernet Hoppy has this very nice feature that it actually minimizes the expected cost over all marginal transports. Here I don't need the M, sorry. It maps all the marginal transport. There's no second M, the first M. For a strictly convex derivative, H prime. So that means you look at the third-order derivative, if that's positive, then you mean it by this constant. So remember, the first and second-order derivative doesn't matter. So that's why we look at Double matter. So that's why we look at third derivative, and this is actually the uniform matter. So in that sense, this is actually the most natural optimal multiple transport. But of course, if you do strictly, the strictly concrete one, then you look at the right curtain copy. So you start from the right, you do the same construction, and then you get a right curtain copy. But I don't need the right one. I don't need the left. Okay. Why do I mention this? I wanted to define something where we call a barcode transport. That is, we want to decompose mu and mu into completely many singular. mu and mu into completely many singular parts. The intra part is munch. And if we can do that, then we can combine them together and come back to some mux. That's what we wanted to do. So we define density d mu and d nu. d mu is the density of mu with respect to the sum of mu and nu and mu is the other one. Of course, these two add up to one. And the Barkle transport is defined by a sequential construction. You first take, you look at the real line, you look at where the nu is better, not larger. Larger and a half means that mu is better. It's not an a half means that mu is larger, not mu. So the first margin is larger. We take that part, we apply the left part to transport ion, and then after that, we get a twist shadow. And then now I have a mu part, I have a shadow, and then remove them. After I remove them, I remain with a different distribution. And now I continue with this argument, I repeat it, and then this procedure remotes convergence in the sense that all of the maths will be beaten by this procedure. And you have to take my word for the convergence because that needs to be proved. Because that needs to be proved. But intuitively, you think about each time you're taking quite a lot of math. This is the picture. So these are two different Gaussians. The second one has larger variants. And the first, you look at where mu is larger, which is the middle part. The third one has smaller variants. And then you take this part, you map it to its shadow, look at this. And then the second step, you look at these two black parts, because after you remove, this black part becomes a larger one. So the black part has a larger density than the new, mu has a larger density than new. New, new has larger data than new because the new has been removed. Originally, this black part has smaller dust, but after the removal of the first step, it becomes larger dust. And you do the same thing, you map it to the black parts. And then so on and so forth. The reason why the code is barcode is because by doing this, eventually you generate a distribution where you really look at barcode. So all these pieces are charted. So you have many, many, many of these pieces that can complement these pieces. So when you want to implement this parkour transport, do you follow this procedure or is there Do you follow this procedure or is there a faster way? Actually, we didn't try to implement, but we don't know about a faster way to do it. So I would agree to this this way. But for Gaussian, I think, because everything is explicit, I can even compute each step with radically where we are. So that's not a problem. Okay, so now if this is doable, then there's only one missing piece. That means piece is that, do I know this is monk? If I know this is mong, then what I can do is I take this part. I can do is I take this part, I do the mode transport, now I have a mode marketing transport, and now I take the second one, I have a modern marketing transport. As long as these transports don't interact with each other, because each of these is like particle, they're chopping, so they're vertical. None of them interact with each other. That you can show. And in that way, we build an overall marketing concept. So that's our sort of idea. That's actually the main thing of our proof. And that indeed is the case, which comes from this second proposition, our second result. And this result says that. Second result. This result says that if you have an assumption, let's look at the top part. So for the left curtain transport, the support has three parts. One part is the reverse right part, the long part, I really like. One part is a diagonal part where you have identity mapping. And then you have the atomic part on one. You have these three parts. And in particular, we can show that if the first margin distribution has a larger density almost everywhere, Larger density almost everywhere, then the left current coupling actually concentrates on the first part and the last part. And of course, if the last part is atominus, if new is atominus and there's no last part, then I only have the first part. That means I have a non-tranc. So that was the chasing. So if I have d mu larger than half and my new is automatic, then I get a non-commercial as well. Okay, so if this is, I haven't proved this yet, but if this is true, then I can use the procedure that I described just now to get this market, this, this, uh, We got this mark, this Barkhold transport. And this Barkhold transport, because each time I'm taking this latter than half, rather than equal, doesn't matter. This is a continuous distribution. Then each time I get a moment transport. And then I have countlessly many pieces. I eventually paste them together. I get a moment's transform. That's the whole idea. So now the remaining thing is I need to prove this. I need to prove that this structure holds. But this is the hard part of the paper to prove that for a Lab curtain coupler, Lab curtain coupling or Lab colour. The left curtain coupling or left curtain transport, the reverse graph part moves. At that moment, the mu can don't necessarily be probability mesh. This only mesh is okay. No, here is a probability matter. Here, every time we write this, it's a probability matter. Okay, so there are some properties of the left curtain the left current coupling. The first one is that the very quick question. Previously on the slide, is there so let's say you take a decomposition of a new into a purely atomic part and an unatomic part? Does that help with anything? I'm just trying to understand the intuition behind the. You can try to prove things there, but the thing is that if you want a Moon transport, the atomic part will really kill the boon. So Boon will not hold. So if I really want a Moon molecule transport, I had to get rid of the atomic parts. And then you will you will uh only do the purely atomic part. What happens then? The purely atomic program. What happens then? You will not have a monotransport. So, the first property is that this set is so-called left monotone set in the sense that this kind of transport is forbidden. So, you can imagine each of the x origin has two legs. Think about two legs there. And then if you have another origin, x prime, it has a lag somewhere, then this leg cannot step in the two legs of the first guy. So, think about you're walking and there's some. So if you think about you're walking and there's some some other person walking, you don't want that person to die to interact with your back. That's basically what you do. So that's the forbidden coupling that I'm writing there. So this kind of transports is forbidden. The reason is very simple. If this would happen, then you take X from this and you reduce your shadow. So if you were able to do this, then you push this a little bit back, then you come out of the smaller. And then by this definition of shadow, this will be better. That's the intuition. And because we start on the left, that's always. And because we start from the left, that's all we so all these restrictions start from the left. So this X would have kind of a privilege compared to X prime. When the X spread left, this X prime cannot do anything. But this one can freely come in the legs of the person to the right, because we start from the left. That's sort of the intuition. A second one is that this was already known by their semi-hook paper, this one. So the second one is to say that this current propelling has. This light current propelling has either it distributes either on atoms or it depends on legs. So if x has an atom, then that's okay. Otherwise, each of the point x has to have only two lengths, the left lag and right lag. So what I'm writing here is actually not very, it's actually representative. Because this x, if it doesn't have math, it has to have two lengths, which probably one happens. And the third one is something very simple. The third one is to say that if you decompose your Lapker coupling, Your Lapker coupling, then you look at a point where it is identity. So it maps it down directly. And on all of these parts, has to be larger than, which is obvious, because this is identity mapped down, then you have something extra. So on the point where you mapped out directly, that's identity, then the new should occur, second mark should occur. Okay, with these three numbers, I should be able to prove the proposition. So let us only prove the last statement. Let us only prove the last statement. That when you have atom list mu and you have the dμ larger than d mu almost ready, this is the condition that we, when we take the, when we drop the barcode, we always follow this condition. In this case, I have a reverse graph. By doing that, first we notice that if mu is atomless, then I don't know about mu, mu can have an atom, mu doesn't. The second model doesn't have item. So that means if mu has an atom at x, then it cannot map down, because if it maps down, the thing. Mapped down because if it map down, then since there's not room because I don't have an atom here, second. And then by the second lemma, which we have before, if the mu had no atom, this is not an atom, then either you map down directly or you have to light. But the third lemma, if you have atom, so if this is the kernel, so if for x you map to as identity for As identity for probability larger than zero, then in this case you cannot have mu larger than mu. Because the lemma three already told you that in that case mu has to be larger than. So sorry, you cannot have new larger than mu. You have to have this. But because we assumed this already, so you put this together, then you realize that if you were able to map down, then you have to have identity. So by doing this, we can remove all these identities. So everywhere, if you have identity map down, then you remove all of these. So eventually, by doing Remove all of these. So eventually, by doing all of these three lemmas, we were able to remove all the parts where you actually have a dilemma. So now we only need to look at the two lakhs. So everything will be two lags. And when we work with the two lakhs, we think about, we suppose that in your support, there are actually three points, like these. The three points actually give you that x and x prime actually map to the same y. And this will kill the moon property. This is something we don't like, right? We want to rule out such probabilities. Now, how do I rule out such probabilities? Now how do we rule out such probabilities? Suppose that you have such a saturation. When you have such a saturation, you have x map to y and x prime map to y. This y can be to the left, to the right, or in the middle of the two points. Suppose that these are three situations. In each of the situations, I try to look at a region, here, this is the region, where my other points cannot leave. Suppose that there is an x star leap here. Then if the x star go, if you map the x star to somewhere here, then you step in the legs of x, which is forbidden. In the legs of x, which is forbidden. And if your x map to here, then this guy would step in. Sorry, if you map here and then this one map to, must map to the right side, then either this one will step in this, or it will step in X. So if you look at this picture, then here in this region, I cannot have X star. The similarity for the other two pictures. Here you have to really do a little bit of calculation, but in each of the pictures, you can find a region where this X star cannot be. Where this X star cannot be. So that means as soon as this kind of non-moon property happens, then you would have a region in which you have no math. So in all of the cases, you will have a region, an interval, that mu cannot assign matter. And that means what? That means this first x must be the right-hand point of one decomposition in the support of mu. So when you decompose the support of mu, you can decompose the complement into complementary disjoint open intervals, then each of these first x. Then each of this first x, this one, must lie in the right-hand point of that interval, but such intervals are only country many, right? And that means there are only countries many of y's that can couple with this kind of x. But because mu is atom list, such y has to have mu magnitude zero. That means I can support my entire Lapcurgeon transform on a reverse graph place. Okay, so this shows the basically the, of course I'm going quite fast, but the idea is this. Of course, there are several statements that need to be further written. Statements that need to be further verified, but the whole idea is: eventually, we use this argument to make sure that this monk property is not valid. Okay, so let me just go to the Straton theorem, why this is important. This is the statement we had in the beginning. One of these directions is trivial. You have just an equality. And you can easily show that in, say, in a space of 2x, 2 elements, you cannot have the other directory. Or you can think about an atomic space. Direction. But you can think about it in a terminal space. So if I'm now in a terminal space, if I want this reverse Janssen direction hold, then I can think about y, I take y as y that generates the whole sigma field. Of course, this is possible sometimes, sometimes not, but if you are in a standard boreal space, then this is possible. So you can have a y that generates the entire space in terms of randomness. And then this means that the expected value of y given g must be a function of y. So you need x to be a function of y. Okay? So that means if I wanted. Okay, so that means if I want this direction, I need the multi-marticle transport to exist. And that's why we started multi-market transport. Actually, this problem was my original problem. That was the one that I tried to solve. And then during the way, I realized that if the multi-marticle transport doesn't exist, then I cannot do this. But actually, as soon as you prove that multi-marticle transport actually exists for atomist Y, you can easily generate it to other cases, and then we get this quiet result quite reasonable and easy. So, this actually from the first theorem to the So, this actually from the first theorem to the second one is not so hard, although I had to have to look for it. So, this is the result I promised in the beginning: that if I have colours order, the equivalent condition is that I can write x as a condition spectrum of y. And why I call it a refinement? Because I don't know how many of you use the book of Shakit Sakshikuma. I use that a lot. In the Shakir Sandipuma book, the version is to build this X and Y side device for this condition on a different space. And that's commonly done. If you read all of these papers who do this construction, Who do this construction? You take the original random variables, you find an alternative space, a different space that is nice enough, and then you build this variable construction. Whereas here, we're doing it really on the original space, which is much more squeezed. So we're really working in a small apartment in Hong Kong, trying to build all these things we need, which is much harder than building it in an arbitrarily constructed space. That's why we call it a confinement. Yeah, so I'm very glad that I'm up to this page because I was thinking that this is the main message to send you. Because I was thinking that this is the main message to send you. But since I still have maybe two minutes, I want to show you another result that I think is interesting. That is this one. Actually, we went one step further because just showing the existence is not very helpful. We also want to know optimality. We want to know whether these mono material transports are good or not. And then we can prove actually they are dense in a very natural sense. So they are actually weakly dense in the set of all multiple transports. And if you have this rate distribution, you also have. And if you have this radius solution, you also have an infinite bus system density. So that automatically gave us the equality in the MONG setting and also in the counterweight setting. So I have the optimum cost in the MONG setting equivalent to the optimum cost in the counterweight setting. So that means I actually have, this is a classic result if you don't have multiple constraints, right? This way no holds under general conditions. Here for the mining case, I need a slowly growing condition. Slowly growing condition that A and B are two L1 functions. This condition is because we only have weak density here. We cannot get it much better. But we actually think that maybe there's a way to get it much better. But right now we don't have it. But this result you can easily find in any option control book when you don't have martin road constraint. You just look at general marketing general transport objects. So that's good, because now I know that I can use this discrete map, a deterministic map, to approximate, at least not necessarily optimal, but approximate. At least, not necessarily optimal, but approximate any transport plan that you have, any control bits that you have. And the proof of this is actually much more involved than the first one. Obviously, I don't have time to finish it. I will just present one little sort of idea that you basically start with some general mapping, and then you use the composition lemma to make it like this, and they become single. So, basically, you're trying to locally make everything singular. That's why you gotta remove. So, I will just skip everything else, and I just want to maybe make one question: that is why I didn't go for the RD. And if you go through the proofs with me, then you will realize that I use a lot of this property model. So, really, the current coupling on RD is not so clear, and all these properties we couldn't prove. So, if we were able to prove this on R D, then we would be extremely happy. And we are sure that the paper would much, much better. So, if any of you have ideas, Much, much better. So, if any of you have ideas to prove this same result in Rd, we would be just super zero. Because this has bothered us for a long time. And we, because the Stresson theorem doesn't require R, Stresson's theorem has nothing to do with the structure of R. But our proof has to do with the structure of R, which makes us very unhappy. And actually, in the first round, we sent this to the analysis of probability, and the report was to say that you can prove this on RDL, immediately publish it, but it didn't go through. So if it's one of you, If it's one of you guys, I'm sorry. But now it's okay. Paper is in a good place. Anyway, so I will stop here. Thank you very much. This is a good joke with this microphone. Questions? Good reasons to think that it's true or not? Well, it's just the condition you need, right? So here we have a conjecture that we have some conditions. We have some conditions about the irreducible components having some atomic structure. We're sure that if you put enough conditions, this is true, but we don't know where to put. And also, we don't know how strong it has to be, but eventually you can put super strong conditions that makes it almost prevalent. That's not interesting. Can I suggest something just to answer also? Mark, maybe that's an alternative idea. I was just trying to write down a couple of things. I'll tell you more later. Things like I'll tell you more later, but my insense is that all you need to do is the knowledge of MEC, and this is where the upper graph could be an alternative approach. Because then I don't want to go into much detail, but then there is a way to find that signal algebra constructively as an agreement signal algebra, because there's a classical result we can talk about later. If you have two monotonic measures, we were just discussing this earlier, you can always find a sub-sigmatology which they agree completely. Like the restriction would be the same measure. Restriction would be the same measure. And that's a Yapuno. And Yapuno does not really require R in itself. So I don't really have a final idea here, but this is one way you could explore and we can talk about it later. Yeah, we can talk about that later. One feeling I have is that this martial constraint really is very difficult to work with. You have to keep the marking goal at every step. If I get rid of that constraint, I can do a lot of amazing things. But that constraint, every point has to map to something with the center and the cent point is really bothering us. Really bothering us when we think about alternatives. Thank you. So you motivated this project by using the option pricing example. So are you planning to have you use this result to find the worst things on the upper? Place on the upper bound and lower bound for you know the value of the option pricing, and what's the application of this result in the context of that example? Okay, thank you. So, first of all, the option pricing example was to motivate market transfer, not to motivate this work. This work has no motivation from that side. This work actually I need this result from all my statistics paper for stupid reason, but but since I think Strassen's theorem is so well known in this community, I think any refinement to it Community. I think any refinement to it should be interesting to us, so I don't really have a real practical application in mind. The output pricing example is a very nice one for the module transfer. And because we now prove that these two transfers have the same cost, so you actually have outer boundary there should be out of the bound here in this restriction. But I don't particularly feel that it's useful in that way. Yeah, I thought you were looking at, say, the I miss anything. So the upper bound and the lower bound of the observed pressure were subject to the marking the autumn straight. Oh yeah, that was the original motivation for the martini volt transport, one of the motivations. That was a very convincing one. That's why they couldn't come back over. And we think about extending into a multi period one, because in a multi period one they also have a similar left long term measure. And left normal terms measure? Yeah, multi-period, yes, we did something like that. It's actually we find this backward deterministic fake Martingale thing in the discrete time, and then one of the conjecture we want to make is that we want to conjecture there exists a stochastic process that is continuous time and has this backward month property. We couldn't prove it, we couldn't disprove it, that's what actually automatically has to send the paper. But for discrete time we can construct it. That's what there's no problem. Already on your paper? This is Or any on the paper, this is okay.