So I got interested in this topic during my postdoc where I overlapped with Antonio Lerario at Purdue. And my training's in analysis, but I like polynomials. And we both got really interested in this idea of looking at extremal problems and providing a complementary point of view by looking at what's the average case or typical. Looking at what's the average case or typical case. And when you have sometimes concentration of measure as degree or dimension or something goes to infinity, you can say the typical case with high probability something will happen, which is really interesting also aspect of that. And so I'm still think this is an important I think this is an important part of the field to provide a complementary point of view to extremal problems. But another interesting possibility is to actually provide new insights on the deterministic side. So probabilistic proofs of non-probabilistic theorems. And we're probably all familiar with this being like a really dominant point of view in combinatorics, where half of them are using that. Where half of them are using that for their work. So I specifically am interested in the possibility of using that same style of probabilistic method from pound rhetorics in the setting of polynomials. So I want to review some of the most basic principles, and they're really like trivial things, except maybe for this, but like the fact that if an event has positive probability, then there exists an instant. Then there exists an instance, that's the basic idea for a non-constructive proof that something exists. And that often gets used with this fact that you're going to have at least one member with at least the average behavior. And then linearity of expectation, another trivial thing, but it's good to, in these probabilistic proofs, to see where. Proofs to see where it is really powerful and helps by dealing with different events that are interdependent. But if this is what you're after, it doesn't matter. You can deal with them separately. Not worry about the interference they have with one another. And then concentration inequalities, this is just one example where the capital S n is a sum of random variables. which are independent but not necessarily identically distributed and bounded. There's other versions for certain cases of unbounded random variables, but just to give a basic example. Basic example. That's one. And then this union bound. So if you have independent events, then you just take the product of probabilities if you want to have all of those events happening at once, to have a lower bound than that. But you can do a very rough estimate, which is first to look at 1 minus the probability of the complement, and then use De Morgan's law. Use De Morgan's law. The complement of the intersection is the union of the complements. And then just do the upper bound that the probability of a union is smaller than the sum of the probabilities. And because there's a minus, you get the inequality going the end of that way. Okay. So think of these as good events. You want all of them to happen. These are all bad events. To happen, these are all bad events. So if they're coming from some estimate like this, where you have some really strong, say, we've heard the term with overwhelming probability, then you can easily get that to happen for a lot of events at once, right? All right, so let me I'm going to look at two specific results. The first one illustrating these two principles, and then the next one includes the other two. Which is current work, which the paper might not appear anytime soon. So working on improving the results. Okay, for the first theorem, it's in the setting of complex harmonic polynomials. So P and Q are univariate complex polynomials, but then we look at the sum of P of Z plus Q of Z conjugate. And then we want to know about the size of the zero set. It's the general problem. And the result And the result is that there exists such a pair so that the zero set has left square square at least n times square root of m uh many zeros. All right. Um All right. And I'll give a little bit of background on this, the problem studying that. And let me state the other theorem. So this is in a different, so both of these problems involve one complex variable, but in this one, because of the conjugation, it really becomes the real algebraic geometry problem. You look at the real and imaginary parts of the equation. It's two real algebraic relation of n of n. Oh, sorry, yes, yes. Yeah, so n is degree degree of p m degree of c. Thank you. Okay. I mean, it's not to do, but yeah. Okay. So this is really runs into the realm of. Really runs into the realm of real algebraic geometry. And the second one is about polynomial lemma skates. Take a monic polynomial with all of its zeros inside the unit disk. And it's a classic problem to find the minimal area. Alright, so the main goal is to actually look into the proofs of these two results and show how some of these methods are used. Let me first say a little bit about this problem. So the problem of studying the number of zeros. The problem of studying the number of zeros of polynomials of this form was posed by Shield Small and then his student Wilmskhurst posed in the 80s and then his student made some progress in the 90s and conjectured that let's call n n n, n the maximum the maximum number of zeros. Over polynomials of degrees n and m. Okay. And he verified it for the case m equals n minus 1. So we're assuming n is larger than m. It's just the Bezou bound in that case. So to show the maximum something, you need to produce an upper bound and then also produce an example. So Bezou's bound gets the upper bound and then the example realizing at E constructed in that case. And then for m equals 1, it was proved by Kavanson and Switech. Well, the upper bound. Well, the upper bound, using a cool indirect proof based on holomorphic dynamics, which I won't have time to go into. And then Lucas Geyer produced an example showing sharpness of the upper bound and showing that this the max for n equals 1, 3n minus 2. And Lucas Geyer also used some holomorphic dynamics to produce those examples. Yes. Yes. So this capital N, this defining it to be the maximum of this number of species. Or what's the conjecture? Yes. So it's defined this way. And the conjecture is that it's equal to this. Excellent number of these years is equal to capital or alpha. Yeah, by definition, and then his conjecture is that this is a formula for the max. This would be an improvement on Bezou's bound, which would just be n squared. If you look at the real and imaginary parts, you get real polynomials, but both equations will get a part of the z to the n term in p, so they'll both give degree n. So the basic bound is n times n squared. And squared. So his conjecture is that there's a better bound refining it in terms of n and m that increases linearly for each fixed m, but quadratically in m. I'd like to just ask like a basic question. So can you explain why you're adding the b and q? Yeah, okay. So, well, the fundamental theorem of algebra would tell us that there's n zeros this case or n of that case, and when you add Of that case. And when you add them together, this is still interesting for complex analysts because it's a harmonic function, complex harmonic. So the mapping still has bigger than a polynomial of the same degree. Because it occurs, yeah, so that's the bigger the n1. Yeah, but actually it gets way more complicated. So now the fundamental theorem of algebra is applied, and you can have, you know, fixing N and M, you can have a variety of different outcomes. A variety of different outcomes, right? Just as usual with studying a parameter space of real algebraic equations. So if you're asking for like why did they care or that kind of motivation work? Yeah. Well like it seems like I didn't care like this is like a decomposition and it's not like a real part and a complex part because they both are complex. I'm used to like taking the variety I'm used to like taking the variety of two polynomials. Here you're just like adding yes, okay. Well, you could, this is like this, maybe one of the simplest things you could do with complex conjugation. You could also look at a product with complex conjugation, or you could make it a polynomial in Z bar with coefficients in Z, and they call those polyanalytic. And you can ask for similar questions. Similar questions, but it becomes more difficult to do anything better than basic bound. So, like, what happens if you switch, you make P the one that has the conjugate, but now that has the larger degree? Like, does that change the reason that you can just pull it? Oh, can conjugate the whole thing with a lower degree? Oh, because it's not a different thing. It's just to fix. Okay. So, So, for complex analysts, there was just a lot of interest in harmonic mappings. Maybe around the time Bieberbach conjecture was proved. So they started looking at harmonic mappings a lot more. But there's classical motivation for studying harmonic mappings. It can be used to parametrize minimal surfaces. They have energy minimizing properties. They have energy minimizing properties in some sense. They're Poisson anti-grills of their own boundary values on certain regions. And this does run into a problem from gravitational lensing, because after Patton and Zwitek proved the result for m equals 1, they generalized it by considering some rational harmonic functions, and then that ended up resolving a conjecture in astronomy, which is really interesting. Which is really interesting. And then the sharpness examples for that other problem were then provided by known examples in astronomy. So there's interesting connection there. Okay, so after it had been verified at the top and bottom cases, I mean, I'm going to go ahead So m equals 1 and m is right below the diagonal. It was shown to be false for m equals n minus 3 and use a number more or less creative. Oh, so there were examples constructed that exceed the conjectures of that test. Your film A disproves this conjecture about? Yeah, so this one also produces counterexamples. So this is the point is that without any construction. Right, so yeah, so this produces counterexamples for a full range. Right. Right, and more importantly, it shows that maybe there's a bound which is linear in n for each fixed m, but it couldn't possibly have a uniform rate independent of m. Because here you're seeing a rate that increases with m. So that's the. So I don't understand why things are not symmetric in m. So if you can z by z bar, does that mean you can? Well, yeah, if you wanted to make it symmetric, you would call this the minimum of energy instead of that one is smaller than. Okay, yes. That's why you would call it. All right, yeah, there's a lot more to say on the background, but I wanted to focus on the everything connected to this, which is the proof of this result. So I'm just saying a little bit about this conjecture and some work on that. And some work on that. And to point out that this, you know, there were already counterexamples constructed, and John Houndstein and Doug Ashmeta and Antonio Lerario and I found more counterexamples for finitely many values. But this really shows that you need a qualitatively different kind of form for the true maximum. The true maximum. Okay. Any other questions on this before I start looking at the proof? Okay. All right. So to do the probabilistic proof, we sample randomly and then with the right model show that you have an expectation basically like this. And then you have at least that many. That many using the principal one. Okay, so the way we sample Q is from the complex cross lin. So the coefficients are complex normals, mean zero, variance m choose k. Choose A. So it probably looks familiar except for the complex normal, possibly if you're unfamiliar with that. It's just a sum of a real random Gaussian plus I times an independent real random Gaussian, each having half that variance. Okay, and then If you look at the real part of Q and also restrict to the real axis, then this is giving you a real Costlin, well almost, times a constant, which I'll just still call it real Costlin. So the one-half. Right, because you're Right, because your variable is real, and you're taking the real part, so you take the real part of the coefficient, and it's just real. But actually, you can restrict to any line through the origin of the same statement as true, because the complex fossiline is invariant under rotations, I mean, under rotations of the Riemann sphere, there's zero sets of agreement, but in particular, under rotations around the origin. So the same statement holds if I do e to the i theta x, some fixed. X. Some fixed theta. Okay. So that's how we sample Q. So there have been studies on this problem from the random viewpoint, just getting a broad perspective, randomizing P and Q independently. But for this purpose, we actually choose P to be a deterministic perturbation of the random Q. So it's highly dependent, not at all independent of. Not at all independent of of q. Just take p to the epsilon z to the n plus q. So then when you look at p of z plus q of z bar. And then take its real and imaginary part and look at the system of equations. So you get, when you add q of z plus q of z bar, you get q plus its conjugate, so twice the real part. And if we look at the zero set real imaginary parts, then you have This is still just the same thing, since it's already real. And it goes away when you take the imaginary part. And the imaginary part of Zvn is just n lines through the origin. So the obvious way to solve the system is to look at each of those lines, well, look at the second, sorry, the first equation on each of those lines. Sorry, the first equation on each of those lines, right? Solve those univariate, it becomes a univariate n-univariate problems, right? So look at those n-univariate problems. Here, those are just lines to the origin. So this has the same distribution on each of those lines. It just becomes the same problem n times. This is going to be lines. Um, so the expected number of zeros are going to be a little bit more than a little bit more. Uh, zeros of this. And actually, when you plug in the line where the imaginary part is zero, you plug that in here, you just get to look in the real. Sitting up to look look at the real line now? Yeah, yeah, it's all the same. Okay, yeah, so the same of zeros of this, restricted to a line. Right, and those are all the same, and now it just becomes bilinear expectation, n times, so we're using property two, n times this one expectation. And then And then this is a perturbation of a costlin. So if you believe that when epsilon gets small enough, we can ignore that, then this is bounded by the number of real zeros of a costlin, which is squared of m. So really minus delta for epsilon small enough, and you have to do some quantitative stability argument to justify that. That's the most technical. Justify that. That's the most technical and least important of the conceptually part of it. Oh, yes, thanks. Yeah. Alright, so that sum is the sum of all the variables, right? Yes. And Zen is one variable, and you have it was not. Exactly. Right. Okay. So again, to belabor the point, thanks to linearity of expectation, we didn't have to worry about the fact that what the random polynomial does. What the random polynomial does on one of these lines does influence what it does on the other ones because we don't care about dependence, right? We can still deal with them separately. All right, and that's that proof, okay? So any questions? Yes? Look, it seems the only property of the concept you used is invariance, no? Yes, and so an open problem would be to use a different model. Use a different model and try to get more, but the real part is like pure harmonic, that has narrow harmonic errors. Then you could increase the scope of MRI. Oh, yes. So, right. The proof might get more complicated because it's going to change on these different lines. But you might get, even if these numbers are different, if you can estimate something, you might get more zeros. Because I do think that there should be. Because I do think that there should be really a cross term, n times m showing up in the true maximum. So if you could do that, though. For the pure monic, you have Yeah, but you won't get the invariance. It's a good idea though. It might still work even though you don't have the invariance. But you have the invariant tool, right? So that it's holding song, right? I think the pure harmonic. I think the pure harmonic one, the still invariant, it has the most zero connection. So we're using complex costlin. Yeah, I agree. So one has to find some random polynomial whose real part is this other distribution. Right. That's the thing. Yes, and you can, but then the one you get is not invariant. No, yeah, it's not. It would be built out of check. It would be built out of Chevy Chev, and then what that does in the rest of the plane is not what it does on the real. Using bias to say that these are all the same in the lays, right? Yes. Okay, so let me now talk about the other theorem and the background problem, which is, you know, study this minimal area, right? Monic polynomial. Right, monic polynomials with zeros in the unit disk. So that problem, this is called a filled lemma state. It's a semi-algebraic set because if you square this, it's conjugation invariant, so it's a real algebraic. But you can view it as a polynomial x and y, and you're just looking at a sublevel set. So you want to find the minimal area of this sublevel set. Area of this sublevel set. That was posed by Erdős in the 40s and the let's see. I'll write down the upper and lower bounds that were proved in the 60s and 80s. So, when Ervish first stated the problem, he said incorrectly that it's The problem, he said incorrectly that it's bounded below by a constant independent of n. Then Maclean showed that it actually goes to zero. Then Pomeranke in the 60s showed it's bounded below by a constant over n to the fourth. And Wagner in the 80s proved upper bound by constructing an example. So for So, to prove an upper bound on a minimum, that's where you need to construct an example. And that's where Wagner made Maclean's prove quantitative. And then, I think in the 70s, Erdős updated or reprised the problem and responding to Pomeranika's lower bound, asked whether the true rate is power law. Is power law? If so, what is the correct power? I think maybe trying to convey that it's going to be impossible to find the constant were very difficult. So our result shows that it's not power law. It's in the realm of very slow decay. So bound below by 1 over log n. And so there's still this question of whether And so there's still this question of whether it's very slow or super, super slow log luck. And so the reason you probably won't see this paper for a while is we're trying to see if we can either improve this round or this one to really see whether it's log n or log log n side of things. All right. But I want to present the Part of the proof, the step where we use a probabilistic argument. Any questions? So, the step that I The step that I won't show you any details that I'll call step one is proving the lower bound but for the wrong class. So where the zeros are on the unit circle. And then the step two is where we reduce the right class to the wrong class by a random approximation. So, moving zeros to the boundary of the disk. But for each original zero, we have to create m new zeros where m is increasing with n. I guess I'll write a double product. Alright, m is increasing with n. We can probably make it smaller than this, but for instance, m equals n squared works. Okay. And I want to show, so the proof of this uses, so starting with this given polynomial, you build a random polynomial which is A random polynomial which is adapted to that specific example. So that's why I call it random companion. This is a companion to that one. What are you random? So you don't see randomness in the statement of the lemma, only in the proof. So I don't know if that's a good name for the lemma, but I want to emphasize the proof, I guess. And before I. Okay, wait, I have to find your so-huh. Okay. Okay. All right. So I want to sh at the risk of switching gears too much, I want to show you a similar random companion lemma where the proof is simpler, cleaner. I can show you all the details and then I'll just explain how it becomes more challenging in this situation. And it's in the setting of trigonometric polynomials, but you could still say complex polynomials just restricted to the circle instead of the condition. Instead of the condition that zeros have absolute value one or less than one, we're looking at the coefficients. So these are complex numbers again. All right, so we look at the trigonometric polynomials with those coefficients. And the statement is that the difference is Is n absolute values bounded uniformly for theta and the interval 0 to 2 pi by big O of square root of n log n. So the worst case, I mean just the triangle inequality bound would be like n, so this, oh sorry, this should be big n. This is an improvement. Okay, and then he used this lemma to solve a problem of Littlewood by first solving the problem for the wrong class and then using the lemma to switch to the right class while controlling the error so that he still solves the original problem. I'm not going to say what the problem was, but it's kind of similar to what's happening here, except he's actually just trying to build an example. Actually, just trying to build an example. He builds an example of the wrong type, then moves it to the right type. Here we're trying to prove a bound on all the examples, but you can still use this random companion idea. But the lemma is very parallel to what's happening there, as is the proof. Okay, so the idea of the proof is to So the idea of the proof is to take your given coefficients and then view them as a convex combination of two points on the circle. Which would be at points having the same angle, right, are going to be at antipodal points. And these are the coefficients you pick. Coefficients you pick. And now the idea is we sample CK prime randomly using these as probabilities. So making CK prime to be one of those two points. Okay, um and so by that construction that expectation of CK prime is CK and the expectation of this sum is that sum. So then we can view them as an instance of this. Give bounded, independent random variables. I should really say a function of theta. So for each fixed theta, you can do the You can do this as a sum of random variables whose expectation is that one. And then the bound that you want, you can get to hold pointwise with high probability using this estimate. Let's call this star. You can have any polynomial you want here. Any negative power of n here. But to show it's big O, you bound the probability that it's not big O by inserting. That's not big O by inserting per lambda constant times this square root n log n. You can make the constant as big as you want, which then makes this power just like small. Okay, so then you can get it to hold at a point or using the union bound at simultaneously at many points. So this one is by three and then It holds on an epsilon net of points on the circles space epsilon apart. Again, with a different exponent, but it's still going to be same type of probability. For the epsilon net, we pick n cubed points. And then you can pass from the epsilon net to the whole circle if you have a bound on the derivative, you know, uniform bound on the derivative of the function. Because moving off of the net, you can control the error if the thing you're estimating has a bounded derivative, right? It's going to be like the derivative times the distance you have to move, which is the spacing of the points. And that part is really easy. Really easy when you have this much power to work with. For applying 3 and 4 in the setting of polynomials, I think this epsilon net is extremely useful. I have a question about W2. This is now a different proof that we see obviously before. No, no. This was a proof of 2. Yeah, I was surprised to find this proof. I was surprised to find this proof back in the 70s. Yes, yeah. Yes, I don't even know his other work that was on ICP. Yeah, I know his Fourier analysis book that I learned from in Grand School. Cattle, just in Cattle. Corner, yeah. Tom Corner. Okay, so then that's it. Union bound union bound. Union bound epsilon net argument. Some of you have probably used before the epsilon net idea. Okay, so then that's in a setting with coefficients instead of zeros. But the idea here is similar, just gets more technical. Okay, so maybe I'll just briefly say some of the ingredients worth of this case. So here we had a sum, and here we have a product, but you can turn the product into a sum by taking the log. And then just looking at one of the terms. Uh just looking at one of the terms. We want to move this zero to the to the boundary, but you can't write this as a x combination of logarithmic terms with zeros on the boundary. But you can first, we don't need that kind of First, we don't need that kind of approximation, we just need an inequality. So we could first estimate this by something else. So they match on the boundary of the disk and this estimate holds throughout. And then you can write this as a convex combination of logarithmic terms with zeros on the boundary. With zeros on the boundary now doing step still, step two, or we've got two. Okay, so this proof is done for the corner. And now I'm looking at just some ingredients on how you use the same basic ideas here, but how what becomes more challenging? Similar idea for merging at one coefficient or looking at one term. Of looking at one term, you can view it as related to this convex combination. First, you estimate it by this, and then you can write that as this integral against the density. So it's a convex combination of logarithmic terms with points on the boundary. And then you can use this as a probability and then sample So you sample this point on the boundary with this density, and this density is related, it's adapted to the zero that you were talking about, so I should call it that's okay. So I should call it that's okay. So for each of the original zeros, we have to do a sample because we need a better kind of error estimate. And you can get a better estimate by increasing your sample, which normally would ruin the estimate. But because of this log, in the end you end up comparing, this allows you to compare the minimum for the wrong problem of a higher degree to the minimum of the right problem of the right degree. But the wrong degree is, you know, just. But the wrong degree is just power of n that comes out for the log just changes the constant. So I don't see the picture. How can why it was not possible before? The log of z minus z can be too like after it's possible. So if you take a superposition of logarithmic terms like this, then you'll get something harmonic in the disk. And this has a hole in the disk, so you can see that there's no way to get this as a combination. Way to get this as a combination of things like that. But this is really the solution to the Dirichlet problem with this as boundary condition, so it's harmonic inside. And that you can write as a so-called single-layer potential superposition of these terms, that is. These are technicalities that are really specific to this problem. That's one reason I thought I would show this. Show this to convince you that there's versatility in the main ideas in this method. Okay, but I think I'm already the function you write. The harmonic function you write is an integral boundary or something like that. Yes, so you write this harmonic function as an integral against a density, which is really think of it as a convex common. Right, which is really think of it as a convex combination of chunks of type. You have a zero on the boundary, then you have a fundamental solution. It's not the same as the Poisson integral. Yeah, it's a little bit different. Right, okay. So then you sample your, so again, by it, so parallel to this step, by this construction. Parallel to this step by this choice of this randomness, the expectation of the whole sum. The expectation of these logarithmic terms like the random samples give you not the original, but this thing that Not the original, but this thing that is bounded, that satisfies this inequality. And then you're on your way to continuing the template, but you actually have to move inside the disk a little bit to get some room, because they match on the boundary. You need some actual room when you add the fan and this. So there's some other challenges that I don't have time to go into. But thanks for listening. To go into. But thanks for listening.