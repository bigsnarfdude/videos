Yes, how can I do this? Okay, let me try okay. One second request a sword idiot. You have to look for something called share screen driving. Yes, that's what I'm trying. No. You should be square buttons at the bottom of the screen. Yeah, this is just okay. Now you select your slides. Yeah. Open your slides so you see the surface. Can you hear now? Very good. Okay, okay, thank you. All right, so the definition of a joint spectrum is right here, colored in yellow. And this is just simply means that we consider a point x1, xn in the projective space such that the pencil x1a1 plus xnan is not invertible. Is not invertible. So if the dimension of a space is finite, then the projective joint spectrum coincides with the determinantal manifold of the matrix tuple A1AN. And this object was under the investigation mainly in the framework of algebraic geometry for more than 100 years. And in this case, of course, the projective determinant of manifold is given by the Manifold is given by the vanishing of the determinant of the pencil. Usually, to avoid trivial dependencies, at least one of these elements of the tuple is supposed to be invertible. And we usually consider the last element to be identity or minus identity. For me, it is a little bit more convenient to have it as minus identity. Entity. So, what we'll be talking about, it is reasonable to look at the part of the spectrum which lies in the chart x and plus one is not equal to zero. So the coefficient for the identity is non-trivial, and we call it a proper joint spectrum. Now, let me describe the spectrum in the divisor form. So, if I consider any point x1, any point x1 xn plus 1 in the spectrum then x1 xn plus 1 belongs to the spectrum of the operator x1 a1 plus xn an and that was the reason for choosing minus identity versus identity and we do denote by multiplicity of mult x the multiplicity of this eigenvalue right i'm talking about eigenvalue because we're dealing with a finite dimensional space and infinite dimensional space multiplicity might be Space multiplicity might be infinite. So obviously, multiplicity is the same for all regular points belonging to the same component of sigma a1 an. Well, of course, the spectrum is an algebraic manifold. In this case, we can say we consider a finite dimensional case. So it's an algebraic manifold. And since one of the operators is invertible, then this manifold is proper and it is of co-dimensional. Uh, it is of co-dimension one. So, the multiplicity, of course, is the multiplicity of the component in the zero divisor of the polynomial, which is given by the determinant, characteristic polynomial. And now we defined the divisor, sum of multiplicities, gamma j, gamma j, where gamma j are the component of the spectrum. And we prescribe to each. And we prescribe to each point of the joint spectrum the sum of multiplicities of all components passing through this point. So the creature which obtained is we call projective joint spectrum in the divisor form and denote by sigma d A1AN. So now, sometime not very long ago, but in the last decade, certainly, there was Certainly, there was established a connection between spectral properties of tuples and representations of group and algebras. And here I would like to mention a few papers. There are many reasons. So I simply cannot mention all of them. So I put four, which in a sense are the most important to what I would be talking about. First of all, it is Grigor Chuke-Yang's paper of 2000. Garchuke-Yang paper of 2017. And in this paper, Slava and Ron actually, among other things, have presented a spectral analysis of left regular representation of infinite dahedral group. And in a sense, it was some kind of a catalyst for many other papers. The second paper, it was a paper by Igor Kleb and Yuri Bolcic. And this paper. And this paper was actually originated like four years ago in the same room where you are now. There was a conference on multivariable operator theory and representation theory. And the question discussed was whether every joint spectrum of the images of every generating set of a group determines a representation of a group. And Iger and Yuri constructed the first counterexample. Constructed the first counterexample that it is not true. And that was important because basically, if you want to investigate spectral properties of representation, then we not necessarily have to look at the minimal representing set. We need something a little bit bigger. The other paper, which I would like to mention, is our paper with Zhelte Chushkiewicz and Alex Chernev, where it was shown that the general spectra of images of Spectra of images of the coxsetter generators for finite non-special coxsetter groups determine the representation. And the last paper, which was actually I would like to mention, it is a paper which hasn't been published yet. It is a paper of getting there is an archive. There is an archive of this paper, and this paper actually gave a spectral analysis of representations of simple Lie algebras, in particular that Joint spectrum determines the representation of simple Allie algebra. So in several cases, studying of spectral characterization of representations used certain type of inverse theorem, which Type of inverse theorem, which I call spectral rigidity theorems. And these are results of type of these two types. If a projective spectra of two tuples are the same, counting multiplicities, then, well, maybe under some mild usual conditions, then these tuples are equivalent. Or of a type, if the joint spectrum of a tuple contains the joint spectrum of another tuple as a subset, then the first tuple is decomposable and it's restriction to the corresponding. And its restriction to the corresponding invariant subspaces is equivalent to the second tuple. So, in this talk, I will try to present several results. In this direction, they will relate to three areas. The first one about Coxeter groups, the second about characterization of Adam R matrices, and the third is related to quantum group, twisted SU2 group, and in particular, to me, more. And in particular, to be more precise, infinitesimal generators of the representations, and in particular, some results for SL2. So I will start with Coxeter group. So the definition, let me remind you, definition of a Coxeter group. So if I have a finitely generated group G, then this is a Coxeter group. If its generator satisfy the relations, then Gi G J raised to the power mij is equal to one. Is equal to one where m j j is equal to one and m i j a natural number including possibly infinity. And what I will be talking about, I consider m i j strictly less than infinity for groups which we consider. And it is easy to see that if m i j is equal to 2, then the element gi and g j commute. So the matrix m i j is called a cox. Mij is called a Coxeter matrix. And once again, I repeat that the Coxeter matrix, which I will be considering, are finite. So if I have a unitary representation of a Coxeter group, then of course the images of Coxeter generators are unitary and self-adjoint involutions. So the first step is to try to figure out what kind of adjoint spectra we have for involutions. For involutions. So, the following lemma, I think it is contained in our paper with Chushkovich and Chernyev. And it says more or less that if you have two bounded operators on a Hilbert space, here I don't need a finite dimension, which are involutions, then first of all, the set of proper spectrum of this A1 and A2 and one and a two and minus probe spectrum is the union of complex ellipses so it is x squared plus alpha x y plus y squared is equal to one where x and y are complex numbers and alpha belongs to the spectrum of a1 a2 plus a2 a1. Second that if this spectrum spectrum of a1 plus a2 is plus a2 a1 is a finite set then each connected Its set, then each connected component of the spectrum is either the line or ellipse for some alpha belonging to the spectrum. And the last, when the v is finite dimensional, then each reduced component is either a line x plus minus y is equal to plus minus one or complex ellipse. So this just simply gives me the idea what kind of idea what kind of curves I might have in spectrum when I have two elements. And the last element, of course, this is related to Chebyshev's polynomials, that if A1, A2 raised to power M is identity, then the spectrum consists of these numbers, which are, of course, Chebyshev numbers. So since for Cox's images under the representation Images under the representation of Hoxetter groups are involutions, then obviously we see that for every representation, the pairwise joint spectra of images of generators consists of either lines or complex ellipses. And the following theorem was proved that if we have two self-adjoint We have two self-adjoint matrices, and we assume that a proper joint spectrum of these matrices contains ellipse E alpha with multiplicity k and no other component of the spectrum passes through the points plus minus 1 0 and 0 plus minus 1 and a sigma p of a1 a2 and a3 where a3 is given by a1 plus a2 times a1. Plus A2 times A1 minus A2 is in this form as it is written in section C. Then A1 and 2 both of them, this pair has a common to K dimensional invariant subspace, which is a direct sum of K two-dimensional subspaces and restricted to each of the subspaces. A1, A2 is unitary equivalent to these matrices. I want to make I want to mention that there is an infinite-dimensional analog of this theorem. It is a little bit more technical, so I don't talk about it, just to mention that it is possible. In this theorem, the condition that both A1 and A2 are self-adjoint is a little bit too strong, and it is possible to get rid of it, at least partially, but this requires quite a bit. This requires quite a bit of technicalities. These technicalities are related more or less to the following fact: that this theory, this previous theorem, was obtained just by considering the spectral local spectral analysis of components passing through different points. And in reality, what could happen is that a point spectral point where we consider our spectrum might be Our spectrum might be actually a point belonging to the singular locus. The simplest point, point of a singular locus, is the point of reducibility. But we are talking when we are considering about spectral analysis, we are talking about certain types of projections associated with these curves. And if I have a number of curves passing. Uh, curves passing through the same point, then of course to have an analysis of the whole bunch is very inconvenient. And what we usually try to try to do, we're trying to find to see and see what happens when we're looking at the projections associated with each particular curve. It turned out that if these curves are tangent, or if God forbid, there is a cusp, it's not just a response irreducibility, then the behavior becomes. Then the behavior becomes rather erratic. So, in order to get a reasonable result, we introduced certain conditions which are called regularity conditions. And we can see the regularity conditions separately at the point zero of a spectral point of one of the operators when this spectral point is zero and non-zero. So, to this end, we will write the spectrum of the tuple A1. Of the tuple A1AN as a polynomial, zeros of a polynomial, R1 of X1, Xn plus 1 to M1, and so on, where R1, Rs are irreducible polynomials. So if a point, spectral point of operator A1 is not zero, then obviously what we have, then the proper joint spectrum is given by the formula. spectrum is given by the formula of the product R1 of X1 XN1 raised to the power m1 and so on. Rs of X1 XN1 raised to the power MS is zero. And we consider I lambda the set of indices such that the corresponding components pass through the point one of the lambda zero and so on minus one and I lambda is the Is the set of these equations. Now, the regularity conditions are just two conditions. Number one, that if we look at each of the polynomials R, then it is point one over lambda is not a singular point for this polynomial. And second, that gradients of two different polynomials at this point one over lambda are not proper. This point one over lambda are not proportional. If lambda is equal to zero, then we have to pass to the chart where x1 is not equal to zero and take x1 equals one and literally have more or less the same definition, except that in this case we consider the first component one and all the rest varying. And if zero is a point, is a spectral point of the operator A1. Spectral point of the operator A1, then 0 belongs to the proper spectrum in this chart, lambda equals 0, and the definition of regularity is more or less the same. So now the following result holds and gives a little bit long theorem and gives a rigidity theorem for representations of Coxeter groups. So suppose that we have a So suppose that we have a costeter group with coxeter generators G1, Gn. Suppose that we have a finite-dimensional m-dimensional linear representation rho of G. And for every i between 2 and n, we denote a rho tilde i, the representation of the dihedral group G1i, generated by G1 and GI, which is induced by rho. We assume that the following condition is satisfied. Condition is satisfied. So for i between 2 and n, no irreducible representations of d1i is included in the decomposition of rho tilde i with coefficient bigger than 1. This is a little bit restrictive condition, but unfortunately, I don't know how to get rid of it. So suppose that a1, an are complex n times n matrices, such that a1 is normal, norm of a. Normal norm of aj is equal to one for every j from two to n, and the regularity conditions are satisfied for the pairs a1j and a1 and the pair a1 times aj at every spectral spectral point of a1. Also, we suppose that the proper spectrum of an contains the proper spectrum of The proper spectrum of images under rho of G1, Gn, and there exists a positive epsilon such that in the epsilon neighborhood of these points, the coordinate points, the spectra of A1AN and G1GN, images of G1GN are the same. Then there exists an M-dimensional subspace invariant under the X. Space invariant under the action of HAJ, restrictions of A1 of AN to L are unitary, self-adjoint, and generate a representation rho hat of G with the proper projective spectrum of the restrictions coinciding with the proper spectrum of the images under the representation row of Coxeter generators. And the third, if G is a finite non-special Coxeter group, finite non-special Caucasus group then a rho hat is unitary equivalent to rho of course it would be very nice to have to have the same statement in for non-finite special costs group but unfortunately this I don't know if it is true because the main reason is that if we have a finite group then the character of the finite group determines the representation for infinite group it is not. For infinite group, it is not. So, and the section three of this theorem, of course, follows from the previous result, which I mentioned by Chushkovich, Chernyv, and myself. And there, the character representations play the most important role. So, this is as far as Coxeter group goals. And now let us turn our attention to Adamar matrix. So a complex values and time say matrix is called complex Adamar matrix if all entries of this matrix are unimodular and rows of the matrix are mutually orthogonal. So Adamar matrices were introduced as far as I heard by Sylvester in 18. By Sylvester in 1867. He considered only matrices with entries plus minus one in orthogonal rows. Matrices with entries being roots of unity were introduced by Batson in 1962 and arbitrary complex unimoduler with arbitrary complex unimodular matrices by Soren Poker in 1983. And they appear in And they appear in several objects. First of all, maximum abelian star subalgebras, the algebra of complex matrices. Then it is unpublished preprint in archive by Jones, where he actually showed that these matrices played an important role in statistical mechanical model, not invariants and others. And I mentioned here only two. There were, I mentioned here only two, but there are several papers of Theo Banika where actually he tied quantum Adam R matrices to quantum permutation groups. Though Adam R matrices were around for quite a while, surprisingly not too much is known about them. And in particular, there is no description, full description of Adam R matrices, except for the cases For the cases dimensions 2, 3, 4, and 5. So there is a paper of Tadei and Zhukovsky, 2006 paper, which actually summarizes more or less what is known about Adamar matrices. And to find the description of Adamar matrices for n equals 2, 3, and 4, it is relatively simple. For n equals 5, it is a paper of Hagerup. It is a paper of Hagerup, 1996. It is much more difficult. And it should be mentioned that the only family of matrices that appear among Adamar matrices in all dimensions is the family of matrices, which is called Fn, so Fourier type. And here it is written that all the entries of these matrices are roots of unity. And this is the And this is the formula for these matrices. These matrices are related to the Fourier transform on Zn. Okay, so now let us consider a diagonal matrix with roots of unity appearing on the main diagonal in increasing order of arguments. And let us consider matrix B, which is given by 1 over n F star N A F n. Fn is the Fourier. Fn is the Fourier matrix of dimension n. It is a relatively simple computation which shows that the joint spectrum of the triple AB and AB is given by x to the n plus y to the n plus minus 1 to the n minus 1 z to the n is equal to 1. And of course, the quadruple ab and b a by x to the n plus y to the n plus minus 1 to the n minus 1. minus 1 to the n minus 1 and e to the 2 pi i over n z 1 plus z 2 raised to the power n is equal to 1. And since all these pairs are generated by Damar matrices of Fourier type, we usually call these hypersurfaces Fourier type hypersurfaces. So the following theorem was proved by Tom Peebles and myself. I think it was I think it was 2021. And so the theorem states that suppose that A and B be two n times n complex matrix such that A is normal, norm B is one with respect to Euclidean norm, and suppose that the joint spectrum of the triple A, B, and AB is given as one of this Fourier surface. Then, first of all, matrices A and B are unitary. A and B are unitary, and the spectra of A and B consists of n roots of unity of multiplicity one. If E0 N minus one and zeta naught, Zeta N minus one are eigenbasis for A and B respectively such that A E J is e to the 2 pi j i over n e j and b zeta j is e to the 2 pi j i over n zeta j. Then radical n times the transition matrix from one base. times the transition matrix from one basis to the other is a complex Adamar matrix. And if n is 3, 4 and 5, then the pair AB is unitary equivalent to the Adamar pair of Fourier type. A small comment. In this theorem, the statement one is pretty much straightforward. There is nothing mysterious about it. It is elementary exercise. Statement two, on the contrary, it's something which has to be It's something which has to be proved, and that's exactly where you need a local spectral analysis. And section 3 follows from the fact that all Adamar matrices for n equals 3 and 5 are of Fourier type, and for 4 there is a complete description which allows to show that those who are not Fourier type have different joint spectrum. Spectrum. Okay, so another theorem, but at this point, there is a still, it is not, spectrum does not determine, in the previous theorem, the spectrum does not completely determine the pair of matrices. And here is a more delicate theorem, which says that if I have a quadruple AB, AB, and BA, AB, AB, and BA, and locally the joint spectrum of this quadruple coincides with quadruple with the Fourier surface. Then there is a subspace of dimension N invariant under the action of NB, and the restriction to the subspace of the pair AB, it is already completely fixed. And that's exactly what we have. And that's exactly what we have. And transition matrix in this case is in the form just lambda one Fourier matrix times lambda two where lambda one and lambda two are diagonal matrices with a unimodular entries of the main diagonal. And as a corollary, it is possible to get a complete characterization of Adamar matrices of a Fourier type. And these matrices are presented in These matrices are presented in the corollary right here. Maybe I will expedite a little bit and move slightly further. So all the proof of these results is actually related to permutation, certain permutation matrices. These are permutation matrices of certain permutation of a type where P of J is equal to Qj plus M, a modula N, where Qn. Modular n, where q and n are mutually prime numbers. And if we look at this particular specific matrix, which is like a circular transformation, then we will see that the following result holds: that if you have two permutation matrices of order n, then we can actually construct a matrix using this permutation. Using these permutation matrices, Fourier matrices, and the matrix A, diagonal matrix with roots of unity on the diagonal. And then if one of these matrices P1 and P2 belongs to this subgroup, then the joint spectrum is necessarily a Fourier surface. And if P1 is in this group, then the pair A and B of P1, P2 is unitary equivalent to P star AP. To P star A P behad N, where P is some permutation. And if P2 belongs to this group, then there exists a permutation such that the pair A and B of P1, P2 is unitary equivalent to B hat P star AP. Now let me devote the rest of the talk to rigidity theorem for Theorem for quantum SU2, and I start with Lie algebra SL2. What else? So the Lie algebra of A2 is the algebra of traceless complex 2x2 matrices, which is generated by three matrices, E, F, and H. And it is well known that the commutation relations between these matrices is a commutant of HE is 2E, commutant of HF. Commutant of HF is minus 2f, and commutant of endf is equal to h. A known fact is the relatively easily provable fact that for n equal to greater or equal than 2, there is only one up to an equivalence irreducible n-dimensional representation of SL2. And it is generated by n times n matrices EN, Fn, and Hn. Fn and Hn, which as operators on Cn act on the standard basis in the following way. So Fn is a push forward, En is a push backward, and Hn it is just a diagonal matrix with n minus 1 minus 2j on the main diagonal. Well, now since there's only Now since there is only for each n, there is only one up to an equivalence irreducible n-dimensional representation of SL2, the joint spectrum of matrices EN, Fn, and Hn determines the representation of this representation up to equivalence. And basically, that shows up since every finite dimensional representation is a sum of irreducible ones, then we obtain that Jones. That joint spectrum of a representation of SL2 is completely determined by the representation is completely determined by the joint spectrum. So the question is what happens with the rigidity? And the situation is quite different. Let us consider a very simple example. So let us consider three-dimensional case. Then the matrix E3 is given by 0 to 0. is given by 020002000 F3 is 0001000 and H3 is 2000000 minus 2. And it is very easy to see that the adjoint spectrum is given by t times 4z squared plus 4xy minus t squared is equal to 0. Now let us consider four complex numbers alpha. For complex numbers, alpha, beta, gamma, and delta, such that alpha, gamma, and beta and delta both are two, and matrices A1 and A2 as here. And A3 is the same diagonal matrix. Then it is a very simple computation shows that sigma of A1, A2, and A3, and sigma of A3, F3, and H3 are the same. But there are commutation relations between A1 and A2, for example. And A2, for example, are completely different from A3. And it shows that germ spectrum in general, if I know that I have some matrices, even if this matrices satisfy reasonable conditions, but not very strict, then the germ spectrum does not tell me that I can actually reconstruct commutation relations. And the question is: how to do this? Obviously, the main reason of this. The main reason of this phenomenon is that, in fact, there is a lot of nilpotency, and nilpotent matrices usually are very bad for the spectral considerations. So, the question is: how can we remedy this situation? Well, the one way which I suggest is understanding that in reality, Understanding that in reality, the algebra SL2 is what is called a limit case of infinite similar representations of the twisted SU2 group. So let me just give a short definition of a quantum SU2 group. This is a group which was introduced by Voranovich in his talk in his idea. In his ICM talk in 1986. And he called these groups matrix pseudogroups, though the terminology pseudo groups is not his. In general, pseudogroups as an algebraic object and pure theoretical were introduced in the late 50s by Kartz and the achievement of, but at that time, except for tensor products, there were no really were no really good examples of pseudogroups. And a real achievement of Voradovich was that from his underlying physical considerations, he's a theoretical physicist, he actually came up with something and figured out that the most appropriate language for this kind of objects would be C-star algebras. And he developed the whole machinery of doing with this. Unfortunately, I can Doing with this. Unfortunately, I cannot give a lot of details about twisted SU2, so let me give only the definition and the result. So suppose that nu is a parameter between a minus one and one, and let A be a C-star algebra generated by two elements, alpha and gamma, satisfying the commutation relations which are written here. are written here. And of course, if nu is equal to one, then we see that this algebra is a commutative algebra. And Boronovich gave actual specific algorithm for constructing this C-star algebra. And then he, in fact, said that this algebra has a structure of a HOF algebra and done this way. So he considers. And done this way. So he considered an algebraic matrix U, which is just 2 by 2 matrix, alpha minus nu gamma star, gamma alpha star. And first of all, he said that C star algebra is generated by a star algebra A. So we consider script A algebra, star algebra, just generated by entries of the matrix. Generated by entries of the matrix U. And this star algebra is dense in our C-star algebra A. Then there exists a star algebra homomorphism, which of course is co-multiplication A into the tensor product A and A, which is given by tensor product, just matrix U with itself. And there exists a co-inverse. A co-inverse k which actually maps script A into itself, and K gives us the inversion of the matrix U. So the idea was that the pair A and this matrix U determine the twisted SU2 group, in a sense that sister algebra A is considered as a non-commutative algebra continuous functions on SU2. Continuous functions on SU2 and star algebra script A plays the role of smooth functions. So Voranovich showed that this object has a structure of a compact topological group. And when nu is equal to one, this group is a classical SU2 group. Okay, so now here also in general, quantum groups don't have Quantum groups don't have like Lie groups, they don't have associated with them Lie algebra. So he had to develop a differential calculus on this, and he introduced the operation, though he used slightly different notation. So if I have two matrices U and V with entries in star algebras B and B prime, then actually the product of these matrices where instead of product element by element, we consider. Product element by element, we consider a tensor product is devoted by this symbol, the circle with a point inside. And now the definition of a finite dimensional representation of SU2 is very simple. So essentially, the co-multiplication has to correspond to the matrix multiplication for the representation. That's the essence of this definition. Of this definition, and in particular, if we look at the Varonovich's theorem, which I presented before, then this matrix which we mentioned gives a determines a C2 representation, which is called a fundamental representation of the twisted SU2 group. Now, the next step is to determine the infinitesimal The infinitesimal generators of this algebra. So we consider the set of 4x4 matrices with non-trivial entries only on the first row and the main diagonal and consider four matrices which are presented here. And it is very easy to check that these matrices satisfy the commutation relations which we mentioned above. So again, So again, it is a verification fact, which was done by Varanovich, that we can actually find the map a script A, algebra script A, into the set of matrices which satisfies the conditions that F of alpha is equal to alpha M, F of gamma, gamma M, F of alpha star, alpha star M, and F of gamma star, gamma star M. And then if we look at And then, if we look at the entries on the first row and the main diagonal, then of course each of them represents a linear functional. And these functionals are denoted by E0, chi naught, chi1, chi2, f naught, f1, and ff2. And these functionals determine, obviously, their infinite dimension uh infinite decimal generators of the presentation by the following formulas. By the following formula. So we have three operators: E of V, which is equal to identity times chi naught v, h of v identity times chi1 v, and f of v identity times chi2 v. And they're called infinitesimal generators of v. And for fundamental representation of SU2, they are practically the same as when nu is equal to. When nu is equal to one, h of u is just h of u, it is exactly what we have as a generator of SL2 E of u, yes, and f of u is just minus the one which we considered as a standard one. So Varanovich proved the following theorem: that infinite is an infinite semogenator's determinant representation for equivalent. Representation. For equivalent representations, the set of infinite similar generators are equivalent, and infinitely similar generators satisfy certain commuting conditions. And if the representation is in the Hilbert space and the representation is self-adjoint, then of course we have minus nu E of nu star is equal to f of nu and h of nu star is equal to h of nu. So, and any triple So, and any triple A0 A1, A2, which satisfies this commuting relation is called infinite similar representation of SU2. So, the main theorem of Voranovich was that there is only one infinitesimal irreducible representation of SU2 in every dimension. And this is written absolutely clearly. Absolutely clearly, it resembles the representations of SL2. One matrix is diagonal with certain steps, the other is push forward, and the third was one is push backwards, but with certain weights. Okay, so now the rigidity spectral rigidity theorem. Rigidity theorem for representation for infinitesimal representations is the following. Suppose that I have a parameter nu between minus one and one, which is not zero. If I have a1, a2, and a3, three n-dimensional matrices such that a1 is normal and the terminantal hypersurface of all these eight elements, a1, a2, a3, a1, a1 star, a1. A1 A1 star, A1 star, A1, A3, A star, A3 star, A3, A1, A3, and IN is the same as for Voronovic's matrices, then A1, A2, and A3 are unitary equivalent to those which I described in Voronovich's theorem. Oops. And one thing, but if I consider nu equals one, then this. One, then these generators are not exactly the same generators of SL2, which I described above, but using exactly the same method of proof, it is possible that the same rigidity theorem holds for the generators of SL2, which I described earlier. The fact of the matter is that here this joint spectrum is not invariant under Is not invariant under the transformations which are under just similarity. We need unitary equivalence. So if two tuples are similar, it doesn't mean that the joint spectrum of this a tuple is basically the same. So every time when you have non-unitary equivalence, you have to prove that the rigidity holds. So in this case, it does. So, in this case, it does hold. And presumably, there are some other algebras. Well, obviously, the first candidate for having this kind of rigidity theorem are finite-dimensional Katz-Moody algebras, but that's something in the future. So, thank you. That's all. Very much. Are there any questions for Michael? Yeah, please. So let me try to rephrase what you said. If I understand correctly, so you were saying that for a certain type of homogeneous polynomials, the determinant of representation is unique up to unitary equivalents. For certain types. For certain types of homogeneous polynomials, not polynomials, but yes, for surfaces. For instance, in the Harmon matrix space. Yes. Yes, but you have to consider certain type of polynomials. You cannot consider polynomials of, say, for example, I have two matrices. If I have x to the n plus y to the n, it doesn't mean that this matrix That this matrix, this pair is unitary equivalent to the standard pair. I have to add something. If I add the third element, A1, A2, then it shrinks the opportunities, but nevertheless, you can consider for the diagonal matrix, which one of them is diagonal, you can consider permutation of the entries on the main diagonal. It is still an. Diagonal. It is still and it will not be unitary equivalent. But if you consider a quadruple A1, A2 and two products, A1, A2, and A2, A1, then it is a complete rigidity. If the joint spectra are the same and the joint spectrum coincides with the spectrum of the standard pair, then it is unitary equivalent to the standard pair. Did I understand your question correctly, Ron? Yeah, we can come later. All right. Okay, all right. So, well, we thank Michael again. Okay, thank you very much. And well, enjoy the rest of the conference. You make me envious. I was looking forward to being there. Unfortunately, I'm not. So, okay, guys, I will try to join you online. I would like to remind everybody that there is a talk here at 19:30 from Constance. It's an NSA. It's an NSF presentation. I'm not sure if it's a presentation of NSF or a mathematical talk. It's about finding opportunities, but I will not take the hour long. Okay. I promise.