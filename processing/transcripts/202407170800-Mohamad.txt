Thank you, Denise. My speaker is muted. Is that okay? Excuse me. Yeah. My speaker is muted. Is that okay? Yep. You can hear us. Okay. Sounds good. Yeah. So today I'm going to be talking about at a high level stochastic dynamical systems, ideas and uncertainty quantification of complex systems. And we'll also look at this in the context of systems where we see extreme events. We see extreme events. And so these lectures are going to be at a high level. I'm not sure how many of you in the audience have looked at or worked with SDEs by show of hands. Okay. So there's a few. So I've structured this and I'll show this later on that hopefully someone can get something out of this. You know, if you're unfamiliar with SD, there's some material in here that will hopefully build up some expertise. That will hopefully build up some expertise. And if you are familiar with SDs already, hopefully there's some interesting ideas here that you haven't seen before and especially interesting models. So I just also want to thank Denise and the organizers for inviting me to speak here and also Kim Senvers for the funding. So a little about myself. So yeah, I'm an assistant professor at the University of. So, yeah, I'm an assistant professor at the University of Calgary. I've been there for almost two years now. And the lectures, these lectures are actually based on several courses we taught at the Krant Institute at NYU, where I was a postdoc, on modeling and the statistical prediction of complex systems and other variations of this course titled, you know, some years it was on data simulation, depending on the flavor of the course that we were. Of the course that we were, or depending on the what we wanted to teach. So these were developed by Andy Maida, Di Chi, Nan Chen, and many of his other postdocs who helped formulate a lot of the ideas I'm presenting today. And so, and I was there for several years, so I had a good interaction with the folks that developed different aspects of. That develop different aspects of some of the work I'm presenting. And my research interests are primarily related to these main topics, right? So UQ, extreme events in the context of physical systems. I also have done work on sampling methods related to surrogate modeling and digital twins for extreme events, data assimilation, and kind of ML ideas and dynamical systems. ML IDs and dynamical systems. And all this work has really been in the context of physical or engineering systems. So by training, I guess I'm a mechanical engineer, but I'm very interested in physical systems in nature and how we can apply modeling techniques to alleviate some of the computational challenges of these good labels. And lastly, you know, anyone who's interested in joining my team, I am hiring from I am hiring for masters, PhDs, or postdocs. Okay. So one thing I also wanted to mention is this lecture, I think, is good because we saw Whitney talk about how to quantify extreme events from a more statistical perspective. Whereas hopefully here, I'll provide maybe a more physical intuition about how these extreme events might emerge in dynamical systems. So I think the contrast of that. I think the context of that will be very helpful for folks in the audience who are studying extreme events in their own problems. So, as we probably are well aware, turbulent dynamical systems are very ubiquitous. And they're characterized by large-dimensional phase space and strong instabilities. And we'll see this, how this arises and impacts the dynamics. And these instabilities transfer energy throughout the system between different spatio-temperature. System between different spatiotemporal scales. And there are many examples of this. You have wave turbulence where you see this, such as road waves in the ocean. There are examples of plasma turbulence and many other phenomena, such as LJO or quasi-geostrophic equations, where the existence of strong instabilities in the large-dimensional phase space are. Large dimensional phase space are very key features and the dynamics of the flow. And if you have questions, feel free to interrupt. So hopefully we can go through all these notes today and tomorrow, but we'll see how far we get. So as I mentioned, some of the challenges are the fact that these turbo dynamical systems have a large phase space and the dimension of the instability. And the dimension of the instabilities is very large. So there'll be some subspace where the instabilities exist, and that's a high-dimensional space, which makes UQ a big problem or a big challenge. And so the, sorry, there's a question. Can you explain what do you mean by dimensional instability? So maybe I will, I, so maybe we'll hold on to that thought. That thought, and I'll note it down. I'll note it down so I'll remember to come back to you. So one of the key topics here is that approaching these problems from a statistical and a stochastic modeling perspective combined with ideas from nonlinear dynamics is very helpful. Okay, so we'll see that. And some of the central And some of the central applied math science issues related to this is we want to, you know, we want to have accurate predictions and representation of suitable statistical mechanisms for observations from nature. But we want to be able to cope with model error. So we have a lack of physical understanding and typically inadequate resolution due to either the curves of ensemble size or computational. Or computational overload, which necessitates some sort of resolution to your simulation, which makes UQ very challenging. So ideally, we want to judiciously select models with carefully chosen model errors that can achieve accurate predictions while coping with the issues related to finite truncation of your numerical solver. Of your numerical solver and finite number of ensemble members, which are not too large, right? So you'll see these ideas throughout this lecture and also in the attached references where approaching this from this kind of information theoretical framework has a lot of benefits. So some of the things we'll just quickly highlight today. Highlight today or tomorrow are looking at Gaussian and non-Gaussian systems. I'll talk briefly about data assimilation. And so one of the exercises is a very simple problem in filtering a complex signal. So hopefully we can all work together to solve that problem and see how data simulation can help us overcome errors in observational noise when we Errors in observational noise when we have a model. I'll also talk about different ideas related to conditional Gaussian, this conditional Gaussian framework, which can help us predict or predict different variables in the system. And then also we're going to look at building complex models with representative features of extreme events and so forth. And there are many examples of this. Some of the key examples we'll look at is: depending on time, we're going to look at predicting observed and hidden extreme events, or you can also refer to the references, potentially looking at the statistics of passive tracers. And then also, there are other examples which we're not going to look at or touch upon, such as plasma models, shallow water. Shallow waterways with depth changes, which trigger non-Gaussianity in the wave elevation, and so forth. So, this is just a highlight of some applications where these ideas have been shown to be helpful. So, for today, these are going to be the main topics for today and tomorrow. First, I'm going to just talk about general turbulent dynamical systems for complex systems, and I'll talk. Complex systems, and I'll talk about the challenges in predicting these systems as I highlighted just momentarily ago. We're going to look at a general setup for turbulent dynamical systems with quadratic nonlinearities. And then we're going to look at imperfect models. So we have some model errors when we're trying to maybe predict or apply UQ to such systems. And then we'll look at the finite dimensional representation of them. Representation of them. I will highlight what state estimation is at a high level, and then also some metrics that are used to predict or to quantify prediction skill. And then after this, I will basically discuss or show you some dynamical systems where we see extreme events. So these are going to be just prototype nonlinear systems. Nonlinear systems where you have extreme events of different natures or extreme events that arise from different characteristics of the system. So they might be instabilities or they might actually be something a little different, like a resonance phenomenon. So I will provide some examples where you can transparently, hopefully, see those. Okay. Okay, and also show some, and these are all going to be low-dimensional models for extreme events, except for maybe a few. Okay, and then related to these topics, I'll briefly talk about some ideas related to systems where the full setup is non-Gaussian, but there is this conditionally Gaussian structure that can be leveraged for data assimilation and other tasks in UQ. So I'll provide some examples with this conditionally Gaussian structure, which is interesting, which is an interesting observation. And this can be used for very rapid data assimilation. And so, and then in the last part, we're going to look at, we're going to just basically provide a stochastic toolkit for UQ. So this is, you know, applied SDEs in 10, 20 slides. So it'll be. 10, 20 slides. Okay, so it'll be a quick overview of the key ideas in STEs with interesting examples. So you can see how we can actually use the theory to analyze and study and understand dynamical SDEs, okay, with some interesting features such as extreme events, such as the amplification of the model error depending on the parameters. So hopefully this is this is this will. This will be interesting for folks that have also studied SDs in the past, just to see them applied to different problems. Any questions on the outline today? Sure. Differential equation. Just a dynamical ordinary differential equation with some noise. Yep, yep. So, okay, so here's the system. Here's the general dynamical system with perhaps some noise. Okay, so this, if you know, this is just some fluctuating forcing if you're not familiar with W dot, right? So this is just some forcing. You can think of it as like highly random, okay? Some very highly random forcing. So when we're looking at When we're looking at a lot of these natural systems, the phase space has high dimensions. So we have U, and this lives with Rn when N is very large. And sigma is a noise matrix, and W here is a K-dimensional white noise. So this white noise, why do we add it? Well, we often add it because it models or represents degrees of freedom in the system that we can't maybe explicitly model. Explicitly model or small-scale fluctuations that we don't really have a good descriptor of. So we just put them into this term, right? Additively at this point. It doesn't have to be a brownie motion. No, it doesn't. But if it's not Brownian motion, the analysis is very challenging, right? So that's why we stick it in as Brownian motion. What's with the little thoughts about it? It's the time derivative, right? This is not written. derivative right this is not written this is white noise right this is white noise the derivative of uh wiener process think of w dot time dt s dw oh yeah yeah it's got a physics notation yeah yeah it's uh right in the physics sets okay um yeah so um you had this question earlier um You had this question earlier. So, the phase space of this dynamical system typically, for what we're looking at, has a large dimensionality of unstable directions. So the number of positive Lyapunov exponents, right, the space of the instabilities is typically large. Or we have non-normal transient growth. Okay, so we'll see an example of this. See an example of this non-normal transient growth towards the end of the lecture. Okay, so that's what we mean that we have a large dimension of unstable directions. And what is very interesting is that these linear instabilities are basically mitigated by energy-conserving nonlinear interactions. And these nonlinear interactions transfer energy. Interactions transfer energy from the unstable modes to the stable modes. So, this nonlinearity in these systems is extremely important, and it's responsible for energy exchange from dimensions that are unstable to dimensions that are stable. So, this increase in energy from these unstable directions is balanced by nonlinear interactions. By nonlinear interactions. And I will break this system down in more detail where this becomes more transparent. And what we see is that through this energy exchange, we have this dissipation of energy, typically in the linearly stable modes, which dissipate the energy from the linearly unstable modes. And through this kind of transfer of energy, these systems often have. These systems often have, or these systems that we're going to look at, have some sort of statistical steady state. So we're not looking at systems that, for example, blow up, right? We're looking at systems that have a statistical steady state. So there is some attractor in the long time limits, okay? So breaking this equation down more specifically or more. Down more specific or more in more detail, we have the following general structure for turbulent systems with quadratic nonlinearities. Okay, so we have this finite dimensional system, mu lives in Rn, and we have several terms here. We have linear terms, L plus D, okay, and then we have this non-linear term, D. And L plus D. And L plus D, we have two terms in here. So these are matrices. L plus D, we have one matrix D, which represents damping. Okay, so this is terms at this state energy. So this is a negative definite symmetric operator. So this is damped energy. And then we have another linear operator, L, which is a skew symmetric linear operator, right? So this contains your non-normal dynamics. Dynamics and this kind of is responsible for that interplay between, or this is responsible for modes that grow, that skew symmetric matrix there. And then we have this beta term or this B term here, which is a quadratic operator, which conserves energy by itself. So we have it to satisfy this. And this term here is responsible for the This term here is responsible for energy exchanges. And then finally, we have forcing plus this noise term. So we basically broken down, we have some forcing here, some deterministic forcing, if you will, plus some fluctuations that live on top of that forcing, right? So this is just the construction that we have. So this is a pretty general setup, right? Setup, right? Obviously, not everything will follow into this structure. You know, if you're looking at certain systems, I don't know, if you're looking at maybe like a Schrodinger type equation, you'll see cubic terms, but those are based on simplifications of the Navier-Stokes-Stokes equation, which is, or rather. So just keep that in mind. Was there a question? What is Question: Yeah, what is K? What is uh K? Are you looking at this? Yeah, this is just how we, you know, this is just how you structure the, it's not that important. It's just, you have summation of how you construct the noise, right? Depending on the dimension of this and this. So pay is from one to N. One to N. Want to add yeah so that's something yes what does you represent like in terms of like the um maybe hold on to that we'll see an example of what you represents um but you is uh you is just your state right it's a sort of state of your system right um if you have you know xyz that That would be u, right? Or if you have some discretization of a PDE, is it x1, x2, x3, x4, all the way to n, right? The components of that would be represented by not necessarily. Well, actually, we'll see this in the next slide, how this. We'll see this in the next slide, how you could represent a PDE like this, right? But this is the state of the system. This is the state of the system, right? If you have X, Y, Z, X is described by some ODE, this is vector of that state, right? The state space, just state space. Can I ask you? Yeah. So the state space is going to, it's going to be called a continuous space. Yes, it's finite dimensional, it's finite dimensional here, yeah. But each of the dimensions, each of each of them is going to take continuous values. Each of the components, yes, yes, yeah, time is continuous, yeah. But we're looking at a fine-dimensional space, okay. Not a huge restriction, actually. We can always take, we can always go to a finite dimensional space, not a huge restriction, but. Space, not a huge restriction. But yes, the components are continuous in time. Any other quick questions? Okay. So we have the system and what are the challenges more specifically? Well, I mentioned some of these and UQ kind of in UQ, we want to deal with the probabilistic characterization of all possible evolutions of a dynamical system with, you know, given, given a set of initial possible. A given set of initial possible states, as well as the statistical characteristics of the random forcing or parameters in the model, right? So we have sources of uncertainty, which also potentially could include boundary conditions or initial conditions, approximations on the model, model error, limited observations and data. And these are associated with computational challenges, right? Challenges, right? So non-stationarity is a huge one, a large number of scales that are difficult to cope with, and non-Gaussian statistics, which makes most analysis very challenging, if not impossible. So those are big problems. And one goal is to basically obtain accurate statistical estimates, perhaps for the changes in the mean and the variance, which oftentimes will. Variants, which oftentimes we're really limited to, and this doesn't even include any high-order statistics when we're looking at extreme events, but obtaining estimates of just the variance often is a big challenge. And four key statistical quantities, when we may be changing, when maybe there are changes in forcing or uncertainties in the initial data. In the initial data. So it's largely a big problem, and that's why there are so many different ideas and algorithms in the space because there's nothing universal, right? A lot of it is problem-specific, model-specific. And so this might be a naive question. Yeah, why okay? Those mean embarrassments are hard. Can't you just do something and You just do something and compute the empirical mean and yeah, you well, if you have a million-dimensional state space and you want to take ensembles, that's the challenge, right? Like sometimes a DNS simulation, you can only run one ensemble. I can take weeks, right? So that's why we have high dimensionality. Yeah, the high dimensionality is a big constraint. We'll look at that in the next slide. So that's a good observation. slide so that's a good that's a good observation so what can we do well you know we have this this truth um underlying any system we're studying there's a there's a perfect model right that's kind of the operating assumption is there's some truth okay maybe model like this okay and our imperfect model right lives in a lower dimensional space right and we want m to be a lot smaller than n right Smaller than n, right? So, you know, in practical systems, n could be a million or a lot higher dimensions. And so you want to construct imperfect models where m is smaller than n. But this is a challenge because you have to make some approximations when you reduce the dimensionality to n, right? So you have model error. So, you have model error. And model error is the key thing that you should always be aware of whenever you're doing anything, right? Anything you do, you have to be mindful of what kind of errors you're making because those can triumph any kind of predictions you make. You can think you're making accurate predictions, or you have some method that's super accurate, but if the model error is larger than the accuracy of your predictions, it's a meaningless. Meaningless, what you've done is meaningless, right? So we have the imperfect model. And what we can do here is we can carefully try to construct this imperfect model with M a lot smaller than N to try to mitigate some of these model errors. So we can hopefully construct an imperfect model that has some of the correct statistics of the truth while still living in a lower dimensional space. Lower dimensional space. And this was your question about ensemble predictions, right? And so you're right. For chaotic systems, even without any noise, right? Without any noise you can think of, like the Lorenz 63, single predictions have little value, right? Chaotic trajectory. So we often look So, we often look at ensemble predictions. So, we hopefully can simulate a whole bunch of ensembles from different initial conditions and look at the behavior of those trajectories in time. So, this is one approach, but the challenge that I just mentioned is we have a large-dimensional N. So, even performing, you know. So even performing simulations where L, which represents the number of ensembles, order of 100, is usually very challenging. And so we hope by making carefully chosen model errors through reductions that we can increase the number of ensembles. But model errors can swamp any kind of gains in statistical accuracy that we make through. Statistical accuracy that we make through taking L to be large. Okay. So that's the key challenge: how do we do that? So I'm not going to talk about too many techniques, little length references where you can look at further ideas where this can be done more concretely, but I just want to give a flavor of the ideas. Okay. So this was the question that we had is how do we actually How do we actually represent these systems in a finite dimensional space, right? And so if you have a PDE, what you can do is you can project that PDE onto spatial modes. So we have this finite dimensional representation of a stochastic field consisting of fixed in time orthogonal basis functions. So we break down the signal into a mean. So, we break down the signal into a mean and deviations from that mean. So, what this means is that if I take the average of this, u prime is zero. So we actually looked at this yesterday a little bit, right? And so this u prime is a fluctuating term that lives on top of the mean, which is non-random. So maybe there should be an omega. Random. So maybe there should be an omega here as well. T dot omega. Anyways, so these b's are bases functions, right? So you can think of them as maybe four A modes. If you had a PDE, right, one way to solve that PDE is, and if it's periodic, you can project the PDE on Fourier bases, which are optimal bases for periodic domains, right? Periodic domains, right? So those v's would be, in that case, EIKX, okay. And these coefficients Zi would represent how these bases change in time, okay, for different k. Is your Z uniquely defined? Is this decomposition unique? Is this decomposition unique? It looks like Mark. Unique looks like a martingale representation to you, although we're not generally assuming you need to do a marketing. You have a exactation part, you have a drift part, sorry, fluctuation part. Yeah. Is it well, I'm not sure if I don't know how to answer that. Is it unique? At this point, there might be, I don't know. I don't know. It depends on your orthogonal basis. Yeah, you can pick any of the organic basis. Yeah, you're right. So I guess it's not, it's definitely not unique. Yeah, yeah, okay. Yeah, yeah. There's, yeah, there are some, you know, there are some conditions on VI which I haven't written down, but the understanding is those constraints should make it, depending on the selection of those bases, should make it unique in some sense, okay? Look in some sense, okay. Um, another thing I should mention is uh, these sort of fixed-in-time bases doesn't have to be true, doesn't have to be true. Um, the bases can actually change in time, uh, which is a very powerful idea. Um, if you have fixed bases and you have a very complicated system, and typically has to be very large, so there are some ideas where they relax the notion that. They relax the notion that Vi doesn't depend on time, that it's it's not a stochastic basis, so it doesn't have any dependence on W, which is the probability space, but it can change in time. But often we keep it fixed, right? If you look at ideas like proper orthogonal decompositions and so forth, the bases are always fixed, right? Which is why we have challenges applying those methods. Those methods because you find out that you need more and more bases to capture what you're interested in. Right. So, regardless of that, let's. Can I ask for one clarification? Yes, yes. So, in your notation, it seems like you're thinking of this u as another argument, x, right? It's kind of hidden in its high dimensionality. Kind of hidden in its high dimensionality, I think. Oh, no, there's no X here. I know, but we're thinking of U as something that is measured on maybe a spatial grid that its components correspond to values of a function on that grid point, right? Sorry, I don't. Sorry, I don't quite follow. I'm not sure if I understand the question. Because you're saying, like, we can think of this as a P, you can represent the solution of a P E in this way by making U high dimensional. Yes. Right? Yes. So the way I'm thinking of, okay, what you originally have is a function of space and time, but then you turn it into something finite dimensional by evaluating on grid points. Yes. Grid points and every value and a grid point becomes one dimension of your U. Sure. Right? So, in that sense, what is the difference between U bar? So, that's say lower dimensional object. No, U bar and they're all in Rn, right? This is just, I mean, you can think of Vi as being this, right? VK is this, okay? Right. Right. So you the average here is again average or eight and you can sell average. Maybe I will give an example or explain to make it more concrete how this how this, I think, but you can just think of But but you can just think of um yeah let me I think I understand your question um or is it's like it's a deterministic portion of it that doesn't have it's not the galaxy projection yeah yeah yeah you get this right it's any exactly yeah yeah yeah exactly yeah um you're You're asking about the difference between high dimensional and low dimensionloading, right? Well, I think she was asking, so I would. I think, but well, I mentioned with the count data. Yeah, we haven't done, there's no, I haven't said there's no reductions, there's no nothing here. Um, I just want to highlight or kind of explain a little more about energy transfers in this system. Transfers in this system, which is already, you know, there's maybe some underlying PDE of X and T. We've done the projection on some spatial bases, and then we arrive at this finite dimensional system, right? Or, you know, if you do any kind of, it could be any numerical kind of, you know, any kind of projection, right? And then we have this result. I guess what I'm trying to say, if you go back to that equation, so you barge. So u bar t does not have any omega in it. Yes. U bar t. Yeah. Yes. You're right. It's deterministic. Thank you. Yeah, it's deterministic. The fluctuations are the only component that have w omega, which is limited in probability space. Yeah. So I think it was maybe my mistake not to, I'll update the notes to include that after, so page 14 to make that very clear, probably. So, um, so you're absolutely right. Yeah, so so, what you can do, okay, so we have this representation, it's arbitrary, okay, but we're assuming we have this representation. The bases are fixed in time, also an arbitrary assumption that we've made and imposed, can be relaxed. So you take this, you plug it into the equation, take expectations, and you can derive an equation for the mean field u bar, and you obtain the following. And you obtain the following. Okay. And you can also do the same thing once you have the u-bar, subtract that from this equation here, and then you get an equation for the fluctuations. Okay. And then you know these are orthogonal bases. So then you can take an inner product to derive an equation for what these coefficients are that are multiplying the bases functions. That are multiplying the bases functions. Okay, so this is algebra. You'd obtain this equation at the bottom here. Okay, it's a good exercise to do this and double-check that you can verify these equations. But this is what you get if you follow through with that idea. This is extremely important here because all those things I said earlier, you can now understand them by looking at these equations. Okay, so these are the coefficients. Coefficients. Okay, so these are the coefficients of the model. The covariance or the variance of this model is obtained by looking at the average of Zi multiplied by Zj. So that's the variance, right? So if I have a signal X T, say omega, right, we know that. x squared t omega is the variance, right? And that's the only component that has elements of randomness. So if I multiply this by zj, take averages, I would obtain an equation for how the covariance evolves for the system. And the result is the equations at the top here. Okay. So these are. Okay, so these are the equations for the statistical mean and the covariance dynamics, whereas I just mentioned Rij represents the covariance of different vary or different components of U. Okay. Now, let's just take a moment to look at this equation here. Okay, so we have So we have the equation for the covariance, and we have this matrix L V. And L V is represented by, or it's given by this. And so you can see this matrix L expresses energy transfers between the mean field and the stochastic modes, as well as there's some energy dissipation and non-normal. Some energy dissipation and non-normal dynamics due to, as I mentioned earlier. And we also have this matrix QF. And this is the main challenge here. We actually talked about this yesterday. I think that was your question about the equations kept, why can't I just solve for the third order? Solve for the third order, right? Well, these equations are not closed, right? So the second order moments are defined in terms of the third order moments. Okay, so then it's like, okay, let me derive an equation for the third order moments. Well, it keeps going and going. So these equations are not closed. And that's due to this QF term. And this QF term has interactions between three modes, M and I. So, this expresses basically the energy exchanges between the different modes due to the nonlinear term B in our equation, that quadratic term. Okay. And remember, this is an energy conserving term. And you can also check that. You can also check that. So this is the challenge, right? So, this is the challenge, right? The question is: okay, so we can't really solve these equations, they're not closed, so we have to do something, right? This is where all these order reduction methods try to cope with. I'm not going to talk too much about it. I will maybe just say something that is important in a moment here. So, we have the truth. Okay, this is the truth, and we want to develop an And we want to develop an imperfect model that can kind of capture the truth, that is close so that we can actually numerically solve it. And there are many different closure schemes, which I'm not going to talk about in detail or at all, really. I will just mention, you know, what you could do is, okay, there's this QF term here, and it seems very challenging to solve. It seems very challenging to solve. So, why don't I just drop it? Right? It seems like something maybe the first thing you would think of: okay, I can't solve this. Let me just drop it and solve it. And this is a very bad idea. This is in general a very bad idea that doesn't work. And the reason is you have when you destroy that term, you no longer have the correct energy exchange between the different modes. So you have energy being kind of. So, you have energy being kind of injected from the mean due to maybe F and the fluctuations. And this energy goes to the unstable modes where the eigenvalues are positive. So maybe I will just put eigenvalues are positive. And then what happens in the original system is you have that. happens in the original system is you have that B term, B U, that quadratic term, which is kind of responsible for transferring the energy from the unstable modes to the stable modes, U prime, where you have negative Lyapunov, where you have the negative Lyapunov exponent, so the stable subspace. And so when I drop QF, I just destroy this energy. Just destroy this energy flow, right? So there's no way to dissipate the energy from the modes that grow in time, right? And so if you went ahead and tried to do this, even for a simple system like the Lorenz 96 or other models, it would blow up, would blow up. And you see that in a lot of numerical schemes where the simulations blow up. And as a result of that, what people do is add additional damping. People do is add additional damping, okay? Which is really not the best idea because you've over-damped the system, so it's not completely realistic. So one of the key ideas is instead of just damping the system, which is not very physical, but it's okay to do if you've resolved the system to a high enough scale. In general, like DNS people, you know, they just, that's often kind of what they do. But you want to, in general, They do, but you want to inject some additional noise. If I kill this term here, what you can do is you can kind of replace this by additional damping and noise so that the covariance does have some sort of steady state. So you could read more about this in the linked paper, and also I'd be happy to recommend some other references on this. But I just wanted to highlight that for these turbo. For these chart systems, and it kind of mentioned why they're so hard to simulate, right? It's kind of non-trivial to get the correct statistics in the imperfect model because of these energy exchanges. So, whatever you do, you have to carefully select or you have to carefully think about. Okay, so okay. So, that's all I wanted to mention. Yeah, question. Fluctuation is zero here? Did I assume the fluctuations were zero? Are you referring to this equation of the top? No, the fluctuations are these are the equations for the fluctuations, right? And here. It's the covariance of the fluctuations. So B completely. B are yeah, R, R is, I would say R. R is the co the um It's not closed because a closed equation, I can solve this and solve this and I'm done, right? But I don't have the equations for QF, right? They're in terms of higher, they're in terms of these third-order moments. I only have the second order moments, right? So it's not closed. I mean, I don't know if there's a better way of. If I mean, I don't know if there's a better way of more unknown equations, yeah, yeah, exactly, right? Yeah, yeah, but degrees of freedom doesn't add up because it matches. You have more degrees of freedom than you have equations. I wouldn't say degrees that you have more degrees. No, the degrees of you have more unknowns than equations, maybe, yeah, but not degrees of freedom. The system is already, yeah, yeah. Degrees of freedom refers to noise, right? Degrees of freedom refers to the voice, right? Well, I'm thinking the degrees of freedom refers to n here, the dimension of the state space. That's fixed, right? Yeah. Okay. Also, let's say you can solve perfectly the low-dimensional system. You got a solution. And what can you infer for the high-dimensional system? Like, is there a way to go back to the high-dimensional system? Back to the high-dimensional facility. Because otherwise, let's say your high-dimensional facility: 100 power plants and its state, and your low-dimensional system is maybe 10 of them. You've got a vector for the dimension of 10 eventually for the low-dimensional system. But you don't know the rest of the top, right? And you don't even know which 10 are. You don't even know which 10 among the hundred. Well, there's some loss of information, right? There, there's certainly some loss of information. Um, I mean, I don't know, it's not like an invertible thing where you can just kind of go back to the high. The goal is you construct the low-dimensional system so that it captures what the high-dimensional system is doing, right? You can think of, um, and there's many examples of this, right? You can, you know, you have systems that live in the high-dimensional space, but the attractor is. In the high-dimensional space, but the attractor is very low-dimensional. So, you know, maybe it's 100%. Maybe it's like you can add fake dimensions to system, but in reality, it only lives on three-dimensional space. So, you know, if I find the dynamics on that space, I can, I know what's happening outside. But in general, it's not like an invertible thing, I would say. There's some. I think in your example, it's not like you've got 100 power stations. You picked 10. It's like you're taking 100 power stations. It's like you're taking 100 power stations and then average, and you can set some 10, and then model those averages and set them 10. Yeah, and hopefully, they're not all so different that you can't do that. The averaging, the power plants maybe near each other behave the same. So you can do that. Okay. Thank you. Okay. State estimation. What's state estimation in one slide? Okay. This is the main idea. This is the main idea. It's not a very complicated idea. You have some true signal, which I don't have access to. I don't see the truth. And I have an observation. And I also have a model for what I think the true signal is doing. So the idea is maybe this is my initial condition, okay, and a T0. And I get an. T0 and I get an observation at T1. Okay, I have a model for the system or what I think is the system. I can forecast and predict what the model is doing at T1. But I see my observation is not where T1 is, right? So there's some error, there's some model error. And this is what this black line is representing. And the observations themselves are potentially, or it should have some noise. Or it should have some noise, right? If the observations didn't have any noise, then it would be the truth. So we want to combine the information from our model with the observations. And that's what's represented here. We want to take our prior information with the observation and obtain an estimate for the posterior. And this is all summarized in one step, right? So we want to find the posterior of the state given the observations. State given the observations. This is a standard application of Bayes formula. Okay. So this is really what state estimation is: how do I actually determine what the posterior is efficiently? So you hear things like the common filter, which is just a very efficient way of doing this exactly, right? And it's also sequential, right? That's just a common filter, I think. That's just a common filter. But if you look at the literature, that's not the only way of doing things. You can try to, depending on the nature of your state and observations, if it's non-linear and so forth, you get all these different variations of the common filter and so forth, right? But this is just the high-level idea. And hopefully, we can all at least work to solve the numerical problem. The numerical problem where it's filtering a one-dimensional signal so you can understand the benefits of data assimilation. Okay. Okay. So I'm going a little slower than intended, but that's okay. Forecasting and how do we quantify prediction skill? Okay, there are generally two types of forecasts that are important, long range. Important long range. You know, what we're interested in here is the equilibrium. How well are we capturing the statistical equilibrium? So this is information when the initial conditions have been forgotten. And sometimes it's also important to do short-range forecasts, right? So what's happening from initial state, right? What's kind of typically these are. Of typically, these are predictions within maybe the decorrelation time of a signal, right? So, depending on what you're interested in, there's two types of forecasting that are relevant. And how do I quantify prediction skill in light of these two is something you need to be mindful of. So I've just written down some general error metrics that are used to quantify skill in any numerical method. any numerical method. Pathwise measurements include RMS error. Okay, so I think this is the most famous or everyone uses this, especially in anything machine learning related. They only look at RMS errors, right? Which is not really representative fully of a signal, right? You can have errors that measure maybe the pattern correlation. So if two signals are closely aligned with each other, Are closely aligned with each other, you have a higher correlation, and that's not really captured by the RMS error. Okay, so you can have good RMS error, but bad pattern correlation. You can also look at the autocorrelation function. So this is how well the signals correlated in time. And so you might want your method to also do well, have close autocorrelation to the truth, right? So that would be important if you want to do a short-range forecast. Want to do a short-range forecast, right? You want to have good accuracy within the decorrelation time scale of the signal. And then finally, if you care about statistical accuracy, there are methods or metrics that measure information distance between distributions. So, the KL or the Kolbach-Libber divergence, which I've written here, is one way of doing that, not the only way. But there is some nice But there is some nice, I would say, information theoretical kind of perspectives on why this is good to use. But I wouldn't, you know, these are arbitrary. All of these error metrics are arbitrary. So I would also not think of, you know, just because you do well on some of these metrics that your method is better or worse. I really wouldn't. A lot of folks focus on that, but I wouldn't put too much weight on that because. Put too much weight on that because the accuracy is sensitive to the metric you choose, and it really depends on what you care about, whether you should weigh a method that knows well RMS or not. And if you care about extreme events, I would maybe not even be using the KL divergence, right? If you really want a method that can capture the tails, this is not that great, right? Is not that great, right? There's maybe something else that you can use that can really emphasize accuracy in the tails, okay? Um, so I'm just putting this up here for your information, okay. Um, now some references on these topics. So these are all actually Andy's books, and they're very good books. Um, so you can look at this slide, and uh, if you don't have these books, I can send them to you. I have PDF copies, or you can look them up in the library. Copies, or you can look them up in the library. So, I've also just written down which chapters to focus in on in these books, right? Some of them have good chapters, and you don't have to read these from start to end, right? You just want to read one chapter. A lot of them are pretty self-contained. So, and I have other paper references in the OneDrive, okay? Okay, so with that out of the way, that kind of introductory material, I want to now look at extreme events in At extreme events in prototypical nonlinear dynamical systems. And I want to talk about some of the representative features of extreme events in physical systems and look at low-dimensional prototype models for extreme events. And this is very, I think, illuminated to actually see, you know, in an equation, how things grow and die. Things grow and die. So I'm a big fan of these kind of conceptual models. Okay, so extreme and rare events. There is some sort of distinction between the two, which I'm not really going to focus in on. You know, rare, extreme. You know, if you want to get into the subtleties and the math, you know, you can do. In the math, you can do that, but I'm just going to be concerned about the general idea of something that is maybe larger than, a lot larger than the typical variance of the signal. And the usual way we quantify extreme events or understand that the signal has some elements of an extreme event is by looking at the probability distribution function of some observable that we care about. And we saw this, we talked about these types of distributions. These types of distributions a lot. But we typically have signals that have some sort of exponential or heavier than exponential, or maybe slightly lighter than exponential tails, right? So these are distributions on a log plot in the Y direction. I think this is CO2 anomalies, I think, in an atmospheric column. And you see exponential tails. And this is a Gaussian fit to the data. And this is a Gaussian fit to the data. So if I took the data and I assumed it's a Gaussian, this is what I would get. In reality, heavily non-Gaussian. And non-Gaussianity is very, I would say, it's pretty ubiquitous in a lot of systems, right? A lot of systems are non-linear. And so maybe under certain conditions, you expect Gaussian dynamics. But more often than not, you'd expect some non- Not you'd expect some non-Gaussianity, whether it's large or small, it depends. But it's, I would say, more typical to have elements of non-Gaussianity or systems that are weakly non-Gaussian when you're looking at non-linear systems. That's a quite radical. Yes, yes. And then if it's linear, it does heavier too. Yeah, exponential. Yep. Yeah, this is this is a quadratic here. So, you know, intermittency and these PDFs are very important for that. So here I just have a simulation of an oscillator where I've triggered an instability. And so you see the signal looks fairly predictable. Fairly predictable, or the variations are within a certain bound, and there's some sort of instability that's triggered, right? And if I look at all the data and I look at a histogram of that data, I would obtain a distribution that looks as follows, right? Again, this is semi-log. And so you see the deviations of these instabilities is a lot more frequent than a Gaussian would predict, right? The probability of a four-sigma event. It's not even on this graph here, where you can see for the actual system, you know, it's 0.02, right? So this is a lot more frequent than your Gaussian prediction would tell you. Very important, right? Very, very important, right? A lot of, I would say, errors have been made in sort of risk analysis, making wrong assumptions. Making wrong assumptions. But it's understandable why people make those approximations, right? It's not like they're trying to do a bad job, it's because it's very challenging to develop any sort of analytical approximations for these non-Gaussian systems. So, you know, it's, you know, people, you know, why are they pricing derivatives with black school models? Well, I mean, it's, it's the alternative is doing numerical simulations, and most people don't like that, right? Um, or it's it's not um. Or it's not an analytical formula, it's always preferred. It's always preferred. And this is an example of something that is not as extreme, right? So there's some deviations. And I think this system is maybe not unstable, but it's like weakly nonlinear. Yeah. I don't think there was instabilities in the system, but so the tails are just, they're closer to the Gaussian. They're closer to the Gaussian, right? So there's just non-linearity in the system, but it's small. And so you have some small deviations from a Gaussian. So perhaps, depending on your application, using a Gaussian is an appropriate approximation here. How do you tell that system is the value that from the plot? I know because I simulated these. That's how I know. Yeah. But if they were linear, would they have Gaussian? Well, if it's non-multiplicative noise, yes. But if it's multiplicative noise, no. Or non-Brownian motion. Or non-Browning motion, exactly. Yeah. Or non-Markovian. So it's not enough. No, it's not enough, I would say. Um, so I kind of already talked about this, so maybe I'll skip this. Um, but the point was that, you know, in the original systems, these live in a high-dimensional space. These live in a high-dimensional space. Okay, so maybe we will now look at a very simple model and see how by simple additions or complications to this model, we can get non-Gaussian features. So, um, this is, and we'll look at this. This is, and we'll look at this at the end, probably tomorrow, actually, where we look at a toolkit for STEs. But I'm just going to write down the equations and maybe don't worry about too much about how we got them. So we have this model here. We have some light noise here. A is linear, so it's gamma x, and b is a constant. Okay. Very simple system. We can exactly integrate this, and we can find the This and we can find the exact equations for the mean and the variance, and also the equilibrium density, which is Gaussian. Okay, this is completely Gaussian. How do I make this non-Gaussian? One way would be to introduce multiplicative noise. So here I just added a noise term that depends on the value of the signal itself. So this is maybe one of the simplest non- This is maybe one of the simplest non-Gaussian models you can propose just by using multiplicative notes, right? So the larger, you know, if this is time, this is U, the larger the value that U is, the larger the strength of this noise is. So that kicks it to be to deviate further away from the origin. So you can see here a simulation of So, you can see here a simulation of this signal. And because of this multiplicative noise, we have non-Gaussianity. And this system is actually simple enough, which we'll probably revisit, where you can solve it analytically. And so you can exactly numerically solve this and collect statistics, right? But it's log normal. So it is. So normal in the sense that if you just take a lot of view. It's linear. Yep. But I don't think I said. Yeah. Yeah. Yeah. So through this multiplicative noise, we got non-Gaussianity. I guess that's my point here. Still linear, if that's, I think, what your point is, right? It's still. I think what your point is, right? It's still, I guess, the main dynamics are linear. If I drop the excitation, the dynamics are linear. But depending on the structure of the noise, which was, I think, the comet here, you can still have non-Gaussianity. The system, I guess, can still be linear, but depending on the structure of the noise, you can have non-linear distributions. Quite non-trivial non-Gaussianity if you allow multiplicative and additive terms to be. Multiplicative and additive terms to be correlated with each other, yes, that creates this family of the homics. Um, which is this slide, right? Um, which is this slide. So you have here both additive and multiplicative noise, okay? Um, and you have nonlinearity here, okay. Now you have nonlinearity here. Nonlinearity here. This system has been used, I think, maybe as just explanation at a high level of variability in atmosphere in certain contexts. And I will maybe talk a little bit more about this in the next slides. But before that, this system is still pretty straightforward because I could represent this as a gradient. As a gradient. So I have some potential energy, right? The gradient of a potential plus a noise structure. And this is the potential function for this system. So depending on the parameters here, the potential is, it can vary from a double well to single well. But this is really just Something like this in the most general form, right? That's the potential function, right? It's x to the power 4, so you have two l's. And for this system, we can also actually solve for the equilibrium state. And let me just be clear here. For this one, I've actually just dropped the multiplicative noise. I've actually just dropped the multiple bit of noise, okay, just to keep it simple. But I've just written down this. And so the PDF has this structure. So I think later on we'll reintroduce the multiplicative noise and look at that. But for now, we're just looking at a system with this type of density. And so this density has this kind of bimodal Gaussian structure. And so you have kind of switching potentially depending. Potentially, depending on the parameters between the two equilibria, right? This is there's two stable equilibria. And so, depending on the intensity of the noise, you can have switching from this well. You know, the system can kind of wiggle around on this well and it can sporadically, or this is really the main, one of the main examples of kind of like rare events in maybe the traditional sense where you have a small amount of. Where you have a small amount of noise that can kick the system from one equilibria to like the other equilibria, right? This is like, I think, one of the classic examples used in that kind of line of work, right? And so we have a non-Gaussian distribution here, okay? But it's not, I would say, heavy-tailed, right? Heavy-tailed, right? It decays kind of with the power of four, right? But then again, there's that forcing term here, which can kind of make it, which introduces some skewness. So there's some subtlety there. So here is just some typical shapes of that PDF. And so you have this kind of, f is zero, it's completely symmetric. And so you kind of have characteristics of like this kind of two Gauss. Of like these kind of two Gaussians, right? But if f is non-zero, you introduce skewness in the system, okay, in the PDF, rather. Was there a question? You can go back first. Okay, so in the last page, is F the solution trajectory? F is a parameter, it's the forcing. It's the forcing. I guess I'm trying to connect between the solution trajectory and the model that you gave. Which slide are you referring to? This one. This slide, okay. You're trying to connect from the solution trajectory. From the first equation to the second. Yes, I'll do this. Yeah, I just dropped this term. This is, I just dropped this term. This is this. Okay, if I take the derivative of this, I get this, right? So minus. This right, so minus fx derivative with the minus sign gives me f and so forth. Okay, sorry, which is the PDF? The PDF is this, of this. This is the PDF of this. You mean the steady station? Steady equilibria PDF. Yeah. Stationary distribution. No, it's okay. So you already got. Okay. And maybe I will have a few minutes left. Okay. So I will also talk about this system, which is something I spent a lot of time working on to develop an analytical approximation of the PDF for this system. Okay. So this is, I would say, maybe one of the simplest. Of the simplest systems you can write down where you have an intermittent instability, okay. Um, and in this system here, I have again noise and I have alpha, okay? And so alpha, in general, when we look at systems like this, like in the Langevin equation, it's a constant. And the constant is typically positive if I care about systems that do have a steady state. Um, so in more general systems, this alpha, um, and especially if you relate it to the original equations that we looked at earlier on, this alpha can capture all the different interactions from the different modes. So, you can kind of think of a system as a very simple reduction of that, where you know, we just look at a one mode, one K. 1k and just kind of lump all those interactions from the different mode into a signal called alpha. Okay, and so this signal here is stochastic. We're not going to model all the interactions. We're just going to assume it's parametrized by some sort of stochastic process with whatever characteristics you want. But what I want to highlight here is instabilities in the system. Instabilities in the system can arise from a very simple mechanism, right? So, as I mentioned, alpha in general is positive for most of the systems that you look at in the Langevin equation. But if alpha is negative, right, so if alpha is negative and let's say I forget about this noise here, I just have an exponentially growing signal, right? And so that's the basic mechanism of the growth. Mechanism of the growth. But alpha is stochastic, so it's not always negative. So we have this switching phenomena from regimes where alpha is positive to regimes where alpha sporadically is negative for a short duration of time, and that triggers growth in the variable u. And so this is exactly what we see here. I have some random signal alpha, and it kind of enters this regime where it's less than zero. And this triggers a large. Zero and this triggers a large growth in the U response. Okay. Very simple mechanism of an intermittent instability. Okay. So what's the role of sigma here? Sorry? What's the role of sigma? The role of sigma is triggering these instabilities, right? Oh, sorry, sigma. You said sigma. I'm sorry. I misunderstood the question. I misunderstood the question. Sigma is noise. It's just white noise. This is W dot here. Many. Alpha is random. Yeah. If you take sigma zero, you observe the same thing. If you take sigma zero, would you observe the same thing? Yeah. You need, you need, well, if sigma is zero, then basically this signal, if it's positive, would decay to zero. U would decay to zero. You would decay to zero. You need some small amount of noise to kick the system. So there is some sort of background equilibria where there's enough energy to then, when alpha switches regime to kick it. So if sigma is zero, you don't really see this, right? Even if alpha is 1. Yeah. Yeah. Because we're also assuming alpha starts from positive. So you need the noise there. You need the noise there. Okay. I'm out of time. Okay, I'm out of time for today. So maybe I will continue tomorrow, and maybe I should speed up when I give my second lecture. But hopefully this was interesting. And then maybe some of the problem sets will be a little more complicated since we didn't get to the later half of the lecture, but we'll still try to make progress. Okay, thank you. Questions I can check the audience. I have some. Yeah, could you go back to the KL divergence page? You said P and Q are PDs and the QL divergence is assessing the two PDFs. But the PL diagram is, I learned. divergence uh i learned is regarding two different like probability measure yeah yeah like i'm just wondering how well one one p would be the truth and the other p would be the model that you have the imperfect model uh so you're measuring sorry for for different dependencies for different yeah i assume let's say p is height measure q is lower Well, P are a scalar. I mean, P is a P is a scalar distribution potentially, right? Or I mean P is a density. I'm just assuming things are smooth enough so that everything has densities. Oh, so PDF is both distributed. Distribution. Yeah, not enough. It's partially differentiated. Sorry, sorry, sorry. Two PDFs, P and Q. Not PDEs. Yeah, that's an easy. Okay. Maybe I should. I don't know what else I could use for PDF. It's my event. Sure. Yeah. So, can you go back to that last example with the output? So here, output is Here, alpha itself can have Gaussian statistics, but UT will have non-Gaussian statistics. Yes, yeah, alpha here is Gaussian, actually. The simulation is alpha, is a Gaussian process. That's how I actually made this signal. And it's important actually that it could be like white noise, but it's far more realistic that it is a signal with. A signal with some finite correlation, time correlation, so that when it does become negative, it doesn't instantaneously just become negative. It lives there for some period of time. And you need that period of time to make sure that this U actually grows, right? And that's actually a lot more realistic because if this was the original U and alpha was all the different modes. Different modes interacting, that would not be white noise, anyways. So it's the more realistic case. It would be a nice exercise to plot the simulate the system, plot the distribution of EUT and look at the tails. Yeah, I have, yeah, you can. We have those simulate, we have all those. I didn't put them here. Probably my mistake. I should have put the distribution of u in alpha, but. But yeah, so well, yeah, that's an easy, yeah, just simulate this, but then you'd have to do alpha. You'd have to simulate alpha, which is doable, but maybe you'd have to, I'd have to like give a lot more information on how to simulate. It's possible, yeah, it's not too difficult. No, you're right. So that is indeed a good exercise. Yeah. You have some thoughts of the PE apps. Why is it wiggling a bit on the tail part? Lack of data. We don't have as much data here. So that's why it kind of wiggles away. Here it's well resolved, but remember, these don't happen that frequently. So when you take the histogram, there's some very There's some variability associated with that. In your last slide, on the reduced order modeling earlier on, and you're talking about the replacement of that with UF by leader by Gaussian nights. So it's thinking that in order to make sure that you're maintaining that energy conservation properly, that the diffusion matrix that goes to you noise. The diffusion matrix for that constituted noise is going to be a really important, but you're expecting the noise in the right direction. Yeah, you have to match the statistics. Yeah, if you want to do it correctly, it's not just any noise, right? You have to make sure, yeah, if you really want to do it correctly, you match the damping and the noise to have the same statistics as the truth. That would be how you do it statistically accurately. Yes, exactly. That's a good point. Yeah. Okay, let me just check if there's questions in the audience. Check if there's questions in the audience or in the online chat, and there aren't. So, okay. Thank you all.