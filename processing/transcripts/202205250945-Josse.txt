Clinical care clinicians who take care of trauma patients. So trauma patients are patients who have issues with a car accident, fall injuries with gun or white weapon, and often they suffer from hemorrhagic shock or head trauma and they require specific care. So the idea of the clinicians is to try to exploit an observational data set to try to improve Data set to try to improve the care of the patients. So we have access to this data set where we have more than 30,000 patients described by 250 variables that can be quantitative, such as the age or categorical, such as the kind of accident. And on this data, there are different challenges. So, for instance, we may want to assess the impact of the administration of a treatment on Administration of a treatment on survival, given the fact that we have observational data. But today, I will focus on a predictive aim. So, for instance, given pre-hospital variables, I would like to predict the levels of placelet. So, from a statistical point of view, it boils down to performing a multiple linear regression or, for instance, a random forest. A random for us, if I want to use non-parametric methods. So I would like to emphasize that this is really a project and that we are in the process of testing the models that we have implemented in real time. So it means that really we will have an application in the phone in the ambulance to try to predict some events. But the issue is. But the issue is that the establishment of even simple models is made complicated by the fact that there are many missing values. So here, on this slide, I have represented a bar plot of the percentage of missing values for a subset of variables. And you can see that it varies a lot. So from 0% for some variables to nearly 90% for Nearly 90% for other variables. And in addition, we have different codes for the missing values. So there are different colours. You can see not in form, not made, not applicable. So if I come back to my data set, I have this NM, IMP, NR. And it's really because, in fact, the medical doctors have encoded the different kind of missing values. So for instance, NM for not made, it is simply because the measure was not. Because the measure was not made, and when it is not recorded, it is because of time. For instance, they have not recorded the measure, they have not filled out the form. But some missing values may be informative. For instance, when we have for impossible, so impossible, it was because the state of the patient was such that it was impossible to make the measurement. So, for instance, Make the measurement. So, for instance, in an emergency setting, it was too extreme and dangerous, so they have not done some measurements. So, there is an abundant literature on how to handle missing values, and here I consider missing values in the covariates. And in particular, with colleagues, we have created a website named RMISTATIC. So, you can find the link on my website, for instance. Website, for instance, where we try to summarize all the work. So it means to put bibliography, to put pipelines of analysis, to put all the different implementations and so on, just to try to give the users a key to enter the field and to know how to handle their missing values. And it turns out that there are more than 150 packages because, of course, there is not only Because, of course, there is not only a generic solution to handle the missing values. So, often when someone has an issue of missing values, such as this one, I want to perform a linear regression despite the fact that I have missing values in my covariates, I suggest them either to modify their estimation process so that it can be applied on an incomplete data set. So, for instance, if I consider estimation. If I consider estimation by maximum likelihood, I can use expectation maximization algorithm or with, for instance, supplemented expectation maximization algorithm to get point estimates and an estimation of the variance of my parameters of my regression despite the missing values. So, for me, this approach is perfectly tailored toward a specific problem. So, if I really want to do my regression or my logistic regression with missing values, I would. Myologistic regression with missing values, this is appropriate. But the drawbacks are that you need to design a specific algorithm for each statistical method that you want to apply. And on a data set, you are not going to do only a logistic regression. So, for instance, you may want to use dimensionality reductions to visualize and summarize your data. And consequently, you need to design a dimensionality reduction method that can. A dimensionality reduction method that can handle missing values. If you want to do clustering, the same thing. So that's why it's not very convenient. And in addition, it's also not really so easy to establish, even for simple models. And there are not many softwares implementing this method. So, in fact, with colleagues, we have recently implemented a logistic regression with missing values using a stochastic approximation because it was. Approximation OM because it was not available. So that's why people often prefer to resort to imputation to get a completed data set on which they can perform any statistical or machine learning algorithm. And you can use multiple imputation to get the appropriate variance that takes into account the supplement variability due to missing values. Missing values. So the advantages are it is kind of generic, so you can perform your logistic regression, your regression, your clustering, and so on. And in addition, there is a powerful, for instance, mice package, which is available and that you can use. The drawbacks is that maybe it's generic, so it's not really tailored toward your specific problem, and you may also encounter computational. Also, encounter computational issues, especially in large dimensions for your variables. Often I suggest these two approaches. So, I do not speak about weighting strategies, mainly because I consider the case where I have many missing values in the weight. But these two strategies are dedicated to an inferential aim. It means that their aim is really to estimate parameters and their variance. estimate parameters and their variance despite the fact that I have given the fact that I have missing values. So the aim is not really a supervised learning framework. And you will perform this under the assumption for the missing value mechanism that we will recall, the classical missing completely at random, missing at random and a missing non-at-random. So it turns out that even there is a huge litter hatcher. That even there is a huge literature, there are few works that are dedicated to supervised learning with missing values, which is the aim of the talk today. Few, no theoretical results, whatever, the missing value mechanism. So it means that here my aim is not to estimate the beta parameter for my regression. My aim is really to predict as well as possible the outcome given the incomplete covariance. So, to address this topic of supervised learning, Topic of superized learning with missing values. I will use the following notation. So I will consider the covariates, the complete covariates that are unavailable. I will denote them as X. And my data with missing values are denoted with X tilde. And you can see that X tilde can be seen as a mix between continuous and categorical data. So it means that when you have missing values, So it means that when you have missing values, your data does not live in a vectorial space, but rather in a semi-discrete space. And that's what makes many learning procedures complicated because your data are not in a vectorial space. Then we will denote M, the mask, so the indicator matrix coding for missing values, where zero when it is observed and one when it is missing. So NA it is for not. So, NA it is for not available, and we will denote ops and miss the indices of the observed and respectively missing entries. So, with these notations, missing completely at random can be written as follows. So, the probability to have missing values does not depend on anything, whereas for missing at random values, the probability to have missing values depends on the observed values. So, when we speak Values. So, when we speak about supervised learning with missing values, our aim is to find a prediction function that minimizes the expected risk. And the best prediction function is known as the Bayes rule, the Bayes predictor. But the difficulty here is that your function takes as an input r tilde, x tilde, your data with missing values. And consequently, Values. And consequently, if you rewrite your base predictor as conditional expectation of y given x tilde, so here I consider a square loss. In fact, it is a bit more complicated than mid-hours because this boils down to performing one model per pattern of missing values. So in my example, here I have two patterns of missing values. Patterns of missing values 0, 1, 0 and 1, 0, 0 over the eight possible patterns of missing values. So imagine we have 50 features, then we will have two power 50 features. So we can see that we have combinatorial issues. Okay, so in practice, we have access to a training test and Training test and a test set, and consequently, we will proceed to empirical risk minimization and we will estimate the generalization error using the test set. So, I will use the definition of Bayes consistent, the classical definition, to say that my learner is Bayes consistent if, given an infinite amount of data, it achieves the Bayes height. So, the best possible error that you can make. Possible error that you can make on your problem. So, what are the differences with the classical iterators? As I previously said here, my aim is really to predict as well as possible. I don't care about estimating my parameters. And in addition, we have a training set and a test set. We have these two sets, and both sets, in fact, have missing values. Because if you have a test set which may arise in practice, Arise in practice without missing values. In fact, you will have issues of distributional shift because don't forget that your data generating process is the generation of your X and your Y, and you have also the missing value mechanism. So with the colleagues, our aim was really to say, okay, is it possible to use all the available work like expectation, maximization, algorithm, multiple imputation, impetition in this setting? Are they consistent in the sense of? Consistent in the sense of base consistent, or do we need to design new approaches? So today I will focus mainly on studying the imputation methods in the framework of supervised learning, and in particular the case where we do imputation and then prediction, because this is the most common approach that you can find in practice. So often people In practice. So often people just use off-the-shelf methods. So first they are going to impute the missing values, and then on the completed data, they are going to use their preferred supervised learning algorithm. And because it's simple, or maybe because they don't know multiple imputation and so on, they use single imputation. So we single impute and then we predict. And there are different ways to do this depending on the way. On the way you use your training set and your test set. But basically, the most plausible or plausible, the most recommended approach is to impute your train and your test set with the same imputation model. So what does it mean? Imagine that I'm going to impute by the means. So I'm going to compute the means of the variable on the observed data. On the observed data on my train set, so for each colon of my train set, and then I will impute the test set with the same means. So, this approach seems plausible, but and you can see, and then on my I will apply my learner on the imputed train set, and then I will then apply it on the imputed test set to predict. So, this approach is very easy. So, this approach is very easy if you use univariate imputation. But it turns out that many of the shelf single imputation methods are really black box. Like if you give an incomplete data set, it outputs a completed data set and you don't have access necessarily to the underlying imputation model. So that's why in practice people really prefer to use I impute by the mean and then I do prediction. And then I do prediction. But we know that mean imputation has a lot of drawbacks. So here I will recall the drawbacks using a simulated toy example where I generate a bivariate data from a Gaussian distribution with a positive structure of correlation. So the correlation coefficient is equal to 0.6 on this data. Then I introduce 70% of missing values completely at round. Values completely at random on the second variable. And then I will impute by the mean. So it corresponds here to the red point. So it's not x and y, it's x1 and x2. And then on my completed data set, I will estimate some quantities. So for instance, I will estimate the mean of x2, the standard deviation of x2, and the correlation coefficient between x1 and x2. And as we And X2. And as we can see, the mean is perfectly preserved, so it is estimated without bias in comparison to the population mean, but the standard deviation is too low, which makes sense because we impute with the same means. And in addition, we have also destroyed the correlation structure between the two variables. So that's why we say that mean imputation differs. Say that mean imputation deforms joint and marginal distribution, and that's why I have spent my life saying to everyone: Please, if you have one thing to keep in mind, is that do not impute with the mean because it is very dangerous. So, this is a toy example. But if you consider a larger data set, for instance, here it was a data set where I have 70,000 species of plants described by six ecological trees. By six ecological traits. We had a lot of missing values because it was an aggregation of data from different countries. And on the top, I have represented the results of a principal component analysis applied on the data where we have imputed the missing values with the mean. And on the bottom, a principal component analysis that is dedicated to handle missing values using an expectation maximization algorithm. And the results are very different. I don't know if you are used to reading these plots. Know if you are used to reading these plots, but basically, the angle between two arrows represents the estimation of the correlation coefficient. So, on the top, the angle between the two traits, P mass and R mass, is 90 degrees. The estimated correlation coefficient is close to zero, whereas on the bottom, the two rows are collinear, so the estimated correlation coefficient is around one. So, it means that it's completely different. So, it means that it's completely different, and the interpretation will be completely different. So, that's why imputing by the mean is really dangerous. So, this is exactly the same thing as what we have seen before, but the phenomena is exacerbated because we have a large data set. But it turns out that in the this is true because you are in an inferential framework, so we estimate parameters like the mean, the variance, the correlation. But if your aim is to do prediction, If your aim is to do prediction, in fact, constant imputation can be consistent for prediction. So, this is the first results that we have shown. This is under the following framework. So, we assume a classical model where my outcome is generated as a generic function of the covariates plus some noise. First, we have assumed that the missing values were missing at random, and the theorem was proof only for one variable to start. Variables to start. So, for instance, the probability to have missing values on x1 depends only on absurd values for x2 and xd. And we can show that constant imputation is consistent. So, here, for instance, I will denote x1 prime the vector where I have x1 when I don't have missing values, and I replace I replace the missing values by the constant alpha when it is missing, and we can show that prediction with constant or meaning is equal to the base function almost everywhere. So, basically, the formulae here just shows that you have three different cases. Either the probability to have missing values is equal to one, and you impute uh the value by the constant. Value by the constant alpha, or the probability to have missing values is equal to one is equal to zero. Sorry, if you don't have missing values, but by chance, in fact, the first entry of your vector is equal to alpha, or you don't have missing values. So that's why we have almost everywhere a consistency, and we can have pointwise consistency, for instance, if we use a constant which is out of range of the distribution. So how do we read this theorem? Read this theorem, it really reads as follows. We say that if we learn on the mean-imputed training data, if we impute the test set with the same mean, so it has to be the same mean or the same constants, and we predict, then this is optimal in our missing out random framework if we use a universally consistent learner. So it means that, for instance, I use a random fores and I have an infinite amount of data. Infinite amount of data, then mean impudation or constant imputation can be consistent. So I can illustrate the rationale of the theorem as follow. So it's really like you consider your missing values as a specific values. It's really like a systematic code for the missing values. Like if you have a categorical variables, it's like a new category to encode that you have missing values. And it turns out that the learner is going to be able to Is going to be able to detect your code at the train time. So, here I have my data imputed by the mean at the train time. And when he is going to look at the test time, he is going to recognize the code. So, that's why it's really important to use the same constant. So, you can use any constant, but maybe if you use an out-of-range constant. But this result is an asymptotic one, so you need a lot of data. One, so you need a lot of data and a super powerful learner, but still, I found it interesting because it is kind of justifies a practice that people use in practice. And in fact, it can be a good strategy to do this. So mean imputation is not bad for prediction, it is consistent despite its drawback for estimations. In a way, it's not so astonishing to have a different To have a different behavior in an inferential framework and in a predictive framework, but still it's interesting to justify practice. And it turns out that we have extended these results to not only to a constant imputation, but to almost any imputation procedure. So, in particular, Procedure. So, in particular, I will define what I call the impute then-regress procedure as a function of the form. So, g is a learner, phi is the imputation, and I will consider the imputation of the which are function c infinite, so infinitely differentiable. So, it means that I apply a learner G on an imputed data set. So, phi is. Data set. So phi here is a deterministic imputation. So it means that it will be a function of my observed value. So if I impute by the mean, this is a continuous imputation. If I impute by regression, this is a continuous imputation. But if I impute by stochastic regression, this will not work. And the theorem is the following ones. We have still our generic regression model with F, which can F star, which can be anything, and we are going to denote G. Are going to denote g star phi, the minimizer of the risk on the data imputed by phi. Then we can show that for all missing data mechanism and almost all imputation functions, this star around phi is base optimal. So it's generalized the precedent results because the precedent results was only for missing out random values. Here it is for Here it is for missing at random, missing volunteer at random, missing another at random, and it is for almost all any imputation function. So it means that, in fact, a universally consistent algorithm of trained on an imputed data set is consistent. So asymptotically, it means that you don't need to impute well to predict well. In fact, you can impute with your constant and this is okay. So I will try to illustrate the rationale of Try to illustrate the rationale of the proof of this theorem, which is generic. And the proof, in fact, is based on arguments from differential topology. So the rationale is really that imputation is going to create many faults to which the learner adapts. So the first part of the proof shows that in fact all the data points with the missing data pattern M are With a missing data pattern m are mapped to a manifold of dimension, the number of observed values. So, for instance, on my example, I have a complete data set with x1, x2, and x3, and on the right, I have an imputed data set. So, for instance, if you have only missing values on x1, then you are going to impute, and it will create a manifold of dimension one. So, same thing if you have missing values on x2 and on x3, and that's why we have this. And that's why we have these three kinds of lines in red, in brown, and in purple. And when we have two missing values over the three variables, we will have many folds into these. So the orange one, the green one, and the blue one. And then the argument is that the missing data pattern, sorry, the missing data pattern of imputed data points can almost Pattern of imputed data points can almost surely be de-identified. So it means that if you give me a data point from the right, I am able to determine the missing data pattern. So given any points imputed, you are able to determine the missing data patterns. And the arguments come from what is called the term transversality theorem. Theorem and something which is called the pairs-wise transversality of your manifolds. So, in fact, that's why the result is valid for almost all imputation, continuous imputation. It's because if you have a case where too many falls will be superimposed, then in fact you are not going to be perways transversed and consequently you are not going to be consistent. And then finally, given to, in fact, we can Given to, in fact, we can build a prediction functions that are independent of the pattern because they will work on the imputed data set and that will be base optimal. So, this gives a bit the rationale of this generic theorem. So, the theorem basically says that, okay, you don't need to impute well to predict well, so I can still use my constant imputation. I don't need to use a sophisticated To use a sophisticated imputation. But we can see by looking a bit at this previous plot that maybe there are some imputations that will make the prediction problem easier. And we can also see this if you look also at the sample complexities of the different implications. So here I have represented, I have different data generating processes on these three. Data generating process on these three plots. And I have different techniques. So, for instance, I have imputed by the mean and then I have used random forest, like my consistent universal learner, or I have imputed by assuming a Gaussian distribution of the data with a specific covariance matrix and imputed by using conditional expectation in a red. And I have also other techniques. And basically, you can see that when the sample size increases, in fact, I will In fact, I will reach the base rate, and the base rate is represented in purple with my mean imputation. But still, for a fixed sample size, it seems that imputation assuming a Gaussian distribution can give better predictions. So, for a finite sample, I may have also better results with another imputation. So, this is the thing that we have studied after: is that, in fact, okay, I have the choice of an imputation. So, which imputation do I need to choose on to kind of which will be the best one? Which imputation will be the best one? Which learner? So, I have two things that I can play with. So, how can I do? And the first thing that we And the first thing that we have studied is that, okay, maybe you can do chaining of oracles. So imagine I have access to the true underlying regression function f star. I will apply f star on the data that are imputed by the conditional imputation, because the conditional imputation expectation of what is missing given what is observed can be seen as the best imputation that you can do in order to minimize. Can do in order to minimize the reconstruction of your data. So it can be seen as a chaining oracle. And in fact, this approach is not base consistent, except if you are in the case of the linear regression. So if you impute biconditional expectation and then you have a linear regression model, then this is okay. But otherwise, it is not this consistent and we can characterize the excess. Can characterize the excess of risk of this approach in comparison to what I've called R-star, which is the risk of the base predictor. And to characterize this excess of risk, so to give an upper bound, we assume that we have positive semi-definite matrices H plus and H minus that bound the H of S star, so the curve. Of S, so the curvature of your regression function as follows. And given these assumptions, your excess of risk is bounded by the following quantity. And the following quantity depends on these matrices H and on the covariance of the missing values given the observed data. So basically, it means that you will have a high excess of risk when the curvature of When the curvature of S is high and when the variance of the missing data given the observed data is high. So it means that if you have a function which varies a lot and it is super complicated to impute, then if you change oracles, you are going to be very far from the base predictor. But if you have a flat function and imputation is easy, then you will be super close to the base predictor. So this is if you chain oracles. Chain oracles. And then we can say, okay, now I'm just going to consider that I fix the imputation. So I impute by conditional imputation, conditional expectation, because this is oracle. This is the best imputation that I can do. And I can wonder: okay, is there a continuous function g such as g applied to this conditional imputation? Is base optimal? So you learn on a conditional. So, you learn on a conditional imputed data set. Impute as well as possible before learning. And why do you want a continuous function? It is just because it will make the problems easier, because it's easier to approximate continuous function. And in fact, the answer is no. In fact, you have no continuous function, so it will be discontinuous, so more complicated. But the point is that the size of the discontinuities are also controlled by this variance curvature trend form. Your trade form. And then you can continue and you can say, okay, maybe now I will keep the regression function as f star and I will look for a continuous imputation so that it is easy. And I'm wondering if this could be a base optimal. And here the answer is someone yes, someone no. So the point is that in fact if you choose an oracle for one step, either imputation or regression, it will impose discontinuity on the other steps. On the other steps. So it means that the problem is going to be much harder to learn. So that's why all these results suggest, in fact, that if you want to do a prediction, despite the fact that you have missing values in the cover yet, the best thing to do is to do a joint learning of imputation and prediction. In fact, it's kind of designing the imputation to your prediction probability. Imputation to your prediction problem. You are going to learn bus things. I'm just wondering how many times do I have? About 10 minutes? Perfect. This is perfect. Okay. And so to jointly learn the imputation and the prediction, in fact, I will use what we call a NOMIS neural network. And in fact, this NOMI. Network and in fact, this numis neural network has a rationale which comes from doing linear regression with missing values. So now what we have seen is like, okay, I can impute with anything and then I can do prediction. It will be a base consistent for almost any imputation function and almost all the mechanism. And we have said that, okay, but still, in fact, you can. In fact, it can be better to kind of jointly learn the imputation so that it is dedicated to your prediction functions. And so, to do this, I will use this NUMIS. But in fact, the NUMIS network is motivated by linear regression with missing values. I'm just going to say that in five minutes. So, I assume here a linear model. Before I was in a non-linear generic model, so I assume a linear model, and under the linear model and under this linear model you have an explicit expression for your base predictor conditional expectation of y given x tilde so you can write it with respect to the upsterm and the miss term and in the framework of linear regression if you are willing to make assumptions on the covariates so there are different assumptions you can have an explicit form for this base predictor because you can explicit this expression in red Expression in REN. So you can have an explicit expression. So, for instance, if I consider that I have a Gaussian assumption for my covariance and missing at random values, I can write the base predictor as follows, which is represented in this orange box. So, this expression is very simple because basically it boils down to performing a linear regression model per pattern. Regression model per pattern of missing values. So, this just means that if you have a pattern where x1 and x2 are observed, you are going to do the regression of y on x1, x2. If you have a pattern with x1 and x3 observed, you are going to do the regression of y on x1, x3, and so on. So, under this specific assumption for the Gaussian covariates, it boils down to performing a linear regression per pattern of missing values. This is not always the case. Values. This is not always the case. So, my point is that even with the linear regression model, in fact, you may have very complicated things when you have missing values depending on the structure of the X, the covariates, and the missing value mechanism. Here it is simple, but still, if I have D covariates, I have two Poor D models. So, that's why you may want to look for an approximation of your base predictor to handle the complexity of. To handle the complexity of your data. And that's where comes this NUMIS network. So basically, when you look at the base predictor, what is complicated is that here it is, I will come back here. Is that you have to invert as many covariance matrices as there are of As there are patterns of missing values, because sigma ops, so this is the covariance matrix for the pattern ops. And so you will have as many covariance matrix as you have patterns of missing values. So this is complicated and there are not so many links to relay inverses of covariance matrix. So that's why we have used an approximation which is defined as follows. So the order L approximation of the covariance L approximation of the covariance matrices is defined by the recursive scheme S order L is equal to identity minus sigma ops S order L minus one plus identity. And in fact, this comes from the Neumann series. So Neumann series, if you consider the first matrix L zero as identity, if if you consider an order L uh L equal to infini infinity. equal to infinity infinity, you will have this expression which relates the inverse of a covariance matrix and this series. So basically this is just a tromutation at order L of the expression of the inverse of the covariance matrix. And this numist neural network is based on this approximation. So it is a neural network It is a neural network architecture that tries to mimic as well as possible the base predictor. So your aim is to target the base predictor. And so we have designed this architecture. So this starts as follows. So in my base predictor, I have x ops minus u ops. And in fact, this is x ops minus ops. Then I will multiply by s zero here and then I will apply the approximation like s0 times i don't like S0 times identity minus sigma ops. Then I will add plus identity and I will continue because I have an order L approximation. So I will multiply it by identity minus sigma ups plus identity and then I will multiply it by sigma miss ups. So the point is that really you design an architecture to target the base predictor of a linear regression with Regression with missing values, and in fact, approximate the conditional expectation of X mass given X tilde. And from a technical point of view, implementing a network where you have weights that are masked differently for each sample, because it depends on the pattern of missing values, can be complicated. But it turns out that you can show that mask weight is equivalent to masking input and output. So at the end of the day, Output. So at the end of the day, in fact, what you do is a classical neuroton work, but you introduce new non-linearities which are multiplication by the mask. So for me, it was an important point to say that there are many attempts to do neural network with missing values, but this one is theoretically sounded and we have a new non-linearity which is inspired by the Bayes predictor. And why I have spoken about And why I have spoken about this NUMIS network is that with this NUMIS block, then you can add any things. For instance, you can add a multi-layer perceptron, a classical network to predict your outcome. So it means that if you just chain an MLP in your NOMAN network, what you are going to do is to try to jointly learn the prediction and the imputation to try to explain the outcome as well as possible. Explain the outcome as well as possible. So, very quickly, we have done a simulation. The simulations are in agreement with the theory. So, we have different settings for the friction function. So, a setting where we have a wave, a setting where it is easy, we have different structure of correlation between the variables, low correlation or high correlation. And this is done, so I will just show you two results. So, the first one is that. Results. So the first one is that if you have missing completely at random values, you can see that chaining oracle is good. Good is when you are with this black line, which corresponds to the base right here, except when in the low correlation case with the wave. So this highlights the theorem, which says that if you have the direction of the space where the function has a high curvature or it is difficult. Or it is difficult to predict the missing values, which is the case when you have a low correlation between the X, then you are not base consistent. And then you can see that this NUMIS network plus an MLP is very close, in fact, as close as we can be from the, it's not as close as we can be, is like closer to the base predictor in comparison to the other approach. In comparison to the other approaches, other approaches are some oracle or some miracle that we can't do in practice, or methods that you can do, like I impute by the mean and then I apply, for instance, an MLP. Okay, so to conclude, I will say that we have shown this base optimality of impute and regress. So single imputation is consistent with a powerful learner. In a way, a good imputation is the one that makes Good imputation is the one that makes the prediction easy with all these discontinuities. It is not the conditional imputation, which is the best one, but it is kind of close and it can work even if you have missing random values and so on. This implicit and jointly learned impute and regress strategy is so rectifically defined because it's a differentiable approximation of the conditional expectation with this non-linearity. And as a conclusion, in fact, all this work of supervised learning with missing values are used if you want to do causal inference with missing values, for instance, using double rodus methods, because you need to estimate as well as possible your propensity score or your outcome regression. So this can be useful in this setting. And I have not talked about multiple imputation or aggregation of learner, things that we are working on or we have done work on it. Working on, or we have done work on it, and also it's only point prediction. And of course, in practice, you want to have an interval of predictions. Like, if you want to work with my clinicians, it's better to have interval of predictions. And you can consider techniques like conformal predictions with missing value. So, this can be have new search. So, thank you very much.