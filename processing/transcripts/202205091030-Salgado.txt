Here's a little outline of my talk. First, I will try to introduce or motivate the class of problems that we will try to study, which is essentially as the title says, time fractional gradient flows, some examples. Then, in order to extract the features that will allow us to develop either a Allow us to develop either a theory or a numerical analysis. I will present both in the theory and the numerics case some details about classical gradient flows. Again, to see what major arguments work there. And hopefully, we can extend them or adapt them to the fractional case. I will then give some numerical illustrations. And finally, I will conclude with some maybe some summary and some. Maybe some summary and some open questions. So, let me start by trying to motivate the type of problems that we want to study. Of course, in this crowd, I don't need to justify why we need to use or we want to use non-local problems, fractional derivatives, and so on. The only thing that I'm going to say is that, say, for instance, in classical diffusion, well, which is essentially the heat equation, that comes from brown. That comes from Brownian motion, where the mean square displacement scales as so the mean square displacement scales like time. And this relation between one time and two spaces essentially explains why you have one derivative in time and two in space. So there are some other cases, for instance, the super diffusive case, in which the mean square displacement square is like a power larger than one. Square is like a power larger than one than time, which I will not talk about today. The opposite case, however, is what interests me here today, the subdiffusive case, in which the power here is less than one. Then what we get will be this subdiffusion equation where this derivative is the so-called Caputo derivative. And equations like this are usually more used. Like this are usually modeled to model memory effects. Of course, in a little bit, I will define what a computer derivative is, although I believe most of us know at least its definition. If we look at this fractional Richard's heat equation, we can interpret it in a different way. Namely, if we define the Dirichlet energy, its derivative in the L2 sense is essentially the Laplacian. Is essentially the Laplacian. And for that reason, the fractional heat equation can be written as caputo derivative plus derivative of this Dirichlet energy equal to some data. Of course, there was nothing magical about the Laplacian. We can do something similar for, say, some sort of parabolic fractional parabolic P Laplace problem. So this is the P Laplacian. We define the P energy. The P energy, the W1P semi-norm. And then this equation once again takes the form couple to the derivative plus the derivative of this energy equal to data. Here, of course, I want P to be strictly larger than 1. There's a little technicality that arises in the limiting case of P equal 1. Another example may be a little contrived, but nevertheless, could be of the type of problems that I want to. Of the type of problems that I want to look at is the case of fractional ODEs with obstacles. So let's say that we have this little particle that is moving inside this one-dimensional well, which is say the interval negative one to one. If this particle is not going to touch the walls, then it's going to obey its usual laws of motion, which let's say is capital derivative of u equals some f, some force. However, if the particle is However, if the particle is going to touch the walls, the particle is going to get reflected in the sense that, well, the wall is just going to do barely enough so that it doesn't leave the obstacle, it doesn't leave the wall, but it stays there. So let's say, for instance, that the particle is about to touch the right wall, the right obstacle. If the Caputo derivative at that instance of time just happened to be negative. Instance of time just happened to be negative, then the wall is not going to do anything because the particle was going to move to the left, anyways. However, if that's not the case, like I said, the wall is just going to barely enough so that the particle doesn't leave. And that means that since right before we touched, we had the equation of motion, we can decompose the F, the right-hand side into its positive and negative part. The reflections means that at the reflections means that at the point of at the instance of contact the the equation of motion is going to be this one right i'm going to do just enough to force the particle to move to move to the left similar analysis can be done on the left wall and in summary i can say that the equations of motion are going to be well if i'm strictly inside my interval the usual ones whereas if i'm either at the right or left wall it's going to be either plus or minus It's going to be either plus or minus the positive or negative part. I can look at that in a different light, namely, I can define this, if you will, energy, which is the indicator function of my interval. So it's nothing, it's zero if I'm inside the closed interval or infinity if I'm outside. Then that previous problem can be Soxintewritten as capital derivative of u plus the sub-differential of this energy. Of this energy contains F, where I will remind you a little bit what the sub-differential is. One more example would be, for instance, fractional parabolic obstacle problems in which in some sort of fractional solvable space. By the way, I cannot see anybody in the audience or online, so if there's questions, let me know. I will define this say admissible setting HS as a collection of functions that are. S as a collection of functions that are above some given function g, which I will call an obstacle. And I will consider the problem given by these complementarity conditions, which it can be shown that is equivalent to the following evolution variational inequality. Once again, if I define this energy, the HSMI norm plus the indicator function of my admissible set, this evolution operational inequality. This evolution variational inequality can be written once again as capital derivative plus the sub differential of this energy is equal to some data. So all these examples can be cast in this following general form. Let's say that we have a separable Hilbert space, has separability just because I want to do, well, yeah, essentially because I want to do numerics. I have a convex and lower semi-continuous energy defined on this Hilbert space. Defined on this Hilbert space. I have some initial conditions on the right-hand side. And I wish to find a function that, in some sense, solves this differential inclusion of capital derivative plus of differential contains this f. Of course, if alpha turns out to be one, this is a classical derivative and I get a classical gradient flow. If f turns out to be zero, this is just steepest descent to try to minimize the energy. So just so that we So, just so that we are all friends and agree in terminology, what I mean by self-differential is the following. We have a convex function, of course it's not necessarily differentiable, but by convexity, I can touch it from below by hyperplanes. And the collection of slopes of these hyperplanes that touch from below at a certain point is going to be the sub-differential. So, in other words, this. This being in the sub-differential, it's the slope of all these lines, for instance, at this point, is equivalent to saying that this inequality is valid. And that inequality is going to be very useful for us in defining our notion of solution. Okay, so having defined what type of problems I want to study, let's take alpha equal to one and see what the classical or the integer order case. Case theory tells us. Well, first of all, how are we going to understand solutions to this problem? Well, I have the gradient flow here, and let's say that we test the equation by u minus w, where w is arbitrary, because being in the sub-differential, this psi, or this parcel of phi, it's equivalent to this inequality. When I test, I will get this in this. Test, I will get this variational inequality over here. And so, what we're going to understand as a solution is this notion of energy solutions. It's going to be a function that has some smoothness in time, satisfies the initial condition in some sense, and satisfies this variational inequality almost everywhere in time. We like this notion of solution because essentially, for free, you can get uniqueness of solution. Pretty much. Pretty much, you assume that there are two energy solutions, write the corresponding variation inequality. Use a stress function one the other solution, subtract them, and we get this inequality for the norms. But the derivative of the norm, of the difference, is non-negative. So that difference is non-increasing, but they started from the same point, so they should coincide. Of course, for existence, you need to do a little bit more. A little bit more, and the way that I'm going to do it is by a sort of minimizing movement scheme. So, essentially, an implicit Euler discretization, as you will see. I'm going to introduce a partition of my time interval of interest. It doesn't need to be uniform, and I will not assume that it's uniform. We're going to introduce some approximations from this point and forever by capitals and. point and forever by capitals I'm going to denote approximate quantity so u capital U N is going to be an approximate approximation to my solution at time instance Tn I will somehow approximate my right hand side maybe if I want to I can also approximate the initial condition but in this maybe I don't I don't want to and recursively I will solve this minimization problem my energy is convex My energy is convex. I'm in a Hilbert space, but not strictly convex, so this problem has a unique solution. And if we write the optimality conditions for this problem, this is nothing but this, like I announced before, implicit Euler discretization of the gradient flow. So now we have for every n an approximate quantity that is going to be an approximation to our solution. An approximation to our solution, but that is only a discrete sample, so that a finite collection of points. We need to connect the dots in a sense. We need to construct an actual function that is going to be our candidate for approximate solution. And we can do that in two ways, if you will. First, we can just feel in between the intervals going from right to left. Going from right to left because of causality with a piecewise constant, that is going to be my object u bar. And I can actually connect the dots with a line that will be my function u hat, which is going to be piecewise linear. These two are very good suitable candidates for an approximate solution. One thing to notice, and that we're going to like, and it's going to be important later, is that the derivative of this piecewise linear Derivative of this piecewise linear function. Well, of course, it's going to be piecewise constant, but that constant on each interval has this form, which is exactly what appears here in the implicit Euler discretization. And so if I write my minimizing movement scheme in terms of these two functions, I can write it as so, which means, for instance, that if I can set my test function as the discrete solution. Function as the discrete solution of the previous instance, I can get some uniform a priori estimates on the derivative of this interpolant or reconstruction, you may call it in some contexts, that is uniform with respect to the partition. So for any partition that I can come up with, I can construct this U-hat, this approximate solution U-hat, and its derivative is uniformly bounded in L2. This plus Plus, maybe the fact that the data is nice, in particular, the initial condition has finite energy, can allow us by compactness to show that in the limit we have a solution. So, with this, we can show existence of solutions. And these results are classical. I mean, they maybe go back to even the 30s and the ideas of rothing. So what were the essential pieces of the arguments that I tried to present that allowed us to develop existence and uniqueness? Well, to get uniqueness, we had a good definition of solution and inequality that's and this inequality. To get existence, we needed two things, a way to construct the solution in discrete instances of time, so some sort of minimizing movement scheme. Some sort of minimizing movement scheme, and then a smart way to out of this come up with a function or with a function in for all instances of time, which is what I'm calling a suitable interpolation of reconstruction, which this is this new hat. What did the trick was that its derivative was piecewise constant? Now, let's try to see if we can extend or If we can extend or adapt or do something similar for the case of fractional gradient flows, so again, just that all we agree on what we're talking about, let's talk about the definition of the Capuchin derivative. So we all know that for sufficiently smooth functions, this is the definition. You differentiate once, integrate one minus alpha times to get the derivative of order alpha. attempts to get the derivative of order alpha of course um this definition requires at least your function to be w11 but so you would like to define such an object for buffer objects and there's many ways to extend this definition to different classes of functions the one yes um Is there a question? I cannot hear that. Sorry, we just thought it would be, it was a little bit, but you're taking differences of Wt minus W of zero, and so it's still Kaputo. I see. So sorry, you were just wondering. Oh, you mean this definition that I... Yes, yes, yes, yes. Yes, yes, yes. So it's double at zero multiplied by the heavy side function and all this is understood in a distributional sense. So yes, this is one way. This is one way to extend this definition to some more general class of functions. And in a series of papers, for instance, by Leili and John Wong Liu, they studied this extension of the definition of Caputo. And for instance, they showed something that we liked, which I'm going to call the, if you want a fundamental theorem of fractional calculus, that's That's essentially the value at some point is equal to the value at the beginning of time plus the integral of the fractional caputo derivative of the function. Under of course some smoothness and some conditions. Okay, so one more important thing is that using this extension of the definition, it's possible to prove some variants of the so-called kinetic over. Variants of the so-called key inequality, what I'm going to call key inequality, what allowed us to prove uniqueness. Remember that this was the analog of this inequality, which allowed us to prove uniqueness. And for this extension of the definition of the caputo derivative, it is possible to show that for any convex function, the caputo derivative of this convex function of a W is controlled by this quantity, completely analogous to this inequality over here. Quality over here. And so, with this tool, with this definition of that tool at hand, I can go and say in which sense I'm going to understand this time fraction of gradient flow. Same way that we did in the integer order case. Let's test with u minus w, w being arbitrary, and we'll use the definition of the sub-differential to get this variational inequality. Covariation and inequality. And so I'm going to define an energy solution to my time fractional gradient flow to be a function that has some smoothness in time, in some sense, satisfies the initial condition. And moreover, more and more importantly, of course, almost every word in time satisfies this evolution variation inequality. Once again, this is a good definition of solution. Is a good definition of solution because almost for free we can get uniqueness of solution. Once again, I assume that there's two of them. I test one variation inequality with the other solution at the map. And I get this inequality, which I can plug into the fundamental theorem of fractional calculus, if you will, after using the key inequality to get uniqueness. Quality to get uniqueness. To get existence, well, we need to define a fraction of minimizing movements. So we need to essentially come up with a good discretization of the Caputo derivative. So here's how we're going to do it. It's, if you will, a variant of some deconvolution schemes, as you will see. We're going to start from To start from Zamethico theorem of fractional calculus, and let's pretend that on some partition of the time interval, my Caputo derivative is piecewise constants. Then we all know that we can decompose that integral into the sum of integrals over each one of these sum intervals of the partition. I can take out the constants and we get essentially a vector identity of this flavor. Identity of this flavor, where w are the discrete values of my function at the partition points. This is the initial condition. V alpha is going to be my piecewise constant, capital derivative, and kp is what comes up of integrating the curve over each one of the sub-intrices. Okay, so this matrix, of course, because of causality is going to be lower triangular. Lower triangular, we'll integrate in a positive kernel so that the diagonals are not going to be zero, so it's non-singular. We can invert it, and then my definition of discrete caputo derivative is, well, just solve for this guy. The discrete caputo derivative is this V alphas, which is the inverse of the matrix times, well, what if this? It turns out that if the partition is uniform, then this matrix is tablet. Then this matrix is tablets. And so matrix multiplication can be understood as convolution. And so this class of schemes is usually called deconvolution schemes. And there's a lot of things that follow from the fact that this matrix is templets, for instance, monotonicity of the scheme and so on. However, because of reasons that I will explain later, I do not want to assume at any instance that my partition is uniform. That my partition is uniform. Then, of course, a lot of the structure of this matrix is lost, but there's enough that retains for at least our purposes. For instance, the diagonals of the inverse are strictly positive, the off-diagonals below are negative, and there is some sort of monotonicity with respect to rows. This allows This allows us, for instance, to prove a discrete version of the key inequality that I want that allow us to get uniqueness. And so with this discretization of the Caputo derivative, we can define a fractional minimizing movements pretty much in the same way that we did for the classical Gradient flow. Of course, because of now we're not going to just take the one over tau, which was The one over tau, which was the integer order case, but all the previous times weighted by the corresponding entries of the integers matrix. Why do we do that? Well, because it turns out that this is, once we write the optimality conditions, completely equivalent to this discretization of the gradient flow, where that here we have the discretization of the couple to derivative that I just described. Capital derivative that I just described, this generalized deconvolution or generalization of deconvolution to non-uniform time measures. Once again, I mean, because of convexity and strict convexity, we have existence and uniqueness of these UNs. And so the question is: out of these UNs, how do we define a function, i.e., how do we define a reconstruction or this piecewise linear, the analog of this piecewinstin? Piecewise linear, the analog of these piecewise linear interpolations. Remember that the trick was that I want the Capuchin derivative of this guy, in this case, to be equal to this expression over here. And so the way that we're going to do it is, essentially, we're going to wish it into existence. We're going to define some functions so that the Caputo derivative of one of the intervals is one and in all the other ones is zero. All the other ones is zero, and then I just kind of interpolate this, it's going to be the values of u on the corresponding solving interval times these phi kind of basis functions. This is the actual way that this reconstruction is going to look like. And the good thing is, like I said, that's because these are constants, either one or zero in each sub-interval. zero in each sub interval, the caputo derivative of this guy is going to be piecewise constant, and thus we can write here the caputo derivative of these u hats so that the minimizing movement scheme can be rewritten in this form, where the u bar was as before the piecewise constant reconstruction. We can again play the same game of trying to find the suitable technology. The suitable test functions and use the evolution variational inequality so that we can get an a priori estimate on the Capuchin derivative of the reconstruction in L2. As expected, we need to start with something from something with finite energy and we have some condition on the F which it kind of seems Uh, it kind of seems reasonable, at least in the sense that if alpha is one, this just because Fb in L2. And more importantly, this is a uniform a priori estimate on the computer derivative of this you hat. So we have enough to pass to the limits. And then if you start from something with finite energy, the right-hand side is sweet. Energy, the right-hand side is suitable in this sense. Then the time fractional creating flow has an energy solution, which, moreover, as part of the estimates that allowed us to pass the limit, we can show that this solution is Helder alpha over 2 in time. We like that this is Helder alpha over 2, because if you recall in the classical gradient flow, that is alpha equal to 1, an energy solution was H1, i.e., Helder one half. i.e., helder one half. So there is this sort of continuity or consistency, if you want, with the alphas, right? That I can let alpha go to one and the regularity is kind of the correct one. So this is, so we have a theory of existence or uniqueness, at least for this class of solutions that I define as energy solutions. Let's try to do some numerical analysis on them. We have Them. We have a very good candidate for numerical scheme, but let's see what can we say about the posterior and a period and a posterior analysis of it. Again, let's start with the classical gradient flows. We had our minimizing movements or implicit Euler scheme. And so some of the ideas that were originally proposed by Mochetto. were originally proposed by Mochetto Savalia and Verdi to do some error analysis for this scheme are the following. So we write this in terms of the variational inequality. We test with the solution of the previous time step that gives us an inequality that depends only on essentially known or computed quantities. And now we're going to do something very And now we're going to do something very tricky. We take everything to one side of the inequality thing so that it reads something is non-negative. And like I said, this something, which I'm going to call E sub n, depends on F, that's data, depends on Un minus 1, which is what I computed, my discrete solution, and the time step, of course, and the energy of this. So this quantity is completely computable. As you compute. As you compute, you can define your en and then extract, hopefully, maybe some information out of it. Indeed, we can write, you remember that we introduced these reconstructions u hat and u bar to say that the implicit Euler scheme can be written in this form where there's two functions. So let's try to write it in terms of just one, say, for instance, our u hats. Sequences are U hats. We can do that by using that our energy is convex and more importantly, that this reconstruction U hat is a convex combination of the values at the grid points. And it turns out that if we write what is the evolution variation inequality that this reconstruction solves, is up to a perturbation term, the same one that the continuous or the exact solution solves, and this perturbation is And this perturbation is exactly this term here, the one that we define in terms of computed quantities. So we can essentially apply the same, the usual tricks to look at, for instance, the same tricks that we use to get, for instance, uniqueness. I can test this with the exact solution. I can test the exact solution with this u hat and get an a posterior error estimate for the for error in the sense that the error u minus sense that the error u minus u big u hat is controlled by something that depends on f but more importantly by this quantity over here where these ens are this and i insist on this completely computable quantities right so you can say okay forget about the f the f is what is what it is but if these numbers if these numbers are big or small i can make a judgment of whether my solution is good or bad Out of this a posterior error estimate, I can get an a priori error estimate by noticing that first here I have a tau square, but if I do the same symbol just with power one of tau, I get something that it's in principle finite and it depends only on data, the initial energy and the norm of my data. So if I start from something that has finite energy out of here. Out of here, I can get an error estimate of order tau, this say the maximum of time step to the one-half, which, if you remember, energy solutions for the classical gradient flow were 100 one-half. So, these are optimal in terms of regularity. So, once again, let's try to recap and see what were the essential steps that are. That allowed us to get this a posteriori and a priori error analysis for classical gradient flows. Well, first, we needed some sort of error indicator or estimator, so some positive quantity that depended only on the computer approximation, the factor of reconstruction or interpolant you had solved a perturbing flow, or the perturbation is this quantity that we previously defined. And to get And to get this previous step, we need the facts that only interpol n is a convex combination of the model values, of the grid values, the ones that you actually compute. Let's see if we can do something similar for the fractional case. And so we recall that our minimizing movement scheme, the ones that was a generalization of the deconvolution scheme, read in this way. Way our new hat was defined so that this is exactly its coupled to the derivative, and so as before, we get volition variational inequality where we have these two functions, the u-bar that is piecewise constant, and this u hat that has piecewise constant caputo derivative. Once again, I take everything to one side, and I'm going to define this quantity E alpha that, as before, depends. That, as before, depends only on known quantities, data, and these new hats and new bars that are defined in terms of only the values at the readpoise. So, this is also computable, so it can serve as an a posterior estimator. And now, remember that we needed, and now let's remember how this new hat was defined. This new hat was defined. I said, I'm going to define some functions u phi, I think they were denoted, whose caputo derivative was either one or zero. It was zero and one at one particular interval. If I look, if I plot these functions, they look like so. Of course, they need to start being zero up to the point where the caputo derivative is non-zero. That's causality. And they're not going to be supported on one interval. On one interval, that's no locality, but the tail kind of decays. If we look at them, say for instance, for alpha very close to zero, alpha says 0.1, I want to say that they look almost as a piecewise constant, right? They jump very high to one and then the tail is very small. For alpha equal to half, they look like so. And if I get alpha very, very close to one. Very, very close to one, they all they want to say that they're trying to look like these classical hard basis functions for one-dimensional finite elements. So this may hint at the fact that these functions could possibly, same as in the case with hard basis functions, form a partition of unique. And it turns out that that is the case. And it turns out that that is the case. These functions are all non-negative, they are up to one, so they are a partition of unity. Why do you care about that? Because then this interpolant is a convex combination of the grid values. That is one of the things that we needed to get the term volution variation inequality only in terms of these u hats. And indeed, we can get that this reconstruction satisfies reconstruction satisfies an evolution variation inequality exactly the same in exactly the same form as the exact solution with a perturbation and that perturbation is once again this positive quantity that we defined with some work we can once again compare the exact and the solution and this you had using the operation inequalities and get an a posterior error And get an a posterior error estimator. In the sense that, well, the error is controlled by something about the f, but more importantly, this quantity over here that depends only on the alpha. So once again, I can judge whether my solution is good or bad depending on whether this E alpha or more precisely this functional on E alpha is big or small. To get an a priori error analysis, Napoleon error analysis. Let's recall that that's the definition of the u alpha and let's see what we can say about each one of these quantities over here. So remember that, for instance, this u hat and u bar, they coincide at the grid points. So this integral cannot be much larger than, well, let's say, let's see. Let's see. So, this is a function that say vanishes at every interval of size tau. So, because of that, you can say that this quantities of order, say, tau to the two alpha. Well, because these coincide at every grid point, so they can differ by at most tau to the alpha because of the held irregularity of this kind. And then you integrate that, you get another. And then you integrate that, you get an order power of alpha. In addition, since U hat is a convex combination with the grid values, this is just convexity. And then we can get using that convexity an estimate on this difference over here that is of order tau to the alpha. All these things combined, and of course, much more algebra, allow us to get a priori estimate saying that. Estimate saying that if we start with something with finite energy, our right-hand side is reasonable, then for every partition, our error is of order the maximal time step to alpha over 2. I remind you that energy solutions were held at alpha over 2. So this is optimal in terms of regularity. I think I'm going too fast, but let's look at some numerical. Let's look at some numerical illustrations. The first example that I'm going to show is a very simple linear example. The same subdiffusion equation that I started with. And I supplemented the homogeneous Dirichlet boundary conditions. We all know that the exact solution to this problem can be written in terms of eigenfunctions of the Dirichlet Levoplausen and metaglefler functions. And so And so, say, just to compute things, final time is one. I do, we're in one dimension, so I know the eigenfunctions of the Laplacian. So, I'm going to the spectral discretization of space because what I really want to see is the error in time. And I'm going to measure the error at the final time and kind of the L infinity H norm here because. Um, norm here because my grading flow here is in L2, and I'm going to say I'm going to compare the error here depending on different types of initial conditions. I'm going to say that, well, my initial condition first is going to be like the modes are going to be like k to the negative 1.5 plus a little bit, so that it has finite energy, but it doesn't have more smoothness, say, for instance, H2, which would be the domain. Which would be the domain of the actual operator. And we see here for alpha 0.3 and 0.7 that, well, the infinity rate, the L infinity L2 rate is 0.3. So half of that is what, say 0.15, which is what we see here. So alpha over 2. And same with 0.7. 0.3. So half of 0.7, I'm sorry, so half. Half of 0.7, I'm sorry, so half of that is 0.35, as we see on this here. However, if we just measure the error, say at the end of time, so at a positive time, the order is one. And I'll mention, I'll talk, and we have, yeah, and I'll mention something about that in a little bit. If, on the other hand, my initial condition is smooth, let's say the modes decay like k to negative 2.5 plus. Negative 2.5, close a little bit so that you're in the domain of the operator. There at positive time is still order one, but the L infinity L2 error is of order alpha. As you see for 0.3, it's 0.3, 0.7, 0.7. So the convergence rate is of order tau to the alpha. And even though I didn't talk about it, we do have a theory that includes this case. Let me show you a fractional parabolic obstacle problem. So, once again, let's say we're in one dimension. I remind you that this is at least the complementary condition versions of this problem. And I supplemented with periodic boundary conditions. I'm going to take an obstacle of this form, essentially this blue thing over here. thing over here very simple initial condition just assign and some constant forcing I do periodic boundary conditions so that I can do a collocation method again what we care about is the the time error here and this is the time step that we're going to use and here are some snatch snapshots of the discrete solution for this problem for the different values of alpha Problem for different values of alpha and different values of f of s, I'm sorry, because remember that we have here a fraction of Laplacian for the s so let's try to parse all this. Here I have on the rows, I have fixed s 0.1 on the top and 0.9 on the bottom. And then I change the alpha. So, say for instance, if alpha and s Say, for instance, if alpha and s are very small, well, s is very small, I'm not requiring much regularity in space, so I can get very, very close to the obstacle pretty much, as you see. For instance, in alpha 0.9 and in 0.1 for t equal to 100, I'm essentially coinciding with the obstacle. And the effect of alpha, we see the order of the time derivative, is how fast things happen. Because for t equal 100, here essentially I'm talking. t equals 100 here essentially I'm touching the obstacle whereas here I'm still yet to reach the obstacle then if I increase my s I need to prescribe more more smoothness so I cannot comply exactly with the obstacle but I need to be a little smoother and the effect of alpha is the same things are just happening fast happening faster we can look also at the time fraction At the time fractional Alankan equation, there's been several works that have been at problems of this form, where to the soft diffusion equation, we add this zero-order nonlinear term with this function g is the derivative of this quartic term. In this case, we do not have a convex energy, but we have a Convex energy, but we have a Lipschitz perturbation of a convex energy, and we do have a theory for this case as well, although I didn't present it. But if we compute, here are some snapshots of the solution, and they seem to coincide with what's out there in the literature. And once, and in this case as well, we can get We can compute error estimates at least compared to a very over-resolved solution. And if we see it with a gentle eye, it seems that once again, the error at positive time is of order one. The error in L infinity L2 is of order, kind of like alpha over two. Finally, we all our theory. We, all our theory, did not assume that we had a uniform time partition, uniform time step, and we had an posteriority error estimator. So we can use that to adapt our time step in order to well minimize the error. So for instance, very simple example, linear self-diffusion equation with alpha equal to half. And given some tolerance, I'm going to I'm going to choose the time step so that the error indicator is equi-distributed in this sense. That this estimator on every sub-interval is of order alpha epsilon squared, because that will guarantee that the error is globally of order epsilon. So let's take some epsilon. The time adaptive solver is going to, in my experiment, is going to require 455 time intervals. 455 time intervals with very tiny minimum time step and somewhat big maximum time step. If we compare that to just doing a uniform time step to reach a similar accuracy, my uniform time step needs to be of order 10 to the negative 5, and I need of the order of what? 10 times 10 times. 10 times more time steps. So, of course, there's a clear advantage and no surprise here in choosing the time step adapter. And as we all know, in the subdiffusion equation, there is a weak singularity in time at the beginning of time. So it's clear here, this plot is showing the size of the time step as time advances. So of course, we need a very tiny time step at the beginning to capture the singularity. To capture the similarity. But then for positive times, the solution is small, so we can take larger time steps. Okay, so yes, I definitely went very fast, but let me recap what I tried to talk about today. Well, first, I gave a discretization of the caputo derivative over any time partition that has nice properties. Properties. With the help of it, I showed the existence and uniqueness of underitial solutions to time fractional gradient flows. This is a very well one-of-fitting numerical scheme, and for it, I gave a post-electric error analysis, so an apostolic error estimator that is reliable, and an a priority error analysis without assuming anything on the solution, just what the flow gives to us. And these estimates seem And these estimates seemed optimal or are optimal in terms of the available regularity of resolutions. There are many things that we also have, but I didn't discuss, or at least not in much detail. For instance, we have extensions of our theory and works to the case of our energy being either lambda convex or ellipsis perturbation of a convex function. This allows us to. This allows us to treat cases like, for instance, fractional reaction diffusion equations, as I presented in some examples, fractional Alan Khan, and some others. Some of the numerical examples that I showed gave better convergence rates than what theory that I presented predicted. And we have some explanations for that. For instance, we have improved convergence rates for some cases. Convergence rates for some cases like linear equations, smooth energies, positive time, and some others. This is a gradient flow. So you expect that the solution should converge to the minimal energy or that the energy is being minimized. So we have some sort of asymptotic behavior for the solution. For instance, if Fe0 and Phi is uniformly convex, then we have that. Then we have that the energy or the solution converges in the classical case, it should be exponentially to the minimum. In this case, it's with this rate. That's metadata left. I'm not surprised there. However, if we only have convexity, then of course there's no way that we can say anything about convergence of the solution, but we can talk about convergence of energies. Energies and it's with rate essentially time to the minus alpha over two which it's not optimal but at least I've never seen results of this form for in this generality for gradient flows or for discrete solutions there's many things that can be done here for instance we don't have a complete answer to why some experiments have Some experiments have a better rate. We know that every sub-differential is a maximal monotone operator, but not vice versa. So we have some results in the case that our operator is a maximal monotone operator. I mean, to begin with, we need to change the definition of solution, right? There is no energy anymore. The very first step in everything that I'm doing breaks if we are not in a Hilbert space, but you could consider evolution in Banach spaces, either for a sub-differential or a monotone operator. I have no idea how to do that. And of course, our goal is to discretize PDEs. So the PDEs have a space and time component. So how to do the space discretization in the sense that how to take into account the effect of the space discretization is open here. Is open here. So, with that, I will conclude. Thank you very much.