The nice sufficient conditions, there are plenty of them. What I don't know about is an equivalence result. Okay, but I'm saying plenty of sufficient conditions. So, an example of sufficient notion we have an example of. For example, a very good example is that if Hessian of V is greater than alpha times the identity with alpha strictly positive. So, we're uniformly convex potential, this is good enough. And then, if you have a bounded perturbation, also okay. If you have a Lipschitz perturbation, Okay. If you have a Lipschitz perturbation of a uniformly convex function, also okay. If you are uniformly convex outside of a compact region, also okay. So plenty of kind of sufficient conditions. But you do require quadratic growth at infinity. At least if you want to work with this particular diffusion generator. If you want to work with this necessary condition. Necessary condition quadratic growth of V at infinity. And then there are kind of plenty of slight reinforcements of quadratic growth at infinity that are sufficient. Okay, yeah, so that's kind of the side. So, yeah, why Fisher information? So, as was mentioned yesterday, Yesterday, the Focus Planck equation associated with this diffusion equation can be viewed as the gradient descent of relative entropy with respect to the Vassarstein distance W2. And then the Fisher information, it arises as the derivative of the entropy along this hole. And because if you have a gradient descent, x dot equal minus gradient of f of x t the dissipation of f of xt is minus gradient of f of xt squared. So we can reinterpret Fisher information as the squared norm of the gradient of the entropy with respect to Lasserstein distance. And then the kind of inequality I mentioned, transport information, can be reinterpreted as a control of the distance by the Control of a distance by the square norm of a gradient of a function. And of course, this is a classical tool to understand convergence to equilibrium for a gradient descent. This is strongly related to Loyazevich-type inequalities. Typically, Loyazevich inequalities involve other, might involve another exponent than two, so to make it more general, but okay, this is one particular instance of a Loyazewich type inequality. Quality. Okay, so that's kind of why Fischer information naturally arises in this kind of problem. There is really an underlying interpretation with respect to Wasser-Stein geometry. Okay, so now before going kind of into implementation of the kind of additional perspective I want to talk about, I want to talk about two kinds of outcomes that come from this. That comes from this viewpoint of viewing, of like taking a variational viewpoint on certain functional inequalities or problems, identifying an integration by parts formula as another Lagrange equation for the variational problem, and then comparing these integration by parts formulas via Stein's method and used Stein's method. Via Stein's method and use Stein's method to get stability. So, the first outcome I want to talk about is about concentration inequalities. So, if you have a uniformly log-concave measure, so a measure with density exponential of minus V, with V uniformly convex, and to fix constants, I will assume H and the V is greater than the identity matrix. So, this is a measure that is more log-concave. Uh, more log-concave than a standard Gaussian. Then, a classical result that has multiple proofs is the Gaussian concentration property. For any one Lipschitz test function, the integral of exponential of lambda f with respect to mu is controlled by the exponential of lambda times the average of f plus lambda square over two. And then this implies that the probability. implies that the probability that f under mu at f is greater than its expectation plus r is less than exponential minus r square over two just by applying channels inequality so markov's inequality and optimizing with respect to m so this is a kind of a functional encoding of concentration inequalities for the shit set functions and it has plenty of applications in statistics And it has plenty of applications in statistics and geometry and information theory. The applications of this inequality are quite numerous. And okay, so we have this inequality and it's optimal because if mu is a standard Gaussian measure and f is just a coordinate, so take mu a one-dimensional Gaussian measure and f of x is equal to x, there is exactly equality here. So there is equality. So there is equality if mu is equal to exponential minus x square over 2 or square root of 2 pi and f of x is equal to x, then this is equality and you get the sharp concentration inequality. And it turns out that this is kind of like the only situation this could happen. And we have the following theorem that we proved a few years ago with Tom Cortez. Tom Corte. So assume that we have this convexity condition, and you can find a function f such that this upper bound is almost sharp, in that you can have a bound from below by the same thing, except that instead of a constant one half, you have a constant one minus epsilon over two. So you have almost equality in the inequality, and f is not identical. Uh, not identically equal to zero, it has to be a non-trivial function, non-constant function. Then, up to a translation and a rotation, your measure must split off a Gaussian factor. So, you might have to make a translation to fix the average of the barycenter of mu at the origin. You can make a rotation, but there is a coordinate, and then you. But there is a coordinate, and then you do the rotation to put, but there is a coordinate that is almost Gaussian, and this Gaussian coordinate is almost independent of what's going on on the orthogonal directions. So your measure is Wasterstein one close to being a product measure with a Gaussian factor. And it's upper bounds on the distance. The upper bounds on the distance that scales like square root of epsilon. So, what's kind of nice here is that this estimate is completely dimension-free. It does not depend on the measure must look like a product measure with an almost product measure with an almost Gaussian factor, independently of the dimension of the ambient space. The constants are the same in dimension one or in dimension two thousand. Or in dimension 2000. So you can kind of do dimension reduction in that you have a dichotomy. Either this concentration inequality can kind of be strictly improved, or there must be a Gaussian direction hidden somewhere. Okay, so that's one kind of outcomes you get out of this, where you have a kind of an inequality of interest, you know the optimizers. Know the optimizers and the optimal measures, and you can show that an almost optimal measure must kind of look like an optimal measure with quantitative balance. Now, another outcome of this kind of philosophy, which is kind of in a different field, which is in classical differential geometry. So, a classical topic in geometry is to optimize some kind of geometric quantity. Some kind of geometric quantity subject to constraints. And a typical example to keep in mind is the isoperometric problem. You look at all shapes, you fix the volume, and you ask what's the shape with the smallest perimeter. And the answer, going back to the ancient Greeks, is that the minimizer is the sphere. And then you can look at stability, you can wonder. Wonder is an almost minimal shape, does it kind of look like a sphere? And the answer is yes. But this is much, much more recent. The first results were in the 1950s, so almost 2000, so around 2000 years later. And the sharp results are quite recent. They started appearing in like around 2008, 2009. So now I will look at the kind of a different kind of optimism. I will look at the kind of a different kind of optimization problem, which is understanding the structure of manure bounds and with certain geometric quantities. So I'm going to consider an n-dimensional Riemannian manifold with a constraint on the Rickshi curvature tensor that is bounded from below by n minus one. And if you don't know what Rickshi curvature is, no problem really, because the picture is quite clear. If you're on Euclidean space and you leave in two different directions with the right angle, and you look at the distance after going off in two different directions for a distance t, then the Pythagoras theorem tells you that the distance between the two endpoints is square root of 2t. Now, if you were to look at the same problem on the sphere, You go around for a time t, you go around for a time t in two orthogonal directions, and you look at the distance is going to be strictly smaller than square root of two times t. And this is kind of typical of positive curvature. So when zero curvature, you have the Pythagoras theorem. Positive curvature means points tend to be closer than what would be told by the Pythagoras theorem. And that's And that's okay. And if you know something about Riemannian geometry, you know I'm lying through my teeth here. What I'm describing is sectional curvature and not Rickshi curvature, but it really doesn't matter. This is the right picture to keep in mind. Zero curvature, Pythagoras theorem, positive curvature, points tend to be closer. And the constants here in the current strain that curvature is greater, is bounded from below by n minus one, is chosen so. One is chosen so that it's the this constraint is exactly satisfied by the unit sphere with a unit radius. So an example of such a manifold is just the n-dimensional sphere with radius one. And something you can seek to optimize are then kind of geometric ones. And one thing you can seek to optimize is the biometric. So you can ask what's the largest What's the largest possible diameter for a manifold satisfying this curvature bound? And the answer is a very classical result in Revanium geometry. It's the Benny-Meyer's theorem. Under the curvature constraint, the space with maximal diameter is the unit sphere. So the unit sphere has remaining diameter pi, and you cannot do better than that under this curvature constraint. Constraint. And sorry, there is a change here. A theorem which is due to Obata in 1962 says that this bound is rigid. Among all smooth n-dimensional matters satisfying this curvature bound, the only manifold with maximal diameter is exactly the sphere. There are no other There are no other such smooth manifolds. You cannot do better than sphere. And then the next question is stability. Does a space with almost maximal diameter, is it close in some sense to a sphere? And then the answer to this is no, it's not. And there are counterexamples. And the counterexamples kind of spurred a lot of activity in geometry along saying exactly. Along saying exactly what these counter examples are, what do they look like, in what sense are they not close to a sphere? Okay, so this is something that this is a very active topic in differential geometry. And in particular, there is a classical work of Chiger and Kolding that kind of fully characterizes in what way this character is. In what way this characterization is unstable. And a theorem that we proved a few months ago with Yvonne Gentiles and Jordan Serre is the following. So take a manifold with this curvature condition and assume that the diameter is almost maximum. So it's greater than minus epsilon or some small epsilon. Then you can find an eigenfunction of the Laplacian on your manifold. Fashion on your manifold, such that if you push forward the volume by this eigenfunction, then the measure you obtain is a measure on the real line. The eigenfunction is real valued. And this measure looks like a symmetrized beta distribution. So it looks like a measure with density proportional to 1 minus x squared to the power n over 2 minus 1. To the power n over 2 minus 1. Now you can wonder what's going on, why is this happening? And the reason is quite natural is that if you look at the sphere, so the actual equality case, you can embed it in Euclidean space. So you take the n-dimensional sphere, you embed it in Rn plus 1. And then the coordinates in this embedding, so just the coordinates along the sphere. Coordinates along the sphere of a you take a random point on the sphere, you look at the first coordinates. This is going to exactly follow a beta distribution. This is the law of a coordinate on an n-dimensional sphere. And coordinates are exactly eigenfunctions of the Laplacian on the sphere. So they are exactly the first eigenfunctions for the first spherical harmonics. For the first spherical harmonics. So, this is kind of you kind of have stability, but in a very weak sense, in that if a manifold has almost maximal diameter, then there is an eigenfunction such that if you look at the image of a random point on your manifold by the eigenfunction, you cannot, you have a hard time distinguishing what you obtain from what you would obtain if you were to do the same procedure on the sphere. If you were to do the same procedure on the sphere, so you kind of have closeness of eigenfunctions, but when viewed when pushing forward the volume measure. And why are we doing this in this way? It's because if you were to look at a coordinate system, a coordinate system for a manifold, this is not intrinsic. You specify it, you have many choices. But eigenfunctions, they are an intrinsic object. Functions, they are an intrinsic object. You don't have to specify an embedding to figure out what they are. So, this is a kind of a coordinate-free, stable version of something that kind of plays the role of a coordinate system. And indeed, there are many results in geometry where eigenfunctions of the Laplacian play the role of a proxy for a coordinate system. If you want to do intrinsic. System, if you want to do intrinsic geometry. So, this is kind of the kind of statement we get. And the true statement we prove is a quantitative statement for space manifolds with what we call an almost minimal spectral gap. And then we get a stability estimate that is dimension-free. And then there are results of geometry in differential geometry that say that. Differential geometry that says that this quantity is indeed almost minimal if and only if the diameter of the space is almost maximum. So, okay, so that's another outcome of the kind of philosophy. And the proof of this result uses Stein's method for beta distributions. So it kind of uses results of Coltein and Reinhertt and Dubler on Stein's method for beta distributions and kind of uses them. And kind of uses them to prove the stability estimate. So, this is where part of the motivation here is to give the next examples are all going to be Gaussian, but I wanted at some point to mention that there are non-Gaussian examples where the philosophy I want to talk about works. Okay. So, now I would like to talk about a very specific problem where I will Will kind of be able to sketch the full proof. It's like you are an RD, a fading space, and you consider an isotropic and centered probability measure. And we look at this Poincaré constant. So this is something that was mentioned in the third talk yesterday. A Poincare inequality for a measure is a control of the variance by the L2 norm of the grade. Of the gradient. So you take any test function f, look at the variance. If you can control it by some constant fixed constant times the L2 norm of the gradient of f, then you have a Poincaré inequality. And the Poincaré constant of the measure, so this is Cp of the measure mu, is the smallest constant Cp such that this holds. So the best possible constants you can have here. And if you test again, And if you test against a linear function, so that's the coordinates, because you assumed your measure to be isotropic, the second moment of a coordinate is one, the variance is one. And of course, the gradient of a function x1 is just one, the L2 norm of the gradient. So you see that the Point Carré constants for an isotropic measure must be greater than one. And this constant. And this constant one is exactly the Poincaré constant for the standard Gaussian measure. And there are many proofs of this result. The simplest proof is to just do an L2 decomposition along Hermit polynomials. You can expand both sides in this inequality as sums over all Hermit polynomials by decomposing your function as a sum of Hermit polynomials. And you can see from this that the best possible constant is. See from this that the best possible constant is exactly what just by compute, direct computations. And then Chen and Liu proved in the 1980s that this is actually a characterization. The only isotropic-centered probability measure whose quantity constant is equal to one is exactly the standard Gaussian measure. So you have no other such methods. And the proof is that, well, they show that such a measure must satisfy Stein's identity for the Gaussian measure. And then you can do a stable version of this. The following is that the Point Area constants of an isotropic central probability measure is greater than not just one, but one plus the squared W2 distance between mu and the Gaussian. Between mu and the Gaussian divided by the dimension. So here this is on R. And W2, I guess I never defined this. W2 between mu and mu, the square w2 distance, is the infimum over all pairs of random variables x with law mu, y with law mu of the expectation of x minus y squared. So this is the squared w tweet of this. And it's greater than. And it's greater than W1 squared. Okay, so you can kind of not only use a measure the only measure with Poincaré constant one the Gaussian, but a measure whose Poincaré constant is close to one must be close to the Gaussian. And the exponent two and the linear dependence on the dimension here are both optimal. So the constant one over ds sharp. constant one over d is sharp. The exponent two here is okay and how does this proof well I'm just going to do the proof in dimension one but the proof in higher dimension works the same way. Yeah there is sorry there is a question. Yes so in the in the previous inequality so the the so the constant Cp can be equal to plus infinity. Yes. Okay. Yeah, if you have a measure, for example, a measure that whose support has several connected components, the best constant here is going to be possible. I'm allowing for possibility. Okay, so how does the proof work? Well, you look at the difference between the right and the left-hand side in the Parker constants in the Parker inequality. Constants in the Parkland inequality, and you look at a function which is a small perturbation of x. So you take a test function x plus epsilon h with some centered function h, and you get that this quantity here is non-negative. And the first term, if the Poincaré constant was exactly equal to one, this would be exactly the Stein identity for what appears in Stein's identity. Appears in Stein's identity for the standard Gaussian measure. So we get something that kind of looks at the first order term, like the which we want to control for Stein's method. And you have a second order term in that silo. And you have a constraint in that this quantity has to be non-negative. And then if you look at the function h that is bounded in Lipschitz and you optimize with respect to epsilon, what you get out of this is that the supreme This is that the supremum over such functions h is bounded from above by the square root of the Poincare constant minus one. This is what you get by optimizing in this label. And then you know that by Stein's lemma, this controls the value one between mu and mu. Yeah. So this is what you get in dimension one. And if you carefully do it in dimension Carefully do it in dimension D, it also works. And due to results of Ledoux, Norda, and Piketty, with this argument, you even control the W2 distance and not just the W1 distance. But this uses a theorem of Le Dumoud and Piquet. It's not an obvious fact. Okay. So now a brief description of Now, a brief description of kind of other results that were obtained using this kind of ideas. So, UTEV proved stability of Moincaré constants for the Poisson target distribution using the same line of arguments. This was in the early 1990s. And more recently, there were works by Aras and Oudre for stable laws. Jordan Serre gave an abstract methodology and criterions for general target decisions. For general target distributions and dimension one under moment constraints. And with Cebron and Maille, we looked at stability of Poincaré inequalities for semicircular distributions in a free probability. And the same philosophy has kind of works. And more recently, Sera has some results on higher eigenvalues of Markov diffusion generator. So you can view the Fusion generator. So you can view the Poincaré constant as the inverse of the smallest positive eigenvalue of a certain Markov diffusion generator. And you can also prove stability results for other eigenvalues. With Tom Courtade, we prove some results for logarithmic sub-alit constants. And there are some results in geometry that are kind of related. They're the results I mentioned for maximal diameter for manifolds, where instead of beta distributions, Where instead of beta distributions, it's a Cauchy-type distributions that appear. Namely, stability of the spectral gap for what are known as RCD spaces with negative dimension parameters. Okay, but this is an obscure problem in geometry. I just wanted to mention that there are kind of applications of Stein's method for Cauchy distributions in some forms of differential geometry. Okay. Okay. So now, because this conference is supposed to talk about open problems, I was told, so I tried to brainstorm a bit about open possible problems that kind of seem related to this philosophy and that might be of interest to this community. So, one thing actually I was wondering about is whether This kind of philosophy can be used to study the stability of the optimizer for infinite-width two-layer neural networks. So, if you look at a neural network with just two layers, we kind of look at functions of this form. So, rho is the activation function. For example, it can be the value activation function. The important thing is that it's a non-linear function. And we look to approximate some target function g with a function that depends on some parameters, which wi, a i's that are linear maps, and the i's that are this constant. And we are looking to find parameters such that this holds with n, n is the width of the n is the width of the layer and and we're going to look at a very large width and then how do you identify what's a good approximation you can look at a loss function and say the smallest the loss function the better the approximation so typically if you do this with l2 loss l2 distance when average with your To a distance when average with respect to some fixed sample distribution. And of course, in practice, instead of using the theoretical law, we use a sample. And then you want to know, okay, what are the best possible parameters to make this approximation? And what you can do is to use a gradient descent in parameter space to approximate optimal parameters. But the problem is that when viewed as a function of these parameters, As a function of these parameters, this function has plenty of local minimizers. Except that what we observe when n is large is that gradient descent kind of avoids, often avoids these local minimizers and works pretty well in practice. And there were works to try to explain this phenomenon. And what I will explain is taken from a work of Dené Shiza and Francis Bach. She's ain't Francis back, but many others around the same time came up with the same kind of explanation. People like Montanari, Van den Ayden, Esquiopoulos. So you can embed your parameter space as a probability measure over Rg plus 2 by taking your n triplets of parameters. Of parameters, taking the Birac mass over your parameter, and taking the empirical measure of the parameters. So, this is one way of embedding a high-dimensional space into a space of probability measure, but probability measures over a space of fixed dimension. And because this function fwab can be written as an integral with respect to these measures. With respect to these measures, we can extend R via this embedding to a function over the space of probability measure. And then you can run gradient descent of this extended function r over the space of probability measures with respect to the w2 distance. And what's really nice of this is that the lifted function. The lifted function doesn't have plenty of local minimizers, it can only have at most a single local minimizer. So, this can be viewed as an explanation as to why gradient descent kind of in practice avoids local minimizers is because when the width goes to infinity, you converge to a problem to a gradient descent of a function that only has a single local minimizer. Local minimize. So the infinite width gradient descent cannot get stuck into local minimize. Of course, this is saying there is at most one local minimizer. There might not be any. So in practice, we add a penalization such as an entropic penalization to force the existence of a local minimizer. And this has adding this entropic penalization has a nice effect on the medium to share. It's in some way. It's in some way, it's kind of a limit of what would happen if you were to do stochastic gradient distance, having this entropic penalization. And you can also add other types of penalizations, such as radio entropies or derelict forms. This has been kind of, this is starting to be studied, the effect of the penalizations in the PV community. There is a recent survey of Fernandes Real and Sigali about this. As Real and Shigali about this kind of issues. Okay, and one question is that, to my knowledge, is currently open: is whether the minimizer of this function over the space of probability measure is stable. So if you have a function with very small eventually penalized loss, are you sure it is close? Or by function, I mean measure. And then you will look at the associated function. Are we sure? Associated function: Are we sure that it must be close to the true minimizer? And this could be some kind of provide some kind of theoretical guarantee of convergence of gradient descent for parameter identification. And the kind of stuff I talked about at the beginning about using Fisher information rather than. Rather than as a function to minimize, kind of suggests that probably we should view the true minimizer as a minimizer of the energy dissipation. So the derivative of R along the gradient descent in Vassar's time space. It's a more likely candidate to be a good control over the distance. So somehow, if you have a fun, if at some point you see that the dissipation of the loss function along the gradient descent is small, is this This end is small. Is this a good guarantee that you have found something that is close to the best possible approximation of your target function? Another kind of question is whether Stein's method can be kind of used as a systematic tool. Yeah, sorry? Yeah. Yes, so in the equation. question uh question one uh there's the i can't yes this is like so the r r of mu r of mu is linear on mu no it's quadratic it's not linear so it's like uh it's like so it's written as an integral of mu but so uh you can write it as a funk as a function of a function of an integral with respect to mu it's explicit you can write it down but it's not linear it's not just an integral with respect to mu just an integral with respect to mu f can be written as a function as an integral with respect to mu and then r is not a linear function in f it's quadratic okay so this this is actually why uh you you end up with a local minimizer is that it's truly is it is it is a kind of a quadratic function and and do we know do we know uh We don't know the form of the density at optimum. We have a PDE description. It can be viewed as a solution of the PDE, but we don't have a form, a closed-form formula. Okay. And it is also known that, so this is the work of Beck and Schizha, that Krasian descent in Wasser's time space. Descent in Vassar's time space converges to this. If the minimizer exists, gradient descent converges to it. Okay, so the second question I was maybe this is kind of relevant to the topics of this conference is using Stein's method as a systematic tool for Loyazevich inequalities. So the gradient Loyasevich inequality is that if you have an analytic function on a complex set, Analytic function on the compact set for any critical point. Sorry, this is not quite right. So, this is only true on a neighborhood of X0. On a neighborhood of a critical point, you can control the power of the function minus the value at the critical point by the norm of the gradients. Of the gradient. And this is a convenient tool to analyze gradient as it is equivalent to being able to control some power of the distance to the set of zeros by the absolute value of your analytic. And there is kind of a question is like these are kind of related to stability of critical points. And you can wonder, okay, can we use Stein's method as a tool to prove such inequalities over spaces of? Qualities over spaces of probability measure. So controlling some kind of distance by the function or the norm of the gradient of the function by variational arguments, ending up with integration by parts formulas, and then use Stein's method to get concrete controls. So some kind of. Yeah, so there are tools for proving Lejadevic type inequalities in such infinite dimensional settings. There are PD-oriented techniques that were developed by Lee and Developed by Lee and Simon with applications to geometric flows. But okay, can Stein's method find a place as a tool for this kind of problems? That's something that seemed kind of relevant, but it's a vague question. Like, yeah, I'm not sure these are good questions. This was more like we were asked to brainstorm about problems. Brainstorm about problems. I tried brainstorming. Yeah. Another question that's kind of related to this, but is more about theory building for Stein's method, is that you kind of have plenty of interesting problems in geometry that are about shape optimization. So you want to, we have different shapes with fixed volume, say, and we want to optimize some other geometric quantities, such as the perimeter or something else. As the perimeter or something else. And stability is to wonder whether you can control bound from below your function by some distance between shapes. And the most natural distance between shapes is the symmetric difference, the volume of the symmetric difference of the shapes. So the symmetric difference is the set of points that are in one shape, but not in the other. And typically, because we only care about the difference between the differences Because we only care about shape, this should be up to a translation. If you care about shape, we don't really care about where we have the shape in space and also up to rotations, but we only care about the browser structure of the shape. And of course, if your two sets have same volume equal to one, Sets have the same volume equal to one, the volume of the symmetric difference is also the total variation difference between the uniform measures on your sets. So you can kind of view this distance as a probabilistic distance. And if you look in the geometric literature, there are plenty of results about controlling natural functionals over space, over shapes from below by symmetric. By symmetric difference between volumes of symmetric difference of sets. So, typical examples, there were some landmark results about stability for isoparametric inequalities by Figali, Maggie, and Quatelli notably that take this form. So, controlling difference of parameters by these kinds of volumes. There were some results by Parasco de Filikitz and Velishkov about Faber-Kran inequalities. There are results about There are results about other types of spectral inequalities and shape optimization, such as the Govainberger inequality. This is a very common problem in geometry, and PD-oriented tools have found a lot of success in that area in the last 15 or so years. And the broad structure of this kind of success is like it's natural to wonder: can Stein's method find this Find a use here in that, okay, if you are, we identify our shapes with the uniform measures over the shapes, then the quantities we want to control, they are integral probability metrics. Can we use Stein's method? Now, the problem here is that the most, at first glance, the most natural way to use Stein's method would be to say, okay, let's come up with the diffusion generator. Okay, let's come up with the diffusion generator, which invariant measures the uniform measure of your shape, the target shape. And let's try to run Schopstein's method with this. And the most natural thing to do would be to use Brownian motion with reflection. It's kind of the first example that comes to mind. But if you start using Brownian motion with reflection, the integration by parts formulas, they have boundary terms. And this is something we never do with space methods. And this is something we never do with Spires method: is to keep like we always try to use integration by parse formulas and classes of test functions so that we don't have boundary terms in the integration by parse. So somehow this is a problem. So I don't know if one should try to add Epstein's method, but include boundary terms. Is it more natural to kind of try to come up with natural diffusion? Diffusion generators on shapes with such that the integration by parse formula for the diffusion generator does not have any boundary term. I don't know what can be the most fruitful, but it kind of feels like there are interesting problems. If Stein's method could be adapted to such a setting, it kind of feels like there should be interesting mathematics to be done if such. To be done with such a tool. Okay, so that's kind of how I ended the brainstorming. So thank you very much for your attention. Okay, so any questions? Yes, I have another question. So from the question. The so the question two in the question two, so what do you have in mind? So I have a very precise problem in mind. All I can see is, okay, the classical Loyazevich theory is in finite dimension. So now space of probability measures in infinite dimensions. So plenty of the tools for Loyazevich inequality. I would like to see a generic tool for proving Loyazevich type inequality. Loyazewich type inequalities for concrete functions over the space of probability measures nice enough. I don't know what should be the analog of analytic function because you don't really want to talk about smoothness over an infinite dimensional setting. You want to talk about more less robust, less constraining notions, but a generic tool for proving Lo Yazevich type inequalities in Wasser's time space. In Wasser's time space. And because we work in an infinite dimensional setting, plenty of the classical tools break down. And there are PD tools, there are other tools that work, but maybe Stein's method could be a good way of tapping for some class of functions. And I don't really have a specific class of functions in mind, but the broad structure kind of feels like the problems might have something to do with each other. Have something to do with each other. So, I mean, I don't know. For example, can we just because do we have some simple examples where we can establish, for instance, can we, for instance, establish, I don't know, Luxabole inequality for Gaussian measure using Spanish method. So Luxabole. So this is not really a Luxabole inequality. Really, a Log Sobolev inequality, this, but this is kind of this inequality is related to Log-Sobolev inequality, it's weaker than Log-Sobolef, but it kind of has the same flavor. It's about controlling one function by another. And this one you can prove it with. This is kind of Stein's method. So, yes, this is an example. But it's not, this is too specific. It's not an enlightenment. I don't know what's the right framework. You have some issues that is questionable with some particular part. Yeah, this is this is more about. We have an example. We have a few examples. We have a powerful tool. In the finite dimensional setting, there is a super generic result, the Loyazevich inequality. So where's the generic inequality? This is just an example. This is not a theory. This is not a theory, but do the examples hit at a examples hint at the broad theory? It would be useful to have Lojazevich inequalities over space of probability measure, a generic tool. And it seems like maybe Stein's method could be a path towards devising an easy to use abstract criterion. Something like a roadmap for a K. Here's a problem. Here are the criterions to check. Criterions to check. Here's the layers of H inequality. This is something that would be interesting, but yeah, I don't have a specific framework in mind. It's examples in search of a general theory. Other questions? I may not. I may have a couple of questions. So Questions. So you are basically trying to find the, I mean, sort of like you have a measure and then you have a functional of that view, and then like you have a minimizer, and then you are looking around that minimizer, right? You want to do it the other way around, which is like, so you have a measure, but then you perturb the measure, let's say, in different ways. So you can basically, for example, take like epsilon times the true measure plus one minus epsilon times. One minus epsilon. Or you can do an optimal transfer perturbation. And then look at the stability of the minimizers of the perturbed one and the true one. Yes. I mean, this is how we derive the La Lagrange equations, really. We write down a perturbation, whether along linear interpolations, whether along Wasserstein interpolations, we have a La Lagrange equation. And the observation is that there are plenty of examples where what pops out is an integration by Pars formula. Yeah, yeah, yeah. Cards formula. Yeah, yeah, yeah. Sometimes very naturally, sometimes just because it doesn't seem obvious. But once you dualize the problem, it works. And my next question is like, you also, I guess I was going to ask, but you also mentioned about the Freyberg Ram inequality. So, I mean, how does this compare with the Freiburg-Khan inequality? Yeah, so what I'm saying is that there are results about the Fabakran inequality. So this is a This is a very nice result by Lasco de Philippis and Velishkov, which states: so Fabern equality states that you look at all sets with fixed volume and you seek to, I always get it wrong, either minimize or maximize the first direct eigenvalue. I always get it wrong in the direction, but okay, you seek to optimize it. And optimal shape is a ball. So that's the Fabergen inequality. So that's the Fabergen inequality. Fixed volume, optimal shape, all. And the stability is that they show that if the you can control the difference between the first direct line eigenvalue and the first direct line value of the ball by some power of the volume of the symmetric difference between the sets. Yeah, yeah. And these were proved by calculus of variations on PD equals. Yeah, so you're basically looking for a different proof. So you're basically looking for a different proof technique for yeah. I'm wondering whether Stein's method can find. So, and this is an example of a broad set of resources. Like this has been a very dynamic area in geometric analysis. They have plenty of results. And the metric they use is an integral probability metric. So, what I'm wondering is whether Stein's method can find a place as a tool for that type of problem. And there is a clear issue is that. And there is a clear issue: is that what, to my knowledge, has not been tackled, is how to handle boundary terms. But yeah, if there were a generic way of using Stein's method for shape recognition, interesting problems, there are plenty of them. Yes, I agree. And probably my last question is like, I guess you also briefly mentioned this at some point, but there are any sort of results opportunity. Do you have any sort of results for perturbations of eigenfunctions? Sorry? Perturbation sort of results for eigenfunctions? Yeah, so yeah, the kind of so one result I should mention, and it's a very nice result, is there is a result of Isabet Mekes that uses Stein's method to show that if you have an eigenfunction of a linear operator satisfying certain properties, Linear operator satisfying certain property that satisfies certain bounds, then the push forward of the volume measure by the eigenfunction must look like a Gaussian. And this is kind of an illustrative example that indeed Stein's method kind of can be connected to eigenvalue and eigenfunction problem. And the point is that if you start looking at the fusion operators, Operators, the Lagrange equation that comes from the Rayleigh quotients of the eigenvalue column, they are exactly integration by parts formula. And are there any further questions? Max again. And we'll meet in twenty minutes for the next talk. 30 minutes for the next hour.               