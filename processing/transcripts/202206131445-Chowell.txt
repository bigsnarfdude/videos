Thank you for the invite. It's my pleasure to be here. So I will be talking about a tool that we have been developing over the last few years. It's an super epidemic modeling framework. And the latest version is what we call an ensemble and super epidemic modeling framework. And the goal of this project is to improve forecasting accuracy, right? And so we have been comparing performance of this. Comparing performance of this framework with some other commonly used statistical models. And I will be talking about that. So before I talk about the actual model, oh, okay. So I wanted to say what you already know and stress the fact that we often work with aggregated data, right? We work with epidemic curves at different spatial and temporal scales, and these epidemic curves result from the aggregation. epidemic curves result from the aggregation of multiple sub-epidemics. And in fact, each epidemic curve is really the result of an epidemic tree, a transmission tree that we don't observe often. And so there may be multiple transmission trees that are substantially different, but could give you the same or very similar epidemic curve, right? So we are working with limited. We are working with limited data, with unobserved data, right? There are some exceptions. For instance, on the right, you can see the transmission tree of the 2003 SARS outbreak in Singapore, where it was very well documented that this happened in Singapore involving several healthcare centers involving patients and healthcare workers. And as you can see in the bottom, In the bottom, this epidemic tree gives rise to this very peculiar epidemic curve with two modes. The first mode is associated with one healthcare, an outbreak in one healthcare center, and the second mode is associated with an outbreak that involves multiple healthcare centers, right? So, if we don't have the information about the detailed information, we would just be working with the epidemic curve. With the epidemic curve, it would be very interesting and important to know what's driving this two-mode epidemic, right? But it's often the case that we have an epidemic curve that has interesting features and it's very difficult to reconcile them or understand what's really causing those patterns. So, one case in specific is the fact that epidemics follow different epidemic. Follow different epidemic growth scaling, right? We know that in the special circumstance where everyone is susceptible and the virus is spreading rapidly through the air, then we have exponential growth, right? But once the epidemic is underway, reactive behavior changes, spatial effects, other heterogeneities that we may not know about are going to affect the growth of the pathogen in the population. Population, right? And even if we know what's exactly causing this epidemic, often we have epidemics that are distorting each other, that are not happening simultaneously, right? In such a way that when they get aggregated, they can give you an interesting pattern. So the textbook example of sub-exponential growth is the epidemic of HIV. Is the epidemic of HIV/AIDS in the United States, where it really followed a cubic polynomial growth pattern. And this pattern was very consistent across different geographic regions and across different ethnic groups. And the Ebola epidemics that we had also not very far ago, far away, the Ebola epidemics in West Africa also follow through exponential growth. Through exponential growth dynamics. In this graph, you can see that the curves are in logarithmic scale, so you can see how the growth happens quickly, but it saturates very quickly as well. And not only that, if you look at the curves one after the other, it looks like this is like a traveling wave, right? In a complex manner, not in a very regular way, but it looks like a traveling wave through West Africa composed of multiple sub-epidemics that. Multiple sub-epidemics that perhaps are driven by rapid behavior changes. As we know, this is a virus that is not spread through the air, it's mostly contact with caretakers. If we move beyond the early pro dynamics, when we look at epidemics, long-range epidemics like the Ebola epidemic that happened in 2008 in Congo, or more recently, the COVID-19 pandemic, we COVID-19 pandemic, we noticed that these epidemic curves are really the aggregation of multiple epidemics, multiple sub-epidemics. And these sub-epidemics could be the result of outbreaks that are happening in different spatial areas within the same population, or it could be the result of transmission happening in a highest risk group, and then the virus finally or the pathogen finally reaches other low-risk areas, right? Areas, right? Or could be the emergence of new variants, which is something very specific to the COVID-19 pandemic. We have seen the emergence of multiple variants that have clearly led to new searches, the most recent of which is the Omicron-dominated wave, right, that we have seen all over the world. This is the COVID-19 pandemic trajectory for the USA, which is the data that we have been using, not exactly this. We have been using not exactly this curve because we have been using the Josh Popkins data set. This is the curve that I just grabbed from the New York Times. But you can see that the epidemic curve, right, that started in 2020 is really the aggregation of multiple sub-epidemics and waves, right? And we need frameworks. If we want to forecast these long-range epidemics, we need frameworks that are able to accommodate this type of dynamics. Dynamics. Just to remind you, this was an op-ed that appeared in the New York Times where they made the point that an epidemic curve could be misleading, right? This is on the left, the epidemic curve in the US, the USA, where you can see that it appeared that it was stabilizing. But on the right, if we disaggregate the curve into two areas, into two different areas, we may see. Areas we may see very different dynamics, right? We can see that one area appears to be growing and the other area appears to be declining. So if we want to forecast this epidemic curve, we would like to know what type of sub-epidemics are driving this type of epidemic, right? In the whole, in the aggregate, in order to be able to generate some To generate some more reliable forecasts. So, this is the building block of the mathematical framework that we have been using. It's the generalized logistic growth model. It's basically the logistic growth model, but with the addition of a P parameter in the C variable, which is the cumulative number of cases. P is the scaling of growth parameter. It helps you accommodate. Accommodate things that are growing more slowly than exponential, right? So, if P is less than one, we can accommodate some exponential growth patterns. The R parameter is the growth rate, and the K parameter is the epidemic size. And as you can see, we can accommodate with this building block roughly bell-shaped curves with different epidemic growth scaling and different sizes. So, the next step is to use this building block to create an overlapping super-epidemic wave model. So, this is a system of differential equations where the building block is the generalized linguistic growth model. C sub I refers to the equation that describes the ID sub epidemic. The parameter R and P. Parameter R and P are fixed, and those are the growth rate and the scaling of growth parameter as in the original superepidemic model. K sub I is the size of the I sub epidemic, and I'll tell you in the next slide how that is modeled, the size of the super epidemics. And here, the most important thing is that the A, A sub I is an indicator variable that dictates when the next sub-epidemic will take off. Okay, it takes off whenever. It takes off whenever the current superepidemic reaches a threshold value, which is the parameter C sub THR. So whenever that superepidemic reaches that cumulative number of cases, the next superpidemic takes off at that particular specific time. So you can see here in the graph on the right, there are three superepidemics, red, blue, and green. The first superepidemic takes off. The first superepidemic takes off, the second one in blue takes off once the first superepidemic reaches the threshold value. And then we have the third superepidemic that takes off once that fixed threshold value happens, right? The gray curve is the aggregation. So that's what we fit to the actual data. So the size of the super epidemics, K sub i, is modeled as a parametric. It's modeled as a parametric function, it's an exponentially declining function. K sub zero is the size of the first super epidemic, so that would be the largest of epidemic. And then subsequent super epidemics will decline exponentially fast with a rate parameter q. So if q is zero, the size of the subsequent super epidemics will be fixed, right? It will be like a traveling wave. But if q is greater than zero, then we will see. Greater than zero, then we will see sequentially smaller and smaller superepidemics. Okay. And at some point, the system will not support additional superepidemics because of that threshold value. The next super epidemic needs to be at least as large as the threshold value in order to take place, right? That makes sense. And this is a nice formula because it gives you. A nice formula because it gives you a closed-form solution for the total size of the sub-epidemic wave, okay, which is given here: k sub dot. All right. So here, with this system, we just need five parameters, right? Three for the building block, growth rate, scaling of growth parameter, size of the super epidemic, plus the threshold value, CTHR at which the next. CTHR at which the next sub-epidemic takes place. And the fifth parameter is the Q, right? The Q that defines the gradual decline of the sub-epidemics. So with just five parameters, we can model many, many sub-epidemics, right? And capture a diversity of epidemic waves, as you can see here. We can capture endemic states, we can capture epidemic waves, growth peaks, or with very long. Or with very long tails, or epidemic waves that have clearly defined multiple modes, as you can see here. In order to fit the model to the data, we use a bootstrapping approach. So basically, we model the error structure in the data, whether that's a normal distribution or a negative binomial distribution, and then generate multiple realizations of that error structure to generate the uncertainty. To generate the uncertainty of the parameters and to assess parameter identifiability, etc. Here's an example with the SARS outbreak in Singapore that I showed you earlier. It's nice to see that the framework actually captures the two sub-epidemics, the two sub-epidemics or the two modes in this epidemic curve. And as you can see here in the panels, the histograms show the uncertainty of the parameters. Uncertainty of the parameters of the model fitted to this data: the growth rate, the p parameter, the k parameter, and the q. And it gives a relatively good fit, as you can see. So when the pandemic started in early 2020, we started to use this framework. We had just published the first paper on this in BMC Medicine. So we started to generate forecasts using this framework. Framework for the United States, and it was working pretty well during the first few weeks, during the first few weeks of the first wave, right? Just before the resurgence in the summer, we assessed the performance of the model against simpler models like the Richards model. And as you can see, all of the performance metrics, the mean absolute error, the The mean absolute error, the MSE, the mean interval score, the coverage rate of the prediction interval, they all outperformed the simpler version of the model, which is good news. But then we started to see that in the summer, there was a resurgence that was overshooting the earlier wave, right? So the framework was not ready to take this. So we had to make it more flexible because, as you remember, for this framework, As you remember, for this framework, the first of the epidemic is the largest, and then the next two epidemics tend to decrease because of interventions and so forth. It doesn't account for the possibility of relaxation of social distancing effects or the appearance of new variants, right? So, in order to accommodate that, here's an example of that. We modify the model, and to avoid confusion, this is what we call the end-sub-epidemic wave model. We call the end superepidemic wave model. But in here, the growth rate, the scaling of growth parameters are independent for each superepidemic, as well as the size of the superepidemics. So we no longer model the super epidemics, the super epidemic sizes parametrically, but we actually fit them to data. So that means that depending on the number of super epidemics that we need, that will scale the That will scale with the size of the parameters, or say I can say the other way around: the number of parameters scale with the number of sub-epidemics. It's three times n with n is the number of sub-epidemics plus one, right? So, if we need two super-epidemics, we will need to fit six plus one, seven parameters, okay? So, you would need substantial amount of data in order to generate, to derive a good fit. Derive a good fit to a well-calibrated model to the data. That's what I want to say. Okay. But the idea is the same. The next super epidemic will take place whenever the current super epidemic reaches the threshold value of THR, which is fixed. Okay, so we can see how this works. And because this is an end-subepilemic model, let me go back to make a point. Let me go back to make a point: is that you can try two super epidemic models, you can try three super epidemic models, etc. etc. Right, there might be situations where three super epidemics may be required, but most of the time, if you use data for the pandemic for the last three months, we're using daily data, two sub-epidemics is sufficient to characterize the epidemic curve. Curve okay, so model selection. Given that with this framework, we have a family of superepidemic models, depending on the number of epidemics that we want to use, we can rank the models from the top to the worst, right? And we can select the best model or the top ranking models. That's a nice feature of this framework, and we can do that by using the AIC. By using the AIC, right? Which is a very well-known metric to assess the ranking of the models. And the value of the AIC depends on the number of parameters and on the number of data points. The next step is to create ensembles from the top ranking models. We have observed that about the four ranking that about the four top ranking models is sufficient to generate ensembles that are very competitive. So we can take, for instance, the first two top ranking models to generate what we call the ensemble two. Ensemble two means the top two ranking models. Ensemble three would mean the top three ranking models. Ensemble four would mean the top four ranking models. Okay. And so we can use Okay, and so we can use them, assess them, how well they perform in forecasting the COVID-19 pandemic. So that's what we did. This is just an example with simulated data, just to show you how the model is able to fit its own data. This is data simulated with a high level of noise. We added here a lot of noise, negative binomial noise, with a lot of embryos. Noise with a lot of dispersion. Now we can use it with actual data so that you can see how from top to bottom you can see how the model is fitting and detecting the resurgence in the summer of 2020. And these changes are daily changes from June 18, June 19 all the way to June 23rd. You can see how the model starts to detect as we add one data point, right? Data point, right, starts to detect that resurgence and it's able to feed better and forecast the trend. And these are the corresponding what we call super-epidemic profiles. So in blue and red. Okay. The previous one is just the uncertainty. The gray line, the gray area is the 95% prediction interval. And I will show you a few of I will show you a few of the forecast on case data that we have generated just to give you a sense of how these are 30-day forecasts, right? Yes, how the model has been performing. And I'm showing you not only the forecast, which is what you see after the vertical line, but also how well the model fits the action data first, the last 90 days of the trend. 90 days of the trend. And as you can see, it actually accounts for that slowly declining epidemic waves as well. Okay, this is the profile for the very last one forecast that I show you. Okay. Okay, so now we want to compare this against. We want to compare this against standard models, right? We want to compare them against good models, models that have been used commonly out there, right? Not with necessarily naive models, but models that are very well used, such as the ARIMA models that we know have been used widely in finance, in nature, in problems in biology, in weather as well. So, what we did was to use the R. To use the R package forecast in R that can give you this forecast very quickly. And what we do is to estimate three parameters associated with the ARIMA models. And we try two different flavors of these ARIMA models. Why do we do this? Because the ARIMA model doesn't can give you a prediction interval for the forecast that reaches into negative values, right? So in order to limit So, in order to limit that issue and give some a little advantage to the ARIMA model, what we use is log transform data. So, we use log transform data with the ARIMA model, and then we take the exponential in order to bring to transform back the data and avoid negative numbers. That's the first flavor that we call logarima. The second version, what we do is to truncate the negative values and make them zero. The negative values and make them zero. The negative values of the forecast basically will make them zero, and that's it, right? Because the negative numbers are not realistic in the system. So it's important to tell that it is possible that the actual coverage probability of the prediction intervals of the second flavor of the ARIMA model could be smaller than the nominal value, right, which is 95%. Okay, so. Okay, so what are the performance metrics that we used? We used the classic ones, and I'm very happy to see that in forecasting challenges and in the literature, the interval score metrics, such as the mean interval score and the weight interval score, which are proper scores, are being used more widely and given more importance. The absolute error and the mean square error basically are measuring how well the model is fitting the data, right? Fitting the data, right? But do not account for the uncertainty. If we want to account for uncertainty, we need to use the weighted interval score or the coverage of the 95% prediction interval. The weighted interval score is even better because it accounts for how width, what is the width of your prediction interval, right? If the data points are too far away from your prediction interval, that's important. That's going to penalize. That's going to penalize your prediction. Okay. So it's accounting for how many data points are covered and how far away those data points are from your prediction interval. And it uses the full probability distribution of your prediction in order to generate the waiting interval score. And so those are the performance metrics that we use. So, in terms of forecasting strategy, what I will present What I will present in the next results is the results when we use the daily COVID-19 deaths reported for the USA reported from the Johns Copping Center for System Science and Engineering, right? And this is from February 2020 all the way to March 2022. That includes even the omicron dominated wave. And we computed short-term forecasts using the top four rankings. Forecast using the top four ranking super epidemic models using n equals two super epidemic models with up to two super epidemics. And then we also assess the ensembles, the top top, the top three ensembles that use two or three or the four top super super epidemic models. And then we compare with ARIMA models, right? And for each of these forecasts, we use a And for each of these forecasts, we use a 90-day calibration period. So there were a total of 98 forecasts that we generated. Forecast, the first forecast is from 20 April 2020, and the last forecast is 28 February 2022. Okay, so again, this is daily death curves, the daily death curve. So it is week to week. So the calibration period. So, the calibration period is using for each sequential forecast is including seven additional days of data than the previous forecast. This is a representative fit of the top-ranking super-epidemic models. This is the very first forecast, okay, that we included. This is the one for April 20, 2020. I wanted to show you this one because I wanted to show you this one because you can see on the left, right, the model fits look very similar, all of them. But if you look at the profiles in the middle for the top rank model, and then the next one is for the second rank model, and the third rank model, the fourth rank model, they look different, right? The sub-epidemic model, the sub-epidemics are showing slightly different or substantially different dynamics, even when Even when the full fit, the aggregated fit to the data, look very similar, the pidemics are different. So, this suggests that the forecast will look different, right? Because if we extrapolate the trends of these super epidemics, the forecast would look different. And so, we want to have that diversity in the forecast so that we can use it in the ensembles and hopefully improve. And hopefully, improve forecasting performance. So, I also wanted to show you this graph with the parameter estimates for the top-ranking super-epidemic model, where it shows that the estimates in the top are for the first super-epidemic, and the estimates on the bottom panels are for the second super-epidemic, just to show you that the parameters are relatively well estimated from the data. Estimated from the data. Okay, not really, not problems with identifiability issues. Now, this table compares the top ranking super epidemic models and the first three ensemble models from the superpidemics with the ARIMA models, with the four performance metrics that I show you. So you can see that the ensemble. That the ensemble models are performing very well. For the 20 days ahead forecast, the ARIMA model is a little bit better. The ARIMA model where we truncate the negative values is a little bit better than the ensemble model. But in terms of the metrics that account for the uncertainty of the predictions, which is the prediction interval, 95% coverage of the prediction interval. Prediction interval and the weighted interval is scored, which is the most important one, the ensemble models outperform substantially the ARIMA models, right? So that's good to see, because as we know, the ARIMA models are commonly used statistical models. And if we can outperform them, that would mean a substantial improvement in forecasting accuracy, at least in the context. Accuracy, at least in the context of COVID-19 in the USA, using the COVID-19 deaths curve. Now, if we disaggregate, what I just show you in the table is the average, but if I show you all of the performance metrics, right, for the 98 forecasting periods, this is what you see here. In red is the sub-epidemic ensemble for, so the top performer. So the top performer, right? And you can see that the red curve is frequently below, right, the blue curve in terms of the weighted interval score. In fact, it's 66% of the time is outperforming the ARIMA model. Okay. And in 88% of the time, in terms of the 95% coverage of the prediction interval. So you can see also that there is no particular long-range trend, but most of the time, the super epidemic and SEMOL4 is outperforming. Outperforming the ARIMA models. The previous one was log ARIMA, this is ARIMA. Results are similar. I wanted to show you the forecasts associated with the feed that I showed you earlier as well. So you can see how the forecast from the first rank model includes a declining phase. A declining phase. Sorry, for the second rank model, it shows a more stable pattern. For the third rank model, it shows a little decline and some stability. And so when we use these top-ranked models and construct the ensembles, this is what, oh, by the way, these are the profiles for those forecasts. But be ready for the next slide. This is for the ensemble models. For the ensemble models. Okay, so the ensemble two constructed from the top two, ensemble three constructed from the top three super epidemic models, and then sembl four constructed from the top four super epidemic models. As we increase the number of super epidemic models, performance increases. And that's a nice trend to see. And that's in line with other studies. Other studies that have shown that ensemble approaches can improve epidemic forecasts, right? Accuracy. Okay, so as a way of summary, just to say that the ensemble superepidemic model is quite promising, has outperformed the ARIMA models in weekly short-term forecasts from the beginning of the pandemic until the recent Omicron dominated wave. Dominated wave. It's nice to see that forecasting performance improves as we include additional super-epidemic models into the ensemble. And so we limit this study to the top four super-epidemic models, in particular because adding additional ones, we didn't see a substantial improvement. So that limits also computational. It's also computational, the computational time necessary to run all the forecasts. And this is a framework that could be useful in another setting, in other settings, in biological problems, in social problems. We are thinking about applying this to other types of epidemics, such as the epidemic of lung injury associated with vaping and also to the spread of viral. And also to the spread of viral information in the internet. We are making code and all of the performance metrics and all of the individual forecasts available in a GitHub repository. And yeah, I'm looking forward to your feedback. Last but not least, I would like to acknowledge my collaborators, my students at Junior State University. Actually, I'm Natalie. Actually, Anna Tariq just graduated. So she just moved to Stanford University to Strat Postdoc. Kimberly Russa has been a postdoc at the University of Tennessee for a year. And my student, Sushma Lakal, will be finishing in about a year. She just delivered a beautiful baby. So contradicts to her. And my long-term collaborator and mentor and friend, Mark Hyman. And my collaborator, colleague at Georgia State University. Colleague at Georgia State University is Ruya Luo, who is a wonderful statistician and scientist. Thank you very much.