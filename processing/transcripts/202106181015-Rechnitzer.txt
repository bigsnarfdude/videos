So, just want to make sure everyone can hear me okay and all of the usual bits and pieces. Okay, and the slides are visible. Great. Yes, yeah, everything is good. Yes. Okay. So I'd just like to, well, I suppose I should start by thanking the organizers, and I'll do a bit more of that near the end. But this is work that's been, you know. Work that's been stopped, started, interrupted by many things, not least a certain amount of global health crisis. But this is work about trying to preserve topology while sampling and sort of some of the interesting things that we've looked at along the way. And this is work together with Nick Beaton, who is online here somewhere, despite radical time zone difference and small children at the University of Melbourne, and Nathan Clisby, who is And Nathan Clisby, who was at University of Melbourne, has now been at Swinburne University, also in Melbourne for a little while. Okay, and this is work in progress, so I hope I have some concrete things to tell you and some things that could just be work in progress, let's say. Okay, so very, very broadly, and I thought I'm trying to come to this particular work with sort of very big questions. And the big question is, how does to The big question is: How does topology influence geometry? And connected with that, you know, what does a trefoil look like? And then you think about that for a little while, and you realize very quickly that you have to ask, well, which trefoil? And so to make this question somehow sensible, you just have to define a probability measure on your space of closed curves and use that probability measure to define what a typical trefor looks like. Looks like. How hard could it be? Well, you know, I think everyone in this room, virtual or otherwise, realizes it can be very not easy. And so here are two of my, I guess, favorite measures, sorry, one of which I've worked with for many, many, many, many years and one of which I'm relatively new to. So the first is self-voiling polygons on the simple cubic lattice. And so this is just the embedding of a simple closed curve. This is just the embedding of a simple closed curve into the z cubed. And the probability measure you take is that each embedding up to translation is equally likely. So each embedding of n edges, n would have to be even here, is equally likely. Okay, so there's one measure on the set of closed curves. And you can take another measure here, which hopefully is going to work. There we go. And this is equilateral. Go. And this is equilateral random polygons in R cubed. And here each edge has unit length, and the edges are chosen uniformly on the two-sphere and then conditioned to close. And you get a nice thing here. And one of the things that we should see immediately from this is not only have I changed colors between the two pictures, but that the self-abotting polygon is a bit more open. The fact that you can't have vertices of degree higher than two means that the thing kind of self-repels. That the thing kind of self-repels a little bit and swells up a little bit, whereas the equilateral random polygon, the vertices can be infinitesimally close to each other, and these things are more compact. And I've done a lot of mucking around on space of self-loading polygons, and I'm quite new to equilateral radon polygons. And indeed, this conference, or perhaps the last one of these conferences that I attended, many thanks to the organizers of that one, got me really interested in that particular measure and what we might be able to do. In that particular measure, and what we might ever do there. So, one thing you realize very, very quickly when you play with these games is that analytic results are really hard, and there's lots of really wonderful work trying to get and getting some analytic results. You know, some of these people like Whittington, who some of you may be familiar with, with Sumner's, Kinlet, Soros, Van Rensburg, Orlandine, Gucci, Canderella, Nicholas, Grossberg, and so forth. And, you know, there's an excellent review here, which is linked in my talk, which gives a Which is linked in my talk, which gives a much more complete list because I'm sure I've forgotten a lot of people. But there's a lot of interesting work, and it's really damn hard to get analytic results out of these things. And so, you know, we have to do what we have to do to get some answers, and we resort to random sampling instead. And there are sort of two broad ways one might do that. If you're trying to find a trefoil, is you might sample a superset of trefoils and then sieve out just the ones. And then sieve out just the ones that you want, namely the ones of the correct NOT type. The other broad approach would be: well, just try to sample within that space. So just sample trepholes without stepping outside into these broader topologies. So two very, very broad approaches one might take. So, you know, if we think about sampling a superset, so perhaps sampling all self-avoiding polygons or all equal actor random polygons. Equilateral random polygons. That might be one way to do this. So there's a really beautiful work by Jason and co-authors giving exact random sampling of equilateral random polygons in R3 in time n to the five halves to produce a completely independent equilateral random polygon. Another option is to use the pivot algorithm. This has been used a lot on the cubic lattice by people like Whittington and Sotoros and so forth. And Sotoros and so forth. And you can use the pivot algorithm on self-ordering polygons of fixed length. And this goes back to Lie and then some very, very good analysis by Madras and Socol, more work by Madras then. And one of the things that's very appealing about the pivot algorithm, not for polygons, but for walks, for open chains, is there's this really wonderful piece of work by Nathan Clisby, which gives you an algorithm that works in logarithmic time to sample a statistically independent. To sample a statistically independent walk. And so, if you think about this for a moment, you realize that it means it's faster to sample a long self-avoiding walk than it is to write it down, which is very counterintuitive that you might be able to do that. But the algorithm itself, the actual basic moves are very, very simple. You know, we take a self-avoiding polygon like this one here. We pick a couple of vertices on the polygon, and then that gives us, splits the thing into two segments. So, we should probably take the shorter segment here. So, we should probably take the shorter segment here, like this, and then we perform a symmetry operation on that segment to get a new segment like this. And if the result is reasonable, then we keep it and we discard the old one. And if it's not, then we stay in place. And so, you know, this, one of the things you should see probably immediately from this is this particular pivot we've done here, which is just a rotation by 90 degrees, this has changed the knot type. Has changed the not type. Now, that's perhaps what we don't want to do later on in this talk. But one of the things that does do is it works very, very quickly. Of course, if you are sampling in this superset and then sieving out just the ones that you want, you have to be able to identify those ones that you want. And I'm sure everyone in this room knows, you know, not identification is very hard. I mean, it's hard in some provable sense, but perhaps worse than that. Sense, but perhaps worse than that is that typically non-invariants are kind of slow to compute, which is also not good. And we also know that if we are trying to get out, for example, trefoils, we know from pattern theorem results going back to Whitners and Summington, Sumner, Sumner's and Whittington and Pippinger, that a given topology will be exponentially rare in the space that you're exploring. So while you might be able to find trepholes at moderate length, Able to find trefoils at moderate length, they become harder and harder to find at large lengths. And so you'll end up throwing away a lot of your data if you're trying to hunt down just trefoils for large polygons. And indeed, this is the primary bottleneck that we face at the moment when we are trying to study large polygons, large systems with fixed topologies. The identification is the bottleneck. The work of Jason, for example, You know, the work of Jason, for example, gives us this really wonderful way to very, very quickly get large polygons. It's just that a large polygon has lots and lots of crossings when you project it. You can get rid of some of those using various techniques, but ultimately it's just going to be very, very hard to ID, and that's going to take a very long time. And so as an aside here, which was in my slide before I heard Eric's talk, if we do have these very, very large polygons, what's a really good way to measure trefoil-ness, for example? That's just an aside. For example, that's just an aside, completely irrelevant to here, but just on the topic of what do we do with large polygons that we can't ID? Now, if we sample just fixed topology, we have this wonderful method, BFACF, dating back to the early 80s. No topological testing is required because it's ergodic on a given not type, which is results due to Bucks and STU. And the way it works is we just take a small seed configuration of the desired topology. Configuration of the desired topology, and we basically just do some local deformations to that thing, and it can be allowed to grow and shrink. We try to tune it nicely so that the growth and shrinking is balanced. And then we get what is a random walk in the length or the size of our system. And that works very, very nicely for sort of moderately sized systems, but it does mean that it can take a very long time to sample large systems because we are doing this random walk-in length. Are we doing this random walk-in length? We spend a lot of time perhaps not where we're interested up here at large configurations and instead spend a lot of time just getting up there. Okay. Fixed topology two, if we have basically fixed lengths, well, there's a restricted pivot algorithm, which is a pivot with excluded area algorithm, which is due to Zhao and Ferrari around 2012. And what they And what they do is they take their polygon and they attempt a pivot and they consider the area mapped out by those segments, the before and after segment, and they look at that area in between and the pivot will be accepted if nothing crosses that area. But it will be rejected if, well, the simple way is if any other edge of the polygon crosses that surface area, you reject it. I think you can also do something a little bit more clever. I think you can also do something a little bit more clever using linking numbers. But one of the problems with this approach is it's extremely computationally intensive, at least the way that they set it up. And so they only considered this pivot algorithm with short segments, namely segments of length less than or equal to five. And they claimed that their algorithm was probably okay for moderate polygon sizes, but we know that this is not going to be ergodic for large polygons, and that's a result going back to Madras and Sokol, namely that. Going back to Madras and Sokol, namely that you have to have global moves or very large-scale moves when you are at a fixed length, when you're sampling polygons of fixed length. Okay, so what can we do to try to speed things up? That's really what we're trying to do here in this piece of work. Well, we can revisit pivots again to reduce the computation, to try to think about how we can make this as fast as possible. And there's a couple of things that we can do to help us think of this. One is to not think Help us think of this. One is to not think of pivots as sort of literal, you know, this chunk of polygon is moving sort of continuously in three spaces. We have to worry about everything in between. No, we just think of a pivot as sort of it starts here, it ends here, and we can choose any continuous deformation get us from one to the other. The other thing we can try to do is use this Clisby log n method of pivots to try to get this thing working much, much faster than a naive implementation. And really, the way that this works is you make some storing of This works is you make some storing of the polygon and its symmetries in a binary tree, and you do, and this is really important, but you do it as much as you can to not write anything down. So you use as much lazy evaluations as possible. And actually, as an aside, this is actually quite close to what is going on in Jason's encoding of polygons in terms of a triangulation. Okay, so we can think of two sorts of pivots here. We could have an inner pivot, if you like, which is just perhaps the one that you're used to. So I'm just going to sort of So, I'm just going to sort of skip through this to save some time. We have an inner pivot with just this local thing. We can also maybe have an outer pivot where we take the pivot segment, pull it off to infinity, rotate it up here and push it back. And there are reasons we might want to do that. Indeed, this made it thinking of Eric that made me think of this as perhaps as a good idea of something to try. Thank you, Eric. So, stuff you can do there to work out how that works. Works and if you do a naive implementation of these moves, it's going to be about order n squared or maybe n log n if you use some clever things like you know stromosh Hoy algorithm for detecting intersecting segments. Now, all of that to one side, one of the things that's very good about the pivot algorithm, even though it might seem like you have to do a hell of a lot of checking to make it work, typically when you attempt a move, it fails. When you attempt a move, it fails very quickly. So you can make lots of attempted moves very, very quickly and just reject them all because it's very quick to reject. And then even though it might take a long time to find something that works, when it does work, it makes this big global move, a global change to the thing. And so it should move you around the state space of your polygons very, very quickly. Of course, one thing that one has to do to quantify that is look at autocorrelation time. That is look at autocorrelation time and see how that works. Okay, so to describe this very sneaky way that Nathan gets everything to work, I want to sort of argue by analogy here for a moment. So consider the product of three real numbers, x to the a, y to the b, z to the c, so that the numbers x, y, and z are changed very rarely, but the numbers a, b, c might be changed often. How should you compute that product to do it as efficiently as possible? And the standard As possible. And the standard sneaky approach for something like this is to basically pre-compute powers of sort of powers of two, namely y squared, y to the four, y to the eight, y to the 16, and so forth. And then when you're asked to compute y to the b, you do it as a product of these pre-computed powers. And this allows you to compute these products in logarithmic time. And that's roughly what's going on inside this Clisby approach to computing pivots. Computing pivots. And so you take your polygon and you store it as a sequence of symmetries and edges. So you have a sequence of unit steps and the symmetry needed to rotate those unit steps into place. You can compute then the position of any given vertex as a sum of products of symmetries acting on those edges. And so you really just have a little bit of algebra here. And by juggling this algebra around carefully, you can use something very like that thing I just described. Something very like that thing I just described with the computing the product of things in order to simplify and reduce the amount of computation you have to do. So, by some careful pre-computation of symmetry products and symmetries acting on vectors, you can really reduce the overall amount of complexity required. I'm just going to check the time in order to compute things. Okay, so for example, we take our polygon and we store it in a tree. Now, what does that mean? Well, at each Does that mean? Well, at each leaf of the tree, I'm going to store the symmetry that rotates each given edge. So, you know, edge zero, I have to rotate it by q0, edge one, I have to rotate it by q1, edge two, I rotate it by q2, and so forth. And so that's what's stored in all the leaves there. And then as I go up the tree, I'm going to store the product of those symmetries in the obvious way. This will be q0 times q1, q times 2 times q3. times two times q3 q naught times q1 times q2 times q3 and so forth so there's a bit of work to do to set up this tree initially but i don't just want to store those symmetries i also want to store a position vector which is going to be the vector that goes from the start of that edge to the end of that edge and then as i go up the tree say up here at this vertex here i want this to store the vector that starts from the the beginning of the very first edge that's underneath that First edge that's underneath that to the end of the edge that's underneath that. So we store all of this information in the tree, we carefully pre-compute all of that, and then we can compute the position of any given vector from that or position of any given vertex from that information. So for example, if we want to compute the actually, I won't do this one, I'll do the next one. Let's do this. If I want to compute the position of the sixth vertex, that's the end of the fifth edge. So I start. The end of the fifth edge. So I start off here at this leaf and I traverse back up the tree taking information. And the way that I actually understood this properly was to actually write out this X6 as this product and then start grouping things together carefully and realizing with a little bit of work that, hey, oh, I've pre-computed this bit and I've pre-computed that bit and that bit and that bit. And it turns out that if you pre-compute things in the right way, then you only Way, then you only require sort of one or two operations at each step up this tree. And so that gives you this log n stuff to make it work. And if you are going to do a pivot on a polygon here, you pivot two vertices, you just update coming up from these two vertices. So if we're pivoting between vertices three and six, we go back up the tree, updating things here, and back up the tree, updating things there. And so recomputing that data. Pre-computing that data to put in a nice big table of pre-computed stuff is also log n time. And if we don't do any topology checking at all and we're just sampling equal actor random polygons, this actually gives us a faster way of compute, of sampling equal actor random polygons. So if you do no topology checks and just this data structure I've described, then you get something which seems to have an autocorrelation time of roughly log n. And so we get equilateral ratio. And so we get equilateral random sampling, or equilateral random polygon sampling in sub-linear time. So, again, it takes longer to write down the polygon at the end of your sampling than it did to actually sample it. So this is at least a nice result that's coming out of this work in progress. Now, to do the topology checking, you need lots of basic vector and quaternion manipulation, you know, stuff that's pretty straightforward. You need bounding sphere construction. You need bounding sphere construction. So, how do you combine bounding spheres around a set of edges to make a bigger bounding sphere? You need spheres intersecting each other and spheres intersecting sphere-capped cylinders. And you need segment quadrilateral intersection testing, which is Moller-Trombed. These are standard technique from ray tracing. All right, now, how am I doing for time? Let's just check here. I'm not doing Just check here. I'm not doing so well. I've got a bit longer, I think. So maybe I will show. Maybe I won't show this. I mean, I'll get to just actually, I will show this. Why not? So if we try to understand how we do actual intersection checking to see if when I pivot a chunk of the polygon to a new position to make sure that I don't cause any strand passages as I go along, I have to check that this bounding sphere where it starts. Bounding sphere where it started, when I drag it across space, that maps out a sphere-capped cylinder. And I need to check: does any other chunk of the polygon intersect that? So I'm looking at sphere intersecting a sphere-capped cylinder. And the way that I've drawn this here is these salmon-colored edges are represented by this chunk of the tree here, and these pale blue edges, spounding spheres represented by these notes. Spounding sphere is represented by these nodes here. And what we do is we refine as we go to try to understand: hey, which bits might intersect which bits. Okay, so I'm starting off here and here, and I'm computing the sort of dragged cylinder thing under these vertices and here, the sphere under these vertices. And so the first thing I do is I realize: hey, these things do intersect, so I need to check a bit more carefully. So what I do is I see. Carefully. So, what I do is I step down the tree to split that bounding sphere in two, and I check both halves. And I realize that, hey, this stuff here doesn't intersect anything else. So I can throw it away and I don't have to worry about it anymore. So then I can go and check the other half of this tree and I realize, hey, that does intersect this thing that I've dragged across. I have to check more carefully. So now I split the other thing and I realize, ooh, hold on, that's still insects. Ooh, hold on, that still intersects. I might have a problem here. And so I do some more refinement and I keep refining in this way. I keep splitting each of these objects. This is sort of an octree, quad tree sort of game, and until I get down to a single edge in both cases. And then I check that more carefully using Molotron there or something like that. Okay. Big question: does this actually work when you actually code everything up? Well, the answer is topology is definitely conserved. So here is. Definitely conserved. So here is a 1024-edge square that I've pivoted about successfully 250,000 times, and it's still an unknot. And I should sing out, where am I? I should, as an important aside, sing the praises of the Topoli library to Joanna and her group. It's an extremely helpful tool for playing these sorts of games. And I would not have been able to debug my code without it. But anyway, you can see you get an unknot after lock. Get an unknot after lots and lots of pivots. This started as just a unit square and then pivoted a lot. Okay, well, does it work? Compare the RG histograms that you get to make sure that if you compute a whole bunch of unknotted ERPs using either some code from JSON or code from this Topoli library, and then you compute a whole bunch of unknots using this pivot algorithm, that you get the same thing. And you can see that the histograms here line up reasonably well. Line up reasonably well. Okay, so there's close agreement at that level. But does it work? Is it actually fast? And so this is really where I'm at research in progress. And here, autocorrelation is everything. How long does it take to make a statistically independent sample? And one of the things that happened immediately when I was doing this was I had these nice pictures. I thought, oh, this is great. I'm getting reasonable estimates for RG. That's great. Let's compute the autocorrelation time. And then I realized that I was having a huge amount of. That I was having a huge amount of difficulty actually computing it. So I looked using the windowing method, using some standard Python module. I used this very nice log binning method, which I highly recommend. Anyone that has to play this game of computing autocorrelation times. And I just couldn't get a good estimate of the autocorrelation time out of it. And I think this is pointing to a few possibilities. So to show you what I mean, here is a plot of RG. Is a plot of Rg, the race of gyration, against time, as in the number of iterations that I'm doing. And you can see here that the Rg estimates are bouncing around wildly, which is good. That's exactly what the pivot algorithm should be doing. It should be making big changes to these unknotted polygons, moving me around state space a lot, sampling well. And then you look at this diagram a little bit more and you realize, okay, that's bouncing around nicely, but what's this thing here? This thing here, like it seems to sort of dip and then keep going. And you can see here, it's bouncing around nicely, and then there's this dip and sort of like a pause and then keeps going. So those canyons, as it were, are kind of worrying. So you look a little bit further. And so this is again at length 256 unknot. I'm plotting the log of the rays of gyration. I'm showing every a thousandth iteration here. And you look at this, and you can see these same canyons here. And you can see these same canyons here, same canyon, and then you see this big, big canyon here where the polygon is sort of bouncing around nicely and then somehow it gets compact and then struggles to get out of there again. And that's really worrying because it shouldn't do that. Not if I'm sampling well, not if I'm producing statistically independent samples in this algorithm that I've been working on for quite a while. So, possibility one of things that are going on. There's bugs in my. Things that are going on, there's bugs in my code. Okay, this is a very distinct possibility. Possibility two, which I hope is maybe a more scientifically interesting one, is that compact conformations in the equilateral random polygons that are unknots and very, very compact are actually not so rare. So here is a very compact unknot generated by that pivot algorithm. And you can see it's very, very tight. Very tight, very hard to pivot away from. Surprisingly, it wasn't so hard to pivot into. By contrast, this is roughly what a typical one looks like. You can see it's a bit more open, has these little compact regions around the edges, but overall it's more open. And here's a very open version of the same thing. So again, very, very open example. And I got these from one run of my code. But I think the problem is that these guys here on the left just aren't that rare in this space. Space. Of course, this does not exclude possibility one. So, one of the things I wonder is that is somehow topological swelling to blame for what's happening here? So, to explain this a little bit more, if we look at how we expect equilateral random polygons, the whole body of them with no topology checking versus unknotted polygons that are equilateral random polygons, they scale differently. We expect their sizes to be different. All equilateral random polygons should be compact kind of space filling. Compact kind of space filling. So they have this metric exposure of a half. We believe, but I don't think anyone's shown adequately that when you fix the topology, they should swell up to get into something like the self-avoiding walk university class. They should be more open and bloated. By contrast, if we do look at self-avoiding polygons, all self-avoiding polygons and all unknotted self-avoiding polygons have the same metric scaling. So perhaps by doing the pivotal. By doing the pivot algorithm, I'm trying to find this very different subspace inside the ambient equilateral random polygon space. So is somehow this topological preservation in pivots, is it, you know, messing up the ergodicity in a way that doesn't happen for self-avoiding polygons? Or are these just bugs in my code? I'm hoping it's something to do with the former and maybe not so much to do with the latter. So, what now? Well, there's a lot of things that are going to be a What now? Well, there's a lot of debugging in progress. I'm hoping to get Nathan and/or Nick to code up the algorithm independently to try to remove some possibilities of bugs. I think Nathan is doing this. Nick has good excuses for not doing this. Maybe to use Wang Lando algorithm to speed up diffusion across this size space. And I've got some of that in progress with various degrees of success. Maybe better encodings of the polygon, other Jason Cantarella and his nice sort of. Cantarella and his nice triangulation game. Maybe consider mixing in BFACF moves with pivots. So you have triangles going to quadrilaterals. I think, so Nick, Nathan, and I did a lot of work trying to work out how to make BFACF work. And we think we know how to make it work without pivots. It'd be nice to try to mix it in together. And there'll be a lot of fun here with data structures and lazy evaluation and things. I think there'll be a potentially very interesting project, at least from a sort of data structure point of view. Data structure point of view. Maybe put in some volume exclusion. So maybe either put volume exclusion into the pivots that I'm doing already, or to do things on the lattice. But there's a problem there because when you do things on the lattice, even just picking two vertices for your pivot segment is non-trivial. It's not a constant time. So at that point, I'm just going to say thanks for listening. And of course, a very, very big thanks to the organizers, Karma, Elani, and The organizers, Kama, Eleni, and Marielle. And I will stop there. Thank you very much for this beautiful lecture. So we have some time for questions. We are a little bit over time, but it's not your fault at all. We shifted things. So if there are some questions and we can continue during the wrap-up if there's more discussion, that definitely looks like you have motivated a lot of interesting questions. Thank you. Of interesting questions. Thank you. So please go ahead.