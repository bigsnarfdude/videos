So what I wanted to talk about today is multi-level Monte Carlo method, which I'm assuming not that many people know about, so I'll go through what that is, and then some kind of linear algebra aspects that come up there. So what the smoothing is, will hopefully also become clear by the end of the talk. And this is joint work with my PhD student, Anastasia Istatuka, who is also in everybody. So the So this is the outline of the talk, so I'm just going to start with a very short motivation, and it's going to be a very similar setup to the two talks that we've seen before, so which PDEs with random coefficients, so I can be quite fast there. I'm then going to talk really about the circulant embedding method. So Elizabeth already mentioned that in her talk as well. As well. Then, of course, these multilevel Monte Carlo methods and how we can use circulant embedding there. Then we'll get to the new parts, and the new part of the talk is really in this smoothing technique. And then I'll show you some numerics in the end. So the motivation for this, so the problem that we're really interested in, is this PDE with random coefficients. So you've seen this before in Elizabeth's talk. So we just have this linear elliptic. We just have this linear elliptic PBE where we now have a coefficient kappa that is a random field. And so distribution U also becomes a random field. And this has applications, for example, in subsurface field modelling, as mentioned as well. So in that case, this is a very simple model for the flow of water underground. And the reason that you model the diffusion coefficient k at random is that you don't know exactly where different types of Where different types of rock are located. So, this K in this context is the hydraulic permeability of the rock. So, it depends on which type of rock is where, and that's just something that you don't really know, and so you model it as a gram flow. So, what we're interested in in the end is usually computing the expected value of some quantity of interest. So, this is the expected value over the distribution that you put on your random coefficient k. This could be something quite simple. This could be something quite simple. We're just interested in knowing what the pressure of the water or the u is at some point in the domain. It could be more complicated things for outflow over parts of the boundary or travel times of contaminant particles. So this Q can really be as simple or as easy as you would like. And having the expected value here, I mean that does really include expected values. It also includes higher order moments if you want something like variances or even probabilities. Or even probabilities, that is also included. You just need to adjust the definition of your Q accordingly. So we are going to be using sampling-based methods. So what we are going to do is we are going to solve this PDE sample-wise. So for one particular realization of the random parameter omega i, we're going to have our PDE that we can't solve now exactly, so we solve that numerically using finite elements. Using finite elements, and that then gives you an approximation to your quantity of interest, or you don't just evaluate the quantity of interest by the finite elements. So, one crucial ingredient in these sampling-based methods is that you are then able to sample from your coefficient k. So, that's really one of the key things that you need to be able to do. One of the most or one of the frequently used modules in this context is to. Modules in this context is to model k as a log-normal random field, and we've seen that in the two previous talks as well. So, in this case, you assume that your random function k is the exponential of a Gaussian field, and it shows that it's positive, so it makes sense in the physical application. For simplicity, I'm assuming that the mean of the Gaussian field is zero. You can extend everything to non-zero means as well. And then the only other thing you have to specify is the variance. you have to specify is the covariance function, which in this case because it's mean zero, this gives you the covariance between z at x and z at y. I'm assuming this is a stationary field, which means that this covariance function, which generally has dependence on x and y, depends on x and y only through the difference. So this is not a stationary field in the sense that it looks the same everywhere in the domain. So the distribution of k doesn't change depending on which point x. On which point X are you looking at. So the thing that you then are left to specify is this covariance function C, and there's a couple of different models that people need to use. So we saw these in turn covariances in Elizabeth's talk. The one that we're using here for most of this talk, although the theory that I'm going to show you later, is for general covariance function C is this exponential covariance function. So this is something where. Or this is something where, now, quite some time ago, some people actually took some measurements from real rocks. So, they did some drilling, they got some measurements of a subsurface problem somewhere, and they actually tried to fit this function C to the data that they had. And what they came up with is this function here. And it's something that is quite rough, so this gives you fields that are not continuous, they are continuous. They are alternate, but they are not differentiable. And so, this is because you typically have quite rough structures of rock underneath the surface. So, you've seen very similar plots again in Elizabeth's talk. So, these are just realizations of this Gaussian, of the log-normal random field. So, these are realizations of this field K. Of this field K that you have, and in particular, we're looking at the influence of these parameters sigma squared and rho that we have in the covariance function. So, this is looking at the influence of sigma. So, the sigma is just a pointwise variance. So, you can see the difference between these two plots here is really on the y-axis. So, here this field goes between 0 and 3 with the variance of 1. Whereas if you have the standard deviation 10, then you move to. Then you move to going between 0 and 3. So it's really kind of point-wise variance. On the other hand, if you change the row parameters, so this is a length scale parameter, that really kind of changes the length scale of the fluctuations that you have. And so you see when you make the arc smaller, you get fluctuations on a much smaller scale. So how do we actually So, how do we actually then obtain samples from our coefficient k? So, we said that's one of the pretty key things that we need to be able to do. So, again, Elizabeth already touched on this in her talk. So, once you're the k is really your coefficient in the PDE, and once you're solving the PDE numerically with something like a finite element method, you only really need the coefficient k at a finite number of points. So, when you're assembling. Number of points. So when you're assembling your finite element matrices, you typically use a quadrature rule. So you only need your coefficient k to be at the grid points that you're using in the finite element method. So really everything becomes a discrete problem where instead of sampling from a continuous random field k, you end up sampling from kind of a finite representation where you just have the Gaussian field evaluated at a finite number of At a finite number of points. If you look at that, that is then just a Gaussian vector. So instead of looking at the Gaussian field z, which is a function of x, you just look at the Gaussian vector z tau, which is the Gaussian field evaluated at a finite number of points. That is then just a Gaussian vector. And to sample from that Gaussian vector, as Elizabeth already explained, you just need a factorization of the You just need a factorization of the variance matrix. So, if you have any factorization of this form, theta theta transpose, then you can get a realization of this multivariate Gaussian vector by just taking the standard normal and then multiplying it by theta. So really, in theory, you can use any factorization that you want. But in practice, you do want to have something that's quite cost-efficient. Quite cost efficient, and so using something like the Kuleski factorization is just not really a good option because that would have, I'm sure you all know, qubit cost. And so you really want something that has kind of a fast or user factorization that is fast to compute. So that's really all the circular net embedding method does. So the circular net embedding method is about finding a fast factorization of your covariance matrix R. Variance matrix are. So, how does it work? So, you start with your mesh. So, as I said, you really only need samples of this Gaussian field on the distributation mesh that you're using for your finite element method. So, we'll be going up quite tau. We're doing the two-dimensional problem for simplicity, but the theory really holds in any dimension. On this mesh, you then get your covariance matrix. You then get your covariance matrix. So, if you look at the samples of the Gaussian field Z on this mesh, Gaussian vector with mean zero and covariance matrix R. What we then do is we take the matrix R and embed it into a larger matrix S. And the S is actually a circulant matrix. So I'll explain in a minute why that is possible. But for the moment, just believe me that you can embed this covariance matrix R into it. In a bedless variance matrix R into a circulant matrix S. And then once you know that S is circulant, you can factorize it in this form here, where the G is related to the Fourier matrix. So we've chosen this so that we get the real factorization here, and then the lambdas are the eigenvalues of S, for which you also have anything explicitly supported. And then once you have this factorization, Then, once you have this factorization, you can then get a sample of your field using this factorization, where the theta is then just g square root of lambda. Why is this useful? So it's useful because this factorization here can actually be computed a lot faster. So this is because you can just use the fast Fourier transform. So this factorization here can actually be computed in log linear time. Computed in log linear time, so you get down from cubic complexity to log linear complexity. You do have to work with a slightly larger matrix, so instead of working with the matrix R, you have to work with the extended matrix S, so that does add to the cost. But the difference in size is really not very large. So, if you were in one dimension, the S would be twice. The S would be twice as big as R, and in two dimensions, it's four times as big, so it's pretty much a bit. So, let me now tell you why you can do this embedding. So, embedding the matrix R into the circular matrix S. That comes from the assumptions that we've made. So, the two assumptions that are important is that one, the covariance function C that we've chosen is stationary. So that you So, that you, it doesn't really depend on where in the domain you are. And the other thing that's important is that we've chosen a uniform mesh. So, the mesh T that we've chosen here is uniform, and that is precisely so that we can write the covariance matrix R, which of course this step just written down in one spatial dimension, looks exactly like this, so it is just a token matrix. And then you can And then you can embed that into, or you can make it into a circular matrix S by just mirroring. So you mirror the columns and rows and then eventually you get this circular matrix S. And you can do similar things in higher dimensions, it just gets a bit more complicated, or a lot more complicated to write down. So that's the circular embedding method. So it's really just a method for So it's really just a method for sampling from a Gaussian random field, which is cost-effective. So let's go back to the problem that we actually wanted to solve. So if you go back to the beginning, we have this PDE that I showed you. Go back to this PDE that we had, right? So this is really what we wanted to do, and this is our workflow. To do, and this was our workflow that we need to be able to sum from K, and then we need to solve E to get our quantities of interest. And then, in the end, what we wanted was to compute expected values. And so, what you can do to compute these expected values is really just to use Monte Carlo averaging. So, that's one of the simplest methods that you can do. So, you just solve your PDE with n different coefficients, and then you take a sample up. Efficient, and then you take a sample average to approximate the expected value. So, the problem with Monte Carlo methods in the context of these PDE-based problems is that the cost becomes very large. So, you can write down the mean square error of this estimator. So, it's defined that this is really just the mean of the square of the error. So, you have the estimator for the quantity that you. Estimator for the quantity that you're really interested in. And then you can use the classical decomposition into the variance plus the bias squared. And so you can see that if you want to make the mean square error small, what you end up doing is having to choose a fine spatial mesh h. So that's to make the bias small, so the error that you get from a numerical approximation. And then you also need to choose a large number of samples n. A large number of samples n that you're using in your sample average, and that's to make the error between the expected value and the sample average small. So, in this methodology, what happens is that you have to solve many PDEs with different coefficients on a very fine grid, and so the cost quickly becomes very expensive. So, what these multi-level methods do is that instead of just using kind of the finest mesh that you need to control the Mesh that you need to control the finite element error, you use a whole sequence of meshes, so you also use very close meshes, and you just use them as kind of computational tools. So the fact that your solution is not very accurate on those meshes does not really matter. So the idea is very similar to multigrid methods. So I think a lot of people have been familiar with multi-grid methods, right? So instead of solving your Of solving your problem in grid on the fine grid, you just solve it on the coarse grid and then you estimate corrections. So that's exactly what this multilateral method also does. You take your expected value of the quantity of interest on the finest grid, which is really what you want, and then you write that as the expected value of the quantity of interest on the coarsest grid, H0, plus these correction times. And then what you do is you approximate the expected values on the The expected values on the right-hand side here, so these expected values that we have here, you then approximate those all individually using a Monte Carlo estimator. So a multi-level Monte Carlo estimator is really just a sum of L plus 1 individual Monte Carlo estimators where you have a Monte Carlo estimator on the coarse grid and Monte Carlo estimators for the correction terms. So it's not immediately clear why that is a good idea, right? Instead of doing one Monte Carlo estimate, Instead of doing one Monte Carlo estimator, you're doing L plus one Monte Carlo estimator. And the reason that it works is that individually, all of these Monte Carlo estimators are cheap. So on the coarse grid, you're only doing solves on the coarse grid, so this is going to be cheap. And then on the finite grids, you're only estimating correction to only estimating these differences, which is again going to be cheap. And that is because you can choose a small number of samples. Small number of samples. So the number of solves that you end up doing on the very fine grids is a lot smaller than the number of solves that you end up doing on the course grid. You can see that with the expression for the mean square error as well. The bias term actually stays exactly the same. That's because you have a telescoping sum here where you're always adding up and subtracting the coarse grids. So the accuracy only depends on the finest mesh that you have. Only depends on the finest mesh that you have. And then you have these terms here, which are the sampling error. You can see that you need to so you just need to balance the number of samples with the variance that you have. So the yl is this difference. And then this difference is going to be small when l is very large. So you're just going to have the correction between two very fine grids. Between two very fine grids, the responsibility is going to be very small, and so you need a smaller number of that response. So one of the issues that you have with this method is when you're trying to apply it to methods where the random field is very oscillatory. So if you have the random field K, which has a very short correlation length rhoic, it becomes very, very oscillatory, and then you can't. Very oscillatory, and then you can't actually represent it or resolve it well on coarse meshes. So, if you think about the two by two mesh that I showed you, that doesn't work. And so, if you actually look at the variance of the difference, the variance of this difference that you need to estimate, that actually is not small. So, what I just told you that you can estimate it with a small number of samples because it's small, in that case, doesn't work. Doesn't work. So, what you have to think about is then how to make this multi-level method work. And what we did was to smooth the samples of the coefficient. So you have this really oscillatory coefficient k that you can't represent on the coarse grid. So when you solve the PDE on the coarse grid, instead of using this original highly oscillatory coefficient k, we use a smoothed version of k. We use a smoothed version of K that you can represent on both grids, and then you just have to think about what this smoothing actually means and how you define it. So the way we do the smoothing is so that you kind of capture the bulk behavior and the variations are resolved more easily, i.e. you can represent them on a coarse grid. So you can think of these contour plots here. So this would be the original coefficient, but very Coefficient with very fine-scale oscillations. And then what we're doing is developing a method to kind of define a smooth version where you can see that kind of the large-scale features of the bulk behavior is the same. You have very low values here, for example, and then higher values over here. But that one is kind of a lot more smooth, and so you can represent it on the cross point. How do we do that? We do that by dropping eigenvalues, right? So remember the way that we're So remember the way that we're generating our samples is through this decomposition here. And this lambda is just a diagonal matrix of eigenvalues. And so what we're doing is we're just setting some of those eigenvalues to zero. Particularly we're setting the smallest eigenvalues to zero. And that kind of corresponds to dropping the sharpest oscillations. So this is a plot of the samples again, so it's not a quantum. The samples again, so it's not a contact book, but of the actual samples, and you can see again that you kind of get a field that kind of has the same bulk behavior as the original field, but has a lot smoother variations. So in the context of the multilevel method, we then introduce different levels of smoothing on the different grids. So we have this whole sequence of different grids that we're using. That we're using, and we need to sort the PDE on all of those different grids. And the different grids have different resolutions, and so you can the kind of scale of oscillations that you can resolve on the different grids is different. And so, we're using different smoothing parameters or smoothest versions on the different grids. So, the coarsest grid will use the smoothest approximation, and then we kind of iteratively include more and more oscillations. More oscillations. And so, to do that efficiently and to know kind of exactly how much you need to smooth, you kind of need an error estimate on the error that you get from the smoothing. So, if you look at the error between the original discrete Gaussian field Z and the smooth version, so the smooth version is the one where you drop k of the eigenvalues. For that, we have this error estimate here. So the things that appear are one, the dimension of the circular matrix S. So that is the parameter little S here. The MOs that appear in this S is just the number of mesh points that you have in your grid. And then these JIs I haven't talked about these at all and I'm not going to talk about them anymore. Not going to talk about them anymore, but they are really just padding values that you might need to use to ensure that this matrix S is symmetric positive definite. So for the covariance function that I showed you in the beginning, you actually don't need any paddings, nor then j i's are just zero, and you can guarantee that the s is always symmetric, positive, definite. But in general, you might have to increase the size of the matrix S even more to ensure that it is positive definite, but in most cases you But in most cases you can prove that there is such a Ji, such that the S is positive, and you also have an upper bound on what the Ji is. So that's the result that we have. And as I said, what this allows you to do is to kind of quantify how much smoothing you should be doing in the different levels of the multi-level Monte Carlo estimator. So let me finish by showing you some numerical Showing you some numerical results. So, here on this job here, what we're showing is the CPU time versus relative root means per error. So, it's really a time versus error plot. And then we're comparing three methods. So, we're comparing the standard Monte Carlo method. So, the method where you just choose one very fine grid and then you just solve your pick E many times on that very fine grid. And that's this. Fine grid, and that's this blue line here. You can see that that has a much steeper slope. So the exponent here is the error, and you can see that it has a power of minus 4.37 here, so it does grow a lot faster than the multilevel method. In particular, already for these accuracies around 10 to the minus 2, you actually get a factor of 100 savings by using these multilevel methods. And then we have our two multilevel methods. We have our two multilevel methods, so the multilevel Monte Carlo method and multilevel Monte Carlo smoothing. So the multilevel Monte Carlo method does not have any smoothing, so there we just have our highly oscillatory coefficient and that's what we use and then we just coarsen as much as we can but don't change the coefficient and always use the highly oscillatory one and then in comparison with that we have the one with smoothing so the one where we do smooth the coefficient and you can see that in terms And you can see that in terms of the rate of growth, they are pretty much the same, and that's what the theory predicts as well. But the one with smoothing does have a smaller constant. So in this case, the length scale that we've chosen is 0.3, so this is actually not a very short length scale. So this is a field that is quite smooth, and so you don't see a lot of benefit. So you may gain a factor of two, but it does work a lot better. Work better, works somewhat better. If you then choose a field that is really quite highly oscillatory, so the correlation length is 90.1, the size of the domain is 1, so that's a relatively short correlation length, and things change. So the rates don't change that much, but the constants do change significantly. And in particular, you can now see that the multilevel quantifier with smoothing does work a lot better than the one without smoothing. Better than the one without smoothing, in particular for quite short or quite small accuracies, which are really the ones that you're interested in in practice, whereas in practice you would only compute it to maybe one or two digits of accuracy and you can gain a factor of about five to ten, depending on the problem at the heart. And in particular, you can see that for small accuracies, if you do multi-level Monte Carlo without the smoothing, it actually Without the smoothing, it actually is worse than just doing standard one Picard, and that's because you really end up having problems with these highly acidic frequencies on coarse grids, which are actually not really bad for us. Okay, so let me just briefly conclude. So, we talked about the circular embedding method, which is really just a method for sampling from a random field. It would be interesting to kind of extend the smoothing idea to different approaches for sampling. So we do actually have a paper where we do something similar for the Plamoon and Love expansion that has been mentioned. And again, it's kind of based on dropping eigenvalues. But kind of these SPDE-based methods for something I don't think these things have been done, but I mean the idea should apply. Should apply. Similarly, I showed you everything for this linear elliptic PDE problem. That's mostly because we were interested in doing some analysis as well. Numerically, it should work for more complicated PDEs as well. However, the analysis would be probably substantially more difficult. And let me just finish with some references. So the top one is the reference for the work that we just submitted. And let me finish there. And let me finish there. Thank you for this questions? Yes, please? You didn't actually say anything about solving the linear equations, so that presumably factors into this. So do you have to have an order n method for solving the linear equations at each level? Or how would the results change if you didn't have that? So we change in the complexity. So, with change in the complexity, so I mean the complexity that you get for multi-level Monte Carlo, this power of epsilon that you get, depends on the complexity of the solver. Yeah, so because you're, and the, I mean, the cost that you have is solving the linear system and the sampling. And so that's really what you have to do for every sample. So if you then have a sampling algorithm that's log linear and you don't have a solver that's log linear, eventually. Don't have a solver sloping. Eventually, that will lead to problems. I should say, in these applications, the sampling is much more expensive than doing the linear solve. So, in practice, you might not see it for a while. So, eventually, for large enough accuracies, you will see it. But just because of something is so much more expensive than the linear solve, I think for small accuracies, examples, what did you do to solve the linear equations? Good question. So we use Phoenix. Whatever Phoenix uses as a default. What was the question? What was the last question? I wanted to know how she solved the linear equations, for example, on the final level. So a direct method or sparse direct method, or is it default in PB? I think that the default in Phoenix is either all LDU factorization or GMS with the EU I don't know which one. The defaulting Phoenix for this standard edit. More questions? Comments? Okay, thank you very much, Gibby. Thank you.