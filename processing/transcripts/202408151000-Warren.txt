Well, thanks. Okay, so thanks to the organizers and all the good talks we've had so far. Happy to be here. This is geometry, I guess. This works. This works. Okay. All right. There we go. Okay. So this is a topic that I've kind of been messing around with for the last 15 years or so. And it's kind of exploring the analogy between special Lagrangian sub-manifolds and collaborator, the way they sit inside Calabia manifolds and optimal. Manifolds and optimal transport plans, which sit inside metrics, a metric that was discovered or invented by Kim and McCann. So, you know, this is something which there's, I think I won't go too far into the analogy. We had a very, very, very short introduction to Calabi-Yao manifolds yesterday, about 10th of a slide. Slide. But I think the, I'm, so I probably won't say much more about this, but the, but there's a, there's going to be an analogy between special Lagrandians. So if you know what a special Lagrandian sub-manifold is, I'm going to try to, I'm going to describe that this is sort of the same thing. And to me, it's like, it's an interesting, it's an interesting topic. And I've kind of come in and out of this over the last like 15 years. And it's, sometimes it feels like. Years and it's sometimes it feels like it's really natural. Um, sometimes it feels like I'm you know, it's you're kind of forcing things. Um, but you know, I so I didn't know if it was like a real problem worth studying, but I learned this trick and it's like, so if you get your hands really dirty in a math problem, um, and then 15 years later you smell your hands, that means it's a real problem. I mean, if you can, if you can still smell it on your hands. You can still smell it on your hands. So, so I kind of had let this slide a little bit and then I'm coming back to it. Actually, you know, a year ago, I got an email from Robert McCann that he had, they were sort of looking at this analogy again. So now I'm kind of re-interested in it. And I also, I, you know, I still think it's sort of fascinating. So unlike a lot of the things we're using optimal transport as a way. know a lot of the things we're using optimal transport as a way you know as a tool to apply the idea here is that the optimal transport is actually kind of the underlying it's got its own underlying geometry that we're that we can we can be interested in um so the the quick results i've been able to i guess harvest so far are uh if i take a flat torus and i put i put uniform densities on it so this is like a really really very basic optional transport Really, really, very basic optimal transport problem. I want to do a um, and I guess I should say that there's a flow. I mean, I think I wrote it up here, I didn't say it, but we're looking at mean curvature flow. I can take this natural occurring metric on a torus or on the product of Tutori. I can run the mean curvature flow, and it will find the optimal transport plan, which in this case is a diagonal, so it's not that interesting. But I have good reason to think this is a lot more robust than that. A lot more robust than that. Another thing I was able to do is look at this in the framework of Lagrangian mean curvature flow and show that, at least on compact manifolds with the Martrini-Gerwang condition, the Lagrangian mean curvature flow is going to converge and is going to behave very nice, converge exponentially to the optimal transport plane. Also, I'll make some remarks. There was a paper. So there's different ways to, you know, the original, or one of the original formulations of the metric had conformal factors. I think the recently it seems like it's nicer to think of this as weighted. And the math becomes much nicer. And there's also a fourth order equation, which I might try to talk about a little bit. So I won't talk too much about this. I think we all know the Monge problem and Torovich. And then let's just jump to the 50s. So Colabi, and this doesn't have anything to do with optimal transport at the time, Calabi looks at the Hessian of a convex function and said, let's just look at this as a metric. At this as a metric, and in particular, if the determinant is one or a constant, he was able to say some things about what the properties that these will have as a manifold, as a Riemannian manifold. So then, Brignier, of course, I think it might have been early, I know, late 80s. Um, polar factorization says that actually these optimal transport maps, at least on the Achilles, are going to be the graphs of gradients of convex functions. McCann is able to generalize that. There's a way in which you can kind of make sense of this in terms of, you know, for lots of, for a wide variety of costs. C exponential map, the cost grade. Map, the cost gradient, or whatever you call it. So I'll connect these two. So this is just a quick, it's R2. I mean, it doesn't have to be R2. This is just easy to look at. So I'm taking H. This is a metric. It's a pseudometric, meaning it has signature 2, 2. So there's two directions which are positive, or I mean, there's two vectors. there's two vectors which are going to be unit length in the positive direction two vectors in the minus direction you can think of this so you might be more familiar with this metric this is kind of your typical Lorentzian metric but you can make it an extra dimension called something like this we have this is sort of the positive cone this is the negative cone and then you have these light directions this metric here which we often call dx This metric here, which we often call dxdy, is just this rotated 45 degrees. So these are the light vectors here. So these vectors, this is, if it's just in the X direction or if it's just in the Y direction, it's zero. It has length zero. If it's kind of diagonal, one X and one Y, then that's going to be positive. And if it's an opposite. If it's an opposite, it's going to be negative. Like this is positive, and then this is negative. And then, of course, it will be useful. You have your unit sphere look something like this. Expressions on the metric? Yeah. No, no, no, this is a background metric. This is just is a background metric right this is just this is just kind of the uh i'm right i i say the clobby metric comes from a pseudo metric but i haven't i haven't told you uh um yeah we start here uh and this is the background metric and then what i'm going to do um it did it okay uh if i take this gradient graph so i now have a gradient graph so this is an n-dimensional submanifold and i take my standard coordinates d1 d2 My standard coordinates d1, d2, etc. I embed that and I pull back this collagen or this pseudometric. If it's a convex function, it's going to induce a positive definite metric, which is the same as the Collabi metric. So, I mean, maybe those are, you know, you're just looking at a convex function. Completely different way to look at it. So. So it was sort of, it was known for a while, I think, I don't know, 90s, who knows who first observed this. But if you take this metric and then you look at a Monjam pair solution, the constant right-hand side, the mean curvature vanishes with respect to this metric. And I guess I should also point out here: when you're looking at, when you're you're looking at when you're looking at a pseudo-Riemannian manifold um you're say the you know it's it's like n space-like dimensions m space-like dimensions an n-dimensional space-like surface is something which can try to be maximum right i mean so it's easiest to illustrate in like a one dimension when you have two points and i say i want to find not the shortest line between these two points i want to find the longest line and so the longest find the longest line. And so the longest, I mean, the longest path is literally this path. Whereas you can, you know, you can see that if I go, I can all these do jagged things and kind of get basically a zero length path between any two points. I mean, if it's along the space-like forward cone or whatever you want to call it. But the, I can, I can't get anything longer than so it's kind of the opposite thing. So, it's kind of the opposite thing. Whereas in Riemannian setting, you're looking for minimal surfaces, which kind of have this really smoothing type property. In the pseudo-Riemannian setting, if you're the right dimension, you're going to have this maximal surface. So it's going to be maximum while you do a variation, and these are small. So, when you compute the mean curvature, you're going to find maximums. Or compute the mean curvature set at equals zero. So in 2006, the first thing I ever did, my advisor, Yuan, said, hey, looks like there's an analogy here between special Lagrangians, the way this metric comes up, and Langian-Perry equations. So go do the work and figure out, you know, show that it's an actual calibration. So I was able to do that. It turns out this was also done by one of Harvey's students, Mealy. Student meal with respect to this metric. So it wasn't completely new, but it was new with respect to the Montjam-Perry equation. So calibrated means that the surface is maximal, not just kind of locally in that the mean curvature vanishes, but maximal over all variations in the same homology class. But you can do this using differential forms. Okay, so after, you know, shortly after that, I went to Ober-Wolfak and I saw that Robert McCann was talking about this measure. And so I got kind of excited because this looks exactly like, well, not exactly, but it seems like a generalization of this thing I just worked on. And, you know, if you can, if you take the standard declining cost, which is like. Euclidian cost, which is like it doesn't, you know, it's either one. I should point out. So you do this: x minus y squared or minus x dot y. These actually have the same mixed, right? And so you take this mixed partial here, minus identity. So you get this metric here, which is this, if you put minus identity there, you get the same metric I had earlier. So this. Had earlier. So, this, um, and this is just a definite addition of the metric. It looks funny, it's partial derivatives of a cost function, but because it's kind of like in a cross-direction, turns out it's a legitimate metric. Um, so why were they studying this? Well, as many of you know, we have this Matruning-Gerwon condition, which had risen, I don't know, 2003 or 2004 or something, which transforms. Which transforms somewhat like a tensor. And it seems like invariant. It was something which you could do in different coordinates, which I guess should be true of anything that's geometric. So Kim and McCann said, well, this has to be like something that's geometric. Yeah. Very last one. So C is a cost function, right? So this is just the cost function. I mean, it's at, I don't know. I mean, it's at, I don't know. This is a metric. So h evaluated on two vectors, x and y. And it's going to be: you take the x derivative, you take the y derivative, second partial in these mixed direct. And it turns out it's a matrix. And I mean, this, I'm writing this, I think this is clear. I'm writing this as a matrix, meaning like I'm, you know, you stick the vector in and you sort of do this. And you sort of do this matrix, and then you do the Euclidean dot product. Or you define it like this, kind of like a more invariant. Yeah, all right, right. This is a background metric. Like this is on R, this is on, well, Rn cross Rn or M cross M bar. So this is a two n dimensional space. So M, you know, this is, yeah, N N. So this is gonna have N space-like dimensions. Have n space-like dimensions, n time-like dimensions. Right. So, yeah, they were looking at this because it was kind of observed that this was a, you know, the Matrina-Gerwang was invariant, right? It was, I mean, the smoothness is invariant, and so you should be able to transfer your coordinates and get a different, you still get smoothness. So there's something geometric going on. And it turns out if you look at this metric, you can find the model. metric you can find the mod-trudinger-wong tensor as a curvature tensor along certain directions i mean it's literally a curve you can take it in any direction but or any four directions but uh it's a four tensor sitting there in this metric so this was pretty fascinating um it also comes with a naturally occurring kayler form so if i i i take these same mixed derivatives and i do the same thing with which if you I do the same thing, which if you studied Kala geometry, it's the same process. You just, except for, I haven't told you what K is. K is this, it takes the place in Kalar geometry or the pseudocalor or whatever we want to call it. Instead of doing this complex rotation, I do this flip thing on the second variable. I mean, it's a, you scroll. You square it, you get the identity. You don't have to take it four times. You don't get the negative identity. But it behaves. There's lots of very nice analogies in how this works. So it's interesting here, to see how close this is to the K-ler case. A lot of this stuff just kind of transfers over once you put this K there. Right. Any other questions? So, this Lagrangian condition is set up so that if you take the cost exponential, so this is, you know, you start with a function u, and then you can find implicitly this cost exponential t here, which comes from the function u. Then the Kayler form vanishes along this sub-manifold. So the Kayler form takes. You might know him from Kayler, you know, general Kayler complex, for example. You know, the Lagrangian, Lagrangian submanifolds are those which are perpendicular via J. So these are sort of perpendicular via K. Okay, so then I was excited. I went and talked to Robert McCann about this. I said, there's got to be some connection here. There's got to be some connection here. And he got back to me about a year later. He introduced me to Jung Han Kim. And Jung Han said, I think we can do something here. And he was able to write down this conformal fact of the original Kim-McCann metric. So this original Kim-McCann metric, and then you take the density. You take the densities here, and then you divide by this factor here. I think it's got to have a minus sign in there. And turns out, there is this very strong analogy between these special Lagrangian and the optimal transport plan. So I take the graph of my optimal transport plan. Of my optimal transport plan for graph of my optimal transport, which is basically the optimal transport plan. And it's going to be a maximal metric or maximal sub-manifold with respect to this metric for very similar analogous reasons, sort of geometric major theory, linear algebra. You just kind of pick a K where there should have been a J, and it all works out. Yeah. Oh, yeah, yeah, yeah, it's yeah, exactly. I probably could have used that word at some point. So I'm mentioning, I decided at some point to like put the dates because some of this gets weird because like an optimal transport, I don't know if it's worse than optimal transport, but like we were talking about this in 2008. The paper gets published in like 20-something. The paper gets published in like 20-something, like 12, or you know. So if you were there, it's weird to say, like, you know, Lau Perre's paper was, you know, came out in like 2000. Yeah, right. But it was, we'd been talking about it for years. So see, yeah, some of these, some of these papers take big, many years. So I guess the point now is what we want to do is now we've characterized this as a maximal surface within a minute. Maximal surface within a metric. So, natural question is: we have maximal surface. How can we find the maximal surface? And the obvious answer is mean curvature flow, I suppose. I don't know if there's any other better way to do that. So we say, okay, can we do mean curvature flow? In order to do that, we have to do mean curvature flow for pseudo-Riemannian manifolds, which sounds really intimidating because it's mean curvature flow, which is hard. And then pseudo-Riemannian, which sounds pretty hard. Money, which sounds pretty hard. But they had studied this, I think, for a while in the 80s, maybe before that. But it turns out it's actually really super nice, and I'll show you why. And this is sort of recently. This is one of those, this isn't necessarily optimal transport, but paper shows up in 2008. It's published in 2011. This is kind of like during all this time that I've discussed so far. If you look at a high codimension, or you know, arbitrary, I guess, high is typically more than one, and you take a product of manifolds and you take g1 minus g2, so it's kind of like this sort of a metric where you have dx squared minus dy squared, and you run mean curvature flow, you get a lot of nice formulas, and you get a lot of nice convert. So it's very hard. Very hard. High codimension mean curvature flow on a Riemannian manifold with little structure is very hard. I mean, it's good luck. This is kind of, I lifted this, I just cribbed it right from their paper because it would have taken a long time to write. But I just want to point some stuff out here. So this is, if you have this high codimension flow, I mean, this is kind of a general theorem, right? If you have a mean curvature flow with M dimension, Curvature flow with M-dimensional mean curvature flow of there's two dimensions, but this is essentially our setting. Now, you can look at this very nice. So you have this nice, if you're just looking at the mean curvature, you have a phosphine curvature minus this term, and then you get a minus sign in front of this. This is just sort of like some sort of contraction with the mean curvature and the second fundamental form. The mean curvature and the second fundamental form, and then you get this curvature term. So, if we get rid of this curvature term, for example, these have nice signs. So, you get Laplace minus nice sign, which means that it's going to behave nicely. The maximum principle is going to be working 100% in your favor here. And then you look at the full second fundamental form, you get again Laplace, and then you get this minus the gradients, that's great. And then you get some derivatives of some curvature. So, this is. Of some curvature. So, this is ambient curvature, I should say, so that you can bound this ahead of time. You don't have to, you're not computing the curvature on the manifold after you get the manifold. So, I mean, you might be familiar with mean curvature flow. This term here, so in the positive setting, is positive. So, it's like B to the fourth, right? And so, these terms, and you can also look at this and you see. And you can also look at this, and you see that, like, I can bound this, use a little Peter-Paul. This is going to be contained in here after I square it, and then I can kind of push this stuff off. So the maximum principle works incredibly well here. The only thing that I think. So I think it all works out. So you get everything kind of it. Yeah, I mean, there's double negative signs which happen when I've gone to the details here. There's double negative signs, which are like, it's going the wrong way, but it's also increasing the volume, but it's still pointing in the right. Yeah. I mean, there's, it's going towards a maximal surface. It's going towards a maximal surface. I mean, time is going towards a maximal surface, so it's not, yeah, it's going in the right direction. We could argue about what that actually means in terms of the signs, because there's different conventions. But yeah, so this B to the fourth term, which is kind of here, that has this wonderful sign, it's the opposite, makes life really hard for Riemannian mean curvature. So, you know, it almost looks like it's the problem. You know, it almost looks like the problem's done. Like, I just run the flow and it's done because it's such a beautiful maximum principle. But there's actually a little bit of subtlety here: you have this, you actually, you have to look and you say you have this metric here. The unit sphere looks like this, right? So the problem here that creeps in is that the unit sphere is actually not compact. So some of the stuff gets a little wild, and this is actually what you end up having to deal with. And this is actually what you end up having to deal with. Makes it more like kind of like a second order bound the second derivatives problem. So, like, if you can bound the slope of these graphs, then everything is great. But if you can make sure that your tangent vectors are like here and not like there, your unit tangent vectors, which show up in these formulas. Because that way the curvature, this looks like a very tame, looks like a very tame curvature, like it's something fixed, but if your eye direction is actually very long. But if your eye direction is actually very long, the curvature actually is kind of a big thing. But it's not something you can immediately get from the maximum principle. You have to bound these second derivatives. What I just said. So for example, if I just have this dx squared minus dy squared, I want to run the mean curvature flow on that. If I can bound this slope by anything less than one, then all the curvature terms are sort of automatically bounded. Terms are sort of automatically bounded. And then I can just run the flow with maximum principle, and I'm good. Yeah, I guess I'm assuming that it, I'm starting with something smooth and then I want to. Yeah, yeah, I'm not worried about like starting, starting L2 or anything. Just smooth. And smoothness will be preserved. And smoothness will be preserved. Yeah, yeah. I mean, well, I guess for some of the specific cases. So, what they do is what they do is they take, I mean, so it's kind of unclear what you should bound when you're just handed like a metric with certain, it's this metric minus this metric, and you'd have to bound the slope somehow. somehow. There's no ambient, there's no like naturally occurring ambient metric. I mean ambient Riemannian metric. You have you have a your metric just sort of that's given to you looks like this. And so you hope your slopes are nice. But you're not given a natural metric which sort of measures all the vectors. sort of measures all the vectors in a Riemannian sense. So what they have to do is they take an n form and they look at the evolution in this n form and use a maximum principle. And so I'll probably come back to this. I tried to do this for this in this mod trooninger long setting for the optimal transport. It might work. It might work. I still don't know that it will work. I kind of struggled. I was able to write down. I struggled with the parabolic case, and I was able to write down the elliptic case, which all I did was able to recover the Ma Trininger-Wong regularity theory using sub-manifolds, not using PDE, but just using sub-manifolds. But I was only able to do it in 2D. I think this is kind of limitation. This is kind of limitation. I mean, it might have been just my own laziness or skill issues, but it gets kind of messy. But I can give you the outline of what works here. So I just take, this is my G1. I take an arbitrary metric on the base manifold. So I have this. So now we go to optimal transport. I have M bar. And so what I do. M bar. And so what I do is I just take a g1 here. And then when I when I do h dot with a vector in there, that's going to actually give me a, it behaves like a one form on the m direct, on the m bar direct. And so it kind of, because this metric flips up. So this is going to behave, this form is going to, if you can bound this form, you can end up bounding the slope. So this is something which is possible. And it's kind of a little bit of not naturalness. Like the metric, I mean, I just have to pick a metric. I mean, I guess that's okay, but you're just sort of picking a metric on M and then, which doesn't necessarily have anything to do with your cost. It's just kind of like, let's pick a metric. And so recently I got, I mean, I don't know if this is actually published. Actually, published yet. It appeared last fall, I think. They were able to look at this case again. They did the elliptic case and they constructed basically a metric on the ambient space. And they said, if we can bound the tangent vectors with respect to this metric, we get a bound on the slope. And they recovered basically the full Law Treating Gerald result. Result with perhaps a little extra because they didn't require, they only looked at the sub-manifolds from a sub-manifold perspective. They didn't say this is a graph from optimal transport problem. It's actually possible to have, if you have some topology, you have things which are maximal submanifolds, which are not necessarily graphs of optimal transparency. But you have to have a little topology to do that. You know, think of just taking a identity map. Identity map on the Taurus and then shifting it. Still looks like an optical transport locally, but it's not global. Okay, so this also kind of happened in, you know, I set this stuff aside for a minute. They had this another long mean curvature flow, pseudo-Riemannian space that took three years to make it through. So this has been Labrador, Jason Latte. They look at this. Jason Latay, they look at this really basic Euclidean metric, and they got some really nice results. So if we just study how this flows, so they're basically using the type of, I mean, the formulas that show up in the Salavesa, but it's slightly easier. And they, you know, they can say a bunch of things about exponential convergence and global things. And I won't go through all that, but I'll just say. Go through all that, but I'll just say that corollary: if you look at what they do, and then you try to apply it to an optimal transport on a torus, like the stick to compact, it's a little easier, then the mean curvature flow in the submantiflue converges exponentially to the diagonal. And the interesting part about this is you're starting with a space-like condition, which is not necessarily like a gradient graph or a cost exponential. Cost exponential, it's weaker. It's like monotone. You're not looking at cyclic monotone, you're looking at monotone. I don't know if that's exact. That's one to one. But you're thinking just kind of roughly monotone maps, which are going to be space-like. So they don't have to have a gradient structure. So you start with these, and then you run the mean curvature flow. It's going to run the mean curvature flow on the graph. It's going to recover the optimal transport, at least in this case. Optimal transfer, at least in this case. So it becomes a little bit messy when you start to add. So I would want, you know, what you would want to do to this is go and add this conformal factor. So to say a little bit more about what they had proved is when you just take, if you take your torus, your flat torus. Take your torus, your flat torus with a flat, you know, with uniform density, and you start with some map. And you know, typically, you know, what is this actually looks like? It looks like something like, you know, periodic. But if you just restrict it and you look here, you get the same metric. It's exactly the same as the Collabi metric, and you run the flow. metric and you run the flow and you and you get it to converge but then what happens if we put this this um this conformal factor on this and run the flow again so we get it's not quite flat um so i and i think it should work it just takes a lot of a lot more effort but i i think it should be there um if you start with the gradient type map i i i think that we can do this already Think that we can do this already because it's the it's basically just interior estimates, right? So the interior estimates, followed by this is June's thesis. And yeah, so this is 2010 and then 12, I guess. And then ongoing. So there, yeah, the crucial part here actually is the interior, or at least what I'm concerned about. There's a lot of very difficult boundary. A very difficult boundary, um, boundary regularity in this paper that I was sort of very scared of, which I didn't touch. But, uh, this is there's a crucial interior irregularity part, which should give this sort of behavior for arbitrary smooth densities on the torus. And that should cover the mean curvature. And then the exponential convergence, this is something which I'll talk about this in a minute. Jung Han Kim, Jeff Streets, and myself, we did this in 2011. And myself, we did this in 2011. Abedin and Kiragawa also kind of were able to make this a little nicer, I think, with a more modern approach. But I think the interior regularity and the exponential convergence, I think, are sort of separate, if I understand the problem correctly. So you could use either. So the important thing here. Important thing here, or I mean, the reason that distinguishes these cases why this isn't automatically true, but this is a Lagrangian mean curvature form. Meaning, if you start it with a Lagrangian manifold, which means it's a basically locally a cost exponential, you start out with a cost exponential. You run the flow. It remains a cost exponential. And so you can actually look at the whole thing on a scalar level. You're just looking at a scalar parabolic function on the scale, which is what's going on here. So, I mean, I don't know if you ever. I don't know if you ever used mean curvature flow. Use some mean curvature at some point, but second point, but not mean curvature flow. But this is Lagrangian mean curvature flow. If it starts Lagrangian, it remains Lagrangian. And so Lagrangian mean curvature is a very interesting thing in the Claudia setting. The Clawbiao setting. So this goes back to the 90s, Smachik. A lot of hard work, a lot of big formulas. But one thing you get is that if you start with a Lagrangian sub-manifold and you run the mean curvature flow, you restrict the California. The change in the Kayler form is going to be dh. So if dH is zero, dH is the mean curvature one form. H's of mean curvature one form. If this vanishes, then you expect kind of infinitesimally that the Taylor form is preserved, or its zero, its zero-ness is preserved, and it should remain Lagrangian symmetric. Of course, there's a little bit more work there, but I guess I should tell you why. So, this is this famous work of Harvey and Lawson in 1982. This is the J. And you have this Lagrangian angle, which is something you can define on Lagrangian to manifold. And you can, it's just this function, it's basically an S1 valued function. You take the gradient of that, and that gives that generates the mean curve. It's something special for Lagrangian manifold. And then so you take dh of that. So you can kind of see if you stare at it, like if I have a gradient, I take dh of a gradient or d of a gradient, I should get like D D zero. I mean, it's a mean curve, it's a one form. So H is a one form. I take big D of D, I get zero. I expect this to vanish in Claudia Yaus. But so the actual proof involves a maximum principle argument. And it's kind of involved. You have to prove, you can't just sort of say it looks like it's infinitesimally preserved. You have to prove that, you know. Infinitesimally preserved, you have to prove that you know any flow actually it's zero, right? You know, any flow for some positive time is zero, and so that involves um maximum principle argument, so it's not just automatic. Um, and in fact, the opposite is or the converse is true. Like if you if you start out with a manifold, which is not reaching flat, you can compute that there's not going to be this potential function theta. Uh, mean curvature flow does not preserve the Lagrangian condition. So you start out with Lagrangian sub-manifold and some. With Lagrangian submanifold, and some you know, it can be nice, Einstein, but you know, if it's not Calabio, so collabi yo is Ricci flat. I don't know if I've said that. Ricci flat is crucial here. But you can, so Ricci not flat means you can destroy Lagrangian condition using mean curvature. So you start out with Lagrangian submanifolds, you run the flow, you're no longer in the setting of Lagrangian submanifolds, and then all your nice formulas don't even work. All your nice formulas don't even work anymore. So that's another reason why this is very rough in Euclidean case or Riemannian. Okay, so we need to actually, if we want to do the mean curvature flow and Hovick's Lagrangian, we have to something, at least something, at least coming from left field, it sort of looks like this is a non-trivial thing to do. And so actually, I thought that maybe this would be hard. Um, you know, why? Because if I look at this metric, there's like, you know, come on, this is never like, there's no way this is Richie flat, right? Um, put a cost function there, put some arbitrary densities, put a, right? There's no way that's Richie flat. So I thought it would never work. But when you go back and you look at what the what the key thing was that the mean curvature was generated as a differential of a function. And so if you take D of that. And so, if you take d of that, you're going to get zero. Um, and that's kind of the key point here. And so, we can try that, and this is something which I'm sort of embarrassed. This took me so many years to figure out, like, I, because I just thought it was too much to believe. But if you take, if you just take these volume forms and you define this, so this is something where it turns out it's in, you know, you can check. This is, if I stick end. This is if I stick n different oriented vectors in here, and I take this form, evaluate that, take the natural log of rho times that, and take the natural log of rho times that. These are just n vectors, which give my, you know, on my tangent space. This is a well-defined function on n manifolds and Lagrangian submanifolds. And the mean curvature, so if this is Lagrangian, the mean curvature is going to be very similar. Going to be very similar. It's got a minus sign here minus k times grad theta. So this is really nice analogy between the Lagrange, the Falabiao and this Kim McCann situation. And I don't know if this is the deepest proof I'll give. One of the things that intimidated me about this is I was kind of doing it the wrong way. But I But I have to admit, I kind of forgot the Kozil formula. It makes, I was doing stuff with Hardware, and then I sat down, I was kind of hammering it out. I'm like, I can use this, and it actually becomes like something you can sit down and do. And you can write this theta here as this on a Lagrangian submanifold. So on a cost exponential graph, you can just write theta like this. So this is the standard metric. It's also, yeah, I guess this is W is what. This W is what mod treating you wrong, pretty common. It's also just the induced metric. It's literally just the induced metric. And it comes from this sort of embedding of the graph. So you just write this down and you check it. It's easiest if you check it against all the normal vectors rather than trying to actually compute the mean whole thing and then project it. That's kind of a mess. So So yeah, no, this is just the make it seem like it's doable on a slide. It's not so bad. At one point, you're like, ooh, the gradient dot with EK, oh, that's just the differential against that vector. Awesome. Same thing. And so what this does is it allows me to do something which I'd worked on 10 years ago. 10 years ago, and that's or 12, 13, we had studied just this parabolic equation. This is again, Yung Han Kim and Jeff Speetz. We just looked at this parabolic equation. And if you have a positive Manchuang thing going on, then it's going to converge exponentially. So then what you have to do, so you just, so what do we do? We take this flow, we run it, we run this flow. We run it, we run this flow, and then we look at this and we project it onto the normal direction, and we get this vector here. That's the mean curvature vector. So you run the flow, you project it, and you get the mean curvature. That means you're doing mean curvature flow. This is actually kind of, this can be kind of a confusing thing. It can throw you off a lot because the computations can sometimes be There's a lot of room for error. When you're studying a parabolic equation, you're studying like a vertical flow. So you're going up or down or whatever, kind of north and south. When you're studying mean curvature flow, you're going along the normal direction, right? So there's always an underlying diffeomorphism where you're saying, as time, this point here is actually going here, but you're running a diffeomorphism. Going here, but you're running a diffeomorphism from this here to this here. Infinitesimally, you can just check that I'm projecting, I'm just projecting down or on. It's the cost gradient, the cost exponential. Yeah, so you're generating it from the function u. So you literally get mean curvature flow, which is what you want. So we get to call it Lagrangian mean curvature equals. We get to keep the L. Not that everyone's collecting L, but this was a good L. So back to this flow, there was an exponential component of this. So first of all, you do the Montridge-Gerwang thing, and then you get the derivative, the second derivative estimate there, or the gradient, or the. Derivative estimate there or the gradient or the slope estimate on the graph. And then, so I was really hungry to just apply geometry to this because it seemed really cool and geometric. So, you know, I needed, you know, a metric. And we had this metric from Kim McCann. Went and talked to like Jeffrey Street, who was like the, you know, the top geometry that I knew. And said, let's do some. And so let's do some cool geometry here. And so if you write it down, you get this weird, like we're trying to apply this Liao. So this is, you know, classic differential geometry or Riemannian geometry stuff. And it can prove like a Harnack equality, which you can use for exponential convergence, et cetera. We're able to write this down, but it's this thing. But it's this thing, which we had to kind of push a little term on there. And then we get this really weird, bizarre. I mean, it's not, it comes from the conformal factor. It's, I don't know, it seemed like it worked, at least in dimensions greater than two. But we were able to fix dimensions greater than two by faking a dimension, adding a fake dimension S1, and then proving it's like once you get the S1. One and then proving it's like once you get the estimates, you know, you can say that it's the same flow, and then you drop back down and prove it's the same flow. You know, this, I wanted to clean this up. And in fact, so I had, I had figured this out at some point in time. Unfortunately, Abedin and Kitagawa had used this, I guess. So instead, I guess, talk about the slide. So here, this is his Kim McCann. This is this Kim McCann Warren metric. If you put a weight here, you actually get the same landscape. The volume functional has the same sort of landscape. So if you vary the volume, you get the same, I mean, you get literally the same volume if you add a weight instead of a just conformal factor. And so it has this really nice, like when you also do this normal flow. Do this normal flow. You take this weighted Laplacian and you get this super nice, you get this super nice expression here. So it looks like a lid, you know, like a heat flow should. And so it turns out this has been since used. And, you know, it's much, much cleaner. So if you look at these papers like 10, 12 years ago, and you might. 10, 12 years ago, and you might have been scared away because this is really nasty like Kim and Streets and I did. There's now a better approach. So, like, this is much cleaner. And it's been worked out in a newspaper. Okay, so briefly, I think I have time to do the fourth order flow. So this is something which I kind of separately was working on, trying to find special Lagrangian submanifolds of Calabi album. Grangiens of manifolds of collab Yao manifolds or Hamiltonian stationary. These are very interesting for a lot of, you know, like mirror symmetry, things like that, Thomas-Yau conjecture, et cetera. But one thing you can do is if you look at the L2 metric on the scalar function, so the mean curvature flow is a gradient flow of volume with respect to the L2 metric on the variational field, right? So when you're writing mean curvature flow, you're literally just taking the L2 metric of all these. L2 metric of all these fields here, right? That's how you measure the, you know, two sub nearby sub-manifolds. You look at the path over these variational fields. Another thing you can do is you can just take the scalar function which generates these manifolds. So if we do that, if we just take the scalar functions which generate these Lagrangians of manifolds and you try to run the volume, you run the gradient on that, you get a fourth order equation. Equation and this has a really nice, like uh, I mean, it's theta. This is just on the torus, of course, it gets more complicated, but ut is minus Laplace theta. So it's like a really nice fourth order equation. Um, it seems like it works near the near the diagonal. Um, it seems like it should work in other cases. Uh, I haven't written this down, it's a little bit more complicated, but um. Yeah, I mean, it's another thing about this, and I have no idea if this is useful. You now have a gradient flow on the scalar function. So, if you're parameterizing possible plans, which you're trying to find the OSPO one, but if you have all these scalar functions, you can literally just, whatever you want to do, I don't know what the kids are doing, tensor flow or whatever, you can just like plug whatever the volume functional is into this and then run it, run a gradient flow, and it should be mimicking this fourth order equation. So, I mean. I mean, in some sense. We mathematicians would like to show that this converges, which might take some work. Okay, and the last thing, I just wanted to show kind of roughly. So, this is a formula which appeared in this paper, Brendel, Laser, McCann, and Rankine last fall, I believe. Last fall, I believe. And then I basically added, so they had h equal to zero, and I added the h if it's not zero and this time derivative. And so you can basically, I mean, if you stare at it, Matt Trunger Wong is here. So it's kind of like the Matron-Gerwang maximum principle argument, but what you get here is like you look at this and you stare at it, you get a little bit extra here. And so the maximum Bit extra here. And so the maximum principle works. But we have one problem, and I can't. I don't know if, again, if it's like a skill issue or if I'm not doing this correctly, this should almost be in the Eclipse case. This term here is going to be the derivative of your maximum eigenvalue. So I kind of rushed through this, but this is the S is some sort of Riemannian metric, which is measuring your maximum slope. So you take the derivative of that, you set it to zero. This should almost vanish. zero, this should almost vanish. And it's even what's frustrating is like, if you had like super nice exponential coordinates on this McKim-McCann metric, that trim would vanish. So there's like these, these like first derivatives of the metric would show up, but they feel like they shouldn't be there. But you can't, you can't, from what I can tell, you can't take like a product, like Riemannian normal coordinates on the product metric, which does what it should be, the cost function. And it would simplify a lot of this. And it would simplify a lot of this geometry. So, this is one of the things where I think it's still like if you could come up with nicer coordinates, I think you could do a lot of this stuff fairly easy. It might not be possible. Is there any like connection between this flow and like algorithms to solve? Yeah, I don't know like the status of mean curvature flow algorithms. I mean, but that's like what I said is if you had to, if you could compute the volume from a scalar, then you could just do whatever gradient flow on that, for example. But I'd actually. Um, but I don't actually know anything about that. Maybe that's really hard. So I got it. So, uh, okay, this is sort of maybe a super vague and too big question, but to define the metric, you need enough regularity of itself, right? But like uh if you know that like So you look at this metric and you can have all sorts of jagged, like, it doesn't have to be, you can define, and maybe it doesn't, it's not defined, it's a metric, like, but this is a sub-manifold is well defined. But this is a sub-manifold as well defined. You have the cost of dimensional graph and the or the um you can look at that, it's still a minimal surface, it's still calibrated in the sense that if I take any other homologous surface, it's going to be the volume of this is going to be bigger. And so you can do this with geometric major theory quite generally. So it's kind of nice in that sense. So this would maybe suggest that, like, even if I, if I was running some sort of flow without knowing even the regularity, it might still try to try to. It might still try to go in the right direction. I mean, the parabolic equation, you can't write it down if there's no, but maybe, you know, if you're just looking at this and trying to go in a bigger volume direction, it would get you in the right direction. Thanks, Micah. I have a really basic question. So, why is it that with the space like sub-manifold when you run the flow, you end up Sub-manifold when you run the flow, and you end up being volume anyway. So, I mean, more, like, like, why do all the negative signs add up or morally? I mean, yeah, I mean, because you're, I mean, I guess this is sort of the desired. I mean, because that's going to be your regular manifold, or I guess the most regular is going to be the one that's volume maximizing. That's volume maximizing because it's harder to do. I mean, morally, it's like this: you have a line, it's going to be the maximal surface. But if I start from here, what is it going to do? It's going to smooth out the line. That's morally. More technically, you just sort of check that all the negative signs kind of point. Yeah. So, are you measuring the So are you measuring the area in this pseudo-metric? Yes. So in that it's induced. Yeah. In that pseudo-metric, the area. Yes. Yeah. But the but if you look at the flow, but it's basically the same as the workflow classic flow flow or same in what sense? Like graph. So, I mean, these stay graphical when they exist because you kind of have this avoids, like, you can't do this. They're not going to, I guess they just cease to exist if they try to not be space-like anymore. Then it's kind of just everything degenerates. I don't know if that was a question. I mean, meaning like you take however you define the mean curve is you take the trace of the segmental form and you get like end, you know, or whatever codimension, and co-dimension vector, which is in the normal direction. So you get this, yeah, normal vector, which is going to be, I mean, it's also, you can literally write down the gradient flow of volume with respect. Of volume with respect to this metric. And so you this is another one of those places where you get the extra negative sign because it's now you're pairing a negative vector with a negative vector. And yeah, yeah. So your normal vector is like, you know, typically your normal vector is doing something like this. And so it's going to be negative vector. And so your gradient flow, I mean, one of the ways, I mean, maybe this is gradient flow, you're kind of resisting or you're gaining. Or you're gaining with the least area. But here you're trying to, you're kind of doing the opposite thing. You're actually, your area is as small as possible while you're doing the opposite thing. I mean, it kind of cancels out. Yeah. Yeah. I don't know if I would say changes. I mean, it, I mean, it's, it's completely different. Like, I mean, it's completely different, but it's because it's your gradient has a name. I mean. it's your gradient has a net i mean because of h is a negative vector like gradient flow is still giving taking you in the in the in the right direction of maximum if you're to run like perfect and then so this oh yeah good good question so you have to write this down like like you can't do this because that's not space like um but i can do this in a torus But I can do this in a torus, and it actually converges very nicely. I mean, you get some very nice, like curvature t is something like KSS. And then, like, instead of like plus k to the third, you get like minus k to the third, or something like this. I might be misremembering, but it's the right sign. It's what you are. And so it converges very nicely. This is a basic question, but so the optimal transport plan is like max globally. Plan is like a globally maximal surface with respect to this Amicon metric. If you unpack what that means, is it the same thing as it being the solution to the launch mentor-fish problem, or is it something deeper than that? So that's something, yeah, that's a good question. I spent some time. You can actually, if there's topology, right? So it's, so the, the, the, the maximality is two conditions. One, it has to be. The maximality is two conditions. One, it has to be Lagrangian, that's part of the calibration, and the other is it has to be volume preserving. So, if you put Lagrangian, so it comes from a cost exponential plus volume preserving, it's maximum. The question is, can you come up with a cost exponential, which is also volume-preserving, which is not an optimal transform map? And globally, you cannot, but locally, you can. So, like, you can do, I mean, the simplest example is I take my Taurus. Is I take my torus, this is the optimal transport map, S1 cost S1. What if I shift that up? It's still a maximal surface in the manifold, but it's now, it's got positive cost, so it's not optimal. So if this comes from the topology, if you can, you can, there's harmonic vector fields, which if you flow by the harmonic vector fields, you will actually find another optimal transport looking map, which is an optimal. Well, let's make our control.