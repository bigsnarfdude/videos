Reasons one because Manton is beautiful, the other is the time zone difference makes it more of a challenge. But thank you to Charlotte for putting me up first just after Sophia. So the title of my talk is the de-biased Whittle Likelihood for Parametric Time Series and Spatial Models. I'm currently based at Lancaster, but I give my Imperial email because I'm actually starting there next week. So this is, I still wanted to officially give the Lancaster affiliation because it would be wrong not to. Affiliation because it would be wrong not to. But if you try to contact me, please note my new imperial address. This is joint work with many people: Arthur Guillaume, who's a lecturer at Queen Mary University of London, Sophia, who you just heard at Lausanne, Frederick Simons, who you heard yesterday at Princeton, Jonathan Lilly, who many of you know, who's now at Planetary Science Institute, and Jake Granger, who is just writing up his PhD. Writing up his PhD at Lancaster in spectral time series analysis. So, today's talk, the motivation, as with many of us, comes from the fact that data sets collected, I'm going to cover both space and time today, are getting big. We frequently see data sets and much, much bigger than a thousand, right? The focus of today's talk will be on parametric inference, but I will be talking about But I will be talking about tapering in that context. So I'm grateful to Peter's talk yesterday because it's sort of a nice segue into this because he talked about semi-parametric inference. And this one's going to be fully parametric in the sense that we have some time series or spatial statistics models with some parameters that we want to estimate from data sets. And usually when we do that, maximum likelihood. Usually, when we do that, maximum likelihood is desirable. But computationally, it can scale horribly. Even if we have a Gaussian model, we're typically looking at an order n cubed operation, where n is the number of data points, simply because we have to invert a large covariance matrix to do that. There are tricks that you can sometimes make that order n squared, that many of you will be familiar with. But as we go to non-Gaussian models, then actually maximum likelihood can be even more computationally expensive to compute because you Expensive to compute because you don't have so many neat analytical expressions to take advantage of. So, some people then turn to quick approaches like method of moments or least squares. So, you might take the spectra, a multi-taper of your data, fit it to some spectra using least squares. And Peter hinted at this yesterday: that actually this can yield far from optimal results. So there's this compromise area, which is to use quasi- or pseudo-likelihoods, which are approximations to the exact maximum likelihood, but are much faster to compute. Typically, they are n log n, and I attach some relevant literature here, which some of you will be familiar with. A lot of these take advantage of techniques like circular embedding, some of you may have heard of, but broadly speaking, they're techniques that try Broadly speaking, they're techniques that try to harness the FFT to approximate much faster way of inverting covariance matrices. And there's been a lot of interest in this recently to try and generalize this, make it practical for massive data sets and potentially add Bayesian machinery and incorporate it into various applications. But the history of this goes, of course, much. Goes, of course, much further back, which is a popular and widely used pseudo-likelihood, is the Wittel likelihood. And I'll give this equation in a few slides' time, but at a high level, it's in the frequency domain, and it can be computed in order n log n operations using the fast Fourier transform. So it's super fast. However, it's quite well known. It's been documented for over Documented for over 30 years now that the parameter estimates can be quite bad from Whittle. Dahlhaus has noticed this, Velasco, Robinson, and many others. And why is that? Well, it's due to these edge effects or spectral blurring that Sophia just spoke about, edge effects. Blurring came up in a lot of talks yesterday, and this fits in nicely with the theme of this program, which is that. Which is that if we are using FFT approximations, then any kind of loss of information from our sampling and the effects that has will blur, leak, whatever you want to call it, into your spectral estimate. And if you're going to use that in a pseudo-likelihood, then it's going to give you wonky parameter estimates. And as Sophia just said, these problems will increase with increased. These problems will increase with increasing dimension. So, that this bias in the non-parametric estimates will affect the parametric estimates. And these problems will get bigger with increasing dimension because more and more points lie close to the boundary, right? If you go into higher dimensions. But even in the 1D time series case, this is a problem. And it's been shown to be a problem, especially, well, I'll show you. Especially, well, I'll show you some examples in this talk. So, with Sophia and Arthur and many others in the last five or so years, we've been building a novel approach, a novel pseudo-likelihood, which we call the de-biased Wittel likelihood, or spatial Wittel likelihood if we're working with random fields, which removes these edge effects to almost completely eliminate the bias. To almost completely eliminate the bias with only a minimal increase in computational runtime. And we've been working hard to generalize our methodology. It can account for aliasing, but also work with missing data, irregular sampling domains, non-Gaussian data, multivariates, and classes of modulated non-stationary processes. And I will expand on these in this talk. And we provide a theoretical framework. Theoretical framework guaranteeing consistency and optimal root n convergence under relatively weak assumptions. Now, there's actually, it sounds very technical, perhaps so far, but I've tried to go for a sort of a necessary and sufficient way of defining the notation and definitions that you need to follow this talk. And there are really just four things, three of which will be on this slide. The first is wherever you The first is wherever you see a C. I'll be referring to the auto covariance at lag U. In this case, this is common. And this is for spatial set D equals one for time series. So U would just be a scalar then, or if it's a vector in higher dimensions for spatial. And that's just your typical lagged operator there. The power spectral density, which we can Which we can, for stationary, define as the Fourier transform of the auto covariance, will be f, a function of frequency omega. And when d equals one, we're going to define the periodogram of a finite sample sampled at regular intervals by i, which is just the absolute square of the direct Fourier transform of our data. So we've got c, f, and i. So we've got C, F, and I. Feel free to note those down, which is going to be auto covariance, theoretical spectrum, periodogram. And then the final thing is this sense of wider generality. We want to define a periodogram more broadly in higher dimensions, but also one that allows for a flexible function here, g subscript s, which Which, for example, would allow us to incorporate if the data is missing or outside an irregular sampling region. Quentes has studied this before. But we could also, by setting it to be not zero or one, we could incorporate tapering, right? So we can when I mention, when I say I in this talk as the periodogram, it incorporates at least single tapered spectral estimates as well. Tapered spectral estimates as well because we can modify this g function accordingly, it's just a data taper. We could also have modulations, so for example, we could, if we know the functional form of g, we could even allow for non-stationary time series. Let's say g was an amplitude modulation for a signal that's being amplified in time. And just in the simple case, set g equals one, you recover the regular periodogram from the previous slide. From the previous slide. So that's all the notation we need: C, F, I, and G for these sort of four objects. And that's what we're going to need to form our likelihood. So going back to Whittle, Peter showed this equation yesterday. The Whittle likelihood here I've given in a discretized form as a summation over frequency, but you take the log of your power spectral density. Off your power spectral density, and you add the ratio of the periodogram to the that actually, there shouldn't be a log there. I apologize, that's a mistake, that log there. So I should have spotted that before. So the log's not there. So the bit I'm highlighting there, removed, but it's the log of the spectrum plus the ratio of the periodogram to the spectrum summed over frequency is your witt or likelihood. That can be computed in n-log n operation. Computed in n log n operations. We instead proposed something called the de-biased witt or likelihood, which is very similar, but wherever you see an f appear at the top, I've replaced it by I bar. Okay, so f, instead of f, we have i bar. Now, what is i bar? Well, i bar is the expectation of i. So it's the expected periodogram. So what we're doing instead of So, what we're doing instead of fitting the power spectral density to the periodogram, we're fitting the expected periodogram under our proposed parameter to the periodogram. And for those of you who know your Percival and Walden, then you will know that the expected periodogram is the convolution of the true spectral density with this kernel here called curly F. LEF. And I'm going to talk a bit more about this kernel in just a moment. So, graphically, what we're doing is here, this is just a 1D time series generated from a matern process, but it's a bit like an autoregressive continuous process. Then in white, we see the periodogram, quite noisy, even in decibel space. Even in decibel space, x-axis is frequency, and that's in white. In green, we see the spectral density, the theoretical form under the parameters used to generate the time series. And this is the blurring, this is the leakage, right, that high frequency that you'll observe with higher order autoregressive processes and many other processes with a large dynamic range. We'll see that the spectral density in the We'll see that the spectral density in the periodogram just don't line up because of this leakage. Some of that's due to aliasing. So the yellow line improves things a little bit, but there's still quite a big gap of around almost 20 decibels there. So one option is you could fix things by moving the white line. You could incorporate multi-tapering, for example, move the white line down towards the yellow line. And you could do that. And you could do that. But what we're saying that you should do is whatever spectral estimate you take, fit it to its expectation. So the red line here is the theoretical expected periodogram. And you can see, as you'd expect, it nicely overlays the actual observed periodogram. So we're basically saying, okay, you don't know the parameters of red. Well, fit that red line to the white line. That's basically what the D. White line. That's basically what the de-biased witter likelihood is. But we can incorporate tapering. So if we move the white line, we can move the red line as well by putting that the taper into our expected periodogram formulation, which I'll shortly show you. So this isn't instead of tapering, it's more a procedure we recommend in addition to tapering if you're performing with all likelihood inference. HID inference. So, what is that curly F that I just spoke about, that convolution that exists in the expected periodogram? Well, many of you will have seen before in 1D and with regularly sampled data, then it's the well-known Feyer kernel, which comes in this form here. Now, in general, when we have higher dimensions and missing data, then it takes a more generalized Then it takes a more generalized form here, which I'm keeping as an FFT for now, which is we simply take a Fourier transform off our, if you like, our modulation function, or it could be our taper, and we scale it accordingly. Okay. But even after we've done that, we have a question about, well, how do we actually efficiently compute this convolution? Convolutions. Compute this convolution. Convolutions can be a nightmare to compute in MATLAB or whatever you use. How do we keep things n log n? Well, there's a trick. And again, you can refer back to Percival and Warden books to find this, which is that a convolution in the frequency domain is a multiplication in the time domain. So if we go back to the time domain, then we can see actually our expected period. Our expected periodogram we can represent as a multiplication between the auto covariance, the Fourier transform of the spectrum, and this triangle kernel here. Right? And then all we need to do is FFT it at the end, which we can do in n log n operations. So we have this triangle kernel multiplying our auto covariance, and we use that to find our expected periodogram. So this equation you can find in Percival and Walden. And what we're saying is that. Boon Walden, and what we're saying is that you should put this into your witted likelihood. So we can compute it directly and exactly, subject to the accuracies of FFT. And therefore, the D-bias Witter likelihood is now uses two FFTs rather than one. We need to compute the periodogram and we need to compute the expected periodogram. And it's still, and it retains its order n log n desirable. Computational speed. Now, in general, when we have high dimensions and missing data and/or tapering, then we can still do things and then log in. So, again, we do the same trick. We go into the time domain, we have our auto covariance function, and now instead of a triangle kernel, we have this kernel CGN. And what CGN is, again, those of you who've studied and personally. Again, those of you who've studied in Percival and Warden will have seen things like this for tapers already, which is that we need to compute here on the top a summation of GS and GS at lag U, and then scale it down accordingly. And actually, well, this can be pre-computed using FFTs because if you think about it, this sits outside of the optimization procedure. There's no Of the optimization procedure, there's no actual parameter involved here. This is something you set beforehand, and it can be computed using just one FFT. So, there are tricks such that you can compute this equation to with an FFT, and we've coded it up for people to use. So, again, it retains its order n log n desirable characteristics. Okay, so that's the theory and the methodology. I just want to show you some examples. I just want to show you some examples for the next five, ten minutes. I've worked a lot with ocean drifters. For those of you who don't know what they are, they're buoys or buoys that have been deployed in the ocean, but without being moored to the ground, they drift around freely, mimicking a particle of water at the surface or near the surface to sort of understand ocean currents. And these sort of trajectories that you see in the top. Trajectories that you see in the top row here of three drifters, one in each of them, Atlantic, Pacific, and Indian Oceans. There's their long latte coordinates. And you can see how they've been tracked by satellites as they traverse the ocean. Now, if we take those positions and essentially use those to compute their velocities, then we get time series. And here are 50-day time series from each of those drifters if their speed in Of their speed in meters per second in each direction, red and blue, corresponding to longitude and latitude. So you can see the Indian Ocean drifter is much faster, it goes up to one meter per second. The Atlantic Ocean drifter, in this case, is slower. Now, if you compute the power spectra, assume these are stationary, which they more or less are in these segments, if you compute If you compute their power spectra, then you see something that what you see on the bottom row here in cycles per day measured in decibels. And what we've been doing in recent years is trying to understand these spectra. They have a high frequency drop-off, a kind of a low frequency plateau, and some kind of rate at which they drop off. And the black line you see is a fit of a matern process. Is a fit of a matern process, which with three parameters can capture both the amplitude of the spectrum, the rate of decay of the slope, but also the plateau effect at low frequencies that you see. And so, what happens if we try and fit these matern processes to these drifter velocities? Well, I'm going to show you results from three methods, one for each. Results from three methods, one for each drifter. So here we have Atlantic, Pacific, and Indian, and for each one, we've computed exact maximum likelihood, which we could was tractable for these short examples, de-biased Whittle and Standard Whittle. And what we found was that standard Whittle gave these biased parameter estimates that I discussed at the beginning. It was getting, it's getting that the slope in this case is the... That the slope in this case is the drop-off. The damping is the plateau at the beginning, and the diffusivity is a measure of the turbulence, it's a measure of the overall amplitude of the spectra. And in all three cases, we're seeing it's quite far from maximum likelihood. It's very fast. CPU time on the far right there shows it's a very fast computation, but it's quite approximate. And here we found de-bias wittel to be a really good compromise. To be a really good compromise, it's marginally faster, marginally slower than standard Whittle, but the parameter estimates are very, very close to exact maximum likelihood at a fraction of the computational speed. A global drifter program has over 100 million data points. So this, if we now implement this on the global data set, we appreciate having a faster method versus maximum likelihood, which will clog up several cores. Several cores, the bias whittler becomes more practical now to run this on hundreds of millions of data points. And this is what kind of the actual method, this was the data we had in mind when we first devised the methodology. Here's a spatial example. So these are random fields from Venus. And on the top left is some real data. This is measuring topography from Venus. From Venus. And what we've done is we fitted a matern process in 2D to this data, and then using three techniques, the devised whittle, regular whittle, and the tapered whittle. And what we found is that if we then simulate from the matern with using those estimated parameters, we get random fields, characteristic random fields, as displayed here. So these are simulated from the model. Simulated from the model using the parameters estimated by each technique. And we find that if we use Whittle or tapered Whittle, like in the bottom row, we get random fields that don't look like the original, which might make you think the model is a poor choice of model. But actually, it's the parameters that were the wrong. The model may be wrong too, of course, but the parameters certainly look very wrong here. When we use the debiased whittle, like in the top right, we see we get something that looks like the We see we get something that looks like the right kind of roughness, the right kind of length correlation length scales in each dimension, and so on. So we found vastly improved results with random fields as well. In the interest of time, and for this audience, I'll stick, I'm going to skip over the theory, just to say that we have a lot there. Under certain assumptions, they're listed here. Don't read them now or refer to the paper. We can guarantee this. This consistency and asymptotic normality and so on, and convergence rates, which are root n, like I said at the beginning. So we can converge the true parameter estimates, just like standard Whittle does. And actually, we found that we could weaken the assumptions as well. So we could work with discontinuous spectral densities, but there were some advantages. Were some advantages. Moving to the general case where we have missing data, there were some additional assumptions required because you can't just arbitrarily allow any amount of missing data. There were two key things. The first, again, don't worry about all the details here, but the first assumption here on the top row of equation three is a sort of law of large numbers sense. The CG kernel, which is G was our. Our function, that our modulating function, we need to observe enough data in a law of large numbers sense to get towards consistency and our convergence rates. And also, we need this idea of for two different parameters, theta one and theta two, in equation four, this idea of things being asymptotically distant. So for two different parameters, where we're observing the data. Where we're observing the data, we can actually tell apart two different parameters, theta one and theta two. So these are sort of important extra assumptions you need to make when you have missing data. And with these additional assumptions, we can still have this root n-type convergence rates and asymptotic normality of our estimates. And examples and counterexamples can be found in the paper, as well as handling multivariate processes. Multivariate processes. So I've just got a couple more examples to show you before I wrap up. We've compared with Guinness and Fuentes who do these circular embedding techniques shown in green and blue here. Our method is in red. And what we find is broadly speaking, we're equivalent to them, but we're faster. Circular embedding actually requires you to FFT longer, larger processes and the cost of And the cost of the costs involved in all the steps. We found that debias woodl because we're just taking two n log n operations, we don't have to make n bigger. We have a fraction of the CPU time as shown on the right. And in this case, the true parameter is five and we converge to it immediately, whereas they take longer to converge. Missing data, we can handle a lot of it. So here's some examples of some satellite. Off some satellite data, you know, the typical sort of stripe patterns you see. We have 72% of data missing in this top example. And we found that compared with techniques by Fuentes, who dealt with missing data, we can get a much, this bottom figure shows we can get much better estimates of the true range parameter of the data here. Just some fun examples. So, Arthur's been working on So, Arthur's been working on this. So, the missing data, we can think about different sampling schemes. For example, if we sample in circles here from a random, a 2D random field, then this is what the expected periodogram looks like. And see how different, this is for a matern. See how a matern has a smooth decay, right? See how this doesn't have a smooth decay. So, if we fit the true matern. The true matern using regular Whittle, we'd get wonky parameter estimates because we're not accounting for the sampling scheme. Because our periodogram is going to look like what we see on the right here, a noisier version of this. But this is the expected periodogram on the right. And if we fit that to the periodogram, we recover these better parameter estimates. So it's a nice display of why it's important to use the expected periodogram. Another one here, Arthur's from France. So we see sampling from France here. Sampling from France here. So your edges are wonky here, and that also creates a roughness in the expected periodogram. So we can try and deal with, we can improve the Wittel estimates there as well at those high frequency regions. Final example, this will just take me a couple of minutes, is Jake Ranger, who I've been working with the last few years on his PhD, has been looking On his PhD, he has been looking at not matern processes but more complicated ones. People may recognize this model here, which comes from surface elevation from wave heights in the ocean. And this is what's called a John Swap spectral density. Some people may be familiar with it. It's got more parameters. But it has matern-like characteristics. It has a decay at high frequencies, but it has a peak rather than having a low frequency. Has a peak rather than having a low frequency plateau, it has a peak at the frequency of the wave. And here's Jake's done some fits here to observe Boy data. And you can see these fits here to periodograms. And what you'll note is that this is Wind C, this peak here. The low frequencies correspond to swell waves. And we can incorporate our procedure to be semi-parametric. So we can fit the Wind C and ignore the swell by performing our By performing our fit over selected frequencies. We're not the first to do this, people have done this lots before, which is to use the Whittle for regions of your spectrum and not everywhere. And also, we can capture non-stationarity by doing rolling windows. We could also do something more fancy, and we've been thinking about that. And this final slide is a comparison of techniques, and the bottom axis here is least squares and Bartlett least squares. So this goes back to my And Bartlett least squares. So, this goes back to my first slide: that if you use least squares, Bartlett least squares is using a smoothed spectrum least squares. Multi-taper would show similar, that we get parameter estimates here. These are the parameters of the JohnSwap, which are more or less centered at their correct value, but with very high variance. So, least squares gives you very high variance estimates, is what we generally found. The next three techniques are all Whittle techniques. This is Whittle. Techniques. This is Whittle, aliased Whittle, and our technique de-biased Whittle. We managed to reduce that variance, but also correct for the bias of Whittle. So the third column, you see that the spread is the distributions are located away from their true value. That's the bias. And we can correct for that bias and get very similar results to maximum likelihood when this is computationally available. So for some resources, we have a demo. So, for some resources, we have a demo. This is really fun, actually. The stuff I showed with France and the circles and stuff, this is available in a live demo that Arthur's put together. There's the link. We have a Python package available called DBiased Spatial Whittle. It'll work for time series too. Just set D equals one. A GitHub repository for all the time for the time series paper is on my GitHub and on Arthur's GitHub for the spatial paper. For the spatial paper. And then for the references, the device-whittle paper is in Biometrica. The spatial version is in series B of JRSS. That's in Press. And then Art has done some interesting work if you're interested in modulation from his PhD in the journal of time series analysis. And Jake's work is in ocean engineering on the ocean wave height spectra fits. So thank you. And sorry if I ran over a couple of minutes, but I think. If I ran over a couple of minutes, but I think I started late too.