For inviting me to this really wonderful meeting here. I'm very happy to be here. So, I will talk about true complement dominance, and I will also show you some applications of weight equations. And what I will speak about will be mainly based on this paper that you can find on archive. The title is Shoe Contem and Dominant Operator Matrices, and there will be also some part based on a preference together with Feta, Schroedinger Operators with Accuitive Potentials in Weighted Spaces. Let me give you an outline. Let me give you an outline first, and I will introduce you. Oh, excuse me. So, first, I will introduce you to or motivate you why to look at operator matrices and study their spectra. I will tell you why true complements are useful for that, and there will be also a little internet, so on. In Tennessee, on a very well-known representation theorem, the Lax Milvern theorem, which I will put into maybe a bit different language, that will hopefully help you to understand the abstract results in this main section number two. And then I will show you the applications to certain wave equations, and I hope that there will be enough time that I can mention also other applications that you can find in the paper on archive. Okay, so let's start with the introduction. Okay, so let's start with the introduction. Why to look at operator matrices? If you think about PDEs coming from physics, in particular this damped wave equation that we will study also later. This is a T is the time variable, X is the spatial variable living in some open subset of R D. And this is a second order equation. In time, this is the damping term. A, you should now, for now, only think about a non-negative function. And this Q is the potential. function and this q is the potential also of this non-negative function. Okay, then what you can do is with some standard transformations you can transform this to a first order problem in time and the price you pay is that in now in this abstract Cauchy problem the differential expression on the right hand side is a matrix differential expression. And if you think about semi-group theory one way to study such equations is to implement the To implement the operator or the differential expression on the right-hand side in a suitable Hilbert space, which now should be a product Hilbert space because of this matrix structure. So the orthogonal direct two Hilbert spaces, H1 and H2. And what you would like to have as properties of this operator that you end up with, it should have dense domain and the resolvent set. And once having defined such, let's call it a good operator, Let's call it a good operator. You are interested in the structure and location of the spectrum, and ideally also in some statement about the behavior of the resolvent more. Okay, now for a moment, forget about the matrix structure. How to come up with such a good operator, so dense domain and non-empty spectrum, and think about this differential expression that is the Schrodinger operator in the space H, which is now just L2 of omega. Which is now just L2 of omega. And for V, you should think about locally integrable potential that is sectorial for now. Then what you can do is you can plug this differential expression into the scalar product of the space, and this will give you a sesquilinear form that is bounded on some natural domain. In this case, the form domain is H zero one of omega intersected with the square root of the modulus of the potential. Of the modulus of the potential and the form you just obtained by plugging A into the scalar product and integrating by parts. Okay, now instead of this form, I want to think about an operator. I will call it distributional operator. So I want to associate with this form an operator that I call a hat, which will be a bounded operator from the space of test functions to the dual space of v. And it will act, it will take a test function. X, it will take a test function and it will give me back this distribution. Okay, and this is indeed a bounded operator from test functions to dual space of test functions. Okay, then the setting should be that this form domain, this Hilbert space V, should be dense in the Hilbert space H where you need to implement your operator, and it should be continuously embedded there. And if you have this, then actually one can identify H with H star. Identify H with H star, and you will get a triplet of spaces centered around H. So V will be dense in H and continuously embedded, and H will be dense in V star and continuously embedded there. And now the operator that you end up with should be the maximal restriction of this distributional guy to the maximal domain so that you don't go outside of this space in the middle. So this is just the restriction of this distributional operation. The restriction of this distributional operator to the pre-image of h under a hat. Okay, and now what does the Lax-Milgram theorem say? It says that if this quantity here, if the quadratic form of A, is not only bounded above by constant times norm square, but also below, which is called corrosivity. So if such corrosivity condition is satisfied, meaning that actually the norm that is generated The norm that is generated by this form is an equivalent norm to the norm on the space, on the form domain, then this implies that this maximal operator, this operator in H, is boundedly invertible there and it has a dense domain. And now what is behind this statement, so how you get this statement is actually that these conditions imply that A-hat is an isomorphism between Is an isomorphism between test functions and distributions. So it is not only bounded from V to V star, but also boundedly invertible from V star to V. Okay, now let's go back to operator matrices. So you know from the P D E that you are given how this matrix should act. You know the action. So the non-trivial question is only what should be a good domain for this operator. Domain for this operator. Now, this domain is a subspace of this product space. And first, maybe the first idea or naive idea is to simply say, okay, I assign domains to ABCD and then I just take the intersection in the first and second component. But the problem is that this domain is typically too small already when you think about density, just because all of these four domains are dense. All of these four domains are dense, doesn't mean that the domain of this matrix given in this way will be a dense domain. So, one thing that you can do is you can look at the true complement of the matrix. Here I will tell you everything for the first true complement. There is also a second true complement, which is completely analogous. So, the first true complement you can use for such lambda that are in the resolvent set of d. So, if d minus lambda is inverted. So if d minus lambda is invertible, then I can define for this lambda such operator function. Now this depends on lambda. And then there is a formal identity, namely an identity relating the inverse of A minus lambda and the inverse of this Schwarz complement. This is called the Fobenius-Schwar factorization of the resolvent, and this is something that you can use already for scalar 2 by 2 matrix. For scalar 2 by 2 matrices. So if these ABCD are just numbers, then this identity will give you a relation between the spectrum of this matrix and the zeros of a certain rational function. Okay, now this you can also do if ABCD are bounded operators. You can also make sense of this formula and you see here all of this, everything that is not yellow is bounded. So you will have a one-to-one correspondence between the proper. Between the properties of this inverse and the properties of this inverse. And by this, you relate spectrum of S to spectrum of the matrix. It becomes more complicated when the entries are unbounded operators, because then, of course, you have unbounded operators, you will run into troubles with domains, but you can assume suitable relative boundedness within the entries. For instance, if you assume that B is d-bounded, then this combination here. Then this combination here will be a bounded operator, etc. So you can put sufficient conditions such that you can still make sense of this equality and such that you can still relate the spectrum of the matrix and the spectrum of the true complement. Okay, and somehow the main goal of this talk will be to give as weak as possible assumptions to make sense of this. To make sense of this equality rigorously and to relate the spectrum of the Schwarz complement to the spectrum of the matrix. And if you think about the previous slide, thinking about applying a representation theorem to the Schwarz complement, getting a good representation of the Schur complement, what are as weak as possible assumptions so that I can transport this representation to obtain a good representation of the operator. Representation of the operator matrix. Okay, and of course, once having established such a connection, you can go ahead and study the spectrum of the matrix by studying the spectrum of the Schuylki. And in addition, you can also obtain resolvent estimates from this formula. Okay, so let's get to the main part of this talk. So I just repeated, you hear this. I just repeated you here this Faubenius Schul factorization. So, what are the ideas behind what I will show you? The first idea is that it is enough that the Schul complement is worse than some neighboring factors in this formula. So it is not needed, for example, that this Bd minus lambda inverse is a bounded operator. It's okay if it is unbounded, as long as it is not more unbounded than the show complement, because in this formula of the resolvent, only this combination needs to be bounded. This combination needs to be bounded. Now, this is an idea that was already applied before. For instance, here in this paper, Freitas, Siegel, and Treta for the Dempwave equation, and in these three papers, Ivovimov, Siegel, Trete, Ivorgimov, and Ivorgimov, Trete for matrix differential operators of a certain type. And so the next idea is that also you don't need to map always back to the Hilbert. To map always back to the Hilbert space. It's okay if you have given your entries as distributional operators, like I showed you before, as long as if you do all these compositions here in this formula, in the end you map back and you land back in the space. And this is also something that was there before, for example, for the Dempwe equation in this book by Amari and Lucase, but the point there was that these spaces of test functions in this. Of test functions and distributions, they were determined by some of the entries, for instance. So, for instance, form domain and dual of the form domain of some of the entries. And the difference here is that the spaces of test functions and distributions, they really come from the Schur complement. And this is also where the name Schwarz complement dominance comes from. And this is a very non-linear approach in the sense that you look at the macro. That you look at the matrix first, then you compose the formula for the short complement, then you look at how the shoe complement is related to the other terms in this formula, and then you go back to defining the entries of the matrix so that you end up with the show complement that you actually wanted. So you are going in a circle. And what I want to say that on one hand, of course, so I will show you abstract results. So in this way, it is more general than these references that I mentioned. General than these references that I mentioned, but even applied to these references here or to the problems in these references, you can weaken the assumptions that appear therein. And why? Because these two approaches are combined. So in these ones, the distributional approach was missing, and here there was no dominant shoe complement used. So somehow the strength is to combine these two things. And I want to just for And I want to just, for now, briefly mention some other references that were actually pointed out to me by Jean-Claude, who is also here today, and that I was unaware of when I was doing this. And these are these three papers, but I will speak about it later more precisely, but these three references actually do combine these two ideas, but in a subsequent setting. Okay, so what do we do? In both of the components of the space H1, H2, we put their such distributional triple. So there is space DS densely continuously embedded in H1 and H1 densely continuously embedded in D minus S. The S here is because the Schwarz complement lives in the first component. So if you want, you can think about form domain of the Schwarz complement and the dual of the form domain, but it is not restricted. The form domain, but it is not restricted to only such cases. But this can be the example that you have in mind. And in the second component, we also extend by d2 and d minus 2. And now the operator matrix is given as a distributional operator, so the entries act boundedly from the spaces of test functions to the spaces of distributions in an according way. So for instance, A-hat goes boundedly from DS. A hat goes boundedly from ds to d minus s, etc. Now, for such lambda, where this is a bit of an abusive notation here, it's a resolvent set of d hat, which I want to call all lambda such that d hat minus lambda is isomorphic between d2 and d minus 2. Okay, so d hat minus lambda inverse should be bounded from d minus 2 to d2. For such lambda, I can define also the Shur complement. Now, again, in a distributional way. And now, again, in a distributional way. So, I don't care about landing back in any Hilbert space, I just compose my distributional operators and I obtain a family of operators acting boundedly from ds to d minus s. So again, from test functions to distributions. And then the objects of interest, actually, of the next theorem I will show you, are again the maximal restrictions to the respective spaces. So I take the maximal restrictions. So, I take the maximal restriction of the matrix to the pre-image of H under A head, etc. Also, the same for the Shoe complement. And I want to point out, of course, behind there are these bounded objects that map between these, let's say, strange spaces, so from test functions to distributions, but actually the objects that will be in the statement are now these things, and these are really unbounded operator matrix in H and H. Operator, matrix in H and family of unbounded operators in H1. So I end up really in the spaces where I wanted to end up with these objects. Now, what is the theorem that for a set of parameters that are good parameters lambda in this resolvent set of the hat? Now, what are such good parameters? Those are all parameters lambda such that there exists a shift such that the distributional operator, if I shift it, Operator, if I shift it, is isomorphic between ds and d minus s. Now, this looks already a little bit like what I showed you in the Lux Milogram theorem before. Now, what happens for such parameters? It happens that on the set theta, the spectrum, the point spectrum, and the essential spectrum of the matrix A and the Schwarz component S, they really do coincide. Now, this essential spectrum, this is the Essential spectrum. This is the essential spectrum that can be characterized via singular sequences. If you know in the book by Edmunds and Evans, it's the second type of essential spectrum. Okay, now moreover, if this first identity here is not trivial, meaning that if the spectrum of S here is not all theta, but there is one point that is actually in the resolvent set of S, then by this identity I get that also the resolvent. This identity, I get that also the resolvent set of the matrix is non-empty and the domain of A is dense in H. Okay, now as I already indicated before, for example, not necessarily, but this condition here, this assumption, can be established by, for instance, form representation methods, because this is exactly looks like the statement of Lux Milogram that I showed you before. And then I also want to mention that this generalizes these patterns of diagonal dominance that I told you before. For instance, the very first one that I am aware of, which is for diagonal dominance, operator matrices by Nagel in 89. And you can also find several results of this type, later results, more general ones, in the book by Christiane Pretta. Okay, so let's go to the applications. I need to hurry a bit. So I will show you how to implement this operator matrix that we already saw with non-negative damping and potential. And the space should be this space, so in second component L2, and in the first component, it is this W. Don't worry about it. I won't go into it. It's the standard choice for this problem of space. standard choice for this problem of space. And for the damping now, think only about non-negative locally, sorry, the potential, non-negative locally integrable potential. One can do also more general like the damping, but now it's not the point. And the damping here, that is this term, can be actually any non-negative form on C0 infinity. Okay, so for instance, you can think about like the potential here that is just a locally integrable function. Locally integrable function. And this form is just the form of the multiplication operator. Or it can be of direct delta type. For example, these papers by Krezzejik with Kurimawa and Boyer or the book that I mentioned earlier. Or in higher dimensions, you can think about gamma to be some hypersurface, and you can take a non-negative locally integrable function on this hypersurface and consider this form as the damping. Damping. Why is this possible? This is somehow very, very irregular, very general. The reason is because I told you the worst guy has to be the show complement. So if I make this damping worse, I also make the show complement worse. So I can make the damping bad because also the show complement will become bad and it will still dominate the other terms that appear in the formula. Now, what are the spaces? So, most important in all of this business. So, most important in all of this business are the spaces of test functions and distributions. ds is the closure of c0 infinity with respect to this norm and indeed if you think about the formula for the school complement then you see that you will end up with simply the form domain of the school complement for ds and b minus s is again the the dual of the form domain and what happens here is that in the first component Here is that in the first component, nothing happens actually. So one does not extend at all in the first component. So no non-trivial spaces of test functions or distributions. And now the domains of A and S are simply these maximal domains. It doesn't really matter now. You don't need to focus on the formula. I just want to mention that for the matrix, only one condition comes on the second component because in the first component we don't extend. So this maximum. We don't extend. So, this maximal domain condition arises only in the second component. And what is the theorem? That actually, this operator matrix that you get by this procedure minus the matrix is L-macrative, meaning that, or implying that it generates a strongly continuous contraction semi-group. And we have the equivalence of the spectrum, point spectrum, and the central spectrum of matrix and Schuylki complement on the set of good. On the set of good parameters, which is now C minus the non-negative semi-axis. And as you saw before, we have DS, D minus S are the form domain and dual of the form domain. And indeed, this set of good parameters condition is implemented by the Lax-Milgram theorem. So the School complement arises due to the Lax-Milgram theorem. And if there is some additional condition, namely that the damping That the damping is relatively bounded with respect to this term in the form sense with bound zero, then actually all of the parameters are good. One has Laxmilgram for all, or shifted Lax Milgram for all of the parameters, and one gets the full equivalence except in zero. Now, this paper that I mentioned before, there this was implemented under more restrictive assumptions because there was not the distributional approach, so the operator domain had to be described. Domain had to be described of the Schur complement, and there these assumptions arose. And what was done there also is that then the Schur complement was studied in order to obtain statements about the spectrum of the matrix. And I want to say that actually under these more restrictive conditions, these two operators are the same, these matrices, that the ones that, the one that is constructed here and the one that was constructed in this paper. Okay. In this paper. Okay, now quickly, let me show you how to implement more general dampings, namely with accretive A here and some differential term and in a weighted space. Now, the weight should be positive, A should be accretive, and this should be a positive semi-definite matrix. And if you look at the Schur complement, then you see that the formula looks more complicated. Plus, the Schur complement lives in a weighted space. So, actually, one cannot apply. So, actually, one cannot apply Lux Milgram, as I said before, but one has to implement Schrodinger operators of this type, where this B is accretive, and here you have a sectorial matrix in a weighted space. And this is what you can find in this fragment with Peter Seeger. And the way how we did this is that we derived general coercivity, generalized coercivity estimates that were needed for generalized Lax Mildrum theorem due to Armour and Elva. Due to Imo NQ. Okay, now under certain assumptions that arise from this, the statement is that minus A is not accretive, but after shift it is M-Accreative, and this implies that you get a strongly continuous semi-group. Now, there are these assumptions that I think I don't have time to show you, so I would like to just briefly say that these first two arise from these generalized periods estimates and this. Percivity estimates, and this third one is from the shifted operativity. And I will go quickly to the last slide where I want to show you the types of other applications that you can find in this paper on archive. So, for example, in astrophysics, certain types of second-order matrix differential equations arise. Then, just quickly, Klein-Borden operators with purely imaginary potentials, where you can show that actually. Potentials where you can show that actually for Ix the spectrum is empty, just like for ARI. And unfortunately, I don't have much time, but you can also show self-adjointness for Dirac operators with Coulomb type potentials. And this relates again to these three papers that I mentioned before. So, thank you very much for your attention. Thank you for the talk. Thank you for the talk. Are there any questions, comments? Yes, I mean we had the same issue with Bloch Doy operator. I didn't see that you mentioned it in your example. The question we were brought up is if the true complement is compact, whether the original resolvent is compact for which values. Have you ever looked into that? I mean it's I have not, but I think one can. I think one it might be possible. It might be possible to get it. I think it's the conclusion of all the point spectrum, for instance, can be derived from such that's what we thought about. So you mean if you get equivalence of compactness, compactness of all sides, the point spectrum I think should be the same, or only point spectrum. Yes, yes, but I think you can even you can just But I think you can even just conclude from compactness of the Schug complement, you should be able to conclude compactness of the resolve just from this formula. So you just need to give sense to the spaces in between. But I think you can do that. But the simple things that we say are particularly blocked, which is minus delta plus, you have the electrofield. Plus, you have the vector field that we make detect the products. So it's the matrix is singular. In our case, it has a zero eigenvalue. Okay. I don't know this particular case, but I would guess maybe you can do something with this, yes? So we can discuss it if you want, that would be interesting. So, further to Janine's question, you can also add more structure to the compactness. For example, finite trace. Yes. So, nuclearity, you will preserve through the SURE complement, and you can study the program in a worse norm, in a stronger norm. I think it should be possible, yes. Just do to this formula. More questions, comments? Online participants? I'm online here. So we see that there's the equivalence of spectrum between the sure component and the matrix. Can you also get equivalence of resolvent estimates? I think what you can I think what you can do is that if you have a resolvent estimate for the Schu complement, you can get one for the matrix. So this, this, I think, or I mean, again, just from this formula, everything boils down to this formula. You put there the correct spaces, but you will have to estimate the resolvent norm of the Schroed complement in different spaces. So you will have to estimate. So you you will have to estimate uh the inverse of the Schroh complement mapping from D minus S to D S. But but you can get some type of estimate, yes. Okay, we don't have any further questions, but thank the speaker again. 