Hello, everyone. My name is Vasilis Kliris, and I'm currently in the last 10 days of my PhD. And today, I will talk to you about the project I was working on for years now, which is real-time detection of unmodeled gravitational wave transits using convolutional neural networks in collaboration with Michael Norman and under the supervision of Patrick Satten. Uh, Patrick Saturn. Are you still still on my screen just to verify? Yes, yes. Good. Okay, so let's start with a challenge. So around my first year, there was the big hype where many papers about machine learning and for detection of regressional waves came out. And most of these applications were using, it was applied. We're using, it was applied, most of the applications were about CDCs or supernova on simulated waveforms. So, in general, it was waveforms that we already know how they look like, we know what we expect. And it was, and most of them were trying to compare themselves to how much too much filtering and try to surpass much filtering, or at least work the same more fast. Work the same more fast. Although there was not much, or not that I know, investigation on unmodeled signals. So this gave me the idea that I should focus mostly on this. So this project happening out of this. And most importantly, we could still see that most of this investigation were focusing on the efficiency because they really wanted to compare the Samsung to much filtering. Compared the sample to much filtering. And we're not really the, and they have a priority rather than the fossil lamp rate, which in LIGO we know is the main measure of events to be significant. So out of all this inspiration and seeing that the conflict exactly was like people had like arguments between how if we should use machine learning, should we not use machine learning? If we should use machine learning, should we not use machine learning, even if we have like an analytical way to do it? I decided to focus on working on the end model search and at the same time bridging the problem with false alarm. So the target is to have a machine learning pipeline that can detect any type of sub-second signals from noise. We focus on sub-second because, of course, the parameter space is big, but most events, the most energetic events we expect to see are. Energetic events will expect to see RB. And sorry, my mouse doesn't work very well. Okay. The second most important target for us is to have a false alarm rate, but it's at least once per year. This is to be able, so when we create this, whatever we create, if that model works, we need to model to be potentially useful to low latency search and low. Low latency search and low latency search has once per year threshold to be considered. So we need to have this as a minimum. Well, we started from once per month and we managed to reach once per year now. Third, we try to do that without using real detector noise, which is something unorthodox, somebody would say, because the main argument three years ago was we need to apply this. We have the proof of concept projects. Concept projects about CBCs and everything. But we need to apply these algorithms to real detector noise, to train with real detector noise, so they can detect the real detector noise better. And I was agreeing with that. Although there is a caveat there, if we want to train with real noise, we need to use the noise we actually want to process eventually. So we cannot have a model ready before a search. Before a search, we have to wait for the search to have some noise and then actually run it. So we focused on the Gaussian, not on the Gaussian optimal noise, Gaussian-like noise. And there are some advantages to this that we'll see later in the presentation. So finally, the most another important thing is to make the code reproducible and accessible. I wanted back then to when we started building this to be able to give opportunity to any, if we create the model, to be able to give it to anyone else and very easily be able to reproduce our results. So we created a library called Emily that can actually generate data for you and test a model with consistency to avoid having bags because machine learning is quite sensitive. A small difference can actually make a model explode. Actually, make a model explode metaphorically. So, this will want to be a robust object. So, I will go now with the data and how this project starts building out. The data are time series, we use a time series of duration of one second and sample frequency of 124. So, we stay in these frequencies because we expect that the first best we might detect will be in the Will be in the most sensitive band of likelihoods around up to 5,500 hertz. So, all of our instances of all instances on data have this one second of noise. And on it, sometimes we might have an injection type, an injection injection, which for training, we use the white noise baths. We use white noise baths because they're quite general, but as As a type of injection, and we can easily generalize, we can generalize them to all frequency bands and all frequency ranges. And the way we make them is we go to the frequency domain, we take in terms of f min and f max, we create white noise in this band, and then we translate this to the time domain using. Time domain using inverse Fourier transform, and every time we do that, we can generate a new instance of a signal with the same frequency rate and the same duration in this case. So, they're quite useful, and we decided to use them to generalize any possible signal. The frequency we used were between 40 and 480. We didn't want to go close to the close to the uh to the to the limits of our of our sample frequency and so we and we made sure that all frequencies are represented uh equally so we made sure there's no over representation of a frequency like the central frequencies in the template map we have uh eventually the injection durations are from 0.06 seconds again we didn't want to go close to the to the limits of the of the The limits of the one-second interval we have here. And all injections on the radges have a sigmoid envelope that smooths them down so they don't rapidly go to zero. So now go to the data types. So this is the basis of how the data look like. And now we have four data types that we generated for this project. One is optimal like a vergonoise. And when I say optimal, I mean we take the we take the We take the efficiency curve of all the detectors for A LIGO, for advanced LIGO and advanced Virgo. And we add some white noise on the frequency domain. So we create different instances of the same type of noise, of the same PSD, and generate different instances of noise every time. As you can see, it looks like it's partly white noise. In this case, White noise in this case, but it's based on the LIBO curves, like LIGO efficiency curves, sensitivity curves. So the second type is WMD injections, projected and time-shifted based on the random sky position, which also call coherent signals. And I'll explain, we'll see later why we call them coherent signals. As I said before, we try to do an old sky search, so we don't care about specific type of case. About specific location or localization for now, at least. So we generate random script opposition and we project any waveforms to our textus, to detectors, and then inject them on the noise. And they look like that. Again, the same characteristics for them. Main characteristics of them is they're nearly quick. Most of the time, they are around the same, appearing around the same time on the districts. Same time on the detectors. Third is again WMB injections to one randomly selected detector. And we do that because we find it's a good way to generate a glitch behavior. So when we inject one of, take one of this random WNB signals we generated and put it only one detector, random inslect detector, it looks a lot like a random glaze up here in the detectors. Again, I will show you again. Detectors. Again, I will show you again, I will explain again why we generate this. The final data type we have is WD injections randomly selected, but for each detector, we choose a different one. These we call the coherent signals. And because, of course, these are not real cases. We try to simulate an extreme case of simultaneous glitches, which they will be different, but they will happen at the same time. But they will happen at the same time. So, because this case doesn't have the time delays between the signals, are not come from astrophysical calculation or source, we use a time shift base value that we call disposition. And with a small graph here, I'm explaining this. So, we have three injections. We define a disposition value, like let's say 0.2 seconds, and then we spread those. And then we spread those three, the signals, equally, to make them equally distance. So we put one in the center, one in the right, one in the left, and the distance between them will be 0.1 seconds in this case. The agent for this is we want to control the random positioning of the signals. I'll show you examples now. So, again, this is type 4, but here it's signals labeled as noise. Sorry, I didn't decide, I wanted to explain that. The only, so we have four types. Uh, so we have four types, and only one is labeled as signal, and the other three are labeled as noise. So, we have a classification problem of two classes: noise and signal. Um, here you see an example of this incoherent signals. We have three different three different signals happening at the same time. So, this is a really challenging case for an algorithm that tries to figure out generic waveforms. Generic waveforms. Here is a disposition of 0.1. So they are distributed between in a range of 0.1 seconds, 0.2, 0.3. You see that they're different signals and they're positioned randomly in this interval. And eventually you have also random disposition positioning for the signals. We start by thinking by applying, when we start thinking about Apply we when we started thinking about this, we started with the running disposition, but then we wanted to control how grouped together, how close together the signals are. Just a summary of the types of data we use. One is noise, two is coherence injections of white noise bursts, three is glitches or white noise bursts injected only in one detector, and four is incoherent signals. So, different WNBs select different. W and Bs, select different frequencies, different durations, different everything, putting around the same time in this one second window we have. Now we'll talk about the models. I hope I'm not going too fast. So if you want any question at any point, please interrupt me or send it to the chat. So models. We use two models that try to answer two different questions. To answer two different questions instead of one, but with the traditional way. Model one asks the question: Is there a signal to more than two detectors? And model two asks the question if there's a coherency between any detectors. So I'll describe mostly model one here. So is there a signal to more than two detectors? How do we train a model that does that, answers this question? We train it with signals and noise, which is the obvious thing to do. Which is the obvious thing to do. Two classes. But we also wanted to ignore cases where a signal is present in one detector. We wanted to have at least two detectors having a signal. Same or different, doesn't matter. We just want to have a signal and some energy in one of the detectors. So to do that, we will also train with the glitch-like signals that I showed you before, that we randomly select one detector. But we randomly select one detector, inject one W and B. The model one is a relatively simple convolutional network that has like three residual blocks and ends with two dense layers. I won't focus on that much. You can discuss it later in the end of the presentation if you want. Model two. Is there any coherency between the detectors? If we want to. If we want to learn, teach a model to identify coherence and incoherency, incoherent intervals, we have to train it with coherent signals, as the real signals will be, and incoherent signals, the ones that describe with dispositions that I have four, or the random WNBs selected to different detectors. Yeah. That's the first point. And the idea of that the model. The idea of that the model can understand coherency is not very clever. I mean, in the beginning, when I was trying, when I was working on this, I assumed that machine learning is so clever, you can understand the coherency information, that the two signals are different coherent. So I decided to. I decided to feed this information by adding the Persian correlation. The Persian correlation is uses two pairs for each pair of detectors. We calculate the correlation for a given time lag between the detectors. So we fix one detector, Hanford detector, for example, and we shift all the other two detectors maximum pixels related to 32 milliseconds plus and minus because that's the maximum time delay we can get between the detectors. The time delay we can get between the detectors. This ends up to be 60 pixels plus 3, minus 3, minus 30, plus 30 pixels. So this model has two inputs instead. We have a strain input and a correlation input. But this is 60 by 3. So we have three detectors, eventually three pairs of detectors. The reason we do that, the reason we add the strain data is because it just helps the model understand the cohesion. The model understands the coherency. And again, a reminder: this model doesn't try to figure a signal from noise, it tries to understand coherent signal and incoherent signal. So, finally, we have two models and that answers the two questions. How do we combine these? First, we have the model one that gets input a strain. And if we get a number close to zero, it's no signal. If we get a number close to zero, it's no signal. If it gets a number close to one, it's a signal. And we have model two, which is a branched model that gets trained and correlations and input. And in the end, it will give us zero if there's no coherency between the data and one if it's coherency between the data. So by using them as voters, we multiply the score and eventually we get a final score that's between zero and one. So that's the main concept. That's the main con, that's the way we have, that's the model about this. So, how do we train the model? First, before I go to the approach we had about this, I want to talk about reproducibility of machine learning algorithms. It was many cases where I was trying to compare two different methods of data and of training. Or of training, and I was getting, I was doing the comparison, I was getting at the one of the other. Then, at some point afterwards, I will do the same, happen to do like a similar comparison, and this comparison won't be consistent. Why is this happening? Machine learning has its main weapon is the random initialization. It starts from random point in the loss plane. Point in the loss plane to reach the minimum. So every time we train a model, we have, we start from a different point, a randomly different point. Compare methods with machine learning when we have this issue. Because if we train a model twice, we will never get the same result. So I decided to approach this the same way I would approach a randomly in a normal physics experiment when I measured a random value with. Experiment when I measure a random value with a random error by doing multiple measurements. So, in this plot, you see a fossil lambrade test of once per month for model one, model two. But model one, I trained 10 model ones from scratch. I trained model one 10 times and 10 model twos. And as you can see in blue model one and red model two, every time I get a different false alarm rate. Time, I get a different false alarm rate curve. Note here that all these models are tested exactly on the same data, exactly on the same data, to make the comparison perfect. So we have this big variance of how the false lamp rate will look like. So if I didn't do that, there would be a case where I would consider that model, if the model one and model two were different methods, for example. Model one and model two were different methods, for example. I could say that model one, I would be able to say model one is much better than model two. Or in another case, I would do that just randomly, they would look to have the same performance. That's why it's important to do that and take the mean of this false alarm rate to be able to compare things. As you can see here, I have 10 model ones, 10 model twos. And by doing all the combinations of all of them, I get to 100, and it's the purple line. Purple line, and from this, I can choose Z, for example. This kind of method to make sure that the methods I claim are better than is consistent is the way to do reproducibility in machine learning. So I'll relate. But let me go from step by step how we do that. So we have, let's say we have two methods, one of the methods, one method. Have two methods, method one and method two. So we do a number, for example, 10 models from one, 10 models from two. We do a false-lamb rate test of once per month for all models with the same data. We take the averages, and based on that, we do an efficiency test using as threshold the score of the loudest event once per month in this case. And then based and then we choose the best performance of the model based on the efficiency. Uh, based on the efficiency, so the 50 percent, uh, when the efficiency goes to 50 percent, we use it as a measure of who is the best. I'll show an example of this kind of method on how to choose the best proportion ratios of data input, of data training data for this algorithm. So, in this one, in this problem, I show you a model, model one. I show you a model, Model 1, had three types of data: noise, coherent, signal, and glitch, but had two classes, noise, and signal. And Model 2 has four types of training data, and again, two classes. How do we choose what is best to train? Should we go 50-50? That's what literature said. But in the case that we don't, if we care, this is the 50-50-50-50 training ratio is when we care equally for. Here, equally for both classes, and as I said before, my main goal here is to make minimize the home rates enough, and then I will care about efficiency. So maybe I don't want a 50-50. What division of what should I use then? Because there was not a clear way to decide that. I decided to train different ratios and compare each other. So, for model one, we tested combinations of We tested combinations of training data size of one, two, three, with choices being 3,6363612 of 10,000. To put in a table so that it's not that confusing. So in the first column here, you see all of these three cases have are equal of 30,000, 30,000, 30,000. Then we have 60,000, 30,000, 30,000, etc. All of them, say again, it's times 10,000. Again, it's times the 10,000. For model two, we did the same. But this time, because we have four types and it will be exhausting to do more types of combinations, we have five and ten only. So we have noise. We have incoherent signals with this disposition value I was talking about before ranging from 0.1 to 0.5, plus the random case. Then the coherent signals with disposition of zero. And this is important because. Zero, and this is important because coherence with zero is the most challenging, it's the one that will bring the model to the limits because you have coincident signals, but different signals, and this is wrong. And coherent signals eventually was the only class, the only signal type, only type class labeled as signal. So, for each one of these columns, you can see here of data in data. Of data in data training, training data proportions. We train seven models and the same thing for model one, seven models for each one of these. And then we can use the combination of, for example, three, three, three with five, five, ten, five. And all the combination between them gives you seven by seven, 49 independent models. And from these 49 models, we can make the average. We're going to make the average. So, all these combinations are 192 combinations. And here you see the averages of this, each 49 models. It gets, I know that sounds might get confusing up to a point. So if you want any questions, just let me know. There's too many models, and this is only the averages. It's not all the models. Clearly, from the averages, we have methods that are better than others, but this all simply. Than others, but this all seem to cluster around. Based on the, as I said, based on the false alarm rate threshold for once per month, this is the 10th loudest, the 100 loudest events for simplicity and memory, of course. So we use the fossil array threshold here to calculate the efficiency for each one of these models I was talking about. And clearly, from efficiency also on WNB. From efficiency also on WNBs, we see that there is a difference between them. So, based on the 50% threshold here, we can decide which is better and which is not. Then, the winners are those two. And here comes the argument of not using real noise. As you can see, both models prefer the proportions that favor the glitches. So, model one, for example, needs more glitches to train. For example, it needs more glitches to train, it doesn't care much about the signal or the noise, it cases more, it cares more about the glitches, it needs to know what this thing I don't want to classify a signal is. The same happens to the model two. It needs to see incoherent signals much more often than see just coherent signals or noise. This shows me that real noise is not enough to simulate to To simulate, to train, it has not doesn't have enough examples of glitches to train machine learning algorithms to do that job. For example, if I go to the case where these coherent are not, for example, this case where coherent signals are more than anything else, this fails tremendously. Really bad false alarm rate. Okay, now that we chose those two models, we have the ratios we want. Ratios we want. I want to talk about another approach to add regarding Virgo. Is Virgo contribution really helpful? And I'll show you the train of thought behind this. So when we were seeing, when we're watching the training data and testing data, we were seeing that Virgo in our presentation of Virgo, the PSD of Virgo in our representation and training data, was much better than the presentation we have in O2. Representation we have in O2. So the ratio between Virgo and Hanford Livingston was much more, much smaller than the real case. So we said, okay, probably we should maybe multiply the PSDO Virgo, the noise on Virgo, to make it more looking like more like O2. This will make sense. And eventually we trained different models with different elevation overgo, as I call it. So multiplication. So number two times two times this is D. Two times this PSD, three four times this PSD, eight times this PSD, 16 times this, 32 times this PSD. And we use the same methodology of I talked before to choose the best method. It was a big surprise to us to realize that our theory of just multiplying Virgo by two or four would improve things was not working. So, as you can see here, the photon rate slightly reduces, but the efficacy diagram. Rate slightly reduces, but the efficiency doesn't for the two or four. But as we elevate Virgo more and more and more and more, having like a cap around 32, the performance increases and the false alarm rate decreases tremendously. And this was a shock for a bit. Then we realized what was actually happening. By doing this, by elevating Virgo, the contribution of the injections, of the SNR in the And R in the injections towards Virgo was decreasing, decreasing more and more. So the model practically was ignoring Virgo unless the signal in the Virgo was really, really loud. In the beginning, we were shocked about this, but it's not something we haven't seen before. For a model search of CWB, for example, the HLV analysis, it's worse than HL. Which is sad because you have an extra detector, but this is true. So we realized that we were seeing something expected after all. So after all these models, we have the best proportion of the models and also this vertical elevation, we choose the best model of 32V. And this is the final result. This is the best model currently, which is also in the preprint of the paper. So this is. So, this is a comparison of the Pulsar rate of Gaussian noise in blue and the Pulsaram rate of O2HLV noise in orange for a Pulsarate test of 10 years. As I said before, we want a threshold of once per year. So our threshold is 0.3 right here. Based on this threshold, we can do an efficiency test on injections now. Here we use five. Oh, sorry, we show you. Oh, sorry. We show you the efficiency on four types of four types of known injections that are used widely. One of them is WNB in this case is the type of we actually trained with. Corkolab supernovae based on MULU 2012, the specific type of waveform projected many times. Cosine Goshen, the green one, CASPS, the pink one, and we will talk about why this is not efficient. We will talk about why this is not efficient in high senators later. And the CBCs, as you can see here, W and Bs and CBCs have the same efficiency around the same efficiency. So we train with WNBs and we are efficient to all other waveforms. So now the only thing left for us was to actually compare with the standard comparisons, the standard measures and tests are done to other pipelines, the CWB, which is the Pipelines, the CWUB, which is the current pipeline the most successful for unmodeled one model searches. So these are the waveforms we generated for using HRSS in this case, not SNR, to compare with CWB for O2. And the comparisons, and then we went to the 50% threshold here and used this as thresholds, as the threshold you see here, to compare the waveforms, the types of waveforms. The types of waveforms. And you can see that the efficiency is comparable. It's the same order of magnitude. Sometimes it's slightly better, sometimes slightly worse, but it's in the same order of magnitude. Important to note, though, because it would be really, really unfair to claim that we're in the same level. Oh, sorry. Yeah, CWB results are for false learning rate of 100 years, once per 100 years, and Years once per 100 years, and our results are for once per year. We can never say that we're better than CWB, but what we can say is that our any event that CWB can detect with a photon rate once per 100 years, we can detect with a photon rate of once per year, which is enough for a low latency search. And this is practically the big news for that. Finally, Finally, I want to talk about something you probably have noted in the efficiency curves regarding polarizations. You've seen in the efficiency curves here and here that there are some waveforms that underperform without an obvious reason, those three cusps due to five and the other one. The reason these underperform is because they are linearly polarized. So, practically, that means So, practically, that means that we're insensitive to linearly polarized signals. And that was also a surprise because I didn't know. This shows actually that the models are actually sensitive to polarization information. So now we have already running a project that we're trying to include those linearized polarization and have a model that understands, can detect both of them. So, some conclusions after all this. So, main advice or like wisdom out of all this project was that using two different models specialized on different features can reduce the fault number rate significantly. We don't need to create this super model that will have, we'll configure everything by itself. And it's unwise to do, even if we can, it's unwise. Unwise to do, even if you can, it's unwise to do it sometimes because machine learning is really sensitive to small changes. You change a small number in the whitening or any parameter, the whole model will collapse in the next time you will run it. We will test it with different data. So we need to be, it's easy to have smaller, less specialized models, more specialized models with less parameters that we can fix easily and have a big model that's really, really sensitive. Model that's really really sensitive. Training using optimal detector noise with control over the glitch number and non-Gaussian features can focus a training on ignoring those features. So as you saw here, we can instead of training with real noise, we can simulate the real noise and save ourselves from the bad habit of using part of the data we're going to actually analyze later. Oh, sorry. This seems to be a robust and computationally cheap way to potentially run a low-latency pipeline with inference times of 50 milliseconds for one second of processed data. So given that we have downloaded the data and processed them, it takes less than millisecond to 50 milliseconds to process one second of data. And this is quite good. And the main advantage is it's computationally cheap, doesn't cost money as much. Money as much. And finally, even at this proof of concept stage, our pipeline could potentially detect any triggers. Current bus pipelines cannot potentially rate once per year. And this is the main win for our project. And that's all from me. And thank you very much. Any questions? Thank you very much. So, any questions from Any questions from here or in Zoom? Yes, go ahead. Yeah, thank you, Vasilius, for the presentation. My first question is related with the architecture of the CNNs. Did you test with several number of Number of kernel convolutional filters, several number of stage. Did you test that, or you just selected one architecture and that was it? Yeah, yeah. No, I didn't start from scratch. No way, this will be quite unwise. So, this architecture, you see, at least for the first model. First model was proposed. It was a very good, it was a position on a paper. I don't have the reference now. I'm really sorry. I'll send it to you. About a way to make a time series signal time series detection model and things that people tried. So this seemed to fit better to my project. Although many things changed. I went trial and error, but it wasn't just. Went trial and error, but it wasn't just trial and error. The conclusion of Michael Norman that is also in the paper was that he has created a genetic algorithm that can actually test many of these models with different current size, different filter size, and all these kind of things to find the best number of filters. So it was very interesting in this case, for example, for problem, the kernel size or filters or filter. Size or filter or filter size. I don't know which term people use. Or 42 and 36 seems to be very, very sensitive to the model. So if I change this to 45, the model will just lose it. It's not going to train. But in the second model, these weights, which are also these filter sizes, which are also created from the genetic algorithm I was talking about, are not. About are not sensitive at all. So I can make this 64 and nothing will change. And it's very interesting to see this kind of differences between the models, but there's not really a clue why they're happening. But this is true. So it's not like it's not just decided to try this. I went through a lot of trials. And this paper specifically has also a previous version with where the model looks quite different. So this next version we have. So, this next version we have now, which I have updated now, has this updated version. And the genetic algorithm really, really helped because it narrowed down the area where I would have to do trial and error, because trial and error takes a long time. Thank you. And my other question is related with this picture you are showing right now. It's about the correlation. How did you compute this correlation? You compute this correlation between the three signals and Y16? So we have three inputs of the detectors. So each pair of the detectors, so we have like 1024 and 1024, let's say one pair. In this position, I've calculated the person correlation, it gives me a number. If I shifted one pixel, it gives me another number, another number, another number, as I shift pixels. Another number as a shift pixels. The 60 is plus 30 minus 30. Okay, it's 30 pixels, which corresponds to 32 milliseconds in real time, based on the sample frequency we have, which is the maximum diff time delay a signal can have from Virgo to Hanford or Livingston. So we want to just, this is the only window we care about. If there's correlation after this, we don't care because it's not astrophysical. Only in this. Not astrophysical, only this window is astrophysical. Did it answer your question? Okay, yes. Other questions? Oh, Deep, yes, go ahead. I don't think I can hear you for some reason. How about now? Yeah, I can hear you now. Yeah. Yeah, I was wondering that. Yeah, I was wondering that. Have you tried running your classifier on, say, the GWTC 2.1, the sub-threshold data? Because I think since GWTC2, there's also the collaboration has also started to put the sub-threshold events out in the public. So this would be a great comparison whether because you're going down to such lower false alarm rates, it would be a good comparison if you can recover all of those order thousands of events that are out there. Are thousands of events that are out there? So we currently work on doing using this model to do a search to 010203. So we already have some results. They're in my thesis recurrently. So eventually we'll be out there. We can detect some of the signals and we have also some sub-threshold events. But it's not that we have all of them. We have like five sub-thresholds, two over thresholds. Thresholds to over threshold. I mean, when I say threshold, it's the same as we detect for doing one month to one between once per month and once per year. That's why I consider in this case a threshold. I cannot say that in CBCs we detect everything. There's no way we do that. Also, because currently we realize that for O3A, this is the O3B, due to the fact that the detectors are more sensitive. The detectors are more sensitive, they have more glitches. And while I'm doing the time shifts for the fault lamb rate, many of these glitches coincide, coincide so good, they look so good, so similar, the model is not as good in all three, for example, for now. So we need to also do something for that because the pulsar rate increases due to them, because of them, because it's the tail, because of these coincident glitches, and we need to probably use gravity space. To probably use gravity spy along with this model to reduce the false alarm rate even more, otherwise, we won't have the same success. Okay, thanks. Any other questions? Yes. Hi, great talk. I was wondering if it makes any sense to play with the ratio of how much strain? Ratio of how much training do you give each of models? Like you give, maybe you could give like more training for the model one and a different rate proportion of training for the model two. Would that affect or wouldn't make any sense to try that? So the amount of training. So these models can train within 10 epochs. And this is good because if they couldn't, it would be really difficult for us to do this, generating this tremendous amount of models, train them from scratch. Tremendous amount of models trained them from scratch all this time. I've realized that if I let it, because the training is based on how fast it will go eventually to the lowest point of the loss, it doesn't change anything. So I compare them all to around the fest and epochs, making sure that in the first and epochs, how good they can go. And I'm always in the first and epochs, I'm always keeping the best. Keeping the best result on the loss, practically, and save the model in this stage. So I don't save just the last model after 10 epochs. I haven't done that, no. And I think the models need more optimizing for the training process because I went to a point and then I stopped optimizing. Point and then I stopped optimizing, so you're going to always do it more optimizing. And I don't think there's to think your question like how this could help me eventually if I do that. Because for a given initialization, random beginning of weights, you can be next to the minimum of the loss by randomness. By randomness. So there's no really, the amount of epochs you could train with is not really a clear measure, a fixed measure of a method, practically. There's not like the best amount of epochs to train with. The more epochs you go, probably the better, unless you can start overfitting. So it's not exactly there's a fixed way to do it. I cannot imagine a fixed way to do that. Fixed way to do that to find the best epoch number. Thank you. Any additional questions? If not, I think we can take Steve again. Thank you very much. And I think now. 