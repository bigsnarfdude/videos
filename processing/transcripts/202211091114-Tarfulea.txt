Maybe you see well the advantage of giving sort of a part two slide talk is that I don't have to spend as much time introducing the material. So I'll mention a few things that I think are important, that haven't been addressed yet. Yet, in particular, the Carlin representation, because I think that's important for understanding why the Boltzmann equation actually has a parabolic nature to it, and why people keep comparing it to, say, fractional heat equation. Then, I should go into the statement of the uniqueness theorem, as well as an outline of its proof, a top-down outline. I hope to emphasize some of the ideas and some tricks and also some. Some tricks and also some overarching methodologies that might be useful in other people's works. So I will try not to keep this boring, but it is top-down, so that means that if I run out of time, I can cut it off at any point, and you don't lose anything fundamental. This is in contrast to how it's presented in the paper and in most papers where it's bottom-up. You start by proving some lemmas that look completely irrelevant. Then these are used to prove propositions that look sort of like what you're aiming at, but not quite. And then finally you use them. At, but not quite, and then finally use them to prove the theorem. So, I think in a talk, you have to go the other way. So, for the setup, we are still looking at the non-cutoff Boltzmann equation. And we're going to work mainly in the following two norms. Weighted L infinity. So, that's the Japanese bracket notation, so I guess it's the first thing I can write down. Let's talk about that. Online they just see yeah but I don't know if we just conference the other one here no this is giving let's see what happens no now let's resume I mean We had this issue yesterday. Now it looks like it's switched. Okay. So these are the spaces we work with. Weights in velocity, but otherwise uniform in space and space and velocity. And then we have the kinetic holder spaces. Make very useful and it works on the regularity program for kinetic equations. But I'm not going to go into the But I'm not going to go into the exact definitions. There's a kinetic distance that measures Tx and the difference between points in Tx and V. And we're going to be using that as our base case. Is something still wrong? Yes, it's showing up in this. Can we just let Andre talk? Okay. I'll try to fix it. Well, you'll try to fix it. I'll just keep talking. No, they didn't remove it. It's good. You're good. You're good. Okay. So the most relevant idea for the kinetic holder space is that it measures distance in velocity like C alpha and x like C alpha over 1 plus 2s and then in T And then in T, like C alpha over to us. So that's sort of a comparison. If you have a function that is older continuous in V like so, X like so, T like so, then it should belong to a kinetic C alpha. And because of one of the ideas in the proof, we actually have to work with sort of horizontal slices. That's somewhat unnatural in terms of it erects this sort of pretty. Of it erects this sort of pretty scaling properties of this holder space. Now you're in an unnatural space because you only measure distance in X and V. So Chris mentioned in his talk that the Boltzmann equation resembles the fractional heat equation. Oh, with kinetic drift, of course, but everyone kind of knows about this, who knows about this, but I assumed that. But I assumed that this would get mentioned somewhere at some point in this workshop, but it hasn't yet, so I added this slide in. You can actually decompose the collision operator into the so-called Carleman representation, where you have a singular part, singular operator, and a non-singular operator. So the Boltzmann collision kernel or collision operator, you may remember, is an integral over five-dimensional space. You integrate over one of the possible. Integrate over one of the possible incoming velocities, and then you integrate over the sphere to measure the angle of collision in some sense. So, unsurprisingly, after you do this manipulation, you should still end up with the most significant, the singular part, could still be an integral over five dimensions. Now, if you ignore this w integral, this inside part, this kernel part, and just treat that as one, this is literally just. This is literally just with a principal value and up to a constant, it is the fractional Laplacian of order 2s. So the issue is that if you have a uniform lower bound on this quantity, maybe some good upper bounds, and it doesn't oscillate too bad, then you can compare it to a fraction of Laplacian. Chris's version had pulled out this factor. So the B tilde is part of the collision kernel, but it's uniformly positive. But it's uniformly positive and bounded from below and above, so we usually ignore it. For estimates, we always ignore it. Pull this part out. You'll notice that it's an integral on a hyperplane. This is what binds it geometrically to one of the incoming variables, V prime, and what makes it so that you can't just pull it out. And what makes it harder to analyze. You'd think maybe we would always have to work in a regularity class that allows for a trace to be taken or to. For a trace to be taken in order to define this thing pointwise. But usually, since you're integrating, that's not really a problem. And the non-singular part, well, due to a cancellation lemma, that actually turns into something very simple. One of the functions just comes out, just right multiplied by, and then the other function gets convolved with, you call this something like a Hardy potential. That one's simpler. That one's simpler, it usually never causes any trouble, but it just tends to make the papers longer. So, Stan pointed out, we have some existence theory. If you start from gamma in the full soft potential case and S in the full range, you can get a local strong solution if you start with initial data that has Data that has a weighted uniform space. No initial regularity assumed for it. And there's also some existence of weak solutions as well. Slightly milder assumptions, but you end up with a weak solution. Here you need a mass core. And I guess I should write down what it means for you to have a mass core. That's going to be shorthand, right? F0 has a mass As a mass core, if there exists x, v. Usually mass core has two parameters, delta and r. There exists an x and a v such that f of x v is greater than or equal to delta times the indicator function of all the radius of the large centered at x of v. How's that? It's not F0 or this. Uh this one, F0. Is this the center of the yeah yeah it's this there exists X and V and that's the center of the ball. The goal for this talk is to show a kind of weak-strong uniqueness for those results. If you have a classical If you have a classical solution and you're looking at some other weak solution in the class generated by the previous theorem, then they actually have to agree for as long as the strong solution exists. And we do have to assume more on the initial data. Specifically, we're going to have to assume that it has mass everywhere, that it has a holder module, that it starts off in this kinetic holder class. Holder class. And that this Q over here is actually going to become quite large. It arises from the estimates, but essentially Q is the trash thing. It accepts everything in terms of interpolation that lets us change norms around a little bit and manipulate the weights. But in the end, somewhere you have to pay for it, and you pay for it by assuming stronger decay properties. Decay properties. And the official theorem in the paper produces a time of existence, sorry, a time of uniqueness. And you know that f is equal to g on that time. But the results should be, we were discussing this, well Stan and I discussed it, and it should be possible to push it all the way to the time of existence of f. Because you reiterate the result. You still have the same polynomial moment that you started with. That you started with. The mass condition, well, that gets propagated forward because of our result on self-generating lower bounds for Boltzmann. That's really the only thing that we need it for. It just guarantees that when you get to this time, the solution still has mass there. It doesn't have mass, but it has a good, useful lower bound. And you're still going to be in this class, because by that point, this function is classic. That point, this function is classical, so you actually have better alpha, better C alpha over there. So, slight variation. F0 has mass everywhere. And again, these parameters are delta and r. It's a condition that has two parameters. If for all x, For all x, uh, there exists a v and has to be so can't be arbitrarily large, they have to all be bounded. And F0 is bounded from below. By mass core is kind of everything. Your x feet. Abundant mass course. So let's open with their discussion on this result. Uniqueness, starting from an irregular initial data, is kind of a hard problem in general. You don't ex sometimes it doesn't hold even. And the typical methods are usually not enough to. Are usually not enough to guarantee it when your initial data is too rough. So we cite, I guess, two sources exemplifying this. Technically three, but one of them is our own for the Landau case. So that kind of doesn't count. Here, they treat Barker-Planck type interactions and kinetic equations using similar techniques to ours. Not like super similar. Not like super similar, but broadly speaking, the same tools: barriers, parabolic smoothing, and the Schouder-type theory. And they can't get uniqueness unless they assume, much like us, that their initial data has some holder continuity. They even point to a book by this name, Daskellopolis and Kennig, The Generic Diffusions, which treats force media type equations and actually shows non-uniqueness. Actually, it shows non-uniqueness when the initial data is rough. This other source gives a treatment of the Berger's equation for supercritical, subcritical, and critical cases. And although they can prove uniqueness when the initial data has some regularity, belongs to HS, so called derivatives, they specifically mention in a remark that in the critical case, you can get a solution from LP initial data, but they can't. From LP initial data, but they can't prove uniqueness with their techniques. Same issues. The Grand Waal estimates just do not close. Standard approach, which is almost the only approach, is that you look at the difference between the two solutions, you look at the equation that is satisfied by that difference, and now you're going to have two collision interactions. This one is generally okay. Remember from before that the first Remember from before that the first input kind of goes inside of the singular piece. The first input goes into the coefficient. The second input is the one that's being differentiated. So from like an integral point of view, this one is okay. Even if you examine it like a maximum of h, this one is still okay. It's this one that causes trouble because it has to put like two s derivatives in v onto f. And then what are you going to do? You have to control that with some. Do. You have to control that with some kind of norm, some kind of high-order or 2s-order norm. And it's got to blow up as t goes to 0. I mean, the exact rates can be computed explicitly. When you do like a Schauder estimate, you do give up some T regularity. This should be in quotation marks. It's like a heuristic Schouder estimate. But you have to give up some. This constant here blows up as t goes to 0, and the rate is non-integrable if you try to take more than 2. If you try to take more than 2s derivatives, even 2s. And even in principle, this DiGiorgi or Shouder theory should not be sufficient to get you there. If you even look at a simple equation like so, fractional Laplacian diffusion, and let's say it has some extra terms over here. If you could control a norm of F that has an order 2. F that has order 2s or higher, and you had a bound on that that blew up as t goes to 0 at an integrable rate. Well, f0, basically f0, if you take, say, cosplus into 2s, 0, that should be equal. This is all the quotation marks, I'm roughly hand-waving, but it's like the integral, sorry, it's the Rule, sorry, it's the it would be f, same number of derivatives, say at a later time, minus the integral from the square root of t of this. Something like so. And if you could control it, you could control it. Something wrong? Let the virus pop up, yeah. Pop-up, yeah. If you could control it, you'd actually develop like a paradox of some sorts. You could know that the function has 2s derivatives at a later time, use the equation to integrate backwards towards the initial data, and then conclude that the initial data is more regular than what you started with, has more derivatives than what you started with. So that's not possible, and it shouldn't be possible even in principle. At least. At least using Shadow type theory or parabolic theory. So you've got to use something else. We integrate the equation for H, which is a generally good idea because for the moment you only know that H is a weak solution. Or it's in the same class as the weak solution G. You can sort of juggle derivatives between H and in that problematic term, you can juggle derivatives between H and F a little bit. You can spread them around, maybe divide them. them around, maybe divide them evenly, and you end up with a term like so. You can do some trilinear estimates on known trilinear estimates on Q, and you get a term like so. But since you don't have any kind of robust lower bound for G, it means you can't use the other term to help you. This term, when you multiply by H and integrate, it's actually, it has a sign. That's why you don't care about it in this L2 framework. But you don't, and sometimes it's useful to help control other terms that. To help control other terms that come from interactions with G and H, but you can't use it to control something that is an interaction between H and F. So it's not going to help you to bound this term. And then you don't have enough conserved quantities. If you use this, you need this. To bound this term over here, you would need a uniform in X control in F in H S V. And you don't have that. Certainly not uniformly in time. Certainly, not uniformly in time. The difference between what you have in terms of conserved quantities that you can start with from the equation and know that you have a priority for your weak solutions and what you need to close this estimate. We do use an L2-based energy estimate initially, and it can't get all the way there, but we pair it with a propagation of a holder modulus, not modulus, just a propagation of a holder norm for the classical solution. For the classical solution. And that's what helps us to control the coefficient term, the highest order term points. And the mass core condition, the requirement that you have mass everywhere, that, by our spreading argument, guarantees that you have a dynamic lower bound that is uniform in t and x. So you have a Gaussian lower bound that does not degenerate as you send t to 0 and doesn't vary in x either. Doesn't vary in x either. So that's needed in order to get the strongest version of the Schauder estimates, which is what we need to close this estimate properly. I guess you might be able to close it in a weaker way, but that I'm not sure what you would conclude from that. There's topological interactions that might get you uniqueness on some compact set, but you don't know how large it is. I don't want to think about that. So, okay, it's about where I expect it to be. Let's dive into the details and see how this is done. We've got a strong solution from theorem one and a weak solution from theorem two. We'll take their difference and we want to work in a weighted space. I'm done being completely hand-wavy. I will start being more rigorous and start waving my hands a little bit more as we go further down. Further down. But at least at the beginning, I want to show how it actually, what's the actual space you're working with. You have to localize in space. You have to. Because we're doing L-tube methods and we're assuming that our function has potentially infinite energy and infinite mass. You've got to localize, otherwise you're looking at infinity on both sides. I guess the first time we used something like this was. I guess the first time we used something like this was in our Herst Landau paper. It's a useful approach, and it doesn't interfere too much with the arguments. You just have to remember when you're doing your estimates to hold off on passing the soup through the estimate until the very end. Usually you need to hold off on that, otherwise you don't quite close correctly. But, you know, it works. It's a fine cutoff function. It makes everything Work out nicely. And then we have the weights. You have to go in weighted spaces because we have the convolution integrals to work with and we can't bound them in unweighted spaces. That's the main reason why we have to put weights in all of our uniform spaces as well. Again, gets rid of, it's the only way to control the singular and non-singular terms. So you get an equation for h, and from there, Get an equation for h, and from there, you get an equation for this norm. This part over here just comes from a transport. Transport ends up being bounded when you, it doesn't cause any problems there. Now you got the standard four, but technically three terms from your quadratic component. So how do you balance? Well, two and three are actually fairly simple. Both the estimate for the... Pose the estimate fairly easy with these. The middle term, I think, is just like five lines, five or six lines in the paper, and that's just because we have to treat in two cases. So it's quite literally a one-line proof that this thing has good bounds. So it's not singular, we just, well, okay, we have to use a lemma about convolutions with the Hardy detection, but it's still really short. Still, really short. The last term is actually the main reason why L2 is really good for weak solutions. Because it's coercive. This thing would be coercive. We're not working quite with that. It's QGH, and then H, and then this stuff over here. It would be coercive if you could take half of this, like I get square root, and slip it onto here. Then it would actually have a good sign. Okay, some lower order terms, but a good sign. And it has symmetric properties. When you integrate it, it has nice symmetric properties. You can get a term that's negative, it has high order, and then lower-order terms. It does not use the Carloman decomposition. This is an aspect of the Boltzmann collision kernel that does not use Carloman. And we have a hard time visualizing how it works in in the Carleman framework. In the Carlogan framework. You really have to use the symmetry of the entire operator to get this to work. It's a change of variables that are difficult to reflect in the decomposition. In either case, you're almost there. You just have to pay for it with a commutator. You move half the weight and the localizer inside of the operator. You pay with a commutator and thankfully it only has derivatives it only puts extra derivatives on these terms, which are fine. Derivatives on these terms, which are fine. Similar to our earlier work on the work we did before on Boltzmann, polynomial decay. It's the same sort of argument, but it was not similar enough that we could just directly reference the result. The can of worms. If this right here, if this had a simple estimate, we'd be done, but it can't. Estimate, it'd be done, but it can't. So, how do you treat the first term? Again, you have two s derivatives on f, so you bite the bullet and do annular decomposition. That's mostly how that singular operator is handled. It's handled through annular decompositions. Say you look at the integral, let's say, between 2 to the k less than or equal to v prime, plus v, technically. Technically. You center it at the singularity. We do your estimates. And then I'll just put a curve k over here. This is how you always treat this problem, some curve over k in the integers. And then you have to consider two or three different cases based on, well, when you're near the singularity, then you. Near the singularity, then you pull, geez, maybe a portrait luncheon F here. Then you have to rely on higher order properties of F in order to control it. And when you're far away, you can get away with just L infinity type, weighted L infinity estimates. Sometimes you also have to worry if V prime is close to zero. Yeah, sometimes that needs to be treated slightly differently. This is the first and last time I care about the coefficients that pop up because formulas like these pop up all the time and I'm not going to keep track of how large these coefficients have to be. But the inner part over here will contribute to C2s plus alpha. You have to, okay, let's let's be efficient. You don't have to get it in the full holder space. You just need it in V. Because this is just a fraction of Laplacian in V. A fractional Laplacian in V. So you do need some moments to compensate for the other terms, but you get with an estimate like this. So do you give up? Do you write your uniqueness theorem in such a way that it assumes the strong solution has a bounded C alpha plus 2s norm, like uniformly bounded in time? No, no. You keep going. You can actually do a little bit better by using. Little bit better by using the Shouder theory over here. You don't have to assume that this is bound in uniformly. You can control it by some C alpha prime polar norm and then get something that blows up as t goes to zero, but is integral. So this is a formulation of that shower theory in the best case. Basically, if you're a classical solution with initial data that has mass everywhere. Initial data that has mass everywhere, then you can explicitly trade some time, some time blow-up, as well as, I'm not going to write down how Q2 depends on Q1, trade some moments, but you go down in the holder order. Now you have a milder requirement, because that extra term is now just a lower holder exponent. Give up now. Do you assume that? Give up now? Do you assume that, do you hypothesize or do you put in a hypothesis of the theorem that F has to have a bounded C alpha prime norm for some alpha prime small? No, you try to propagate that from the initial data, because that way you can just put all the assumption on the initial data. And this is the main workhorse that does it. Overall, size of C alpha norm by a slightly larger C alpha norm. The alpha norm on the initial data. And again, you have to pay for it by losing moments. Technically, this is where the TU shows up, because we're going to get an equation for something that might blow up in a time that's less than capital T. But we don't, when you think about it, you don't really need it, because the existence theorem already says that this type of norm is bounded away from any neighborhood but t equals zero. And this gives Of t equals 0. And this gives you a bound when t in a neighborhood of t equals 0. So that kind of finishes the job. It would mean that the can of worms is now controlled by something different, by an assumption purely on the initial data. So, okay, we've brushed the hard work. We've swept the hard work under the rug, the rug being this lemma. Now let's air out the rug. It's quite challenging and technical. Quite challenging and technical in the sense that our paper has seven sections and an appendix, and depending on how you count it, proving this is two and a half sections. And that's not counting the last section that actually runs through the proof of uniqueness that I outlined, that I just outlined here. So there's a lot of work that goes into this. And if you ask us if it's optimal, we haven't agreed on an answer, but I'm going to say we. But I'm going to say we don't know. There might be some way to streamline the approach where you don't even need to use this next idea that I'm going to be talking about, because it doesn't quite jive with the kinetic setting. It's an idea for propagating a holder modulus that we've used in the past, but came from, well, Constantine and Wickle when they applied it to SQG. And then later when they brought me on board as a grad student. Me aboard as a grad student. It might not be the best way to do this, but it was the way that we saw how to do it. So, what you do is define the following function. It is technically belongs to R13. It's like a 13-variable function. Because you're going to double up the x and v variables or the perturbations. See, x, v are going to. Xv are going to come coupled with perturbations chi and nu. And you're going to take the difference in the function across, you know, it's a finite difference in x and v, and you're going to divide by this Polder type denominator. So you can tell, you know, if you have an upper bound on the magnitude of capital G, if you ignore this weight for a moment, then an upper bound on the magnitude of capital G tells you that F has to belong. That F has to belong to holder class alpha double prime in X and also in V. And the reason why this doesn't seem to work well with kinetic theory, or the kinetic holder space, is because, one, there's no time perturbation. And two, these have the same size. This one should have alpha double prime over one plus two s. It should be less, it should be lower. So is that not inefficient? The method doesn't like anything other than this approach. Like anything other than this approach. Although, maybe we didn't look too carefully into what happens if you bake a time perturbation into this as well. You'd have to go to zero as t goes to zero, this is the acceptable range, but we haven't looked into that too much. But you look at the equation that it satisfies. The trick for it, and the reason why Konstantin Vickel and for SQG and other equations, why this works so well, is because Why this works so well is because you actually get an operator, even if you have drift, when you examine the equation form for g, there's a clever way you can rewrite it that shows that you have an operator that satisfies a maximum principle. A priori, you would guess that this equation should involve some extra terms that involve take derivatives of f, like a drift applied to f directly. But no, you can secretly turn that gradient x. Gradient x of this term, that's what you would be left over. The gradient of x of this term is actually the gradient of chi, the gradient in chi of the full difference, because this doesn't depend on chi. So that means you can stick this on the inside as well and just get another drift for g. You pay for it with a term like so. And you want this term, you desperately want this term to be bounded. It's not bounded unless It's not bounded unless these two exponents are the same. That's why that forces us, that forces our hand. We can't work with a kinetic norm. We have to work with this XV. And then on the right, you end up with four quadratic Q terms. The four terms come. You look at this equation. You look at this equation. Okay, there's some heuristics as to how to justify this next step. Like, I'm going to say that you take this equation and you evaluate it at a global maximum. In reality, it's a barrier argument with a barrier that is only time-varying, and then some somewhat topological argument that says because of the smoothness of f and its decay and velocity, you can rely on the fact that it will have a global maximum. So we evaluate on a global maximum. We evaluate on a global maximum, all the gradient terms go away, and now we're left with these four quadratic terms, which we do break up in the Carloman decomposition. The delta means difference across f, the tau just means translation. So tau f is basically f. Okay, the second term is nearly the term that you expect to be good, right? The coercive term. Expect to be good, right? The coercive term, the one that looks like lambda to 2s, the fraction of Laplacian of the order 2s, applied to g. When you evaluate that at a global maximum, it actually has a good sign. And yeah, we can get a good bound for that. In fact, let me write it out because, as you can imagine, there's weights floating around in here. There's weights floating around. So, how does that? So, how does that work with weights? One of the tricks/slash techniques that, little techniques that I think is worth emphasizing. You're going to have on the outside some kind of weight, and you've got an integral. You've got the g function, but it doesn't have the weight in it. So I'm going to write this as g over g of v prime over psi of v prime. of v prime over psi of v prime. And once you push this denominator inside of the QS, you're left with this, because you're missing the weight. The weight is still on the outside. The weight in V is still on the outside. This would be like delta F over the denominator, and this is like that V to the M double prime. You want to push it inside, you have to do things in the carefully. Okay, so I'm looking at this. So you're looking at this and a kernel. Different, I guess. In this case, it's actually proper K. And V happens to be a global maximum for G. What you do, this is actually less than or equal to G V by itself. No, times an extra weight. Time, an extra copy of the weight. add an extra copy of the weight, and then 1 over psi v prime minus 1 over psi v prime k prime. So see what happened? This is smaller than g of v, so that if you replace this by g of v, then you get an inequality. You're out at the maximum. You have this inequality. And now the 2s order derivative is no longer happening to g, it's happening to It's happening to the weight function, the one over the weight function. That's formally how the trick plays out. Why this term is good, why it has a good sum. When you evaluate at a maximum, you can do this, and in your angle decomposition, you only need bounds on the, not on the entire orders that you need. That was fine. The last two are bounded. Are bounded in a straightforward manner, same sorts of estimates, and they give the same exact type of bound, up to a constant, bounded like so. The bottom half, I guess, of the can of worms is this term over here. Because you have the same problem, right? You have to put 2s derivatives on f once more. And it's kind of like gf, qsgf. So you pull the same tricks, you end up with 2s plus alpha double. You end up with 2s plus alpha double prime and another v to a different power. Seems like we're going around in circles, but I promise you we're not. We're going around in a spiral. We're circling the grain. So we'll use Chopper estimates one more time here, as well as some interpolation to get it down to the same level. The reason why we did this is because this is a C alpha double prime holder normal C alpha double prime holder null. Capital G in L infinity measures a C alpha double prime holder norm of F. So we would like to say that this is actually just an extra copy of G. That's why it looks like it's going to close, because you're just going to get, this thing is going to be replaced by a time integrable thing, a conserved thing, and then an extra positive power of g and L infinity, which, you know, you don't get global existence. We'd be very famous if we could do that. But no, you end up with local. Could do that. But no, you end up with local existence, and you get a bound on a neighborhood of zero. But here is where I said it's the bottom half of the can of worms. The problem is this is a full holder. It comes from the Schauer theory. So this is the full holder, sorry, full kinetic holder space. It has a time component over here. It has an x component measured by alpha prime, double prime over 1 plus 2s, and a v component. prime over 1 plus 2s and the v component. This only has v and x measured in holder alpha double prime. It's missing a time piece. So that means we're not done yet. We have to figure out a way to control this quantity by g and God forbid if we have to use the shader estimates again it should not generate a time decaying component, a time blow-up component that would fight this term. Thankfully it doesn't, but this is still like one full section of the paper. Of the paper is dedicated to the time regularity proposition. Turns out, you don't even need to lose your order. You don't even need to lose any order at all in terms of the holder size. You go from kinetic holder to kinetic, but only in X and V. Obviously, this isn't true for generic functions. It has to be for classical solutions to the Volts-Well equation. You give up again moments. Up again, moments. And we can finally, that lets us close the previous estimate. So, modulo proving this other lemma, the holder propagation lemma is actually proven. And with it, so too is the uniqueness theorem. I put this star here to remind myself that this constant now technically depends on norm f in L infinity q, but that's fine. See how that would work. See how that would work. So then, how because in particular, there's no T component anymore. There's no blow-up part with T over here. Which is to be expected because you don't gain, like this part of this alpha is the same. You don't estimate it in a lower order space. You don't have to. How do you prove the time regularity? Well, you look at another finite difference. Now you might be asking, seriously, though, are we going around in circles? Because this is the same. Because this is the same problem. If I'm going to do a barrier argument on this, how is that different from the situation that we've been looking at before? I'm still going to get that same, shouldn't I still get that same collision term that tries to put another two s derivatives on f? Am I not just going around in circles? And the answer is no, because this is a local argument. This term here is a constant. The function that we're looking at is going to be defined. The function that we're looking at is going to be defined on potentially small, like order R, balls or ball crossballs, squares, whatever you want to call them, small compact sets. And this is the only, this is the kinetic dilation. It's going to vary in a kinetic rescaled manner. This part is just the value at the center, and we're the parabolic center, and we're just subtracting it off. Tracking it off. So because of that, we don't generate that, don't generate that term like Q delta F delta F F. We only have two terms that are basically like, I'm really going to go all the way back, sure a little bit, we only generate this type of term and this type of term when you do that other that this last argument to prove that level six. Okay, so this one was fine. Okay, so this one was fine, and in this case it'll be fine for the same reason. You're evaluating at a maximum, and so you use the same trick over there. It's nice. The turn you actually get for the singular collision operator is nice. But even though, even with that, this still takes a tremendous amount of work and case-by-case analysis and careful choices, even of the localizer. Even the localizer had to be different. We tried to use the same localizer as before, that phi A from before, of some variant close. From before, some variant close to it. And it didn't work. The estimate just didn't close. And then we picked a different localizer that had, I remember that clearly, right? We were in like a, not really a panic, but it was just really frustrated over zoom. We couldn't get this thing to work with the original phi, so we started tweaking and perturbing the argument. And apparently the perturbation that worked was to change the localizer. The super solution itself, the barrier that you Itself, the barrier that you use, that too, you have like three different components to annihilate things that happened on the barrier to work well with the localizer. What I'm saying is that if a more refined approach similar to this could have replaced the holder propagation argument, it would not have been obvious to see. I don't know if it's still possible to do it that way. And. And with that, I think I'm done. Pardon me? There's no way you could work without the weights right now. Without the V weights? Yeah. Um no, there's a lemma that there's a pretty crucial lemma or family of lemmas for uh controlling the um Controlling the or controlling convolutions with like in some sense the problem is that the you always see something like this to us. No matter what you do, you always end up with something like this. And if f is just an L infinity, then this could be infinite or potentially arbitrarily large. Arbitrarily large. So you need some weights to control this convolution. You need to be able to pull F and L infinity out and leave behind something. 1 over V plus W 2m with F L infinity M. Now this turns into another V weight when you evaluate the integral, but it's finite and you have a bound on it. Like just the fact that this and things like it appear. Just the fact that this and things like it appear all the time means that it's it's kind of not possible to get rid of the weight. Maybe if you were in relativistic Baltimore and velocity is bounded above. Yeah, I have weights and momentum, yeah. But then I don't know what Carleman looks like in a relativistic setting, or if that's even possible. His eyes opened up wide. Yeah, okay. Yeah, okay. Momentum weights from a lot of places. So either the velocity is bounded, depending on what you're trying to do, you can lose weights. So it seems that you don't use the entropy or localizer. That you don't use the entropy or localized versions of the entropy for the whole program at all? Is it just not useful or not convenient for the technique? There's not enough, I think. But then there's but then we we had no way of using the patient. This equation is L2 and D and X, and that's not enough to call it to the weight, I think it was called 5 and there was the power minus 2 in X and V. It was uniform in X because you took the shift with respect to some A and then you took the super ball there. Some A and then you took the soup of all the A's. Yes, were you too far? Too far? No, no, not far enough. It's literally like the first slide. Okay, actually, I remember. Here we go, exactly. So I understand that you want something uniform in X, and that's why you have this soup in A. And for V, it's different because. And for V, it's different because zero is special. Somehow I understand that. But I was surprised, and this might be like a stupid mistake. Why is it, I mean, R3 for X and in V, right? But why is it minus 2, which I understand for X, but why is it also minus 2 for V? It's bounded well if you when you hit it with the transport term. And And where can I see that? I would. I think the power has no meaning. Maybe 27. What? Yeah. No, okay, then I'm totally lost. I thought this is like because we are in R3 and you choose something which is less than the dimension and it just works. So okay, then I need an explanation. What's the role of the power? Something that's bounded by itself after you hit it with all the derivatives you need with. So really. You did it with so really it's not it's it's just a it's a hack, really it's not anything prevent something from happening like if you're in you can have a situation where you you kind of have some something nicely bounded and you have mass along some kind of that skew strip and then as you go forward in time the transport kind of lines all of that could could if you're unlucky line all that up and cause some issues. That up and cause some issues. It gives you some weight that says, okay, I can kind of ignore this sort of weird. But you just need something that decays, and I think that for the transport reason, you just need the powers to be the same. I could be wrong, but my memory is. Okay. Absolutely. zero doesn't play any role because it's one class so we speak about infinity and then uh I think we took this from remote to Yahoo. Yeah, the point is when you have a take a gradient, you'll have something linear. Esther? Thank you. Great. One quick question. These are some of the annoying. Well, you did all this hard work and it's provided with It's surviving this. Oh, Trading is a harder question. But the question is: would it be, could you adapt this to handle if you added like a Poisson term? Like a Vlaso-Poisson. Oh, you mean like a force term? Yeah. I feel like I don't see how that turn would be a terror cannot be worse than all of this, right? Yeah, um, that is something that I was. Um that is something that I was personally I was under the impression that um because the regularity theory for last time equations was uh better established then uh adding that term would mean repeating an enormous body of literature but wouldn't add any new anything new mathematically. Oh, really? Okay, okay, interesting, cool. Okay, and then one comment: when you mentioned that lemma five, when you propagate the holder, I understand. I understand that that technique is very well known in how the global equations to propagate not just Holder but Lipschitz bounds. Is the Ishilions? I was about to make sure. Yeah, so everything you have a comparison principle, like since the 80s, I guess the way people will build solutions that are lip shifts would be, okay, the gap U, you shift it, right? And then initially, if it's lip shifts initially or older, initially Older initially is not less than a power of the constant, and then you can propagate it. You have like the rise. Yeah, you could go and try to prove that the given supremum is non-negative. I don't know. Have there been versions of this that incorporate time, like a time perturbation as well? I mean, there are so many papers, I would be surprised that this at some point, but I mean, you need to make it work for your setting, so it's not difficult. I'm not completely sure it's going to work, but I don't know. It's natural to want to try, but just to mention that that's connected to the thing that people use. I didn't know that, so yeah. Yes. Thank you. So you are trading some weight here in LAML5. Is this because you are dealing with a software? Oh, a weight. A weight? weight a weight if you start with the weight q zero and then you went through a mega time it's more of the saying really that the q terms if you omit the weight every uh the equation becomes much simpler to look at it's just that at the end you end up with something that you can't bound without without the weight decay um without the without the weight decay yeah you end up with the same sort of thing all over again. Same sort of thing all over again. But there's some coefficient in there that you cannot control by just pulling out the norm of g, right? Because there were some instances where you would see g develop inside of the coefficients. Here, this will turn into a g, more or less. And so this will turn into a g as well. And it's inside the coefficient, that means that this is literally going to be g placed over here. Integral of g against this party. Of G against this party potential, or convolution with this Hardy potential, and then you're not going to know, you're not going to be able to, that might be finite, but you're not going to be able to bound it in terms of G, in terms of the L infinity norm of G. If G did not have the weight baked inside of it. Now that it does have the weight baked inside of it, you pay a little bit with more computations, but it's something that you can actually estimate. If you integrate delta F over this thing against W gamma plus just gamma essentially because that's what's going to happen inside the non-singular term. You're going to have this value v plus w and it's going to have a tau f at v. If you had just this You can't pull this out in L infinity, because then you're not going to get an integral that converges. You have to pull this out in a weighted space. You have to pull this in L infinity with some weight, in this case m double prime. And doing that will give you an L infinity norm for G.