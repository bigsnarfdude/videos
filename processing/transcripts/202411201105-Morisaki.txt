I'm Soji no Moisaki. I'm working as an assistant professor at the Institute for Cosmic Ray Research at the University of Tokyo. Today I would like to share some of the results from our recent collaborative work with the students. So, with students in our group, Masaki Iwaya and Kaze Kobayashi. So, those students leading this collaborative work. I would like to share some of the results from this recent study. The goal of this work is to accurately recover the spin distribution of binary records from gravitational data. And we are to, for that purpose, we are investigating any potential systematic biases in the analysis, such as the The analysis, such as the inappropriate choice of the spin model distribution and also the numerical errors arising in the likelihood variations. So originally I was planning to go through all of the ongoing projects, but I decided to make this talk more specific and I'm gonna focus on one specific project. Focus on one specific project which drafts we are preparing, and we are actually the manuscript is being circulated in a collaboration review. And I'm going to talk about our recent work where we derived the analytical spin distribution and investigate its impact on the art rate inference on the spin distribution. So, okay, as you know, that. Okay, as you know that we have already detected many, hundreds of gravitational wave events from the compact binary cores. So by the end of 03, we have detected in total 90 compact binary mergers. And in the ongoing run, we have already reported around 150 additional compact binary mergers. And one of the interesting astrophysical question is Astrophysical question is when and where those emerging compact binaries formed in our universe. And to understand this problem, one of the ways to understand this problem is to estimate the source parameters from those gravitational wave data and study their statistical properties. Especially the spins of criding objects are probably very Objects provide us a very valuable information to understand the whole mission scenario. Because, for example, in the scenario of isolated field binaries, where the stars first form binary, after the aircraft interact holds, then due to the interaction between stars in the early stage, the spins tend to align with the ambient orbital angle of momentum. But on the other hand, in the Momentum. But on the other hand, in the dynamical formation scenario, where the black holes get close to each other and form binary in a dense environment, then the spin directions tend to get randomized. So by checking the spin directions from gravitational wave data, we can check which channel is preferred for data. Unfortunately, it is difficult to measure. Unfortunately, it is difficult to measure all of the spin components from gravitational wave data, but it is known that there are two spin combinations which can be relatively easy to measure. One combination is so-called chi-effective, that is a mass-valid sum of the spin components in parallel with the orbiter and their momentum. And another component is spin component is Kype P, that basically measures the That basically measures the spin components orthogonal to the orbital and their momentum. So, LIGO-Babo-Kapola Corroboration studied their distribution from gravitational wave data. The results are shown in the light figures. So, in this analysis, the Liebal-Kochla correlation assumed that the distribution of chi-effectiveness can be modeled by the two-dimensional Gaussian distribution and fit them. Fit them, fit data by this distribution to obtain their mean and covariances. You can see that, for example, the chi-effective distribution is concentrated around the chi-effective equal to zero with a small width around 0.1. The chi B is basically broad, but it's big. It's peak is likely around 0.2. So basically, the spins are likely to be small for many of the mergers. Okay, so I'm going to also talk about how we recover those distributions. So the likelihood in the population analysis is given like this. So here this I represents the I indexes each. I indexes each event, and this N data represents a number of elected events. And we basically, likelihood is basically given by the product of these quantities from each event, sorry. And that large lambda represents the hyperparameters defining the population model. For example, this lambda can include the mean and variance of the Gaussian chi-factor distribution. Distribution. And the numerator Zi is a quantity called a single event evidence. And this is given by the integral of the product of the likelihood and operational model distribution over the and its integral over the source parameters such as the masses and spins of the colliding objects. And this gai represents a detection efficiency. Basically, that's an integral of the Basically, that's an integral of the detection probability. That is the probability of detecting binaries with this parameter. Assuming that the true source parameter is theta, then this detector probability is basically the probability of such pipelines detecting that signal. And basically, this detection efficiency is the integral of the Is the integral of the product of this detection probability and the model distribution over the source parameter values. This detection efficiency is necessary to correct the selection, to incorporate the selection effects. So for example, for example, the higher masses are easier to detect. So just studying the distribution or observe the source parameter is not enough to recover the. Parameter is not enough to recover the actual distribution of the inverse. Okay, but of course, this integral, in practice, it is impossible to evaluate this integral numerically because it involves a lot of source parameters. So, what we do is to currently approximate this integral as a multi-travel sums like this. So, for example, for the single evidence single evidence, Evidence single event evidence, we utilize the posterior samples obtained from the parameter estimation of individual events and calculate approximately a multi-carbon sum over the posterior samples. But we need to be careful that the posterior samples, when we do a parameter estimation of individual events, they assume as prior distributions. Assumed prior distribution, which is denoted by Ï€PE, is generally different from the assumed model distribution. So, we need to reweight each sample by this ratio. And for detection efficiency, we prepare a rural injection and run such pipelines for it and calculate the Monte Carlo sum over the detected simulated signals. But again, we need to reweight each sample by Each sample by the way, it's proportional, inversely proportional to the distribution, model distribution to populate injections. And what we are investigating in our study is that basically we investigated medical errors involved in these distributions in the denominators. So in the So, specifically in the analysis of chi-vectum and the chi-p distribution, for calculating those distributions are proportional to the joint spin distribution, that is a two-dimensional distribution of chi-feptum chi p conditioned on the tube, and the assumed spin distribution. And typically, we assume that those distributions are isotropic visits over the visit. Is this vector spins and uniform over the spin lengths? So we needed to calculate this joint distribution under those spin assumptions. And because chi-vectority of chi-p is not like, you know, not very simple combination of those spin parameters, so actually calculating this distribution is not straightforward. So in GWTC 3 population analysis, we decompose this distribution into the chi. Distribution into the chi marginal chi distribution and the conditional distribution on chi B. And actually, we can calculate this distribution analytically, but we didn't have an analytical form of this distribution. So, what we did is we made a lot of samples of chi-p and applied a kinder density estimates to estimate, approximate this distribution. However, we have found that this kinder density estimation Color density estimation does not accurately calculate this conditional distribution. So, this figure shows the conditional distribution calculated from KDE in comparison with the true distribution shown as a green line here. And as you can see, that this by the way, that this HD distribution has an error bar because we utilize the random samples. And you can see that See that this distribution does not accurately capture the, for example, the sharp gradients near the chi p equals 0 and chi p equals 1. And also, this KDE does not accurately capture the cusp arising at the maximum chi p value achieved by the only by the secondary stem. So if you use if you use this could generate Could generate, cause biases in the recovery of chi-effective type B distribution. We also want to point out that if you use random samples, then we always have a statistical error coming from this random sample. So it can impact the user. Impact of the development usability of the analysis. So, what we do is, rather than optimize this theory scheme, we derived the analytical expression of the disjoint distribution, which was not obtained in the previous study. So, we needed to basically analytically calculate this integral over the spin nodes and the cosine field angles. And our students. and just our student Masaki derived that this found that this can be actually analytically calculated. This joint distribution can be represented by some of the four integrals, four quantities, one of which is non-zero, depending on the parameter space we consider. But this is a growth But this is a growth bit growth. But anyway, using this distribution, we have found that so this is a this green line shows this analytical prior analytical spin-joint distribution in comparison with the Landam samples. And you can see that this analytical distribution accurately captures the structure of the distribution, and also by construction, it doesn't utilize any. Construction, it doesn't utilize any random samples, so it doesn't incorporate any, it does not have any statistical errors. And also, this analytical distribution varies. The representation is very close, but it's much faster to evaluate than the KB prior distribution. So we analyzed the O1, 2, 03 BBHs. Actually, these are changed very recently, but actually. Changed very recently, but actually, so this is the full corner plot of the hyperparameters. Sorry, they are very small, but you can still see that actually the results do not significantly change by changing the joint spin prior to its analytical hold. If you focus on the spin hyperparameters, then the results look like this. We see a small tendency. We see a small tendency that this analytical prior and analytical distribution gives you a slightly smaller value of the standard deviations. And this can be also checked by relating the KDE results with our analytical variants. And yeah, so this is a comparison of This is a comparison of the posterior predicted distribution for chi factor on chi p. And we can also see that our new analytical distribution does not significantly change the results. However, this doesn't mean that this somewhat subtle numerical errors do not do not, that does not mean that this is not very important because This depends on the case, for example, the number of events. So, actually, if you calculate the log likelihood errors, so this is errors caused by the KDE. So, basically, we just compare the local acclaimed values calculated with KDE, the original KDE distribution and our analytical distribution. On the hyperparameter samples obtained from A hyperparameter sample is obtained from that KDE analysis and found that actually that this logic areas caused by KDE are already in the order of unity. So if it's vanishing, then that means that the livelihood ratio is unity. But if it's in your order one, then that means that they could be changed by a factor of. They could be changed by a factor of few, or could be, could glitch order of them. And this local agli-to-error, because this is proportional to the NODAT, so detection efficiency part is proportional to the number of detected signals and also that it contains a sum of the single event evidence. So this logarithm could, this somewhat to subtle numerical errors, can further grow as. Errors can further grow as the MZ increases. So, for example, these errors could be significantly biased results when we analyze, do the same analysis for all four events. We also found that the errors in prior gets very pronounced at the boundaries of CIP near the zero unity. So, if you, for example, consider the sub-population. Example, consider sub-population models that concentrated on those boundaries, then this subtle main guidance could significantly bias your population analysis. So that also highlights a difficulty of population inference that the that any small errors can significantly pronounce, yet get significant significantly pronounced if the number of detections get uh increases. Okay, so that's basically it. But we also investigated several aspects of this spin recovery analysis. And if you are interested in, please talk to me. So that's it. Thank you. Questions? Go ahead. The errors that you were pointing out in the previous slide. That you were pointing out in the previous slide depend on the simulations that we, or the inductions that we make for calculating the selection effects, right? Because those are still numerically rows because we can assume whatever distribution we have of the component spins there. So does that still exist, right? That still exists, right? Or is this a different is this an extra state? So we didn't check the convergence of these regular activity data for the different sets of injections, to be honest. But well, I think that the statistical errors coming from the finite number of injections is different issues, right? Because, you know, like the the analytical prior is always Prior is always gives you a correct value of the spin distribution. Yes, of course, definitely. But will the errors from this bigger than the errors that we get from having finite number of equations? You mean this part? Sorry, single event, but searching. No, are the KDE years? They have a bigger impact than the errors. Errors from having a finite number of injections. Beat in the selection function? Yeah, bits present. I need to suggest, but I think the number of the injections is I think the ET also depends on the hyperparameters. For example, if you consider very like distribution very concentrated at the boundaries, then the you need a more and more injection at at that point. More and more injection at that point. And but so, yeah, it depends, but yeah, so that finite setting physical areas also could be dominated compared to the systematic areas, but I think it depends on really depends on the hyperparameters. I was wondering what kind of I was wondering what kind of like a wall button number, what kind of increase do you expect in the log likelihood errors, let's say, in a time-site telescope error or context or if you just kept using the stuff maybe for it just basically so if only if you're only focusing on this for example selection but it does just simply increase growth. The errors just uh simply incre growth as a uh in proportion to the number of detections. So if you instant case growth detector is the thousand or ten thousand events, then the errors is larger in proportion to that number. All right, can we thank Switchero one more time? 