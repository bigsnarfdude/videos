Thanks. Yeah, sorry about that. And thanks for having me. I'm very excited to talk about our project about principle of incompleteness corrections to estimate galaxy luminosity distributions in X-rays via incomplete X-ray and optical survey data, which started as part of my PhD project with David van Dijk, who's here, and David Stanning Anetta is here as well. I think Vinay is online. The luminosity Um the luminosity distribution is specified specifies the distribution of source intensities in a population. And here we particularly look into galaxy luminosity distributions in X-rays. And astronomers are interested in the luminosity distributions to get some information about the structure and geometry of astronomical sources. Astronomical sources and the universe in general. And they're particularly interested in the log n-log s relationship. So, what's log n-log s? We have the log of source intensities on the x-axis and on the y-axis the log of number of sources that are greater than the source intensity times the population size, which is basically one minus the CDF of your um population distribution times the size. Times the size. To illustrate the problem, let's start by being very naive. And we've seen some X-ray catalogs already. And being naive, we say we have sources detected in an X-ray catalog. We look at source regions and we basically just subtract some background counts that we expect to be in the source region and write our look and look S by subtraction giving the source intensities that we estimate this way. Now this Now, this obviously leads to a lot of problems. First of all, we shouldn't just subtract the background discretely to negative surface intensities, but secondly, that would assume we have a complete x-ray sample. And to show that this is not the case, we can look into various different other surveys, in this case an optical survey, get detected sources in the optical catalog. Optical catalog, extract the locations, go back to the X-ray survey, and estimate the source intensities. So, what we see here is that given the sources detected in optical catalog, we go far further in the low intensity end of the distributions, which directly illustrates the extra catalog is not complete, but also we have to assume that the optical catalog is not complete. So, the challenge: we want to model this population. Want to model the population of galaxies, but we have a strong incompleteness in the X-ray catalog. We further have to assume that that incompleteness is in the most relevant part of the distribution, so the highest density part of the population distribution. And our aim is, and the question is, can we leverage optical data to get more data in that low intensity end of the distribution by assuming that both the X-ray catalog but also the optical catalog are in the distribution. Catalog, but also the optical catalog are interpreted. So, in this case, we have to, it's just a sketch of these different detection cases that we have. We have a lot of sources that are detected in optical catalogue but not in X-ray. We have some sources, here in 2, that are both detected in X-ray and optical. A small proportion of only X-ray sources, and we have to assume a large proportion of sources that are just not detected in either of the catalogs. And the strategy. And the strategy that we have is to incorporate incompleteness functions for each of the survey in our model. That is, given the source intensity, we have detection probability of being included in the survey, which for the X-ray catalogue we assume to be known from previous simulations. And for the optical catalog, we add an additional incompleteness function that is combined optical plus X-ray with parameters that we learned from the model. Parameters that we learned from the model. Well, unfortunately, this is not all complications. So, now this is an actual patch of the Chantra Deep Field Survey with X-ray data. We can see, so those all blue circles are optical surfaces, the yellow circles are X-ray circle X-ray detected surfaces, and we have a lot of background contamination, all those white dots in between not in circles. Not in circles. Plus, if we go farther away from the center of the field, we have some off-axis corrections that we have to make and some exposure corrections. Furthermore, we say a lot of overlapping sources, and we somehow have to match the optical and X-ray sources to combine both surfaces. And let's first target the modeling of source intensity. Modeling of source intensities of observed sources. Well, instead of just subtracting, we can model the observed source counts at the location as a sum of latent background and source counts. As we have source counts with source intensities, there is Moisson distributed with rate, the source intensity times the effective area and the exposure time. And we have a background model being possible. And we have a background model being Poisson with Poisson rate times the source area and the time. And we have more data, which is just the entire background, subtracting all source locations, which gives us the number of source counts in background X. Alright, given the model for our sources, we now want to have a population. To have a population model, and the astronomers assume to have a linear or log linear or piecewise linear relationship of the log n log s. And we obtained this piecewise or linear relationship by modeling the population distribution as a broken power law or double Pareto distribution, which is basically a truncated Pareto, a mixture of truncated Pareto and a Pareto distribution, which gives us. Which gives us four parameters that we're interested in: the population minimum, one breakpoint, and the two power loss slopes. And I apologize for a very technical slide, but I guess this is the key slide for our new methodology. So how can we account for incompleteness? We have those three different cases. Case 0 is we just don't observe that source. We can't observe that case either. We have case 1 and case 0. We have case one and case two, which case one means we observe a source in the optical catalogue with some probability, which is characterized by V. And we observe sources in both X-ray and optical, where we assume the inconvenience function to be known. Now, for the two observed cases, we can model the joint distribution of C and lambda, the source intensities, in the two observed cases in the optical. In the optical by the double parliament distribution, our population distribution times the x-ray case times the income difference function of being detected in x-ray and similarly or analogously for the optical. And the idea is now to model that incompleteness by looking into small bins given on lambda. This is the last step that we have. So instead, we look into a small bins. Instead, we look into small bins on lambda, and given on those bins, we can count if sources we can count the sources being observed in optical or X-ray. That is, the number of sources in a pin K on lambda with being observed in optical is for sale distributed with rate parameter being the population distribution density times the spin. times the spin width times the incompleteness function of optical and see the expected population size and analogously for the x-ray detected which can be formulated as non-homogeneous for some processes again here we have the inconvenience function being on lambda and we want to do the binning on lambda again unfortunately it gets a bit more complicated because It gets a bit more complicated because the incompleteness function that we have of x-ray data is not just dependent on lambda, but dependent on further nuisance parameters, such as the off-axis angle, effective area, and background rate. But to do depending on lambda, we want to have the incompleteness function dependent on lambda. And I don't want to go into much detail here, but basically what we do is, or what we can do is we marginalize over all those nuisance parameters. And by doing so, this is what And by doing so, this is what we had on the left-hand side of the previous slide. And we have the population distribution times the integral instead of the g of lambda that we had before, which we can compute in advance and just evaluate in our sampling step. And again, not much detail, but the way I think about the Gibbs sample, the MH within Gibbs sample that we have is we have our source model to get source intensities of observed sources and we We get the inconvenience corrections to obtain the population distribution. Alright, going a bit quicker now through the examples, we made some simulation study to mimic X-ray previous studies with an expected population size of around 1000 with 124 observed sources to show there's a large proportion of incompleteness. Those are the marginal distributions of the parameters that we're interested in. That we're interested in, which is the population size, and the truth is given by the red lines, which shows, especially for population size and the population parameters, we can recover the truth quite well. And those are the parameters that we're interested in. I will skip this part, but yeah, now we applied it to X-ray, Chandra X-ray data, and again, we have 1,500. Again, we have 1524 sources being observed in optical data and around 70 sources just being observed in X-ray, which shows we can largely increase the data size by looking into optical sources. And we obtract get the source counts based on optical locations. And this is the final log n-log S that we observe, that we obtain, so this is the mean posterior of all parameters. With double Pareto being in orange, we have the Pareto being in orange, we have to also fit a single Pareto model in blue and the Snahe estimate. One thing that we observed in the C Chandra deep field solve is that fitting a single or double Pareto model gave us almost the same results, which can be explained because in the deep field there's not many very bright sources, in which case the breakpoint here is not necessarily needed. But by accounting for the incomplete. By accounting for the incompleteness, we have an expected population size of around 110,000 in that field, which shows there's a large incompleteness proportion that we have to assume. Of course, we have some challenges. One of the main challenges is how to adopt. Some of the main challenges are how can we actually model the overlap between services, which would again largely increase the data size that we can use. Can use. Can we improve the survey matching between optical and X-ray sources? Uncertainty on source locations, and potentially also including a zero-inflated population model to get that breakpoint, the first population minimum correct. And yeah, in summary, it's a new methodology to combine two incomplete surveys to estimate the population distribution given that double parade to the population distribution. Population distribution in a hierarchical basis framework. Sorry if I just missed this, but does this make an assumption that the ratio of x-ray to optical luminosity is fixed? I'd actually let me follow up with that because I think it does. Because there are some sources that are going to be x-rays. Because there are some sources that are going to be x-ray sources that will have no opticals counterparts. There will be some optical sources that will have no X-ray counterparts. And you seem to be using the optical to address the X-ray, but there's going to be this wide range of where the optical sources are going to fit on the X-ray velocity function, including another zero-ecleta model where they might have absolutely no X-ray intensity associated with them. And so I don't see. Though. And so I don't see, personally, how you're not conflating those things in a way that is very surprising and perhaps incorrect. Moreover, I'll just, if you look at just the actual data, sorry, the actual data where you show your parameters versus the naive data, there seems to be an exceptional mismatch well away from the x-ray completeness limit. Completeness level. So I'm a little bit. If you go to the next one, keep it going. Keep on going there. So the x-ray completeness is probably around, let's say, minus 6.3, minus 6.4. But you've got what seems to be this huge mismatch between the double Pareto, single Predo, and that naive estimate. So there's almost no mismatch between single and double Pareto. This is only two circumstances. Between the double slash single Pareto and the naive. Massive print and the naive estimate. I mean, for the naive estimate, those search intensities are far away from being correct. Plus, of course, there is a high incompleteness, which is not accounted for in the naive estimate. And the way we so I'm not sure maybe we have those different cases where two means it's observed in matched, meaning observed in optical and x-ray. And then another case, being observed in optical. The important thing is that there's no relationship whatsoever between the luminosity of the optical and the X-ray. It's you just using the optical where to look for the X-ray. The intensity comes from the X-ray counts and the X-ray counts as well. So we look at the optical catalog and then get the counts in the X-ray. We should talk. I'm very confused, but I don't want to hold us up. Are there other questions? I'm just wondering how do you choose the pink size? Yeah, well, that's a computation, more computational question. As fine as possible. In this case, I choose bin size to have 0, 1 binning. Binning given some initial initialization. So no bin had more than one alteration, yes. Okay, so the different bin size will affect the utilities? Basically the approximation that we have. So if you increase the bin size to have 0, 1 bins and actually look at the lambda k at the observed lambdas, or well, the latent lambdas, then this is the exact solution. And this is the exact solution, but otherwise you just approximate it quite a bit. Are there any more questions? If not, let's thank Maximum. A rapid talk from Solvey on a title, I have no idea what it is. Surprise. And searching compact star systems for black holes in their centers using the James Webb Space Telescope data. How hard is the temporal build? We have notches tall because you're saying you can tip with as much as you want. I think it's my Casca. So it's about nine-ish. So it's about nine-ish. Twelve is why eighteen no it's it's hovering around the ten ish. We'll see by having this in. Ah yes. So dusty, dusty. Dusty, dusty, Joseph. All right.