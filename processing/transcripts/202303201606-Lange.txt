Let's just start. So, yeah, thank you very much for giving me the opportunity to be here. I would like to go on a little bit on the topics which. On a little bit on the topics which Henrik already introduced to you. So, I will talk a bit about the representativeness of the observations as well as the observations being input to air quality forecasting and how, yeah, or what we can learn for these input data and for these important information for different regional scales. My talk is divided a little bit into two sections. I will start off with the emissions and later on go on and focus more on the observations. So what is the motivation to look at the emission optimization? So nowadays, yeah, life gets more and more fluctuant and changes a lot, and we know it, especially from the COVID. And we know it especially from the COVID pandemic. And in Germany, for example, or in Europe, there's all over the world also, there are renewable energy, so a transfer of the energy system. Such that we won't have that much emissions anymore, as well as we have a change in the mobility sector and so on. And therefore, also emissions are changing, and we have those very Very informative emission inventories. However, they are normally collecting data on a yearly basis or a seasonal basis. And air quality models have to deal with this. And here I would like to show you one study which we performed where we actually compared our model forecast, which you can see here in the background. Can see here in the background with observations taken by a Zeppelin flight, which went around the power plant in Weishaija at the late lockdown time of the first COVID lockdown period. And what we can see here, exactly within this slide I'm presenting here, is that with the model we catch quite well the direction as well as the amplitude. As well as the amplitude of the emissions or the concentrations connected to the emissions by the power plant. However, actually an hour later, so another circle later of the zephyline around the power plant, it looked completely different and our model was off. And why was this? Because we used a general temporal profile for our missions, which are Which are not any more valuable when power plants are driving up and down, connected, for example, by the availability of renewable energy. On the other hand side, I show a plot of a study which is a review of more or less all first available COVID-related publications being connected to air quality. To air quality, and here, for example, you should see the percentage change of. Oh, I see I missed the constituents. But let's say this is NOx, this is ozone, and I think these are CO. I'm not completely sure, but let's concentrate on the two left ones. I'm very sorry, I should yeah, re-upload it maybe such that you have all information. Maybe such that you have all information. And it's collecting the data of all these publications, and we see a strong decrease of ozone, like all over the world, in these first studies being available within the first half year of the COVID pandemic. However, we see an increase of ozone. So there are a lot of non-linearities going on, and saying just reducing emission doesn't mean that we have a general Emissions doesn't mean that we have a general increase and improvement in air quality. So, understanding the emissions is quite important. Sorry, one direction. And therefore, I would like to talk to you about the representativeness of these annual emission inventories for good air quality forecasting. So, you're familiar now with this, the cost range. We do the joint upsystemization, which Henrik just introduced. Candlik just introduces to you. So, doing on the one hand side the initial value optimization and on the other side the emission factor optimization. So, what we do, we assume a certain time profile for the emissions and we optimize the amplitude of this time profile. The studies which I'm presenting are all connected to the URL model, and you've heard this morning already. And you've heard this morning already quite a lot about CAMS. EURATIM is a long-time component of the regional air quality forecasting ensemble and therefore it's an operational system where we perform every day four days forecast on different scales in Europe and several times analysis of air quality in Europe depending on different Depending on different observational and validated data being available. But this is actually connected to a 3D bar system just because of this operation system and the time limit that you have an operation that scales. So, but now I would like to come back to the 4D bar system and what we did within the last years. We were asked by the German Environment Agency, the UNBET Bundes Ant. The Umwet Bundesand, to assess the uncertainties of their emissions. So they have to report emissions every year to the European Union and so on and collect all this data and they have their top-down and bottom-up approaches to assess this, but they don't and they more or less assess all information for whole Germany and then. Whole Germany, and then they use a lot of proxy data, like, for example, the population density, street networks, and so on, to redistribute the total data back on the map of Germany and to the regions. And they ask us to more or less use different observations being available to use our system and to assess if the input of this emission inventory. Of this emission inventory provided by them. If these emissions are very certain, if we see variances and so on by using this FOLIBA emission optimization system. And here we did a full year analysis, so quite computationally costly for the year 2016. 2016 was chosen because it was more or less kind of representing the trend of the climate. The trend of the climate change. However, it didn't have much exceptional events in Europe. So there were no volcanic eruptions, there were not extreme heat waves or extreme flooding. We had a general impression of a mean emission scenario. And we did our analysis on different scales. So for Europe, we did it on a 15 times 15 kilometers grid and nested. grid and nested then another central Euro grid with a five kilometer resolution and for certain scales we also did a one kilometer analysis. So now I would like to show you some of the results. Here on a European scale you can see the emission corrections being analyzed by our system as a average over the full year. Yeah, average over the full year. And I would like to start on the left-hand side. It is showing the root mean square error averaged for each country between the reference runs, so which doesn't include any data simulation, so emission data only being included to the validation stations. And these are actually becomes general validation stations from the European Union. Stations from the European Environment Agency collected all over Europe. And you can see that the overall underestimate CO emissions in Europe. However, there's quite discrepancies between the different countries being available. Now, here in the bottom, And now here in the bottom pa uh picture I show the change of the RNC and you can see that the RNC is getting smaller and this is due to the increase of CO emissions being analyzed by our system. Overall we see a discrepancy more or less between the northwestern part of Europe and the southern and eastern part of Europe where we have much stronger. Much stronger emission corrections being analyzed. This can be due to several reasons, it can be due to the observability, for example, but it can also be due to the uncertainties within the emission inventory. Here you can see a timeline of the analysis, which is an average overall validation stations. And what I want to emphasize here is just that. To emphasize here is just that, for example, we do have ozone corrections. So, sorry, I have to introduce red are the observations. Green is a reference brand without assimilation, and blue is the analysis. And we are able to correct, for example, also ozone, of course, because we observe it, and we can see a reduction in the bio. A reduction in the bias and different and the normalized mean bias and so on, as well as we did for NOx. Then here I would like to show you the temporal evolution of the emission correction factors, which we analyze. And this is actually the mean over whole Germany. And what you can see here, you might be a bit, yeah. Be a bit disrupted by those times here. And this is actually where we did kind of a restart of the analysis such that we were able to run more or less all seasons in parallel. Otherwise, it would have taken probably a year to do the full year reanalysis. So we did like four seasons in parallel and therefore we restarted our analysis at these points again with an emission factor of one. With an emission factor of one, so meaning no change of emissions. But what you can see is shown for two different grids, that for example for NOx, which is the blue or which are blue lines, you can see that somehow NOx is generally underestimated. So we have corrections to higher NOx emissions all over the year and it somehow stabilizes at one time. At one time. Another example, for example, is NH3, so ammonia, which is unfortunately not observed at all in our data simulation system. So we do have corrections for species which are unobserved. However, they are not well, probably not well constrained by, for example, just the PM2 observations, which are leading to these corrections. There might also be some problems with the There might also be some problems with the time profiles we have or we assume in our model, especially for ammonia, but we have to analyze this in farther detail. Here now you get an impression what this full emission correction analysis led to for Germany, so the focus point of the German Environment Agency. On the left-hand side, you can see On the left hand side, you can see the original CO and MAX emissions, and on the right-hand side, you can see our emission correction factors like averaged over the year. And you might be surprised that it looks so diverse and so extreme, but what we can see here in the lower table are actually the total sum of the emissions. And as I said, the German Empire. Said the German Environmental Agency assesses a total CO2 emissions amount for Germany and then extrapolates it with proxy data on the map. And we can see this is the original amount of emissions in kilotons per year and the analysis amount the analyzed amount is actually more or less exactly the same. Exactly the same. So it seems that we do have, of course, spatial variant, strongly spatial variant in the emission corrections, but it seems to be connected a little bit with these proxy data which distribute the total amount on the map. For NOx, it's a bit different. You can see that we have an increase of the NOx emissions being Increase of the NOx emissions being analyzed here. So, in total, there is much more red than blue included. But what we know here is that in the NOx emission being provided by the German Environmental Agency is that the NOx emissions which are connected to the dieselgate affair are not included in this data set. And the discrepancy of these 20% is somehow. 20% is somehow close to the assessment what the diesel emissions or which is the missing amount of diesel max emissions. So this gives us the impression that our system is somehow working and it is supporting our approach that it's valid to be used for such analysis. Here I will give you Uh here I would give you or would like to give you a small insight into the different um analysis results depending on the model resolution. So this is the area of multi-rise failure here in Western Germany where we did a special mess on a 1 kilometer resolution. This is the cutout of the 50 kilometer grid, the 5 kilometer grid, and this is the 50 km. The five kilometer grid, and this is the final nest of one kilometer. On the left-hand side, these are the original Uber or Environment Agency emissions. This is the analysis increment. And you can see, for example, that when we do this, like I call it now global, of course it's regional, but global European analysis for Europe, we do have a decrease of 0.12% of the max. Of the max emissions here, while if we analyze on a very fine grid and just for this really local area, just including the information of local observations and so on, we do have an increase of 10% of the emission. So, yeah, depending on the grid resolution you are using, the representative turbulence, how I call it in my time. Call it in my title can change because the amount and the sum of the information being included in the system is quite different. We actually did another step. So all the analyses I introduced so far were just that we analyzed correction factors for the different emitted species. Emitted species. But within one PhD project, we went a little bit farther, such that we adapted the cost function in that way that we are able to derive emission correction factors for different polluted groups. So to assess the uncertainties, for example, being connected to road transport or to industry or to agricultural activities. And yeah, you can see. And you can see here that the state vector of emission changes, and that we kind of sum up then all information being included and connected to the GNFR sector. So this is a just classification concept for polluter groups. And we now implemented this cost function and the system within. And the system within the EurofIM model. And however, we had to do one assumption to keep the degree of freedom somehow to handle. And we say that we keep the chemical composition per polluter group constant. So if we and therefore we finally And therefore, we finally analyze the emission correction factor per polluter group, and not per polluter group per species, but all species do have the same emission correction factor. And here I actually show it in an identical twin experiment, so it's an artificial experiment where we did a 50% decrease of 50% decrease of road emissions and a 100% increase of industry emissions. And on the left-hand side, you can see we analyzed emission factors for the road transport. And 50% of decrease means more or less an emission factor of 0.5. And you can see that in that area, which we are looking at here, more or less everywhere, it's kind of fitting what we expect in our different. In our artificial experiment. And at the same time, we do see a lot of industries where we have an increase, so an emission factor around two for industry stations. Okay, so this is all for the emissions so far, and now I would like to come to the observation representativeness. So we have all this already. We have heard this already today a few times. All observations are piling up, and we need to know a lot about the information gain we actually get from all these informations. And therefore, we are also looking at what information we gain is provided by the diverse observations and how representative are the observations for the regional scale. For the regional scale air quality analysis. And therefore, I would like to introduce you to some or to one project we are working on. So, here there's a new attempt to perform air quality measurements on a drone basis, so on a UAV. So, it has the advantage, of course, that we are finally able to not get only ground-based data or Only ground-based data or atmospheric column information, but we are able to really well observe the planetary boundary layer, which is the layer we are living in, and where a lot of chemistry is going on and which needs to be understood and also modeled well to get a good air quality forecast. And therein, one of my PhD students is. My PhD students is now looking at more or less the information gain which we get by assimilating such air quality data from clones. And here are some first results. She assimilated ozone and NO profiles at the same time. And here in black you can see the model background, in red the observations in yellow. The observations in yellow, the analysis. And you can see that by assimilating these profiles, we are able to get much closer to the observations and we can get a good increase. On top, coming back a little bit to the emission optimization, so this is just data from this single point here illustrated, and you can see that you kind of can assess local emissions and Emissions and through truth emission correction packages for that region. Coming now to another PhD project, so here we now come to some approaches which we want to solve also with machine learning. So we do have the problem that we have a lot of observation stations being placed close to roads within inner stations. Within inner cities, and so on. And so far, what we did for assimilating on a regional scale, we already sorted those out because they are not representative for our scales, for our grid resolution and so on. And so what we want to assess now is, or what we want to achieve, is to find a way of pre-processing the data such that it's kind of scaled from the in the Kind of scaled from the inner city scale to the regional scale, and this is done with neural networks. And yeah, to get an impression how diverse a regional scale in inner cities can be. So this is a map of Cologne, actually. And here you can see one time, one kilometer more less grid boxes. Here, this is all like very inner city-like. Here, you have a big park where a lot of Where a lot of biogenic emissions are, of course, included. Here are railways as well as highways included, and therefore you have very different patterns in what air quality can look like in these regions. And the approach we are doing here, so we are currently developing a neural network and to upscale this. And to upscale these inner city observations to the regional scale, and as additional information, we use, for example, information on the street canyon, the street canyon architecture about the traffic being, yeah, traffic counts in that area, and for example also vegetation. So, to get information about parks and Parks and other land use more or less when they missed. And this is then connected to the model data. So we have wind directions included and so on. And we use a multi-model neural network. And on the right-hand side, I show you first results where we did a more or less test environment by trying to upscale our 1 kilometer forecast to our 3 kilometer forecast. Three-kilometer forecast, and on the upper plots show the NO2 results having the training data set on the left, and for example, the testing on the right. And here at the bottom, it's ozone. And you can see that, for example, that for ozone, it's where the correlation between the targeted concentrations and the prediction is generally much better. Is generally much better compared to NO2 so far, and we are still trying to improve that. And now I'm coming to another project which is connected a little bit also to the observability, which Henrik introduced. So what I already introduced also in this first project I talked about. The first project I talked about are these EEA ground-based air quality observation stations, which you can see here on the right-hand side. And what we generally get, for example, from the CAMPS project is that they give us a certain data set where they say, okay, this observation you must assimilate, and the rest of the data, this is all for validation. But actually, we don't have. But actually, we don't have a clue how they decided on which observation goes into one data set and which goes to the other data set. And we want to figure out if we can learn certain things to do the best split for that more or less on our own and to learn this out of the data itself. So, to take into account the representativeness of the data for a For air quality in Europe. And here we use a clustering algorithm, or two actually. So one is a C, or it's just called Q C D. So it's a K me clustering approach which relies on the mean and the variance of the chemical concentrations being observed on an annual average. On an annual average diurnal cycle. And the other approach is more or less the same clustering approach, but also including the geographical information of the different stations. And coming now to the results, you can see here. So what we did, we did some data simulation analysis by using these different subsets being used. So this one is the Being used. So, this one is the one we get from CASP. This is the one just using the mean and the variance, and this one also includes information about the geographical position of the stations. And what you can see, or what I want to show on the left-hand side, this is the discrepancy between the assimilation data set or between the analysis. Or between the analysis and the assimilation stations, and on the right-hand side, between the analysis and the validation stations. And you can see that the discrepancy between these errors is quite large for the original data set. And with our KSC clustering, we are able to even reduce this. Yeah, I'm almost done. And yeah, here are also some measures, and you can see that it's quite different. See that it's quite differently for different times when we do the analysis. So it's not completely sure that the KSC clustering approach is the best one. Now coming to the first project I want to shortly introduce to you and which is quite new for us and it's already a little bit of an outlook. So what we want to do now within the destination Within the destination Earth, so which is a program in the European Green Deal and digitalization efforts of the European Union and so on. And main drivers here are ECMWF, ESA, and UNESCOT. And they are actually developing digital twins of the Earth system. And what we want to use here, or what we want. Use here, or what we want to do here is to connect these digital twins, which are main what the digital twin, which is mainly connected to geophysical and weather extremes, with our system connected with different machine learning approaches to provide better information to the actual end users, such for example environmental agency and so on, so they can do their decision making. Do their decision-making and policies. And these are my conclusions. I sum here up a little bit some replies to the questions I posed in the beginning of my talk. I think I won't just not go into detail here and you can maybe just remind you what I was just talking or telling you. And at the end, I would like to, of course, thank all of my co-authors. Of my co-authors, and I want to thank again for inviting me here, and thanks to you all for listening. Thank you very much. I have a question. So on slide 16, when you were talking about the machine learning for upscaling, is the data set that you're training on just from one city, or are you looking at multiple cities? Um we are currently looking at one city and it's actually Cologne where we now focus at because we are quite restricted to the data we have available at the moment and it's quite some effort to get all this information on traffic counts and architectural things and so on. So in Juri it's not so yeah. It's not so widely collected or so far. So, for us, it's more or less getting into contact to different institutions and so on and collecting really data set by data set and so on. So, we want to start with one city, Cologne, and then we will hopefully be able to build up on that and also widen it and probably also more or less have a possibility to connect. Possibility to connect the different cities because, yeah, some yeah, you can learn, of course, from one city for the other city, and so on. But at the first attempt is to just post something aloud. I just heard that sometimes machine learning algorithms can learn things that out, habit to learn. So I was just curious if it would be specific stunt to cologne that may not transfer to another. Transferred to another. Yeah, I think this will be part of the investigation because, of course, if you have a certain structure, for example, of Eustreet Canyon and connected to a specific wind direction, for example, such that you have really, for example, the tunneling effect, such that pollution is just driven out quickly or so, then you can, of course, I think, learn. can of course I think learn very well from data sets from different cities for such scenarios and then of course it's always an advantage to have more and more data in machine learning so therefore it would be very good and this will be part of the investigation while developing parties. I saw you try to get your emission correction At your emission correction factors at different resolutions. You have enough observations to really determine those correction factors. And another more general question is how do you evaluate your image correction results? So regarding the resolution, I think we do have an Enough emissions to at least be able to correct the emission factors. However, you like so and you can see that they are like kind of diverse depending on the resolution. And you have areas, for example, which are close to the boundaries. Close to the boundaries of the smaller grid or grid domains. And you have, of course, your regions which are not well observed. And I think this is a clear question of the observability. But to get an impression for these well-observed regions, we do have, yeah, as a first point, enough emissions to get. But to really To really analyze, you know, if it's fully observed, the region it would be additional investigations. And then you remind me of the second question. Hold you evaluated your emission correction response. Yeah, so of course the emission corrections are then used as an input, for example, for our An input, for example, for our air quality forecasting system, and we can re-evaluate it with observations which are not used within the data simulation system. So we have independent data where we can assess this with. But as I also pointed out here, we do also need a little bit of additional information and therefore the Information and therefore, the information that within this data set this diesel emissions were missing was quite helpful. And actually, so there's one other idea. So with emissions, there are always like reporting cases. So this is actually the reporting of 2019 where the diesel emissions were not included. Not included, so we could, and for example, for 2020 they then included the diesel emissions. And for example, we are now able to compare our results with the newer report of the emissions. Also, to more or less get a feeling on how our analysis is working. But yeah, this has not been done so far. One short question, perhaps? Yes. Perhaps? Yes. How do you specify the prior uncertainty on your emission factors and also the spatial correlations? So we do have more or less fixed emission error coverance matrix being implemented. So this is more or less, yeah. So, this is more or less based on character knowledge and so on. Of course, it somehow fixed, but we use also the diffusion approach to also propagate the information and the spatial correlation and so on. So, that's how I would explain it. 