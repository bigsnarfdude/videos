For inviting me here today. So, it's really wonderful to be here in Oaxaca. So, today I will be continuing talking about partial exchangeability and dependence. And basically, we will be describing an index of dependence other than correlation using the vast restaurant distance. Okay. So first of all, I would like to thank my co-authors. So these are Hugo Laveno, Antonio Leoi, and Igor Kunster from Bocconi University. So Hugo is actually working on optimal transport, whereas Antonio and Igor envision on parametrics, of course. And it has been really fun in It has been really fun in this year to actually find some topics at the intersection of these two fields and actually find questions and possibly answers which could be of interest both from the point of view of optimal transport and from the point of view of vision and parametrics. Already got it. Okay. Okay, so the introduction. Okay, so the introduction is more or less similar to what we already saw with Igor and Betricia. So we are considering the most simple Bayesian and parametric model we can think of so that we have our observations that are conditionally IID given a random probability. So here the parameter of this model is the entire distribution of the data. And so the prior And so the prior is now a law on the space of probabilities. So a law for the random parameter is a law for the distribution of the data. And well, what one can actually prove quite easily is that this class of models entails exchangeable observations in the sense that the law is invariant by permutation of the indices. And while also the opposite is remarkably true. So whenever you have an exchangeable sequence, Have an exchangeable sequence, you know that there exists a random probability such that the observations are conditionally IID. And well, with this conditional formulation of exchangeability, we can actually see how exchangeability is an intuitive homogeneity assumption in the data, and it can be considered somehow like the Bayesian equivalent of the frequency's IID assumption. So, well, then, in order to specify this model, we only need a prior for the random probability. And well, there are many proposals in the literature. Of course, probably the most famous one is the Dirich process, but this is only one example. And more in general, it turns out that many of the proposals in the literature can actually be seen as a transformation of random. Formation of random measures that have a particular structure. And these are completely random measures CRMs. So I'll just say the definition since it's really intuitive. Basically, we are just asking for the random measure that when evaluated on disjoint sets, it provides independent random variables. So basically, it's like a natural generalization of what Of what in R is having independent increments, basically. And well, since the CRMs are going to have an important role in my talk, I will just focus a bit more on these structure. So what is remarkable about this definition is that even by just stating this simple condition about the independence of the increments, you immediately know that the CRMs are almost surely discrete random measures. Discrete random measures. And actually, something more: you also have a representation for the joint law of the jumps and the atoms of the complete random measures in terms of an underlying Poisson random measure. So basically, in order to specify the law for a complete random measure, it suffices to specify Livian density for this underlying Poisson random measure. For some random measure. Nice. And once we have a general way to specify the law for a complete random measure, then there are many possible transformations T of the random measures that can be used in order to specify the law for the random probability measure. And well, these are some of just popular examples. So, for example, you have normalization that we Example, you have normalization that we talked about a lot today, and where you basically normalize a random measure. So, to obtain a random probability measure. And for example, it is well known that when you have a gamma completely random measure and you normalize it, then you retrieve the Diricher process. But then this is just one possibility, because indeed, random measures can also be used to model continuous quantities through kernel mixtures. And for example, one can use them to model. One can use them to model both densities and hazard rates. And moreover, also one can use them to model survival functions through exponential transformation. So it's just to say that there are many possible way of specifying this transformation t of a completely random measure. So to obtain a random probability measure. Very well. So to recap what we said in the previous slide, well, um well often it is um it is an intuitive assumption to say that the observations uh do not uh are not really affected by the order in which they are coming and in such case one of the most intuitive models is to assume that the observations are conditionally iid given a random probability measure and to specify the law for a run for a random probability measure one can consider a transformation of a series Consider a transformation of a CRM, which in turn is specified by assigning a Levian density. Now, as we said a lot today, exchangeability is fantastic whenever you have homogeneous observations, but when you have covariates, typically you want to introduce some form of heterogeneity in the model as well. And for example, one intuitive way to do so when you have categorical covariates. You have categorical covariates. So, basically, when you have observations coming from distinct groups, can be achieved in the following way. So, basically, we say that the observations in one group are conditionally IID, the observations in the other group are conditionally IID, and then one of the most natural ways to introduce dependence between these two groups is then to model the dependence at the level of the random probabilities. Of the random probabilities. And as we saw today, whenever the two random probabilities are equal almost surely, then you can see that this kind of model go back to the one at the top of the slide. So basically, you retrieve the exchangeability assumption. Now, nice thing about this class of models is that they also satisfy an intuition. Satisfy an intuitive symmetry assumption, and moreover, they also characterize this symmetry. So, basically, here we are in the domain of partial exchangeability, and one can actually prove that this class of model is equivalent to asking for partial exchangeable observations in the sense that basically you can permute observations within the same group and still have the same distribution. And still have the same distribution of the random variables. So, for example, here, okay, might be shifted a bit. Ah, yes, it's true. I have to point. Okay, maybe now pointer is fine as well. So, for example, in this example on the left, we are basically exchanging x1 and x2 that belong to the same group. And so we know that the distribution will stay the same. And this refracts. Stay the same, and this reflects an idea of homogeneity within groups. In the second example on the right, we are exchanging x1 and y1 that belong to different groups, and then we are not assured that the distribution will stay the same. And this reflects an idea of heterogeneity among across different groups. So, basically, we see that partial exchangeable models are really ideal when we have observations that come from distinct groups that we Come from distinct groups that we believe are related somehow. But then the question becomes to understand how much borrowing of information we want across different groups. And while it turns out that in order to encode different levels of borrowing of information, one has to model the dependence between the random probabilities. So all the borrowing of information is actually contained in the dependence between the random probabilities. Random probabilities. We can picture out two extreme situations. So basically, whenever the two random probabilities are independent, this will entail that we also have two independent groups of observations. And so there will be no barring of information at all. On the other hand, when the two random probabilities are equal almost surely, so when they are maximally dependent, Are maximally dependent, we will retrieve the assumption of full exchangeability of the observations, and this entails a maximal barring of information. So this is to say that when we have our partial exchangeable model, it's really crucial to elicit the prior dependence structure between the random probabilities, since this will have a huge impact on the final inference of our model. So now So now I actually realize that sometimes it's actually confusing here. What does it mean to have two independent groups of observations? So this is really different from saying that the observations are also IID within each group. So we have two independent groups of exchangeable observations. So for this reason, The problem that we are facing can actually be extended, I think, also to measure the dependence between two groups in somehow less orthodox way, because usually when we are comparing two different groups, it's always like comparing the two distribution of two IID groups. Well, here we are really comparing the distribution of two exchangeable groups. Comparing the distribution of two exchangeable groups. So it generalizes also the most common framework on the analysis of two different or the different groups of populations. Okay, so partial exchangeability. So we said that it is crucial to elicit the prior dependence structure of these models. And interestingly, there have been many, many models and proposals on how to model the dependence between these random probabilities. Dependence between these random probabilities. And indeed, we've seen many interesting examples today as well, but not many ways on how to measure this dependence. And indeed, what we saw today basically focus on correlation between these random probabilities. So this is essentially the question that we dealt with in this paper with Antonio Ngor. So basically, we focused on a broad class of models where the dependence between random probabilities Dependence between random probabilities is achieved in a two-stage procedure. So, first one models all the dependence at the level of an underlying vector of random measures with independent increments. So, Betricia already talked about these kind of vectors. We call them completely random vectors in analogy to completely random measures. And I think it's one of the most intuitive extensions of complete random measures to multivariate. To a multivariate framework. And well, completely random vectors have similar properties with respect to completely random measures. So once again, we have the measures are almost surely discrete. And moreover, in order to assign the log to a completely random vector, it suffices to specify Levy intensity of an underlying Poisson random measure, which this time will be on a more This time will be on a multivariate space. So instead of having a one-dimensional Levy measure, we will have a multivariate Levy measure. Nice. And once we have this class of completing random vector, we can actually no, I wanted to go back. Here. Here. Thanks. And so, and then we can actually apply a component-wise transformation T in order to obtain a vector of random of dependent random probabilities. And once again, this T transformation can be one of the examples that we saw in the first slide. So normalization, but also kernel mixtures or exponential functions. So the idea for our work has been For our work has been to find a measure of dependence. And instead of focusing on only one specific transformation T, since basically in these models, all the dependence structure is introduced at the level of a completely random vector, we wanted a measure of dependence that held irrespectively of the choice of transformation T. Then we also wanted to measure dependence in the situation. Wanted to measure dependence in the situation where the categorical variance has more than two levels. So basically, you have a vector of random probabilities and not just a pair of random probabilities. And of course, we wanted an expression of these measure of dependence in terms of the hyperparameters of the model, because basically, once we know the measure of dependence in terms of the hyperparameter, we In terms of the hyperparameter, we can actually choose the hyperparameters and maybe also the hyperprior so to ensure the desired level of dependence in our model. And as we said maybe at the beginning, we also wanted to go beyond this current measure of linear dependence for pairs of random probabilities. Okay, so what we Okay, so what we did, well, first idea was to, so since we, as we said, we are focusing on dependent random probabilities that are achieved as a component-wise transformation of a completely random vector, we basically observed that since all the dependence is introduced at the level of the completely random vector, we could actually reframe the problem of measuring dependence by looking at the completely random vector. Looking at the completely random vector. And well, turns out that whenever the two random measures or the random measures are equal almost surely, so when they are maximally dependent, we retrieve the full exchangeability assumption in the observations. So basically what this is telling us is that exchangeability can be pictured as a diagonal line in this graph. So this orange, the diagonal line. So, this orange, the diagonal line, and as we said, exchangeability corresponds to the situation of one and mu2 being equal, almost surely so, of maximal dependence of the random measure. And then our idea was to actually measure the dependence in the model in terms of the distance from the maximal dependent scenario. So, distance from exchangeability, distance from the orange diagonal line in the graph. In the graph. So, in order to do so, what we needed was simply to introduce a distance between completely random vectors. So, the results that we achieved in this work have been the following. So, first of all, we defined what I believe is the first distance between completion of vectors available in the literature. And then, since we wanted to compute To compute the value of this distance in practice. And as we said, the complete random vectors are specified through the Levy intensities. We basically found tight bounds of the distance that were explicit in terms of the Levy intensities. And this actually allows us to compute the distance in explicit examples. And finally, we use these tight bounds in terms of the V intensity. Tight bounds in terms of the V intensity to actually measure the dependence in well-known examples in the vision of parametric literature that are actually built on completely random vectors. Okay, so a couple of words on the distance between completely random vectors. So the definition is pretty intuitive, I think. We consider the vasters and distance between the setwise evaluations of the vectors of random measures, and then Vectors of random measures, and then got the supremum overall Borel sets. So here the Vaster sine distance is the Vaster sign distance between probabilities in R2. It's actually the Vasters and distance of order 2. And well, in order to introduce the Vaster Sen distance, for those of you who are not familiar, well, basically, I think that one of the most intuitive ways is the following. The most intuitive way is the following. So, if we have a random object x and we wanted to compare it to a fixed point x0, then one of the most natural quantities is simply to take the expected value of the squared distance. And now, if we have two random quantities, we would like to take the expected value of the square distance again. But if we are only given the laws of the two random quantities, The laws of the two random quantities and not their dependent structure, we cannot compute this quantity because to compute the expected value would need a dependent structure between x and y. And so basically we can take the infimum. No, it's okay. It's okay. Perfect. So, and basically we can take the infimum overall possible dependent structure between x and y. Dependent structure between X and Y. And this notion of dependent structure between X and Y is called a coupling between X and Y. So it's simply a bivariate distribution that has X and Y as marginal distributions. Okay, so why did we choose the Vasters and distance? What is nice of the Vasters and distance? Well, the fact that it gives a lot of importance to the understanding. importance to the underlying geometry of the space. So basically it is really ideal whenever we want to compare distributions that have different support. And this is actually our case because as we said, the exchangeability is supported on the diagonal. So in practice, when we will be using this distance to measure dependence, we are always comparing distributions that have different support. And well, in this scenario, many common distance as Hellinger. Many common distance as Hellinger, total variation, Kolbach Liber, just would not give an informative evaluation of the distance. So once we have defined the distance in terms of Basrian distance, as we said, we wanted to find quantities that were explicit in terms of the Levy intensities, because this is the only way to compute. Because this is the only way to compute the distance in terms of the hyperparameters of our model. And so, no, no, it's okay. Thanks. And so we actually managed to find tight bounds of the distance in terms of the levy intensities. And now, to give you a flavor of the way we prove this result, I will just be saying a few more words on the Just be saying a few more words on these multivariate levy intensities that I mentioned before. So, for this, so just I mean, for the sake of presentation to keep things simple, I will focus on the so-called homogeneous complete random vectors, where basically you have the atoms that are independent of the jumps. So, I guess it's in line of with the multivariate species something we were talking about earlier. We're talking about earlier. And so I will basically call raw the Levy measure corresponding to the joint distribution of the jumps of these random measures. So let's have a look at what kind of object is this raw. So raw is a measure, and it is supported on the upper right quadrant. And it is actually a very particular kind of measure because it can be both functional. It can be both finite and infinite, having infinite mass around the origin. And well, it turns out that in most applications in vision and parametrics, we are actually dealing with this framework where raw has infinite mass around the origin. But if we were to consider raw with finite mass, then one could actually prove that the corresponding Sawwise evaluation of the complete random vector has a compound Poisson. Has a compound Poisson distribution. So basically the idea is that given a complete random vector, we can always find a compound Poisson approximation of the complete random vector by simply removing a neighborhood of the origin. And well, thanks to this compound Poisson approximation that copes well with the vast rest distance, we managed to find a Managed to find a time bound of the Vaser sign distance between complete random vectors, which is explicit in terms of the Levy intensities of the complete random vectors. So basically here, what you see here is the Lasercent distance between two probabilities that you basically obtain by cutting away a neighborhood of the origin in your Levy measure and re-normalizing it so that the total mass is one. So that the total mass is one. And well, it turns out that this distribution is actually the distribution of the jumps of the compound Poisson approximation of the complete random vector. Very well. So now that we have a quantity that is explicit in terms of the Levian densities, we were actually able to compute this explicitly in many examples in the Bayesian parametric literature. Parametric literature. So, for example, here we have compound random measures that are a very flexible way to model the dependence between random measures that was introduced by Jim Griffin and Fabrize Seleisen in 2017. And I won't dwell on how they introduced this dependence between the random measures for time reasons. I only say that there is an hyperparameter phi. There is an hyperparameter phi that does not influence the marginal distribution of the completely random measures, but only their dependent structure. On the other hand, since phi appears in many different layers, it's not easy at all to understand how this parameter phi influences the dependent structure. And well, thanks to our techniques, we were able to bound the vastness and distance from exchangeability with a quantity. Changeability with a quantity that is explicit in terms of this hyperparameter accounting for the dependence. And then one can ask whether this bound is tight or not. Well, this, I mean, theoretically, it is an interesting question that we're actually trying to understand better. But graphically, we do have some empirical proof of the fact that the bound is tight indeed. That the bound is tight indeed. So basically, in blue, you can see our value of the upper bound. So the quantity here on the right. Whereas in red, you can see the simulated value of the vast resin distance. So our theoretical upper bound in blue appears to be very tight with respect to the quantity that we are trying to understand. And well, yes, then we have similar. Well, yes, then we have similar results also for gem-dependent random measure or additive random measure. They go under more than one name. Once again, we have these bounds that appear to be tight of the vestration distance from exchangeability in terms of the hyperparameters, in this case Z, that account for the dependence of the model. Okay, so a brief re- So a brief recap. How much time do I have? 10 minutes. Okay, great, great. So a brief recap of what we said. So I guess that the main contributions of this work have been two. So first of all, the idea of introducing a distance between completely random measures or completely random vectors, which basically has not Which basically has not been explored in the Bayesian parametric literature. And I think that it can actually be used for many different scopes, even beyond the idea of measuring dependence. And then the second contribution has been the idea that in order to measure the dependence for these infinite dimensional structures, such as random measure, then probably one of the most intuitive ways. Probably one of the most intuitive ways to do so is by introducing introducing a distance. Introducing a distance and no, no, it's okay, it's okay by introducing a distance and evaluating the dependence in terms of distance from the maximal dependent scenario. Okay, on the other hand, there are also some questions that. Also, some questions that were left open. So, first of all, as we said, we measure the dependence in terms of distance from the maximal dependence scenario, which is exchangeability. But this really leaves out the question of how do we incorporate also a measure of the discrepancy from the other extreme situation, which is the one of independence. Then no, it's okay, more or less. Then, another point, which is a bit more subtle, I think, but also quite crucial, is that measuring the dependence in terms of a distance, I think it's really natural to do comparison. So, basically, you say, So basically, you say one random vector, computing random vector, is more dependent than another one whenever they are closer in terms of a distance from the maximal dependent scenario. So in this case, from exchangeability. But then this really doesn't allow you to do absolute quantifications of dependence. So when does a complete random vector have an intermediate dependent structure, or maybe a highly dependent. Or maybe a highly dependent structure. And this and this is because we do, at this stage, we do not really know what is the maximum possible value for the distance from exchangeability. And so without knowing the maximum, basically you cannot do absolute quantifications of the dependent structure. And so this is to say that we really had to find the maximum of the distance in order to. Of the distance in order to extend the use of this measure of dependence, also in these frameworks. And finally, a more technical detail, if you want, but also important is that, well, it's true that we have this nice natural distance at the level of the random measures, but then in order to perform exact computations, we were always We were always actually computing an upper bound of the distance. And so a question could be: can we actually interpret also the upper bound in terms of a distance from exchangeability, this time at the level of the Levy measures? And if we do so, we would basically retain the idea of measuring dependence in terms of distance, but at the same time, we could also perform exact computation. Perform exact computations of this measure in many notable models. And well, these are the questions that we dealt with in this follow-up work, together also with Yugo Lavenon from Bocconi University. And so, first of all, how we address the problem of understanding this upper limit. So, the problem with showing that this quantity is a distance comes from the limit. comes from the limit in the expression. Because basically, when you have a quantity that is defined through a limit, proving the non-degeneracy property of distances becomes non-trivial, let's say. And so basically what we began thinking with Rugo is that actually we could maybe interpret this limit as an optimal transport problem, this time not at the level of the random measure, but at the level of the levy measures. Level of the Levy measures. And well, we spent a lot of time trying to prove this was a distance and showing the most important properties of the distance. And then we were actually able to show that this coincides with the extended buser sign distance that was introduced by Figali and G in 2009 in a different context, but that has this very desirable. That has this very desirable property of allowing for comparisons between measures that have different mass and possibly also infinite mass, which is actually crucial in our context because, as we said, these Levy measures we are dealing with in Bayesian parametrics usually have infinite mass around the origin. So, I would just, so 10 minutes. Okay, I wanted to give you. minutes okay i wanted to give you um an intuition of uh this uh distance but maybe it's uh 10 minutes ah okay i thought it was five minutes ago so great then i will give you some intuition of this distance um so because uh since it is very intuitive i think it's it is worth spending a bit of words on this now uh i will fix to give you an intuition on the case of levy measures on the real line On the real line, even if it's different from our context, but just because it's easier to grasp the main ideas in this one-dimensional context. So basically, with the Vasierstein distance, we saw that we had this expression that was an infimum over all possible couplings. And so the idea is when we have measures in zero plus infinity, how can we define couplings? Define coupling? Well, basically, in the same way, so we say that a coupling is just a measure on the joint space that has row one and row two, so the two Levy measures we want to compare, as marginal measures. Then there is a technical detail, if you want, that instead of focusing only on these random measures on the On the product space zero plus infinity, zero plus infinity, we also add the axis. And why do we add the axis? So it's simply because in this way, if we consider measures that instead of being defined on zero plus infinity, open set times zero plus infinity, open set, we actually consider them as defined on zero plus infinity with the origin. Infinity with the origin, then basically the set of all possible couplings becomes weakly star compact. And this is really a crucial thing in order to prove that the optimal coupling exists. And I mean, for those of you who are a bit familiar with Wasterson distance, this is a crucial point in proving that it is a distance indeed. Okay, so it's a very natural extension, I would say, of what happened. I would say of what happens with probabilities. And so, once we have this notion of extended coupling, we simply consider the infimum overall possible couplings of the integral of the square difference between x and y, and which is exactly the measure theoretic equivalent of taking the expected value of the squared distance between x and y when working with random variables. Random variables. Nice. And well, at least for Levy measures on R, we actually were also able to find the expression of the optimal extended coupling for Levy measures, which actually brings us to the expression of the extended vast distance between Levy measures on R. And it really comes, just wanted to show it to you because it comes in a very intuitive and simple expression. And simple expression. So basically, we consider the tail integrals of the Levy measures. So instead, let's say, from instead of taking, so we take the mass between t and plus infinity, which I mean, K integrals are kind of a standard quantity when dealing with Levy measures, because since you have infinite mass in the origin, you don't want to take a CDF, but rather start from infinity, basically. And then one can prove that the And then one can prove that the extended vascular distance is simply the integral of the, so the altered distance between the inverse of the difference, sorry, this distance between the inverse of the tail measures of the Levy measures. So this is to say that at least in R, one can actually prove a very Can actually prove a very similar result to what happens with random variables, also in this context with measures that at first appear as extremely more complicated than probability distributions. But now, turning to our topic of measuring dependence, we are not dealing with Levy measures on R. As we said, they are multivariate quantities. So we are living with Levy measures, if you want, in R2. Are two. And so basically, in this context, as it happens with the standard vast and distance between probabilities, we do not have the expression of the optimal coupling in principle. So it's a different situation, but yet the interesting thing is that we, I mean, our goal was, as we said, to find the maximum possible distance from exchangeability in order to perform at. In order to perform absolute quantifications of dependence. And so basically, you can see that this involves solving an intriguing maximum problem because the extended vast distance is, as we said, an infimum over all possible couplings. And then we want to find the maximum completely random vector that, sorry, the completely random vector that maximizes this quantity. Quantity. And well, we actually managed to solve this problem. And well, the main idea, if you want, a bit, I mean, just a hint. So first of all, the fact that since we want to find a maximum, rather than working with the so-called primal formulation of the distance, which involves an infimum over all possible couplings, we actually work with the dual formulation, which From relation, which reframes the same quantity, the vast resin distance, in terms of supremum over all possible potentials, whatever this means. So this was kind of the menadir. Then by using this dual formulation of the vast indistance, we actually managed to find the expression of the optimal pair of potentials, which is the pair of potentials that maximizes this quantity. That maximizes this quantity, and this is not simple, it actually involves also finding the optimal coupling for this transport problem. But we were really helped by the fact that we are comparing the distribution to something which is degenerate on the diagonal. And so, basically, the fact that the support is degenerate makes this problem easier than the general problem, which is currently unsolved in the optimal transport community. Transport immunity. And well, by using this result, we were actually able to prove that the maximum distance from exchangeability is achieved when the Levy measure is supported on the axis. So exactly what one would expect by looking at a drawing. And basically, the Levy measures that are supported on the axis correspond to the situation of independence between the random measures. Random measures. And so basically, we managed to prove that the maximum distance from exchangeability is actually achieved in the other extreme situation that we highlighted, which is independence. And this is, I think, it's nice because we have this intuitive result. I mean, we imagined that the maximum from exchangeability in this class of models would be under independence, but actually proving it, I mean, we needed a bit of theory behind that. Bit of theory behind that. Okay, so two minutes. Great. So basically, once we have this maximization result, then we immediately had an intuitive notion of index of dependence for a general vector of dependent random probability measures simply by normalizing it. And while this Vastersen index of dependence has some Index of dependence has some desirable properties. So, first of all, we don't have to restrict to two random probabilities. We can actually measure the dependence for a vector D of random probabilities. Then we have that this index is equal to zero if and only if the groups of observations are independent. Once again, this doesn't mean that they are IID and independent. This means that they This means that they are two independent groups of exchangeable observations. And it is equal to one if and only if the observations are fully exchangeable. And as we said, the nice thing of defining this index at the level of the Levy measure is that we can actually compute exact values of this index. So, for example, you can see the value of the index. Index for compound random measures and GM-dependent random measures as the hyperparameters phi and z respectively grow. And well, the nice thing about this plot, I think that now that we have an index that is between zero and one, we can actually answer to questions of when does a completely random vector have an intermediate dependent structure. So this will happen when the in So, this will happen when the index is close to one half. For example, for compound random measures where phi is close to one, for gm-dependent when z is close to one half. And this can actually be used to fix the hyperparameters of our models. So, to ensure the exact level of dependence that we want, but also to choose the hyperparameters of our hyperpriors of our model. Of our hyperpriors of our dependent structure, if we want to learn also the dependence in our data by, for example, centering them around a specific value of interest. Okay, yes, maybe, okay, I don't have any more time, so I will skip the recap and the future research and leave you with some references. Thanks. Thanks. Yes, thank you, Marta. And there are some questions here in the forum. I did. No, I didn't pass the answers for the questions. So thanks for the So, thanks for the talk. It's very interesting. I was wondering: so, how much is it dependent on the fact that you're using a two Versus Train distance? Like, if you had chosen one Versus, would you get something different? Yes, this is an extra question. Actually, in principle, thank you. I need an assistant. I'm already at that stage. So, basically, So, to define the distance, it would be perfectly fine to just consider vaster sign one or vaster sign p and makes no difference. What is actually matters a lot for the vaster sign two is for the fact that for solving this max min problem. Because basically, what I hinted in the solution here is that solving this Solving this dual formulation actually involves finding the optimal coupling. And well, with the Vaster sign 2, you have this Bernier theorem, which has an equivalent also at the level of the Levy measures, basically, which gives you a way to understand when a given map is an optimal transport map. Basically, if you can express it as a gradient of a convex function, then you know that it is a That it is an optimal transport map, which basically you don't have for vast one or the vast two. So fixing the vast is really important to find the expression of the optimal coupling, which actually is one of the components of solving this maximum problem. That's all. Thanks. There is another question here, Julian. Julian. Yes, I wanted to ask the question that could relate to some talks we had earlier today about negative dependence. Do you think you could somehow treat this in this option as well? That's a great question. Well, the fact of these models of these CRVs that I introduced before is that they only account for Before is that they only account for positive dependence. So, the reason why the maximum distance from exchangeability is independent is exactly because you can only achieve positive dependence in this castle models. Now, the talk about Betricia and their work has been actually very inspiring. And I am thinking that possibly Instead of focusing on CRV, so CRVs have these different jams and common atoms. But yet one could also define a diagonal if one considers these multivariate Levy measures where you have both different jumps and different atoms. You could still define a notion of diagonal, taking into account not only the jumps but also the atoms. Atoms. So I'm wondering if most of these results could actually be extended also in this extended class of random measures by preserving certainly the same intuition and possibly also the same techniques. So, yes, that's a great question, something I will be considering for sure. 