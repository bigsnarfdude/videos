Andrew Sher Professor in the science at the Technion in Israel. He has worked, as many of you know, he has worked in various areas of the shape reconstruction and analysis in computer vision, image processing, deep learning, and as well as computer graphics. Rome received numerous recognition, including HIV Fellow and the SIM Fellow, and his achievement at And his achievement actually also further extends to industry. He's a founder and co-founder of many very successful companies, including geometric image processing. Without further ado, today Ron is going to give us a talk on geometry and learning. Ron, please. Okay. Thank you very much. Thank you for the invitation. So what I want to tell you today, To tell you today, let's see if it works. Yeah, is my way of thinking, my very biased way of thinking of how can we apply learning method to problems in geometry. Now, it's not, it would also involve learning the geometry, but before we get there, let me just tell you of something that probably everybody knows. This is the huge revolution that computer science went through the last decade. Through the last decade. So, rather than inventing algorithms, we start training networks. And the idea of training networks is fundamentally different than thinking about a solution. You just throw everything on the machine, a lot of data inside outside, and inside that, usually you have some annotations of what this data looks like. And you train this machine to have a new input and output something that's similar to something, some observation that it has seen before. Some observation that it has seen before. Now, part of this revolution occurred because of something which is called the process of convolution. Okay, the fact that you can apply the same operator to each and every point in the image in the same manner allowed the training of many parameters involved in these networks to do something useful. And in fact, if we go to the beginning of this revolution, the ImageNet, the AlexNet, All of them were involved convolution neural networks. So you had this layer in which you apply the same kind of operator again, again, again on the image. Now, this is good. And let me show you an example of something that we are doing in my lab, in which we apply convolution neural networking in order to solve a problem, a very painful problem. And this is the problem of classifying histological images. Histological images for cancer. So, usually, what happens if somebody feels some lump in his chest, he goes to a physician. The physician takes a sampling of this tumor. And then a pathologist is slicing and coloring this tumor and looking at it through a microscope and decides whether this is cancerous or not, whether it is benign. And if it is cancerous, how severe is the cancer? Cancerous, how severe is the cancer, what is the grade of the cancer, and then it splits into what kind of cancer it is. And when you explore what kind of cancer it is, usually you have to color, to recolor the something that you got with something which is called immunohistochemistry. So, in this research that was actually led by my former student, Gil Shamai, now a post-doctor in my group, as well as some physicians, you have been in the Joav, Binenbaum, Gil Ziv, and Iri Duek, as well as Ron Slossberg. What we did is try to infer from the original coloring, which is called dematoxyliniosine, what would happen if you would have colored it with immunoistochemistry. It means that can we look at the simplest way of coloring the tissue and infer The tissue and infer what would be the outcome, tell what would be the outcome as if we continue processing this tissue. And this is very important in shortening time. In fact, that in several countries you don't have access to this immunoistochemistry. And in fact, we got interesting results for estrogen. And now we are continuing to process it for. So, what you see here at the top is what I said. Here at the top is what I call the hematoxidin and uazine coloring. This is the basic coloring that every histology, I mean, tissue sampling is going through. And what you see at the bottom is immunohistochemistry coloring, and it actually emphasizes where the nucleus of the cells are sensitive to the specific coloring that you are doing or not. And what we are able to do is look at the top part. Is look at the top part and say what would be the outcome as if we colored it with the bottom part. Now we don't stop there, we can actually now look at the original coloring and infer more information from there. And this involves classical convolution neural network. There are a lot of magic and geometry that is going on behind the scene, but the core is using convolution in order to process the image. Now, in geometry, we don't have the large. In geometry, we don't have the luxury of applying the same machinery again and again. So, therefore, we have to take the geometry into some space in which we can do that. Okay, so let me just remind you of the basic signal representation that the courses that we are doing. Usually, you have a function, and this function is the function that you would like to explore. And usually, you are looking for a basis or a space in which you can represent this function by a few co- Represent this function by a few coefficients. Okay, so you multiply these basis functions or other functions by as few as possible coefficients. And for example, we know that if the family of functions that we would like to represent Dirichlet boundary bounded functions, then go to the operator, go to the Laplacian, diagonalize it, and what you get are the Fourier functions. So the Fourier functions are optimal, and as Frank-Brasi showed us, in fact, And as Keimberg's issued us, in fact, unique in order to represent in as small a number as possible coefficients the given function. Okay, so just remember that in one dimension, what happens is that if you have discontinuities, then wavelets would be the optimal one. But if you get into more dimensions, then Fourier would be your choice of selecting this kind of function. Okay, now let's get to the most simple problem. Let's get to the most simple problem in geometry, and this is the problem of trying to quantify the similarity of two planar shapes. So you have two curves in the plane, two simple curves, they don't intersect one another, and they are closed. And what you'd like to do is ask whether they are the same or not. And in fact, you would like to also quantify the distance between them. I mean, how different they are. And let's not limit our discussion just to Euclidean transformations in the plane, rotations of translation. Transformations in the plane, rotations of translations, but let's also consider equiaffine, affine, projective, scale, etc. I mean, all the sequence of regular groups of transformations in the plane. Why? Because they are really important when we are trying to analyze images and shapes and they are really interesting from a practical point of view. So, what would we do classically? Classically, what we would have done, let's see, is first of all, Is first of all define a measure of length on the curve, and a length that would not change as a function of how I travel along the curve would be the Euclidean one. So the Euclidean arc length would be the usual one. And then if I would like to generate a signature for the curve, what I do is plot the curvature, one over the radius of the oscillatic circle, as a function of the arc length. This would be a signature for the curve. So and interesting. And in fact, from the signature, I can go back to the curve integrating twice up to rotations and translations. So, this was geometry 101. So, I have two plumber shapes and I deliberately present it because it would allow us to lift our thoughts into more complicated scenarios when we start dealing with surfaces and stuff. Okay, so we have the two curves, and when I plot the curvature of these two curves, they should align if I start from the same point. Align if I start from the same point. If I don't start from the same point, then I have to shift one signature with respect to the other. And in fact, if I now travel along the curve with the Euclidean arc length, and I'm looking at the second, at the Laplacian operator, the second derivative, and Diagonalizes it, what I would get are the Fourier functions. So I can, in fact, project my curvature onto these Fourier functions and ask what would be the coefficients, the first k coefficient. Would be the coefficients, the first k coefficients of the red and the yellow sharks, and compare between them and verify if they are similar or not. In fact, in order to get rid of the shifting, I can take the absolute values. I mean, I can get rid of the phase, but then I would lose the discriminative power of my descriptors. So now what happens about scaling? If I would like to scale my, if I would like to think of Like to think of a length that would be invariant to scaling, uniform scaling of the shapes, what I usually do is take the change of orientation as my arc length, which is nothing but the curvature times the Euclidean arc length. Again, you learned it in your basic geometry courses. Curvature is nothing but the change of orientation divided by the arc length. Okay. But this is also a scale-invariant arc length. Why am I telling you that? Why am I telling you that? Because as we go to surfaces, it would become more and more interesting. Okay, so how can we represent kappa s, the curvature, by projecting it onto the Fourier? It's very trivial. It's very simple. What I need to do is project it onto the let me try to what I can here. What I can, not pen, but. What I can, not pen, but rather projector. What I uh what I would like to do is project my curvature onto the uh onto the Fourier. And the Fourier is something that I can do by just taking the Fourier over the circle. Okay, and then I just take the first k or the first n coefficients and try to compare them between different shapes. Now, remember that there is another way of measuring arc length on my along. Arc length along my curve, and this is the scale invariant one. And if I'm taking the eigenfunctions with respect to the scale-invariant one, I'm basically embedding the curvature as part of my eigenfunction. So I have the Fourier, which is extracted from the second-order derivative with respect to the Euclidean arc length, and I have the curvature some way embedded in the eigenfunctions of the second derivative with. Of the second derivative with respect to the scale invariant one. Now, you can see that in order to mimic this effect of having these first coefficients, what I can do is hit these two eigenfunctions, one with respect to the other, and thereby capture the essence of the geometry. In fact, I need to do it only for the first eigenfunction here and hit it with the Fourier, but let's assume that I do it for all of them. So, what I'm proposing. So, what I'm proposing is taking the Laplace, sorry, the eigenfunctions with respect to the scale invariant metric and the eigenfunctions with respect to the regular metric, which are nothing but the Fourier. So I have the harmonics with respect to the scale invariant metric, and I have the harmonics with respect to the regular one. And hitting them one with respect to the other, what I get is a set of coefficients. And the set of coefficients are nothing but, you heard yesterday, I mean. nothing but you heard yesterday I mean some of you heard yesterday the lecture by Mark Socianikov are nothing but the functional map coefficients of the shape with itself. Functional map it would take I mean this is this is by definition what is going on and in fact my PhD student Oshri Halimi took this idea into something that she called self-functional max. In a moment I will show you some examples of what self-functional max but state Cell function of max, but stay tuned. Just remember that this is a way of taking the curvature and somehow projecting it onto the Fourier. It doesn't matter in which metric, either the scale invariant or the regular one, and getting these coefficients by which I can translate the problem of comparing between two smooth manifolds into one of comparing between just a set of numbers and a ray of numbers. Okay, now before Now, before we go on and give some examples, let's ask the following question. Can I learn the curvature? Can I train a neural network that would look at points that describe a curve and learn the curvature or the arc length for the current discussion of a curve? And the answer is yes. And the first idea was presented by Galton Pay and Arlen Wetzler a couple of years ago, in which we computed the curvature, and now In which we computed the curvature, and now Roy Velev is working really hard also to evaluate the arc length as well. And the idea is that, in order to be reparametrization invariant, what you do is you sample your points along the curve differently in each and every time. And what you would like to do is train a Siamese network, two identical networks, I mean, identical from the point of view of the value of parameters inside the network, that would look at, I would say, here, seven neighborhoods about a point. Heard about a point and would spit out the same number if it belongs to the same curve and a different number if it belongs to a different curve. Now, what I showed you here is a framework for training a neural network to output a curvature. Now, the curvature doesn't need to be invariant with respect to the Euclidean one. You could fit in more complicated scenarios like the equiaffine, the scale invariant, etc. But what you need to take care of is the fact that it needs. Of is the fact that it needs to be meaningful. I mean, you cannot take the affine and make your ratio of eigenvalues be equal to infinity or zero, because otherwise you just collapse all participants. I mean, you need to have some numerical support for your network to work by. So this is a nice idea of training a network to come up with the curvature. In fact, the arc length is a little bit more complicated because in arc length, you don't measure a point quantity, a differential point quantity. Point quantity, a differential point quantity, but rather you need to somehow integrate. So you need three anchor points and verify that the triangle equality holds for these three points along the curve. And the points need to be corresponding to one another when you apply different transformations on the same curve. Okay, so now with curves, we were really, really lucky. There is order to the points as you travel along the curve. What happens with surfaces? What happens with surfaces? With surfaces, we have a huge problem. There is no order. I mean, there is still the notion of vicinity and neighboring points, but if I take one point and another, there is no unique order by which I visit the points on my surface. However, there is another remedy here, and this is the fact that if, in a usual curve, what I would measure is the Euclidean distance between points, then all planar, simple planar curves would be homomorphic to a unit. Would be homomorphic to a unique circle. With surfaces, the geodesic distances on the surfaces between points are actually very informative. They would define the surface up to its isometries. So rather than trying to so what we will do is in fact measure the geodesic distances between points and use this measure as a way of comparing between surfaces. And this is something In between surfaces, and this is something that we tried, that we started work on, I would say, about more than 20 years ago. And we have been really busy using this kind of measure in order to quantify the distance between surfaces. Okay, so assume that I have two surfaces, S and Q, and some oracle is giving me all geodesic distances between all points on S and all geodesic distances between all points on Q. Now, the moment I'm trying to sell... Now, the moment I'm trying to sell you that, remember that I'm a computer science scientist and also now, by courtesy, an electrical and computer engineer. So, it needs to be computable. I mean, someone needs to give me a formula by which I know how to compute all these distances. Now, distances are luckily turned out to be a fairly simple equation. It says that the gradient of the distance function on the surface is equal to one, what we know as the icono equation. And the question is, how do I solve it? And the question is: how do I solve it efficiently between all points to all points on the surface? Okay, this is the input to my algorithm that would try to measure whether two surfaces are similar or not and quantify that. Okay, so to that end, we used to have something which is called fast marching method. But with my student, Moshe Lichtenstein and Gautam Pai, what we did is also try to train a network to solve the iconal equation. And what Moshe. And what Moshe did is fitting these neighboring points into a neural network, training it. I mean, in the fast-marching method, it used to be that you need to solve the quadratic equation, but why couldn't I just train a network to compute a quadratic equation? I mean, we didn't train the network to solve a quadratic equation, but rather computed distances analytically on all known surfaces that we can think of. Known surfaces that we can think of, I mean, mainly plane and spheres, and see if the network can generalize it to more complicated scenarios. And here we just did it for weighted planes. Now, on regular grids, the problem of computing accurate distances, accurate in the sense of the distance between points is defined as h, and on surfaces, the length of the edges is defined. The length of the edges is defined as H. And what we would like to have is an algorithm that converges as fast as possible. So the higher the power of H, the better is my algorithm. Now, on triangulated domains, on two-dimensional surfaces, there has been the fast marching method, the extension of the fast marching method that we did with Jamie Setian a while back. And there was also an exact algorithm, which is an implementation of Malpapa Dimitri. Implementation of Mount Popadimitri and Mitchell algorithm. And it took about 20 years for Suratsky, his wife, and colleagues to implement the algorithm that was called exact algorithm. It was exact because it was able to compute exactly the distances on the polyhedra approximating the continuous function. But if you think about it for a moment, this is an order of H squared algorithm. So the complexity. So, the complexity was squared in the number of points describing the surface. The accuracy was h squared. So, why bother? I mean, why don't you just use the linear algorithm that was able to compute the distances in order of h accuracy and go to the n-square? I mean, you can think about it as just populating your grid with a tensor grid, n-square the points, and have the same accuracy. And the question is, why bother? So, the task. So, the task of Michelle Lichtenstein was to take this problem, this training problem, and try to have an order n algorithm with order h squared accuracy. Now, obviously, the results would be only numerical rather than a theoretical result, but he was able to do that. I mean, numerically, he was able to do that if you train, to show to show that if you train a network, then you would converge. Converge in eight squared, what you have here is log of the distance between points, the edges on your surface, and what you have here is log of the error. And you can show that the exact and the learned algorithm were actually producing the same slots. And this is the fast marching method, which is order of one accurate. Okay, so we can, in fact, extract, and this is with two norms, the L1, DL infinity. Infinity. And you need to be cautious here because when you have shocks, usually you would lose your accuracy. But still, we can get an order in a linear algorithm in the number of points, which is second-order accurate by training a network to locally solve the iconal equation. Okay, so we can compute distances between points on two surfaces S and Q. And now the question is. The question is: How do I measure the discrepancy between two surfaces? And the idea goes to the Gromov-Halfdov distance, or sort of Gromov-Haupt of distance. We heard talking about the Gromov-Wassenstein, in which you also have some measure on the space on which on the Q and S spaces, and you're trying to also manipulate these measures one with respect to the other. So, assume that I have my two integeodesic distances on Q and S, and the Gromov-Hausov distance is telling us how to map basically this Roy is mapping S onto Q so that the distances in Q would be S similar to the distances on S. And the Gromov Hazdoff is basically maximizing overall permutations that would take the points in S to the points in Q. Okay, so assume that we have that and this is the measure that we would actually. And this is the measure that we would like to compute. The problem is that this measure is really challenging to compute. Our first attempt: so, again, this is what we would like to compute. Find me the mapping from S, the left hand, to Q, the right hand, that would minimize the distortion of the distances of corresponding points. The question is: how to do that? First attempt, usually as engineers, what we are doing is going for We are doing is going for the simplest possible solution. So, the simplest possible solution is applying PCA, principal component analysis, or if you like, multi-dimensional scaling, same thing. So, you take all intergeodesic distances on the left on the surface, and what you do is you try to embed them onto a flat domain. And this flat domain needs not be only three-dimensional, it can be four-dimensional, five-dimensional, etc. I mean, you can look at how the eigenvalue. I mean, you can look at how the eigenvalues of after you do double centering of these matrix look like and decide what would be the acceptable error that you allow when you treat the curved metric as embedded into a flat one. Okay, so basically this is the problem that we solved. We asked what would be the embedding of the points in S into Rn so that the distances would be as close as possible to those in S. And we applied the same machinery. And we apply the same machinery to S and Q, and then by just rotations and translations in the Euclidean space, we ask the question whether these two surfaces are similar or not. And surprise, it was actually doing well for very simple tasks. But there is a fundamental problem, and we wanted to be more delicate. What is the fundamental problem? We all know that when we take a curved domain and we try to embed it into a flat one, we introduce unavoidable. Flat one, we introduced unavoidable errors. So, in this case, what I show you here is that I have the sphere and I have these two half of great circles, and I have this part that is going along the equator. And assume that the distance between A and C and the rest of them is pi over 2. Okay, assume that the sphere has radius of 1. And now I populate my intergeodesic distances matrix and I try to embed it into a flat domain. So I'm taking point A, point B. So I'm taking point A, point B, and point D. And long and behold, it doesn't matter what would be the dimensionality of my Euclidean space, it would be a straight line. Why? Because the triangle inequality holds as an equality. Okay? But what happens to the points A, C, and D? They should also be a straight line. And in fact, what I see is that point B and point C should be the same point, which means that there is a problem. That there is a problem, there is an inherent problem of taking curved domains and embedding them into flat Euclidean spaces. Let me just tell you that what Gil Shamai did is try to embed into convex spaces, and in some cases he was able to show that you can simplify these problems into burial ones. Okay, the first attempt was to say, okay, assume that somebody, for example, multi-dimensional scaling, the linear idea, is giving me some good initial position. Initial position. Let's treat this idea of finding the Gromov-Hausdale distance that Memoli and Saphiro first introduced into this into this suggested into this game by minimizing the location of points on Q. So I'm mapping each and every point on S to a point on Q. And now I'm allowing these points to find a better position so that they would minimize this idea of idea of of uh uh of the gromo-waltov distance of this local distortion so you can define it as a gradient descent process and you can hope that something good would uh would get out of it obviously you immediately change the l infinity norm by some regularization by the l2 norm and and it does it does it does some uh it does the job i mean you would converge into this local minimum of permuting s into q but uh we can do better now we resort back Now we resort back to our good old Fourier. So you can present the problem of mapping the intergeodesic. So assume that capital D S are all intergeodesic distances in S and capital D Q are all intergeodesic distances in Q. And what you're looking for is permutation of the points in S into those in Q. So you can write it like that. In fact, the first one that wrote it like that was Anastasia Dubrovina, who now lives in California. So. In California. So, together with Obrovina and Aflalo, what we did is translated the problem into the spectral domain. What do I mean by that? If I take the intergeodesic distances on S and the Laplacian on S, and I'm projecting the distances as functions onto the Fourier on S, then what I get is the following form that you see over here. Okay, so I get the eigenfunctions, and there is hopefully And there is hopefully a low-dimensional matrix, a low-ranked matrix that is sitting in the middle, and then those guys, okay? And the same goes for Q. And obviously, for Q, I have a different Fourier, I have a different spectral functions, okay? Spectral decomposition. Now, apparently, the permutation can be written by nothing but the following form. Okay, so I take the Fourier of S and the Fourier of Q, and there is a connection matrix between them. Is a connection matrix between them, which is nothing but the functional net matrix. Now, if you assume that the distance functions are smooth, in effect, they are smooth. I mean, the Dirichlet boundary, the Dirichlet energy of the distance function are nothing but one times the parameter of the surface. So it is bounded. And the same goes for Q. So if I write it as follows, it translates into optimizing for a low dimensional in a low dimensional. In a low-dimensional space of the matrix that you see over here. But still, this is still a challenge. Okay, so I translated a high-dimensional problem into a low-dimensional one. And you can actually solve it if you do things correct. And you are welcome to look at Aflalo and Dubrovina's spectral GMDS. So it was multi-dimensional scanning, the linear case, then the generalized one, and then the scale, and then the spectral one. And then the spectral one. Okay, now if I write the functional map as a system, it would look like that. The basic functional map of Ostyanikov, Mirela Berkhan, and the rest of the authors. What you do is you take the two surfaces. Now, rather than S and Q, I have X and Y, and you extract, hopefully, corresponding features. Now, what do I mean by corresponding features? Now, what do I mean by corresponding features? You assume you have a nose detector, an ear detector, or a head detector, and you have these feature functions that are defined on the surface. What you do next is you hit them with the spectral eigenfunctions, I mean the Fourier, the harmonics on X, that would give you these coefficients alpha. I mean, you have a lot of eigenfunctions and you have a lot of features, so you have to them. Features, so you have to then, I mean, you have a matrix of alphas. You do the same for y, and you get these betas. And in fact, the permutation of taking x to y is nothing but alpha divided by beta. I mean, beta minus one times alpha. Okay. And in order to make a proper permutation out of it, you have to multiply it by the eigenfunctions and then project it onto the set of permutation functions. Okay. First of all, we project it onto the space of Of probabilistic functions, and then by taking the maximum, you get the permutation of it. Hopefully, the question is: how to translate this functional idea into a neural network? So, to that end, what Litani and his colleagues did is said, look, let's take a standard feature that operates on surfaces and let's blend them differently. them differently using a neural network. So again, what Ritani suggested is extract, think about it as colors. You have your surfaces and you have red, green, blue, but it's not colors, but rather outputs of some feature detector. So you have 300 colors that describe X. Now, in order before you project them onto this low-dimensional spectral space, what you do is you blend them differently so that the projection onto the spectral space. The projection onto the spreadful space would have more meaning when you're trying to extract the correspondence between X and Y. Now, in order to optimize this network, what Vitani had to do is know how X corresponds to Y. So he had to have this training data in which he has two hands, for example, where he knows exactly which point in X corresponds to which point in Y, which is written here at the top. But what Oshvri had. What Oshmi Halimi thought is: wait, I don't want to supervise this learning. Can I resort to the previous idea of having the fact that intergeodesic distances between points need to be similar in order to train the network? And this is what she did. So what Oshri Halimi did is basically replace the penalty in Litani's framework into one that penalizes for the similarity between points in Y and those. In Y and those in X. So rather than having explicit matching between points in X and Y, what she does is optimizing for the following measure while training for the neural network. Okay, so again, the neural network is nothing but a blend of these short features. And I will not get into what the short features are because I don't have time, but you train it by optimizing for the permutation of points in one. Of points in Y to those on X. Okay, and the permutation is something that you can quantify because you have computed previously the intergeodesic distances. We got good results. Forget about the errors. And now let me jump into the almost conclusion of what I'm going to tell you. So remember that we started our story by the fact that I can look at the same curve having two ways of measuring distances on the curve. Let's adopt the same. Let's adopt this idea and take it to surfaces. So I have my same surface, and I'm using two different metric spaces, one way of measuring metric on the same surface. One of them is the scale invariant one. And the scale invariant one metric is nothing but multiplying the regular metric by the Gaussian curvature. And the other one is the usual one that would get when you're taking the Laplace-Montermani operator, would give me the regular spectrum. And the other one would give me the And the other one would give me the scaling variant spectrum. Now, taking the inner product between these two eigenfunctions would give me the self-functional map, the good old self-functional map. Okay, so if I'm taking these four horse and I'm computing this regular eigenfunction, a scan invariant one, the inner product between them can be presented as a two-dimensional table of numbers that I presented here as a picture, as an image. Presented here as a picture, as an image. Okay, so if I take these horses at the top and this lady at the bottom, and I do this exercise to the horses and the ladies, what you get are two different set of images. And for all ladies, you'd have more or less the same picture, and for all horses, you'd get more or less the same image. And here, I was cheating. Why was I cheating? I was cheating because we all know that the eigenfunctions are defined up to a sign. Okay, so there is design ambiguity for the eigenfunctions. Design ambiguity for the eigenfunctions. In fact, if I have two very similar eigenvalues, then we can have this idea of having this button subspace in which I have all possible rotations between eigenfunctions that would still be legitimate as being representative of this space. So I need to be really careful here, and you can be careful. Or, and by the way, this is what Ashuri was doing. She was able, each and every point here represent a shape in different pose. And here it was a dog in different pose. Uh, here it was a dog in different pose and a horse in different pose. And what she did is measure the discrepancy between these images and then plotting them using multi-dimensional scaling, for example. Okay, so you see that similar shapes are falling onto the same location in the plane. You can actually do it for more complicated examples. These are shapes from the files data set, and you can see that the same person in different poses are all landing more or less at the same location. Are all landing more or less at the same location? Okay, so the cell function on that was working. So let's revisit this idea of Oshvi. Let me just remind you again what she was doing. What she was doing is replacing the penalty by resorting to the intergeodesic distances. And she had these short features in order that were blend by the neural network. So the next question that came to mind is why should we use Is why should we use features at all? I mean, okay, we see that the spectral domain is somewhere, is a space that I can, that is a natural way of representing functions on the surface, but why should I use features at all? Can I train a neural network to get rid of these features and somehow train them to look at something more universal? Okay, and the idea was quite simple. In fact, this is what Amit Brafa was doing. Fact, this is what Amit Braha was doing. So, again, we would like to replace that part by resorting to something which is more universal, more general. And what Amit Braha was doing was saying the following. If you accept the fact that harmonics are a good space to look at, let's look at the scale invariant harmonics and the regular harmonics and try to train for the Train for the self-functional maps. So, again, what is going on here is taking the eigenfunctions of the regular Laplacian and the eigenfunction of the scaling variant Laplacian and is hitting them one with respect to the other, and then he's trying to find the permutation by minimizing the same penalty that Oshri was presented, was using in the previous measure. But now, what happens is that if I assume that X and Y are isometric or Are isometric or almost isometric, it turns out that there is a simple rotation or almost rotation function, or let's call it a narrow band unitary matrix that is connecting between these two harmonic spaces. Okay, so what he was doing now is training for this harmonic, for this unitary function in order to rotate the eigenfunctions, the scanning variant eigenfunction from x to those of y. Okay, so he was optimizing. Of y. Okay, so he was optimizing for this rotation matrix. And optimizing for this rotation matrix, let me show you how the so basically he was training, but at the end of the day, he was getting the neural network free method by training just a linear transformation. But he needs to train for this linear transformation for every two shapes. So, since the process of extracting the eigenspaces is not insensitive, you need to retrain it for any new shapes that you are feeding. Retrain it for any new shapes that you are feeding the system. Okay. So let me show you how the alignment of the eigenfunctions would look like if the, I don't remember if this is the fifth or the eighth eigenfunction, the scanning eigenfunctions of these two ladies looks like, then this is before training for R and this is after training for R. Okay, so after training for the right rotation, you'd get that this eigenfunction. That these eigenfunctions would correspond to one another compared to the previous one where they did. Okay, you can see that, for example, the length here is red, and here it is yellow, and here the same colors that define the numbers of the eigenfunctions render on the surface look like. Okay, and this is for a richer, I mean, if it works for one, it would work for the rest of them. So, this is for this person in two different poses, and this is for the ladies in these two different poses. For the ladies in these two different poses, etc. So you can see that it basically works for this rich variety of shapes that you're feeding the not network, but rather system with. This is more or less what I had to say. I mean, I have a lot more, but I see that if I would get into it, it would be much more than what I anticipated. So I would finish at this point. So thank you for your attention. I'll be happy to take questions. Happy to take questions. Thanks, Jerome. Thanks so much for the very excellent talk and also this very nice pictures. Any questions? These are pictures, by the way, that I drew myself. Oh, you drew by yourself? Oh, that's cool. Yeah, I mean, this one is inspired by Picasso, but the rest of them are actually. I do have a not exact question, but sort of clarification. So, well, when you mentioned you trend to solve the icon equation, so that means for a given surface, you need to rate train a network or you just train a universal solver to solve any given input of the surface? No, no, the solver is universal. I mean, once you train the network, it is fixed. It is fixed in the iconal case. Okay, so think about in the first marching, you have to solve this quadratic equation at each and every point. Okay, now this quadratic equation has many ifs. If the point has a larger value, then you don't put it into the stencil. And if it has a lower value, then you put it into the stencil. And in fact, if you look at the evolution of solving of iconod solvers, it was so slow because thinking of all these switches took a lot of time. If you think about Took a lot of time if you think about it as essentially non-oscillatory schemes. Okay, but now think that you can train for you don't know what is essentially innovative, so you don't know what are up to in the Gudonov type methods, but you do know that they have to satisfy this kind of causality property of you cannot go beyond your current value. I mean, there is this written rule that you know that you need to look only at points that are smaller than yourself. So you can train a network to do that. So, you can train a network to do that for you. I mean, you don't need to think of all these complicated switches by putting a sequence of fully connected networks, and each and every vertex knows its value, X, Y, Z, as well as the U value, the distance value at this point. You feed it into a network, and the network speeds out, knows how to do these switches because it has values inside in order to speed out the right solution. So, you can do So, you can do that. I mean, this is something that the networks works for does for you. And in fact, you can now argue: why did you think that you can go into a second-order approximation? And the reason that I didn't, I mean, I know that there is this theorem that allows you to go up to first-order approximation if you have shocks. But if you think of surfaces and of distances, they are mostly smooth. I mean, the number of shocks that you would get in the distance function are really rare. Distance function are really rare in most common cases. So let's just work out the smooth version as good as we can. And we got it. By the way, we couldn't pass the second order one, but I think that this is because Apple offered Moshe a lot of money and he moved there and he didn't continue for a PhD rather than anything fundamental. But this is yet to be seen. I mean, I suspect that this is the case. I see. I see. I see. Thanks. Thanks. See, I see. Thanks, thanks. Because I was thinking about like a sort of general PDE, right? So even like people have this way to solve like functions elliptic PDE and with change coefficient, constant coefficients. And that's actually not quite easy if you try a universal solver. In other words, I give you a new constant, then you sort of need to retrain the network. But in your case, because manifold change, the metric change, then that portion change. Now the portion changes. The iconic question actually changes. So, in essence. Well, look at what we are trying to do. What we are trying to do is fit a viscosity solution, if you like, for a very simple problem. What we need to do is look at all points where you have values and decide what would be the value of a specific point so that the gradient would be equal to one. This is it. I mean, it's a very simple. To one. This is it. I mean, it's a very simple equation. And this is what the local iconal update is doing. Now, it needs to be a little bit more clever than that because it cannot consider the support of points whose value is larger than the value itself. Okay? So it needs to be a little bit more clever. But it's not a universal PDE on a surface. I don't claim that we can do that. I mean, this would be obviously beyond what is claimed here. What is claimed here? This is a very simple algorithm, a very simple problem in which all you need to do now should be invariant to the way that you sample the surface. This is true. It should be invariant to the way that the surface is oriented in 3D. This is true. But remember that if you land at a point and you look at the k-nearest neighbors of the point, what you can do is you can always rotate them so that they are at the zero at the origin, and all the points are more or less oriented about the origin, and you can scale them to be more or less the same. And you can scale them to be more or less the same, and then the icon equation is not equal to one, but or always equal to one. So you can canonize that. And if you canonize that, then the network can digest this type of canonization. From my point of view, it is exactly the same concept that happens when you do convolution. In convolution, you take three by three nine numbers and you feed them into universal solar. And it works the same for each and every point in the image. After that, you make After that, you make the non-linear combination by fitting it into a regression layer where you have this fully connected and you pick up the parts that you like. Here it's the same. For each and every point, you do this combination. First of all, okay, so what we do is something similar to a point that we first of all lift it into a very high-dimensional space and then we max pull it in. And then we max fool it into some vector, and then we do this fully connected on this network. But it's basically the same, it's the same machinery that knows how to look at these four-dimensional numbers and extract the update for a specific point. Now, there is a problem there. There is a problem because I can train it only on, I don't know, on spheres and simple permutation of the plane. What would happen when I mean, what would happen? What would happen when I have something more complicated? I mean, when I have saddle points, for example. So if you don't have exact distances on saddle points, you would have a problem here. But in the limit, you can assume that as you take your saddle to the limit, I mean, when you sample it more and more and more, locally, it would be more or less like a plane. And then your approximator would do a good job there. So as long as your local geometry sort of has some sort of exhaustive search of the local geometry that Exhaustive search of the local geometry that should be okay. Otherwise, if you have a new kind of geometry, you in the training, you don't touch that geometry locally. Well, then you have to. Well, remember that what you're doing is you're sampling your surface. I mean, you get the sampling surface. Now, obviously, if you come to me with a really terrible sampling of the surface, I mean, everything is oriented, that the distances are large, then you can fail the network. But if you more or less sample the surface uniformly or any something which is Something which is repetitive in the sense, I mean, all surfaces that you are given, that you are feeding this network would have more or less the same way of sampling them, then the network could digest that. I see, I see. Thanks, thanks. Any more questions? We still have more time for questions. So, I have a question that's kind of more towards being able to use these same ideas. Being able to use these same ideas that you're replacing features by, say, function maps or things like that, but in maybe higher dimensions, because in dimension three, for example, the Riemannian tensor can be expressed just using the Ricci curvature. So if you're now thinking of like triangulations by tetrahedra and dimension three, and then like using a discrete version of Ricci curvature, then that would be a way of using these same ideas for. Of using these same ideas for, say, surfaces and shape analysis in dimension two and three, but just for these combinatorial three manifolds, say, made up of tetrahedra. I think that would be quite interesting. So kind of the question is, how flexible are these architectures? Can I just put any sort of function into these boxes? Because that could be done with these sort of Ricci curvatures or scalar curvatures and then. Or scalar curvatures, and then that would give me something. And then, if I know that those information, that data is enough to specify my manifold, then hopefully that'll work. Sure. So, let me tell you that the following. It's an excellent question. It's a huge problem. So, computing intergeodesic distances for three manifolds is a problem. It's a computational problem. In fact, we had very In fact, we had a very old paper with the Brunstein brothers in the Journal of Computational Physics in which we show how to do that. And over there, it was no longer quasi-linear, but we had to, because you have this problem of what happens when you have a three-manifold. So everything I presented here is for two-dimensional manifolds, okay? For two, I mean, for surfaces. Now, even there, the problem is not the trivial. If I would listen to If I would listen to what you're saying, and I think that what you're saying is that is there a way of translating the problem into a low-dimensional space in which you capture the structure of the surface of the manifold, be it Gaussian curvature, Dirici. And in fact, I think that Peter Olber has a nice paper where he shows that if you'd like to compare between rigid transformations of surfaces in 3D, then you can map them into a five. Then you can map them into a five-dimensional space or a six-dimensional space using some combination of the local curvatures. And still, it would be a two-manifold in this five-dimensional space, but you remove all rotations, translations, and if you think about isometry with all isometries. Can you do it in higher dimensional spaces? I think that you can, but it would be not trivial to compute, at least not from the point of view of. At least, not from the point of view of intergeodes. I mean, think about it. The Von Correct congestion was just proved a couple of years ago. Now, taking into something useful and without even thinking that it would be useful for it, probably for optics. Yeah, it could be useful for optics. Yeah, it could be a great challenge. I mean, I don't know. Yeah, no, I did kind of think about like, I didn't want to ask something like, could you just do this for like Richie Flow and then like. Richie flow, and then like have a network learn because that's obviously like super hard, but something, something just like for specific values of these curvatures, and then use that as the input. I think that that could be referring to the Ricci flow, my journey in this world of geometry started from the curvature flow, I mean the Grayson curvature flow. And if you think about it, it's not that, okay, it's fundamentally different, but it was a challenge. But it was a challenge, and then with the Osha-Sethian method, it was embedded into implicit surfaces and it was done. Well, you should know about it, and it was done in an implicit manner. So looking at things from an implicit way, in an implicit way, and if you think about it, networks are looking at our world in an implicit way, is an interesting way of computationally solving problems. It would not give you any bounds or anything like that, so you need to rely heavily on all your axiomatic. Rely heavily on all your axiomatic understanding of the world.