Workshop, as I've already seen and learned a lot, and try to put this somehow into context of what I'm doing. So I'm working on currently on, and for some time now, on hierarchical tensor approximation. And that is, you could say, some new version of tensor approximation, which was not so known in mathematics, but actually in physics, chemistry, it was well known since the nineties, so not that new. Not that mu, but compared to other tensor formats with former mu. And it's not complicated, it's very simple, it's linear algebra. So that's why we propose to use this, because it's trivial. And you will see why and what this means, trivial. And we've seen this already in some talks. What trivial mean and whatnot. And now come challenges. Because the basic energy was clear, like you develop LAPEC, but then you think what can I do with it? That becomes very contentious. Well then it becomes very complicated and I will um also come back uh to the problem of uh possibly two important methods. Okay, so I will introduce, I mean I will first tell you a little bit about the format, and then I will introduce the model problem so that we roughly know what we are thinking about. It's a continuous problem, so you have to discretize it. After discretization you end up with tensors and then you have to compress them because they're extremely high dimensional and it's too much data you can't And it's too much data, you can't really work with it. Help me do it in the Hierarchy Taka format. I tried to explain what this is, a little bit maybe on the blackboard. And you're supposed to just ask questions. So I'm happy to answer all questions because everything else is not so important. I mean, for me, it's important, but for you, maybe it's a little bit less important. Okay, that's each one of solvers are multiplied. So for me, because I know multi-plit, I think that's a good way to. And I think that's a good way to solve the systems if they stem, for example, from discontinuation of the PDD. And that's what we have in the model problem, of course, in a DPD. You could say a toy problem, very simple, very easy, but it gets complicated enough. And in particular, I have another model problem at hand, not such a nice pre-problem. It's essentially a Markov chain, and it's about problem. It's about probabilities and also probabilities should be and one of the favorable districts that my colleagues use is a KL one and that means negative or zero entries don't have any meaning at all. You can't process them. Okay, so that's why. Okay, so that's why I call these challenges because some things are easy and we think we have a way to solve this, but some things will help you. Okay, so first of all, of course, there's lots of possibilities in the past to get financial support in Germany. We have everything centralized, of course, for the whole state. And some programmes are these priority programs are nationwide. And this one, the latest one, it's a research center. It's a research center now in AH, Sparsity and Similar Structures, and everything you can imagine, machine learning and deep learning and stuff like that. Okay, so what is this format? Well, it's fairly straightforward, one of these tensor formats, and the easiest way to think about it, and I would propose that you keep in mind that if you have Tucker format, you can imagine the name. You can imagine the matrix with indices ij, and now you have a third index there. That's all. So, this is our third-order tensor, and you decompose it, and when you decompose, think about like for matrices, basically algebra, you look at all the rows, they form a subspace, the span of those rows, and that's one part you want to represent. You can do the same with the columns, with the rows, and here. The columns with the rows, and here in particular, also with tubes. So, in this third direction, all of these tubes, all of these vectors together, the span of them form a subspace. And that's, if we put these pictures here, that's what we mean by these line form matrices, but now this is the row span, the column span, and tube span. And all of them together can be combined any way you like with coffee. Combined any way you like with coefficients. Because you have three indices, i, i here, j here, and i here, because you have three indices, you have to connect all possibilities of these vectors. It's threefold, so you have a little tiny coefficient. Coefficient with three indices. And that's the column. That's the tunnel fold. You store these, and then the column. So two. Problem. So typically you would expect that these dimensionalities, so this guy here, this is rank 1, this is the rank 2, this is the rank 3, they could all be different, and you would expect them to be smaller than worst case. Worst case would be full-dimensionality, which in general would happen. If you draw them at random, there's nothing you can save. You have to store everything, so the core is the same size as the core. So, the core is the same size as the original tensor, it's completely useless. So, for random tensors, it doesn't make any sense. Okay. And all that the tagger or the hierarchical tagger format does is you restructure the core. You don't say, okay, this is dense. You start off with the same, the identical subspaces. Nothing has changed. These are identical. If you like, you can make them orthogonal bases if you like. So they're always linear independent. Always linear independent. If they were linear dependent, you couldn't use a take a smaller version, a smaller rank, or a smaller parameter. So, this is everything we do. We structure this little tiny column. And that doesn't make any sense in, for example, two dimensions. Of course, your Lumeric matrix can factor this into the left or right. You don't store it. It isn't there. For order 3 tensors, it is there. You can't get rid of it. You can't make it either. Get rid of it. You can't make it, I don't know. You can't multiply it to any side. It doesn't work. If you multiply it to these two, then you have here these huge faces, and that's too much storage. So you can't figure it in anywhere. But in high dimensions, this little guy, that's the dominating part of all the complexity. You can forget about all the rest. Therefore, this one has to be structured, and that's the torque that happens for it. Okay, how do you imagine this to be? This to be. There's one simple, straightforward explanation. It's very difficult to draw because it's geometry in more than three dimensions. I'll try my best. You have a core and now I symbolize just the indices. So I, J, L, P. And I group these indices. So previously they were all independent, just a coefficient for. Independent, just a coefficient for indices. And now I group them, let's say, in two parts. I just stack them together. Two indices together, and look what will happen. Maybe, if I form the matrix, with these two indices as rows and these two as columns, be careful, the matrix is now huge. Because I combine these with a squared size in rows and columns. When I separate them, And columns. But I separate them as a matrix. Maybe this matrix has a small rank, two, three or something. And that's what we insert here. So our core, I can't draw the picture, it's like a matrix by two dimensions times two dimensions. So that's what we do. So we restructure this, the core and the taco form. So if you remember the tucker formula, that's what we're doing here. Okay, it's a generalization of the CP format. That's an easy statement. Format. That's an easy statement because all tensor formats that I know, unless they are blocked or somehow strange-structured, are generalizations of the CP format. The CP is the easiest. That's the one where this core is actually completely diagonal. Okay. That's of course nice. But in general, you don't have this. And what we can do is we can do the standard basically in algebra. So we can project into subspaces, and that's it. How do you get best approximations? You don't. Best approximations, you don't. It's impossible, but you can get quasi-best. So, what's the problem? You will see this on a later slide. It's an intersection of low-ranked manifolds, and you have to project onto each of these manifolds. And every time you project, we do, of course, the best with respect to this manifold, but not with respect to the others. So, you have to add all these errors, and since it's orthogonal projections, you can wisely estimate. Orthogonal projections, you can wisely estimate. It's a quasi-best approximation and quasi-best with a constant that depends on the dimensionality. So if you're really keen on, I don't know, super-duper high dimensionality, that might be a bottleneck, but I don't know, I did this up to a dimension 1 million. Then you get into other technicalities like managing EBCs of very, very, very, very low okay. But in principle it's very low complexity. But in principle it's very low complexity. As you can see, important thing here, it's linearly dependent on the order of D of the tensor, linearly dependent on the sizes. This is now an abbreviation for the maximum of the both sizes. And this is the inherent rank. Here there were three. In our format, there are many more. And this is, you could take the Mexican, for example. And the exponent is three or four. Depends on whether you want to store that it's three. Whether you want to store, that is three, or you want to do singularity decompositions, that's four. And of course, there are slight changes in the format, so you can change one of these rings with an n. So you get a factor of n and save one of the k's. That doesn't make much sense for matrices, but for tensors, it does make sense because the rank will grow. And n can, in physics, for example, typically two. It's tiny and small, so you want to exchange one of the two. Exchange one of the two, but that's a technical detail. Okay, now what we do is we transfer, we have a simple algebra like a NAPEC, you can just work with matrices and vectors in this strange format, treat them as usual vectors, usual matrices, and then solve linear systems and so forth. So that's what we do. Okay, and as I told you, the dimensionality, that is only one of these factors, not so crucial anymore. Not so crucial anymore, but the range is crucial because that's what, if you have a random tensor, will grow as large as possible. And that means, like with matrices, the minimum of the product of the sizes that you use here for this fitting, that will be, of course, the rank that appears, because if it's random, you get this. Actually, you can also prove this. I mean, it was rather intuitively clear that the additional structure will not reduce the rate. Reduce the rank somehow? But the representation has no meaning. So we have seen a nice talk on blind source circulation, where these vectors that come out have a meaning, and there's even a reasonable meaning if two of them are identical. For matrices, this can't happen. They have to be independent. In this Tucker format, it can't happen. They have to be linear independent. For the CP format, they can even be identical. CP forward, they can even be identical. So it has a meaning that there are two times the same thing. It might be a signal, I don't know, to play a violin and some other instrument. I'm not very familiar with the instruments. But then you might see, for example, the pitch or I don't know where you can see the same pattern appearing twice. And nonetheless, you've got a circle. Here it doesn't have any meaning. So it's just subspace basis. Typically, we often. Spaces. Typically, we orthogonalize, but that's only because, for technical reasons, because we like to work with orthogonal vectors, we like to preserve norms. Okay, then it's good to have something orthogonal, but that's just made up for us. Otherwise, it's just the subspace that has a meaning, but it's too complicated to always talk about subspaces. Therefore, we fix these bases, which are all their bases, but they have no meaning. But they have no meaning. It's just one possibility to choose a representation system for the subspace. Okay. Now, what is my one problem? Just a diffusion. And so this is this one identity. This would be the Lajrasian. But with respect to x. So x is my spatial variable in two or three dimensions, typically, so something very, very tiny one dimensional here, just to simple square out. Simple square not. So the domain, the usual domain, something computable, I don't know, maybe even more than three, but few dimensions. And the other one is this parameter p, and that's arbitrary. It can be many, many, can have many forms. We have seen this already. In uncertainty quantification, you would start here with a random field. And then, because it's a random field, you want to write it in terms of independent variables. Write it in terms of independent variables, and then you introduce these parameters probably independently. In our case, it's much easier. I start right away with the independent uncoded equivalent p. And then I want to compute the solution independence of these parameters. Also, it could be on the right-hand side, but that's not crucial. Okay, just for completeness' sake, in order to have a simple theory, it's supposed to be essentially. That's supposed to be essentially bounded away from zero so that it's uniformly elliptical. Okay, that's the little technicality. And I switch immediately to the discrete case. So my parameter space, in your queue, this could be something continuous, and then you think about disputization. I skip the step, I say, okay, I have certain parameters. These could be conductivity values, for example, and here is a diffusion parameter, and I somehow have. And I somehow have a finite number of possible values that I think could be current. You can also think of it as some discretization, and then this index set corresponds to the width of the mesh that you use for such a discretization. It could be more complicated, maybe Q, typically it goes from minus to plus infinity. You have to be a little bit careful how you discretize to infinity. But that's nice. The parameter is an. The parameter is an index set or the parameter is an index set? The parameter P is just default 2. Q. These are indexes. Yeah, this is the index. This is the index set. The capital I is the index set. Okay, so it doesn't assume real values. It's part. Actually, it could be anything. It's just a numerical. Actually, it could be anything, it's just an enumeration. You can pick whatever you want. Yeah, it's just labeled, and of course, I typically take this as interval from 1 to nj. So there's no reason to pick something else. The meaning comes when you insert this as a p into your model problem, then whatever a however you define this shows dependency on your parameters. In our case, very simple. In our case, very simple. So, you can see this. This is roughly speaking after discretization. Now I do spatial discretization, X, finite elements, typically, but okay, if it's a superdomain, you can switch to anything else. Okay, so this is my system of equations. Colleagues of mine would prefer better basis, fresh basis, and they have other problems. They are easy in terms of solving this. They are easy in terms of solving the system. That's better. But many other problems, so I will not do this here. But since my parameter is a default tuple, and each of these has a certain cardinality, the product of all of them together, let's say they're all cardinality, and this little index has all of them together n to the power d possible combinations, and you can see this explodes quickly. Now. Now, if my parameter you can see here is nice, I end up with such an F independence. So you can see, this is what my model problem would look like. It's just independent pitches of these coefficients. So I have different diffusivity parameters. In the background, it's everywhere one, but for the Persian, and then on certain cookies, it's not one. It's not one, it varies. That's what's written here. So, this is my background in Glacier. And here, that's the modification, a scalar value. Now it's the real value, a real scalar value times your discretization on this patch. So, this is the nicest case you can think of, which is used in model reduction, for example, or hello, because then if you project from left and right, you project this one and a plus here, and you project these guys, and everything is tiny. And everything is tiny. So it's FI. Fits perfectly with the projection from the left and right. We don't need this essentially, but something is needed in terms of the structure, and we use this. It's complicated. And we've seen this preconditioner that I would also say is a good choice. So we do some averaging, or you take from the very beginning the A0. IA depends. So if I write down the cookie problem, So if I write down the cookie problem, my aim may be something unreasonable. You should take the average. But basically you could take something like uh this paper and the left plus. This could be a reasonable preconditioner. So it doesn't depend on the parameter, you just invert it. You have a favorite method for me, multi-grid method. So I set up the multi-grid method for, let's say, Laplacian. Whenever I want, I can use this. It's fixed. The fixed procedure, or you can think of it. The fixed procedure, or you can think of LU decomposition. If it's a sparse LU decomposition, you can nicely store it and then use it almost as an except solver, but an except solver for A0, so it's just a precondition for the true system. But you see the difficulties come in when this diffusion coefficient is not one, but either rather small or rather large. There, is not worth it. Okay. Okay, so that's what we want to do. We want to approximate this full map from the parameters to the solution. Maybe not the solution, but the quantity of interest, but that's for the sake of simplicity, assume we really want to compute this format. But that's just because we want to do it. In many applications, you don't need to. You have a quantity of interest, you do uncertainty quantification for this quantity of interest, and that's it. With Monte Carlo monthly, you have a Monte Carlo. With Monte Carlo, multi-like with Monte Carlo. But here, we are fancy, and we want to do more multi-general, we want to compute maybe an approximation with full length. As some similarity to model reduction, you also try, if you have a quantity of interest, you would want to want to try to project the whole system down to a much smaller version that you can efficiently evaluate. That's roughly what we are doing. That's roughly what we are doing. But in modern action, oftentimes you think of a rather core security or of a very small rank, then you can project down. And then it's slightly more involved here. So yes, we also have a surrogate. That's our analytical format. But you could think of this as a sparse matrix. You don't think of a sparse matrix as a model reduction. You just say, okay, I throw away the zeros. Why should I store every zero? Throw away the zeros, why should I store everywhere zero? And that's what we're doing here, but in terms of rare. And then you can do either post-processing, you can do reuse it for something else. Okay, that's what you might want to do. Let's think just about this representation. So if I write it down in this way, this means I compute the solution u for all parameters at the same time. Philosophically, that's Philosophically, that's easy. You just say, okay, I don't want to fix parameters at the beginning. I compute everything, and later I can just pick the parameter. I like and have the solution include. That's what's happening. Okay, I will not talk about why there should be low rank, because that's complicated. So for very, very simple situations, we can prove that these solution maps allow in principle for low rank representation. Prove for low-rank representation. But this is very, very tough. Even for very simple operators, it's nowhere easy to prove. What people do is exploit smoothness. But smoothness is not really perfectly suited in order to analyze lowering. So, what people do, they first analyze smoothness, then they say, ah, okay, it's smooth. So, we take these H mixed spaces, we have a sparse prediction. Spaces, you have a sparse bit representation. The sparse bit representation translates into a low-end tensor representation. Okay, but that's very strange. It's true, but you don't get any information of it because you use the smoothness that you don't need, and as a result, we have a complete overestimation of the necessary layer. Okay, so first of all, what does it really mean? Our representation. So, this is the total number of So, this is the total number of representation parameters I would call them. So, they are not truly degrees of freedom, but these are my representation parameters. And then you can see, okay, this is what I showed you at the very beginning. This is the complexity of these outer guys that you would also have in the CP format and the tanker format everywhere. And forget about this guy. This is what we use. This is what we use to store this core data sparse. And remember, this is default, data manageable, but here we have only one factor. So that's where something happens. Okay, a sketchy example, so don't look at it for too long. So the full tensor in 20 dimensions has, of course, a gigantic number of entries, but now comes the point. In our tensor format, much less, no surprise. Less, no surprise, but now what is really important. The CP tensor is much more data sparse. And in some applications, if you have few measurements and few data, factor of three is critical. So you want to take the data sparsest format that you can get, and then this gap is crucial. But for our computations, I want to use it in scientific computing to solve, for example, these parametric high-dimensional linear systems. I don't really care about. I don't really care about counting number of parameters. I'm counting CPU time. And in terms of CPU time, this one is trivial. You get an example. So there is nothing going on. And this one, this is magic. So you need an elaborate library. And we've seen, well, as of finding these decompositions is not true. I mean, there's a big machinery, you can use that, but it's in no way true. So, what is this formula? So, what is this for? You've seen the first stage. Now we need to know something about the core. But here, I start differently. I don't start with a core. So I start with a matrix. It's two-dimensional decomposition groups. And you should keep this in mind. I have decompose M in this form. Here I have my index I1, here my index I2, and this is contraction. And this is contraction over this index j. So this you could say is a shorthand notation for that. And now you leave out all indices. So this is the simple, this is the matrix. Because it's a low-rank representation of a matrix. A matrix would have two indices, I1 and I2, and I just decouple them. So that's my low-rank representation. So that's my low-reg representation. That's what people in physics, for example, use in order to represent these formats. This is the first one. And now, how do you generalize this to more than two indices? And one possibility, if you put more and more here, you could say, well, I use a contraction over several of them. That's what's written down here. I've now put the contraction index, the J down. The J down here and contract all of these factors all at once, all at the same time. So each one has its own row, column, tube, whatever index, I mean, contract. So this is the CP formula. And it's a non-trivial set. It's not even closed. There don't even exist best approximations always. Sometimes there is a best approximation. Sometimes there isn't. Okay, so that's a complicated thing. And you want to use And you want to use this complicated structure because you get this magic uniqueness. If you have uniqueness, then the vectors have a meaning, and that's what you want to explore. So artificially, what you do, you have a two-dimensional problem. Artificially, you try to make it three-dimensional by looking at different frequencies of something, then it's three-dimensional, and you have units. Okay, now what we do here is easy. I have I have four indices. I do the same trick from the very beginning. So, this contraction over J, and then I put these indices here, but it's now twofold indices, so I can continue. And that's what everyone got wrong. One idea would be I separate here further. So that would be this picture. I1, I2, I3, I4. And you would imagine: well, I separate this guy and this guy and continue. But then, what would happen? You have to represent this guy as this summation, but each of these here, each of these depends on j, so for every j you would have a further decomposition. Would have a further decomposition, and then for every L and every L and so forth, you get more and more and more decompositions. That's like a massive S V D. Take something eight-fold, decompose it into four-fold and four-fold, then you decompose further and further, but this quickly escalates and the complexity explodes. But this is not what we are doing here. We're doing something else. We take all of these vectors together, all of them, and represent all of them in this basis. All of them in this basis. So we pick, let's say, uniform basis for all of them. So this guy does not depend on j, this guy does not depend on j, but if I want to represent the jth of these vectors, the coefficients will depend on j, of course, because we have to say what does this guy look like. So these coefficients are not like a matrix case with two indices, but one further index, the third one that says it's this vector one we are putting. And the same I do for the right side one, and that's all. So, this is the whole formula. This is just this nestedness condition, so the hierarchical decomposition. And you don't understand much because now you get only a glimpse of one little tiny piece of the big picture where you see one decomposition, and you think, okay, this is trivial, this is a matrix decomposition, nothing is happening. But that's But that's the point. You do this recursively, of course. If you have one million BCs, you separate into 500K, 500K, and then go on. Okay. Implicitly, someone has thought of this tree. And that's not what I will tell you, how you get this tree, why this tree is there. I suppose it's God-given, or you know, well, it's a straightforward way how to decompose. Decompose. The topology can be chosen. Obviously, if you have ordering indices, how do you group them into two? Like for grids, you have geometry information, but here you have indirectly this rank information. You want the ranks to be small, but you don't typically know the small ones. Okay. But since this is always matrix decompositions, it's in fact closed. It's a closed set. So it's nice. Mass. Okay. Now, this picture is roughly this. So if you decompose the matrix, this is the tiny coefficient. You could make a diagonal if you wish. But this is the tiny coefficient matrix, and then you have the row and column spaces. Further decompose, and each further decomposition step you get these threefold guys from the previous slide, three indices. And then you decompose, decompose, decompose until you end here. Compose until you end here. And these are exactly those guys over here that you already had for the tacophone. So they're identical. These are the vectors from the tacophone. Okay, but you can go on. As you see, these are data spars. These are tiny. So this is a binary tree. This is the only part where you have to store only a tiny matrix, another little cube. Otherwise, it's a binary tree of these cubes. So you count how many cubes are there, and then you're not. Count how many cubes are there and then they don't. Okay, now a little surprise. You could say, I don't want to think about topology, forget about it. I will always take this tree. No matter what my application is, I don't even look at it. I always take this tree. So I split one index and one and one and one. So I don't do this in a nice balanced way, but completely unbalanced. Completely unbalanced. And the trick is, and that's why it's used on physics, you can rewrite this guy as follows. Such a long contraction and its matrices of size 1 by first ring, R1, R1 by R2, and so forth, and so forth. So it's lots of natural matrices. You could write it straightforward. Straightforward in this way. In the end, you want to have something scalar, so you multiply with a vector, give it the transpose. So this is a product of matrices. We count these as matrices as well, special ones. So this is a total of one entry. You just take the product of all. Therefore, it's called matrix product states. Because one entry is written in this way, now where do you insert these, the parameters from the very beginning? The parameters from the very beginning, everywhere exactly one. So these depend on the parameters I1, I2, I3, and so forth, and your ID. But you can make a picture of it that is stacked one after the other. But if you want to evaluate one of those guys, you pick one of these matrices for each component, multiply all of them together, and that's the attribute. So it's nice, you can implement it. That's why we would want to use candidates. Tensor, it's cheaper. Okay, again, the common number of entries, and maybe the literature is not so important, but it goes back a long time to the 90s. And you get nice surveys that explain many, many of these facets. Okay, so the basic idea is as follows. You do computations, like with matrices, you add two matrices, the ranks add, so the rank will increase and increase. You take a Hardenbook product of two matrices of low rank, the ranks will increase. Matrices of low rank, the ranks will multiply. When you do anything else, rank will increase. Okay, if you have a rank increase, you have to project it back. That's what we're doing here. You want to project it back to smaller rank. Since it's matrices, I told you it's an intersection of these matrix molecules, you can directly, theoretically, form a huge S V D, so you have gigantic matrices, you form a full S V D and that's it. That gives you your approximation. That gives you your approximation. The only trick is you can compute all SVDs for all matrications that ever appear in the complexity that are stated. That's the only trick. Theoretically, you do the full matrix if you like, but it's computationally feasible. Okay. And that gives you, since my set, this is my set. I didn't tell you what the tree is, the picture on the left. This is the rank. This is the rank. Depends many, many, many rank parameters. For every node in the tube, one rank parameter. And it's an intersection of these low-rank manifolds. It's an intersection I project. For each of them, in total, I add here in the intersection. And for every projection, it's not the best possible approximation. I have to add them all together. I take the experiment. And this number here is exactly the number of these interior nodes. Number of interior nodes, and you have to subtract a little bit because on top of the matrix, you don't have to project on a row and columns, because if you project on one of the two, already it's lower range. Matrix, you don't need to have two-sided projections to end up with a ring here. So here was the number. It actually fits nicely into memory. It's 1,000 dimensions, mode size 100, so it's 100. Both size 100, so it's 100 to the 1000, and it's not so small rank 25. I compute the whole SVTs, and it's in seconds. Okay, now this is my linear solver. Even if you don't know anything about smoothing multi-grid, this is your basic linear iterative scheme. You write it down. All you do is you insert tensors. Insert tensors. So everything now depends on the parameter. And for some instances, that's easy, like these vectors. This is what I described to you, how to represent it. You can represent the operator as well in this form. You can represent the right-hand side. It's an assumption. Right-hand side is complicated. You can't. If you have a representation, a nice compact tensor representation, you can do this whole iteration in the tensor form. So whenever you add two of them, it's trivially done. It's trivially done. Here, you need some, I don't know, smoothing some simple iterations. People use Richardson. Why Richardson? Because then you don't have to think about anything. You can just insert identity. It's the most simple. That as a severe drawback, this parameter that you need, I mean, I'm not talking about the solver, but just for smoothing, this parameter has to be incredibly small because you have all these different diffusions. Because you have all these different diffusivity coefficients, and you have to take the worst case, the smallest, and that makes it very, very bad as a smooth. You can improve it by taking the diagonal. That's the Jacobi smoother. But keep in mind, our diagonal has these zillions of entries, so you can't form an element once. You have to have a compact representation in tensor form, and that's done in exponential sums. Exponential sums. So the basic trick is always the follows. You have here one over such a linear combination. You can't handle it. You need a product form because you want to end up with such a structure, a nice product structure. That's the matching of the exponentials. You have a short summation of the exponentials. So this is your product form. And I mean, this is really used everywhere. Chemistry, physics, biology, mathematics, always. Mathematics, always the same trick. This is the only trick we have. So essentially, everything points down to the same trick. Okay. And you can analyze the convergence, well, I mean, as you used to. The only thing is that everything now depends on the parameter. You prove the smoothing approximation property for these parametric dependencies. And that was the first, this philosophy to understand what does it mean that you solve such. What does it mean that you solve such a parametric system if you really want to solve it for all parameters? This means the solver has to be working for all possible parameters at the same time. So if you take A0, maybe it will work if the parameters are not so extreme. In other cases, not. And you have to check that you have the smoothing property that you can ensure with this Jacobi smoother, and you have the approximation property, also pretty fat. Property, you can also prove that, for all parameters at the same time. So it has to be uniform in terms of the parametric dependence. And then you can prove it, and of course, yeah, you see, okay, multi-grid convergence textbook-like I would say. You can't see a difference. Why? Because it's the multi-grid method. It's just many of them at the same time, but since it's smoother and we have the Smoother, and we have the approximation property for all the parameters. Doesn't matter what you insert, you can solve for all of them at the same time. Okay, and now little d2, what happens here? I don't compute exact. I mean, previously I was very careful that my approximations are always very accurate. So I always use not machine precision, but something rather accurate. And here I virulate this extremely like a Extremely like a factor of 100k away. So I just throw away digits, but there's no tomorrow. And you can see you still retain the convergence only if you throw away almost everything, and you turn k to extremely small meaning. Then you lose the properties of a multi-grid method, and then you don't converge. But this is not the u, not the vector u that doesn't converge. This is really my precondition because my multi-grid method itself will deteriorate as expected. Deteriorate as expected. But it's not as expected, but it's so named. So up till here, you would say, well, this is still almost textbook-like ultimate conversion, so I would be happy with this. Okay, I have to be careful. I have to go over time. Okay, I'm not talking about the ranks and intermediate steps. It's complicated to see and analyze and the details of the complexity. Okay. Yeah, this is now what is complicated. How do we incorporate our solutions? Complicated. How do we incorporate all the smugglers? You can't do an ILU or something, you can't look at anything entry-wise. You can't do a Gaussian, you can't make upper triangular blocks because it destroys most of our structure. Okay. Now, in the last minute, just a glimpse. So, this is my little toy problem, collaboration with biologists. It's a It's an evolution of cancer. So these are actually parts of the DNA that get modified, but it's not tiny. It's already known parts that we know and can observe and have tests for. They accumulate and finally you get something that you can measure where the patient comes to the doctor. So at one point we see this condition. And the question is, how do we get that? So what was the way we got it? So, what was the way we got it? And there's transition probabilities that this mutation will appear. In our case, they appear and they stay forever for simplicity. So, this will appear. But if you have one mutation, maybe it's more likely that you get this other mutation as well. That's true. So, there's dependency between those. And we want to find out what are the dependencies. So, it's the inverse problem: find out dependencies for measurements, but measurements Measurements. These are patients, so it's very limited, and we can't really start an experiment from here. We start somewhere here, and we don't know where. Okay, now everything works fine, lower rank appears, all the method works fine, we do this in tensor format, it's a very high-dimensional problem. Okay, now what is the probability? So now we have a certain structure, and that is, in our case, positivity. Positivity, we can simplify it to non-negativity, and we want to preserve it. And we see in experiments that is helpful. It's not only that for some technical reasons it should be positive, and it's odd, so we're not happy with it, but it really is better if you impose such a condition. You get better approximations because you have more prior knowledge. Seems to be reasonable, completely true. And what people are doing is not representing this. Representing this non-negative, but instead the whole factorization. And for matrices, it's a tiny step. Here, the problem is if all your factors are positive, it's a severe limitation. And if you optimize, you can immediately see, I don't know, you take your interior point method, you optimize one of these guys at a time, not many parameters, you can do this. You keep all the others fixed and keep. Irras fixed and keep this positive. But that's actually a problem because what you would want to do, we actually provide to orthogonalize to the left and right. That's an essential step. Orthogonalization will completely destroy non-negativity, so it's forbidden. People do diagnose rescaling, it's allowed, but not very efficient. In particular, if you do the usual In particular, if you do the usual optimizations which work for matrices, once you go beyond dimension 20, the convergence diminishes. So you see it completely breaking down. So at high dimensions, that's what we want to do, dimension 100 or more. There is no convergence at all. You can't do this alternating optimization over one core at a time. And there is some way out. Some way out that is something like an orthogonalization that preserves positivity. And that's not trivial. And thumbs up, so I don't say anything about it. And this is probably what I asked, because this I find very difficult. We want to actually compute in other norms. And usually what we do in linear system sources, we take a use of use for these bases, reference bases, for example, with perfect bases and represent. Perfect basis and represent everything in a little L2 norm. Little L2 norm means, in our case, the fitting of cell. So if we have the perfect basis, we could use this norm and it would fit. Typically, we don't do it, we do finite animals. And then this is the wrong norm. And of course, you hear stories that catastrophes can happen. So far, they don't happen for other reasons. But that's a big problem. But that's, I don't know, it's a big change. I'm interested to hear. I'm interested to hear any ideas. And I'm not talking about the crazy ones, not Kaliel divergence, which is very difficult. But here, really, just a slight replacement without an outright typical matrix norms, two norms. Now keep in mind, the gap between the two norm and Freudian snowy is gigantic because we have this 100 to the 1000 as the size of the matrix. So the square. So the square root of that is still too much, so it's completely uncomfortable. Okay, some few references where you can read lots of details. If you keep that picture in mind and these pictures, then you've got it. Okay, thank you for your attention. So in your format, the parameters seem to be like pre-chosen and you just discretize your parameters. I mean in a typical UQ application, these parameters would have a distribution. And you can think of this as some parameters are more important than others. Is there a way to adapt your format to this? Somehow that you only zoom That you only zoom into the important parameters where your distribution is somehow concentrated. Okay, indirectly, this is done via these ranks. So you would like to split away these rather less relevant variables, and that means that the first ranks, starting left or right, will be very, very small, and only a few places you have increased ranks, for example. Okay, so you can do this for So, you can do this for the variables. So, leave out the unimportant variables. I don't leave them out? Not resolve them. But can you do this also for regions and the parameter space? Well, I don't know. Because regions, in 100,000 dimensions, regions are complicated to handle. That's why people use these simplicities to approximate it in high dimensions. C is to approximate that in high dimension, but that's a mantra will be business. What Peter Binev does, I don't know, it's too complicated because you have extremely long indices and then binary representations and no, that's fine. Okay, thanks. This is already off. Is this already off? Yeah, when it when the bell goes, it's telling you that it's. Oh, so the switch is off and you. It'll have to ten minutes. If it goes on, it's still on. You have to repress it when you otherwise it will switch off. I'm just doing the HTML. I guess probably can just look at. I guess I'll probably get this one. Thank you. Oh, yeah, of course.