Yes, so this is uh going to be a joint work uh with Nam Gavish. Nam is my former master's student, just submitted his thesis a few days ago. Yeah, so the setup is an high-dimensional estimation, parameter estimation problem. So we have a parameter space, theta, plotted here. Theta is a subspace, subset of Rd in this talk, although the results are more general. Talk, although the results are more general than this, and we have some loss function of which we measure estimation accuracy. For this talk, think about the loss function as a norm. Again, the results are more general, but all we will talk about here is norms. And there is a sample model. So, nature picks some value for data. And what we get to observe is an n-dimensional vector yn drawn according to Ya and drawn according to the corresponding distribution beta. This is a parametric family. We have a distribution for each data in the parameter class. And nature picks one of them. We see n observations from this p data. I do not assume the observations are IID, right? So it's n observations, but the model can be very complicated. And of course, what we want is to estimate. And of course, what we want is to estimate the parameter data. So there is a class of estimator data head that just maps any possible output for the observations to some point in this parameter space. And we're in a minimax setting. So what we want is the loss between data and our estimator to be as small as possible. So we care about the expected loss. Our expectations with respect to the measurements. And the measurements are uh drawn Measurements are drawn according to P theta. So this value depends on theta. And we don't know anything about theta except for the fact that it lives inside this parameter space. So we consider a worst case scenario. So we assume that nature chooses the worst data it can for our loss. And we're trying to come up with the best estimate or to minimize the worst case. So this is a minimax setup. And this is a very well-studied object. And in fact, in this case, And in fact, in this talk, we will talk about the proxy of this object, which is very similar in nine-dimensional setting, which is the back proxy, if you like. So, what we want is the probability of the loss exceeding some threshold, delta, to be very small. In particular, we want it to vanish with the dimension. So, we're basically asking what is the largest, sorry, what is the smallest delta that we can guarantee with flight probability. And again, we want this probability. And again, we want this probability to vanish uniformly over all deltas in a parameter class. And we try to come up with estimators that guarantee this. Think about this picture, right? So this is data, this is the ball, we try to circle around it, and we're happy as long as uh our estimator is inside this ball. Or in other words, uh if the estimator is not outside of the ball, just uh we're eventually we we're blind to We're trying to make go to zero. So, this is an I-dimensional setup where D and N both go to infinity simultaneously. And we are interested what motivated this work is involved non-IAD models. So, examples can include the spiked Wigner model that we have seen on Wednesday several times. So, for this particular one, we know more or less anything about the minimax risk. Anything about the minimax risk, but there are other problems like multi-reference alignment or the minimax risk is not known because our mixture model also we do not know that much about the minimax risk if we're trying to estimate the centers. Our results also applied to discrete product distribution like stochastic block model. So in in general what we're trying to come up with here is sort of a universal estimate or Universal estimate or a unified analysis that can be applied to all of these models, which are in general quite difficult to analyze. Every time we need to analyze the minimax error for one of these problems, we have to come up with some new techniques. So we don't want this. We want to develop a unified framework for all of these problems and more. So of course the first universal estimator anyone would think about is the maximum likelihood estimator. What can be Estimator and what can be state than this. And we know very good things about the maximum likelihood estimator, it is consistent and efficient and a lot of other stuff. But this is for fixed D and n going to infinity. So if we're in the regime where we know we have enough measurements such that there is already very small tasks at is close to the truth at t and we just care about how fast it decreases with n uh this is great, but this is not. Uh this is great, but this is not the regime we care about. Mostly in this problem, what you want is to understand uh what are the parameters for which the error becomes non-affrequent, but you can do better than random guesses. So this is very far from the small error regime of grammar bound and all of this. And moreover, often it is uh very hard to analyze to analyze the maximum likelihood estimator. Just yesterday Aria gave a talk about the mistivation problems. About an estimation problem, some logistic regression model, and you mentioned that they wanted to analyze maximum likelihood estimation, but it's virtually impossible. So, this is not good enough, usually. So, our objective is to develop a unified information-theoretic framework for upper bounding this max pack or probability. So, it's important to notice that what we want is upper bounds, achievable skills. Achievable schemes. But nevertheless, just to demonstrate what I'm looking for, I will spend the next two minutes or so about talking about lower bounds. Why? Because for lower bounding the minimax risk, we have a very nice unified information theoretic framework, sort of an easy recipe we can apply to any problem. This recipe is described in detail in the book of Poliansky and Wu. It is very simple. We want the minimax. It is very simple. We want the minimax risk, right? The minimax risk is always lower bounded by the base risk for any prior applications. Any prior is easier than not assuming anything. So we set any prior we like on theta, say pi. And then we have this Markov triplet. We have theta, yn, and theta, f, which is our estimator. So theta is run according to pi. And we think of the relation between theta to yn as some channel, right? What is this channel? It takes as input theta. It takes as input theta and it outputs vector yn drawn according to p theta. So it will be convenient to denote p theta as p of y n given theta, just to emphasize that this is a channel for us. Right? So let's examine the mutual information between theta and theta hat. So it can be lower bounded by the rate distortion function. This really follows by definition of the rate distortion function, right? The rate distortion function. Function, record the rate distortion function for the prior theta and the loss L. Why? What is the rate distortion function? It's the largest lower bound on the mutual information. If we know that theta and theta star have expected loss smaller than some delta. So this is easy. Everything in this slide is easy. The neutral information between theta and theta star can be upper bounded using the DPI, right? So it's smaller than the information. So it's smaller than the information between delta and y. And this we can upper bound by the capacity of the channel from theta to y. So what's so nice about this bound? So it completely decouples the two things that we have in the estimation problem. When we need to derive bound for an estimation problem, there are two things we need to look at. First is what is the parameter space and how complicated is it to describe it according to our laws. It according to our laws. So the rate distortion function takes care of this. It sort of measures how hard it is to be close to theta, to describe it and estimate what is close to theta under our laws. And we also have the question of how informative our measurements are. When we see Yn, we need to learn a lot about data. So this is captured by the capacity of this channel. Of this channel. But it's completely decoupled. What I need to know about the channel is only what its capacity is, how much information it conveys about the time. And what I need to know about the loss in the parameter space is only captured here. But what I do not measure here is how related the channel law is to the loss I care about. Right, so this bound is tight in many cases if you choose pi. In many cases, if you choose pi properly, but it cannot be tight in general, you cannot prove a matching upper bound just because you completely ignore the relation between what you care about, your loss, and the channel low. So, as a simple example, consider the case where theta is the unit sphere. And you see the vector on the unit sphere theta plus Gaussian noise. So, if the loss was L2 norm, everything is nice. Los was L2 norm, everything is nice, the channel is completely matched to the loss I care about, right? For Gaussian distribution, all the information is encoded in the squared distance from y n and theta. So this is good. In this case, this kind of problem will work perfectly. But assume what I want to know about theta is something completely different. I'm interested in the smallest LSPs in each coordinate of theta. So this is completely covered by noise. I will not learn anything about. By noise. I will not learn anything about these LSPs just from this noisy measurement. But this pound will not capture it. The capacity is largely. I can get to small distortion. And the problem here is that this decoupling. It ignores the fact that the channel does not give me information about what I want. Alright, so this is sort of what we're trying to tackle in this work. We're trying to prove upper bounds. So we need some quantities that we replace. Need some quantity that will replace capacity that is sensitive to the dependence of the channel to the loss function. Now, I mentioned there's been quite a lot of prior work on coming up with sort of matching lower bounds for this, I mean it was not exactly trying to match this, but sort of following this information theoretic paradigm. But in those prior works, this In those prior works, this problem of coupling between the channel and the loss that I was talking about was sort of bypassed. How was it bypassed? So, all these works, their loss function was some divergence between the true distribution, distribution for the true parameter and the distribution that corresponds to the output parameter. In some of these problems, you don't even have a p-teta star. You just try to come up with a distribution that is close to p theta, like in the young pair. To be dead, like in the young peril case. But in general, yes, once this is your loss function, it's always sort of matched to the channel because what you measure here is how sensitive the channel is to your loss. Right, so but I will mention that this work of Birge and Birge and Lacam, so a lot of the ideas we use in our derivation are really borrowed, copied, inspired from them. Something okay, so I mentioned that the main problem is replacing capacity with something that uh sort of describes the dependency of the channel to L. So the way we characterize this sensitivity is using a mismatched binary hypothesis testing problem. And I will describe, so now I will describe this function will come up in order to describe this sensitivity, and after I Sensitivity, and after I give you this function, I can describe our main results. So, this is the picture we need to have in mind in order to define this binary hypothesis testing error exponent. So, our parameter space is in Rd, our loss is a norm, the norm of the difference between theta and theta head. And throughout this talk, we also assume that theta is bounded, it has some bounded diameter, but this diameter can be as large as double, almost double. Can be as large as double, almost double exponential in the dimension. So it's a technicality we need for the proofs, but as engineers, we understand that this is not a true limitation. Never deal with spaces, theta that are larger than this. So fix some tau greater than zero and let theta theta n and theta f be three points in our parameter space, such that the distance from such that the distance from theta to theta n is at most at most tau. And the distance from theta to theta f is at least delta. Right? So theta n is for theta near. It is near theta. Theta f is theta phar. It is far from theta. And the measurements are drawn according to p theta. So you would expect, if the channel is somehow matched to your loss function, you would expect that the likelihood of near That the likelihood of a near point will be much larger than the likelihood of a far point. And therefore, we define, okay, we look at the probability under theta that the likelihood of theta f exceeds the likelihood of theta f. And then we take the logarithm and put it back normalized by minus one over this, so that it will look like an error exponent. exponent and then so so we have this expression for any triplet theta theta f and theta n but then we minimize it over all triplets of points that satisfies this condition that the distance to theta n is at most tau and the distance to theta f is at least delta right and what the function we get is what we call the binary hypothesis testing Gauer exponent for tau and delta right so this is the most important Is the most important function for our results. And it turns out that you need to look at this function and subtract from it log delta over tau. So this is called the share error exponent for reasons that will become clear very soon. And you are looking for the first delta for which this share error exponent is always positive. So you're looking for a delta such that for any larger delta, the exponent is positive. Their exponent is positive. So we call this the critical loss for tau because we did the tau fixed here. And then we can minimize over tau and we get delta star. This is just the critical loss. And our result is that there exists an estimator for which the probability of the loss exceeding this critical loss vanishes uniformly, exponentially fast that I mentioned. That I mentioned. Okay, so really what you need is to characterize this delta star. This is sort of our results are the form delta star is less than something for a particular problem. This guarantees that you can get to this error. And in order to do this, what you have to do is lower bound this binary hypothesis testing error exponent and then choose it out. Yes, question. So I mean where is the deep wasn't added was. No, so if it's okay, it's it's a matter of of uh how I chose to define things. So the error exponent I normalized it by D. Why? Because the complexity of the parameter space under so the parameter space is in Rd and it's a norm loss, right? So it's complexity, this function log delta over tau. Actually, right, I need e to the d log delta over tau points in order to cover. Delta over tau points in order to cover this space well. And as long as my rock quad beats this number, I'm good. And you'll understand better in a few slides. But these, the right scaling here, of course, this also grows with n in normal models, right? They have more measurements, they have more information. But the right normalization is by d. It might depend on n, but it's okay. I just want it to grow faster than this function. Okay, so uh. Okay, so this is the result. Let me tell you now how we prove it. So it's a positive result, right? So what I need to do is I need to show you an estimator that achieves the bound I just showed on the excess cost, right? So our estimator is something we call the quantized maximum likelihood estimator. It's a very simple estimator. So we start by picking some. Start by picking some discrete net, some tau cover of theta, which we call C. So what is this tau cover? It's just a discrete set of points such that for any theta in my parameter space, there is some point that is at most tau away from it. So it's a constellation with covering radius and tau in our language. And our estimator is just computing the likelihood of The likelihood for each one of these candidates and outputs the one with the maximal value. So if tau is going to zero, this approaches the standard maximum likelihood estimator. But it turns out that it is useful to analyze for larger tau, for tiles that do not vanish. So yeah, it's just a maximum likelihood estimator on a discrete set. That's it. And how do we analyze the error probability? Well, so theta is chosen by nature. We have no control on this. And then we open a ball around this theta. This ball radius delta around this theta. And as long as the maximum light field estimator is here inside this ball, one of the black points inside this ball where the error is small enough, and the candidates that cause And the candidates that cause their events are those red candidates, right? So, what we want to make sure in the analysis is that none of these points meet. Those are the maximum electron estimators. Okay, so those are disjoint events, right? So we just need to sum the probability of any one of these far candidates being the winner. Right? Now, let me first describe a bed attempt for Let me first describe a bed attempt for upper bounding this. So I can just count the number of far candidates, or at most number of the size of C in Z. And then for each one of the candidates, how do I bound the probability? So this candidate is a distance at least delta from the true parameter. But I know that there exists some theta near that is close to the true parameter. That is close to the true parameter because this is a tau cover. So there must be some point theta near with distance at most tau from the actual parameter theta. So in order for this theta f to be the maximum likelihood estimator, it needs to be the one with the highest likelihood. In particular, it must have likelihood greater than theta n, because theta n is theta n is closer, right? So what must happen for such a candidate to be the maximum. Such a candidate to be the maximum likelihood is that this event should occur. The event, the error in the mismatched binary hypothesis testing problem. The likelihood of tetf needs to exceed the likelihood of tetan, and I've bounded this probability universally over all treatments that satisfy these distances constraints, right? So I know that their probability for the entire probability for the entire code book, the probability that my estimator is more than delta far from theta is bounded by the number of candidates times e to the minus this 0 exponent. So I get a bound on the operating. What is the problem? The problem is that if I'm interested in having my parameter space almost all of Rd, or some set with a very large diameter, then in order to cover it, I need a huge number. You need a huge number of points, right? A grand number of points. So this term will always win eventually. So this is a useless, so pretty useless bump. In order to get something useful, I need a more delicate analysis. So what I've not explored yet in this analysis is the fact that I bounded the probability of each of these points to be the winner as if their distance is delta from delta. But this point From theta, but this point maybe is very close. This is distance delta from theta. But this point is much greater distance. So it should probably have a much greater error exponent as well. And I need to capture this. So to do this, we do what we call a shell analysis. So we partition, these red points are all the points we need to worry about, and we partition them according to spheres. And the spheres with growing ratty. Routing. And then we upper bound the probability inside each sphere by itself. So for each sphere, we upper bound the probability that the winner came from the sphere. And then we sum overall spheres. So how do I upper bound the probability of a particular shell contributing the not the sphere but the shell, sorry, or its shell to upper bound. So how do I do this? Well, not so. How do I do this? How do I control the probability that someone from this shell is the winner? Okay, so for any candidate in this shell, I know that their exponent is at least E of h t in tau comma delta i, right? So now I know this point is at least delta i away from my parameter, so where it's going to room. But I also need to control the number of candidates within this shell. And this turns out Within this shell. And this turns out to be not so trivial because, okay, so the easy part is a first upper bound, the number of candidates in a shared by the number of candidates in the full ball. We know that everything happens on the boundary, so we don't lose anything important here. But the problem is that I don't know where this shell is centered. So it can be here, it can be here, it can be where the shell is. The ball is around theta, which nature chose. Is around data which nature chose, right? And I want a uniform control over all of them. So what I need to control is the intersection of my discrete set with ball of radius delta i, or delta i plus 1, maximized over all possible centers. It's convenient to take logarithm and normalize by 1 over d. So we call this function the net correlation function of the code, the constellation c. And it's defined for any. Constellation C. And it's defined for any R, right? I need the same constellation to give a small net population for any radius and any center in space. Okay, so how do we bound this? So we prove this theorem. It says the following. So if the dimension is more than twenty-five, theta is a subset of Rd, our loss is norm loss for some arbitrary norm. Is norm loss for some arbitrary norm, doesn't matter which one. Then there exists some tau cover, actually a lattice tau cover of theta, for which this density function or the net population function is upper bounded by log delta over tau plus three log d over d plus some constant over one over d simultaneously for all tau greater than the for all delta greater than tau. So in other words, I can find the lattice. I can find a lattice with the properties with the following properties. It is a tau cover. And whatever ball I open with radius greater than tau, in whatever location, the number of points I catch inside this ball, the log number of points normalized over one over t, is uppermost by this number. But this is really tight. It's very easy to see from volume considerations for any tau cover. For any tau cover, this number is at least log delta over tau. So the only slackness here, I mean, it's a big open question in Lattice theory whether this really can be improved to one. But a part of this is really clear. And the proof is basically a very simple corollary of something I've proved with and weiss recently about uniform lattice cover. And if you don't care about lattices, if you're fine with any cover, If you're fine with any cover and you don't want to have this for any delta but just for some finite number of delta, you can prove this using a result by Erdos and Rogers as well. Okay, so now we can control the number of points here. So we know it's at most log delta i plus 1 over tau. We have this error sponge. So any shell contributes at most e to the minus this number to their probability. And this denominator. And if the number of shells is sub-exponential, right, so the contribution of the number of shells will not be important to this exponent. So really, the error probability is controlled by the minimum overall shares. Right, so this gives us basically our theorem that uh there exists some uh okay, but the term is this, take take C to be some tau tau cover with good density. We know from the term I showed this From the term I showed the previous slides that such C exists, and do the quantized maximum likelihood estimator on this for this C. And choose the shell radi to grow exponentially, but very small exponentially, such that we can also, with a sub-exponential number of shells, we can get a super-exponential diameter. But log delta i plus 1 over tau is. And log delta i plus 1 over tau is very close to log delta i over tau. So the shells are more, their density grows very slow. So if you do this, their probability is bounded by this expression, and we can further relax it and minimize not with respect to the discrete shell value we chose, but with respect to any delta. Right. Now we don't care about the error exponent, right? We just want small loss. We we're not in this regime where we want We're not in this regime, we want the loss to actually decrease exponentially. What we care about is what is the smallest delta I can plug in here such that this goes to zero, right? So what delta am I looking for? I'm looking for the delta that balances the term, right? Where is it zero? And this is very similar already to what we had in the lower bound. Remember, in the lower bound, we needed to find delta that balances the rate distortion function of delta and capacity. So this term. Capacity. So, this term, this is sort of the rate distortion function. All rate distortion functions for norm loss look like this. So, this plays the role of the rate distortion function. We also got it from covering it, so it's not coincident. But instead of capacity, we have this term, this binary mismatch types testing error exponent, which captures not only the power of the channel, but the power of the channel in discriminating between. In discriminating between far candidates and close candidates, which is what we enabled, right? So we got something pretty similar in spirit to the lower bound, but something that is actually an upper bound. Okay, so this I've already shown, right? So what you need to do in order to apply this bound is to compute or lower bound this binary quantity stress function and choose the tau. So tau, the size of our cover. Our cover, it's not a real parameter of the problem, right? It's something we invented for the analysis. So you choose the tile that gives the tightest results. Okay, so let me show a few examples for how we use this and what it gives. So let's consider the class of this following class of problems. So our loss is squared loss. We have a transformation g from theta to Rn. To Rn. And what we observe, or P theta, is G of theta plus Gaussian noise. Is this for me or is it delayed with very quick about it, right? So the point here is when everything is Gaussian, right, then this binary hypothesis has to become a pure geometrical problem, right? But so this is an exact expression. So this is an exact expression for the binary hypothesis testing error exponent for this kind of problem, but the role G plays here is very important. So it's not just Gaussian location model. You can capture very interesting problems with this model. So I will just say that for Gaussian location model we recover the known bounds just as a sanity check. For spi spiked Wigner model we can why? Why? Why didn't we? Yes, so for Spike Twigner model, you can take this G to be like the elements in the matrix theta times theta transpose. And it's maybe a bit hard to compute this optimization for this G, but you can do it, and it's only elementary. And it requires some algebra, but nothing fancy. And you recover the optimal results up to constants. So these are some. Up to constants. So these are things you knew how to do before this work. It's nice, but not that interesting. But we do have new results for the multi-reference alignment. It's a very nice problem. So let me spend one minute explaining what the problem is. So in this problem, there is some vector theta in R D. And what you get to observe is cyclically shifted versions of this theta corrupted by noise. And By noise. And the cyclic shifts are random, so you don't know in how many locations it was shifted, and you don't get this information. So when you view this, you have M measurements of this point. So if there was no noise, this was how your measurements would look like. If there is a little bit of noise, those are your measurements. And with a little bit of noise, it's not a problem because you can just do pairwise alignment for each measurement. You know exactly what is the shift between this one and this one. The shift between this one and this one. It's very easy. But sometimes you have a lot of noise, such that you cannot do pairwise alignment, but still you can solve this problem. You can still estimate very, very well. So what we show, so we try to characterize the minimax error for this problem. And what we show basically is that you can get up to this loss, what vanishing error. And what's interesting about this loss, or Mir is the number of measurements. So m here is the number of measurements. So as long as this number of measurements is less than d over log d, then what you get here is a constant. And then the expression here is the same as Gaussian location model. So for Gaussian location model, there will decrease as a sigma square v over number of measurements. So this is what you would get if there were no cyclic shifts at all. And the result is that if the number of measurements you have is not so large, this You have is not so large. This model is as easy as Gaussian location model. I have to be careful when I say easy because this is a workshop about algorithms, so information theoretically easy, not algorithmically. Right, so this is a new result. And I'm out of time, so I will stop. I think