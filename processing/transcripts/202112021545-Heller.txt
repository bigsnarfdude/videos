Okay, so I'm going to talk to you all about trustworthy machine learning in medicine and the role of uncertainty. All right, so this talk is really divided up into three parts. The first part is I'm talking about sepsis wash. Each of the three parts is a different. Watch. Each of the three parts is a different piece of work that I have. The first is sepsis watch. So that looks at how we can integrate trustworthy AI systems into medical workflows. So we developed a sepsis detection system that's been deployed to the emergency departments of Duke University Hospital. The second is how does uncertainty impact demographic fairness, right? So we look at our work in looking at uncertainty. In looking at uncertainty in electronic health records for the purpose of mortality predictions. And then the last is how can we improve the trustworthiness of algorithms that we deploy. So we look at some work on underspecification and just sort of being able to test for do we expect that this algorithm or the system will transfer well to another domain or not or to another demographic group or not. Okay, so I will start with our success detection system. Start with our sepsis detection system. What is sepsis? It's a life-threatening complication from infection. There are over 750,000 new cases of sepsis every year in the United States. It's typically a hospital acquired infection, right? So you typically get it because somebody inserts like a line in your body in the hospital and it has the infection on it. And it comes with a very high mortality rate. So typically, Very high mortality rate, so typically about 40% of people. What they were using to try to detect this at Duke University Hospital prior to us coming in was something called the news score, which I have which I've put here in this table. And so one of the problems with this new score, which I'll talk about more later, is that it was false alerting a lot. They've actually stopped using it because it was false alarming so much that the nurses like kind of just turned it off, like whenever they were around. And so it wasn't effective. But it was also more generally a score for decompensation, looking at decompensation of the patient, not just success. So the way that it works is like this. On every row, you have a particular vital or lab. Have a particular vital or lab. And in every column, there's a number of points. And basically, you sum across the rows. And if the patient exceeds a threshold, like six, you say that they're at risk for decompensation, right? And so this score has worked. It's actually worked differentially well in the UK and the US. So was working better in the UK. They've ported it to places in the US for reasons that people don't understand. It doesn't work as well in the US. But they're also like. But there are also like some like weirdnesses about this particular scroller system. So, like if you look at systolic blood pressure, if you have a systolic blood pressure of 219, like zero points, you're totally fine. If you have a systolic blood pressure of 220, like three points, like, oh my God, you're going to die. Okay, so we looked at the score and we were like, okay, we can probably do better than this score. And that was really our goal for sepsis in particular. Particular. And we worked with, and you will see me be a large proponent of collaborations with all kinds of people. But here we worked a lot with infectious disease specialists who were at Duke University Hospital. Okay, so what's our problem? So our goal is to be able to detect sepsis. And so in a sense, that's like a binary classification problem. You're likely to have sepsis or you're not. Or you're not. But the place that we start out is from time series data that's coming to the electronic health record system. And so if you look at this is a particular patient who's coming to Duke University Hospital. If you look at the x-axis, this is that patient over time. And if you look at the y-axis, it's just a whole bunch of different LAPS and vitals that were collected on that patient over time while they were in the hospital. And you can notice if you look at the frequency of collection that The frequency of collection that you can mark out different transitions of that patient within the hospital system pretty easily. So you can see here where they move to the ICU. So it's our goal to get from this set of irregularly sampled time series data to something that's going to classify whether or not you're likely to be septic. And so that's a system we really look to develop. I probably won't spend as much time on this as I should, but this is a system we do. This is what I should, but this is a system we did, in fact, develop. So, in order to model the irregular time series, basically, okay, so if you have a classifier and you want to make a binary classification, like somebody over time is septic, is likely to be septic or not, what you need are inputs into that classification system. And generally, if you're going to use something like a recurrent neural network, you need them to be evenly spaced. But like our hospital data is not evenly spaced, right? Data is not evenly spaced, right? It's a regular in time. So, what we did is we did smoothing, which was by guessing process. And so we could say, okay, we're going to use our guessing process to do an estimate of what that lab or vital is like at all possible times. And so we can always get an estimate at an evenly spaced interval. And we use that estimate at an evenly spaced interval as input to our classification system. As input to our classification system, and then we use that classification system in order to make the prediction about whether or not somebody is likely to be septic. And that's really the entire idea behind this class, this sepsis classification system. And so we can do end-to-end learning of the guessing process combined with the classifier. We use a whole bunch of tricks in order to be able to do it, which I won't go into details about largely because this was like several years ago and I'm not sure I remember. Several years ago, and I'm not sure I remember. But, like, the guessing process combined with a classifier, end-to-end learning, lots of tricks in order to make it run in real time. And then we did some things that are specific to the medical setting that we were in. So when we went to compute the mean of the Gaussian process, we would look at something like, okay, so say you're a patient, you don't have a fever. Why don't you have a fever? It could be that you don't have a fever because you're not sick. Don't have a fever because you're not sick, right? But it could also be that you don't have a fever because I just give you a fever reducer. And so, those are two very different things. And we want to be able to take those two different things into account when we actually go to classify whether or not somebody is, somebody is sick, somebody has substance, right? So, what we do is when we compute the mean of the gaussian process, we subtract off the time it's been between now and when they were last given a particular kind of medication when we look at all the lots and vitals that were that were. And vitals that we're smoothing over. We also smooth over not just the patient over time, right, but we smooth over various patients that we see, right? So it could be that for a particular patient, we never saw a lab or vital, but we can still estimate what a value might be for that lab or vital by saying you look a lot like this other patient who we've seen. This other patient's value was this, so we estimate that your value might be this. Be this. And then for the classification system, user or neural network. Okay, so this is a particular patient who's coming to Duke University Hospital, real patient. The patient came in for a lung surgery. You at the top in the dark black is the risk prediction that our system is making. So we think it's a particular likelihood that you have success. Each of the blue lines is. Each of the blue lines is just an interpolation of values of different labs and vitals. So you can see temperature, pulse, oxygen saturation, respiration, white blood cell calcreatine. And so we can look at these all over time. Okay, so the patient comes into the hospital. So the blue dot means that they were given antibiotics. It's pretty common to be given antibiotics in the runoff to a surgery. The red cross means Um, the red cross means that that's where the surgery took place, so the surgery they were scheduled to have. And you can see they come out of surgery, and almost immediately, we see some funniness with respect to some of their vitals, right? Their oxygen saturation rate continues to go down, their temperature continues to go up, the person starts decompensating, and they're admitted reasonably quickly after surgery into the ICU. Into the ICU at Duke University Hospital. At this point, our algorithm thinks that it's pretty likely that this is a sepsis patient. The green cross means that they were given a lactate lab. So this is a particular lab that can test for the likelihood of sepsis in the person. It correlates quite highly. And the person came out high, which again is the correlation, the high correlation with being a sepsis patient. Correlation with being a sepsis patient. At this point, our algorithm thinks it's very likely that this is the sepsis patient. Again, you see strange values continue, the person continues to be compensated. They are eventually given more antibiotics by their clinician and they're eventually diagnosed as being septic at the vertical red line. And then at the purple cross, the sepsis infection is found. So this is basically like they find evidence of the sepsis infection. Evidence of the sepsis infection in your blood. Okay, so what is it that we can see here? We can see that it's basically 17 hours between when our algorithm thinks it's likely that this patient is septic and when the person starts being administered antibiotics by their clinical staff. And it's 36 hours until they're actually diagnosed as being septic. And so even 17 hours going back is a very long time in the life of a septic. A very long time in the life of a sepsis patient, right? Again, this is a disease with a very high mortality rate. And so, we would really like all the tools we can possibly have to try to cut down on the amount of time before something like this patient is administered antibiotics to try to clear out that infection. Okay, so that's one patient. We ran this on 52, we trained on 52,000 patients at Duke University Hospital. University Hospital. So, this is all the sepsis patients from 18 months. And we tested on 14,000 patients. So, this is the further six months worth of data. We used 31 longitudinal variables, so that's six vitals of 25 labs. And then there were other covariates and medication classes that we used as well. And what you can see, so I have other plots. This is just one, but like, is that when we compare to But, like, is that when we compare to lots of other algorithms, our multivariate gasing process RNN system tends to do better in terms of how likely it is to pick up a sepsis infection. So the multivariate gasin' process RNN, which I described to you, is the blue line. And then we compare it to like a raw RNN, logistic regression, other kinds of things that the hospital does set scores and simplistic scores that the hospital was using, like the new score, which I explained to you. Using like the new score, which I explained to you earlier in the talk. We also did things, and my first plug for uncertainty will be: we did things like we looked at what happens if you just look at the mean function of the Gaussian process and plug that into the recurrent neural network in order to make the prediction. We actually found that it really did help if you did something like take about 10 samples of the Gaussian process instead of just plugging in the mean function. So, we did see evidence that that kind of uncertainty going into the classifier. Uncertainty going into the classifier was helpful in this kind of setting. Another thing that we did is, if you remember back to the news score, I talked about that it was false alarming so much that the nurses just started ignoring it. So another thing that we did was we worked very closely with infectious disease specialists in order to figure out, okay, how many false alarms can the nursing staff handle before they kind of just mentally tune out? And what we ended up doing is we And what we ended up doing is we got to get together with them and we chose 0.8 sensitivity, so basically one false alarm per true alarm for this system, because they thought that that was a reasonable amount for the nursing staff to be able to handle. But it wasn't so much that it was degrading the accuracy of the algorithm. Okay, and so this is a picture of what the interface to this looks like. What the interface to this looks like. So, again, deploy, this is what a rapid response team nurse would see. So, every single patient who comes into the emergency department, into your university hospital, is represented by a box. These are not real patients because that would be a violation and that would be bad and I would be opening myself up to a lawsuit. So, okay, and so you can see demographic information like the person's name, where they're located, age, gender. Located, age, gender, etc. And then, color-coded here is some kind of like risk assessment that our algorithm is giving in terms of how likely we think it is that this is a patient in the emergency department who is septic. And then if they are eventually diagnosed with sepsis, one of the things that our algorithm does is it helps track, okay, so when somebody is diagnosed with sepsis, there's a treatment bundle, which means that like every Which means that, like, every three hours you have to give them IV fluids, every six hours you have to give them vasopressors. Again, in the US, and if you don't do this, your hospital is kind of dinged for it. You're really supposed to do it. So now you would think every single hospital would do this all the time. Obviously, not really the case. Duke University Hospital had a baseline rate at about 40% compliance with this particular bundle, right? So one of the aims of our system. The aims of our system is also to just remind the nurses about the treatment bundle so they can stay more on top of giving IV fluids or whatever when appropriate. So anyway, so if they start treating somebody for subsis, all of these things that they're supposed to do come pop up on the screen and they can see them. Okay, so again, this is looking at bundle compliance. Compliance. So the baseline rate here at the time that this was done was about 50%. And we saw that we could do much better when we were actually reminding people that they were supposed to stay up with the treatment bundle. So the black dots are people who have actually been diagnosed as being septic. And so we could see we could get that number to almost 70%. Okay. And so now I'll talk about And so now I'll talk about part of this, which I think, like, I didn't actually do myself. So, this is me giving somebody else's talk. I'm sorry. But, like, there are people, my former grad student got together with somebody who is an ethnographer and they looked at this system and the sort of fairness and transparency aspects of this system. So, when we actually used this system at Duke University Hospital, we quickly learned. Hospital, we quickly learned, but we kind of like learned on the fly, just as a rule of thumb, that you can't do certain things. So, for example, you can't really use this system on children because children have very different like pulse rates and other vital signs, and it's just not going to predict well for children. But aside from like people internally knowing about those rules of thumb, we wanted to figure out: were there sort of broader things about deployment? About deploying this kind of system within a hospital setting that we could take lessons away from or that we could learn socially more about. So they did things like they looked at like developing model card. I don't know if you guys know model cards. So model cards are like basically transparency sheets for a particular algorithm that tries to like lay out this is where the data is from, this is how the model performs, that kind of thing, so everybody can see it. We found Can see it. We found maybe unsurprisingly, I should say Madeline found maybe unsurprisingly that like clinicians have somewhat limited tolerance for like actually reading through a large page worth of like material on like a particular algorithm that they're using. So I think that's work in progress for us. Like for example, like at Google, right? Like how is it that we can present this transparency information, which is really important for everybody to be able to understand to people like clinicians in a way that Do people like clinicians in a way that they can digest, right? So, that's, I mean, it's more of a user interface kind of question, but it's an important question. Okay, so she did studies where she interviewed a whole bunch of people in the emergency department at Duke University Hospital and tried to figure out like how it was that they were interacting with this system, this AI system coming in, right? And how it fit into their workflow. And so, what she found was that organizations. And so, what she found was that organizational norms were very destabilized in ways that weren't anticipated by the designers that would be us. But even more surprising was how the destabilization was stabilized again and made to be a little bit better. And trust and accountability, which are really key in these situations, right? You've got to trust that you're getting something reasonable. You've got to trust the other people around you, was fostered. Around you was fostered through methods that didn't include interpretability. I mean, basically, that means it wasn't fostered through an automated algorithm, it was fostered through interpersonal interaction. So, for example, one of the issues is that the way that things typically go in the ED is like the doctor kind of looks at the patient and makes a decision about whether they think it's like the patient is likely to be septic or not. The patient is likely to be septic or not, and tells the people around them. And this kind of has a reversal of flow of information. So the system is alerting that this is possibly a septic patient who's seeing it. It's the rapid response team nurse. And so then the rapid response team nurse has to tell the doctor that, hey, this automated algorithm just flagged this patient as potentially being septic. And like, that's a reversal. That's a reversal of social hierarchy that they're not used to, right? Hierarchy that they're not used to, right? So, how they reacted to information coming that way was very dependent on things like their mood, right? So, here it's like they just didn't like their medical judgment being questioned, right? So, that was a problem. And so what you find is like rapid response team nurses like picking up the phone and being like, hey, how are you feeling today? Right. To try to get like a sense of how the doctor was actually doing. Doctor was actually doing, and how receptive they were likely to be to this information coming in. So, there was a lot of like insecurity on the part of the nurses and emotional labor put in. Another thing that happened was that the rapid response to nurses also did a lot of chart review. So, this is like looking for additional evidence in somebody's chart about why it was that the system might have been making the kind of decision that it was. Might have been making the kind of decision that it was, or what other information could back up the decision that the system was making. So, that was something else that goes on. So, then if somebody asked for it, they could point to other evidence as well. And so, what was found is that the repair work that was performed by the rabbit response team nurses was effective, and it was effective because in part it was their expertise was supported, right? So, it's really, it's really important. It's really important to have an environment where somebody's expertise and knowledge is going to be supported on a social level when you deploy, like an actual statistical system in the real world, if you want it to be successful. And so trust and accountability, again, are very significant and serious parts of what is going on in order to have a practical system that works. Okay, so at this point, that's all I would. At this point, that's all I was going to say about sepsis. I will move on to some of our certificate work in HR birth mortality. So basically, the future stuff that I'm going to talk about is going to be boring in comparison to what I just presented in the sense that what I just presented is actually being used on real patients. And so I have some stuff to say about that. I had some stuff to say about that. This is all. About that. This is all more methodological in terms of looking at uncertainty. And I will start out by saying, okay, so one of the first things that happened, so I then transitioned from to Google. And that was moving to a very different environment than the one that I was coming out of. And so this is basically the first paper that I worked on when I moved to Google. And so it was very interesting. And so it was very interesting because they cared a lot about Bayesian methods and uncertainty, right? And they cared a lot about integrating Bayesian methods and uncertainty into deep neural networks, right? Because Google is sort of like a hub for deep neural networks and that part of machine learning. And they recognized, rightly so, that like you're missing all this information that you get when you look at full distributions and when you take uncertainty into account. The unfortunate thing for me was that they. unfortunate thing for me was that they were using very different language than what I was used to. So I will talk a little bit about it here. So uncertainty, right, you have some kind of inputs coming in. So I'm going to talk in this particular setting about electronic health record data. So it's this is publicly available electronic health record data. So you have again like labs and vitals that you have coming in over time. And what you're trying to do is make a prediction. Do is make a prediction. Last time I was trying to make a prediction about whether or not somebody had sepsis, this time I'm trying to make a prediction about whether or not somebody is likely to die, right? So, this is like mortality prediction, right? And so they there's been a lot of work on something that they call data uncertainty, but we really want to do more work on what we call model uncertainty. And I was like, I don't know what you're talking about. If anybody knows what they're talking about, I'm okay. You have something to teach me because I feel like when they were talking about data uncertainty, I'm like, okay, you're kind of talking about the likelihood, and when you're talking about model uncertainty, you're kind of talking about the prior. But yeah. So they say, okay, so we have this neural network, and this neural network is this function which is a combination of inputs and weights. And it's deterministic, right? Because the way Deterministic, right? Because the way that recurrent neural nets are run is in this deterministic function, in this deterministic way, where you do some kind of like optimization over your parameters. And you get out lambda, which you can then feed into like a Bernoulli function if you're doing binary classification. Is this person going to live or die? And you get a solution. So you have uncertainty in the likelihood, right? But you don't have uncertainty in anything before the likelihood. And so basically. Likelihood. And so basically, people are like, people have looked at this a lot, but we want to look at what happens if you move to using a Bayesian recurrent neural network or somehow have uncertainty at the parameter level. And so they were like, okay, well, what happens if we put a prior on a leads? We can still compute the probability of like somebody dying. Like somebody dying or not, given what we're seeing coming in in terms of their lots and vitals by looking at these equations. And I'm like, okay, great. I still don't understand what you're saying, but that equation makes a lot of sense to me. Okay, so what they did is they looked at one of the things that they looked at was deep ensembles, right? So this is again in the determiner. This is again in the deterministic setting. So now they're kind of inducing a prior over deterministic neural networks, but not in a way that they recognize, right? So they're saying they have M deterministic neural networks, and this is an ensemble, but they're going to basically say that each one of these M corresponds to a neural network that has a different random speed, right? So it's like if you do this option. So it's like if you do this optimization of parameters, right, you start with just some different random seed that there's no real understanding of, but it's just different. And so you get n different samples of what, say, that burning leaf parameter is like. And so we can get a whole bunch of samples by using a bunch of random seeds. Again, this is deterministic by, or this is a deterministic neural network by looking at different random seeds. By looking at different random speeds. And so, in this way, you can kind of get a distribution over the different models that can come out for a particular patient. So, this is one patient. We're looking at like the prediction or we're looking at the likelihood that this person is going to die depending on the different random seed that we're predicting. Right? And so, the thing that you're going to see me focusing on at the end of the day is going to be like this distribution. Like this distribution over whether or not the patient was likely to die is going to look very different between different groups of people. Okay, so one of the things they compared to was that deep ensemble that I just talked about. Another thing they compared to was Bayesian recurrent neural networks. And so they did inference in the Bayesian recurrent neural networks using variational inference. But basically, it's like they put a prior on the weights in different. Prior on the weights in different locations, and like looked at, you know, you can put them on the input weights, you can put them on the output weights, you can put them on the pin and layer weights, you can get right. So they looked at all these different combinations of that and they compared to the different ensembles and they compared to each other to find out how they were doing in terms of predicting mortality. Okay, so here basically what you can see both for standard What you can see both for standard deviation and range is kind of what I said before. So, this is looking over the population of patients, right? And what you can see is that for different patients, you have very different amounts of uncertainty for that patient, both if you look at standard deviation and if you look at range of the distribution. And again, this is going to be important kind of going forward. Important kind of going forward to keep in mind that different patients express different amounts of uncertainty. And so one of the things we really care about is, given that we're sampling this Bernoulli parameter about whether patient is going to live or die, is the decision that we make based on that Bernoulli parameter, right? And so here we say that the decision that we're making. Say that the decision that we're making is a very simple, just kind of threshold kind of base decision. So it's like if more than a certain number of models are classifying you in a range where we think that you're likely to die, we're going to say you're at risk of dying. If not, not, right? Again, very simple binary classification. Okay, for our actual experiments, Our actual experiments, we looked at the clinical task of taking publicly available ICU data and making this mortality prediction. So, we looked both at MIMIC3, which is a very common publicly available trichelfrequer data, and the EICU data. Again, so the MIMIC3 data set has about 46,000, 47,000 patients in it, and the EICU data has about 200,000 encounters. 20,000 encounters. Those are slightly different things. And we looked at, we really looked at just one: binary inpatient mortality. And you can see when we do the big comparison of all of these different methods that I described, the deterministic ensemble, various kinds of Bayesian methods, I'll try to decode these. So this is like a Bayesian prior on the inputs. This is a Bayesian on the input layer. This is a Bayesian piger on the... is basing a prior on the on the this is a prior putting a prior on the um on the output on the weights of the output layer this is putting a prior on the hidden layer and the output layer this is putting a prior on the weights of the lstm layers the fully connected hidden layer and the output layer and this is putting a prior on the weights of everything and so sort of our takeaway from this was that in general Was that in general, kind of putting a prior on the input layer did better. Don't completely hold me to that, but that was what we found in the paper. But the thing that I think is really important here, and the reason why I'm talking about this paper, is that we found when you look over the distributions that were induced by these various methods, and you looked at the decisions that were coming out again, here it's very That were coming out. Again, here it's very simplistic decisions, right? What you found is a lot of variability. So, for some patients, it's like really clear what decision you should be making. It's really clear that like they're likely to die or they're not likely to die, right? But for some patients, it's really uncertain whether what kind of decision you're likely to get coming out. And that depends in part on the distribution. So the more uncertainty you see, the broader university. More uncertainty you see, the broader your distribution is, the more likely you are to have split probability weight between it being likely or not likely that you're going to die. Okay, so again, the reason why I point this out is because, like, while this correlates to the spread of your distribution. The spread of your distribution, it also correlates with demographic information. So there are certain demographic categories that are more likely to experience more uncertainty, maybe unsurprisingly. Can I ask a question? Yeah. So in your study, do you train a different regular neural network for each patient, or do you have like a general training at the beginning and then just see what the output for the different patients are? Yeah, that's right. So I should say. Yeah, that's right. So I should say that this is something at a population level, which is performing about the same. So it's like you could be in a situation where at a population level, so if you're looking across all the population, all the population of patients that you're seeing, you're performing about the same regardless of what method you're using. But like when you break it down and look at an individual patient, right, you could be performing very differently patient to patient. Differently patient to patient. So you can see a lot more uncertainty in some patients than in other patients. And you can see a bigger difference between different methods or between different random seeds even, right, in one patient versus another patient. And that's really kind of the key point I'm trying to make here. And that larger amount of uncertainty correlates with being a woman, it correlates with being an older person, it correlates with. Person, it correlates with ethnic classes, right? Like it correlates with everything that you would hope to not see in a health equity kind of setting, but you know exists in our society, right? And so this is basically my plug for regardless of what you think about this work, looking at patients on a individual level, right? Because you see such differences when you look at somebody as an individual versus when you look at the patient as a whole. Versus when you look at the patient as a whole, when you look at the patient population as a whole, you can be doing really well, you can be having really good performance on a population level, but then have this very, very drastic change and uncertainty levels for a particular individual, and that can really lead to unfairness in your population. So just to make sure I'm following you, and I can the statistical way of saying it is the same as looking at sort of the uncertainty in terms of like the Of the uncertainty in terms of like the integrity losses versus the point-wise loss, well, point wise in terms of the probiotics-that's what you're saying, yeah, exactly. Um, but and also per patient, right? So, yeah. Um, okay, so this is just a plot of like, so here you have men versus women, so women are on the y-axis, men are on the x-axis. This is really just showing that, yeah, it's another line of work that I am looking at. Of work that I am looking at right now, right? So, this is showing that, like, as you do better for men, you do worse for women, right? So, this is a trade-off that you can see, unfortunately, in some healthcare settings. And so there's a line of work that I'm engaged in where it's like, okay, when you see that, you really need to break up your methodology because you should never be performing worse for any particular subgroup, right? You should always be sure that you're performing better for that subgroup. Subgroup. And this is another plot, which basically shows that for any kind of variability that we're looking at, we're always doing worse basically for the older patients. Okay, and then the last thing I'm going to talk about, I've got eight minutes left, is just being able to flag kind of when it is that you're likely to have generalization problems. Generalization problems. So, this is looking at like, I have a method, it's working really well at hospital A. I want to transfer it to hospital B, and I want it to work well there. And that's like a very canonical problem. It's unfortunately not a problem that we tend to do very well at. So generally, methods don't transfer very well to new settings, but we don't really understand why. And this is saying that this isn't helping us really particularly understand why, but it's saying that this is. Why, but it's saying that this is potentially something that we can flag, right? If we're able to say, okay, look, like we're training in a particular domain, we're training and testing in that particular domain. We think we're capturing some causal mechanism behind like our disease, but we're not necessarily potentially capturing that causal mechanism. And so we think it should generalize to the rest of the world, but we haven't necessarily created something that's going to generalize to the rest of the world. Going to generalize to the rest of the world. So, what we'd like to be able to do is to flag settings where we haven't actually captured the causal mechanism that we think we have. And so, our transfer is not necessarily going to be good. So again, this is like the typical machine learning contract, right? So, it's like you've got some training domain. This is like hospital A, you train and validate in that hospital and you test in that hospital and you tell what. In that hospital, and you tell what you do. And that's how, like, 99.9% of like machine learning algorithms go. But that doesn't necessarily mean that you've captured the causal mechanism that you hope or think that you have. And that doesn't necessarily mean that you're going to generalize well to a new class of patients or to a new setting. And so, what you really want to do is to be able to train and validate in a particular domain, like at a particular hospital, but then to Like at a particular hospital, but then tests like somewhere else that can try to give you a better sense of like, have you actually, with your model, captured the causal mechanism that you hope that you've captured? And so we're going to look at ways where you might give a flag when you haven't. Okay, so this is just an example of somebody not doing this properly. This was a nature paper a couple years ago. So these are skin lesions. Years ago, so these are skin lesions, dermatology lesions. Um, and people were like, We're doing really well at like trying to classify whether or not this lesion is likely to be cancerous or not. And then what they found out when they looked at it more is that actually when something is when a lesion is at risk of being cancerous, you typically have a biopsy and you have these purple markings, which are that the lesion is marked for surgery. Marked for surgery. And what the algorithm was picking up on was these markings in the image, right? That they were being marked as potentially being searched. So it's like, it's not really capturing the causal mechanism that you hope that it is. And again, this is not giving you a solution for that problem. It's being able to identify when you might be in that situation. Okay, so what we'd really like to do is that we're going to be Okay, so what we'd really like to be able to do is to design better evaluations. So one of the, okay, so first off, this is really useful in the situation where we're using kind of black box algorithms, right? So it's like keeping in mind that there's not a lot of understanding necessarily behind like what people are using in machine learning, right? So I think the kinds of models that you all I think the kinds of models that you all come up with, typically you have some kind of intuition or understanding behind how they're working. But a lot of the methods that are being used in machine learning, particularly at a place like Google, are not algorithms that they understand how they're working or why, right? You have a neural network, you just apply it to a problem, you don't really understand what you're capturing. And so we want to be able to, for those algorithms in particular, have a way of saying, you know, a way of saying. A way of saying that you're at risk for having not captured the causal mechanism behind it and not being able to transfer well. Okay, so what we use is basically a sentence to the analysis in order to make the gap between what you think you should be doing and what you actually are doing more concrete, right? So there are potentially many different functions that satisfy the That satisfy the machine learning contract rate. So, there are many different functions for which you could get good results, but they basically generalize differently. And so, what we're going to do is we're going to construct ensembles, like I talked about deep ensembles in the last work. We're going to construct ensembles of methods that only vary, like I talked about in the last piece of work, by something like a random seed. And we're going to see how we do for each of the random seeds. Now, if we're actually Random seeds. Now, if we're actually capturing the causal mechanism, we shouldn't actually see that much of a difference in performance based on like a random seed, right? But if we're capturing something else, right, like the purple markings and the dermatology images, you might expect a larger distinction in how well you're performing based on some random criteria. Okay, so the experimental template is as follows. So we have a suite of stress tests. So this is Suite of stress tests. So, this is basically like you know, if we train at hospital A, this is hospital B, hospital C, hospital D, maybe different kinds of patients are your stress tests. They're things that are going to help you flag when you're not generalizing as well as you possibly can, can be, could be, or think that you might be. Okay, you're going to construct an ensemble of models. Those models are things that. Models. Those models are things that just vary by some innocuous parameter, right? So some random C that you think you should be kind of impervious to. And then you're going to confirm that you get performance that's not varying very much in your original training domain. So, regardless of what this random seed is, you're doing pretty well in your training domain. And then you're going to look at how much variation you get. How much variation do you get in your testimony, on your stress test, right? So, when we go to evaluate it in different hospitals, how much difference are we seeing in performance unlike the hospital that we trained in? Okay, so we test this in computer vision. I know we're almost out of time. We test this in computer vision, right? So, we test this both on natural, oops, sorry, natural looking images, but then also But then, also, Google does something called diabetic retinopathy, where they look at retinal images of people and try to determine whether they're at risk for diabetes and skin lesions. And we look at all these computer vision settings. We look at, okay, we start by looking at ImageNet images, just plain natural images found on the internet. We look at using a very basic sort of like RedNet 50, so it's a very basic neural network. This is a very basic neural network model. We look at the domain that we trained on, the image as it like normally. We see that there's not a lot of variation. But then as we look at different kinds of perturbations of that particular image, particularly ones like selfation, we see very large amounts of variability. And that really signals to us that we might not be generalizing in the way that we think that we're generalizing. So if we look at a pixelated bird, we can still. At a pixelated bird, you can still recognize it as a bird, right? So, there's something about the way that we're doing generalization that is not completely matching up with the way that that neural network is doing generalization. Same thing kind of for the eye images, right? We can train on a bunch of different camera types, but when we go to generalize to like a new camera type, we're not doing as well as the camera types that we generalize based on. And if we look again, like at skin color, if we Skin color, if we test on skin color, which is not a skin color that we trained on, we're not necessarily doing as well either and picking up skin lesions. And that is really all that I have to say. I'm only like two minutes over. Oh, I basically just wanted to say I am a very big proponent of collaboration with lots of different people. There are very clear risks and harms, particularly to vulnerable demographic groups, which can be seen only if Which can be seen only in situations where we look at uncertainty. So, kind of the takeaway from a lot of this is like, you really need to look at uncertainty because, particularly with these vulnerable demographic groups, that's really going to signal whether you're at risk or making drawing conclusions that you shouldn't be. But to be able to fix those bad conclusions or to make better, basically, conclusions, we're going to have to have a deeper understanding of the methods that we're using. So, this is all. Methods that we're using. So, this is also me being a proponent for having a better understanding of what we're doing and the exchange of ideas and just values, like values of like different people coming from different places, honestly, is something that be used in this community. Okay, thanks. You have the mic, so I stay just close to you, not small. Thank you very much, Catherine. I think we have. I think we have not time, but for questions, for the sake, we still have coffee break, right? So, this we have time. So, I think if you lend me this so I can go running with Michelle and he wants to do this. No, no, no, but you're here. Just here, yeah, okay. So, we can ask a question. Yes, yes, yes, just with one question. Oh, and also Peter had a question. Oh, also Peter had a question. Go ahead. Go ahead. So, my question was more, I don't know, maybe a comment. So, at the beginning, you were discussing about the different wording and terminology. And so I was, so when you were discussing the type of causality, this discussion of causality, I was thinking that so at the same time, that was not exactly what I was thinking about, thinking about causality with FC. So, one thing that I The DC. So, one thing that I was wondering is that it looks like many of the problems that we've presented today, which I agree like a lot, were actually made with the problem of heterogeneity in the data that is not captured by the machine learning algorithms. I think, for example, in the first example, you have a Gaussian process, but it looks like the parameters are fit on the entire population, right? For example, that you have, right, without considering possible differences across. Across the patient population. And similarly, also the deep sample that you had in the second project, this looks like you are still considering the correlation as a unit one, right? So is it indeed a problem of heterogeneity? And so also close to what we are discussing these days, the vision of parametrix models that we be able to capture. We don't want to say clusters, but yeah, no, so. Yeah, no, so um please, please, yeah, no. So, um, some of the work that I've done is looking at things like survival analysis or looking at different kinds of methods that can capture and make predictions based on time series data in a medical setting that do look at, do start looking at exactly like what you're saying. What happens if you put like a GP behind behind some of the methodology that we're doing? What happens if you use a direction process before you do some kind of like cocksuper? Or you do some kind of like cox regression or something like that in a survival analysis setting. I mean, that's like a very simple example, right? But like, you can potentially learn a lot about different demographic groups and it can help you in the situation where you are, in fact, seeing a decrease in certain demographic groups, right, to get an increase in other demographic groups by just separating them out. And one of the things that we've seen work pretty well is like letting the algorithm sort out who to put in what cluster, right, and who to create a different model for. Who to create a different model for? I think that's really, really important, but I think that's like current, really current work, right? Like looking across demographic groups, looking at different ways of like testering people and breaking them out into different levels. Yeah, so sorry, so she asked, for those of you on Zoom, that if you do clustering before you do the analysis. Before you do the analysis. So we don't. We do them sort of at the same time. But you could try, right? Like you could try to do clustering before you do the analysis. We do them simultaneously. Okay. I think we can carry on the discussion over the coffee break for those that are here. I don't see questions from the audience there. So thank you very much, Catherine. Thank you. 