The only problem is. Ah, here we go. Can you see that? Yes. Excellent. Excellent. Okay. Thank you. A quick introduction. Sam Iqval probably needs no introduction to anyone here. What a delight to have you here, Sam, talking about revealing hidden genetic variation in the bacterial accessory genome. Sam, please take it away. Sam, please take it away. Thank you for the intro, John-san. Thank you for the invite, everybody. Okay, I have my stopwatch on. I'm not going to overrun. I am going to talk to you about, okay, let me give you an overview of the talk. I want to start with some key observations about bacterial genomes. I think all of you will know everything I'm going to say, but I'm going to say it anyway. And I'm going to talk about how that leads to the fact that, you know, we have problems detecting SNPs inside the accessory genome. Again, you probably know this. Accessory genome. Again, you probably know this. And the talk is then going to be about essentially about solving this problem. It's actually quite nice talking after Pierre. It's nice to see Pierre again, but it's also nice because I come from the same place, at least from a K-mer world, and one of the things this work is supposed to address is the interpretability problem. So, I mean, we want to be able to do better at genotype-phenotype inference. We want to be able to do Inference, we want to be able to do better at genomic epidemiology, and to do both of those things, I think we'd like to be able to access SNPs from the accessory genome better. There's a lot of material underlying this, so I'm really going to focus on giving ideas and results, and I'm going to, for details, you can talk to me afterwards, or we've got a forthcoming preprint. Okay, so basic things. Bacterial genomes are gene-rich. Genes cover a large proportion of the genome, and so shared gene content is commonly used. And so, shared gene content is commonly used or can be used as a proxy for genome-wide similarity. Horizontal gene transfer leads to shuffling of genes. And so, as a consequence, even within a species, two genomes within a species may share what may seem a surprisingly small amount of their genes. No surprise to this audience. I mean, this really kills the idea that a single reference is going to work for a species when you're doing SNCC calling. And if you look, there's a kind of canonical. There's a kind of canonical gene frequency curve where most genes. I've got a plot here from my postdoc Lear, who's in the audience. If you take sort of 10 genomes for multiple species, you can see most genes are either, and therefore most regions of the genome are either sort of rare or common. So that brings us to the main question, which is how can we do SNP calling in amongst a set of bacterial genomes? These are genomes, these are genes, this is a toy obviously. These are genes, this is a toy, obviously, and the numbers are SNPs. I want to be able to call all 50 of these SNPs, but as soon as I pick one of these genomes, I'm stuck, right? Because the only things I can really address or discuss or represent are the things that are on this background. So I'm missing the grey and the light blue and so on. So I'm going to describe the work, but it turns out it's already eight years of work from three people. Rachel Caquon, who was a PhD student of mine, is now a postdoc in Edinburgh. PhD student of mine, he's now a postdoc in Edinburgh. Michael Hall is a current PhD student in Leandro, who also contributed to the previous talk. He's a really brilliant scientist, he's joined my lab. And together, what they've done is build a set of algorithms and software called Pandora. So our strategy, the motivation for our strategy is this. Genomes evolve by recombination mutation. It's no surprise. So new samples should look like noisy recombinants of old ones if you've seen a decent number. Recombinants of old ones, if you've seen a decent number of old ones. And again, that's a standard technique used in population genetics. So, what we want to do is start by storing a reference panel of genomes, and we store it in such a way that when you map your reads to that panel, it allows reads to map across recombinations. And then we perform a two-step approximation. The first thing we do is given our reads from an unknown genome and a graph reference panel. And a graph reference panel, we find the closest mosaic in our reference panel to the unknown genome. So that's our first approximation, it's just the mosaic of our previous genomes. And it turns out that that's actually already a pretty good approximation, but then we fix up all the gap errors with local assembly. So we do this with a pangenome reference graph, the pan RG. It's an unordered set of directed acicle graphs, which is a fancy way of saying Which is a fancy way of saying it's a set of multiple sequence alignments. Each graph represents an MSA of some orthology block of genes and intergenic regions of what we've been using. You could use mobile elements. And we completely let go of higher order structure, operons, gene order, things like that. We don't think about where things are in the original input reference genomes. Everything is in terms of these blocks. And these blocks, so here's a multiple sequence alignment, here's how we turn it into a graph. Here's how we turn it into a graph. So, this is the shared sequence at the start, shared sequence at the end, and then alternate paths that you might have. And then this is our process. So, you start with your reference graph, you index it so that you can map to it. Each of these squares is a different sample. I want to analyse a whole cohort of bacteria. Each one is independently handled on a different cluster node. You map your reads to the graph, you quasimapap. To the graph, you quasi-map them because you're not really doing actual full alignment, which is a really expensive process. But you do enough mapping to work out where in the graph the read lies, and then you can collect coverage and infer the mosaic sequence through dynamic programming. And then you do local de novo assembly on the bits where it doesn't quite work. You find new alleles, you collect them all from all your samples and put them back in the graph, and then remap. And now you've got a structure that contains all the variation in your data set, and it's a consistent structure. And it's a consistent structure for all of them, so you can try and get a nice matrix of variants and samples. And you compare all your samples, you get a multi-sample VCF. Okay, that last bit, how the hell have I done that? Where do you get a VCF out of this graph? So we have a, that leads to a representation problem. So the toy example here, I'm interested in what reference I use, not for discovery, but for description of a variant. Of a variant. So here I've got a SNP between the red and the blue, but if I use the black genome as a reference, then I can't describe this without a coordinate. So I have to start over here. And then I have to add this flank before and after the SNP. So I get a record that looks like this. If, on the other hand, I'd chosen the blue thing as a reference, then it would be easy. I'll just get a SNP. So the question is, how to do that? What we do is what we want is a data-driven process. Process given my cohort, I want to find a reference genome for my cohort that gives me the most succinct description in my VCF of the variants that are in the cohort. So what I've done is I've already inferred a mosaic approximation for every sample, that's the closest path. So I wipe the graph clean and I draw on all of these mosaics and I increment counts on all the shared regions. So now I know where they're similar and where they're not. And I use dynamic programming again. On whether or not, and I use dynamic programming again, this time to find the most supportive path. And what I've done is I've found the best reference for describing my samples with more SNPs and less long alleles. And it's easy to annotate, it's just a gene. And so most users just get VCFs in terms of genes and integenic regions. It's easy to annotate, easy to understand. No one needs to think about the graph. Okay, so my recap halfway through is for bacteria, one reference genome is just For bacteria, one reference genome is just not viable. For reasons, I say hard and soft bias. Hard bias means there's enormous chunks of genome missing. Soft bias means diversity, SNP diversity makes mapping harder as the diversity dials up. So we invent Pandora. It's a set of local graphs and multiple sequence alignments. For each sample, you find the closest mosaic. Use local assembly to find the missing SNPs and indels that are not in my reference graph. And then I augment the graph. Reference graph, and then I augment the graph with all those missing things. I infer a reference for representation, and Bob's your uncle, you now have a multi-sample VCF. So, let's give you some results. We built one of these things for E. coli. We took 350 RefSeq genomes, that's 23,000 genes. We took 228 other genomes, it's a bit annoying that we used other genomes, but we did. And so we took intergenic regions from those. Now, you remember there was a U-shaped curve of the frequency distribution of genes. Of the frequency distribution of genes in a population. So we expect a bunch of genes to be rare in my reference panel. So they'll be, you won't see them very often. So 60% of our genes and 40% of our antigenic regions were actually linear with only one allele. Okay, what do we do? That's Leah in the corner who did this. We took 20 samples which have matched Illumina and Nanopore data and high quality assemblies. So we wanted to be able to High quality assemblies, so we wanted to be able to. We're going to do this both on nanopole and illumina data, and we want we need the high-quality assemblies to assess truth. These genomes are distributed across the E. coli phylogeny. These are the phylogroups that they come from. The left-hand bar here shows where all the samples are. They're slightly enriched in red and green. And these guys over here are reference genomes, which are the closest. We took the closest genome to each of these either by match. These either by MASH, measured by MASH, or from a tree built to them all. We just wanted a load of reference genomes which we could use for standard variant calling tools. And then we find all the true SNPs between the 20 genomes to see if we can find them. And we do that with all pairwise alignments between the 20 genomes, generate truth SNPs. So there's a tricky thing that you have to decide if a SNP between genomes 1 and 2 is the same as a SNP between genomes 3 and 4. I refer you to Genome 4. I refer you to our paper for that because that's a tricky question. And here's some results for 100x of Nanopore on my 20 samples. Over here we've got Pandora. On the x-axis is the error rate, so how many records in the VCF are wrong. And on the y-axis, you've got a particular type of recall, which I can discuss afterwards if you want, but it's just recall. And so we manage about 85% recall at about half a percent error rate. Half a percent error rate. And we just, and we have a curve here because you can, if you dial up the genotype confidence, you get a more confident call at a cost-in-re-call. Here, you've got the results for Nanopolish. It's a standard tool for Nanopore. It's got many curves, one for each of the 24 reference genomes you use. And depending on the reference genome, you top out to the different level of recall. And over here, you've got Madaka, which is the tool from Oxford Manacore Technology, so it's got a much higher error rate. So it's got a much higher error rate. And roughly speaking, I guess the bottom line is we get a higher recall and we've got a better error rate. And it's a very simple story for NanoCore. If I look at the data in a slightly different way, I can ask how precision depends on reference. So on the x-axis, I take all of my 20 samples. On the y-axis, I show the precisions of the calls in that sample. And you get a box plot, of course, because there are 24 different references, and each one gives you a different answer. And so you can see there's Answer. And so you can see there's quite a big range of variation depending on your reference genome. It's a terrible, I mean, these are pretty bad results for Madaka. These are unfiltered results, so you can improve them slightly with filtering, but it's complicated, essentially because of a bug in software at the moment. And up here, you've got the results for Nanopolish, which does better, and flat at the top is Pandora. And if you do the same thing on Illumina data, 100x Lumina data instead. Luna data, 100x lunar data instead. Then, so there are two interesting things here. One is surprisingly, we get almost identical results for Pandora. We have the same recall of about 85%, the same error rate of about half percent. So, that's kind of amazing, but you're getting just as good results for nanopores as Illumina. That's great. Sam tools over here, different recalls, depending on your reference. You can get the reference that gives you the best precision error rate is not the same as the reference that gives the best recall. Error rate is not the same as the reference that gives the best recall. And over here, Snippy does much better than us for error rates. I can talk about that afterwards, but not right now. And you get essentially the same story for precision on the Illumina data as well. So the reference makes a big difference for samples, for precision. It makes a difference for Snippy 2, but it's doing so well, it's not so visible. But then here's where it gets. I mean, this is all fine, but this is all averaged. This is averaging over all SNPs. SNPs and that's SNPs in the core genome and SNPs in the accessory genome. The whole point of the talk is to find out how we do in the accessory genome, and this is where you find out. So on the x-axis, you've got the number of samples that a locus exists in, or a SNP exists in, the locus in which a SNP lies exists in. So these are SNPs that are only in two people. So there's two alleles, one in one sample, one in another, and the rest of the other 18 samples don't have anything at all. And you've got two box plots, one for SAMTools, one for SNPI, for the 24 reference genomes. SNPY for the 24 reference genomes. And up here, you've got Pandora. So we're basically calling many thousands of SNPs that are completely inaccessible with the reference genome. We do about the same in the middle of the frequency spectrum. And we actually do worse over here in the core, which we're going to have to dig and find out about. So a thing to remember, of course, is that you've got this frequency distribution. So there's actually not many genes in the middle. There's less stuff to find in the middle. But there is more on the left and on the right. Well, that's what you'd expect. So it was probably a good. You'd expect. So it was probably a good idea to normalize this. And if you do that, you're basically showing that we're calling half of the rare variants. Whereas with the reference-based thing, you're either calling a quarter of it, somewhere between a quarter and 33% or something like that. And this is why we built Pandora. Basically, so we're not there yet. We're still only calling this many, and I'll talk about why only that many, but it's far more than you can access with using a reference genome. So, if you follow this up, references actually work better for genomes that are in the same phylotype, phylogroup. This is not a big surprise, but let me just talk you through it. Each of these histograms is the results for SNPI. Each of these bars is one reference, one of the samples. The height of the bar is how many of the SNPs you've called. This black line is how well Pandora's done. The pink colour in the background means that the reference genome comes from the In the background means that the reference genome comes from the pink phylotype, which is A. So basically, what you see is if you've got a pink reference genome, then the pink gene samples do well. If you've got a blue reference genome, blue samples do well, same with green, same with red. Slightly different for yellow, but yellow is an outgroup. We don't have any samples in yellow, but yellow is closest to purple, and that's what you see there. So I don't think like microbial genomicists are very surprised. I mean, they would normally choose a reference genome from the same. Normally, you choose a reference genome from the same phylogroup anyway. So, this just confirms what you would expect, which is that you get better results if you choose a reference genome from the right phylogroup. What's interesting is if you now restrict yourselves to rare SNPs, so SNPs that are not seen, they're only seen in two samples, and replot the same thing, and then it's a very different story. So, we still are semi-flat, that's a kind of spiky kind of flat with Pandora in black, but you're accessing much less of it independent of what reference genome you see. To what reference genome you see. And actually, there's no real correlation between phylogroup and of the reference and how well you do detecting rare things. So what does this open up for us? Well, microbial genome-wide association studies, I mean, it means you're going to start being able to really address SNPs and know where they are in genes. You can use the actual SNPs or indels or whatever as markers. I mean, more other methods for genotype, phenotype analysis. Other methods for genotype, phenotype analyses. I'm quite interested in using it for population genetic models of bacteria because you can basically reduce this back to zeros and ones, or maybe zeros, ones, twos, threes, and fours. See what I mean? You can turn this into sort of the standard kind of haplotype models that people use. And you can use them for studying diverse, more diverse data sets than you can normally using reference genomes. So, what are my conclusions? These guys are awesome on the left. On the left. Frequency distributions of genes lead to a huge range of variation that's currently inaccessible. Pandora recovers more of it. It would do better if we'd used a different reference panel. So we built it from RefSeq, which is not exactly well sampled. Gal Varesh et al. produced a very well-curated set of E. coli earlier this year, and I think we should try building from something like that. Still a work in progress. One component. Still a work in progress. One component in the middle is implemented in Python, and we're using cluster style omega for our multiple sequence alignments, which is not the fastest tool in the pack, in the tool in the whatever you keep tools in. There's a preprint which should be out this week, all be well, and the code's publicly available, and I'm happy to take questions. Great. Thank you so much, Sam. People are welcome to put questions in. To put questions in the chat or in the shed, which is normally where I think most people keep tools unless they keep them in other directories. Great. And so in the absence of questions in the chat, Sam, you left a little teaser for why, oh, there's now a question from Celia Greenwood. How does this relate to graph genomes in humans? Okay. So the thing is, there's no real graphs are very general things, and there's no real consensus on how to build a graph genome. We have built at one end of the spectrum, you can build something that just throws in all your sequence data and it becomes a sequence representation which doesn't you probably lose your ordering information, like a De Bruising graph, there's very little ordering information, but that all your sequences in there. Information, but all your sequence isn't there. The human pangenome community is really working on the basis of having long read assemblies and then trying to put them together. They don't necessarily want to retain a completely ordered directed graph because they want to be able to be to allow inversions, all kinds of things. And that community is primarily up to now is. Up to now, I think it's been most successful at working at different methods of building these graphs, but not really at how to address variants within them or even to do variant calling within them. I'm working in a world where we've got much more diversity, but at the same time, we've got atomic units which are preserved by selection over evolutionary time, that's the genes, the intergenic regions, to then meaningful units into which I can break up the problem. Break up the problem. The human genome community is also very much committed to one reference genome that they really want to talk, like stick with, whereas the bacterial community really doesn't care about that. It's very much easier to convince them to move references because they already do. There's some move away from a single reference genome, but absolutely, I agree with everything else. Yeah. Okay, I mean, I agree with them moving away from it, but it's still, as far as the community is concerned, it's quite hard convincing people to move off HG19, leather level. People to move off HG19, let alone entrepreneur. So, I mean, there's a thing of it's quite hard to sell a tool which no one will use. So, I've got an easier sell, I would say, within my crypto community. That's what I'm saying.