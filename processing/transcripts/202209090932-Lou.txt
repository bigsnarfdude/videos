I'm Tony on the POS development network with finite dimensional link group. So to start with, I want to give an introduction on the path development with our motivation and how it connects with signatures. So I think everyone here are quite familiar with signature. It is a sequence of non-community monominals, and each of the monominal is given by this interest integral. And signature has been widely applied in the machine learning, and thanks to universality property. And thanks to universality property, the signature works quite well with just simple linear models. Also, we have a two side to combine signature with the deep neural nets, and this has been shown to be quite successful in various tasks. However, there are two main bottleneck signatures. First, curves of dimensionality. As we know, for the d-dimensional part, our case term is d to the power k, so it grows geometrically. Secondly, this is a deterministic efficient map. This is a deterministic feature map, so it may not be data adaptive. And then, in practice, we actually need to do quite heavy feature augmentation either on the signature level or on the original path level to make this work for different data sets. So, in order to deal with this, we actually introduce called path development layer and just want to start with the path development by definitions. So, the path development is a solution to this. The solution to this control differential equations. And here the M is a linear mapping from Rd to the D algebra. So in our case, we only consider the matrix K algebra. So this is subspace of some general linear matrices. And the multiplication here is understood as a matrix multiplication. Then we have this control differential equation. The essentially pass development is. The essentially past development is a solution of this, very similar to signature, but signature doesn't have this linear map, and then the multiplication is tensor product. And one way to think about this is actually from the angle of generating functions. So we know the generating function on the infinite serial of real number can be written as this. So in replace of the lambda for the positive element, we can think of this linear map as a role of the lambda for the real case. the lambda the real case are actually mapping each monomial of the of the path signature onto a finite dimensional matrix space and then taking the sum of the infinite series then we have actually finite dimension representation of the path and also this m can be trainable so and then also in the later part i will show it's actually quite a desired property we make it to a trainable map okay um so to move on i just want to give So, to move on, I just want to give a quick visualization what this is. So, if we're taking a simple two-dimensional piecewise linear part that I show at the light figure, and then if we define this linear map to a predefined matrix form, so this actually is a Lie algebra of the orientation preserving isometry on the H2. Then, by solving the control differential equations, we actually have a development, and this is just a pause. And this is just a path in the hyperbolic space. So, as you can see on the right figure. Okay. Yeah. So, I think then let me move on to, I think, because everyone is quite familiar with security, might not be quite familiar with development, so it's quite nice to have this linkage between these two. And one important link between these two is actually when the IM is. When the M is a linear map from Rd to some matrix space, then there is this canonical extension, such as given by this equation, such that it maps from the tensor algebra space to the matrix space, denoted by the M tilde. Then we have this lemma, such that the development of the development on the path x given under this linear map M is just given by M is given just given by the extension map on the signature of the path. So, this actually allows us to bring a lot of good property of signature into development. For example, the multiplicative property on the signature is also known as chance identity. We also have a multiplicative property of the development, but the multiplication is understood as matrix modification. Notification and they are both environment to time requirementization. And third thing is, as we said, signature is a deterministic map. And development, we can't make this linear map to be trained. The last two probably are very important for signature. And then we actually can show that under certain conditions, if we make this our linear map injective, then we have the universality as well as characteristics. Universality as well as characteristic property for the development. And one example is if we take the d algebra in the value of the complex sympathetic algebra, then we do have these two property. So this is actually given by this theorem. So I don't just want to give two remarks on this theorem. So essentially, this theorem is telling us two messages. The first was that if we take The first was that if we're taking this linear map from Rd to the complex simplicity d outro, then we can actually find a corresponding extension map to separate the two signatures up to level k. So this is actually quite important that it first knows that we can find one such map. It doesn't mean all the maps have this separation point property. A property. So, this actually makes us want to learn this map because we want to find this map to have this characteristic property, have the universality. And the second thing is a message on the dimension case. So we know the truncated signature dimension of the degree k is given by this value from the geometric theory. Then, this actually we know that the development is taking this value, can separate. Is taking this value can separate the truncated signature up to level k. So, this actually shows us that development have a much have less dimension to able to separate the point. And this is a more compact representation of the original time series compared to signature. Okay, so to move on, I want to just introduce how we actually construct these network architectures of this power. Of this path development layer. So, very simple, just like standard time series model, like time series model, like the required neural network or all the continuous time models, like control differential equations. We're taking an input of sequential data, and then development layers of a sequence of a group value, or is just if you want. If you want to just take in the last element, the endpoint of development, it's a single group value. Then our layer is actually constructed by this multiplicative property from the development. And essentially, this is a solution, give us a solution from the previous control differential equations by this recurrent structure. And the theta was to know that the theta is our parameter. Is our parameter to parameterize our linear map mapping from the R D space to the Lie algebra. But this can be just given by the D independent copies of Lie algebra matrices. And D is the dimension of the path. So we actually make this trainable. Yeah. And this is just a summary, summarize the figure summarizes our naval architecture on the left-hand side. Architecture on the left-hand side is the control differential equation of the development. On the right-hand side is how we actually implement it for the discretized time series. We take the increment change between any two point between two consecutive points and then apply our linear map to the Lie algebra and push with the exponential map to the Lie group and then take the recurrent multiplication to have. Modification to have the final solution which is developed within the lead group. Okay. Okay, so now we move on to in order to actually construct this network, we need to do this parameter optimizations. So essential goal is that we, if we have some function which take in the development as the input, and then this is so we can have a loss function or objective function. Can have a loss function or objective function which mapping the development to a real value. Then the goal is to find the parameter theta which minimizes objective. And in order to do this, we actually using two techniques. One is a trivialization, the other is a bipropagation through time. And by propagation through time is actually naturally given by our recurrent structure of the network. So it's So it's very similar to the recurrent neural network structure. We mimic the recurrent neural network to do this bypropagation through time. And another thing, the trivialization is trying to tackle the is trying to tackle the problem that our development is leaving us, leaving our group. So every iteration, when we update our model ways, and then we have Our model weights, and then we have the development, it should always stay on the group, so it wouldn't be jump out of the manifold when we do this optimization. So, we use the tripleization to tackle this problem. So, trivialization is quite a simple idea. So, if we have a subjection map mapping from Euclidean space to the group, then we actually can reduce this constraint optimization. Reduce this constraint optimization problem on the manifold to an unconstrained one. So, for the development case, we have some function on the development, then we want to find the optimized development. Then, instead of doing it on the group, we can do it on the Lie algebra space. So this is, and then here our file actually is given by a canonical trivialization, which is the matrix exponential. And this actually allows us to, because the Because the algebra is a vector space, so we can't do the gradient descent just like on the Euclidean space. So, this actually allows us to use some modern optimization techniques such as stochastic gradient descent and item. Okay, to move on, so in this slide, I just want to show how we actually calculate in this gradient, but I don't want to jump to But I don't want to jump too much detail about this, but just give a simple view of what exactly they do. So on the first equations, we actually, this is the part corresponding to the biprovocation through time. Exactly, what's saying is that if we want to compute the gradient of the loss function, overall loss function respect to our parameters theta, then this is equal to the summation of getting the To the summation of getting the gradient from each time point. This exactly mimics the way of the bipropagation through time in the recurrent neural network structure. And at each time point, when we copy the gradient, we utilize the trivialization method. Essentially, this part is corresponding to in this previous slide when we copy this gradient of phi composition of phi. And the phi here, we're taking this canonical sharpization. Taking this canonicalization, which is exponential map, then we have this explicit form such that it is given by the differential of the exponential map. So this actually concludes our network architecture, which we define the forward part and then the backward part. And then the backward part is make sure that each iteration, we actually make sure the update to maintain our development stay on the manifold we predefined. And stay on the manifold we created point. Okay, so I think to move on, I just want to give us some numerical results we have been implemented with this network architecture. So we actually separate into two categories. One is a general time series. The other one is a model dynamics on the Euclidean space. So on the general time series, we actually And we actually use our master just one simple layer of development, also a hybrid model which is combined with RSTM. This is a very simple architecture, just using a development layer followed by the RSTM layer. And to benchmark our results, we compare with baseline the signature method, also RLNs, as well as continuum time series models. And we also include some state-of-the-art. Include some state-of-the-art results on different data sets. And we use three data sets. One is a character trajectory data set, speech comment, and sequential image data set. They have a different characteristic. For example, the image have a very long sequence. So normally the recurring neural network does quite have an unstable performance because the sequence is very long. Sequence is very long and the speech command has a high dimension. So, to start with, we did this comparison with signature on the speech command data set, because speech command data set have a high spatial dimension 20. So, actually, we can't compute the signature up to two high degree. Up to degree four already have two hundred thousand feature size. And this curve basically showing that curve basically showing that we have much better performance when the feature dimension is the low for the development. So this again is showing that development can is a more compact representation. This can encode more information with a less dimension. This actually can be quite good property for machine learning. And on the other hand we also On the other hand, we also, in all our experiments, we observe that we have a much more stable training progress as well as faster convergence. This is illustrated from the left figure. So when we, surprisingly, when we add in some development layer followed by RSPN, we have much faster convergence, especially when we're taking the development into this ISO group, special orthogonal group. Orthoconal group. Then we actually have this because this group has the property that the norm of this group is always equal to one. So actually, it gives the constant norm of the gradient when we do the bypropagations. So it gives a much stable training progress compared to others. So this is illustrated on the left-hand side. On the right-hand side, we show that We show that against the different hyperparameter. So, this is the learning rate. The RSTM is very unstable against different learning rate. However, our model can always perform very well across different learning rates. So, this actually allows us to get some optimized model quite easily without tuning the hyperparameter a lot. Okay, so in terms of accuracies, we did this on the speech command data set as well. The speech command data set as well as the sequential image data. On the speech command data set, with the similar amount of parameters, our model significant outperform continuous times model as well as sequential as well as RN type of models. We only sell it off against a large convolution. A large convolution lab, but they use a much more high parameters. And on this, the same story for the sequential image with the same very small compact model, we can enable to improve the performance of RSTM by more than 10%. And then we can achieve a similar performance with these large models. But we're still quite far away on this side button data site. Our way onto the side button data set in comparison to this deep convolutional network. But we are essentially because we are using like 50 times less parameters in the model. Yeah. And we also did robustness comparisons on this character trajectory data set. So this, so on the life table, is showing that if we apply different Is showing that if we apply different dropping rates on the data set, so we drop the random data at a different percentage, how well the model can keep up. We can see our model can always achieve quite good performance, stay our performance acquired different dropping rate. And this is a good feature from the continuous time series models for the ODE methods. We can see from the recurrent neural networks, you can get very bad when we have a large. Get very bad when we have a large dropping rate. However, we alleviate this problem by adding the development layer. And on the right-hand side, is using different sampling rates on the train and test data set. So when the sampling rate of the train and test data set are equal, all the models can perform very well. But taking different sampling rates, most of our models failed, but our model still performed very well. Perform very well. This is thanks to the property of the invariance to the time requirementization. Okay. Okay, to move on, we run two last experiments on the something to model some dynamic on the non-Euclidean space to explore questions. Okay, okay, yeah, sure. So I just want to quickly go through. So we use we use some We use some ISO group to output a value to make sure the Brownie mode, our predicted Brownie motion is still on the sphere. So this actually allows us to get much better performance and still maintain on this manifold. And then another is the M-body simulations. Instead of we're actually using a special Euclidian group to model the transformation from Transformation from different points to actually achieve much better performance on this. Okay, so I think this is where I end all my talks on this. I think to conclude, we just want to include some future work because I think essentially we built up this new tool site to construct the development layer. And there are several things we can do. First is generalize the linear vector field to. Generalize the linear vector field to non-linear. Second thing is the expected development under the unit group actually is a characteristic function of the stochastic process. And this actually can characterize the law. And third thing is we can apply our method onto different to explore the geometry of the data and getting better performance. Okay, thank you very much for your attention. All right. Thank you very much. Okay, I have a quick question.