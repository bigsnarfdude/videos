Okay. Hi. Yes. So thank you for coming to my talk. I'll talk. My talk is going to have quite a well, significant, I'd say, overlap with Patrick's talk. I'm also going to, so to begin with, I have the caveat that I don't really work in this area. So I'm going to review certain things that people have done in the past. I'll try to offer, I'll try to talk about some thoughts, at least like kind of of the topic. Some thoughts, at least, like kind of off the top of my head, that I get when I look at these problems, make more of a statistical flavor. So, the difference, I'll go more into some of the statistical details or the mathematical details of these recommender systems. So, you'll see similar things, at least like similar topics to what Patrick talked about. But like he gave a quite a nice introduction or quite a nice overview basically of a lot of different things. I'll focus specifically on a few of them and yeah, we'll see how it goes. Yeah. Goes, uh, yeah, and stop me at any time. So, I think we have a to begin with, the initial parts are actually quite similar. So, you have a user rating matrix where you have you know n users who rate m items and then you rate it on a scale and you have a rating matrix. So, you know, these are things that you've seen just in the previous, like in the last hour or so, and you want to predict unknown ratings. So, this is this is the kind of the problem. And then you also talked about the Netflix price problem. It all talked about it, where you have essentially 100 million ratings, but then you have a real 100 million ratings, but then you have a user rating matrix, which is more than 98% sparse. And then he also talked about content-based filtering and collaborative filtering. So, you know, just to he talked about this, I think, quite well. So in content-based filtering, you for each item, you want to give like some, basically some characteristics of that item. So basic, and then you create a profile for every user. So I think I will kind of skip over these things because these things we just covered. Just covered. So, then again, collaborative filtering is in the previous thing, content-based filtering would be more like in some sense, a supervised, well, not really, that's not, it's kind of a vague use of the term, I said. But in some sense, there you have an explicit idea about each item. You have some characteristics for each item. Here, you don't. So, here it's kind of you have a bunch of different items, and then you don't have an explicit description of the items. So, this is what the idea is. This is what the idea is. So, you know, again, so this kind of model is something that you have seen. So you have the main idea here is that you have some latent vectors for each user and for each item. And then the way the users and the items, they interact, you just take a U, this kind of thing. So each user I has a latent vector UI, and then each item has a latent vector. Oh, sorry, this should be Vs. This is a typo. Sorry about that. And then you take. And then you take an inner product of the user's vector and the item's vector, and you use that to model the rating matrix. So, you know, in more in particular, so I'll talk about this probabilistic matrix factorization, which also you heard. So the main idea is that you again take a rating matrix and you just put a normal distribution on that. And then the nice thing about the fact, so if you have a latent factor-based model, statistically, the nice thing is that because you have a lot of missing values. That because you have a lot of missing values, you can just ignore the missing values. That's essentially what you do. So, when you take your likelihood term, you just ignore missing values. So, this is a nice idea about the latent factor modelings. And then you press priors on these matrices. This is the user matrix and the item matrix. So, here they just, so initially, this initial paper, they just found the maximum posterior estimate using just this, and then they showed that this kind of works pretty nicely. Kind of works pretty nicely. Subsequently, no, no, so you can do a few other things. Like, for example, the ratings might be so the ratings could are usually discrete. They can be from one to five, one to ten, you know, one to hundred, various kinds of things. And you can always transform them into zero, one. Use a particular map, just transform it into zero, one. Yes. Sorry, there is a question. Sorry. Yes, David. I don't mean to interrupt, this is fascinating, but it seems to me we're making sort of It seems to me we're making sort of strong assumptions about people's preferences or tastes living in a Euclidean space here. Does that seem warranted? Well, so when you take a latent factor model, the idea is that you have a bunch of different latent factors, right? And you don't, so you know, you have different factors which could be. The idea I would say here is that each movie would have a bunch of different factors that you don't observe. Now, these could be. Factors that you don't observe. These could be things like how graphic it is, or how much it is on the drama content, or you know, various things. So, in that case, you use this is, I mean, this is kind of very standard, right? When we do a latent factor modeling in statistics, this is exactly a standard item that you have an observation and you assume that there are some lower dimensional latent factors which actually affect your observation, and then you do a latent factor modeling and through that. So, what you are saying is that if you assume that it is like non-Euclidean space, now this is not. Is like non-Euclidean space. Now, this is not exactly clear to me. So, what would that mean? So, what would it mean that, because here you don't, so the idea is that here you as it's kind of you interpret these matrices as the user rating, like, so this is what the user would like. That's how you interpret it. But then, at its core, it's just a latent factor model. Yes, but it seems that it's making But it seems that it's making strong use of sort of linear model type assumptions. That is true. So it does make strong use of linear model assumptions. But I think that essentially linear models have proven to be very popular in statistics, in machine learning, and use very successfully in a whole variety of contexts. And it seems like these things seem to work quite well, work quite well, at least in these kinds of things. Again, these are like, this is an old paper. This is not. This is an old paper. This is not state of the art. But again, like some of the problems, again, like Patrick talked about, where if you want to look at state of the art, it's difficult to find out what people are doing because they don't want to basically reveal their stuff because then other competitors can also use that. So there are all these kind of privacy issues or confidentiality issues to do. But I think that would be my thought on this. I hear you. I hear you. And I'm not disagreeing at all. I'm just trying to sort of think through some of the implicit assumptions here. One thing that would not come out of such a model is something like a max or a min function if somebody will always watch a Johnny Depp movie, no matter how terrible it is. Yes. So I think that in principle, these are naive things in some sense. Well, I'm going to say naive things. Some sense, but I wouldn't say naive things, but these are kind of first things. And we can be pretty imaginative, I think, in this context. And we can come up with a whole bunch of different kinds of models and just to see what works. Because I think this problem is for the wider academic community, this is kind of an, well, I wouldn't say an open problem, but it's kind of an unexplored problem in a certain sense, like again, we was talked about in the previous night. Because if you go and work in a company, you might be privy to their secrets, what they're doing. Private privy to their secrets, what they're doing. But then, as an outsider, you may not know. And then, you know, there could be like in the Netflix Prize example, in that particular case, there was this hybrid system where they used 100 over 100 recommender systems. They just put it together and got something. So it's kind of, I would say, chaotic in some sense, this whole world of recommender systems, because we don't really know what is going on. There are no like journals publishing stuff, and there is no proper studies going. Like, you know, proper studies going on. It's just whatever seems to work for whatever particular Amazon might have something, Netflix has something, you might have a whole bunch of other eBay may have something different. And, you know, whatever seems to work for them seems to be the way going forward. So again, yes, unless I think one of us actually works in industry or we actually work in industry and we have kind of inside information about what is going on, I don't think it is possible to answer. I mean, that's what I'm saying. We could come up with all. I mean, that's what I'm thinking. We could come up with all sorts of interesting things, and then maybe they will work better in one particular example, and then it doesn't work in another example, and so on. So for particular data sets, so you know, in some sense, these are all machine learning papers. And I'll even like in the next few slides, I'll talk about one particular paper, which was, I think, like, this is called Multi-Domain Collaborative Filtering, where they do something, they call it a prior, they call it a post. They do something, they call it a prior, they call it a posterior, but it's not really a prior and posterior. Like they statistically, it would be kind of quite this unpleasing to see what they are doing. They are quite fast and loose with the terms you're using. So my point is that I don't think this is like study in a very well-principled manner. I mean, some things are because in the previous slide, like someone mentioned the Shasa paper, which of course will be a proper statistical paper with like theory in it and. Theory in it and with proper studies analysis. But I think a lot of this is to be. I mean, as a personal comment, yes, I'll talk about this later. Like in one of these papers, when they talk about prior and all, it's quite anyway. I shouldn't criticize people, but I will go on. Any other questions? Sorry, I'm not sure how to see. If you have a question, please unmute yourself and ask because sometimes with this raising hand thing, I cannot see it. This raising hand thing, I cannot see it. I can only see like five people here. And if someone else asks a question, I don't know if it kind of comes to the top of my video list or not. So I'm not quite sure. Raising the hand should put them at the top of your list. Okay. All right. So let's continue. So this is the kind of basic probabilistic matrix factorization. Then they also propose the same year, they propose something called Peiger. The same year they proposed something called Bayesian probabilistic matrix factorization, where they don't really change a lot of things. So, you know, what they do is instead, basically, they consider hyper priors for the parameters. And they also sample from the posterior instead of just finding the maximum posterior estimate. So basically, basically what they do. So earlier you had, you did not have this matrix lambda u inverse, you just had a scalar, a diagonal matrix, basically with one particular scalar times identity matrix. Times identity matrix, same for the item rating matrix. And now you just put a Gaussian Vishard hyper priors, and then you do give sampling. I mean, sampling-wise, since these are all Gaussian things, things are conjugate, it's not very difficult to sample. And then you do sampling, and it turns out that this works better. Now, this is not surprising because you are making the model just a lot more flexible. Earlier, you just had a diagonal matrix of everything. So, you have statistically, you do a more well-principled thing. A more well-principled thing, a way of doing it, and this they show that it works better. So, this is the thing I mentioned: this multi-domain collaborative filtering. So, here you assume that you have a bunch of different domains. So, you have, you know, use movies which could be partitioned into different domains, or it could be some other things that are K different domains, capital K. And then you have a user item rating matrix again for each domain. So, you have MK items in the kth domain. So, you have now used the rating that user I give to item J. User i give to item j, and then you want to do the same thing, you want to factorize it into k different domains, however, you want to share information in some sense between the different domains. So, you know, so for every domain k, you have you, you have a user rating matrix, uk, but then you assume that the ith column always represents the same user, but then in different domains. Okay, so what they do is they have a likelihood over observed ratings, which is the same as what we had before. Again, so this. What we had before. Again, so this is, you might do things like you might transform the rating to 0, 1, the interval 0, 1, transform this to a transformation, and so on. And again, they all rely on this kind of latent factor models because then you can easily ignore things that are not coming into your model, the missing information. So now I put priors in quotes here. You'll see in a bit why in the next slide, basically. So you put priors over the user matrix and the item matrices. And the item matrices. But then you also want to share the information. So, what they do here is that you have a prior uk, I mean, on each uk for the k domains. But then you also take all of them together and then you basically stack them into a matrix. You vectorize each of them and then you put it together as you put them in a column-wise manner, right? And then again, you consider a prior. So, basically, what they do is they consider two priors. And the way it's again, not very statistically well. Again, not very statistically well justified, but then they just add it to the likelihood term being optimized. And I'll show you in the next slide. So, what they do is they consider, yeah, like two priors, essentially. And this is a matrix variant normal distribution. So, you have what's going on within the domain. So, within each domain, and then you have what's going on between the domains. So, within each domain, you're assuming that the way the users are, I mean, the different for the user rating matrix, the different components are independent. Different components are independent, which makes sense because that's what you want to do. But then, in the for the so now you also have what's going on between each domain, and that you're capturing using a matrix omega. So, omega is modeling the relationship between domains. Now, in a proper manner, you would not really put these priors here on UK, would only put the prior here, and then you would also put a hyper prior on omega. They don't do that. What they do. They don't do that. What they do is they simply consider what they call the log posterior, where this comes from the likelihood, and they just take both priors basically. So they take the priors for the u case, and they also add the prior for the u together, extract one, stack together, and then they optimize it over all the parameters and the hyperparameters, including omega. And then they describe an algorithm to do it. So this is again, as a statistician, it might be a bit offense. Well, not offense. Might be a bit offens, well, not offensive, but it might be a bit strange. And I believe that actually, because both of these are Gaussian, so you can actually, it's not too difficult, I think, to come up with a prop prior for you, because you're basically adding two Gaussian terms. Basically, in the likelihood, you're just multiplying two Gaussians, right? So this would still be, I think, a Gaussian, but you can actually come up with the prior mean and the variance. But they don't really do that here. They just describe the optimization and then they go forward. The optimization and then they go forward with that. Yeah, so again, so this now I'll talk about something that Tim, who was a former PhD student of David, he did in his PhD dissertation. So what he did was he considered something called multiple domain Bayesian matrix factorization. So he just combined multiple domain collaborative filtering with Bayesian probabilistic matrix factorization. So, you know, you So, you know, you now assume that each domain has its own user, latent user feature matrix, an item feature matrix. So, one thing that you did differently from multi-domain collaborative filtering was that you thought that each the dimensionality decay of these matrices, they vary across domains. So that means you have for uk is in r dk cross n and v k is in this. So then you have in each domain, you might have different dimensions. Domain, you might have different dimensions for the user: latent feature matrix, feature vector, and then item feature vector. Now, in the simulations, most of them he actually chose them to be the same. So he defined it in this way, but then in the simulations, it was all chosen to be the same. So I think in practice, it doesn't matter so much what you do. Things seem to work. And then you do something very similar to what you had before. You look at the likelihood. So you put, look at you, look at the. So, you put look at you look at the rating matrix, and then you put priors on these things in a very similar way. Now, the way you would put priors on the user matrix is you again, you stack them together, and then you put a prior on that. So, now you have, so this is the prior basically, where you stack everything together, you put a prior on that, and then you take your hyper parameters, sorry, you take your parameters and put hyper. Your parameters and put hyper priors on these guys, and this is kind of what was being done. So, the way you would do posterior inference, again, you sample actually from the posterior. It's interesting, the sampler is interesting in the sense that in certain steps, since you're using deep sampling, you can do a bunch of sampling in parallel. So, in that when you do deep sampling, when you update certain variables, you can basically do that in parallel. Because here, remember that you have really like high-dimensional things. You can have pretty high-dimensional things because you have a lot of users, a lot of Because you have a lot of users, a lot of items, and so on. Well, by a lot, I mean you have quite a lot. I mean, because you would need to invert certain matrices and so on, which might be expensive in certain contexts. I mean, of course, but there exists methods to kind of make that faster, to make matrix inversion faster and so on. So that's being done. And I'm going to kind of summarize some experiments that he had. So I'm going to kind of I'm going to kind of summarize some of the main things that we had, and so then I'll spend a few minutes talking about other aspects. I think that, you know, from more from a statistical perspective, what could be interesting, how we can use. So these are all kind of seven, eight year old things, well, six, seven years old things. So, you know, kind of more modern development statistics that could be used potentially to tackle these kind of problems. So, you know, here you have. Uh, so you know, here you have again a so here you have a hundred thousand ratings from a thousand users, and then you have different genres for the movies. So, what he did was he constructed a database basically. So, you want to have movies, so movies can be in multiple genres, like they can be action, comedy, or something. So, but then what he did was he picked movies that fall uniquely into one genre, so only action or only the comedy and so on. And then he got a data set which would look like that. Okay, so you had. Okay, so you had 32 action movies, 210 comedy movies, and so on. And then you had ratings on all of these things. So he looked at a bunch of different things like to compare. So you had multi-domain collaborative filtering as before. You could have Bayesian probabilistic matrix factorization, but you do it independently on each of them. You could have a pooled one where you just ignore that you have different domains, you put everything together, and then you run inference. And you also do the multi-domain. And you also do the multi-domain Bayesian probabilistic matrix factorization. And in summary, so you know, what he did was, I'm not going to show you the results again because, to be honest, these are not mine. But what he did was he selected 80% of the ratings as training and then 20 as test. And you learn it for different dimensions. Unsurprisingly, if you increase D to up to 20, the performance of Bayesian models continues to improve, but one might imagine. It continues to improve, but you might imagine that if you increase the dimension too much, then you might overfit and don't improve there. Some of them, the MCF, the multi-domain collaborative filtering model, does overfit when D is large. But it turns out that the pooled Bayesian probabilistic matrix factorization does actually quite well in this context. It's not much of a difference when I saw the results. That's actually, I have the other references, but I could mention like a few things here that when I see these things that seem interesting. So, you know, one thing of That seems interesting. So, you know, one thing, of course, is that you don't know how many dimensions you should have. So the dimension D is unknown. But in statistics, in the last few years, there have been a lot of last decade or so, there's been a lot of work on this infinite factorization models, where you basically put a shrinkage prior. You have, you assume that you have infinitely many factors. So basically, your D could be infinite, and then you automatically select D by putting a shrinkage prior on that. So that's, I think, one thing that's doable. Doable. Again, the thing I struggle with is that I don't want to recommend things to people because I don't know how interesting they would be. So I tend to think of from a more statistical perspective, where my perspective would be that would it be an interesting methodology? Could we prove some nice results, or at least could we intuitively understand some nice results on all this? That's not very clear to me. Very clear to me. So I think that again, what's more, what's really going on, to understand what's going on, we need to really talk to people working there. And in some sense, these are all, again, I might be biased as a statistician, but I think that sometimes you observe really a small improvement, like one percent or half percent improvement. And that's actually a big improvement when you look at from an industrial perspective because you have a lot of users and a lot of Perspective because you have a lot of users and a lot of ratings and so on. But then, one thing that I do struggle with sometimes is to understand. And I know Patrick also mentioned this reproducibility thing, because it's when you really have such a minor improvement, is that even reproducible? Maybe in a particular competition, it works great. You know, you might beat in your Netflix example, you might do better, or in a multi-letter set, you might do better. But then, would that be really something that you can generalize? Really, something that you can generalize. So, definitely, I do not think that these things can be generalized. I mean, I think that what's, I will just stop sharing. I think what it would be quite interesting would be to come up with some sort of, or we don't have to necessarily come up with new theory, but to kind of encompass all this into a statistical framework. And this might have already been done. I do not know. Because if there is a JASA paper on recommended system, certainly like things. On recommended system, certainly, like things, statisticians are also working on these kind of problems. So, in some there is, I would say, a disconnect between statisticians and people doing this kind of problems, working on this problem. These are definitely very, not just applied, but very, very much application-driven, right? So, you know, what works for Amazon may not work for Netflix. Like, Amazon might, you know, like if I buy. You know, like it recommends me if I buy a heater, it tells me that you buy a blanket or something. And that may not work with you know when I watch Star Wars, and I will also want to watch, I don't know, something Star Trek. So that may not work here. But yes, that's kind of my talk. I had thought of doing more and reviewing other things, but kind of I'm glad I didn't because Patrick did that very well. So then it would have been. Very well. So then it would have been like a lot more overlap with things you did. But yes, I mean, these infinite factor models can be tried out. Yeah, that's, I think, more or less what I will have to say. Thank you for that's great. And it's a fascinating area. And I think that there's still lots of potential work there. Did you or Patrick or perhaps anybody else see any attempts to try? Any attempts to try and use discrete choice theory modeling for this problem? I did not. Okay. I'm sure it will be too simple. Discrete choice models are just going to be too simple for this application, but nonetheless, it's one of the standard tools. Another one is another sort of standard tool is to have a A buy graph in which you have users on one side and movies on the other, and you try and predict the presence or absence of an edge between them. And I mean, I think that the thing is that these are all very interesting things, but you know, to in some sense to be motivated to do these things, I think that we really have to be in working for a company like who actually cares about this. Because if we do it and write a paper about that, I don't. It and write a paper about that. I do not know that whether what would be the appeal per se. If you're if you're working as a consultant for a company or if you're working in the company themselves and then we try some of these things and then we show that it works, the thing is, we don't have to make a big improvement, right? We make like one, two percent improvement. And that's still in terms of like actual monetary, financial thing, that's a huge thing. So I think that in some sense, yes, if we actually, these things may have already been done, but you know, as May have already been done, but you know, as they're just not publicly available, which would not be surprising. I mean, when because, like, he's Patrick said, there were 107 things, and of course, like these probably combine various different things. And this was already like, I don't know, 10 years back. So, since then, I'm sure this has grown to, I don't know, maybe 200 or even more. And we don't really know what is going on. Yep. I certainly agree that for any particular application, the method will have to be. Application: the method will have to be hand-fit to the problem, and so it's not going to be an off-the-shelf solution. Yes, at the same time, I think there's real value in trying to think through how to structure such a problem, and that will have that will be useful to the entire span of recommender systems, whether it be recommending movies, advertisements, whatever. Yes, so doing it in a more formal way, that I definitely agree. That I definitely agree. So, this is the thing that I find probably missing or lacking: that there does not seem to be kind of an overarching framework or kind of an umbrella for doing these things. They seem to be quite disparate in a certain sense, like there are different things going on. So, yes, to be able to even connect the different methods that people are using, because some of these things, like the matrix factorization or whatever, statistically, they are very straightforward methods, but then, you know, they get sighted. Good methods, but then you know, they get cited thousands of times, they work successfully, you know, they win prizes and so on. But then maybe we can do better. I mean, there are definitely things like, you know, theoretically, maybe we can also study these algorithms because all these things I think asymptotic theory probably would apply in the state in statistical asymptotic theory because everything is very large here. So, then to come up with, you know, in a Bayesian framework, put some Know in a Bayesian framework, put some you have something that is sparse, so you might just consider like some sort of shrinkage estimates to maximize information you have. And also, you know, you know, information theoretic framework. I mean, again, these are just random thoughts. Oh, excuse me, I'm gonna talk. Okay, I'm sorry about that. Okay, no problem. So, yes, in an information theoretic framework, if you try to If you try to study these things to see how do we maximize the amount of information we can get from these really sparse examples, which might also be, and I, yeah, and I'm sure there are people here who know a lot more about these things than I do, so I'm just throwing out thoughts. Well, let me ask people here: if you picked a very narrow genre, say horror films. To what extent would people agree on ordering the horror films? Would they have the same, would the same 10 movies be in everybody's top 20? Or would there be enormous disagreement between people based on individual taste? I think there are also other aspects to take into account. Like, for example, we assume that users are homogeneous, which is not going to be true in practice, right? Users probably In practice, right? Users probably, you know, people from a certain country or in a certain demographic may like certain movies a lot better than others, or people with a certain kind of background, cultural, social. Sort of where I was going with my question, because if it turns out that everybody's top 20 is different, then that tells us that users are completely heterogeneous. But if it turns out that, you know, there might be, you know, the same 10 movies appear in everybody's top 20. The same 10 movies appear in everybody's top 20 that suggests that there's a certain unanimity of opinion. So, what you're saying essentially is that we take it because we have additional information for users. We have some covariates for each user. We know where they live, we know how old they are, we know their gender, we know, I don't know, we might know a few other things about them, like Amazon, you have a profile. So, how do you incorporate these covariates to also improve your predictions? Now, this is not something at all that was done in this factorization ideas. This may have been done, I mean. May have been done. I mean, I would my guess would be that these are done because I don't think that Amazon and whatever Netflix are stupid to not do this. I mean, they are obviously very smart the way they recommend system, build this recommender system. Yeah, demographic filtering is definitely a thing that people have put a decent bit of thought into. It's not quite as popular as some like collaborative filtering is, but like people have definitely done it. Like, I know there's even like social media linkage models for like. Models for, like, you have people linked together, and you can kind of like track recommendations that way. So, that stuff's not so cool. Yes, so you might also, you know, you might have a network where you know who is friends with whom and then use that information. I mean, that's the thing, right? There are like all sorts of other information that you might collect and use. And we are looking at in these papers or what works a very narrow band of, a very narrow amount of information. We just have the ratings matrix. That's all we're trying to model. But in practice, there would be a lot of other information. Practice, there would be a lot of other information, yeah. They would also be like for cross-domain recommendation, at least you kind of run into the same problem where, like, you know, Tim Howe's thesis was like, in order to do cross-domain recommendation, you got to take movies and like segment them by genre, where it's like, what you really probably want to be doing is like, here's movies and here's books, and we're trying to learn about one from the other. Yes. And that's the data set just doesn't exist unless you work at Amazon. Yes, that is true. So, you know, you know that this user likes these movies and they likes his books. likes these movies and they like these books. And you know certain things about the user. You know that, I mean, based on the movie, actually, you can also make the movie have certain characteristics, which mean that they might have this kind of taste and then they might predict that you like these books or whatever yes, like, but then you have to really belong to someone or like belong to an organization that access to all these kinds of different kinds of data, which would not be good at breakfast. Time check. We're on the half hour. We're on the half hour now. I think this is turning into a conversation among just three of us, and I don't want to bore other speakers, other participants. Let me use Anru as a foil. Anru, would you unmute for a moment?