So, the most important thing in the talk is maybe my co-author Miria Marman, who's my PhD student, and she did most of this work. I only played a minor part in this. I will explain all the words there. And here is a picture, non-AI generated, but just plotted, of a customer curve. And geometrically, that's the kind of object I'll be talking about. I'll be talking about. Okay, so in part one, I will explain all the words in the title, what I mean by them. And there will be a field, small k. It's of characteristic zero. And then I will fix what I'll call a numerical semigroup. And that means our fixed numbers, P1 up to Pd, natural numbers, which have the property that the monoid they generate under addition inside the natural numbers is cofinite. You get all the natural numbers. Fine it. You get all the natural numbers but a few, which is equivalent to the fact that the greatest common divisor of these natural numbers is equal to one. And well, then how do you get algebraic geometry? Well, by passing to the semi-group algebra, so the monoid algebra, which is the subring of the polynomials in a variable t generated by t to the p1, t to the p2, up to t to the t3. Up to t to the td as a k algebra. And that will always be the coordinate ring kx of such a caspital curve as you have seen on the front page. And the main example for me is just the example where p1 is 3 and p2 is 2. And then, of course, you get the classical cusp given in the plane by x squared equals y cubed. Okay. Okay, so that's the first bit of ingredients. And well, the main theorem of the talk, as you can guess, is that the ring of differential operators of that commutative k-algebra, and I will tell you what that means, is a Hof algebra. What precisely is a left-hof algebraid, and under nice conditions, it will be a full Hoffalgebroid. Okay. Okay, now what does that mean? In a nutshell, it means that the category of modules over this ring DA has a closed monodal structure. Well, that's not good enough. More precisely, it has a closed monoidal structure such that the forgetful functor 2A modules is compatible with that closed monoidal structure. Or more concretely, if you give me two DA modules, M and N, you can either cancel them over A, and that Tancel them over A, and that has again a DA module structure, or you can take the A linear maps from M to N, and that has a canonical DA module structure, and all of that fits nicely together in a categorical set. Now, when it comes to proving this theorem, I was not aware that Tom Nevins had been influential in this conference, so that this fits perfectly. In fact, I think the only other paper so far. The only other paper so far in which both the names, the words differential operator and Hopfider Boid come up, is a paper by Ben Svian Nevins from 2004, I think. And the theorem could be derived from one of the theorems in that paper. So they work with what they call good corn-Macaulay varieties, and they work in a topological setting with pro-coherent sheaves over such good corn-Macaulay varieties. And they consider And they consider a dual object the jet algebra. And they prove that this carries a Hof algebra structure. And then you can dualize this, and you have to work a little bit. And then you find out that what I say up there is true. You could also derive this from a famous paper by Toby Stafford and Paul Smith. The latter one, I'd expect, would be here. He's not, unless he's hiding very well. Unless he's hiding very well. And so they studied more generally rings of differential operators on curves. And they proved that this ring DA in the setup that I'm considering is actually Morita equivalent to the ring of differential operators on the affine line, which is the normalization of that curve. And then you just have to check a few things to make sure that indeed you realize that this Merita equivalence can be made monoidal. And then by And then, by some Tamaka crime reconstruction result, which is kind of hidden in this statement here, you would also recover that whole fighter right structure. However, that's not what we do. We instead invent kind of a new machinery, which is about descent theory for Hopkalgebraid structures. And we work with the smooth locus of the curve, which is then given by the Laurent polynomials. Given by the Laurent polynomials, and as I will explain a bit later, the differential operators on that always have the structure of a Hopf algebra. And then the question is whether that kind of restricts to the sub-algebra which describes the singular curve. Okay. Good. So let's get to definitions and the notion of a Hopf algebra. So what's a Hopf algebraic? It's first of all an augmented A-ring. A ring. So A is my commutative algebra here. There's a more general theory where A could be non-commutative, but I simplified definitions today a bit to make them easier. So A is this commutative algebra, and H is, well, it wants to be maybe an algebra over A, but I don't assume A is in the center of H. So I start with any K algebra map from A to H. A is commutative, H might be not. The image of eta might not be in the center of H. Not be in the center of H. And what do I then mean by an augmentation? I mean a splitting epsilon back from H to A, which is not supposed to be an algebra map, but it is supposed to be a linear. Okay, so this means basically, okay, A can be treated from now on as a subring of H, and there's a map back, and the kernel of this map back is a left ideal, but it's maybe not a two-sided ideal. But this does mean. But this does mean that I get an action of H on A. So A always carries an A module structure. And the simplest case would be the one that I'm interested in, where H is simply some intermediate ring in between A itself and all the Keylinger maps from A to A, all the operators on A. Okay, so that's the first bit of ingredient. And now, of course, we want some sort of code for. And now, of course, we want some sort of co-product. And to do this, from now on, we always treat H as an A module with respect to multiplication from the left. Okay, so we multiply inside capital H with small eta of A from the left. That turns H into a module over the commutative ring A. So I can form the module H tensed over A with H, and I can demand that this carries a coproduct coming from H into this. Co-product coming from H into this, so that I had, in the ordinary sense, that we all know, a co-algebra over A, whose co-unit should be given by that augmentation. Okay, so far so good. And well, at this point, of course, the next natural thing to do is to demand that this delta is an algebra map. But the problem that was first addressed by Swedler in his 1974 paper. In his 1974 paper, is that H tensored over A with H is not an algebra. You would like to now define component-wise multiplication and make H tensored over A with H into a ring, but you realize since A is not assumed to be in the center of H, that doesn't work. So if you take an element, a tensor, and represent it in two different ways and you multiply, you might get different answers. You might get different answers. So, therefore, it's a bit more tricky. And Swedler already understood what has to be done. And initially, it looks a little bit a weird condition. He said, well, in good cases, that co-product will land in the universal subset of H tensor over A with H, where this product is well defined. And what is that universal? And what is that universal subset? Well, it's the set of those tensors where if I act from the right on the first component, I get the same answer as if I act from the right on the second component. Right? So I said I consider H as an A module by multiplication from the left. And this tensor product over A is about multiplication by A from the left. But I still have a bimodule structure here. I still have a bimodule structure here, an A bimodule structure on H tensored over A with H, which is given by the right multiplications here and here. And I can take the center of that bimodule. And Sweder showed that this is the universal subset where that product is well defined. So this was later generalized by Takeuchi to non-commutative algebras, but for today, this is good enough. Okay. Okay. Now, once we have that, hold on. Yes. And once we have that, we can now define what we would call a bi-algebraid, which is the condition that, well, this is now an algebra. I think I said that. Okay, good. And I should also point out: Sweder didn't call them yet bioalgebroids. This question just came up in discussions with Frank and Dan. With Frank and Dan. Swedler called them cross-A bi-algebras. And also later they were called, sort of the corresponding Hopfalders were called Hopf cross-A algebras. But Ying Chu, in the connection with Poisson geometry, rediscovered the same notion and he coined the term bialgebroics, which sounds much more sexy. Hence, that kind of gets more and more stuck nowadays. Okay. Good. Good. And well, now you would maybe want to define a Hopfalgebroid by saying that a Hopfalgebroid is a bi-algebroid with some sort of antipode S with these and these axioms. And Swedler has already thought about this. So in the Sweder paper, there's something called an S-map, spelled out E-S-S-map. And that is kind of a precursor to an antipode. And in fact, fortunately, Fortunately, Lou is no longer here. So she had an initial stab at this in 96, I think, still making some assumptions. And from then on, people thought more and more about sort of a good definition of what it means to be hop in this setting. As pointed out by Dan, there's a whole zoo of generalizations here. And Peter Schaunbuck gave a definition. Gave a definition which is not about antipodes. He said to be Hopf is about what I would call the Galois map and what was called the junction map in some other talk. It's about the fact that a Hopf algebra is a torsor. It's a Galois object. It's a Hopf-Galois extension of the ground field. And if we generalize this to this setting of biologists then the condition becomes that this map here should be bijective. Should be bijective. So, if you work with bi-algebras over a field small k, then the bijectivity of this map is equivalent to the fact that the thing is a Hof algebra as an antipode. You can just construct the antipode from the inverse of this map. If you have a bialgebroid, this doesn't work. So, we can write down counterexamples of bi-algebraids for which this map is bijective, but we can show there is no end. But we can show there is no antipode. Okay. But as I said in the beginning of the talk, a few other equivalent definitions of being a Hopfalgebra do carry over. And I think for me, the most interesting one is this categorical one in the spirit of Tanaka-Crine reconstruction, which says, okay, if you give me any bi-algebraid whatsoever, then the co-product will give me a monoidal category structure. A monoidal category structure on the left H modules. And more precisely, a bi-algebraic structure in exactly that definition of Swedler is equivalent to the monoidal category structure on H modules, such that the forgetful functor to A modules is monoidal. And when you're in that situation, it is always true that the category of H modules is closed monoidal. There's always an internal home functor, but up. Functor, but a priori, this is not compatible with the forgetful functions. So it's not given by taking m and n and forming the a linear maps from m to n. It's rather the h linear maps from h tensor m to n. And it's precisely in this situation where this becomes isomorphic to the a linear maps from m to n. So that at the end of the day, in that paper by Scharnburg, he also pointed out that indeed, having a Hof algebra really means Having a hot fire deroid really means this. It means having a nice closed monodal structure on the modules of my ring H, such that the forgetful functor to that modules over that subring A is compatible with that closed monologue structure. Okay. Very good. Any questions up here? Um just interrupt me if you want. Mean by g bracket one times g bracket two. What are g one and g two in your definition? So, g okay, so this is the Sweden notation for the copra. I'm sorry, this this is something um yes, this is a this is a problem of people doing hop algebras. Where is it? Here we are. So if you have an algebra, right, you have a multiplication map mu, which goes from h tensor h to h. H tensor h to h. And how do you write it down? You take an elementary tensor x tensor y and you send that to xy. You write the multiplication just as concatenation, and everyone is happy. Now, when you have a core algebra, you have a map the other way around. Now, you want to say this takes an element x and kind of splits this up, but that's not true, right? It could be a sum of elementary tensors here. Of elementary tensors here. And Swedler and Heinemann came up with this notation in the very, very early days of the theory. So they said, oh, let's have these small numbers here. And whenever in my papers you see these small indices, that means the code product has been taken. And it's a clever way to kind of write down the outcome of applying delta to an element, which works very well with multilinear algebra, because Multilinear algebra, because you can now, by coercutivity, do this again and twist things, put them into linear maps, and so on. And if you did that for 20 years, it gets very, very natural. But it is true that anyone who sees this at the first time thinks, no, yeah. But okay. So long story short, long story short, what this means is that this map is you apply the co-product to the first leg. You apply the co-product to the first lag, then you are in H tensor H tensor H, and then you multiply the second and the third leg. So, for example, if you're used to topological groups, there this comes up as the multiplicative unitary. So, this map comes up in many, many, many contexts. Okay. Thank you. Good. Okay. So, suppose we know what a hop filter broid is. Hop filter bright is. You need to know what a differential Hopbright is. And well, so this is the notion of Brotendiek that I'm after, as it appeared in Guan Shi Wu's talk at the first day. So I have my commutative algebra A. And now, of course, every element of A defines a linear map from A to A. You just multiply by that element. And those multiplication operators, I will call differential operators of zeroth order. Operators of zeroth order. And Brotendieck had a very simple and ingenious idea how to generalize the notion of a differential operator from analysis to algebra by an inductive procedure. So he says, okay, we define the A ring of differential operators to be a filtered sub-algebra of the K-linear maps from A to A. And I define the filter. And I define the filter pieces step by step. The zeros filter piece should just be A identified with those multiplication operators. And then if you have already constructed the differential operators of order up to n, you define the differential operators of order up to n plus one as all those k-linear maps from a to a, which when I take the commutator with the multiplication operator, produce a differential operator. Use a differential operator of order at most n. Okay. And in this way, you can give me any commutative algebra, and I can produce this non-commutative ring. And the question is, what do we know about this ring? Okay. So in particular, to illustrate this, let's think about differential operators of order at most one. So we now have a k-linear map capital D. Have a k-linear map capital D from A to A, such that if I take the commutator of D with any element small a and capital A, I get a multiplication operator by some element of A, which I denote by C up there. Okay. And now I can define a map, which I denote by partial, which maps the element A that I've put in to that element C. Okay, um, so in other words, that is given by taking small b to be one up there, so you subtract from d the value at one, and uh, then an immediate computation will convince you that what you get is a derivation of my algebra A. Okay, and what does this mean? It means, if you think a little bit further, that we completely understand the differential operators. understand the differential operators of order at most one. They are a direct sum of multiplication operators. So think regular functions on an affine variety and derivations, think vector fields on that variety. But from here on it gets really hairy. So in particular, even on a very nice smooth variety, or smooth manifold, if you want, you could also take A to be the smooth functions. Take a to be the smooth functions on a compact manifold here. Even in a very nice setting, the embedding of the differential operators of order at most n minus one into the differential operators of order n will not split. So this is what in global analysis leads to principal symbols. There's a filtered algebra. I can only talk about the top degree terms, so to say, of a differential operator of order at most n. I can't. Order at most, and I can't split it into graded pieces in general. So, this filtration is an actual filtration. It does usually not come from a grading except in very simple situations. Okay, good. And now that we know what a Hopf algebra is and what differential operators are, that theorem is clear. So, I claim that the ring of differential operators are. So, I claim that the ring of differential operators on those curves has such a structure. And I said I proved this as a special case of a descent-theoretic statement. And the statement I have in mind is the following. So suppose you now have at first the bigger Hopf algebra G over an algebra extension K of A. Okay. Then I can look at what's noted by capital H up there. By capital H up there, I take those elements of G, which map elements of A, the sub-algebra of K, into capital A again. Okay, and this way I produce some sub-algebra H of G. And then I can define certain a linear functionals from H to A, namely those which are kind of the span of the linear functionals which are given by evaluation in elements. In elements. So you have a map from A tensor A to the A linear dual of H, which takes a sum over I A I tensor B I to this linear functional here. Okay, so I call this R. And I say that D A is R locally projective as an A module if for every element Uh, if for every element, every single element small h in capital H you find some tensor in R tensored over A with H, which I consider as a subset of the alien maps from H to H. So in this way, I think of this as a single map pi from H to H. And I find such a pi which fixes H. So by an elementary induction argument, So by an elementary induction argument, you will see that this means that for any finitely generated submodule of D of A, I find Aπ, which when restricted to this submodule is the identity. And if R would be the whole dual of H as an A module, that concept has been introduced under many, many names by many, many people again. So universal torsionless, flat. Blatt and Wittag Leffler came up, and a few others. And I think, in that generality, it goes back to Wiesbauer, but I'm not totally sure. But that's what we need. So it's about a kind of projectivity, but only locally on a finitely generated submodule. And when you have the usual diagram that defines projectivity, where you kind of lift a given map to a Given map to a bigger module, then the map that you construct is not any a linear map from H to H. It lies in this kind of finite rank operators where all the linear functionals come from my set R. Okay, so pretty subtle module theoretic condition. And so on the proof side, sort of the main theorem of our paper is that if in the situation up there, I mean that the theorem in the paper is more. I mean, the theorem in the paper is more general. I focus on a special case here. If H's are locally projective, and if the multiplication map from K tensor AH to G is an isomorphism, then the Hopfy-Drauk structure will uniquely restrict to a Hopp-Fider-Royce structure on H. Okay, and we apply this with the normalization K of the coordinate ring A of our singular variety. Singular variety. And well, it's a kind of a strange, usually in descent, you want a into x to be faithfully flat, and then you derive stuff. That's not given here. We think mostly about the embedding of the smooth locus of a variety into the variety, and then the corresponding dual map is a localization which is not flat, which is not faithfully flat. It's flat, of course, but not faithfully flat. So we rather demand something on the module. Okay, very good. Okay, very good. Now, now you know all the words. So, why do I care? So, I personally cared because of the Cartier-Milner-Moore theorem that came up two talks ago. But let me relate a little bit more to that story by going two steps back. So, what happens in the smooth case? In the smooth case, this theorem is trivial. In the smooth case, it is In the Smooth case, it is well known that the differential operators over a coordinate ring in characteristic zero are what's called the universal enveloping algebra of the Lie algebra or Lie-Reinhardt algebra of derivations of A. Okay, the vector fields on my right. So what is this universal enveloping algebra? Well, it's the universal algebra generated by generators labeled by the derivations and by the elements of A. And these generators should satisfy all the relations. And these generators should satisfy all the relations that you expect. Meaning, the multiplication of elements of A inside that universal enveloping algebra is the multiplication in A. The commutator of derivations in that universal enveloping algebra is the Liebracket of derivations. The commutator of a derivation with an element in A is what you get by applying the derivation to A. And the multiplication of a derivation by an element in A is the A module structure. element in A is the A module structure on the set of derivations. Okay? So you implement all the algebraic data that you have inside a universal algebra, which was introduced by Reinhardt in 1963. And well, Groteneag slash Swedler, in Grotenig, it's pretty hard to find. In Swedler, it's explicitly, more explicitly stated, showed that there's always a canonical. That there's always a canonical map, of course, right? But because this is the universal algebra and the ring of differential operators definitely satisfies these properties, there is a map. And if the variety is smooth, this is an isomorphism. So this part is relatively easy, follows from the PBW theorem or universal enveloping algebra of Lee-Reinhardt algebra. Okay, now from here, this leads pretty quickly to the Nakai conjecture, which I want to. To the Nakai conjecture, which I want to mention in passing. So, but first, the most banal example, maybe, which is the one that maybe people care about here is the Weyl algebra. If your variety is just the affine line, the coordinate ring A is the polynomials in one variable T with coefficients in K, the ring of differential operators is the first violat. Okay, this is one case where there is a grading. You can take any differential operator and write it in graded pieces. In graded pieces. But once again, this is a coincidence, which, even in the smooth case, won't be good in general. Okay. And well, the Nakai conjecture states that actually this map from the universal enveloping algebra to the differential operators is an isomorphism if and only if x is smooth. Yes, x is a fine. x is affine. X is a fine. And characteristic zero. I said that in the beginning. It has to be stressed in characteristic P, a totally different story begins. Okay. So I say sort of because in 1961, those words didn't exist yet. If you look into Nakai's paper, you don't see any of that. You first think, hmm, is this really the right paper? Yes, it is. But clever people thought a bit later about it and said, this is what Nakai conjectures. Is what Nakai conjectured, and I think he had no say in this. From this moment, it was Nakai's conjecture. Okay, so what does that mean explicitly? It means kind of it's the smooth case where that ring of differential operators is generated in degree one. And in the singular case, this ring becomes really crazy. And certain differential operators pop up in degree Z, two, in order two, three, et cetera, which you cannot build out of multiplication. out of multiplication operators and derivations. Okay, now one might ask what smooth means. Okay, and there is one thing which I like, which is a, I don't know whether that's the right reference. The first reference where this is explicitly in, I would say, is the paper by Hochschel, Kossen, and Rosenberg, where they compute the Hochschl homology and Hochschlt homology of coordinate rings of smooth affine varieties over per. Of smooth fm varieties over perfect fields. And they point out that smoothness, meaning regularity of the local rings, is equivalent to the fact that the Kerlo differentials of my fine variety are a projective module. So I'm talking F-1 variety, so I assume irreducible here before Alexei asks. Otherwise, this is false. But in the irreducible case, smoothness just means. Smoothness just means the cotangent bundle is a locally trivial vector bundle. Its sections, omega one, will be finitely generate projective. And once you have that, the dual of a finitely generated projective module is also finitely generate projective. And because the Taylor differentials are the universal module with a derivation, so they represent the functor of derivations, that dual is just all the derivations of my commutative algebra. Of my commutative algebra. So, in the smooth case, the derivations of A form a finitely generated projective module. And then there's another famous conjecture in commutative algebra due to Zariski and Lippmann, independently, as far as I know, which says that's also an if and only if. So, this sounds much more tempting to try to prove, but I don't think this is easy. Otherwise, someone would have done it already. Would have done it already. In fact, this one follows from the Nakai conjecture. And in our paper, we kind of include that step. So long story short, the smooth case, when x is a smooth fine variety in characteristic zero, that means omega one is finally generated projective, the tangent bundle, the derivations, is finally generated projective. And then the Poincaré-Bilkov-Witt theorem proved by Reinhardt implies Proved by Reinhardt implies this fact that I had on the previous slide that the ring of differential operators is the universal enveloping algebra of the Lee-Reinhardt algebra of derivations. Okay. And with that preparation, I can now formulate my reason why I wanted to know this. So in 2010, Ike Modij and Janus Schmurzun proved an analogue of the Cartier-Mill-Namore theorem for Hoch-Halder Broids. And it sounds exactly as you expect. So we're on characteristic zero, we have a co-commutative, and well, it was called connected this morning. I call it conial potentially, which really means the same. We have a co-commutative conial potent Hopfalderbroid. But we need a certain homological addition here, rated projective. So there's an analog of what was called the core radical filtration this morning. Others would call it the primitive filtration. Others would call it the primitive filtration. And what they need is that the associated graded A module is projective in all its pieces. And if that is true, then what do I can merge and prove that my Hopfard must be the universal enveloping algebra of the Lee-Reinhard algebra? And the Lee-Reinhard algebra is the primitive elements. L is the Lee-Reinhard algebra. So the statement is: give me a So the statement is give me a connected slash colliopotent co-commutative Hopforderoid. Assume that the associated gradient with respect to the core radical filtration is projective as an A module. Call L the primitive elements of that Hopf algebra. So those where delta of x is 1 tensor x plus x tensor 1. This will always be a Le Reinhardt algebra over A, and the Hopf algebra is the universal enveloping algebra of that Hopf algebra. Of that hot vyto point. L is the set of primitive elements. So you take, there is a board here. Is there also a piece of chalk somewhere? Yes, exactly. Maybe not. Okay. Anyway, good. Okay. Um So, yeah, at this point, I said, well, take these rings of differential operators for a singular variety. And if you start computing with examples and toy a bit around, you really see that they want to be off algebraids. And this would not contradict that theorem by Mordak and Mirchun because being not smooth means they're not greater projective. They're not greatly projective. Okay? So basically, I wanted to see what kind of crazy structures you get when you go away from the smooth case. And so the statement is that at least in this nice situation of those caspoil curves, it still works. So the ring of differential operators is then an example of a Hof algebra with all those features, except those homological properties, and A itself becomes a module algebra over that. Over that. And then there was a side question. Well, as I said, these hot vegetabroids in general do not have antipodes. So, in my youth, I was thinking about this question, and I'd constructed one pretty awkward example of a hopfy droid without an anticode. So, I wanted more natural examples and to understand what it means that a hop filter has an anticode. And there seems to be a nice answer here, which I will get to at the very end. Get to at the very end. Okay, so this is a slide which I added last night as a kind of response to Dan Rogalski's talk, where he quoted an eminent American Hopf algebraist saying that all these generalizations of Hopf algebras, I don't really see what that's good for. And well, I could now make a long philosophical speech on that, but I. Make a long philosophical speech on that, but I skipped that part. But let me stick to theorems. Okay, so what did I say? I said being a Hopf algebra means the category of modules is closed to monodule. Okay, what does that mean down to Earth? It means that first line there. You give me two H modules, M and N. I can talk about the H linear maps from M to N. I can now split off that A on the left-hand side. So A, the base algebra, is the unit object in that model. The base algebra is the unit object in that monologue category. I can then take the m over to have the internal hum between m and n on the right-hand side. So that's really what Hopf algebraids are about. And I should say a weak Hopf algebra is a Hopf algebra. Not one in the sense of the definition I gave. There is a source and a target map. Things are a bit more complicated. But what I say here is still true. And well, whenever you have such an equality, And well, whenever you have such an equality, you now can take derived functors. And you will get a spectral sequence. On the left-hand side, you get xmn. On the right-hand side, you get a spectral sequence because there's a composition of two functors, hom A and hom H. Of course, in full generality, there's an issue with convergence. But then was in the situation where A was a semi-simple algebra. Now, this is also a theorem of Schaunburg that if you recop algebra, Book that if you read Kopp algebra, that A is always a separable algebra. And that means that this spectral sequence just breaks down immediately. And you see that X over I Mn is X over I between A, the unit object, and that internal home object. And in Dan's talk, there was this condition that for all finite dimensional representations, well he called them V, of his V-Kopf algebra, the Gorenstein condition holds. The Gorenstein condition holds. And Zara asked whether this is enough to check for the base algebra. And this wasn't clear on Tuesday, but I say this follows immediately once you realize this. And that would be my reply to that sometimes it's good to look at things from a more general perspective because things become easier. Good. Okay. Um and then of course it continues, yeah. And then, of course, it continues. I mean, all the homological properties of Hopfile Broids, things like if you want to determine the global dimension of the ring of differential operators, that's how you start. So basically, in good cases, it will be the projective dimension of A as an H module and stuff like this. Okay, good. How am I doing with time, by the way? Well, that doesn't tell me anything. Well, that doesn't tell me anything. 20 minutes. Very good. That's perfect. Good. Okay, excellent. So, this was a lot of theory. I want to talk about explicit formulas for people who like these. So, well, as I said, most of this work is done by Miriam. So, I asked the original question, but I But I only asked it for the simplest case, the cusp, x squared equals y cubed. Okay, and I told Miriam, well, there are explicit generators and relations in the literature. Let's try to write down that Hopfaldebridge structure explicitly by playing around and prove it's a Hopfaldebridge. That's a very nice PhD project. Everyone is happy. So, as often, it turned out to be much. As often, it turned out to be much harder than I thought. Typical mistake by a supervisor. And then typical mistake by a student. Once it was done, Miriam said, well, in the solution, she had written some Python code to check those axioms. And she said, we need to understand why this works. I want a general theorem here. And I tried to prevent her from working on this, but well, to stop Miriam from working on something is very difficult. So basically, when I So basically, whenever you ask her a question, she will just start thinking. And later, she came up with all this numerical semi-group business. But in the original version of the paper, we only looked at the cusp. And, well, here, as I said, what will matter is this capital K, the Laurent polynomials, which geometrically are the coordinate ring of the normalization of my Caspital curve. Of my Caspian curve. In there, I now look at A, which is the sub-algebra generated by T squared and T cubed, and otherwise, in other words, all the polynomials without a linear term. That's the algebra we are talking about. Okay. And by that general result of Smith and Stafford, we knew already: okay, so what's the differential operators on this? We have to look at differential operators of K, which map that subalgebra A to itself. Algebra A to itself. So now you can start explicit computations, right? Take any polynomial without a linear term, apply a differential operator over the Laurel polynomials. This is just the Weyl algebra localized at t, and check whether the outcome is again in that sub-algebra, the coordinate ring of the cost. And you find out that there's a set of generators. So I think the earliest reference for this set of Bloom from 1910. At Bloom from 1975. Yeah, so you see, there is a few derivations, like t partial, partial is now d by dt, and t squared partial. Why do I treat these differently? Because t is not an element in my algebra A. Only T squared and T cubed is. Therefore, these two generators should be seen as two generators that you both need. And then, as is clear by the Nakai conjecture, which is Clear by the Nakai conjecture, which is known for curves, you need some crazy generators of higher degree, and you can write them down and they will do the job. Okay, now you have a set of generators. So, of course, when you have a Hochfard rug given in terms of generators and relations, you just need to guess the code product on the generators, check all the relations, and you're good. And that was my naivety. I thought this would be easy. Here is the relations. Here is the relations. So, doing this by hand is a pain in the neck. Now, this is where I really thought Paul would be here. I think the attribution to Paul is completely wrong. This is mainly a provocation of Paul to make him tell me what the real story is. Even the theorem might be wrong. So, the truth is, Paul has written a paper in 1981 about this. About this, and Miriam has extracted these relations from the techniques in that paper. And we're not even sure this is a complete set of relations. So I'm slightly lying, although I'm pretty sure this is okay. In any case, you see it's hard. So there's a strange algebra. There's a very, very easy curve. But when you write down the ring of differential operators, many generators, many relations, and it's not easy to see the structure. So even with See the structure. So, even with very, very multi-singularities, these become very complicated rings, and hence it's not easy to write down these structures. Okay, but as I said, that's nothing which will stop Miriam. So at some point, we have these co-products. PR formulas. Okay, also in these formulas, there's something very intriguing, I think. And it is, it is. It is, it is, it is in that second line, E minus two. Where is it? It's it's there should be an E3 somewhere. I can't see it from here, already. But the point is, the point is these E lower minus L's, so they are differential operators of order L. Now, I was talking about the core radical filtration, and you define this more. And you define this more or less exactly as you would do it for Hopf algebras. And for a Hopf algebra, core radical filtration is a core algebra filtration. For Hopf algebra, it's not. So if you read the proof, say, in Radford's book on the quoretic filtration, you really see he uses that K as a field. And I think for cool algebras over commutative rings, this will break down. Also, what do we mean by simple cool algebra? If you don't work over If you don't work over the field, it's pretty tricky. So the base algebra A might not be a simple sub-co-algebra. So all this standard technology breaks down a little bit. And I think this is also a neat example of a co-algebra over a commutative ring, where sort of the primitive filtration is not a co-algebra filtration. Because in, yeah, of that one line. Second thing, Second thing, so Alexei asked about L, the primitive elements, the derivations. Used to Hopford brass, you would expect that any derivation, any element of order one, will go primitive, but that can't be. Sorry, no, it will be primitive. And if you have an antipode, you would expect that antipode to just send x to minus x. But it turns out that this hopfyldroid has an antipode. This Hopfyldroid has an antipode in the sense of Gabriela Boone, whatever that means. But you see, for example, the antipode of D0 is one minus D0 rather than just minus D zero with an antipode, but the antipode is in general not unique. If you look at the definition of a Hopfile debride, the fact that the antipode is a unique thing is not true any longer. And this is one that we found. This is one that we found. Okay. So, yes? No, yeah, okay, say again. Yes, yes. Oh, yes, it's just the derivations spent by D0 and D1. I mean, not me. I mean, not me. As I said, Bloom has done in 1975. Yes. Okay. So in the last 10 minutes, I'll briefly talk about symmetric numerical semi-groups. I mean, I should say, as I said, I was interested in the cusp. And at some point, Miriam came and said, all this works for numerical semi-groups. And neither the word numerical nor the word words. Miracle, nor the world semi group really gets me going, to be honest. So initially I said, who cares? And Miriam, in her usual way, said, Google it. And I realized there are most likely more papers about numerical semi-groups per year than on Hof algebras. So I accepted that this might be a good thing. So let's look at an example. So up there, I look at the numerical semi-group generated by three and eight. And I just list the elements. And I just list the elements. So you see, they're all the natural numbers. And I underline those elements which are members of the city group. Okay, so zero, for example, is in. Sure. One and two is not. Three is a generator. In order is not. Five is not. Six is three plus three is in. Seven is not. Eight is eight is a generator. And so on and so on. Now you keep computing. At some point you realize 13 is also not in. 13 is also not in. You can't build 13 out of threes and 8s. But 14 is in, 15 is in, 16 is in. And since 3 is in, every other number will be in. Once you find three consecutive numbers and 3 is one of your generators, this means you're done. From here onwards, all elements of the natural numbers will belong to this numerical simulation. And well, now there's an old common natural problem called the coin problem. Problem called the coin problem or the Frobenius problem, which is to find that last number which is not in. In this case, 13. Okay, so it's called the coin problem because obviously if you would have two big bags, one has coins with three UN value and the other one has coins with eight UN value. Well, if you want to order a beer and pay 13, you're in trouble. You have to tip. No, you have to tip. So the question is: what is the largest number that I cannot build out of my three and my eight? The answer is 13. Okay. And now you look closer and you realize something interesting happens. There's a certain symmetry in that interval where there are numbers which are in and which are out. So 0 is in, 13 is out. And then there are two numbers, 1 and 2, which are out. Numbers one and two, which are out, and on the other end, there are two numbers 12 and 11, which are in. So you see, there is a symmetry here from the left to the right. A number i from 0 to 13 belongs to the semi-group precisely when 13 minus i does not belong to the semi-group. So that largest number is called the Frobenius number of the semi-group, and the semigroup is called symmetric if it has this other property here. Other property here. And here's an old theorem due to Sylvester, the one that you mentioned in your linear algebra class, bilinear forms and so on from 1884. Whenever your numerical semi-group is generated by two co-prime numbers, P and Q, it is symmetric. And the Frobenius number is simply this: P minus 1, Q minus 1 minus 1. Okay, so for two numbers, this is doable. For more numbers, this gets hard, but because of But because of improvements in computational techniques, meaning computer algebra, as in Frank Moore's talk, people nowadays make progress on this if you have many generators. Okay, so there are lots of papers written on this. Okay, but if we keep going, in 1974, Krunz proved that a numerical symmetric will be symmetric if and only if that algebra A, its coordinate. That algebra A, its coordinate ring would be Borenstein. Okay. And then more recently, Quillen Galiego studied differential operators over these numerical semi-groups and showed that in this case, the ring of differential operators is isomorphic to its opposite ring. There's a more hybrid version of this due to Amnonier Kuchieli. In general, the opposite of D of A is obtained by kind of conjugating with the canonical module. Conjugating with the canonical module. And in this case, you just get that it's the same. And when you now think a little bit longer, you find that indeed the symmetry of the semi-group means that we have an antipode. So I think this is a very neat situation in which you have an old combinatorial problem, which is usually not at all what I care about. But then you keep thinking it has a homological reformulation. And at the very end, there's a very down-to-earth algebraic formulation. Down-to-earth algebraic formulation in terms of the existence of an antipode. And that's why I really like that.