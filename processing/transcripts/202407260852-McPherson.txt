Yeah, that's a useful figure for kind of visualizing some of that difference there. So, like I said, a big part of what I'll do, hopefully, here real quickly is just explain how you go from brain data to the tables of brain features you are probably more familiar with dealing with and point out the caveats of why I think there's still improvements to be made for what's going on here. So, T1 structural imaging is a high resolution of the individual's brain. You can see it. Individuals brain. You can see it there. And this is mostly collected for capturing high-resolution spatial information. So you can localize psulci and gyra and the actual anatomy and shape of the brain. This is kind of the reference scan that most images end up being aligned to, so you can assign good labels and have a reliable sort of map of where any information you're aligning to comes from. Problem. The most common analysis done with this is FreeSurfer, which is a standard pipeline for creating cortical surfaces and assigning kind of standard anatomical labels to the different brain regions. This is something you'd probably be more familiar with because these labels end up being the very standard names for a lot of regions, and it's very easy to average information within those regions to. Other modalities of information. So the kind of reference space of FreeServer ends up being a very standard starting point for most imaging derived. And then the products, you get the volume and the surfaces. You get measures assigned to those surfaces. There's any number of label sets that can be assigned. FreeSurfer has a standard set, but there's many you can choose from. And the whole idea is all of this gets produced into the data frame that you would pull from UKB or some of the biobanks to. Or some of the biobanks to train features on. Diffusion imaging is what I mostly specialized in. It's mostly observing the movement of water within the brain. So the idea is axons, the connections, have water constrained across cell barriers, so you have free movement along them, but constrained movement across. And by collecting and looking at different orientations, At different orientations of information, you can kind of summarize how water is moving through the tissue. This was originally described as in vivo histology, where you could look at properties of the connectivity within your scan to actually visualize in a living brain how the kind of tissue layout is. And there is, I spent a lot of time working on this, there are many different varieties of models and scans you can do depending on the quality of data that you would be. Data that you would be collecting within the bio-concern. This is just a visualization kind of showing how the image looks a little bit different for different orientations. I probably don't have time to go much more into that. These are the standard processing pipelines. So the big emphasis here for me is there are kind of the ideal steps that I would need to do to publish this, but the majority of biobanks do kind of this minimum subset of these are the most necessary, and that's where. And that's where some of the problem for me comes in: is that you can get a very large increase in SNR for clinical populations by adding those denoising steps that a lot of biobanks don't do by default. So you can run into this issue of losing SNR in populations you're hoping to model well, and you could lose quality of information. So just to summarize the other products, you would get here, you could have your raw or pre-processed. Your raw or pre-processed diffusion image that's ready for modeling, a modeled derivative, average features within the space. A lot of my work was fitting more advanced models and doing tractorography to get networks or more advanced anatomical segmentations to kind of have better, more individualized features to hopefully find genetic components or variations better. And again, this all then collapses into the data frame that would be part of the biobank. Be part of the biobank. Finally, there's functional imaging. It's looking at blood oxygenation over time. So it's kind of a slow-moving proxy signal for activity. It can be observed with tasks or, more commonly, as a rescue state scan. One of the middle grounds of that is naturalistic viewing, where you have people watch a movie to kind of stimulate something a little more active and natural that they would do, because the only time they would be staring at a fixation cross is for your skin. Fixation crosses for your skin. And again, there's very similar pre-processing tools for removing signal. I can't quite highlight the steps because this one's a little more nuanced, but the same thing kind of applies. The UK Biobank did a very minimum set of motion and alignment correction, where there's many and a lot more nuanced options for denoising that drastically improve the quality of your image, especially in potentially clinical populations. Potentially clinical populations, but not necessarily kind of it kind of leads a lot of the biobank features to be closer to quality control features from a neuroimaging perspective than high quality data. And just going over, you could get task activations, functional networks, or you could do independent components analysis and have kind of signal processing extracted features. Signal processing extracted features. And again, all of these can be, whether it's network features or stat tables taken from your peak activations, all summarized into tables that can be generated for the biobank. So hopefully that answers some of the questions about how the data gets here. And I've kind of been highlighting the points for me as far as where the value of the data is. This is mostly just to give you a bit of an idea of the kind of scale. Mostly, just give you a bit of an idea of the kind of scale that goes into processing some of the data. I know the pie chart of genomics information to imaging information for UKB makes imaging look like a small part of it, but at least as far as a lot of the larger high-throughput processing, it ends up being quite a bit of work to maintain many different files of information. And the derivatives from most of these pipelines end up being the starting points. Lines end up being the starting point for the analysis. So it's, as I kind of described already, a lot of the features end up being sort of the minimal processing required to QC features or kind of get a good start to the derivatives. And they may end up being sub-optimal, so it's very critical. And a lot of my work is motivated by creating good, useful, high-quality. Useful, high-quality, pre-processed neuroimaging data to create the best features we could, with the idea being we could compare to those minimal features because it's a rather empirical, or it is an empirical question, whether or not the minimal pipeline is enough for some kinds of models or derivatives, or if we do need these more advanced denoising features, which are often dismissed as just taking up more analysis time. I'll very quickly go over our standard NIPOP. NYPOPPY and Neurobagel projects that are going on in our lab. We've gone over the challenges of working with multi-site data. That's been discussed a few times. Our project, NYPOPY, is mostly for organizing neuroimaging data locally and kind of providing a standard protocol for processing and creating these data tables that I've been describing. Whereas NeuroBagel is a tool for kind of doing a decentralized search across a harmonized set of the data so we can kind of search and combine across. We can kind of search and combine across consortia or just other labs that might have similar information to create larger data sets. So, the NIPOPI was sort of motivated by, it took way too long to answer a question like this for one of our projects, and we endeavored, because it was combining demographic, clinical, imaging, and it was just a mess and it didn't need to be. So, we worked on coming up with kind of a standard protocol for sorting that. Protocol for sorting that. And in the process of that, in parallel, we were coming up with, we would be interested in trying to search across the different data sets we have as well. So, NIPOPI is meant to kind of be a living project as far as you're able to add new participants to your study from all of the raw sources that comes in, annotating the structure and keeping a standard working set of imaging data and phenotypes, working through containerized pipelines to Containerized pipelines to findable version sets of derivatives and the table of working image features at the end. And the idea with NeuroBagel is it provides a way of annotating and transforming these standard outputs into a searchable graph object that sits separate from the data and lets you de-identify it, but allows you to, with other sites, search through kind of a harmonized set of shared vocabulary. Shared vocabulary. So the different diagnostic and clinical features that you might have within your data set can be mapped through, in our case, SNOMED and cognitive outlooks, so that information can be shared in a decentralized way between sites, but without actually sharing any identified information. It's strictly a metadata exchange and able to be searched in a decentralized way. Oh, we have Google Summer of Code students working on a LLM interface for the search. A LLM interface for the search and an LLM coding for mapping your data set to the standard control book edge, but it is. So just looking, you can imagine multiple sites. You can control access, so you can be selfish and set it up for yourself and not share data with anyone else. Or you can kind of set granular controls from yes, no, we have information to the actual sample size to if you were. If you put it in an open database, you could actually share the data through this search procedure, and people would be able to link and download the files. So you can maintain a lot of control, and it's the definition of a federated ecosystem. So our use case for this was we've recently started working with some groups, or of some groups from the Enigma Consortium, which is a large initiative that combines data sets, straight large samples for. Data sets, create large samples for doing brain genomic interactions. Specifically, it was the Parkinson's data set group in Amsterdam. So we had a week-long blitz of going through and coding up a bunch of their Parkinson's data sets that were based at different hospitals. And we got a bunch of them done. There was a little over a thousand subjects, and we kind of search and query through for different age and other features that were available in the data sets through the ecosystem. And you can see the little line. Ecosystem, and you can see the little live search there, which I don't have time to explain. But we're kind of growing this ecosystem, and we hope this would be a useful thing to eventually incorporate genomics information into the search to kind of grow what is useful there. It was real exciting to hear a lot of the decentralized parts of projects that are going on here, or in this space, I should say. So, what do we hope to do? So, what do we hope to do? So, I'll actually get on to the project I'm working on now, which is looking at variational inference and polygenic risk scores with the imaging-derived phenotypes from UK BioPython. So VIPERS, as I'll go ahead and call it, is a Bayesian summary stats approach to kind of more rapidly estimate the posterior distribution of PRS features. It performs faster than standard Monte Carlo approaches. Monte Carlo approaches, and the thing that was exciting for me is the improved transferability to new populations. So, being able to work with something like UKB that actually has the power and sample size and then generalize to one of these potentially quite smaller Parkinson's sets or a federated set of Parkinson's data is quite a few. I'm basing the pre-trained GWAS features on a previous paper by Elliot that looked at the full UKB set for a little over 3,000 images. A little over 3,000 imaging phenotypes. There was good replication, and specifically features relevant to white matter. So I was excited to get started there. Our goals were to replicate previous features, estimate the PRS, kind of attempt a sort of co-localization. So Shadi is the primary developer and this is sort of a goal of his, to generalize the features, 'cause some of these imaging features will probably have a lot of shared commonality, especially bimodality. Especially bimodality and to generalize to new populations. I recently reported at OHBM. I didn't have time to make better, more readable figures, but you can see a bit of the shift that I was describing. Longitudinally, it's there. I'm out of time. UKB changed terms. I haven't been home yet to see what that means for me. This is everyone. Thanks. So, I have a question. So, in our cohort, we have because we play neural condition data that's clinical to neurodiversion. Is that of any value? Any other