this conference. At the beginning I realized my talk will be related to the statistical learning part of this workshop. So and the topic I'm going to present today is about the feature extraction in distributed parameter estimation and I'm going to present a local information geometric approach to give more insight of these problems and with the joint work with my PhD student Xing Yi Tong. Home. Okay. Okay, so to start with, I will start with some brief motivation of why we consider this problem. So first, I have a brief introduction of the distributed learning, where in these systems we have that center and anyway. Anyway, so we have the center. Oh, it appears again. Okay, so we have a center and we have some distributed nodes where in this distribution scenarios, we assume that there are some restricted communication between the nodes and the center. I mean, they cannot communicate as what they want, but there's some restriction of their communication. And in these systems, the distributed nodes can observe some data, and the goal is to. Data and the goal is to make some learning tasks for the center, such as label prediction, parameterization, or bug training. In this talk, we focus on parameterization. And in this talk, we focus particularly on the situation where we assume that the display nodes can communicate with each other or to the center just by the statistic of the data. By the statistics of the data, and specifically suppose that the node observes a sequence of data x1 to xn. Then, we assume that they can estimate a statistics, which is an average of this data with respect to some function f of x. We assume that this nodes can communicate not the original data, but The original data, but the statistic of this data. But, okay. And this happens quite often in the real scenario because, I mean, in the real fair digital learning problems, we are often communicated with something like the gradients or the feature statistic of the data. Because those kind of, I mean, those kind of features are more important. Features are more efficient to compute for high-dimensional data X. So, in this talk, we focus on sharing this kind of statistics among the nodes. In such situation, the communication constraint between the nodes becomes two kind of constraint on the dimensionality of this feature function that you can transmit and receive. Okay, now let's look. Now, let's look at a more concrete example, which we call the collaborative distributed parameter estimation problem. In this problem, we have m plus 1 nodes, where we have m distributed nodes here and 1 decision nodes here. And we assume that each of these nodes i can observe an n-dimensional sequence xi, which is IID generated from the distribution, and the decision nodes can observe the data x. Can observe the data x0, which is also n-dimensional vector. And we assume that for the m plus one-dimensional vector x0j to xmj, their IID generated from some joint distribution of x0 to xm parameterized by the parameter theta which we want to estimate. And this IID according to the time index j from 1 to n. And in this problem, we assume that each point i can transmit Can transmit the statistics of these observations with respect to some function f of x. And we assume that this genotype can receive all these m statistics as well as its observed data. We want to make a decision to see the head about the two parameters. So the question here is: how do we design How do we design the Ki-dimensional feature function fi for each node i such that the decision nodes can make the best estimation about the two parameter. Okay, so this is a kind of feature interaction problems for the distributed nodes where we want to design the informative features for these nodes. In this talk, we're going to present local information. Present a local information geometric approach to give you more insights of how the informative features the nodes should be extracted. Okay, so before we introduce the geometric approach, let's first briefly summarize some classical results along the parameter estimation direction. Okay, so the classical result in the numbers and parameter estimation is the well-known lower bound, which says The Krameral lower bound, which says that suppose that we have a parametrized distribution P, and we have a sequence of theta x, y, and x, and they are ID generated from this distribution. And the theta is a parameter we want to estimate based on the other theta, which is a k-dimensional vector. Then, for any unbiased estimator, zeta hat, which maps your data into an estimation of your parameters, for any unbiased estimator, the mean square root of the. Bias estimator, the mean square error of estimating the value of the parameters is overbounded by 1 over n times the stress of the inverse of the vision information matrix, where this j is the vision information matrix defined as a product of a metric s2 tag of theta times its transpose, and where this s2 tags of theta is called the scale of the scale of the score functions, which is defined in this way, where this term, if you read it out, But this term, if you write it out, it can be, I mean, if you write out its derivatives, it comes to a derivative of px divided by px. And this term is the conventional score function. I will put a scale in here of the score of px, which reflects the expectation nature of this issue information. Okay, so so far these are results that we should learn in the, I mean, many statistical inference classes. Okay? And there's the issue of the current. And there's the issue of the Kerbar lower bound because not because this lower bound is not always achievable for any kind of models. However, if we consider the asymptotic regions, which means that we consider the case where n is very large, then the so-called mixed molecule estimators will achieve the permanent role bound. Basically, the mixed molecule estimator is defined in this way where we want to find the parameters theta to maximize this likelihood. And you can rewrite. Likelihood. And you can rewrite this term as expectation of the likelihood function with expectation over the empirical distribution p hat x. And because our data are originated from two distributions, so the order of these symbols doesn't matter. And the empirical distribution is sufficient statistic to estimate the parameters. It turns out that the next micro estimator has very nice properties called the syntactic normality, which says that Task normality, which says that when n goes to infinity, your estimator would approach to the zeta, I mean, with the variance of the JX inverse of zeta, which is efficient information. And this means that the MLE, this estimator, asymptotically achieves the corrimoral lower bound when the n is very large. So, in this asymptotic regime, you have the optimized data. And remember that everything here is unbiased estimator. And from now on, Esimator and from now on, we always focus on the affairs estimator. Now we can go through the local intermediate geometry for the primary lower bound and this mixed liquid estimator, which want to have more understanding interpretation for this kind of parameter estimation problems. Okay, so suppose that this space is a space of the distribution with alphabet x, and here With alphabet x. And here we let's first consider the case where your parameter theta is a one-dimensional parameter, and this curve is a family of distribution of x parameters by theta. Let's say this point, px, is the true distribution, meaning that this theta is true parameters we want to estimate. Okay, and here, let's suppose that we have Let's suppose that we have a God-given reference distribution px zeta 0, which is close to this pxeta. So this one is not given for free, but let's say at this point we have such a reference distribution as God gives us this reference distribution. And then we also have the empirical distribution, which is from the observed data i degenerated from the true distribution. True distribution and check this empirical distribution onto this family, we get the distribution parameterized by the mixed microestimator, zeta MLE. And because we consider the regime where A is very large, so all these empirical distributions and this PX zeta MLE distribution are all equal to the true distribution with high probability. That's how we're calling the symptom of phase. So, because the reference distribution is also kind of close to the true distribution, so all these distributions are closed in some small neighborhood. And in such a neighborhood, this trans family can be kind of translated as kind of a linear family. And we're going to work on this local neighborhood and get some more insights. And then, if you look at this band, every distribution. Every distribution in this band corresponds to one parameter theta. There's one-to-one correspondence between a parameter theta and a distribution on this band. Okay, now we're going to be a bit more complicated. Because, I mean, in this kind of promise, we care about the KL divergence, which measures kind of the performance of our decimation. So here, we consider the KL divergence between the Tail divergence between the empirical distributed p hat x and the reference distributed p x zeta. And because these two distributions are close in some neighborhood, this is our assumption. So we can approximate this scale divergence by the tail expansion to the second order and approximate this KL divergence into quadratic functions, which we can denote this term, which is a function of x, as a function prime of x. Phi of x. And if you look at this form, it comes to the auto norm of this function of x, and we can equivalently denote this as a vector from the reference distribution to the empirical distribution. Okay, so this is from the tail expansion of the KL divergence. Okay, so we have a vector phi from the reference distribution to the empirical distribution. And we can also similarly And we can also similarly define another vector psi from the true distribution pxzeta to the empirical distribution p tat x. Okay? The definition is similar. We approximate the KL divergence between these two distributions and we define a vector side here. Yes, please. Sorry, maybe I'm missing something, but so p-hat's an empirical distribution, right? Yes. And then what is, how do you know this, with the assumptions on p of x, such that these KL divergences are finite? I mean, if it's Are finite? I mean, if it's PR, it's considered the discrete x here. Oh, everything's discrete. Yeah, discrete. For simplicity. So we define these two vectors, phi and psi. And then we want to know what will be the direction from the reference distribution to the true distribution. Because these three distributions are linearized in a linear family. In a linear family, so this is one direction because it's u here. If we want to know what this direction, basically, we want to express the unit vector along this direction. So, in order to do that, let's compute what's the vector from the reference to the true distribution, which will compute this in a very nice form, which is a dθ dpx divided by square root times the difference of the parameters, which is a very simple computation. And then you can normalize this vector, which gets you. vector which gets you a unit vector u which is just scaling of this function. So we can express the unit vector along this direction u. So the reason I want to do that is we're going to do some projection story here. Now if we want to compute or want to denote all be the maximum equal estimator here, we can compute the vector from the reference distribution. From the reference distribution to the PX, Hermitrus Paceta MLE distribution. If we compared, if you compute this vector from the reference to this MLE distribution, this is simply a projection of the vector phi onto this direction u, which we just computed. And we get this projection equals to this. This projection equals to this term. It turns out this is a projection from phi to u, which is this projection. So we can express the mixed micro estimator in terms of this projected story by just comparing this unit, the vector direction, and these projections, which we get the mixed micro estimator can be expressed as starting from the reference parameter σ0. The reference parameter zeta zero plus some adjustment of this vector. If you take a closer look on this projection, we can see that this projection can be expressed as the expectation of the empirical distribution with respect to the functions of x. And this can be further expressed as a statistics of your data with respect to a function f defined similar to. F defined, similar to the score function. Okay, so from that, we can see how the statistic can help us to solve the parameter estimation problem, which appears in the adjustment of your reference. And besides, we can compute the mean square error of this MLE estimator simply by computing the average of some random function, psi, which is a random. Function psi, which is randomness from the true distribution to the empirical distribution. And by the central theorem, this randomness is kind of like a Gaussian. So you can take this expectation and get the classical results. So here we're trying to give some insight of the classical result as a projection from the empirical distribution to the family. Now, what is more interesting here is that supposedly Here is that suppose now we now do not know what would be the right direction U we want to project in this picture. Suppose that we are instead of projecting onto the correct direction U, we are projecting onto a wrong direction V. Meaning that we are using we are not using the correct statistics here, but we are using a mismatched statistics. A mismatched statistics to do to solve this parameter estimation problem. Then we know that because this f of x is optimal, now we are using a sub-optima estimator. So our performance will be degraded due to the mismatch estimator. But suppose that we're not using, we're not projecting to the direction u, but instead we're projecting to the direction v. Then the mean square error will be degraded by a factor of 1 over cosine. 1 over cosine alpha squared, where this angle alpha is basically the angle between the correct direction and the mismatched direction. Okay, so the good part for the local information geometry is that it can help us to characterize how bad a mismatched detector, a mismatched feature function is. So, if we're using a wrong statistic, we just need to see how the difference. Need to see how the direction of this wrong statistic and the right statistic, what's the angle between them. And we can characterize the mean speed error of the wrong sub-optimized meter. Okay, so now we can also extend this, because we know what will be the performance of the mean-square error based on the wrong statistics. And now we can generalize this to a more vector parameter case. Vector parameter case. Suppose that now the parameter is a k-dimensional vector. I want to estimate this vector, this parameter θ, based on some features, some statistics, according to V1, 2, VK. Then, the performance of the mean square error will be degraded, characterized by the angle between your function and the substrate strength by Spanned by these directions. Okay, so you have a function, a score function, which is some direction, and you have some subspace spanned by the statistics that you can compute. And the performance is measured by the angle between the score function and the space that you can compute. If they match together, you get up to a performance. If they don't match, you do projection of your score function. You do projection of your score function onto this subspace, you measure the angle, you know how you perform a specific gradient. Okay? So, this is pretty nice because in the distributed learning problems, we cannot compute all the joint functions across different nodes due to the communication constraints. You can only communicate certain kinds of specific form of functions. That means you are not always. That means you are not always being able to compute this u. You may only be able to compute certain kinds of functions. For example, you can compute the statistics of the distributed units. So in that case, the functions you can compute forms a functional subspace of all possible joint functions from x1 to xn. In that case, you need to know how your performance is degraded due to. Your performance is degraded due to the distributed structure. And the issue here is that in real problems, we might not have the reference distribution. So how do we apply the local information geometry to analyze the real problems? So let's look at a concrete example here. Suppose that we have just one node here and a decision node here. And this node X1 can observe a sequence of data X1 to Xn, and this node observes Y to Y and their IR degeneration. Node after point Y and their II degenerative. And in this problem, the node 1 transmits a statistic f of x, the decision node. And the decision node will make a decision based on the statistic as well as the data y observed. In this problem, inherently we have the empirical, we have the reference distribution because it's between those observed some data y, which gives us empirical distribution and it's close to the true distribution. So we have a very The true distribution. So we have a reference distribution for the decision node here. So we can apply the information jump approach to analyze the mixed variable of this. So the question is, what would be the optimal k-dimensional function that this node y should extract? So if you look at this problem, the discrete node can only observe statistics of this, because I observe the data of y, so you can compute any statistic of this y. And it can also perceive a statistic. It can also receive a statistic of x according to some function f. So this node cannot compute any function of joint x, y, but instead it can only receive a statistic of x and compute some statistics of y from its observation. So in this case, the computable function for the decision node becomes to a span of the functional space of y and the span of the function for different dimensions of f. Okay. Of f. Okay, because it cannot compute all the joint functions of x, y, which is the large space, the ambient space, but instead the computable function for the decion node is lying on the span of the functional space of y, and the functional node is being spanned by f of x. Okay? So in this case, the mean square error is measured by the projection of the score function, which is The score function, which is a joint function of x, y, onto the computable substates span by fy and span of f. And the performance can be computed by this projection operator, where this v is a projection of this score function onto this subspace formed by these two projection operators. And to get more insight of this projection, if we consider the scale of the If we consider the special case where the x and y are conditionally independent provided by the RJ theta, which has this form, then the mean square error can be further reduced to the inverse of the matrix composed by the feature information of y plus the score function project onto the functions of x. In this case, if you want to design In this case, if you want to design the feature functions f, which make this mean square error as small as possible, we should choose the features of x as, I mean, the singular vectors, the top k singular vectors of the feature, the information matrix, because this is a quadratic application problem, and the ultimate solution for this feature path corresponding to the singular vectors, the top singular vectors of the The top singular vectors of the feature information matrix. So we're going to provide actually an interpretation of the top singular vectors of the visual information matrix as the informative features of the data that we want to track for solving the parameterization problems. And then the generalization to the M-node case is very straightforward. Here you have M-Nodes, each observes ID data scheduled to be distributed. So, and the design of this, each And the design of this each of PI-dimensional feature functions fi is simply to project your score function, which is a joint function of x0 to xm, onto the space spanned by the functional space of x0 and the span of each of these functions, f1 to fm. And the projection is very similar to the two node case. So the last thing I want to mention is that, okay, so even before this problem was also considering some other Was also considering some other scenarios, such as the Hani and Mari considers a case where each node can communicate an order of log and bits to the three nodes. And in their papers, they show that the optimal strategy is to communicate the empirical distribution p-hat x. Or sorry, it should get p-hat x1 to p-hat xm. Sorry, it's a typo. So they should communicate the empirical distribution to the distributed node. And in such case, because the empirical distribution, when we have the empirical distribution, we can communicate, then we can. Distribution, then we can compute any kind of statistics of the data. And if we know all the statistics of data, we can reversely recover the empirical distribution. So communicate the empirical distribution in this scenario is equivalent to project the score function onto the space spanned by the functional space of each distributed node. And in that case, our projection story can recover their results. Cover their results or two nodes. So, this was a classical result in the distributed parameter estimation problems and show that the local information geometric projection can recover their classical result, although in their setup there is no reference distribution. So, I mean, the information geometric story is a kind of interpretation of this result. It's not technically proof, but it's very good to understand how the distributed structure can limit your performance. Can limit the performance of your parameter estimation. Okay. So I think I'll end here. So do some kind of time constraint. Yeah. Oh, by the way, so I mean, let me just mention one more thing. So I'm 2024 will be in Shenzhen and we are hosting this workshop. So I mean all of you are welcome to Shenzhen to I mean join us in November. Okay. In November. Okay? Thank you very much.