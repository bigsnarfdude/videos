Real pleasure being at such a wonderful process. Today I'm going to tell you something about probabilistic python function. So this is a joint work. And Orphan and Alex, the men are still in Abillium's. And Rotridio is my postdoc. And then you will see it. Yes. I'll try my best. Thank you. Okay, so let's first recall with our parking function. What are parting functions? So, parting functions have an equivalent formulation in computer science. They are called hashing functions. And then, parting functions were first studied by Cong and Weiss in the 1960s. And then, the classical situation is the following. So we have a one-way street and we have a parking lot, and then the cars will enter the street one by one, and every car will have a desired spot to park. And if the spot is a Park. And if the spot is available, the car will just park there. And if the spot is already occupied, then the car will move further along the street until it finds an available spot. If the car cannot find an available spot in the end, then the car will directly exit the street. If every car could park in this situation, we say it's a parking function. And again, we can reformulate it in the language of hashing functions as well. Of hashing functions as well. We can say every item has a desired cell, and then if the cell is empty, we will just place the item there. If it's not empty, we will move further along the queue until we find an empty cell. And then this parking function has been studied a lot. There are lots of correspondences for combinatorics community. For example, the number of parking functions is n plus 1 to the n minus 1. is n plus 1 to the n minus 1, and lots of things about these parting functions have been studied. And today we are going to look at the new line of research that is adding an extra layer of probability to these functions. So the idea is the following. In the classical situation, when the spot is already occupied, then the car can only move further along the street. How about we give it some probability? So the car can not only move further along the street. only move further along the street but can also drive backward. And then to make this problem actually a trackable, so the first stage we compute is that we have all the cars follow a same coin tossing probability. So if the spot is open, then the car parks there and if it's occupied then with the same probability p, the car moves forward and then try to find the spot to park. And then with the same probability 1 minus p, The same probability 1 minus P, the car moves backwards and tries to find the spot to park. And as you can see, the classical situation corresponds to P equals 1, because every car will move forward, yes. Do I only throw a coin once and then keep the direction? Or do I throw a coin if the next coin is? That's a very good question. So, right now we're only throwing the coin once. But if we throw the coin multiple times, that'll be even more interesting for sure. Yeah. Yeah, so then we wonder like what will happen with this situation and we found some invariance and also some non-invariance as we will see. So again like a picture illustration, make sure everyone is on the same page. So every car has a desired spot. So we have the desired spot placed on top of the car. As we park, we see that car number one, car number two, we directly park as their desired spot. Car number three, Their desired spot, car number three, because the preferred spot is already occupied. Now, with probability p, it will move forward, with probability one minus p, it will move backward. So, for this situation, for this given vector preference, only with probability p, all the cards will park. Right? Okay, so and then before moving forward, let me also point out there are some nice features about this model. The first thing is that now The first thing is that now adding this extra layer of probability, there is a so-called preference symmetry. Because if we describe, so the same number problem can be described using two possible ways. One is that we have all this parting preference and then we have probability P of moving forward, 1 minus P of moving backward. Or we can have these parting preferences thinking that the cars are entering from the opposite side. cars are entering from the opposite side of the street and then with probability P it moves backward, where minus P moves forward. So this preference symmetry will play a central role later in our calculation. And the other thing is that now adding P, there is a lack of permutation symmetry. For classical parking functions, if a vector denoting the preference of the cars is a parking function, then no matter how much you permute the vector, Are permeate to the bacteria, it will still be a parking function following the pigeonhole principle. So the parking outcome will be different, but then it will still be a parking function. So all the cars will still be available to park. But here, like adding P, then when you permute it, it will not necessarily have the same preference probability of being a parking function anymore. And we can have many simple examples to illustrate that. However, we will still. Illustrate that. However, we will still use this shorthand piing PFN to denote that all the n cards with the preference vector pi part. And then they will come with different probabilities. So now let's first look at this table. What does this table mean? So it means that here I give you a preference vector pi, and I compute the probability this preference vector gives me a parting function. Bacteria gives me a parting function. So there are 27 of them, as you can see, and everyone will be a parting function with some probability. And this will always be true. But the magic here is that if you add up all of these, then the probability of being a parting function is still the same as for p equals 1. And this is quite surprising because originally you added a probability, but then you are asking the same question if I select a random vector. If I select a random vector, what is the probability of parting function? It's the same probability independent of P. And we are going to give an outline of the proof. So an outline of the proof, the first step is that we will extend a pallet circle argument. So Pollock circle argument for a classical parking function is that instead of considering a one-way street with n squared, Considering a one-way street with n spots, we will consider a circle with n plus one spots, and then we will have all the cars parked along the circle. And it's a parking function if the last spot is unoccupied. And this happens to be true as well when you have different values of p. And then the second step is that we are going to use the symmetry of the circle. And this is a crucial step, I would say. So we will let FAI denote the expected number of preferences. Note the expected number of preference bacteria which have leading preference A and have spot I vacant after all N pass part. And then using the symmetry of the circle, we'll see that F A superscript I is the same as F B superscript J when I J A B satisfy this condition. We can think that we just modify the preference of the cards along the circle by doing a mod n plus 1 operation. And then after that And then, after that, for illustration, we can construct a matrix here. So, but because of the symmetry just explored, we see that this matrix is the same as this matrix. So, because here, F21 is the same as F1 superscript M plus 1. Because 2n1 and 1n plus 1 are the same modulo n plus 1. n plus 1. And now you can see that it's just a shift of all those numbers from the previous row. And then what we did is that we moved this F11 here and then T2 here and this one up to here. And then we are doing this shift for all the rows which implies that the row sum and the column sum will be the same for this situation. And then with a little bit more combinatorics, we Bit more combinatorics, we then apply the basic law of total probability. We'll see that summing up of all the preference vectors give you the same probability independent of the value of p. So I'm going to talk more about this invariance later. But still, after seeing the invariance, a natural question we wonder is that now like we're adding this probability, P actually does. probability p actually does anything to the parking function because if it's just invariant then it's not that interesting. We want it to be invariant for some statistics showing some universality but also some other mean statistics. And there are a bunch of statistics that are affected by this probability P and here in this talk I'm only going to show you one of them. So the statistic we studied is a distribution, the conditional distribution. The conditional distribution of the parking preference of the last car, and we did it for 100,000 samples. We even did it for 1 million samples. It just takes longer for the computer to run. And then we are going to choose parking preference vectors uniformly at random. And I'm going to show you some simulation plots. So here are the simulation plots. So this is for p equals zero. equals zero and this is sorry this is for so this is for p equals one this is p equals zero this is p equals point five this is p equals point seventy five this is p equals point twenty five you can see so as you can see like so uh from here So from here to here to here, like it gets more unif most uniform at p equals 0.5. And we will show you why you have this pattern here. And then here and here, they are symmetric. And they are symmetric because of the preference symmetry we talked about earlier. And this 0.25 and 0.75 are also symmetric. So 0.5. Also symmetric. So 0.5 is by its own class. And then we also see that there seems to be like getting here to here and then here, it's more and more uniform. And then the idea is that when p is equal to 1, for example, you know that all the cards, no matter how small the preference vectors are, once the spots are taken, they will just move forward. So all the smaller So all the smaller spots have more preference of being chosen because even if they are small and then they are occupied, you can move forward to make it a parking function as well. So that's why you have this picture here. And then gradually as p becomes 0.5, then you have that uniformity. Is it actually uniform on 0.5? We will talk about that. Yeah, so it's conversion. Yeah, so it's converging to uniform much faster than the other parameter regions. The other ones also converge to uniform, but in total variation distance. Yeah, exactly. Well, you're going several steps ahead of me. Okay, all right, so now let's look at some exact formulas. So here is a problem. So here is the probability of pi n equals j. So for any possible j, and this is exact formula. So from the formula, we notice first preference symmetry. And this is not surprising because of the preference symmetry we observed in the model. So probability pi n equals j and then pi n equals n plus 1 minus j. They have their symmetry here. And then what is more so? And then what is more surprising is convexity. So the conditional probability of pi n equals j can be expressed as a convex combination of the situation at p equals 0 and p equals 1. So I remember meeting Maybosh quite a while ago and he taught me a problem about convexity in parting function. Till now I was not able to solve that one. No but nobody would. I don't want to see convexity here in this function. Convexity here in this function. And then the convexity will also play an important role in the proofs later. So, outline of proof, yeah. Can you remind me what PFN is? Oh, P, you mean PFN? It means, so when we write pi in PFN, it means that the probability, the preference vector, actually gives you a parking lot. All the cars can park. Yeah. So, uh outline of proof. So, um uh the idea is that we are going to use a combinatorial construction called parking function shuffle. And using these ideas, we can actually calculate the conditional distribution of several cars, not just the one car. But then if you wanted to calculate using several cars, To calculate using several cards, we are going to use something called a parking function multi-shuffle. So, all these can be generalized, that's what I wanted to say. And then, also, after going through this parking function shuffle, which we will see in the next slide, then we also recognize that when we are thinking about the last car, we can first park the first M minus 1 cars first, and then the M minus 1 cars will park in all the M minus 1 spots, leaving n minus one spots, leaving one spot open. And then you just consider the last car. It could either have preference the same as this open spot, or preference bigger than or smaller than the spot. And then you will need the respective probability to make it a piking function. And then the third step is that we are actually going to do some explicit calculations using Abel's binomial theorem. Again, we can extend this by thinking about several cards, but instead of using Abel's binomial theorem, we are going to Abel's binomial theorem, we are going to use Abel's multinomial theorem. So, again, like all of these methods can be extended. So, what is a parting function shuffle? Before you question? Oh, sorry. Somewhere you said this complexity that you wrote at the bottom, and I don't know how it is related to your matrix that you formed earlier, the matrix you showed that. The matrix you showed us, it is a circular matrix. That means if you have the first row, the others are just circular. A stochastic matrix. Ah, yeah, yeah. That stochastic matrix, uh I think I'm wrong, if you have any such matrix, it is a convex combination of permutation matrices. Permutation matrices. Ah. So you can get extreme values. That might actually be the deeper reason behind this. Yeah, thank you. Okay, so what is the parking function shuffle? It's a combinatorial construction first introduced in diaconics and hips, and then later explored by more people, and then also extended to other classes of parking function. Of parking function. So the idea is that the first n minus 1 cards form a parking function shuffle if we can express this as a union of permutations of the two words. So I guess the example is better. So here we have this is a parking preference of the first seven cars. And then we say that it's a parking function shuffle of this alpha and Shuffle of this alpha and this beta because we see that if we add, so here k equals 4 means it's a non-taken spot, k equals 4. So we will add 4 to every number here in beta. So 1 becomes 5, 2 becomes 6, 4 becomes 8, 3 becomes 7. And then alpha still stays the same. And then we just permute these numbers in whatever way we like. So this is a parking. So, this is a parking function shuffle. That's the idea. And then, parking function shuffle helps us express the probability of the last preference vector as counting the possible ways to shut and all the possible parting functions in each component. And also, this is the classical Abels binomial theorem. The classical Abels binomial theorem, as we can see, is a generalization of the binomial theorem. So, here there are three basic recurrences for Abel's binomial theorem. So, x, y, and yx can be switched, as you can see from the formula. And also, it can be computed recursively like this. And then, this is another relation. These are three basic iterative relations of Abel's binomial function. And from these three basic relations, we From these three basic relations, we can compute many other functions for Abel's binomial setting, like this, and these. I just listed a few here. And with these tools, we will be able to get that formula I showed you using these tools. And again, let me just say one more time: there are generalizations of these tools. So we can study multinomial versions. Can study multi-long motions. It's not too complicated, just need pages of calculations. Alright, so now after you're studying that, then a natural question is, what about expected value of these things? Can we see anything? And here, we did some more computations, and this is the formula we got. So, this all of one term is of lower order. It's just a constant order, actually. So, again, So again, we see preference symmetry and then the sum of expected value here under parking preference P plus this expected value under parking preference 1 minus P is exactly n plus 1. Not surprisingly. And then let me just say that the proof needs some asymptotic extension methods, Sterling's formula, and Edgewood's extension for Poisson variables. So because it's So, because it's Poisson, these three random variables, when we apply a true extension, we also need to apply a continuity correction. It's all the classical stuff. And this is the picture. It's the expected value of the parking function and then different values of p. And as we can see, this is a linear dependence on p. And let me also formula that you showed with the endorse expansion. Oh, I see, there's the P and now. Okay. So then going back to Robin's question earlier, why is the forward probability p equals one half special? So we see in the simulations at one half it looks very uniform. It looks very uniform. At the other parameter regions, not that uniform. So the thing is that at P equals one half the symmetry provided by P, because every car has the same probability of moving forward and backward, then it plays an important role of speeding up the rate of convergence of pi n to a uniform distribution of 1 to n. And then, so to be more specific, for other parameters. Specific for other parameter values of P at the corner points, you will have Borel distribution. So it's a little bit different from uniform. And then P is very, very close to uniform. So exactly how close to uniform, again, simulation picture one more time. So exactly how close to uniform we have quantified it. So for P not equal to half, in total variation distance, this is of order 1 over root n. 1 over root n, and then for p plus 1 half, in total variation distance is of order 1 over n. So it's much faster. And then the total variation distance has many equivalent formulations. Here, I'm just using this formulation here. And again, let me mention that these can also be generalized to more cars, not just one car. And again, outline your proof. And again, I outline your proof. So the outline proof is that first we are going to write a QMP, meaning the conditional distribution of the preference of the last card. In the previous theorem, first we notice the preference symmetry and also the concavity here. And then we will establish a chain of inequality concerning the total variation distance convergence norm. Distance converges known. Yeah, just some standard analysis techniques. And then we also identify a test function so that we use an equivalent formulation of the total variation distance. Using this, we will be able to find a lower bound for the total variation distance. And then lastly, we apply a most binomial synonym here and do some listed asymptotics. As we call it. So this is my theorem. Alright, so for the last couple minutes, let me switch gears a bit and talk about some related combinatorial results. And then at the very end, I will talk about some newer things I'm thinking about. So the thing is that parking functions, there are many combinatorial constructions people are interested in, like how are they related to trees. Like, how are they related to trees, patterns, and stuff? So, earlier, like, Stefan gave a talk on the trees, and that also gave me a question to think about. Because we know, for example, if we consider the displacement of parking function, and then we do the sum of Q to the displacement, it's exactly the same as sum of Q to the inversion number over all trees. But then I don't know what statistics in parking function will call. Statistics in parking function will correspond to when some appeal to the descent, for example. There are many interesting questions out there. And here, what I'm showing is that as you can see, every row here denotes the number of preference vectors with leading preference fixed at spot one, parking in a circle, containing K unlucky. Containing pay unlucky cars. So a car is unlucky, means it's not able to park at its preferred spot. It will either need to drive forward or backward. So here, this row, for example, depicts a situation. We have three cars parking in a circle with four spots. And then the first car will park at spot one. And then all the other cars, you consider all the possible places they park. Places they park, and then this is situation they contain zero unlucky cars, one unlucky car, and two unlucky cars. So it's not possible they have three unlucky cars because the first car will always be lucky. The street is empty at that moment. And then this is actually an open problem posed by Navalian Zibone. And then they asked OEIS, well, in a paper, they say, like, we found this. Like, we found this generating function, what does this correspond to? And we found that it corresponds to, at least in this situation, the parking scenario. And also, another thing is that we can consider one-way street, it will correspond to another generating function. And then this is also like three cars, one-way street, and then three spots here. Unlucky cards, number zero, number one, number two, and so on. And so on. So, as you can see here, the two formulas look quite similar, but also a little bit different. Here is some k equals 0 to n minus 1. The previous one is this to n minus 1 q to the k. And then, yeah, almost like I think. So, if you look at OEIS, the explanation of the n k there is given in terms of In terms of trees, of liters of trees. But then, if we use a construction between trees and parking function in Canu's book, The Art of Programme, Computer Programming, we'll see it as an equivalent simulation in terms of parking function. Alright, very lastly, let me also say that, like adding a probability layer, there are other things we can study. Study. So, for example, instead of having n cars and n spots, what if we have m cars and n spots, like fewer cars than the spots? I believe it can be studied in the same way, just more complicated combinatorics. And also, like, what if we consider more cards, consider like higher-order correlations, and all this thing? So, I think this is all different. I think this is all doable, but like what I don't know how to do is that here we are in one-dimensional parking situation. What if we are parking like a plane? What would the parking functions look like? Are there some combinatorials similarly here? And also like study of the asymptotics in the related combinatorial constructions. And one particular thing I'd like to mention is that for parting functions, For parting functions, similarly hashing functions, the area statistics is of particular importance. So the area statistics meaning the total number of steps the car need to travel in order to park. And then I did some simulation tries after finishing this work. I think because of the simulation symmetry, if you vary the parameter p, the total number of this Of this area statistic is still invariant. But then, if it's invariant, then that gives you other questions to study: like what other things are invariant or invariant, and so on. Yeah, but yeah, that's it. Thank you very much. I got any questions for your questions case would be cool. Yeah. Can I see the slide with the five graphs? Sure. Zero, one, half, etc. Yeah. So I'm looking at the top one with p equals zero, one. And so from what you said, it sounds like we're seeing a graph of one half. Graph of one half constant, right, plus n to the minus a half times a function. Asymptotically, I guess there's an expansion, right? Constant plus a correction of, so like the three last posit the positions one and n differ from the from that by uh by order of the normal By order n to the minus a half if that's what you're going to do. Uh you mean um so this is a probability conditional distribution function of the are you asking function right and um but it's not a it's not a constant and there's different probabilities for it to be in different places. Right and that and the difference is order n to the minus a half. n to the minus a half times the time rescaled so like if if each of the probabilities were one over n, it's differing by what amount by order n to the minus three halves? That's a very good question. So like this is the exact formula and for sure we can do asymptotics. But like when I got the expected value I just uh changed the sum and stuff. I didn't actually compute. That I didn't actually compute what you just asked, but that's definitely interesting. It looks like that's true. If you're saying that a total variation distance should be order n to the minus a half, then it looks like it's spread out over a function, then it's order n to the minus a half times f, where f is a macroscopic quantity. Yes. And then, so then I'm asking, I guess, so that's a symbol that perturbation is, that n to the minus a half perturbation is symmetric, but the next perturbation. Symmetric of the next perturbation is an n to the minus one perturbation that's that's an odd function, but there's an n to the minus one perturbation that might be an even function, so that when you when you add it to its reverse, when you get the fequal to half graph, then you only see this n to the smaller order perturbation. I'm asking if that's right. Um understanding the orders of magnitude of things in your crown intuitively, that's right. Intuitively, that's right. But I guess I'd better do some more computations to be 100% sure. Yeah. In your order of convergence for P one half and P not one half, you have right. And here if we look at it all together global globally. together global globally with a vari which by changing P, then I suspect that this should be continuous over P. And so then in that case now there is a sudden drop when P equal to one half. So why should it be so singular there? And that means is there a transition? The leading term grows up, so it's probably big over 1 over n there, and then as you move away, the constant in the 1 over root n is very small and growing linearly out. So it's like two. Right, it's not necessarily that there's a drop. It could be that there's a gradual, you know, if you let p close to one half, but not constant, right? You know, like a half minus epsilon or something like that. Yeah, there is something. Yeah, that'd be interesting to know what is the behavior. You can filter out, you know, if you can answer Robin's question, you can probably figure that out. Yeah, I feel there are many interesting questions to explore further. The constant 2p minus 1 times 1 over root n plus the 1 over n. So yeah, like, I think our So yeah, like I think ours is the first paper to study this model in a probabilistic way. I mean probability plus karminator. So definitely there are a lot more to be said. It's only the first step. Thank you. So we said hey we have twenty six minutes. So all the shorts are. So why not? Why do I have to let you go? Well actually it is our so because I think it will be out all sorts of things. So there's that or it's all the way out. I'm checking all these complications anyway. When I go to the next one, all the ingredients in place for 10 minutes, although it's important to qualify because we don't have any questions. You actually use a methodology. You actually informed yourself. I think just a little bit of equality. Semi-local. We need to learn that you have to send that completely. It would be nice if you could say that one hour. Yeah, there's  Oh, do you have questions on other and alternative? I have a lot of different things. Nobody does it, this of course. I think that's complicated, right? I mean, there has to be actually