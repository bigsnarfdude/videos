Okay. Okay, and we are live. Great, so I'll also start the Also, start the recording. So it's just about all I wanted to say today. Welcome, everyone, to the fourth OOPS session of the week. So today we will have two lectures. Both are being recorded. So if you don't wish your audio and video to appear in the recording, please keep your microphone muted and your video switched off. And let me just And let me just also mention that next week we have a rather full week of OOPS material with a mini course from Ivan Corman Monday, Tuesday, Wednesday, followed by several short lectures on related topics on Thursday and Friday. So there will be an email to the mailing list about that this afternoon. With that, I'm going to hand it over to Nina Ganter to introduce the speakers. Over to you, Nina. Yes, thanks, Luigi. So today we will have two talks. So today we will have two talks. And the first speaker is Piotr Dyszzewski. The second is Sam Johnston. You can see them both here. And Piotr, who is a postdoc in Munich right now and also in Wroc≈Çaw, will start and his topic is branching random walk and stretched exponential tails. And the screen sharing works for me and I hope for you as well. Okay, Pioto. You can start. All right, thank you, Nina. Yeah, so for the most of Nina's lecture, she focused on a branching random walk where the displacements have fin tails. What I want to do today is to show you how you can work with branching random walks whenever your displacements are have heavy tails. And what I mean by that is whenever the exponential moments exist or not. All right, so first off, let's just recall the setting. So I will work on a real line and And we start with one particle at the origin, and after some time, this particle dies and gives birth to a random number of new particles. And we're going to assume that the reproduction law is, the reproduction of the particles is governed by a supercritical Gartner-Watson process. So in this in particular, means that the mean of the reproduction law is greater than one. Is greater than one. All right, and we're also going to assume that the displacements of the particles, because after the particles are born, they are moved somewhere on the real line, all the shifts are an IND copy of one given random variable with so-called stretch exponential law. So this means that the tail of x decays as e to the minus t to the power r. power power r and this and this twiddle means that the quotient of you know the both sides tends to one as t tends to to infinity so instead of you know giving you a full scope of of what happens with habitat displacements I'm just gonna focus on the stretch exponential case and give you the the whole treatment but most of the phenomenas do replicate in with with Do replicate with different tails. Alright, so the dead particles are removed from the system, right? And then they reproduce as previously. So they produce some random number of other particles, which are then placed on the real line. And this system evolves for some time according to this rule. And after some time, you do get a collection of particles on the real line. On the real line. And the aim is to somehow understand how this collection of particles evolves, right? And the first question you want to ask is the behavior of the extremes. So in our case, this is going to be the position of the rightmost particle. And this case study of stretch exponential displacements. Stretch exponential displacements basically boils down to explaining the math behind the result that I did obtain with Nina Gunther and Thomas Huffelsauer. All right, so Nina does Nina needs no introduction. And Thomas is her former PhD student. And if my memory serves me right, he did graduate two years ago and now he works in the private sector. All right, so under some technical are some some you know technical technical assumptions that I do want to to omit because I don't want to make my slides too too complicated we have the we have the following so assuming that the law of this generic random variable is you know such exponential and that this guy is centered and has variance one and then it turns out that you have two different regimes of the behavior Different regimes of the behavior of your rightmost particle. So, depending on whenever r is smaller or greater than the critical value of two-thirds, your rightmost position either fluctuates or you have an almost true behavior. So, in both cases, the leading term is the same. This is alpha times n to the power 1 over r, right? But Right? But for smaller values of R, you know, the rightmost particle fluctuates in the neighborhood of this value with a limit being the shift of a Gumbel law. So here this W, this random shift, is a martingale limit associated to the underlying Galton-Waltzen process. And we will see exactly how this. And we will see exactly how this guy pops up. And whenever R is big, you have more balanced behavior, which translates to almost show limit. All right, so let me just introduce the setting that Mina did gave in order to be self-contained. Let Zn be the underlying Gautam-Woltsen process. All right, so maybe I want to go back and say two things. So first of all, my aim here is to somehow explain where this exponent is coming from and where this different type of behavior is coming from. But most importantly, From, but most importantly, I do want to give you some height, some idea of how you can use the heaviness of your tails to your advantage when you study the branching random walks. All right, so as I said, let's Zn be the underlying Galton-Watson process and assume just for the simplicity that this process never dies out. So assume that the probability that your descendant has no children is zero. Children is zero. And if we don't have this assumption, then everything that I'm going to tell you will still work. However, you will just have to condition on the set of survival of your population. All right, so then you can draw your Gautam Watson tree, right? So then you assign labels to each of the individuals. So of the individuals. So let's say that the first particle has a label with Label with with which is an empty word, and then particles from generation one has a label which is a string of length one, then particles from generation two has a label which is a string of length two, and so on and so forth. So once you have your Gautam-Watson tree, then you take the displacements. So you take those a family of IAD random variables and Variables and assign them to the edges of your Galton-Watson tree. So you think of your labels, of your X's as the displacement of a given particle from the place of birth. All right, and then if we are interested in a position of a particle from generation N, then what you need to do is just look. is you know you just look at the label on your tree look at the path from this particle up to the up to the root right and sum of and you you just sum all the displacements along this path right and then you get this quantity which I'll denote by vx which represents the position of the particle with label x and now your you know your rightmost particle is just the maximum of those guys Guys. Alright. So one technical thing that we're gonna need is the large deviations for random walks. So you've seen that in Nina's curse, and she did use large deviations in the case when the displacements have fintails, when you have some exponential moments. Exponential moments, and this is basically a common knowledge, right? In the case which I'm looking at here, that is the stretch exponential case, this is not so well known. So let's just look at what's happening. So the lack of this exponential moments, morally, this translates to the so-called principle of one big jump. Namely, you know, if you look at a statistic, a reasonable statistic of A reasonable statistic of your ensemble, then the probability that this statistic is big in some sense is asymptotically equivalent to saying that one of the guys in your ensemble is big. So for example, if you look at the probability that the sum of the two guys is big, this is asymptotically equivalent to saying that one of those guys is big. And now we want to somehow try to push this phenomena and see if we... Push this phenomena and see if we can get some large deviations result out of that. All right, so let's take Sn to be the sum of those X k's, right? And take the maximum and denote it by Xn star. And now we want to understand, you know, what's the probability that this Sn is greater than some threshold Tn and you know as And as it will turn out in the future, we don't need the full scope of Tn. It will be sufficient for our needs to take T of the form C times n to the power 1 over R. So we want to somehow exploit this principle of one big jump. And naively, what you can do is say, okay, so Do is say, okay, so this the biggest displacement does something, right? And then the other displacement somehow contribute, right? So you can try to write this supremum and just see what happens. Yeah, so we know what is the behavior of this maximum, right? This is exactly this part. And then you check what's the behavior of the Check what's the behavior of the rest of those guys. And since you think of those guys as the typical displacement, you expect a Gaussian behavior from that. All right, so you have some expression in your exponent and you just simply optimize this guy, right? And what you get is exactly that. All right, um all right, um, yeah, so now when you look at this probability of typical particles. this probability of typical particles, what you can do is, since you expect those this x this Sn to be close to Gaussian, you can write Sn minus 1 divided by square root of n, and this is supposed to be greater than this constant that we somehow get times n to the power 3 over 2 minus 1 over r. So you can actually see from that that From that, that the value of R is being equal to two-thirds, it's somehow critical for the behavior of the deviations. And this is exactly this critical parameter that we have in our result. All right, so you write down what's the behavior of the product, right? And this leading term from the maximum translates to the leading term in the exponent. In the exponent, and then those smaller guys somehow transfer to something smaller. So, the key fact here is that the leading term, the asymptotic of the deviations of Sn really comes from the maximum. And then, depending on the value of your parameter, then you can get something different or not. All right, so what can we do with that? Well, we can redo the first. Redo the first standard term of the asymptotic of the rightmost particle. And this is again something that Latina did in the previous course. But still, this will help me drive my point. So let's just do that. So the probability that Mn is greater than C times N to 1 over R, and 1 over R is the correct scaling because I said so. So you look at the probability that one of the guys is greater than this threshold. Of the guys is greater than this threshold, and you can use the first moment method to bound this basically by an expectation of the number of guys that are greater than this threshold. And what you get is the expectation of Zn, which is roughly m to the n, times your deviation. And this is something that we did compute just now, right? So, um So, what we get is, you know, asymptotically n to the n times e to minus c to the power r times times n, right? So, now we need to tailor this c n such that this term compensates this m to the n, right? And this leads to basically a prediction of the first other thing, term being n to the power 1 over r times times alpha. And this. times times alpha and this this alpha is is exactly given even here all right so what we can learn from that learn from that well um everything you know this this first term really comes um from from this parameter right and we just said that you know this uh this leading term this asymptotic of of the deviations comes from the The deviations come from the biggest displacement. So we can try to argue that the first term, the asymptotic expansion of the rightmost particle, is related to the biggest displacement in your collection. And this biggest displacement, right, this is just a maximum of IID guys. And the only twist is that the number of guys that you're considering is random. But since those two quantities are Since those two quantities are independent, this is still easy to handle. So, you know that after a proper centering and scaling, you will see a nice limit, which is again a random shift of the Goomba distribution. And this is something that's so easy that we can actually try to prove that. So, for instance, take some threshold xn that goes to infinity. xn that goes to infinity. And let's just try to compute the probability that this biggest displacement is below this threshold. So what you do? Well, you just condition on your tree. So this leads you to writing that basically you are saying that you have a random number of guys that are supposed to be smaller than this threshold xn and this random number of guys is just the number Number of guys is just the number of the edges that you have up to time n. And this is easily expressed in terms of your branching process. And then, you know, looking at the expression under the expectation, it behaves as e to the minus the product of, you know, this yn and the tail of x. All right. So now this is So now the random shift comes into play because we know that this n behaves pretty nicely. That is, after scaling by m to the n, you get a martingale. And since this guy is non-negative, it will have an almost sure limit. Moreover, since I'm implicitly assuming that the constant statement condition, this limit will be non-negative. Will be will be non-negative. So, this cassette-stigma condition is one of those ugly assumptions that I just omitted in the second slide. All right, but now we know what the behavior of this yn, right? This is again something that grows in the same fashion with a random random shift. All right, so now we can take a very concrete threshold here and just start to compute things. So, first off, things. So first off, we do only the simple bits. I mean first off we just take the asymptotic equivalent of yn, this is here, and we just plug in the definition of xn. So nothing happens. Then we look at the exponent on the right hand side and we simply use the Taylor expansion. And when we do that we notice that you know this That this exponent here is tailored such that it will cancel with a derivative that pops up from the expansion. So you're left with a constant. All right, but now we just need to recall what is the role of alpha. Alpha is taken such that this guy will cancel with m to the n. Right, so we are in business. Right, so we are in business. We are in business because if we call our constant, you know, constants accordingly, then we get a nice limit. And if we go back to what we have done, this basically shows that, basically shows that this biggest displacement after a proper centering and scaling does converge to this nice random shift of the Goomba law. So, as you can see, the analysis of the biggest displacement is pretty straightforward. So, what can we do with that? Well, when we look at the first order asymptotics of the biggest displacement and the rightmost particle, they are the same. So, it's not difficult to convince yourself that if you look at the position. that if you look at the positions of the particles which didn't have a big jump on the on the along the along the branch those guys are negligible somehow right and if you have that then you know that you need you need only to concentrate on the on the particles that had a big big jump right and how many are there so all the all the particles up to time n which we see in our system um the number of those Our system. The number of those is roughly m to the n, right? And then you look at those particles that did encounter a big jump. And those will be fewer, right? And it turns out that whenever r is small, you only see a polynomial number of these big jumps. And whenever r is big, you see something which is almost exponential, but Exponential, but not exactly exponential. And I do know that this line is somehow vague because I don't tell you what do I mean by X having a big jump. So the takeaway message from this line is that for small values of R, we have small number of big jumps that contribute. And for big values of R, we have a lot of Of R, we have a lot of big jumps that can contribute. All right, and in the end, you know, those particles that did give a big jump form a small subset of the whole tree. So what can we do with that? If we look at the whole tree, the first step that you want to do is look at those particles that look at those those particles that you know with the displacement comparable you know to that the first order asymptotic and color the the edges green then you know you color the the edges which which are below in in red and then you look at the path that goes you know from the green edge to the root and you will have an orange segment and a blue segment and the way this this orange color and the blue color This orange color and the blue color works is that when you take into account other big jumps, what you will see is that you have a segment which is orange and which is disjoint from different paths. And the moment they start to meet each other, they will be painted blue. So the thing is that those orange guys are independent. Yes. Yes. Did were you trying to ask the question, Nina? Oops. Let me uh there's a question in the chat. No? question in the chat no what's the question I don't see a question I think you can maybe it was privately but I don't see a question in the chat okay um I'm I'm all right so I I I'll just move on where was I yeah so the orange guys are in basically independent and Are basically independent, and those blue guys are the bits when you see some kind of dependence. And yeah, so the set of those admissible particles, I mean, those that you see at the very end, right, each of those positions of those guys, actually, can be decomposed in the following fashion. So you have this, when you, you know, you go along here, you see a blue path, you know, a path in the orange colour. As path in the orange color, then is this one green big jump, and then there's the red path. All right, so what's the contribution of each color? So if you really believe that those big jumps are a sparse subset of your random tree, this means that if you take two green edges, they will be far away from each other on the tree. And this means that those paths will meet. Halves will meet very high up. Hence, this blue bit will be relatively short. In conclusion, this red part is negligible. So now let's focus on the red part. And this is something that requires some. This is something that requires some computation, but since the maximum mal displacement grows faster than linear, right, this grows like n to the power 1 over r, when you look where those displacement happened, I mean in which generation it's more likely that they will happen in a generation which is close to n. So this in turn tells you that those, you know, those red segments they also be relatively short. So this red guy Short. So, this red guy can also be neglected. All right, but this tells us that the positions of these admissible particles asymptotically are just the orange parts and those green jumps, right? And those guys are independent. So, we are quite happy. So, now we need to compare those two. Those two. So you know that asymptotically, your rightmost particle is consisting of these big jumps, this NX, and the position of the typical particles. And you know that this big jump, well, it is big. And those typical particles, this is their behavior should be asymptotically normal. So you can do the same optimization game that we do did previously Game that we did previously. And you would come to a conclusion that the contribution of those typical particles, it is also of the order n to the 2 minus 1 over r. So this is the exact same exponent that we did get from this optimization. On the other hand, we know what is the behavior of the biggest particle, right? So this is, you know, we have the leading term, and we know what are the fluctuations. So now we just need to compare those two, right? So if r is small, then, you know, whatever the contribution of the typical particles is, it's scaled by the fluctuations of the biggest particle. And, you know, this rightmost particle, it scales the same as the biggest. It scales the same as the biggest particle, and you get this nice limit. Moreover, there's one comment that I would like to make at this point, that is, we can actually show that the difference between the right-most particle and the biggest displacement is negligible in compared with the order of the fluctuations. So, this Mn, this is. So, this Mn, this is something that consists of the maximum of your guys, and this Mn, this is something that has the sums. So, this line somehow really screams that one particle did all the job. So what happens when R is big? Well, you need to go back to our picture, right? And what you do, you first look at the green displacements, at the green edges, right? And this fixes the first order, right? And then you know that the contribution of the orange parts should somehow kill the fluctuations of those green guys. Of those green guys. So, what you can do is start looking at those orange paths and see whatever is the biggest. And as I mentioned, you know, in this case, there are actually a lot of those green jumps. Hence, there will be a lot of those orange paths. So, you have a very big number of those orange paths, which in turn gives you a more balanced behavior. Behavior. And now you can basically play again the same optimization game and say that, all right, so this is the contribution of those orange guys is in fact of the other n to the two minus one over r, right? And if you do this optimization, right, you can squeeze out the limiting constant. All right, so what happens in the boundary case? Well, you notice that Well, you notice that both of those exponents are equal to one-half, right? So now, if the contribution of the typical particles should be, is of this order, then you would conclude that they will actually fluctuate. All right, so what, but still, you can work around that, right? You can, you know, take a family of independent Gaussians and if... Gaussians, and if you look at the rightmost particle, after centering and scaling, you will see again a maximum of those admissible particles, right? So you see the proper centering of the rightmost position, and then those typical displacement would be asymptotically Gaussian. But remember that we did decouple the whole thing, right? So we can still basically work as if we would. If we would be working with a family of independent guys, right? So, still, you know, centering this rightmost particle is the same as, you know, pardon, the cumulative distribution function of the rightmost particle is the same as those big displacements with some random Gaussian shifts. But still, this is something which is manageable. So you do the exact same computations that we did for the maxima, right? For the maxima, right? The only thing is that you know, this Gaussian shift transfers to this integral, and here, you know, this big displacement transfers to this e to the minus e, pardon. And then, well, the Gaussian and the variable transfer to this to this Gaussian term, right? But still, if you look at this integral as a function of t, this is still a constant times e to the minus minus t. So not So, nothing really changes. I mean, those fluctuations they do contribute, but the contribution can be hidden in a constant. All right, so this is that. I mean, in the case where R is very small, the behavior of the whole system is really governed by one big jump, right? And in this boundary case, this this this this uh one big jump is is supplemented by this this Gaussian Gaussian shift uh however this this contribution can be can be hidden in this in this constant gamma so in particular this this gamma it's it is not a this it is not a continuous function of our parameters right and in if r is big then you know you have a lot of the typical particles that somehow contribute which gives you a more balanced Which gives you a more balanced behavior. Yeah, so remember that the way that we started our analysis is first by looking at the biggest displacement. And on one hand, it did give us all the information we needed, or it allowed us to decouple the whole thing. And this was the key step which we needed in our analysis. All right, so. Alright, so this was based on a joint paper with Nina and Thomas, and it's available on archive. And let me just mention that this first order asymptotic was established by Nina in the 2000s. And the large deviations for the stretched exponential random variables go back to the Nagaev in the late 60s. However, it's not an easy thing to read. Alright, so I'm out. All right, so I'm out of slides, so I guess this is a good place to stop. Thank you. Thank you very much, Piota. So maybe you can unmute people, Omer. And so are there other questions to Piotr, as long as the screen is still shared? Some questions were already answered in the chat. were already answered in the chat, I see. Seems that they're all muted currently. Yes, I know it's here. So we will stop. It seems that there are no questions right now. So then I think oh you stop the recording for questions. We'll stop now the recording and start it again for Samstock.